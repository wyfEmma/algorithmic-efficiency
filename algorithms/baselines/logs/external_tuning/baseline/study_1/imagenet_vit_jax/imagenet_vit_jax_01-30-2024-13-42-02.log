python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_1 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3390244169 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_01-30-2024-13-42-02.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0130 13:42:23.273298 140184451094336 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax.
I0130 13:42:24.281774 140184451094336 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0130 13:42:24.282432 140184451094336 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0130 13:42:24.282564 140184451094336 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0130 13:42:24.283888 140184451094336 submission_runner.py:542] Using RNG seed 3390244169
I0130 13:42:25.403271 140184451094336 submission_runner.py:551] --- Tuning run 1/5 ---
I0130 13:42:25.403483 140184451094336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1.
I0130 13:42:25.403867 140184451094336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1/hparams.json.
I0130 13:42:25.587249 140184451094336 submission_runner.py:206] Initializing dataset.
I0130 13:42:25.602551 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:42:25.612580 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:42:26.000024 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:42:35.199589 140184451094336 submission_runner.py:213] Initializing model.
I0130 13:42:44.331734 140184451094336 submission_runner.py:255] Initializing optimizer.
I0130 13:42:45.490308 140184451094336 submission_runner.py:262] Initializing metrics bundle.
I0130 13:42:45.490490 140184451094336 submission_runner.py:280] Initializing checkpoint and logger.
I0130 13:42:45.491719 140184451094336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0130 13:42:45.491863 140184451094336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0130 13:42:45.841309 140184451094336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0130 13:42:46.158723 140184451094336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1/flags_0.json.
I0130 13:42:46.172049 140184451094336 submission_runner.py:314] Starting training loop.
I0130 13:43:28.898046 140022527284992 logging_writer.py:48] [0] global_step=0, grad_norm=0.32914111018180847, loss=6.907756805419922
I0130 13:43:28.915353 140184451094336 spec.py:321] Evaluating on the training split.
I0130 13:43:28.924068 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:43:28.932916 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:43:29.015213 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:43:46.178168 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 13:43:46.186662 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:43:46.199814 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:43:46.278445 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:44:03.856960 140184451094336 spec.py:349] Evaluating on the test split.
I0130 13:44:03.864255 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 13:44:03.869584 140184451094336 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0130 13:44:03.920896 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 13:44:09.141895 140184451094336 submission_runner.py:408] Time since start: 82.97s, 	Step: 1, 	{'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.74321413040161, 'total_duration': 82.96978783607483, 'accumulated_submission_time': 42.74321413040161, 'accumulated_eval_time': 40.2264838218689, 'accumulated_logging_time': 0}
I0130 13:44:09.161339 139990482802432 logging_writer.py:48] [1] accumulated_eval_time=40.226484, accumulated_logging_time=0, accumulated_submission_time=42.743214, global_step=1, preemption_count=0, score=42.743214, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=82.969788, train/accuracy=0.000918, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0130 13:45:12.495258 140021361272576 logging_writer.py:48] [100] global_step=100, grad_norm=0.3865211009979248, loss=6.906054973602295
I0130 13:45:55.666210 140021369665280 logging_writer.py:48] [200] global_step=200, grad_norm=0.3772917091846466, loss=6.898754119873047
I0130 13:46:41.030359 140021361272576 logging_writer.py:48] [300] global_step=300, grad_norm=0.541094183921814, loss=6.8608527183532715
I0130 13:47:26.226457 140021369665280 logging_writer.py:48] [400] global_step=400, grad_norm=0.6423545479774475, loss=6.81467866897583
I0130 13:48:11.336325 140021361272576 logging_writer.py:48] [500] global_step=500, grad_norm=1.0609103441238403, loss=6.820887565612793
I0130 13:48:56.322953 140021369665280 logging_writer.py:48] [600] global_step=600, grad_norm=1.0887295007705688, loss=6.748291969299316
I0130 13:49:41.642390 140021361272576 logging_writer.py:48] [700] global_step=700, grad_norm=0.902330219745636, loss=6.688329696655273
I0130 13:50:26.519514 140021369665280 logging_writer.py:48] [800] global_step=800, grad_norm=1.198623776435852, loss=6.623266696929932
I0130 13:51:09.602023 140184451094336 spec.py:321] Evaluating on the training split.
I0130 13:51:21.457688 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 13:51:29.609147 140184451094336 spec.py:349] Evaluating on the test split.
I0130 13:51:31.354834 140184451094336 submission_runner.py:408] Time since start: 525.18s, 	Step: 897, 	{'train/accuracy': 0.013144531287252903, 'train/loss': 6.433816909790039, 'validation/accuracy': 0.012319999746978283, 'validation/loss': 6.444516181945801, 'validation/num_examples': 50000, 'test/accuracy': 0.009400000795722008, 'test/loss': 6.486231803894043, 'test/num_examples': 10000, 'score': 463.12793254852295, 'total_duration': 525.1827142238617, 'accumulated_submission_time': 463.12793254852295, 'accumulated_eval_time': 61.97929525375366, 'accumulated_logging_time': 0.0289614200592041}
I0130 13:51:31.372377 139990491195136 logging_writer.py:48] [897] accumulated_eval_time=61.979295, accumulated_logging_time=0.028961, accumulated_submission_time=463.127933, global_step=897, preemption_count=0, score=463.127933, test/accuracy=0.009400, test/loss=6.486232, test/num_examples=10000, total_duration=525.182714, train/accuracy=0.013145, train/loss=6.433817, validation/accuracy=0.012320, validation/loss=6.444516, validation/num_examples=50000
I0130 13:51:33.029853 139990499587840 logging_writer.py:48] [900] global_step=900, grad_norm=0.9506202936172485, loss=6.568531513214111
I0130 13:52:13.105429 139990491195136 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.217010259628296, loss=6.588651180267334
I0130 13:52:58.235827 139990499587840 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4745036363601685, loss=6.617262363433838
I0130 13:53:43.477166 139990491195136 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3848999738693237, loss=6.459885597229004
I0130 13:54:28.957737 139990499587840 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2306503057479858, loss=6.666186809539795
I0130 13:55:14.166765 139990491195136 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.5689690113067627, loss=6.416594505310059
I0130 13:55:58.949455 139990499587840 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.6667418479919434, loss=6.666590690612793
I0130 13:56:44.126224 139990491195136 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.7794758081436157, loss=6.2571210861206055
I0130 13:57:29.291961 139990499587840 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.4973371028900146, loss=6.252086639404297
I0130 13:58:14.416240 139990491195136 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.6543091535568237, loss=6.26201057434082
I0130 13:58:31.606652 140184451094336 spec.py:321] Evaluating on the training split.
I0130 13:58:43.346545 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 13:58:51.394227 140184451094336 spec.py:349] Evaluating on the test split.
I0130 13:58:53.009392 140184451094336 submission_runner.py:408] Time since start: 966.84s, 	Step: 1840, 	{'train/accuracy': 0.04189452901482582, 'train/loss': 5.843362808227539, 'validation/accuracy': 0.03983999788761139, 'validation/loss': 5.8705878257751465, 'validation/num_examples': 50000, 'test/accuracy': 0.033400002866983414, 'test/loss': 5.988006591796875, 'test/num_examples': 10000, 'score': 883.3025920391083, 'total_duration': 966.8372595310211, 'accumulated_submission_time': 883.3025920391083, 'accumulated_eval_time': 83.38200998306274, 'accumulated_logging_time': 0.056799888610839844}
I0130 13:58:53.026873 139990499587840 logging_writer.py:48] [1840] accumulated_eval_time=83.382010, accumulated_logging_time=0.056800, accumulated_submission_time=883.302592, global_step=1840, preemption_count=0, score=883.302592, test/accuracy=0.033400, test/loss=5.988007, test/num_examples=10000, total_duration=966.837260, train/accuracy=0.041895, train/loss=5.843363, validation/accuracy=0.039840, validation/loss=5.870588, validation/num_examples=50000
I0130 13:59:17.454396 139990491195136 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.0175461769104004, loss=6.21044921875
I0130 14:00:00.612098 139990499587840 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.7718685865402222, loss=6.453032493591309
I0130 14:00:45.640396 139990491195136 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.9056745767593384, loss=6.120102405548096
I0130 14:01:30.724100 139990499587840 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.657209873199463, loss=6.135160446166992
I0130 14:02:16.012695 139990491195136 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.8283368349075317, loss=6.090671062469482
I0130 14:03:00.680885 139990499587840 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.6611088514328003, loss=6.027038097381592
I0130 14:03:45.656572 139990491195136 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.7603594064712524, loss=6.551357269287109
I0130 14:04:30.878906 139990499587840 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.23032808303833, loss=6.031240463256836
I0130 14:05:15.930330 139990491195136 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.5463231801986694, loss=6.296196937561035
I0130 14:05:53.425520 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:06:05.490081 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:06:13.598607 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:06:15.217582 140184451094336 submission_runner.py:408] Time since start: 1409.05s, 	Step: 2785, 	{'train/accuracy': 0.07291015237569809, 'train/loss': 5.394949436187744, 'validation/accuracy': 0.06604000180959702, 'validation/loss': 5.446426868438721, 'validation/num_examples': 50000, 'test/accuracy': 0.049400001764297485, 'test/loss': 5.625874996185303, 'test/num_examples': 10000, 'score': 1303.642668247223, 'total_duration': 1409.0454559326172, 'accumulated_submission_time': 1303.642668247223, 'accumulated_eval_time': 105.17406916618347, 'accumulated_logging_time': 0.08444643020629883}
I0130 14:06:15.235379 139990499587840 logging_writer.py:48] [2785] accumulated_eval_time=105.174069, accumulated_logging_time=0.084446, accumulated_submission_time=1303.642668, global_step=2785, preemption_count=0, score=1303.642668, test/accuracy=0.049400, test/loss=5.625875, test/num_examples=10000, total_duration=1409.045456, train/accuracy=0.072910, train/loss=5.394949, validation/accuracy=0.066040, validation/loss=5.446427, validation/num_examples=50000
I0130 14:06:21.658457 139990491195136 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.2406537532806396, loss=6.029984951019287
I0130 14:07:01.871467 139990499587840 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.8309412002563477, loss=5.974533557891846
I0130 14:07:46.879390 139990491195136 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4174001216888428, loss=6.379247665405273
I0130 14:08:32.019082 139990499587840 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.8938871622085571, loss=5.92416524887085
I0130 14:09:16.965250 139990491195136 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.5646814107894897, loss=6.579897880554199
I0130 14:10:02.053122 139990499587840 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.466343879699707, loss=5.883175849914551
I0130 14:10:46.984909 139990491195136 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.8414762020111084, loss=5.899253845214844
I0130 14:11:32.172423 139990499587840 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.6443192958831787, loss=5.834131240844727
I0130 14:12:17.422112 139990491195136 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.7070825099945068, loss=5.911438941955566
I0130 14:13:02.496701 139990499587840 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.5703760385513306, loss=6.162324905395508
I0130 14:13:15.220405 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:13:27.158672 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:13:35.382632 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:13:37.006002 140184451094336 submission_runner.py:408] Time since start: 1850.83s, 	Step: 3730, 	{'train/accuracy': 0.10576171427965164, 'train/loss': 5.051905155181885, 'validation/accuracy': 0.09595999866724014, 'validation/loss': 5.101688385009766, 'validation/num_examples': 50000, 'test/accuracy': 0.07540000230073929, 'test/loss': 5.3332037925720215, 'test/num_examples': 10000, 'score': 1723.5683901309967, 'total_duration': 1850.8338613510132, 'accumulated_submission_time': 1723.5683901309967, 'accumulated_eval_time': 126.95962452888489, 'accumulated_logging_time': 0.11334896087646484}
I0130 14:13:37.026851 139990491195136 logging_writer.py:48] [3730] accumulated_eval_time=126.959625, accumulated_logging_time=0.113349, accumulated_submission_time=1723.568390, global_step=3730, preemption_count=0, score=1723.568390, test/accuracy=0.075400, test/loss=5.333204, test/num_examples=10000, total_duration=1850.833861, train/accuracy=0.105762, train/loss=5.051905, validation/accuracy=0.095960, validation/loss=5.101688, validation/num_examples=50000
I0130 14:14:07.146925 139990499587840 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.5859217643737793, loss=5.983300685882568
I0130 14:14:51.622714 139990491195136 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.6097651720046997, loss=5.692972183227539
I0130 14:15:36.706959 139990499587840 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.3656221628189087, loss=6.348203659057617
I0130 14:16:21.607897 139990491195136 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7011265754699707, loss=5.889796257019043
I0130 14:17:06.975316 139990499587840 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.5402050018310547, loss=5.6458420753479
I0130 14:17:52.028118 139990491195136 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.4306156635284424, loss=5.597650527954102
I0130 14:18:37.194782 139990499587840 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.3056679964065552, loss=6.185180187225342
I0130 14:19:22.433911 139990491195136 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.5547940731048584, loss=5.569180965423584
I0130 14:20:07.670854 139990499587840 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.0891996622085571, loss=6.550442695617676
I0130 14:20:37.184187 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:20:49.180150 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:20:57.493286 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:20:59.111443 140184451094336 submission_runner.py:408] Time since start: 2292.94s, 	Step: 4667, 	{'train/accuracy': 0.13990233838558197, 'train/loss': 4.692224025726318, 'validation/accuracy': 0.12933999300003052, 'validation/loss': 4.748983860015869, 'validation/num_examples': 50000, 'test/accuracy': 0.09700000286102295, 'test/loss': 5.055785179138184, 'test/num_examples': 10000, 'score': 2143.667026281357, 'total_duration': 2292.9393298625946, 'accumulated_submission_time': 2143.667026281357, 'accumulated_eval_time': 148.88687324523926, 'accumulated_logging_time': 0.14498591423034668}
I0130 14:20:59.128597 139990491195136 logging_writer.py:48] [4667] accumulated_eval_time=148.886873, accumulated_logging_time=0.144986, accumulated_submission_time=2143.667026, global_step=4667, preemption_count=0, score=2143.667026, test/accuracy=0.097000, test/loss=5.055785, test/num_examples=10000, total_duration=2292.939330, train/accuracy=0.139902, train/loss=4.692224, validation/accuracy=0.129340, validation/loss=4.748984, validation/num_examples=50000
I0130 14:21:12.762649 139990499587840 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.6689212322235107, loss=5.601174354553223
I0130 14:21:54.682282 139990491195136 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.6071416139602661, loss=5.759624481201172
I0130 14:22:39.979624 139990499587840 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.1182467937469482, loss=6.500463008880615
I0130 14:23:26.059708 139990491195136 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.23354971408844, loss=6.299657344818115
I0130 14:24:11.594043 139990499587840 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.7901734113693237, loss=5.324480056762695
I0130 14:24:56.781583 139990491195136 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.8064014911651611, loss=5.577021598815918
I0130 14:25:41.662012 139990499587840 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.7485671043395996, loss=5.381826400756836
I0130 14:26:26.709637 139990491195136 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.3401061296463013, loss=5.8896098136901855
I0130 14:27:12.022162 139990499587840 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.173908233642578, loss=5.451757907867432
I0130 14:27:57.031623 139990491195136 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.603018879890442, loss=5.869570732116699
I0130 14:27:59.420996 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:28:11.588214 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:28:19.740236 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:28:21.368369 140184451094336 submission_runner.py:408] Time since start: 2735.20s, 	Step: 5607, 	{'train/accuracy': 0.19443358480930328, 'train/loss': 4.300422191619873, 'validation/accuracy': 0.17613999545574188, 'validation/loss': 4.393296718597412, 'validation/num_examples': 50000, 'test/accuracy': 0.13379999995231628, 'test/loss': 4.728091716766357, 'test/num_examples': 10000, 'score': 2563.898540019989, 'total_duration': 2735.196244239807, 'accumulated_submission_time': 2563.898540019989, 'accumulated_eval_time': 170.83421778678894, 'accumulated_logging_time': 0.17491602897644043}
I0130 14:28:21.385258 139990499587840 logging_writer.py:48] [5607] accumulated_eval_time=170.834218, accumulated_logging_time=0.174916, accumulated_submission_time=2563.898540, global_step=5607, preemption_count=0, score=2563.898540, test/accuracy=0.133800, test/loss=4.728092, test/num_examples=10000, total_duration=2735.196244, train/accuracy=0.194434, train/loss=4.300422, validation/accuracy=0.176140, validation/loss=4.393297, validation/num_examples=50000
I0130 14:28:59.093636 139990491195136 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.6568408012390137, loss=5.469358921051025
I0130 14:29:44.316768 139990499587840 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.575666904449463, loss=5.2276129722595215
I0130 14:30:29.537880 139990491195136 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.444048523902893, loss=5.618692398071289
I0130 14:31:14.847326 139990499587840 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.5180810689926147, loss=5.03950834274292
I0130 14:32:00.180305 139990491195136 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.4119378328323364, loss=6.423209190368652
I0130 14:32:45.974252 139990499587840 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.4786310195922852, loss=5.886030197143555
I0130 14:33:31.903969 139990491195136 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.153920292854309, loss=6.217538356781006
I0130 14:34:17.518829 139990499587840 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.6855792999267578, loss=5.032085418701172
I0130 14:35:02.668715 139990491195136 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.648835301399231, loss=6.288179397583008
I0130 14:35:21.858832 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:35:33.812963 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:35:43.820076 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:35:45.449711 140184451094336 submission_runner.py:408] Time since start: 3179.28s, 	Step: 6544, 	{'train/accuracy': 0.2354101538658142, 'train/loss': 3.982255458831787, 'validation/accuracy': 0.21879999339580536, 'validation/loss': 4.063016891479492, 'validation/num_examples': 50000, 'test/accuracy': 0.16630001366138458, 'test/loss': 4.450675010681152, 'test/num_examples': 10000, 'score': 2984.3140094280243, 'total_duration': 3179.2775950431824, 'accumulated_submission_time': 2984.3140094280243, 'accumulated_eval_time': 194.42507314682007, 'accumulated_logging_time': 0.2019352912902832}
I0130 14:35:45.470715 139990499587840 logging_writer.py:48] [6544] accumulated_eval_time=194.425073, accumulated_logging_time=0.201935, accumulated_submission_time=2984.314009, global_step=6544, preemption_count=0, score=2984.314009, test/accuracy=0.166300, test/loss=4.450675, test/num_examples=10000, total_duration=3179.277595, train/accuracy=0.235410, train/loss=3.982255, validation/accuracy=0.218800, validation/loss=4.063017, validation/num_examples=50000
I0130 14:36:08.277681 139990491195136 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.7616173028945923, loss=5.0389723777771
I0130 14:36:51.693028 139990499587840 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.5688859224319458, loss=4.962404251098633
I0130 14:37:36.985889 139990491195136 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.139816164970398, loss=6.311656475067139
I0130 14:38:22.306335 139990499587840 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.5026440620422363, loss=5.066608428955078
I0130 14:39:07.362980 139990491195136 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.7703524827957153, loss=4.998900890350342
I0130 14:39:52.509549 139990499587840 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.8418914079666138, loss=4.843635082244873
I0130 14:40:37.651607 139990491195136 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.3002572059631348, loss=5.972620010375977
I0130 14:41:23.012757 139990499587840 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.8652777671813965, loss=4.885505676269531
I0130 14:42:08.860108 139990491195136 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.5086485147476196, loss=6.267533302307129
I0130 14:42:45.832479 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:43:00.484780 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:43:15.124639 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:43:16.749530 140184451094336 submission_runner.py:408] Time since start: 3630.58s, 	Step: 7483, 	{'train/accuracy': 0.2684960961341858, 'train/loss': 3.741316556930542, 'validation/accuracy': 0.25015997886657715, 'validation/loss': 3.8385329246520996, 'validation/num_examples': 50000, 'test/accuracy': 0.19110001623630524, 'test/loss': 4.25723123550415, 'test/num_examples': 10000, 'score': 3404.61616230011, 'total_duration': 3630.5774047374725, 'accumulated_submission_time': 3404.61616230011, 'accumulated_eval_time': 225.34209632873535, 'accumulated_logging_time': 0.2349696159362793}
I0130 14:43:16.772625 139990499587840 logging_writer.py:48] [7483] accumulated_eval_time=225.342096, accumulated_logging_time=0.234970, accumulated_submission_time=3404.616162, global_step=7483, preemption_count=0, score=3404.616162, test/accuracy=0.191100, test/loss=4.257231, test/num_examples=10000, total_duration=3630.577405, train/accuracy=0.268496, train/loss=3.741317, validation/accuracy=0.250160, validation/loss=3.838533, validation/num_examples=50000
I0130 14:43:24.185123 139990491195136 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.7215838432312012, loss=4.823868274688721
I0130 14:44:05.817435 139990499587840 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.7119688987731934, loss=4.730626106262207
I0130 14:44:50.854017 139990491195136 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.6416105031967163, loss=4.914402961730957
I0130 14:45:36.034918 139990499587840 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.5059642791748047, loss=4.772788047790527
I0130 14:46:21.209088 139990491195136 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.1783407926559448, loss=5.76662540435791
I0130 14:47:06.446301 139990499587840 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9468117952346802, loss=4.816465854644775
I0130 14:47:51.606664 139990491195136 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.5175256729125977, loss=4.876980781555176
I0130 14:48:36.913103 139990499587840 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.594990849494934, loss=4.983175754547119
I0130 14:49:21.934156 139990491195136 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.5380440950393677, loss=4.767671585083008
I0130 14:50:07.186121 139990499587840 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.4212690591812134, loss=5.963482856750488
I0130 14:50:16.913456 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:50:28.763100 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:50:40.286279 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:50:41.894946 140184451094336 submission_runner.py:408] Time since start: 4075.72s, 	Step: 8423, 	{'train/accuracy': 0.31019529700279236, 'train/loss': 3.4238476753234863, 'validation/accuracy': 0.2865000069141388, 'validation/loss': 3.551166534423828, 'validation/num_examples': 50000, 'test/accuracy': 0.22040000557899475, 'test/loss': 4.016154766082764, 'test/num_examples': 10000, 'score': 3824.698988676071, 'total_duration': 4075.722831964493, 'accumulated_submission_time': 3824.698988676071, 'accumulated_eval_time': 250.32358050346375, 'accumulated_logging_time': 0.2688426971435547}
I0130 14:50:41.913730 139990491195136 logging_writer.py:48] [8423] accumulated_eval_time=250.323581, accumulated_logging_time=0.268843, accumulated_submission_time=3824.698989, global_step=8423, preemption_count=0, score=3824.698989, test/accuracy=0.220400, test/loss=4.016155, test/num_examples=10000, total_duration=4075.722832, train/accuracy=0.310195, train/loss=3.423848, validation/accuracy=0.286500, validation/loss=3.551167, validation/num_examples=50000
I0130 14:51:13.097325 139990499587840 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.5768166780471802, loss=4.964874744415283
I0130 14:51:58.038055 139990491195136 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.6908015012741089, loss=4.5881757736206055
I0130 14:52:43.535900 139990499587840 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.3443337678909302, loss=5.793058395385742
I0130 14:53:29.410499 139990491195136 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.7369377613067627, loss=4.562419414520264
I0130 14:54:14.535802 139990499587840 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.246155858039856, loss=5.766653060913086
I0130 14:54:59.617553 139990491195136 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.203993797302246, loss=6.206511497497559
I0130 14:55:44.834206 139990499587840 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.0742594003677368, loss=6.216817855834961
I0130 14:56:30.041675 139990491195136 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.6162848472595215, loss=4.468288898468018
I0130 14:57:14.786592 139990499587840 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.1930478811264038, loss=5.396079063415527
I0130 14:57:41.896651 140184451094336 spec.py:321] Evaluating on the training split.
I0130 14:57:53.989781 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 14:58:07.617878 140184451094336 spec.py:349] Evaluating on the test split.
I0130 14:58:09.241576 140184451094336 submission_runner.py:408] Time since start: 4523.07s, 	Step: 9362, 	{'train/accuracy': 0.36613279581069946, 'train/loss': 3.119126558303833, 'validation/accuracy': 0.3231000006198883, 'validation/loss': 3.3199212551116943, 'validation/num_examples': 50000, 'test/accuracy': 0.25030001997947693, 'test/loss': 3.823859453201294, 'test/num_examples': 10000, 'score': 4244.6240670681, 'total_duration': 4523.069453001022, 'accumulated_submission_time': 4244.6240670681, 'accumulated_eval_time': 277.6685001850128, 'accumulated_logging_time': 0.29800844192504883}
I0130 14:58:09.268560 139990491195136 logging_writer.py:48] [9362] accumulated_eval_time=277.668500, accumulated_logging_time=0.298008, accumulated_submission_time=4244.624067, global_step=9362, preemption_count=0, score=4244.624067, test/accuracy=0.250300, test/loss=3.823859, test/num_examples=10000, total_duration=4523.069453, train/accuracy=0.366133, train/loss=3.119127, validation/accuracy=0.323100, validation/loss=3.319921, validation/num_examples=50000
I0130 14:58:24.916217 139990499587840 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.8013845682144165, loss=4.77291202545166
I0130 14:59:07.621220 139990491195136 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3697007894515991, loss=5.024934768676758
I0130 14:59:52.556452 139990499587840 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.5129890441894531, loss=4.463255882263184
I0130 15:00:37.891556 139990491195136 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.3071892261505127, loss=5.316513538360596
I0130 15:01:23.425121 139990499587840 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.6287689208984375, loss=4.433424949645996
I0130 15:02:09.019140 139990491195136 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6824418306350708, loss=4.666973114013672
I0130 15:02:54.221564 139990499587840 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.2442213296890259, loss=5.618276596069336
I0130 15:03:39.667747 139990491195136 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.8271301984786987, loss=4.4341583251953125
I0130 15:04:25.226805 139990499587840 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.503998875617981, loss=4.607532501220703
I0130 15:05:09.491646 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:05:21.877479 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:05:38.476508 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:05:40.089838 140184451094336 submission_runner.py:408] Time since start: 4973.92s, 	Step: 10299, 	{'train/accuracy': 0.3646875023841858, 'train/loss': 3.0610296726226807, 'validation/accuracy': 0.3439199924468994, 'validation/loss': 3.1753134727478027, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.6979408264160156, 'test/num_examples': 10000, 'score': 4664.77720451355, 'total_duration': 4973.9177231788635, 'accumulated_submission_time': 4664.77720451355, 'accumulated_eval_time': 308.2666883468628, 'accumulated_logging_time': 0.3484940528869629}
I0130 15:05:40.108911 139990491195136 logging_writer.py:48] [10299] accumulated_eval_time=308.266688, accumulated_logging_time=0.348494, accumulated_submission_time=4664.777205, global_step=10299, preemption_count=0, score=4664.777205, test/accuracy=0.265700, test/loss=3.697941, test/num_examples=10000, total_duration=4973.917723, train/accuracy=0.364688, train/loss=3.061030, validation/accuracy=0.343920, validation/loss=3.175313, validation/num_examples=50000
I0130 15:05:40.921339 139990499587840 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.635239839553833, loss=4.410063743591309
I0130 15:06:21.675345 139990491195136 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.89126718044281, loss=4.2357072830200195
I0130 15:07:07.157973 139990499587840 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.8231927156448364, loss=4.313989162445068
I0130 15:07:52.337008 139990491195136 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.055945873260498, loss=5.672284126281738
I0130 15:08:37.159781 139990499587840 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.1102499961853027, loss=5.811826229095459
I0130 15:09:22.535685 139990491195136 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.5117419958114624, loss=4.309699058532715
I0130 15:10:07.651640 139990499587840 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.4092563390731812, loss=4.656362056732178
I0130 15:10:52.988844 139990491195136 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.7270973920822144, loss=4.487980842590332
I0130 15:11:38.272550 139990499587840 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.350661277770996, loss=4.459987640380859
I0130 15:12:23.919881 139990491195136 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.2337387800216675, loss=5.477513790130615
I0130 15:12:40.542088 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:12:52.978986 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:13:06.707686 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:13:08.321800 140184451094336 submission_runner.py:408] Time since start: 5422.15s, 	Step: 11238, 	{'train/accuracy': 0.3969140648841858, 'train/loss': 2.903132915496826, 'validation/accuracy': 0.3653799891471863, 'validation/loss': 3.0522048473358154, 'validation/num_examples': 50000, 'test/accuracy': 0.2824000120162964, 'test/loss': 3.593581438064575, 'test/num_examples': 10000, 'score': 5085.150776386261, 'total_duration': 5422.149684429169, 'accumulated_submission_time': 5085.150776386261, 'accumulated_eval_time': 336.04639506340027, 'accumulated_logging_time': 0.37902069091796875}
I0130 15:13:08.345374 139990499587840 logging_writer.py:48] [11238] accumulated_eval_time=336.046395, accumulated_logging_time=0.379021, accumulated_submission_time=5085.150776, global_step=11238, preemption_count=0, score=5085.150776, test/accuracy=0.282400, test/loss=3.593581, test/num_examples=10000, total_duration=5422.149684, train/accuracy=0.396914, train/loss=2.903133, validation/accuracy=0.365380, validation/loss=3.052205, validation/num_examples=50000
I0130 15:13:33.519495 139990491195136 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.4619601964950562, loss=4.178302764892578
I0130 15:14:17.936077 139990499587840 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.4362716674804688, loss=5.077298641204834
I0130 15:15:03.350046 139990491195136 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.3588887453079224, loss=4.444948673248291
I0130 15:15:48.406979 139990499587840 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.6400268077850342, loss=4.445829391479492
I0130 15:16:33.777486 139990491195136 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.4262182712554932, loss=4.253189563751221
I0130 15:17:19.130615 139990499587840 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.7124263048171997, loss=4.2555060386657715
I0130 15:18:04.092379 139990491195136 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.6147722005844116, loss=4.159179210662842
I0130 15:18:48.850922 139990499587840 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.3558157682418823, loss=4.661101341247559
I0130 15:19:34.024292 139990491195136 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.403770923614502, loss=4.27171516418457
I0130 15:20:08.347383 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:20:20.755342 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:20:34.570461 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:20:36.189408 140184451094336 submission_runner.py:408] Time since start: 5870.02s, 	Step: 12177, 	{'train/accuracy': 0.4346679449081421, 'train/loss': 2.682563304901123, 'validation/accuracy': 0.3948400020599365, 'validation/loss': 2.876403570175171, 'validation/num_examples': 50000, 'test/accuracy': 0.3043000102043152, 'test/loss': 3.4292190074920654, 'test/num_examples': 10000, 'score': 5505.0947265625, 'total_duration': 5870.017268896103, 'accumulated_submission_time': 5505.0947265625, 'accumulated_eval_time': 363.88840103149414, 'accumulated_logging_time': 0.41297030448913574}
I0130 15:20:36.209868 139990499587840 logging_writer.py:48] [12177] accumulated_eval_time=363.888401, accumulated_logging_time=0.412970, accumulated_submission_time=5505.094727, global_step=12177, preemption_count=0, score=5505.094727, test/accuracy=0.304300, test/loss=3.429219, test/num_examples=10000, total_duration=5870.017269, train/accuracy=0.434668, train/loss=2.682563, validation/accuracy=0.394840, validation/loss=2.876404, validation/num_examples=50000
I0130 15:20:45.829879 139990491195136 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.7914388179779053, loss=4.367741584777832
I0130 15:21:27.817392 139990499587840 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0961722135543823, loss=6.067523956298828
I0130 15:22:13.769629 139990491195136 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0946519374847412, loss=5.209117889404297
I0130 15:22:59.526216 139990499587840 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.070637583732605, loss=5.784761428833008
I0130 15:23:44.711503 139990491195136 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.610099196434021, loss=4.13007116317749
I0130 15:24:29.815311 139990499587840 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.2522200345993042, loss=5.2811994552612305
I0130 15:25:15.277993 139990491195136 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.3460503816604614, loss=4.543999671936035
I0130 15:26:00.372352 139990499587840 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.1771042346954346, loss=5.742062568664551
I0130 15:26:45.862488 139990491195136 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2454949617385864, loss=5.245078086853027
I0130 15:27:31.021383 139990499587840 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.5202600955963135, loss=4.191991806030273
I0130 15:27:36.591050 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:27:49.599076 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:28:04.783356 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:28:06.395312 140184451094336 submission_runner.py:408] Time since start: 6320.22s, 	Step: 13114, 	{'train/accuracy': 0.4341992139816284, 'train/loss': 2.6847686767578125, 'validation/accuracy': 0.40498000383377075, 'validation/loss': 2.8233370780944824, 'validation/num_examples': 50000, 'test/accuracy': 0.32040002942085266, 'test/loss': 3.36568021774292, 'test/num_examples': 10000, 'score': 5925.405028104782, 'total_duration': 6320.223189115524, 'accumulated_submission_time': 5925.405028104782, 'accumulated_eval_time': 393.6926965713501, 'accumulated_logging_time': 0.45781588554382324}
I0130 15:28:06.418959 139990491195136 logging_writer.py:48] [13114] accumulated_eval_time=393.692697, accumulated_logging_time=0.457816, accumulated_submission_time=5925.405028, global_step=13114, preemption_count=0, score=5925.405028, test/accuracy=0.320400, test/loss=3.365680, test/num_examples=10000, total_duration=6320.223189, train/accuracy=0.434199, train/loss=2.684769, validation/accuracy=0.404980, validation/loss=2.823337, validation/num_examples=50000
I0130 15:28:41.344311 139990499587840 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9956525564193726, loss=6.068511009216309
I0130 15:29:26.984318 139990491195136 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.6473357677459717, loss=4.0821685791015625
I0130 15:30:12.486581 139990499587840 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.4816638231277466, loss=4.161062717437744
I0130 15:30:58.452952 139990491195136 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.874664306640625, loss=4.086776256561279
I0130 15:31:44.397880 139990499587840 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.4722614288330078, loss=4.088231563568115
I0130 15:32:30.118424 139990491195136 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.5021841526031494, loss=4.077174186706543
I0130 15:33:16.015475 139990499587840 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1629490852355957, loss=5.641823768615723
I0130 15:34:01.254909 139990491195136 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4029713869094849, loss=4.061066627502441
I0130 15:34:46.592357 139990499587840 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9028776288032532, loss=5.964545726776123
I0130 15:35:06.717616 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:35:20.073815 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:35:33.977499 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:35:35.590300 140184451094336 submission_runner.py:408] Time since start: 6769.42s, 	Step: 14045, 	{'train/accuracy': 0.4446093738079071, 'train/loss': 2.6638307571411133, 'validation/accuracy': 0.4107399880886078, 'validation/loss': 2.8218674659729004, 'validation/num_examples': 50000, 'test/accuracy': 0.3206000030040741, 'test/loss': 3.385582685470581, 'test/num_examples': 10000, 'score': 6345.643961429596, 'total_duration': 6769.418182611465, 'accumulated_submission_time': 6345.643961429596, 'accumulated_eval_time': 422.5653555393219, 'accumulated_logging_time': 0.495150089263916}
I0130 15:35:35.612496 139990491195136 logging_writer.py:48] [14045] accumulated_eval_time=422.565356, accumulated_logging_time=0.495150, accumulated_submission_time=6345.643961, global_step=14045, preemption_count=0, score=6345.643961, test/accuracy=0.320600, test/loss=3.385583, test/num_examples=10000, total_duration=6769.418183, train/accuracy=0.444609, train/loss=2.663831, validation/accuracy=0.410740, validation/loss=2.821867, validation/num_examples=50000
I0130 15:35:58.010504 139990499587840 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.4328705072402954, loss=4.076382637023926
I0130 15:36:42.425700 139990491195136 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.5900918245315552, loss=4.043241500854492
I0130 15:37:28.410652 139990499587840 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.536202073097229, loss=4.08493185043335
I0130 15:38:14.436918 139990491195136 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.3548963069915771, loss=4.152199745178223
I0130 15:39:00.464163 139990499587840 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.375208854675293, loss=4.0445556640625
I0130 15:39:45.684066 139990491195136 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.1663734912872314, loss=4.38067626953125
I0130 15:40:31.290524 139990499587840 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9381045699119568, loss=5.620486259460449
I0130 15:41:17.105767 139990491195136 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.1366358995437622, loss=5.544763088226318
I0130 15:42:03.065485 139990499587840 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.1665502786636353, loss=4.860767364501953
I0130 15:42:35.649638 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:42:48.413433 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:43:02.877653 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:43:04.487747 140184451094336 submission_runner.py:408] Time since start: 7218.32s, 	Step: 14974, 	{'train/accuracy': 0.4664062261581421, 'train/loss': 2.4795210361480713, 'validation/accuracy': 0.4297599792480469, 'validation/loss': 2.666552782058716, 'validation/num_examples': 50000, 'test/accuracy': 0.33500000834465027, 'test/loss': 3.2387516498565674, 'test/num_examples': 10000, 'score': 6765.623802185059, 'total_duration': 7218.315635204315, 'accumulated_submission_time': 6765.623802185059, 'accumulated_eval_time': 451.4034585952759, 'accumulated_logging_time': 0.5285844802856445}
I0130 15:43:04.507300 139990491195136 logging_writer.py:48] [14974] accumulated_eval_time=451.403459, accumulated_logging_time=0.528584, accumulated_submission_time=6765.623802, global_step=14974, preemption_count=0, score=6765.623802, test/accuracy=0.335000, test/loss=3.238752, test/num_examples=10000, total_duration=7218.315635, train/accuracy=0.466406, train/loss=2.479521, validation/accuracy=0.429760, validation/loss=2.666553, validation/num_examples=50000
I0130 15:43:15.509136 139990499587840 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9825131297111511, loss=5.072032451629639
I0130 15:43:58.033548 139990491195136 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.4970546960830688, loss=4.070737361907959
I0130 15:44:43.551872 139990499587840 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0155032873153687, loss=5.94342041015625
I0130 15:45:29.057210 139990491195136 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.3331108093261719, loss=4.044200420379639
I0130 15:46:14.475103 139990499587840 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.6053345203399658, loss=4.259178161621094
I0130 15:47:00.001780 139990491195136 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.4205092191696167, loss=4.136560440063477
I0130 15:47:45.433106 139990499587840 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0868995189666748, loss=5.037866115570068
I0130 15:48:30.950235 139990491195136 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.8385627269744873, loss=3.9477410316467285
I0130 15:49:16.476523 139990499587840 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.3663662672042847, loss=3.904937982559204
I0130 15:50:01.662912 139990491195136 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.51228666305542, loss=4.03138542175293
I0130 15:50:04.668216 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:50:17.965420 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:50:32.364405 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:50:33.981366 140184451094336 submission_runner.py:408] Time since start: 7667.81s, 	Step: 15908, 	{'train/accuracy': 0.4827929437160492, 'train/loss': 2.4374914169311523, 'validation/accuracy': 0.4357599914073944, 'validation/loss': 2.6655783653259277, 'validation/num_examples': 50000, 'test/accuracy': 0.33640000224113464, 'test/loss': 3.2438104152679443, 'test/num_examples': 10000, 'score': 7185.726698637009, 'total_duration': 7667.809241294861, 'accumulated_submission_time': 7185.726698637009, 'accumulated_eval_time': 480.71657729148865, 'accumulated_logging_time': 0.5597198009490967}
I0130 15:50:34.001656 139990499587840 logging_writer.py:48] [15908] accumulated_eval_time=480.716577, accumulated_logging_time=0.559720, accumulated_submission_time=7185.726699, global_step=15908, preemption_count=0, score=7185.726699, test/accuracy=0.336400, test/loss=3.243810, test/num_examples=10000, total_duration=7667.809241, train/accuracy=0.482793, train/loss=2.437491, validation/accuracy=0.435760, validation/loss=2.665578, validation/num_examples=50000
I0130 15:51:12.149016 139990491195136 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.3408362865447998, loss=3.9585282802581787
I0130 15:51:57.914653 139990499587840 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9536285400390625, loss=5.567601680755615
I0130 15:52:43.379667 139990491195136 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0693360567092896, loss=5.870359897613525
I0130 15:53:29.899193 139990499587840 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.3255970478057861, loss=4.431447982788086
I0130 15:54:15.535588 139990491195136 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9977748990058899, loss=5.249464511871338
I0130 15:55:01.174383 139990499587840 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.5901094675064087, loss=3.758734703063965
I0130 15:55:46.795188 139990491195136 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.505357027053833, loss=3.898956775665283
I0130 15:56:32.418231 139990499587840 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.5783164501190186, loss=4.008469581604004
I0130 15:57:18.066653 139990491195136 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9463340640068054, loss=5.484851837158203
I0130 15:57:34.196466 140184451094336 spec.py:321] Evaluating on the training split.
I0130 15:57:47.775325 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 15:58:01.461458 140184451094336 spec.py:349] Evaluating on the test split.
I0130 15:58:03.095570 140184451094336 submission_runner.py:408] Time since start: 8116.92s, 	Step: 16837, 	{'train/accuracy': 0.4757421910762787, 'train/loss': 2.441497802734375, 'validation/accuracy': 0.4432799816131592, 'validation/loss': 2.600684881210327, 'validation/num_examples': 50000, 'test/accuracy': 0.34950003027915955, 'test/loss': 3.1816678047180176, 'test/num_examples': 10000, 'score': 7605.864891529083, 'total_duration': 8116.923457622528, 'accumulated_submission_time': 7605.864891529083, 'accumulated_eval_time': 509.61567425727844, 'accumulated_logging_time': 0.5903451442718506}
I0130 15:58:03.116378 139990499587840 logging_writer.py:48] [16837] accumulated_eval_time=509.615674, accumulated_logging_time=0.590345, accumulated_submission_time=7605.864892, global_step=16837, preemption_count=0, score=7605.864892, test/accuracy=0.349500, test/loss=3.181668, test/num_examples=10000, total_duration=8116.923458, train/accuracy=0.475742, train/loss=2.441498, validation/accuracy=0.443280, validation/loss=2.600685, validation/num_examples=50000
I0130 15:58:28.828279 139990491195136 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.3663243055343628, loss=3.808234691619873
I0130 15:59:14.886113 139990499587840 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.4019134044647217, loss=3.905024766921997
I0130 16:00:00.436891 139990491195136 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.310082197189331, loss=4.039313793182373
I0130 16:00:45.981969 139990499587840 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.561502456665039, loss=4.039870738983154
I0130 16:01:31.428200 139990491195136 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.1014156341552734, loss=4.4606404304504395
I0130 16:02:17.370011 139990499587840 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9539688229560852, loss=5.064855098724365
I0130 16:03:03.424643 139990491195136 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9478926658630371, loss=5.779280662536621
I0130 16:03:49.737028 139990499587840 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.3463799953460693, loss=4.002158164978027
I0130 16:04:36.024532 139990491195136 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.2234135866165161, loss=3.7780165672302246
I0130 16:05:03.239102 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:05:16.190067 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:05:32.867763 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:05:34.474691 140184451094336 submission_runner.py:408] Time since start: 8568.30s, 	Step: 17761, 	{'train/accuracy': 0.48970702290534973, 'train/loss': 2.3479857444763184, 'validation/accuracy': 0.4527199864387512, 'validation/loss': 2.532827854156494, 'validation/num_examples': 50000, 'test/accuracy': 0.3516000211238861, 'test/loss': 3.1301140785217285, 'test/num_examples': 10000, 'score': 8025.931247711182, 'total_duration': 8568.302577495575, 'accumulated_submission_time': 8025.931247711182, 'accumulated_eval_time': 540.8512902259827, 'accumulated_logging_time': 0.6215465068817139}
I0130 16:05:34.494884 139990499587840 logging_writer.py:48] [17761] accumulated_eval_time=540.851290, accumulated_logging_time=0.621547, accumulated_submission_time=8025.931248, global_step=17761, preemption_count=0, score=8025.931248, test/accuracy=0.351600, test/loss=3.130114, test/num_examples=10000, total_duration=8568.302577, train/accuracy=0.489707, train/loss=2.347986, validation/accuracy=0.452720, validation/loss=2.532828, validation/num_examples=50000
I0130 16:05:50.492590 139990491195136 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.2031646966934204, loss=4.070901870727539
I0130 16:06:33.345688 139990499587840 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.2660397291183472, loss=4.857976913452148
I0130 16:07:19.208162 139990491195136 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.2963086366653442, loss=4.551539897918701
I0130 16:08:05.016678 139990499587840 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.1161715984344482, loss=5.105085372924805
I0130 16:08:50.516515 139990491195136 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.1452502012252808, loss=4.42798376083374
I0130 16:09:36.442822 139990499587840 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.4835338592529297, loss=4.099478244781494
I0130 16:10:21.828127 139990491195136 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1412156820297241, loss=5.9981689453125
I0130 16:11:07.250889 139990499587840 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.2941471338272095, loss=4.2164106369018555
I0130 16:11:53.007932 139990491195136 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.4025217294692993, loss=4.076776027679443
I0130 16:12:34.611629 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:12:48.344026 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:13:03.779152 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:13:05.385515 140184451094336 submission_runner.py:408] Time since start: 9019.21s, 	Step: 18693, 	{'train/accuracy': 0.5077343583106995, 'train/loss': 2.280947208404541, 'validation/accuracy': 0.46017998456954956, 'validation/loss': 2.506450653076172, 'validation/num_examples': 50000, 'test/accuracy': 0.3596000075340271, 'test/loss': 3.1122446060180664, 'test/num_examples': 10000, 'score': 8445.988456726074, 'total_duration': 9019.213403463364, 'accumulated_submission_time': 8445.988456726074, 'accumulated_eval_time': 571.6251528263092, 'accumulated_logging_time': 0.6539442539215088}
I0130 16:13:05.407692 139990499587840 logging_writer.py:48] [18693] accumulated_eval_time=571.625153, accumulated_logging_time=0.653944, accumulated_submission_time=8445.988457, global_step=18693, preemption_count=0, score=8445.988457, test/accuracy=0.359600, test/loss=3.112245, test/num_examples=10000, total_duration=9019.213403, train/accuracy=0.507734, train/loss=2.280947, validation/accuracy=0.460180, validation/loss=2.506451, validation/num_examples=50000
I0130 16:13:08.631121 139990491195136 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.4096095561981201, loss=3.8425846099853516
I0130 16:13:50.620253 139990499587840 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.3600294589996338, loss=4.022330284118652
I0130 16:14:36.252265 139990491195136 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2771260738372803, loss=4.023187637329102
I0130 16:15:21.676813 139990499587840 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.4773527383804321, loss=3.858916759490967
I0130 16:16:07.510971 139990491195136 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.3349521160125732, loss=4.017153739929199
I0130 16:16:52.916445 139990499587840 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1581768989562988, loss=5.649651050567627
I0130 16:17:38.645917 139990491195136 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4244937896728516, loss=3.923802614212036
I0130 16:18:24.267270 139990499587840 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.4578691720962524, loss=3.6599628925323486
I0130 16:19:09.726649 139990491195136 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.4483394622802734, loss=3.851325750350952
I0130 16:19:55.242862 139990499587840 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.1685388088226318, loss=5.486492156982422
I0130 16:20:05.567332 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:20:19.225901 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:20:35.078960 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:20:36.688843 140184451094336 submission_runner.py:408] Time since start: 9470.52s, 	Step: 19624, 	{'train/accuracy': 0.5035741925239563, 'train/loss': 2.2893991470336914, 'validation/accuracy': 0.47039997577667236, 'validation/loss': 2.448329448699951, 'validation/num_examples': 50000, 'test/accuracy': 0.36660000681877136, 'test/loss': 3.0513882637023926, 'test/num_examples': 10000, 'score': 8866.089724779129, 'total_duration': 9470.51671743393, 'accumulated_submission_time': 8866.089724779129, 'accumulated_eval_time': 602.7466416358948, 'accumulated_logging_time': 0.687971830368042}
I0130 16:20:36.710290 139990491195136 logging_writer.py:48] [19624] accumulated_eval_time=602.746642, accumulated_logging_time=0.687972, accumulated_submission_time=8866.089725, global_step=19624, preemption_count=0, score=8866.089725, test/accuracy=0.366600, test/loss=3.051388, test/num_examples=10000, total_duration=9470.516717, train/accuracy=0.503574, train/loss=2.289399, validation/accuracy=0.470400, validation/loss=2.448329, validation/num_examples=50000
I0130 16:21:09.262426 139990499587840 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.804306149482727, loss=3.750309467315674
I0130 16:21:54.810338 139990491195136 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.5427719354629517, loss=3.9264888763427734
I0130 16:22:39.673811 139990499587840 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0637294054031372, loss=5.454963684082031
I0130 16:23:24.880954 139990491195136 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9539249539375305, loss=5.767032623291016
I0130 16:24:10.927168 139990499587840 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.4079002141952515, loss=3.9235126972198486
I0130 16:24:56.778344 139990491195136 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.0608035326004028, loss=4.88385009765625
I0130 16:25:42.560523 139990499587840 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.388729453086853, loss=4.291342258453369
I0130 16:26:28.190408 139990491195136 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.2442630529403687, loss=3.8299460411071777
I0130 16:27:13.750067 139990499587840 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.2570338249206543, loss=4.35007381439209
I0130 16:27:37.097595 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:27:50.953588 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:28:06.528514 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:28:08.132958 140184451094336 submission_runner.py:408] Time since start: 9921.96s, 	Step: 20553, 	{'train/accuracy': 0.5167187452316284, 'train/loss': 2.226701259613037, 'validation/accuracy': 0.4785199761390686, 'validation/loss': 2.39848256111145, 'validation/num_examples': 50000, 'test/accuracy': 0.37620002031326294, 'test/loss': 2.9945783615112305, 'test/num_examples': 10000, 'score': 9286.414787769318, 'total_duration': 9921.960839271545, 'accumulated_submission_time': 9286.414787769318, 'accumulated_eval_time': 633.781985282898, 'accumulated_logging_time': 0.7244741916656494}
I0130 16:28:08.155547 139990491195136 logging_writer.py:48] [20553] accumulated_eval_time=633.781985, accumulated_logging_time=0.724474, accumulated_submission_time=9286.414788, global_step=20553, preemption_count=0, score=9286.414788, test/accuracy=0.376200, test/loss=2.994578, test/num_examples=10000, total_duration=9921.960839, train/accuracy=0.516719, train/loss=2.226701, validation/accuracy=0.478520, validation/loss=2.398483, validation/num_examples=50000
I0130 16:28:27.358072 139990499587840 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.9074418544769287, loss=5.260525226593018
I0130 16:29:12.236096 139990491195136 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.4542272090911865, loss=3.7385945320129395
I0130 16:29:57.771107 139990499587840 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.3476639986038208, loss=3.8042540550231934
I0130 16:30:43.590260 139990491195136 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.0139236450195312, loss=4.8983049392700195
I0130 16:31:29.095129 139990499587840 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.2518165111541748, loss=3.7751002311706543
I0130 16:32:15.247713 139990491195136 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.2995398044586182, loss=3.646676778793335
I0130 16:33:00.696822 139990499587840 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.480989933013916, loss=3.652616024017334
I0130 16:33:46.468720 139990491195136 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.318705439567566, loss=3.670098066329956
I0130 16:34:32.258616 139990499587840 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.086830973625183, loss=5.4201765060424805
I0130 16:35:08.183722 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:35:22.344177 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:35:39.023516 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:35:40.632873 140184451094336 submission_runner.py:408] Time since start: 10374.46s, 	Step: 21480, 	{'train/accuracy': 0.5309960842132568, 'train/loss': 2.138967514038086, 'validation/accuracy': 0.4841599762439728, 'validation/loss': 2.3563876152038574, 'validation/num_examples': 50000, 'test/accuracy': 0.3792000114917755, 'test/loss': 2.9629838466644287, 'test/num_examples': 10000, 'score': 9706.385151386261, 'total_duration': 10374.46075630188, 'accumulated_submission_time': 9706.385151386261, 'accumulated_eval_time': 666.2311322689056, 'accumulated_logging_time': 0.7581882476806641}
I0130 16:35:40.654355 139990491195136 logging_writer.py:48] [21480] accumulated_eval_time=666.231132, accumulated_logging_time=0.758188, accumulated_submission_time=9706.385151, global_step=21480, preemption_count=0, score=9706.385151, test/accuracy=0.379200, test/loss=2.962984, test/num_examples=10000, total_duration=10374.460756, train/accuracy=0.530996, train/loss=2.138968, validation/accuracy=0.484160, validation/loss=2.356388, validation/num_examples=50000
I0130 16:35:49.079163 139990499587840 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.3344711065292358, loss=3.7703537940979004
I0130 16:36:30.852755 139990491195136 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8614662289619446, loss=5.298666000366211
I0130 16:37:16.382009 139990499587840 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.3863635063171387, loss=3.7933812141418457
I0130 16:38:01.962806 139990491195136 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2098157405853271, loss=3.9706575870513916
I0130 16:38:47.522216 139990499587840 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.4404282569885254, loss=3.8240790367126465
I0130 16:39:33.293065 139990491195136 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.2806121110916138, loss=3.980441093444824
I0130 16:40:18.723441 139990499587840 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.4052557945251465, loss=3.8461570739746094
I0130 16:41:04.149121 139990491195136 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2935510873794556, loss=3.86306095123291
I0130 16:41:49.973790 139990499587840 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.1737372875213623, loss=4.0690107345581055
I0130 16:42:35.562033 139990491195136 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.4451884031295776, loss=3.7239270210266113
I0130 16:42:40.712979 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:42:50.609294 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:43:09.445353 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:43:11.089457 140184451094336 submission_runner.py:408] Time since start: 10824.92s, 	Step: 22413, 	{'train/accuracy': 0.5240234136581421, 'train/loss': 2.189439535140991, 'validation/accuracy': 0.4843199849128723, 'validation/loss': 2.3730432987213135, 'validation/num_examples': 50000, 'test/accuracy': 0.37860003113746643, 'test/loss': 2.9851624965667725, 'test/num_examples': 10000, 'score': 10126.384377241135, 'total_duration': 10824.917355537415, 'accumulated_submission_time': 10126.384377241135, 'accumulated_eval_time': 696.6076049804688, 'accumulated_logging_time': 0.7909915447235107}
I0130 16:43:11.110916 139990499587840 logging_writer.py:48] [22413] accumulated_eval_time=696.607605, accumulated_logging_time=0.790992, accumulated_submission_time=10126.384377, global_step=22413, preemption_count=0, score=10126.384377, test/accuracy=0.378600, test/loss=2.985162, test/num_examples=10000, total_duration=10824.917356, train/accuracy=0.524023, train/loss=2.189440, validation/accuracy=0.484320, validation/loss=2.373043, validation/num_examples=50000
I0130 16:43:46.476074 139990491195136 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.4763396978378296, loss=3.5872440338134766
I0130 16:44:30.137305 139990499587840 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0309183597564697, loss=4.699203968048096
I0130 16:45:15.828633 139990491195136 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.4347516298294067, loss=3.6839442253112793
I0130 16:46:01.037837 139990499587840 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.005192518234253, loss=5.7933173179626465
I0130 16:46:46.687386 139990491195136 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0670523643493652, loss=5.3714165687561035
I0130 16:47:32.303150 139990499587840 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0389823913574219, loss=4.869505405426025
I0130 16:48:17.985179 139990491195136 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.2378591299057007, loss=4.046485424041748
I0130 16:49:03.624856 139990499587840 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.05233895778656, loss=4.350937843322754
I0130 16:49:48.939753 139990491195136 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.3966878652572632, loss=3.696483612060547
I0130 16:50:11.500777 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:50:20.976573 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:50:40.785027 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:50:42.394360 140184451094336 submission_runner.py:408] Time since start: 11276.22s, 	Step: 23351, 	{'train/accuracy': 0.5264257788658142, 'train/loss': 2.1538989543914795, 'validation/accuracy': 0.4958399832248688, 'validation/loss': 2.312279224395752, 'validation/num_examples': 50000, 'test/accuracy': 0.3881000280380249, 'test/loss': 2.9460103511810303, 'test/num_examples': 10000, 'score': 10546.718402862549, 'total_duration': 11276.222237586975, 'accumulated_submission_time': 10546.718402862549, 'accumulated_eval_time': 727.5011661052704, 'accumulated_logging_time': 0.8213632106781006}
I0130 16:50:42.416120 139990499587840 logging_writer.py:48] [23351] accumulated_eval_time=727.501166, accumulated_logging_time=0.821363, accumulated_submission_time=10546.718403, global_step=23351, preemption_count=0, score=10546.718403, test/accuracy=0.388100, test/loss=2.946010, test/num_examples=10000, total_duration=11276.222238, train/accuracy=0.526426, train/loss=2.153899, validation/accuracy=0.495840, validation/loss=2.312279, validation/num_examples=50000
I0130 16:51:02.458775 139990491195136 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0039854049682617, loss=5.599103927612305
I0130 16:51:45.656697 139990499587840 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.5201913118362427, loss=3.957730770111084
I0130 16:52:31.141527 139990491195136 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3559763431549072, loss=3.577216148376465
I0130 16:53:17.033892 139990499587840 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3592948913574219, loss=3.831763744354248
I0130 16:54:02.641407 139990491195136 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1869726181030273, loss=3.903078556060791
I0130 16:54:48.072017 139990499587840 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1437604427337646, loss=4.4461669921875
I0130 16:55:33.322908 139990491195136 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.3459959030151367, loss=3.8347458839416504
I0130 16:56:18.880434 139990499587840 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.3547768592834473, loss=3.761136054992676
I0130 16:57:04.511784 139990491195136 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.2782841920852661, loss=3.6207027435302734
I0130 16:57:42.458157 140184451094336 spec.py:321] Evaluating on the training split.
I0130 16:57:52.365254 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 16:58:10.460655 140184451094336 spec.py:349] Evaluating on the test split.
I0130 16:58:12.098455 140184451094336 submission_runner.py:408] Time since start: 11725.93s, 	Step: 24285, 	{'train/accuracy': 0.5478906035423279, 'train/loss': 2.0787417888641357, 'validation/accuracy': 0.5089600086212158, 'validation/loss': 2.2755093574523926, 'validation/num_examples': 50000, 'test/accuracy': 0.3977000117301941, 'test/loss': 2.882901906967163, 'test/num_examples': 10000, 'score': 10966.702552080154, 'total_duration': 11725.926349878311, 'accumulated_submission_time': 10966.702552080154, 'accumulated_eval_time': 757.1414499282837, 'accumulated_logging_time': 0.8539583683013916}
I0130 16:58:12.117952 139990499587840 logging_writer.py:48] [24285] accumulated_eval_time=757.141450, accumulated_logging_time=0.853958, accumulated_submission_time=10966.702552, global_step=24285, preemption_count=0, score=10966.702552, test/accuracy=0.397700, test/loss=2.882902, test/num_examples=10000, total_duration=11725.926350, train/accuracy=0.547891, train/loss=2.078742, validation/accuracy=0.508960, validation/loss=2.275509, validation/num_examples=50000
I0130 16:58:18.523848 139990491195136 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.2651983499526978, loss=4.35563325881958
I0130 16:58:59.687983 139990499587840 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.9690841436386108, loss=4.664713382720947
I0130 16:59:45.346823 139990491195136 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3721877336502075, loss=4.029332637786865
I0130 17:00:30.772872 139990499587840 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.2128441333770752, loss=4.1777496337890625
I0130 17:01:16.437051 139990491195136 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.4455760717391968, loss=3.724627733230591
I0130 17:02:02.405146 139990499587840 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.3043923377990723, loss=3.537712812423706
I0130 17:02:47.788971 139990491195136 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0529931783676147, loss=4.974912643432617
I0130 17:03:33.498033 139990499587840 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.4246951341629028, loss=3.6747429370880127
I0130 17:04:19.237673 139990491195136 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0606982707977295, loss=5.311233043670654
I0130 17:05:04.819350 139990499587840 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.3375576734542847, loss=3.7799932956695557
I0130 17:05:12.406001 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:05:22.291824 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:05:43.169334 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:05:44.777001 140184451094336 submission_runner.py:408] Time since start: 12178.60s, 	Step: 25218, 	{'train/accuracy': 0.5692187547683716, 'train/loss': 1.966854214668274, 'validation/accuracy': 0.5087199807167053, 'validation/loss': 2.2506370544433594, 'validation/num_examples': 50000, 'test/accuracy': 0.40050002932548523, 'test/loss': 2.87434720993042, 'test/num_examples': 10000, 'score': 11386.934196472168, 'total_duration': 12178.604896306992, 'accumulated_submission_time': 11386.934196472168, 'accumulated_eval_time': 789.5124342441559, 'accumulated_logging_time': 0.8832418918609619}
I0130 17:05:44.796800 139990491195136 logging_writer.py:48] [25218] accumulated_eval_time=789.512434, accumulated_logging_time=0.883242, accumulated_submission_time=11386.934196, global_step=25218, preemption_count=0, score=11386.934196, test/accuracy=0.400500, test/loss=2.874347, test/num_examples=10000, total_duration=12178.604896, train/accuracy=0.569219, train/loss=1.966854, validation/accuracy=0.508720, validation/loss=2.250637, validation/num_examples=50000
I0130 17:06:17.990087 139990499587840 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.3403747081756592, loss=4.10723876953125
I0130 17:07:04.975993 139990491195136 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.6501574516296387, loss=3.588381290435791
I0130 17:07:52.067587 139990499587840 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.5436606407165527, loss=3.807286500930786
I0130 17:08:37.811295 139990491195136 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.2163255214691162, loss=3.69828724861145
I0130 17:09:23.505137 139990499587840 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.132323980331421, loss=4.606240749359131
I0130 17:10:08.989219 139990491195136 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1780877113342285, loss=5.8734869956970215
I0130 17:10:54.445440 139990499587840 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2665072679519653, loss=3.7192678451538086
I0130 17:11:40.210728 139990491195136 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0855740308761597, loss=4.05596923828125
I0130 17:12:26.081423 139990499587840 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.2381958961486816, loss=3.935875177383423
I0130 17:12:44.810225 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:12:55.033688 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:13:14.161312 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:13:15.762210 140184451094336 submission_runner.py:408] Time since start: 12629.59s, 	Step: 26143, 	{'train/accuracy': 0.546679675579071, 'train/loss': 2.098712205886841, 'validation/accuracy': 0.5127400159835815, 'validation/loss': 2.264395236968994, 'validation/num_examples': 50000, 'test/accuracy': 0.40050002932548523, 'test/loss': 2.883815288543701, 'test/num_examples': 10000, 'score': 11806.891574144363, 'total_duration': 12629.590085029602, 'accumulated_submission_time': 11806.891574144363, 'accumulated_eval_time': 820.4644253253937, 'accumulated_logging_time': 0.9127271175384521}
I0130 17:13:15.780709 139990491195136 logging_writer.py:48] [26143] accumulated_eval_time=820.464425, accumulated_logging_time=0.912727, accumulated_submission_time=11806.891574, global_step=26143, preemption_count=0, score=11806.891574, test/accuracy=0.400500, test/loss=2.883815, test/num_examples=10000, total_duration=12629.590085, train/accuracy=0.546680, train/loss=2.098712, validation/accuracy=0.512740, validation/loss=2.264395, validation/num_examples=50000
I0130 17:13:38.998978 139990499587840 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.3397146463394165, loss=3.7342073917388916
I0130 17:14:22.743871 139990491195136 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.3172599077224731, loss=3.591219425201416
I0130 17:15:08.469448 139990499587840 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.106109619140625, loss=5.341139316558838
I0130 17:15:54.433655 139990491195136 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1172690391540527, loss=5.683623790740967
I0130 17:16:39.906977 139990499587840 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.3730593919754028, loss=3.6922225952148438
I0130 17:17:25.830899 139990491195136 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.3713887929916382, loss=3.583789348602295
I0130 17:18:11.385376 139990499587840 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.1150057315826416, loss=5.809918403625488
I0130 17:18:57.077876 139990491195136 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.3239892721176147, loss=3.64198637008667
I0130 17:19:42.850975 139990499587840 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.4049769639968872, loss=3.576648235321045
I0130 17:20:16.042493 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:20:26.071989 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:20:47.468357 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:20:49.073648 140184451094336 submission_runner.py:408] Time since start: 13082.90s, 	Step: 27074, 	{'train/accuracy': 0.561718761920929, 'train/loss': 2.005185127258301, 'validation/accuracy': 0.5216599702835083, 'validation/loss': 2.1846542358398438, 'validation/num_examples': 50000, 'test/accuracy': 0.40570002794265747, 'test/loss': 2.8111982345581055, 'test/num_examples': 10000, 'score': 12227.0956864357, 'total_duration': 13082.901546955109, 'accumulated_submission_time': 12227.0956864357, 'accumulated_eval_time': 853.4955842494965, 'accumulated_logging_time': 0.9416196346282959}
I0130 17:20:49.095092 139990491195136 logging_writer.py:48] [27074] accumulated_eval_time=853.495584, accumulated_logging_time=0.941620, accumulated_submission_time=12227.095686, global_step=27074, preemption_count=0, score=12227.095686, test/accuracy=0.405700, test/loss=2.811198, test/num_examples=10000, total_duration=13082.901547, train/accuracy=0.561719, train/loss=2.005185, validation/accuracy=0.521660, validation/loss=2.184654, validation/num_examples=50000
I0130 17:20:59.877036 139990499587840 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.446480393409729, loss=3.5266716480255127
I0130 17:21:42.247758 139990491195136 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.218505620956421, loss=4.080562114715576
I0130 17:22:27.854614 139990499587840 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.2090188264846802, loss=4.456612586975098
I0130 17:23:13.743406 139990491195136 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.246720552444458, loss=3.5421276092529297
I0130 17:23:59.283808 139990499587840 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.3868935108184814, loss=3.7576818466186523
I0130 17:24:45.159314 139990491195136 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9244942665100098, loss=5.318943023681641
I0130 17:25:30.903878 139990499587840 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.302369236946106, loss=3.511721611022949
I0130 17:26:16.588468 139990491195136 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.470914363861084, loss=3.611696720123291
I0130 17:27:02.199738 139990499587840 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4616526365280151, loss=3.6558704376220703
I0130 17:27:47.945339 139990491195136 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.5848970413208008, loss=3.8940560817718506
I0130 17:27:49.483627 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:27:59.663434 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:28:21.506922 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:28:23.100588 140184451094336 submission_runner.py:408] Time since start: 13536.93s, 	Step: 28005, 	{'train/accuracy': 0.5656445026397705, 'train/loss': 1.9743703603744507, 'validation/accuracy': 0.5188599824905396, 'validation/loss': 2.1966774463653564, 'validation/num_examples': 50000, 'test/accuracy': 0.4076000154018402, 'test/loss': 2.8148574829101562, 'test/num_examples': 10000, 'score': 12647.427459478378, 'total_duration': 13536.928488254547, 'accumulated_submission_time': 12647.427459478378, 'accumulated_eval_time': 887.1125380992889, 'accumulated_logging_time': 0.9730579853057861}
I0130 17:28:23.120062 139990499587840 logging_writer.py:48] [28005] accumulated_eval_time=887.112538, accumulated_logging_time=0.973058, accumulated_submission_time=12647.427459, global_step=28005, preemption_count=0, score=12647.427459, test/accuracy=0.407600, test/loss=2.814857, test/num_examples=10000, total_duration=13536.928488, train/accuracy=0.565645, train/loss=1.974370, validation/accuracy=0.518860, validation/loss=2.196677, validation/num_examples=50000
I0130 17:29:02.087544 139990491195136 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.5926008224487305, loss=3.747685432434082
I0130 17:29:47.569661 139990499587840 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.3352634906768799, loss=3.799147605895996
I0130 17:30:33.343280 139990491195136 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.302045464515686, loss=3.67880916595459
I0130 17:31:19.032864 139990499587840 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.0747603178024292, loss=5.120578765869141
I0130 17:32:04.837756 139990491195136 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.3077220916748047, loss=3.604135751724243
I0130 17:32:50.665130 139990499587840 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2430542707443237, loss=4.359004974365234
I0130 17:33:36.393034 139990491195136 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.4773879051208496, loss=3.556020736694336
I0130 17:34:22.293084 139990499587840 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.118451476097107, loss=5.00105094909668
I0130 17:35:07.998774 139990491195136 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.4269121885299683, loss=3.5820999145507812
I0130 17:35:23.212159 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:35:33.523147 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:35:54.098319 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:35:55.698442 140184451094336 submission_runner.py:408] Time since start: 13989.53s, 	Step: 28935, 	{'train/accuracy': 0.560351550579071, 'train/loss': 2.0256459712982178, 'validation/accuracy': 0.5274800062179565, 'validation/loss': 2.1879539489746094, 'validation/num_examples': 50000, 'test/accuracy': 0.41130003333091736, 'test/loss': 2.815878391265869, 'test/num_examples': 10000, 'score': 13067.464753627777, 'total_duration': 13989.526335477829, 'accumulated_submission_time': 13067.464753627777, 'accumulated_eval_time': 919.5988109111786, 'accumulated_logging_time': 1.001277208328247}
I0130 17:35:55.720578 139990499587840 logging_writer.py:48] [28935] accumulated_eval_time=919.598811, accumulated_logging_time=1.001277, accumulated_submission_time=13067.464754, global_step=28935, preemption_count=0, score=13067.464754, test/accuracy=0.411300, test/loss=2.815878, test/num_examples=10000, total_duration=13989.526335, train/accuracy=0.560352, train/loss=2.025646, validation/accuracy=0.527480, validation/loss=2.187954, validation/num_examples=50000
I0130 17:36:22.249913 139990491195136 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.138697862625122, loss=4.986916542053223
I0130 17:37:06.944036 139990499587840 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.422940731048584, loss=3.722393035888672
I0130 17:37:52.829616 139990491195136 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.12374746799469, loss=5.634825229644775
I0130 17:38:38.687160 139990499587840 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.3618247509002686, loss=3.590163230895996
I0130 17:39:24.638558 139990491195136 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.0622414350509644, loss=5.791377544403076
I0130 17:40:10.506086 139990499587840 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.3415089845657349, loss=3.5712857246398926
I0130 17:40:56.069483 139990491195136 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.4295867681503296, loss=3.574739456176758
I0130 17:41:42.029612 139990499587840 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.5079290866851807, loss=3.5264408588409424
I0130 17:42:28.013210 139990491195136 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.4382436275482178, loss=3.5290496349334717
I0130 17:42:56.140610 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:43:06.375737 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:43:27.089090 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:43:28.705766 140184451094336 submission_runner.py:408] Time since start: 14442.53s, 	Step: 29863, 	{'train/accuracy': 0.5651562213897705, 'train/loss': 1.994471549987793, 'validation/accuracy': 0.5298799872398376, 'validation/loss': 2.168131113052368, 'validation/num_examples': 50000, 'test/accuracy': 0.41690000891685486, 'test/loss': 2.7868216037750244, 'test/num_examples': 10000, 'score': 13487.82652425766, 'total_duration': 14442.533663749695, 'accumulated_submission_time': 13487.82652425766, 'accumulated_eval_time': 952.1639549732208, 'accumulated_logging_time': 1.035024881362915}
I0130 17:43:28.732075 139990499587840 logging_writer.py:48] [29863] accumulated_eval_time=952.163955, accumulated_logging_time=1.035025, accumulated_submission_time=13487.826524, global_step=29863, preemption_count=0, score=13487.826524, test/accuracy=0.416900, test/loss=2.786822, test/num_examples=10000, total_duration=14442.533664, train/accuracy=0.565156, train/loss=1.994472, validation/accuracy=0.529880, validation/loss=2.168131, validation/num_examples=50000
I0130 17:43:43.937268 139990491195136 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.2578035593032837, loss=5.469872951507568
I0130 17:44:26.516847 139990499587840 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.331176996231079, loss=3.5466699600219727
I0130 17:45:12.276862 139990491195136 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.3875131607055664, loss=3.4373741149902344
I0130 17:45:58.150952 139990499587840 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.269487977027893, loss=3.4544460773468018
I0130 17:46:43.770163 139990491195136 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.331229329109192, loss=3.694371223449707
I0130 17:47:29.492568 139990499587840 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.4453257322311401, loss=3.5098631381988525
I0130 17:48:15.249706 139990491195136 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3238192796707153, loss=4.193452835083008
I0130 17:49:01.226994 139990499587840 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.3764822483062744, loss=3.4652462005615234
I0130 17:49:46.858042 139990491195136 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.4876127243041992, loss=3.7094790935516357
I0130 17:50:29.127634 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:50:39.452904 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:51:00.755223 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:51:02.353070 140184451094336 submission_runner.py:408] Time since start: 14896.18s, 	Step: 30794, 	{'train/accuracy': 0.5801953077316284, 'train/loss': 1.9031869173049927, 'validation/accuracy': 0.5334599614143372, 'validation/loss': 2.116943836212158, 'validation/num_examples': 50000, 'test/accuracy': 0.4204000234603882, 'test/loss': 2.7451517581939697, 'test/num_examples': 10000, 'score': 13908.166835308075, 'total_duration': 14896.180969238281, 'accumulated_submission_time': 13908.166835308075, 'accumulated_eval_time': 985.389402627945, 'accumulated_logging_time': 1.0702705383300781}
I0130 17:51:02.380528 139990499587840 logging_writer.py:48] [30794] accumulated_eval_time=985.389403, accumulated_logging_time=1.070271, accumulated_submission_time=13908.166835, global_step=30794, preemption_count=0, score=13908.166835, test/accuracy=0.420400, test/loss=2.745152, test/num_examples=10000, total_duration=14896.180969, train/accuracy=0.580195, train/loss=1.903187, validation/accuracy=0.533460, validation/loss=2.116944, validation/num_examples=50000
I0130 17:51:05.186425 139990491195136 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.4154781103134155, loss=3.574018955230713
I0130 17:51:46.395456 139990499587840 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.3455965518951416, loss=3.6079602241516113
I0130 17:52:31.578890 139990491195136 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1082463264465332, loss=5.434693813323975
I0130 17:53:17.477576 139990499587840 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.2204928398132324, loss=5.47062349319458
I0130 17:54:03.053596 139990491195136 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.1759377717971802, loss=5.084168910980225
I0130 17:54:48.719404 139990499587840 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.481140375137329, loss=3.5607454776763916
I0130 17:55:34.391011 139990491195136 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.198642611503601, loss=4.589638710021973
I0130 17:56:19.950233 139990499587840 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.3631579875946045, loss=3.5407731533050537
I0130 17:57:05.746516 139990491195136 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.6736336946487427, loss=3.5658323764801025
I0130 17:57:51.264229 139990499587840 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.160262107849121, loss=5.68737268447876
I0130 17:58:02.411703 140184451094336 spec.py:321] Evaluating on the training split.
I0130 17:58:12.741259 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 17:58:33.775974 140184451094336 spec.py:349] Evaluating on the test split.
I0130 17:58:35.386645 140184451094336 submission_runner.py:408] Time since start: 15349.21s, 	Step: 31726, 	{'train/accuracy': 0.5763476490974426, 'train/loss': 1.9141044616699219, 'validation/accuracy': 0.5375999808311462, 'validation/loss': 2.1075029373168945, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.7345998287200928, 'test/num_examples': 10000, 'score': 14328.138955116272, 'total_duration': 15349.214524507523, 'accumulated_submission_time': 14328.138955116272, 'accumulated_eval_time': 1018.3643229007721, 'accumulated_logging_time': 1.1095900535583496}
I0130 17:58:35.409014 139990491195136 logging_writer.py:48] [31726] accumulated_eval_time=1018.364323, accumulated_logging_time=1.109590, accumulated_submission_time=14328.138955, global_step=31726, preemption_count=0, score=14328.138955, test/accuracy=0.422800, test/loss=2.734600, test/num_examples=10000, total_duration=15349.214525, train/accuracy=0.576348, train/loss=1.914104, validation/accuracy=0.537600, validation/loss=2.107503, validation/num_examples=50000
I0130 17:59:05.424818 139990499587840 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.4020098447799683, loss=3.5506343841552734
I0130 17:59:50.341305 139990491195136 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.0327508449554443, loss=5.4626898765563965
I0130 18:00:36.524833 139990499587840 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.2794499397277832, loss=4.208305358886719
I0130 18:01:22.137729 139990491195136 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.6092370748519897, loss=3.5632455348968506
I0130 18:02:07.839114 139990499587840 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.4074547290802002, loss=3.6168689727783203
I0130 18:02:53.570167 139990491195136 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0490167140960693, loss=4.68865442276001
I0130 18:03:38.947151 139990499587840 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.3395600318908691, loss=3.7471678256988525
I0130 18:04:24.593530 139990491195136 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.2894588708877563, loss=4.128152847290039
I0130 18:05:10.350466 139990499587840 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.456681251525879, loss=3.6705429553985596
I0130 18:05:35.445902 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:05:45.612786 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:06:06.502562 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:06:08.108080 140184451094336 submission_runner.py:408] Time since start: 15801.94s, 	Step: 32657, 	{'train/accuracy': 0.5758984088897705, 'train/loss': 1.9533084630966187, 'validation/accuracy': 0.5400399565696716, 'validation/loss': 2.1200501918792725, 'validation/num_examples': 50000, 'test/accuracy': 0.421500027179718, 'test/loss': 2.763683319091797, 'test/num_examples': 10000, 'score': 14748.117035627365, 'total_duration': 15801.935980558395, 'accumulated_submission_time': 14748.117035627365, 'accumulated_eval_time': 1051.026505947113, 'accumulated_logging_time': 1.14223051071167}
I0130 18:06:08.128150 139990491195136 logging_writer.py:48] [32657] accumulated_eval_time=1051.026506, accumulated_logging_time=1.142231, accumulated_submission_time=14748.117036, global_step=32657, preemption_count=0, score=14748.117036, test/accuracy=0.421500, test/loss=2.763683, test/num_examples=10000, total_duration=15801.935981, train/accuracy=0.575898, train/loss=1.953308, validation/accuracy=0.540040, validation/loss=2.120050, validation/num_examples=50000
I0130 18:06:25.751177 139990499587840 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1905068159103394, loss=4.108162879943848
I0130 18:07:08.750714 139990491195136 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.1737977266311646, loss=4.674890518188477
I0130 18:07:54.436747 139990499587840 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1170897483825684, loss=4.618264198303223
I0130 18:08:40.227745 139990491195136 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.416333794593811, loss=3.6293015480041504
I0130 18:09:25.903333 139990499587840 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.1827960014343262, loss=4.143532752990723
I0130 18:10:11.572136 139990491195136 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.3900598287582397, loss=3.5297272205352783
I0130 18:10:57.382063 139990499587840 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1818897724151611, loss=5.489214897155762
I0130 18:11:43.304284 139990491195136 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.3224904537200928, loss=4.083372592926025
I0130 18:12:28.932928 139990499587840 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.4880778789520264, loss=3.490247964859009
I0130 18:13:08.551525 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:13:19.031008 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:13:40.615998 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:13:42.216959 140184451094336 submission_runner.py:408] Time since start: 16256.04s, 	Step: 33589, 	{'train/accuracy': 0.5841405987739563, 'train/loss': 1.894763708114624, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 2.0790975093841553, 'validation/num_examples': 50000, 'test/accuracy': 0.42670002579689026, 'test/loss': 2.706271171569824, 'test/num_examples': 10000, 'score': 15168.483533620834, 'total_duration': 16256.044842720032, 'accumulated_submission_time': 15168.483533620834, 'accumulated_eval_time': 1084.6919131278992, 'accumulated_logging_time': 1.1724753379821777}
I0130 18:13:42.237151 139990491195136 logging_writer.py:48] [33589] accumulated_eval_time=1084.691913, accumulated_logging_time=1.172475, accumulated_submission_time=15168.483534, global_step=33589, preemption_count=0, score=15168.483534, test/accuracy=0.426700, test/loss=2.706271, test/num_examples=10000, total_duration=16256.044843, train/accuracy=0.584141, train/loss=1.894764, validation/accuracy=0.544820, validation/loss=2.079098, validation/num_examples=50000
I0130 18:13:47.043409 139990499587840 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.3824793100357056, loss=3.569331407546997
I0130 18:14:28.712026 139990491195136 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.4806383848190308, loss=3.4551825523376465
I0130 18:15:14.418703 139990499587840 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.37412428855896, loss=3.5482633113861084
I0130 18:16:00.351207 139990491195136 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2879544496536255, loss=3.922435760498047
I0130 18:16:46.226745 139990499587840 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.2833508253097534, loss=3.3530783653259277
I0130 18:17:32.087702 139990491195136 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.093548059463501, loss=4.235422134399414
I0130 18:18:17.858431 139990499587840 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.304013967514038, loss=5.424083709716797
I0130 18:19:03.604928 139990491195136 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.2747935056686401, loss=3.8521411418914795
I0130 18:19:49.466739 139990499587840 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.204372763633728, loss=4.017388820648193
I0130 18:20:35.025087 139990491195136 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.5148422718048096, loss=3.5710456371307373
I0130 18:20:42.482165 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:20:52.941952 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:21:13.223015 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:21:14.820790 140184451094336 submission_runner.py:408] Time since start: 16708.65s, 	Step: 34518, 	{'train/accuracy': 0.6061328053474426, 'train/loss': 1.7760976552963257, 'validation/accuracy': 0.545799970626831, 'validation/loss': 2.0625627040863037, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.6870129108428955, 'test/num_examples': 10000, 'score': 15588.672712087631, 'total_duration': 16708.648688793182, 'accumulated_submission_time': 15588.672712087631, 'accumulated_eval_time': 1117.0305380821228, 'accumulated_logging_time': 1.202235460281372}
I0130 18:21:14.845450 139990499587840 logging_writer.py:48] [34518] accumulated_eval_time=1117.030538, accumulated_logging_time=1.202235, accumulated_submission_time=15588.672712, global_step=34518, preemption_count=0, score=15588.672712, test/accuracy=0.431300, test/loss=2.687013, test/num_examples=10000, total_duration=16708.648689, train/accuracy=0.606133, train/loss=1.776098, validation/accuracy=0.545800, validation/loss=2.062563, validation/num_examples=50000
I0130 18:21:48.048367 139990491195136 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.08778715133667, loss=4.545159339904785
I0130 18:22:33.251615 139990499587840 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.4204492568969727, loss=3.505959987640381
I0130 18:23:19.443016 139990491195136 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.1052151918411255, loss=5.115506172180176
I0130 18:24:05.374764 139990499587840 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.4249929189682007, loss=3.5467545986175537
I0130 18:24:50.903555 139990491195136 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.8947793245315552, loss=3.8565847873687744
I0130 18:25:36.871282 139990499587840 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.4176726341247559, loss=3.4915480613708496
I0130 18:26:22.497214 139990491195136 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.4274102449417114, loss=3.690183639526367
I0130 18:27:08.501649 139990499587840 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.6330881118774414, loss=3.4903435707092285
I0130 18:27:54.158032 139990491195136 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.1379011869430542, loss=5.2215118408203125
I0130 18:28:15.241598 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:28:25.805615 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:28:47.605659 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:28:49.202822 140184451094336 submission_runner.py:408] Time since start: 17163.03s, 	Step: 35448, 	{'train/accuracy': 0.584667980670929, 'train/loss': 1.8941220045089722, 'validation/accuracy': 0.5425599813461304, 'validation/loss': 2.086238145828247, 'validation/num_examples': 50000, 'test/accuracy': 0.4293000102043152, 'test/loss': 2.7089946269989014, 'test/num_examples': 10000, 'score': 16009.01386475563, 'total_duration': 17163.030723571777, 'accumulated_submission_time': 16009.01386475563, 'accumulated_eval_time': 1150.9917540550232, 'accumulated_logging_time': 1.2357745170593262}
I0130 18:28:49.224816 139990499587840 logging_writer.py:48] [35448] accumulated_eval_time=1150.991754, accumulated_logging_time=1.235775, accumulated_submission_time=16009.013865, global_step=35448, preemption_count=0, score=16009.013865, test/accuracy=0.429300, test/loss=2.708995, test/num_examples=10000, total_duration=17163.030724, train/accuracy=0.584668, train/loss=1.894122, validation/accuracy=0.542560, validation/loss=2.086238, validation/num_examples=50000
I0130 18:29:10.485893 139990491195136 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.4175467491149902, loss=3.633195161819458
I0130 18:29:54.134411 139990499587840 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.1340131759643555, loss=4.367343902587891
I0130 18:30:39.669131 139990491195136 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.3643916845321655, loss=3.4629971981048584
I0130 18:31:25.250983 139990499587840 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.5041753053665161, loss=3.493704080581665
I0130 18:32:11.192201 139990491195136 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.4362013339996338, loss=3.422741413116455
I0130 18:32:56.863565 139990499587840 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.4251450300216675, loss=3.432533025741577
I0130 18:33:42.584819 139990491195136 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.2513142824172974, loss=3.693380355834961
I0130 18:34:28.442696 139990499587840 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.014291524887085, loss=5.26397705078125
I0130 18:35:14.299989 139990491195136 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.4958884716033936, loss=3.5697436332702637
I0130 18:35:49.564716 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:35:59.918293 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:36:21.189339 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:36:22.797394 140184451094336 submission_runner.py:408] Time since start: 17616.63s, 	Step: 36379, 	{'train/accuracy': 0.5904101133346558, 'train/loss': 1.8927258253097534, 'validation/accuracy': 0.5471999645233154, 'validation/loss': 2.0837810039520264, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.709493398666382, 'test/num_examples': 10000, 'score': 16429.294507026672, 'total_duration': 17616.62527346611, 'accumulated_submission_time': 16429.294507026672, 'accumulated_eval_time': 1184.2244091033936, 'accumulated_logging_time': 1.2700214385986328}
I0130 18:36:22.823762 139990499587840 logging_writer.py:48] [36379] accumulated_eval_time=1184.224409, accumulated_logging_time=1.270021, accumulated_submission_time=16429.294507, global_step=36379, preemption_count=0, score=16429.294507, test/accuracy=0.431400, test/loss=2.709493, test/num_examples=10000, total_duration=17616.625273, train/accuracy=0.590410, train/loss=1.892726, validation/accuracy=0.547200, validation/loss=2.083781, validation/num_examples=50000
I0130 18:36:31.620455 139990491195136 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.020410180091858, loss=5.251381874084473
I0130 18:37:13.704964 139990499587840 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.5148566961288452, loss=3.4947433471679688
I0130 18:37:59.348642 139990491195136 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.3403830528259277, loss=3.4690356254577637
I0130 18:38:45.054488 139990499587840 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.461312174797058, loss=3.3884315490722656
I0130 18:39:30.695613 139990491195136 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.3402507305145264, loss=3.340620279312134
I0130 18:40:16.533653 139990499587840 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.4454776048660278, loss=3.8251664638519287
I0130 18:41:02.153457 139990491195136 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.4001654386520386, loss=3.4776363372802734
I0130 18:41:48.184032 139990499587840 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.4195107221603394, loss=3.6261677742004395
I0130 18:42:33.951265 139990491195136 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.505954384803772, loss=3.5739569664001465
I0130 18:43:19.697184 139990499587840 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.4140129089355469, loss=3.4732487201690674
I0130 18:43:23.165927 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:43:33.484437 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:43:54.425957 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:43:56.024965 140184451094336 submission_runner.py:408] Time since start: 18069.85s, 	Step: 37309, 	{'train/accuracy': 0.6047655940055847, 'train/loss': 1.7837172746658325, 'validation/accuracy': 0.556119978427887, 'validation/loss': 2.017679214477539, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.6612629890441895, 'test/num_examples': 10000, 'score': 16849.5801718235, 'total_duration': 18069.852862596512, 'accumulated_submission_time': 16849.5801718235, 'accumulated_eval_time': 1217.0834302902222, 'accumulated_logging_time': 1.3062067031860352}
I0130 18:43:56.048943 139990491195136 logging_writer.py:48] [37309] accumulated_eval_time=1217.083430, accumulated_logging_time=1.306207, accumulated_submission_time=16849.580172, global_step=37309, preemption_count=0, score=16849.580172, test/accuracy=0.439700, test/loss=2.661263, test/num_examples=10000, total_duration=18069.852863, train/accuracy=0.604766, train/loss=1.783717, validation/accuracy=0.556120, validation/loss=2.017679, validation/num_examples=50000
I0130 18:44:33.123302 139990499587840 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5727194547653198, loss=3.5054850578308105
I0130 18:45:18.845602 139990491195136 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.4314758777618408, loss=3.8233611583709717
I0130 18:46:05.377515 139990499587840 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0991510152816772, loss=5.668730735778809
I0130 18:46:51.327895 139990491195136 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.3514634370803833, loss=3.8842926025390625
I0130 18:47:37.220017 139990499587840 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.4853217601776123, loss=3.4196150302886963
I0130 18:48:22.953535 139990491195136 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.4170726537704468, loss=3.4535837173461914
I0130 18:49:08.528430 139990499587840 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.4548144340515137, loss=3.7115402221679688
I0130 18:49:54.297170 139990491195136 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.5579252243041992, loss=3.453522205352783
I0130 18:50:39.994993 139990499587840 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.2785475254058838, loss=4.001104354858398
I0130 18:50:56.200432 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:51:06.503952 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:51:27.977545 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:51:29.577696 140184451094336 submission_runner.py:408] Time since start: 18523.41s, 	Step: 38237, 	{'train/accuracy': 0.592089831829071, 'train/loss': 1.8625539541244507, 'validation/accuracy': 0.5530399680137634, 'validation/loss': 2.0462992191314697, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.677217721939087, 'test/num_examples': 10000, 'score': 17269.676019191742, 'total_duration': 18523.4055621624, 'accumulated_submission_time': 17269.676019191742, 'accumulated_eval_time': 1250.4606716632843, 'accumulated_logging_time': 1.3389804363250732}
I0130 18:51:29.605300 139990491195136 logging_writer.py:48] [38237] accumulated_eval_time=1250.460672, accumulated_logging_time=1.338980, accumulated_submission_time=17269.676019, global_step=38237, preemption_count=0, score=17269.676019, test/accuracy=0.439400, test/loss=2.677218, test/num_examples=10000, total_duration=18523.405562, train/accuracy=0.592090, train/loss=1.862554, validation/accuracy=0.553040, validation/loss=2.046299, validation/num_examples=50000
I0130 18:51:55.207566 139990499587840 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.5002131462097168, loss=3.4256153106689453
I0130 18:52:40.009065 139990491195136 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.3107177019119263, loss=4.057416915893555
I0130 18:53:25.826769 139990499587840 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.1423686742782593, loss=4.5778679847717285
I0130 18:54:11.889723 139990491195136 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.3227827548980713, loss=3.455810546875
I0130 18:54:57.466407 139990499587840 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.219510793685913, loss=4.222101211547852
I0130 18:55:43.636844 139990491195136 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.5256558656692505, loss=3.3803744316101074
I0130 18:56:29.314344 139990499587840 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.4866715669631958, loss=3.428171157836914
I0130 18:57:14.993036 139990491195136 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1192309856414795, loss=4.342160224914551
I0130 18:58:00.736232 139990499587840 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.5591650009155273, loss=3.3779990673065186
I0130 18:58:29.846748 140184451094336 spec.py:321] Evaluating on the training split.
I0130 18:58:40.026835 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 18:59:01.236340 140184451094336 spec.py:349] Evaluating on the test split.
I0130 18:59:02.847084 140184451094336 submission_runner.py:408] Time since start: 18976.67s, 	Step: 39165, 	{'train/accuracy': 0.5917382836341858, 'train/loss': 1.8414897918701172, 'validation/accuracy': 0.5532400012016296, 'validation/loss': 2.0242159366607666, 'validation/num_examples': 50000, 'test/accuracy': 0.43560001254081726, 'test/loss': 2.6622133255004883, 'test/num_examples': 10000, 'score': 17689.861362457275, 'total_duration': 18976.674981355667, 'accumulated_submission_time': 17689.861362457275, 'accumulated_eval_time': 1283.460999250412, 'accumulated_logging_time': 1.3763537406921387}
I0130 18:59:02.870632 139990491195136 logging_writer.py:48] [39165] accumulated_eval_time=1283.460999, accumulated_logging_time=1.376354, accumulated_submission_time=17689.861362, global_step=39165, preemption_count=0, score=17689.861362, test/accuracy=0.435600, test/loss=2.662213, test/num_examples=10000, total_duration=18976.674981, train/accuracy=0.591738, train/loss=1.841490, validation/accuracy=0.553240, validation/loss=2.024216, validation/num_examples=50000
I0130 18:59:17.283543 139990499587840 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.4292064905166626, loss=3.4043478965759277
I0130 19:00:00.215456 139990491195136 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.4852544069290161, loss=3.3713696002960205
I0130 19:00:46.089361 139990499587840 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.195954442024231, loss=4.741022109985352
I0130 19:01:31.796408 139990491195136 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.2419098615646362, loss=5.4300079345703125
I0130 19:02:17.705894 139990499587840 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.4288041591644287, loss=3.587066650390625
I0130 19:03:03.428936 139990491195136 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.4635909795761108, loss=3.65734601020813
I0130 19:03:48.964224 139990499587840 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.3298683166503906, loss=3.3436553478240967
I0130 19:04:34.765973 139990491195136 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.1369892358779907, loss=5.350645542144775
I0130 19:05:20.218027 139990499587840 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.313336968421936, loss=3.9026432037353516
I0130 19:06:03.160015 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:06:13.714907 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:06:34.517082 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:06:36.110666 140184451094336 submission_runner.py:408] Time since start: 19429.94s, 	Step: 40095, 	{'train/accuracy': 0.6004687547683716, 'train/loss': 1.7826272249221802, 'validation/accuracy': 0.5520200133323669, 'validation/loss': 2.0079150199890137, 'validation/num_examples': 50000, 'test/accuracy': 0.4385000169277191, 'test/loss': 2.6310582160949707, 'test/num_examples': 10000, 'score': 18110.095131635666, 'total_duration': 19429.93856573105, 'accumulated_submission_time': 18110.095131635666, 'accumulated_eval_time': 1316.411651134491, 'accumulated_logging_time': 1.409106969833374}
I0130 19:06:36.131084 139990491195136 logging_writer.py:48] [40095] accumulated_eval_time=1316.411651, accumulated_logging_time=1.409107, accumulated_submission_time=18110.095132, global_step=40095, preemption_count=0, score=18110.095132, test/accuracy=0.438500, test/loss=2.631058, test/num_examples=10000, total_duration=19429.938566, train/accuracy=0.600469, train/loss=1.782627, validation/accuracy=0.552020, validation/loss=2.007915, validation/num_examples=50000
I0130 19:06:38.536162 139990499587840 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.5245176553726196, loss=3.438066005706787
I0130 19:07:19.555114 139990491195136 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.445359706878662, loss=3.8905630111694336
I0130 19:08:05.364559 139990499587840 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.5716115236282349, loss=3.379852771759033
I0130 19:08:51.378832 139990491195136 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.4634934663772583, loss=3.574855089187622
I0130 19:09:37.048400 139990499587840 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.15788996219635, loss=5.319512367248535
I0130 19:10:22.793723 139990491195136 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.5002436637878418, loss=3.478807210922241
I0130 19:11:08.472852 139990499587840 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.4618339538574219, loss=3.508358955383301
I0130 19:11:54.296449 139990491195136 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7043888568878174, loss=3.525538921356201
I0130 19:12:40.009077 139990499587840 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.2336658239364624, loss=4.708193302154541
I0130 19:13:25.680293 139990491195136 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.5600725412368774, loss=3.3237500190734863
I0130 19:13:36.249295 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:13:46.509035 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:14:07.139033 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:14:08.740339 140184451094336 submission_runner.py:408] Time since start: 19882.57s, 	Step: 41025, 	{'train/accuracy': 0.6124023199081421, 'train/loss': 1.7426855564117432, 'validation/accuracy': 0.5575399994850159, 'validation/loss': 1.9958285093307495, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.604980707168579, 'test/num_examples': 10000, 'score': 18530.15157198906, 'total_duration': 19882.56823849678, 'accumulated_submission_time': 18530.15157198906, 'accumulated_eval_time': 1348.9026863574982, 'accumulated_logging_time': 1.444082498550415}
I0130 19:14:08.761100 139990499587840 logging_writer.py:48] [41025] accumulated_eval_time=1348.902686, accumulated_logging_time=1.444082, accumulated_submission_time=18530.151572, global_step=41025, preemption_count=0, score=18530.151572, test/accuracy=0.447900, test/loss=2.604981, test/num_examples=10000, total_duration=19882.568238, train/accuracy=0.612402, train/loss=1.742686, validation/accuracy=0.557540, validation/loss=1.995829, validation/num_examples=50000
I0130 19:14:39.158050 139990491195136 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.3878079652786255, loss=3.3152060508728027
I0130 19:15:24.561766 139990499587840 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.177536964416504, loss=4.383570194244385
I0130 19:16:10.429945 139990491195136 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.8958839178085327, loss=3.3896644115448
I0130 19:16:56.139192 139990499587840 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7174104452133179, loss=3.5101404190063477
I0130 19:17:42.048210 139990491195136 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.3734245300292969, loss=3.3524107933044434
I0130 19:18:27.999319 139990499587840 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.6338883638381958, loss=3.4890987873077393
I0130 19:19:13.733822 139990491195136 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1394802331924438, loss=5.269253253936768
I0130 19:19:59.517717 139990499587840 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.6430630683898926, loss=3.3965532779693604
I0130 19:20:45.351258 139990491195136 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1488314867019653, loss=4.897794246673584
I0130 19:21:09.080401 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:21:19.737271 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:21:40.718948 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:21:42.325299 140184451094336 submission_runner.py:408] Time since start: 20336.15s, 	Step: 41953, 	{'train/accuracy': 0.6009570360183716, 'train/loss': 1.8395626544952393, 'validation/accuracy': 0.5607799887657166, 'validation/loss': 2.0273380279541016, 'validation/num_examples': 50000, 'test/accuracy': 0.4416000247001648, 'test/loss': 2.6486172676086426, 'test/num_examples': 10000, 'score': 18950.41338968277, 'total_duration': 20336.153192281723, 'accumulated_submission_time': 18950.41338968277, 'accumulated_eval_time': 1382.1475772857666, 'accumulated_logging_time': 1.474935531616211}
I0130 19:21:42.346737 139990499587840 logging_writer.py:48] [41953] accumulated_eval_time=1382.147577, accumulated_logging_time=1.474936, accumulated_submission_time=18950.413390, global_step=41953, preemption_count=0, score=18950.413390, test/accuracy=0.441600, test/loss=2.648617, test/num_examples=10000, total_duration=20336.153192, train/accuracy=0.600957, train/loss=1.839563, validation/accuracy=0.560780, validation/loss=2.027338, validation/num_examples=50000
I0130 19:22:01.562641 139990491195136 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.4287033081054688, loss=4.440485000610352
I0130 19:22:45.241504 139990499587840 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.2586638927459717, loss=3.8008341789245605
I0130 19:23:30.719884 139990491195136 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.3272407054901123, loss=3.505321502685547
I0130 19:24:16.714363 139990499587840 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.3786098957061768, loss=3.7615036964416504
I0130 19:25:02.110917 139990491195136 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.3608742952346802, loss=3.7995500564575195
I0130 19:25:47.905570 139990499587840 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.2434847354888916, loss=4.171104907989502
I0130 19:26:33.819456 139990491195136 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.448113203048706, loss=3.321528434753418
I0130 19:27:19.634912 139990499587840 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.469867467880249, loss=3.6541285514831543
I0130 19:28:05.541733 139990491195136 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.4303631782531738, loss=3.510758399963379
I0130 19:28:42.676940 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:28:53.318981 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:29:12.567250 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:29:14.187936 140184451094336 submission_runner.py:408] Time since start: 20788.02s, 	Step: 42883, 	{'train/accuracy': 0.60546875, 'train/loss': 1.7532869577407837, 'validation/accuracy': 0.5643599629402161, 'validation/loss': 1.944724678993225, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.5693717002868652, 'test/num_examples': 10000, 'score': 19370.686596870422, 'total_duration': 20788.01582312584, 'accumulated_submission_time': 19370.686596870422, 'accumulated_eval_time': 1413.6585688591003, 'accumulated_logging_time': 1.506608247756958}
I0130 19:29:14.214008 139990499587840 logging_writer.py:48] [42883] accumulated_eval_time=1413.658569, accumulated_logging_time=1.506608, accumulated_submission_time=19370.686597, global_step=42883, preemption_count=0, score=19370.686597, test/accuracy=0.447900, test/loss=2.569372, test/num_examples=10000, total_duration=20788.015823, train/accuracy=0.605469, train/loss=1.753287, validation/accuracy=0.564360, validation/loss=1.944725, validation/num_examples=50000
I0130 19:29:21.427212 139990491195136 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.4553401470184326, loss=3.3533401489257812
I0130 19:30:03.214203 139990499587840 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6869198083877563, loss=3.491180419921875
I0130 19:30:48.789603 139990491195136 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5840054750442505, loss=3.369037628173828
I0130 19:31:35.080704 139990499587840 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.4060043096542358, loss=3.3802695274353027
I0130 19:32:20.700336 139990491195136 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.4394550323486328, loss=3.429361343383789
I0130 19:33:06.529471 139990499587840 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.2067946195602417, loss=4.12631893157959
I0130 19:33:51.815754 139990491195136 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.4502661228179932, loss=3.336242914199829
I0130 19:34:37.885253 139990499587840 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.097196340560913, loss=5.409968376159668
I0130 19:35:23.676632 139990491195136 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.4027913808822632, loss=3.515472173690796
I0130 19:36:09.601143 139990499587840 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.5305083990097046, loss=3.4452288150787354
I0130 19:36:14.264633 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:36:24.736931 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:36:46.248959 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:36:47.850700 140184451094336 submission_runner.py:408] Time since start: 21241.68s, 	Step: 43812, 	{'train/accuracy': 0.6259570121765137, 'train/loss': 1.6537854671478271, 'validation/accuracy': 0.5655800104141235, 'validation/loss': 1.9352000951766968, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.540505886077881, 'test/num_examples': 10000, 'score': 19790.67938184738, 'total_duration': 21241.678597688675, 'accumulated_submission_time': 19790.67938184738, 'accumulated_eval_time': 1447.244621515274, 'accumulated_logging_time': 1.5436184406280518}
I0130 19:36:47.875293 139990491195136 logging_writer.py:48] [43812] accumulated_eval_time=1447.244622, accumulated_logging_time=1.543618, accumulated_submission_time=19790.679382, global_step=43812, preemption_count=0, score=19790.679382, test/accuracy=0.457100, test/loss=2.540506, test/num_examples=10000, total_duration=21241.678598, train/accuracy=0.625957, train/loss=1.653785, validation/accuracy=0.565580, validation/loss=1.935200, validation/num_examples=50000
I0130 19:37:23.792109 139990499587840 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.0539493560791016, loss=5.61409330368042
I0130 19:38:09.357890 139990491195136 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.4942518472671509, loss=3.387824058532715
I0130 19:38:54.979929 139990499587840 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.763765811920166, loss=3.4608097076416016
I0130 19:39:40.466598 139990491195136 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.5325757265090942, loss=3.3167872428894043
I0130 19:40:26.389098 139990499587840 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1492518186569214, loss=4.52647590637207
I0130 19:41:11.825855 139990491195136 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.42760169506073, loss=4.434504508972168
I0130 19:41:57.605679 139990499587840 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7702751159667969, loss=3.359090805053711
I0130 19:42:43.320504 139990491195136 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6089551448822021, loss=3.420222282409668
I0130 19:43:28.831826 139990499587840 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1892744302749634, loss=4.4798431396484375
I0130 19:43:48.107632 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:43:58.669981 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:44:19.282440 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:44:20.882348 140184451094336 submission_runner.py:408] Time since start: 21694.71s, 	Step: 44744, 	{'train/accuracy': 0.60791015625, 'train/loss': 1.7625596523284912, 'validation/accuracy': 0.565779983997345, 'validation/loss': 1.9503344297409058, 'validation/num_examples': 50000, 'test/accuracy': 0.454800009727478, 'test/loss': 2.5629172325134277, 'test/num_examples': 10000, 'score': 20210.85508942604, 'total_duration': 21694.710247278214, 'accumulated_submission_time': 20210.85508942604, 'accumulated_eval_time': 1480.019334077835, 'accumulated_logging_time': 1.577929973602295}
I0130 19:44:20.903836 139990491195136 logging_writer.py:48] [44744] accumulated_eval_time=1480.019334, accumulated_logging_time=1.577930, accumulated_submission_time=20210.855089, global_step=44744, preemption_count=0, score=20210.855089, test/accuracy=0.454800, test/loss=2.562917, test/num_examples=10000, total_duration=21694.710247, train/accuracy=0.607910, train/loss=1.762560, validation/accuracy=0.565780, validation/loss=1.950334, validation/num_examples=50000
I0130 19:44:43.708726 139990499587840 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.5848582983016968, loss=3.3752710819244385
I0130 19:45:27.575917 139990491195136 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.3923842906951904, loss=3.536482572555542
I0130 19:46:13.091931 139990499587840 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.354712724685669, loss=5.518131256103516
I0130 19:46:59.118820 139990491195136 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.4805153608322144, loss=3.3849451541900635
I0130 19:47:44.720676 139990499587840 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.161118745803833, loss=5.556333065032959
I0130 19:48:30.548495 139990491195136 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.4210841655731201, loss=3.6958768367767334
I0130 19:49:16.303736 139990499587840 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.504763126373291, loss=3.4461793899536133
I0130 19:50:02.039054 139990491195136 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.3354575634002686, loss=4.257967948913574
I0130 19:50:47.443307 139990499587840 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.2796010971069336, loss=4.033113956451416
I0130 19:51:20.909048 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:51:31.313626 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:51:52.847615 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:51:54.454979 140184451094336 submission_runner.py:408] Time since start: 22148.28s, 	Step: 45675, 	{'train/accuracy': 0.6089062094688416, 'train/loss': 1.7846567630767822, 'validation/accuracy': 0.568619966506958, 'validation/loss': 1.9842305183410645, 'validation/num_examples': 50000, 'test/accuracy': 0.45410001277923584, 'test/loss': 2.59224271774292, 'test/num_examples': 10000, 'score': 20630.804674863815, 'total_duration': 22148.282872200012, 'accumulated_submission_time': 20630.804674863815, 'accumulated_eval_time': 1513.5652458667755, 'accumulated_logging_time': 1.6084258556365967}
I0130 19:51:54.480611 139990491195136 logging_writer.py:48] [45675] accumulated_eval_time=1513.565246, accumulated_logging_time=1.608426, accumulated_submission_time=20630.804675, global_step=45675, preemption_count=0, score=20630.804675, test/accuracy=0.454100, test/loss=2.592243, test/num_examples=10000, total_duration=22148.282872, train/accuracy=0.608906, train/loss=1.784657, validation/accuracy=0.568620, validation/loss=1.984231, validation/num_examples=50000
I0130 19:52:04.891511 139990499587840 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5183993577957153, loss=3.594517469406128
I0130 19:52:47.173709 139990491195136 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.4897141456604004, loss=3.3457987308502197
I0130 19:53:32.889903 139990499587840 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.5080701112747192, loss=3.4371986389160156
I0130 19:54:19.160711 139990491195136 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.5545779466629028, loss=3.455435276031494
I0130 19:55:04.819749 139990499587840 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.462040662765503, loss=3.3308074474334717
I0130 19:55:50.684644 139990491195136 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.6506741046905518, loss=3.674680709838867
I0130 19:56:36.697407 139990499587840 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.5763120651245117, loss=3.401373863220215
I0130 19:57:22.294343 139990491195136 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.12676203250885, loss=5.438960075378418
I0130 19:58:08.051889 139990499587840 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.4774571657180786, loss=3.3760290145874023
I0130 19:58:53.716068 139990491195136 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.411536455154419, loss=3.34332275390625
I0130 19:58:54.740621 140184451094336 spec.py:321] Evaluating on the training split.
I0130 19:59:05.137114 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 19:59:26.208159 140184451094336 spec.py:349] Evaluating on the test split.
I0130 19:59:27.811094 140184451094336 submission_runner.py:408] Time since start: 22601.64s, 	Step: 46604, 	{'train/accuracy': 0.619433581829071, 'train/loss': 1.6914805173873901, 'validation/accuracy': 0.5701199769973755, 'validation/loss': 1.9323407411575317, 'validation/num_examples': 50000, 'test/accuracy': 0.4522000253200531, 'test/loss': 2.561401605606079, 'test/num_examples': 10000, 'score': 21051.006894111633, 'total_duration': 22601.638983488083, 'accumulated_submission_time': 21051.006894111633, 'accumulated_eval_time': 1546.635691165924, 'accumulated_logging_time': 1.6452360153198242}
I0130 19:59:27.838657 139990499587840 logging_writer.py:48] [46604] accumulated_eval_time=1546.635691, accumulated_logging_time=1.645236, accumulated_submission_time=21051.006894, global_step=46604, preemption_count=0, score=21051.006894, test/accuracy=0.452200, test/loss=2.561402, test/num_examples=10000, total_duration=22601.638983, train/accuracy=0.619434, train/loss=1.691481, validation/accuracy=0.570120, validation/loss=1.932341, validation/num_examples=50000
I0130 20:00:07.435881 139990491195136 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.544974446296692, loss=3.3630855083465576
I0130 20:00:53.153926 139990499587840 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1025742292404175, loss=4.989229679107666
I0130 20:01:39.121006 139990491195136 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.352749228477478, loss=4.108968734741211
I0130 20:02:24.978881 139990499587840 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1182810068130493, loss=5.045267581939697
I0130 20:03:10.666743 139990491195136 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.6079871654510498, loss=3.5033864974975586
I0130 20:03:56.053316 139990499587840 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.3903710842132568, loss=5.459667682647705
I0130 20:04:41.795335 139990491195136 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.1608381271362305, loss=4.945300102233887
I0130 20:05:27.718343 139990499587840 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7810518741607666, loss=3.429708957672119
I0130 20:06:13.294312 139990491195136 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1697700023651123, loss=5.56146764755249
I0130 20:06:27.821150 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:06:38.070204 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:06:56.668147 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:06:58.277312 140184451094336 submission_runner.py:408] Time since start: 23052.11s, 	Step: 47533, 	{'train/accuracy': 0.6141015291213989, 'train/loss': 1.700886845588684, 'validation/accuracy': 0.5776199698448181, 'validation/loss': 1.8775568008422852, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.502983331680298, 'test/num_examples': 10000, 'score': 21470.933007240295, 'total_duration': 23052.105201005936, 'accumulated_submission_time': 21470.933007240295, 'accumulated_eval_time': 1577.0918953418732, 'accumulated_logging_time': 1.681839942932129}
I0130 20:06:58.303678 139990499587840 logging_writer.py:48] [47533] accumulated_eval_time=1577.091895, accumulated_logging_time=1.681840, accumulated_submission_time=21470.933007, global_step=47533, preemption_count=0, score=21470.933007, test/accuracy=0.460700, test/loss=2.502983, test/num_examples=10000, total_duration=23052.105201, train/accuracy=0.614102, train/loss=1.700887, validation/accuracy=0.577620, validation/loss=1.877557, validation/num_examples=50000
I0130 20:07:25.525619 139990491195136 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.3398761749267578, loss=3.731006622314453
I0130 20:08:11.068849 139990499587840 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.295241355895996, loss=5.411546230316162
I0130 20:08:57.208544 139990491195136 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.667396068572998, loss=3.4635438919067383
I0130 20:09:43.023682 139990499587840 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.5325860977172852, loss=3.473668336868286
I0130 20:10:28.953535 139990491195136 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0425922870635986, loss=5.385128974914551
I0130 20:11:14.863381 139990499587840 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.4289305210113525, loss=4.073784351348877
I0130 20:12:00.722201 139990491195136 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.510500431060791, loss=3.638873338699341
I0130 20:12:46.868573 139990499587840 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1881601810455322, loss=4.9044036865234375
I0130 20:13:32.509158 139990491195136 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.3991656303405762, loss=3.490675926208496
I0130 20:13:58.674570 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:14:09.387805 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:14:29.960520 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:14:31.560003 140184451094336 submission_runner.py:408] Time since start: 23505.39s, 	Step: 48459, 	{'train/accuracy': 0.6114843487739563, 'train/loss': 1.7474989891052246, 'validation/accuracy': 0.570580005645752, 'validation/loss': 1.9303126335144043, 'validation/num_examples': 50000, 'test/accuracy': 0.4547000229358673, 'test/loss': 2.5657715797424316, 'test/num_examples': 10000, 'score': 21891.246530532837, 'total_duration': 23505.38790154457, 'accumulated_submission_time': 21891.246530532837, 'accumulated_eval_time': 1609.9773399829865, 'accumulated_logging_time': 1.718390703201294}
I0130 20:14:31.581948 139990499587840 logging_writer.py:48] [48459] accumulated_eval_time=1609.977340, accumulated_logging_time=1.718391, accumulated_submission_time=21891.246531, global_step=48459, preemption_count=0, score=21891.246531, test/accuracy=0.454700, test/loss=2.565772, test/num_examples=10000, total_duration=23505.387902, train/accuracy=0.611484, train/loss=1.747499, validation/accuracy=0.570580, validation/loss=1.930313, validation/num_examples=50000
I0130 20:14:48.387794 139990491195136 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.549294352531433, loss=3.3265905380249023
I0130 20:15:31.394401 139990499587840 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.4033948183059692, loss=3.3411312103271484
I0130 20:16:17.067628 139990491195136 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.4705865383148193, loss=3.324033260345459
I0130 20:17:03.339150 139990499587840 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.2157648801803589, loss=5.1633148193359375
I0130 20:17:48.944107 139990491195136 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.434946894645691, loss=3.418557643890381
I0130 20:18:34.888252 139990499587840 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.4469513893127441, loss=3.3382973670959473
I0130 20:19:20.932947 139990491195136 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.6358637809753418, loss=3.440755605697632
I0130 20:20:06.580843 139990499587840 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.4610754251480103, loss=3.8397376537323
I0130 20:20:52.372831 139990491195136 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.225773811340332, loss=5.07200813293457
I0130 20:21:32.021662 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:21:42.321636 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:22:04.251775 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:22:05.857120 140184451094336 submission_runner.py:408] Time since start: 23959.69s, 	Step: 49388, 	{'train/accuracy': 0.6169726252555847, 'train/loss': 1.6989887952804565, 'validation/accuracy': 0.5762199759483337, 'validation/loss': 1.9017544984817505, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.5268452167510986, 'test/num_examples': 10000, 'score': 22311.626733779907, 'total_duration': 23959.685015916824, 'accumulated_submission_time': 22311.626733779907, 'accumulated_eval_time': 1643.8127937316895, 'accumulated_logging_time': 1.7531659603118896}
I0130 20:22:05.880761 139990499587840 logging_writer.py:48] [49388] accumulated_eval_time=1643.812794, accumulated_logging_time=1.753166, accumulated_submission_time=22311.626734, global_step=49388, preemption_count=0, score=22311.626734, test/accuracy=0.462600, test/loss=2.526845, test/num_examples=10000, total_duration=23959.685016, train/accuracy=0.616973, train/loss=1.698989, validation/accuracy=0.576220, validation/loss=1.901754, validation/num_examples=50000
I0130 20:22:11.087065 139990491195136 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.5136553049087524, loss=3.394279956817627
I0130 20:22:52.430664 139990499587840 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.4290791749954224, loss=3.39443302154541
I0130 20:23:38.108869 139990491195136 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.8176648616790771, loss=3.4375686645507812
I0130 20:24:23.974328 139990499587840 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.4889165163040161, loss=3.4034171104431152
I0130 20:25:09.611925 139990491195136 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.5380544662475586, loss=3.238369941711426
I0130 20:25:55.570205 139990499587840 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.5000042915344238, loss=3.289304733276367
I0130 20:26:41.040364 139990491195136 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.4587537050247192, loss=3.3583450317382812
I0130 20:27:27.064727 139990499587840 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.246113657951355, loss=4.834474563598633
I0130 20:28:12.882607 139990491195136 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.6771918535232544, loss=3.2898974418640137
I0130 20:28:58.700782 139990499587840 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.5836460590362549, loss=3.4035589694976807
I0130 20:29:06.307649 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:29:16.507287 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:29:38.345665 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:29:39.956396 140184451094336 submission_runner.py:408] Time since start: 24413.78s, 	Step: 50318, 	{'train/accuracy': 0.6458203196525574, 'train/loss': 1.5972343683242798, 'validation/accuracy': 0.5758799910545349, 'validation/loss': 1.9063783884048462, 'validation/num_examples': 50000, 'test/accuracy': 0.4619000256061554, 'test/loss': 2.520209550857544, 'test/num_examples': 10000, 'score': 22731.99612236023, 'total_duration': 24413.784294366837, 'accumulated_submission_time': 22731.99612236023, 'accumulated_eval_time': 1677.461537361145, 'accumulated_logging_time': 1.7863349914550781}
I0130 20:29:39.980334 139990491195136 logging_writer.py:48] [50318] accumulated_eval_time=1677.461537, accumulated_logging_time=1.786335, accumulated_submission_time=22731.996122, global_step=50318, preemption_count=0, score=22731.996122, test/accuracy=0.461900, test/loss=2.520210, test/num_examples=10000, total_duration=24413.784294, train/accuracy=0.645820, train/loss=1.597234, validation/accuracy=0.575880, validation/loss=1.906378, validation/num_examples=50000
I0130 20:30:13.344385 139990499587840 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.6280615329742432, loss=3.432696580886841
I0130 20:30:58.903566 139990491195136 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8729182481765747, loss=3.5332376956939697
I0130 20:31:44.997424 139990499587840 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.485007405281067, loss=3.563593626022339
I0130 20:32:30.773321 139990491195136 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.414557933807373, loss=3.5591723918914795
I0130 20:33:16.856905 139990499587840 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.5991133451461792, loss=3.399075508117676
I0130 20:34:02.744090 139990491195136 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.5162290334701538, loss=3.3138997554779053
I0130 20:34:48.365656 139990499587840 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.2276684045791626, loss=5.530890464782715
I0130 20:35:33.924105 139990491195136 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.5903569459915161, loss=3.321678638458252
I0130 20:36:19.704980 139990499587840 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.3884100914001465, loss=3.8141989707946777
I0130 20:36:39.974034 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:36:50.697504 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:37:10.955434 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:37:12.565963 140184451094336 submission_runner.py:408] Time since start: 24866.39s, 	Step: 51246, 	{'train/accuracy': 0.613476574420929, 'train/loss': 1.7316758632659912, 'validation/accuracy': 0.5759199857711792, 'validation/loss': 1.9049936532974243, 'validation/num_examples': 50000, 'test/accuracy': 0.4612000286579132, 'test/loss': 2.528799057006836, 'test/num_examples': 10000, 'score': 23151.934993743896, 'total_duration': 24866.39386487007, 'accumulated_submission_time': 23151.934993743896, 'accumulated_eval_time': 1710.0534682273865, 'accumulated_logging_time': 1.8191730976104736}
I0130 20:37:12.592042 139990491195136 logging_writer.py:48] [51246] accumulated_eval_time=1710.053468, accumulated_logging_time=1.819173, accumulated_submission_time=23151.934994, global_step=51246, preemption_count=0, score=23151.934994, test/accuracy=0.461200, test/loss=2.528799, test/num_examples=10000, total_duration=24866.393865, train/accuracy=0.613477, train/loss=1.731676, validation/accuracy=0.575920, validation/loss=1.904994, validation/num_examples=50000
I0130 20:37:34.577088 139990499587840 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.7605476379394531, loss=3.391890048980713
I0130 20:38:18.650451 139990491195136 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7160110473632812, loss=3.3069980144500732
I0130 20:39:04.359294 139990499587840 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.4972161054611206, loss=3.3174755573272705
I0130 20:39:49.901174 139990491195136 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2870432138442993, loss=4.070634841918945
I0130 20:40:35.662090 139990499587840 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.7075392007827759, loss=3.368302822113037
I0130 20:41:21.457132 139990491195136 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.597864031791687, loss=3.3512532711029053
I0130 20:42:07.214464 139990499587840 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.559353232383728, loss=3.2107508182525635
I0130 20:42:52.907285 139990491195136 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.4623591899871826, loss=3.2762982845306396
I0130 20:43:38.895805 139990499587840 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.4389142990112305, loss=5.0182294845581055
I0130 20:44:12.601655 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:44:23.302336 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:44:44.119755 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:44:45.731508 140184451094336 submission_runner.py:408] Time since start: 25319.56s, 	Step: 52175, 	{'train/accuracy': 0.6240624785423279, 'train/loss': 1.6678780317306519, 'validation/accuracy': 0.5776399970054626, 'validation/loss': 1.8867851495742798, 'validation/num_examples': 50000, 'test/accuracy': 0.465800017118454, 'test/loss': 2.5022594928741455, 'test/num_examples': 10000, 'score': 23571.889559984207, 'total_duration': 25319.559384584427, 'accumulated_submission_time': 23571.889559984207, 'accumulated_eval_time': 1743.183295249939, 'accumulated_logging_time': 1.854111909866333}
I0130 20:44:45.763634 139990491195136 logging_writer.py:48] [52175] accumulated_eval_time=1743.183295, accumulated_logging_time=1.854112, accumulated_submission_time=23571.889560, global_step=52175, preemption_count=0, score=23571.889560, test/accuracy=0.465800, test/loss=2.502259, test/num_examples=10000, total_duration=25319.559385, train/accuracy=0.624062, train/loss=1.667878, validation/accuracy=0.577640, validation/loss=1.886785, validation/num_examples=50000
I0130 20:44:56.182161 139990499587840 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.4214526414871216, loss=3.346773862838745
I0130 20:45:38.697220 139990491195136 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.3103256225585938, loss=3.8932371139526367
I0130 20:46:24.161059 139990499587840 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.634277582168579, loss=3.3181116580963135
I0130 20:47:10.159684 139990491195136 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.5949130058288574, loss=3.2439706325531006
I0130 20:47:55.983268 139990499587840 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.59116530418396, loss=3.2942357063293457
I0130 20:48:41.401525 139990491195136 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.4548286199569702, loss=3.3393967151641846
I0130 20:49:27.275944 139990499587840 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.4478018283843994, loss=3.924406051635742
I0130 20:50:13.110273 139990491195136 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.4969407320022583, loss=3.3321239948272705
I0130 20:50:59.010624 139990499587840 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.4444377422332764, loss=3.3252029418945312
I0130 20:51:44.854108 139990491195136 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.6128597259521484, loss=3.2576942443847656
I0130 20:51:46.040186 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:51:56.482710 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:52:15.699486 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:52:17.307525 140184451094336 submission_runner.py:408] Time since start: 25771.14s, 	Step: 53104, 	{'train/accuracy': 0.6388866901397705, 'train/loss': 1.6128058433532715, 'validation/accuracy': 0.5816599726676941, 'validation/loss': 1.8688938617706299, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.480672836303711, 'test/num_examples': 10000, 'score': 23992.10686635971, 'total_duration': 25771.13542485237, 'accumulated_submission_time': 23992.10686635971, 'accumulated_eval_time': 1774.4506244659424, 'accumulated_logging_time': 1.8986506462097168}
I0130 20:52:17.333200 139990499587840 logging_writer.py:48] [53104] accumulated_eval_time=1774.450624, accumulated_logging_time=1.898651, accumulated_submission_time=23992.106866, global_step=53104, preemption_count=0, score=23992.106866, test/accuracy=0.468000, test/loss=2.480673, test/num_examples=10000, total_duration=25771.135425, train/accuracy=0.638887, train/loss=1.612806, validation/accuracy=0.581660, validation/loss=1.868894, validation/num_examples=50000
I0130 20:52:56.805127 139990491195136 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.63360595703125, loss=3.4129409790039062
I0130 20:53:42.533805 139990499587840 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.4454277753829956, loss=5.569953441619873
I0130 20:54:28.635168 139990491195136 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.5806455612182617, loss=3.3579282760620117
I0130 20:55:14.110989 139990499587840 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2879889011383057, loss=3.787665605545044
I0130 20:55:59.674206 139990491195136 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.112705111503601, loss=4.880122184753418
I0130 20:56:45.167857 139990499587840 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.536741852760315, loss=3.2366631031036377
I0130 20:57:31.267825 139990491195136 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.4730679988861084, loss=3.2401275634765625
I0130 20:58:16.967379 139990499587840 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.6585514545440674, loss=3.475846767425537
I0130 20:59:03.075201 139990491195136 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.4661461114883423, loss=5.085489749908447
I0130 20:59:17.524128 140184451094336 spec.py:321] Evaluating on the training split.
I0130 20:59:28.578501 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 20:59:49.396878 140184451094336 spec.py:349] Evaluating on the test split.
I0130 20:59:50.987957 140184451094336 submission_runner.py:408] Time since start: 26224.82s, 	Step: 54034, 	{'train/accuracy': 0.6194140315055847, 'train/loss': 1.7486610412597656, 'validation/accuracy': 0.5802599787712097, 'validation/loss': 1.9259588718414307, 'validation/num_examples': 50000, 'test/accuracy': 0.460500031709671, 'test/loss': 2.5516300201416016, 'test/num_examples': 10000, 'score': 24412.24143385887, 'total_duration': 26224.815851688385, 'accumulated_submission_time': 24412.24143385887, 'accumulated_eval_time': 1807.9144456386566, 'accumulated_logging_time': 1.9343111515045166}
I0130 20:59:51.014113 139990499587840 logging_writer.py:48] [54034] accumulated_eval_time=1807.914446, accumulated_logging_time=1.934311, accumulated_submission_time=24412.241434, global_step=54034, preemption_count=0, score=24412.241434, test/accuracy=0.460500, test/loss=2.551630, test/num_examples=10000, total_duration=26224.815852, train/accuracy=0.619414, train/loss=1.748661, validation/accuracy=0.580260, validation/loss=1.925959, validation/num_examples=50000
I0130 21:00:17.824328 139990491195136 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.147457480430603, loss=5.145996570587158
I0130 21:01:02.531465 139990499587840 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.5927053689956665, loss=3.4141290187835693
I0130 21:01:48.534472 139990491195136 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.083095669746399, loss=5.472773551940918
I0130 21:02:34.074684 139990499587840 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.6404751539230347, loss=3.359210729598999
I0130 21:03:19.986503 139990491195136 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.590437889099121, loss=3.3433995246887207
I0130 21:04:05.812697 139990499587840 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.6996551752090454, loss=3.177798271179199
I0130 21:04:51.599949 139990491195136 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.3949657678604126, loss=4.259321689605713
I0130 21:05:37.443483 139990499587840 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1935420036315918, loss=5.55859375
I0130 21:06:23.323238 139990491195136 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.6565423011779785, loss=3.363443374633789
I0130 21:06:51.290035 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:07:01.641008 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:07:22.881072 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:07:24.490169 140184451094336 submission_runner.py:408] Time since start: 26678.32s, 	Step: 54963, 	{'train/accuracy': 0.6254101395606995, 'train/loss': 1.6814249753952026, 'validation/accuracy': 0.5814599990844727, 'validation/loss': 1.8866448402404785, 'validation/num_examples': 50000, 'test/accuracy': 0.46650001406669617, 'test/loss': 2.5163378715515137, 'test/num_examples': 10000, 'score': 24832.460919380188, 'total_duration': 26678.31806921959, 'accumulated_submission_time': 24832.460919380188, 'accumulated_eval_time': 1841.1145780086517, 'accumulated_logging_time': 1.9702081680297852}
I0130 21:07:24.513929 139990499587840 logging_writer.py:48] [54963] accumulated_eval_time=1841.114578, accumulated_logging_time=1.970208, accumulated_submission_time=24832.460919, global_step=54963, preemption_count=0, score=24832.460919, test/accuracy=0.466500, test/loss=2.516338, test/num_examples=10000, total_duration=26678.318069, train/accuracy=0.625410, train/loss=1.681425, validation/accuracy=0.581460, validation/loss=1.886645, validation/num_examples=50000
I0130 21:07:39.704088 139990491195136 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.4662870168685913, loss=3.239314556121826
I0130 21:08:23.200037 139990499587840 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.5441620349884033, loss=4.1621503829956055
I0130 21:09:09.172564 139990491195136 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.5704729557037354, loss=3.9187426567077637
I0130 21:09:55.386430 139990499587840 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.2713844776153564, loss=5.456391334533691
I0130 21:10:41.137558 139990491195136 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.6275862455368042, loss=3.322079658508301
I0130 21:11:26.902756 139990499587840 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.5624607801437378, loss=3.8225247859954834
I0130 21:12:12.705285 139990491195136 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.6983916759490967, loss=3.2750959396362305
I0130 21:12:58.361441 139990499587840 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.4684842824935913, loss=3.594724416732788
I0130 21:13:44.047899 139990491195136 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.3221428394317627, loss=5.271064281463623
I0130 21:14:24.952387 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:14:35.175298 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:14:56.222592 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:14:57.831706 140184451094336 submission_runner.py:408] Time since start: 27131.66s, 	Step: 55891, 	{'train/accuracy': 0.6341210603713989, 'train/loss': 1.6259918212890625, 'validation/accuracy': 0.5867399573326111, 'validation/loss': 1.858447790145874, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4755938053131104, 'test/num_examples': 10000, 'score': 25252.844719171524, 'total_duration': 27131.659603118896, 'accumulated_submission_time': 25252.844719171524, 'accumulated_eval_time': 1873.9938821792603, 'accumulated_logging_time': 2.003046751022339}
I0130 21:14:57.860400 139990499587840 logging_writer.py:48] [55891] accumulated_eval_time=1873.993882, accumulated_logging_time=2.003047, accumulated_submission_time=25252.844719, global_step=55891, preemption_count=0, score=25252.844719, test/accuracy=0.472000, test/loss=2.475594, test/num_examples=10000, total_duration=27131.659603, train/accuracy=0.634121, train/loss=1.625992, validation/accuracy=0.586740, validation/loss=1.858448, validation/num_examples=50000
I0130 21:15:02.029706 139990491195136 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.6186308860778809, loss=3.284651041030884
I0130 21:15:43.503057 139990499587840 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.292710304260254, loss=4.887600898742676
I0130 21:16:29.012567 139990491195136 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.5243120193481445, loss=3.3592824935913086
I0130 21:17:15.021451 139990499587840 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.539370059967041, loss=3.177551746368408
I0130 21:18:00.966339 139990491195136 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.5304629802703857, loss=3.332702159881592
I0130 21:18:46.599224 139990499587840 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.4741973876953125, loss=3.2428855895996094
I0130 21:19:32.201761 139990491195136 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.247808575630188, loss=5.29560661315918
I0130 21:20:17.946705 139990499587840 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.5584696531295776, loss=3.6693661212921143
I0130 21:21:03.807460 139990491195136 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.5296992063522339, loss=3.3560142517089844
I0130 21:21:49.668151 139990499587840 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.3949692249298096, loss=3.5220417976379395
I0130 21:21:58.040765 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:22:08.163697 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:22:30.053402 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:22:31.659675 140184451094336 submission_runner.py:408] Time since start: 27585.49s, 	Step: 56820, 	{'train/accuracy': 0.6281445026397705, 'train/loss': 1.6840720176696777, 'validation/accuracy': 0.5877199769020081, 'validation/loss': 1.8711938858032227, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.5007386207580566, 'test/num_examples': 10000, 'score': 25672.96810555458, 'total_duration': 27585.487575769424, 'accumulated_submission_time': 25672.96810555458, 'accumulated_eval_time': 1907.6127750873566, 'accumulated_logging_time': 2.042140007019043}
I0130 21:22:31.682463 139990491195136 logging_writer.py:48] [56820] accumulated_eval_time=1907.612775, accumulated_logging_time=2.042140, accumulated_submission_time=25672.968106, global_step=56820, preemption_count=0, score=25672.968106, test/accuracy=0.467800, test/loss=2.500739, test/num_examples=10000, total_duration=27585.487576, train/accuracy=0.628145, train/loss=1.684072, validation/accuracy=0.587720, validation/loss=1.871194, validation/num_examples=50000
I0130 21:23:04.077560 139990499587840 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2457208633422852, loss=4.565386772155762
I0130 21:23:49.528840 139990491195136 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.6210194826126099, loss=4.5454277992248535
I0130 21:24:35.395318 139990499587840 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.4034221172332764, loss=3.7036690711975098
I0130 21:25:20.975057 139990491195136 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.2304327487945557, loss=5.141230583190918
I0130 21:26:06.687502 139990499587840 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.38156259059906, loss=3.785970687866211
I0130 21:26:52.250793 139990491195136 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.489722728729248, loss=3.437202215194702
I0130 21:27:37.770976 139990499587840 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.5388633012771606, loss=3.543383836746216
I0130 21:28:23.490078 139990491195136 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.341617226600647, loss=4.0752272605896
I0130 21:29:09.012113 139990499587840 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.3274526596069336, loss=4.451423645019531
I0130 21:29:31.790543 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:29:41.985168 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:30:03.231919 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:30:04.843148 140184451094336 submission_runner.py:408] Time since start: 28038.67s, 	Step: 57752, 	{'train/accuracy': 0.6265624761581421, 'train/loss': 1.685849905014038, 'validation/accuracy': 0.5827000141143799, 'validation/loss': 1.870428204536438, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.495413303375244, 'test/num_examples': 10000, 'score': 26093.019316911697, 'total_duration': 28038.671048879623, 'accumulated_submission_time': 26093.019316911697, 'accumulated_eval_time': 1940.665373325348, 'accumulated_logging_time': 2.074948310852051}
I0130 21:30:04.869081 139990491195136 logging_writer.py:48] [57752] accumulated_eval_time=1940.665373, accumulated_logging_time=2.074948, accumulated_submission_time=26093.019317, global_step=57752, preemption_count=0, score=26093.019317, test/accuracy=0.464800, test/loss=2.495413, test/num_examples=10000, total_duration=28038.671049, train/accuracy=0.626562, train/loss=1.685850, validation/accuracy=0.582700, validation/loss=1.870428, validation/num_examples=50000
I0130 21:30:24.468808 139990499587840 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.4732401371002197, loss=3.342569589614868
I0130 21:31:07.767346 139990491195136 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.602695107460022, loss=3.37397837638855
I0130 21:31:53.922173 139990499587840 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3084293603897095, loss=4.509321689605713
I0130 21:32:39.904588 139990491195136 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.7138192653656006, loss=3.4185822010040283
I0130 21:33:25.725418 139990499587840 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.3194606304168701, loss=4.970986366271973
I0130 21:34:11.518346 139990491195136 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.4006932973861694, loss=4.014666557312012
I0130 21:34:57.170265 139990499587840 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.7273240089416504, loss=3.442483425140381
I0130 21:35:42.898731 139990491195136 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.535852074623108, loss=3.6358301639556885
I0130 21:36:28.524127 139990499587840 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.2303649187088013, loss=4.672483444213867
I0130 21:37:05.220255 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:37:15.449115 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:37:36.576698 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:37:38.177796 140184451094336 submission_runner.py:408] Time since start: 28492.01s, 	Step: 58682, 	{'train/accuracy': 0.6369921565055847, 'train/loss': 1.6104422807693481, 'validation/accuracy': 0.5866000056266785, 'validation/loss': 1.8379977941513062, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.459522008895874, 'test/num_examples': 10000, 'score': 26513.309331178665, 'total_duration': 28492.005694627762, 'accumulated_submission_time': 26513.309331178665, 'accumulated_eval_time': 1973.6229138374329, 'accumulated_logging_time': 2.110410690307617}
I0130 21:37:38.204247 139990491195136 logging_writer.py:48] [58682] accumulated_eval_time=1973.622914, accumulated_logging_time=2.110411, accumulated_submission_time=26513.309331, global_step=58682, preemption_count=0, score=26513.309331, test/accuracy=0.470800, test/loss=2.459522, test/num_examples=10000, total_duration=28492.005695, train/accuracy=0.636992, train/loss=1.610442, validation/accuracy=0.586600, validation/loss=1.837998, validation/num_examples=50000
I0130 21:37:45.814880 139990499587840 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.710651159286499, loss=3.296088695526123
I0130 21:38:27.829305 139990491195136 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.271722674369812, loss=4.502632141113281
I0130 21:39:13.277216 139990499587840 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.5424878597259521, loss=3.317586898803711
I0130 21:39:59.137251 139990491195136 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.5115280151367188, loss=3.475510358810425
I0130 21:40:44.858222 139990499587840 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.6548244953155518, loss=3.4176979064941406
I0130 21:41:30.629860 139990491195136 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.577149748802185, loss=3.3192942142486572
I0130 21:42:16.514822 139990499587840 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.7651021480560303, loss=3.3884596824645996
I0130 21:43:01.970605 139990491195136 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.5127798318862915, loss=3.4574522972106934
I0130 21:43:47.725420 139990499587840 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.4913203716278076, loss=3.6324241161346436
I0130 21:44:33.388216 139990491195136 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.4043610095977783, loss=4.297719955444336
I0130 21:44:38.424889 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:44:48.906306 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:45:10.314285 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:45:11.915471 140184451094336 submission_runner.py:408] Time since start: 28945.74s, 	Step: 59613, 	{'train/accuracy': 0.6574023365974426, 'train/loss': 1.5401033163070679, 'validation/accuracy': 0.5890399813652039, 'validation/loss': 1.8447201251983643, 'validation/num_examples': 50000, 'test/accuracy': 0.4741000235080719, 'test/loss': 2.4697728157043457, 'test/num_examples': 10000, 'score': 26933.473502397537, 'total_duration': 28945.743367433548, 'accumulated_submission_time': 26933.473502397537, 'accumulated_eval_time': 2007.1134917736053, 'accumulated_logging_time': 2.146523952484131}
I0130 21:45:11.940587 139990499587840 logging_writer.py:48] [59613] accumulated_eval_time=2007.113492, accumulated_logging_time=2.146524, accumulated_submission_time=26933.473502, global_step=59613, preemption_count=0, score=26933.473502, test/accuracy=0.474100, test/loss=2.469773, test/num_examples=10000, total_duration=28945.743367, train/accuracy=0.657402, train/loss=1.540103, validation/accuracy=0.589040, validation/loss=1.844720, validation/num_examples=50000
I0130 21:45:47.371093 139990491195136 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.7333475351333618, loss=3.276531934738159
I0130 21:46:32.828284 139990499587840 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.2198275327682495, loss=5.445492267608643
I0130 21:47:18.725483 139990491195136 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.5347667932510376, loss=3.2300374507904053
I0130 21:48:04.482516 139990499587840 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.5646878480911255, loss=3.3343586921691895
I0130 21:48:50.229084 139990491195136 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.553992509841919, loss=3.389604330062866
I0130 21:49:35.906473 139990499587840 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.5907264947891235, loss=3.2239270210266113
I0130 21:50:21.715648 139990491195136 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.5186198949813843, loss=3.2295966148376465
I0130 21:51:07.572553 139990499587840 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.448376178741455, loss=3.191903591156006
I0130 21:51:53.333277 139990491195136 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2973977327346802, loss=5.211490631103516
I0130 21:52:12.337530 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:52:22.579696 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 21:52:43.151428 140184451094336 spec.py:349] Evaluating on the test split.
I0130 21:52:44.747907 140184451094336 submission_runner.py:408] Time since start: 29398.58s, 	Step: 60543, 	{'train/accuracy': 0.6248632669448853, 'train/loss': 1.6834291219711304, 'validation/accuracy': 0.589419960975647, 'validation/loss': 1.868920922279358, 'validation/num_examples': 50000, 'test/accuracy': 0.4699000120162964, 'test/loss': 2.503729820251465, 'test/num_examples': 10000, 'score': 27353.815348386765, 'total_duration': 29398.575788736343, 'accumulated_submission_time': 27353.815348386765, 'accumulated_eval_time': 2039.5238630771637, 'accumulated_logging_time': 2.1801421642303467}
I0130 21:52:44.781032 139990499587840 logging_writer.py:48] [60543] accumulated_eval_time=2039.523863, accumulated_logging_time=2.180142, accumulated_submission_time=27353.815348, global_step=60543, preemption_count=0, score=27353.815348, test/accuracy=0.469900, test/loss=2.503730, test/num_examples=10000, total_duration=29398.575789, train/accuracy=0.624863, train/loss=1.683429, validation/accuracy=0.589420, validation/loss=1.868921, validation/num_examples=50000
I0130 21:53:07.988820 139990491195136 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.6712247133255005, loss=3.6148688793182373
I0130 21:53:52.665897 139990499587840 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.5961782932281494, loss=3.326486825942993
I0130 21:54:38.574472 139990491195136 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.700892448425293, loss=3.3185508251190186
I0130 21:55:24.494664 139990499587840 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.60482919216156, loss=3.4787752628326416
I0130 21:56:09.915386 139990491195136 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.6690443754196167, loss=3.280987501144409
I0130 21:56:55.679971 139990499587840 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.3093862533569336, loss=5.375790119171143
I0130 21:57:41.296837 139990491195136 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.6000196933746338, loss=3.317080497741699
I0130 21:58:27.163566 139990499587840 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.6398001909255981, loss=3.599641799926758
I0130 21:59:12.959431 139990491195136 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.4708973169326782, loss=3.496502161026001
I0130 21:59:45.139073 140184451094336 spec.py:321] Evaluating on the training split.
I0130 21:59:55.325042 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:00:14.492016 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:00:16.094596 140184451094336 submission_runner.py:408] Time since start: 29849.92s, 	Step: 61472, 	{'train/accuracy': 0.6370507478713989, 'train/loss': 1.664278268814087, 'validation/accuracy': 0.5917400121688843, 'validation/loss': 1.8678573369979858, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.495166063308716, 'test/num_examples': 10000, 'score': 27774.116877794266, 'total_duration': 29849.922494888306, 'accumulated_submission_time': 27774.116877794266, 'accumulated_eval_time': 2070.4793763160706, 'accumulated_logging_time': 2.2234983444213867}
I0130 22:00:16.119735 139990499587840 logging_writer.py:48] [61472] accumulated_eval_time=2070.479376, accumulated_logging_time=2.223498, accumulated_submission_time=27774.116878, global_step=61472, preemption_count=0, score=27774.116878, test/accuracy=0.475500, test/loss=2.495166, test/num_examples=10000, total_duration=29849.922495, train/accuracy=0.637051, train/loss=1.664278, validation/accuracy=0.591740, validation/loss=1.867857, validation/num_examples=50000
I0130 22:00:27.744942 139990491195136 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.197131872177124, loss=5.307271480560303
I0130 22:01:10.131044 139990499587840 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.5658984184265137, loss=3.286623239517212
I0130 22:01:56.048868 139990491195136 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.385588526725769, loss=3.650127649307251
I0130 22:02:42.107835 139990499587840 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.7093126773834229, loss=3.35642409324646
I0130 22:03:28.111795 139990491195136 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.4606964588165283, loss=3.6802585124969482
I0130 22:04:13.936055 139990499587840 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.6060572862625122, loss=3.262538194656372
I0130 22:04:59.448866 139990491195136 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.3728686571121216, loss=4.856407642364502
I0130 22:05:45.138365 139990499587840 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.3604031801223755, loss=4.269625663757324
I0130 22:06:30.968329 139990491195136 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.4453867673873901, loss=3.6109321117401123
I0130 22:07:16.704226 139990499587840 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.2802660465240479, loss=5.346624374389648
I0130 22:07:16.719014 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:07:27.069353 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:07:48.831841 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:07:50.430233 140184451094336 submission_runner.py:408] Time since start: 30304.26s, 	Step: 62401, 	{'train/accuracy': 0.6454882621765137, 'train/loss': 1.614796757698059, 'validation/accuracy': 0.5929200053215027, 'validation/loss': 1.8566802740097046, 'validation/num_examples': 50000, 'test/accuracy': 0.4774000346660614, 'test/loss': 2.467177152633667, 'test/num_examples': 10000, 'score': 28194.661197185516, 'total_duration': 30304.258091688156, 'accumulated_submission_time': 28194.661197185516, 'accumulated_eval_time': 2104.190548181534, 'accumulated_logging_time': 2.257309675216675}
I0130 22:07:50.461452 139990491195136 logging_writer.py:48] [62401] accumulated_eval_time=2104.190548, accumulated_logging_time=2.257310, accumulated_submission_time=28194.661197, global_step=62401, preemption_count=0, score=28194.661197, test/accuracy=0.477400, test/loss=2.467177, test/num_examples=10000, total_duration=30304.258092, train/accuracy=0.645488, train/loss=1.614797, validation/accuracy=0.592920, validation/loss=1.856680, validation/num_examples=50000
I0130 22:08:30.876590 139990499587840 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.551839828491211, loss=3.275179147720337
I0130 22:09:16.570175 139990491195136 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.424613118171692, loss=3.5291123390197754
I0130 22:10:02.360096 139990499587840 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.3520863056182861, loss=4.182805061340332
I0130 22:10:48.337560 139990491195136 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.5858789682388306, loss=3.4633169174194336
I0130 22:11:34.145625 139990499587840 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.7494964599609375, loss=3.2075531482696533
I0130 22:12:20.027606 139990491195136 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.406926155090332, loss=5.110101222991943
I0130 22:13:05.812141 139990499587840 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.5857664346694946, loss=3.299912691116333
I0130 22:13:51.217198 139990491195136 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.4360417127609253, loss=4.687863349914551
I0130 22:14:36.814427 139990499587840 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.4802213907241821, loss=3.327039957046509
I0130 22:14:50.655534 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:15:01.473522 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:15:23.315601 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:15:24.915416 140184451094336 submission_runner.py:408] Time since start: 30758.74s, 	Step: 63332, 	{'train/accuracy': 0.63623046875, 'train/loss': 1.6195902824401855, 'validation/accuracy': 0.5971999764442444, 'validation/loss': 1.8102082014083862, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.4436748027801514, 'test/num_examples': 10000, 'score': 28614.799318790436, 'total_duration': 30758.743307828903, 'accumulated_submission_time': 28614.799318790436, 'accumulated_eval_time': 2138.4504079818726, 'accumulated_logging_time': 2.2979867458343506}
I0130 22:15:24.939193 139990491195136 logging_writer.py:48] [63332] accumulated_eval_time=2138.450408, accumulated_logging_time=2.297987, accumulated_submission_time=28614.799319, global_step=63332, preemption_count=0, score=28614.799319, test/accuracy=0.475500, test/loss=2.443675, test/num_examples=10000, total_duration=30758.743308, train/accuracy=0.636230, train/loss=1.619590, validation/accuracy=0.597200, validation/loss=1.810208, validation/num_examples=50000
I0130 22:15:52.565074 139990499587840 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.5510802268981934, loss=3.336294412612915
I0130 22:16:37.002137 139990491195136 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.6119449138641357, loss=3.227421998977661
I0130 22:17:22.850547 139990499587840 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.5463981628417969, loss=3.1676483154296875
I0130 22:18:08.591496 139990491195136 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.606364130973816, loss=3.298097848892212
I0130 22:18:54.287678 139990499587840 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.5105834007263184, loss=3.466646671295166
I0130 22:19:40.059270 139990491195136 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.6753134727478027, loss=3.3138952255249023
I0130 22:20:25.903612 139990499587840 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.826545238494873, loss=3.275142192840576
I0130 22:21:11.757037 139990491195136 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.3243738412857056, loss=4.053977012634277
I0130 22:21:57.554001 139990499587840 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.3172097206115723, loss=4.489781856536865
I0130 22:22:25.224291 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:22:35.588692 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:22:56.767497 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:22:58.366226 140184451094336 submission_runner.py:408] Time since start: 31212.19s, 	Step: 64262, 	{'train/accuracy': 0.6394140720367432, 'train/loss': 1.6281222105026245, 'validation/accuracy': 0.5967199802398682, 'validation/loss': 1.8196301460266113, 'validation/num_examples': 50000, 'test/accuracy': 0.47840002179145813, 'test/loss': 2.4457361698150635, 'test/num_examples': 10000, 'score': 29035.028126716614, 'total_duration': 31212.194100141525, 'accumulated_submission_time': 29035.028126716614, 'accumulated_eval_time': 2171.592320203781, 'accumulated_logging_time': 2.3315210342407227}
I0130 22:22:58.391083 139990491195136 logging_writer.py:48] [64262] accumulated_eval_time=2171.592320, accumulated_logging_time=2.331521, accumulated_submission_time=29035.028127, global_step=64262, preemption_count=0, score=29035.028127, test/accuracy=0.478400, test/loss=2.445736, test/num_examples=10000, total_duration=31212.194100, train/accuracy=0.639414, train/loss=1.628122, validation/accuracy=0.596720, validation/loss=1.819630, validation/num_examples=50000
I0130 22:23:13.984214 139990499587840 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.5460271835327148, loss=3.774380922317505
I0130 22:23:57.283581 139990491195136 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.500944972038269, loss=3.3355202674865723
I0130 22:24:43.046149 139990499587840 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.851148009300232, loss=3.2180886268615723
I0130 22:25:28.893821 139990491195136 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.340865135192871, loss=4.783679962158203
I0130 22:26:14.434426 139990499587840 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.4464902877807617, loss=4.040989875793457
I0130 22:26:59.945124 139990491195136 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.5219110250473022, loss=3.4135334491729736
I0130 22:27:45.688867 139990499587840 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.6117345094680786, loss=3.2571074962615967
I0130 22:28:31.144799 139990491195136 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.5430781841278076, loss=3.2272462844848633
I0130 22:29:17.324274 139990499587840 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.5071616172790527, loss=3.8821399211883545
I0130 22:29:58.404443 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:30:09.015793 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:30:30.428076 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:30:32.023174 140184451094336 submission_runner.py:408] Time since start: 31665.85s, 	Step: 65192, 	{'train/accuracy': 0.6471484303474426, 'train/loss': 1.5985087156295776, 'validation/accuracy': 0.5951799750328064, 'validation/loss': 1.8246562480926514, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.464690685272217, 'test/num_examples': 10000, 'score': 29454.9858648777, 'total_duration': 31665.85106754303, 'accumulated_submission_time': 29454.9858648777, 'accumulated_eval_time': 2205.211054801941, 'accumulated_logging_time': 2.3653695583343506}
I0130 22:30:32.052195 139990491195136 logging_writer.py:48] [65192] accumulated_eval_time=2205.211055, accumulated_logging_time=2.365370, accumulated_submission_time=29454.985865, global_step=65192, preemption_count=0, score=29454.985865, test/accuracy=0.473500, test/loss=2.464691, test/num_examples=10000, total_duration=31665.851068, train/accuracy=0.647148, train/loss=1.598509, validation/accuracy=0.595180, validation/loss=1.824656, validation/num_examples=50000
I0130 22:30:35.654150 139990499587840 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.6372184753417969, loss=3.0890538692474365
I0130 22:31:17.360071 139990491195136 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.474987506866455, loss=4.038753032684326
I0130 22:32:03.548635 139990499587840 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.570500135421753, loss=3.319009304046631
I0130 22:32:49.826136 139990491195136 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2119147777557373, loss=4.645279884338379
I0130 22:33:35.698832 139990499587840 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.6142009496688843, loss=3.3685481548309326
I0130 22:34:21.507802 139990491195136 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.7560232877731323, loss=3.324721574783325
I0130 22:35:07.618667 139990499587840 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.8439959287643433, loss=3.255378007888794
I0130 22:35:53.219122 139990491195136 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.4566223621368408, loss=3.5121591091156006
I0130 22:36:38.875748 139990499587840 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.5149842500686646, loss=3.748621940612793
I0130 22:37:24.369788 139990491195136 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.383100986480713, loss=5.443327903747559
I0130 22:37:32.280061 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:37:42.313842 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:38:03.946767 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:38:05.548191 140184451094336 submission_runner.py:408] Time since start: 32119.38s, 	Step: 66119, 	{'train/accuracy': 0.6446288824081421, 'train/loss': 1.5758085250854492, 'validation/accuracy': 0.6003199815750122, 'validation/loss': 1.7816158533096313, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.4119138717651367, 'test/num_examples': 10000, 'score': 29875.157160520554, 'total_duration': 32119.37609243393, 'accumulated_submission_time': 29875.157160520554, 'accumulated_eval_time': 2238.4791843891144, 'accumulated_logging_time': 2.404351234436035}
I0130 22:38:05.576206 139990499587840 logging_writer.py:48] [66119] accumulated_eval_time=2238.479184, accumulated_logging_time=2.404351, accumulated_submission_time=29875.157161, global_step=66119, preemption_count=0, score=29875.157161, test/accuracy=0.486400, test/loss=2.411914, test/num_examples=10000, total_duration=32119.376092, train/accuracy=0.644629, train/loss=1.575809, validation/accuracy=0.600320, validation/loss=1.781616, validation/num_examples=50000
I0130 22:38:38.372649 139990491195136 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.392378330230713, loss=4.906465530395508
I0130 22:39:23.904704 139990499587840 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.61099374294281, loss=3.4472408294677734
I0130 22:40:09.597599 139990491195136 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.2960572242736816, loss=4.530588150024414
I0130 22:40:55.532622 139990499587840 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.4925353527069092, loss=4.800212860107422
I0130 22:41:41.475658 139990491195136 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.7468122243881226, loss=3.250248670578003
I0130 22:42:27.617349 139990499587840 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.5268018245697021, loss=3.4639065265655518
I0130 22:43:13.100643 139990491195136 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.6193361282348633, loss=3.5454277992248535
I0130 22:43:58.760200 139990499587840 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3246197700500488, loss=4.905058860778809
I0130 22:44:44.504611 139990491195136 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.6230942010879517, loss=3.3769960403442383
I0130 22:45:05.738783 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:45:16.172497 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:45:38.570580 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:45:40.169218 140184451094336 submission_runner.py:408] Time since start: 32574.00s, 	Step: 67048, 	{'train/accuracy': 0.6425976157188416, 'train/loss': 1.5925278663635254, 'validation/accuracy': 0.6013799905776978, 'validation/loss': 1.785370945930481, 'validation/num_examples': 50000, 'test/accuracy': 0.4790000319480896, 'test/loss': 2.427741765975952, 'test/num_examples': 10000, 'score': 30295.264727830887, 'total_duration': 32573.9971203804, 'accumulated_submission_time': 30295.264727830887, 'accumulated_eval_time': 2272.9096236228943, 'accumulated_logging_time': 2.440931558609009}
I0130 22:45:40.198447 139990499587840 logging_writer.py:48] [67048] accumulated_eval_time=2272.909624, accumulated_logging_time=2.440932, accumulated_submission_time=30295.264728, global_step=67048, preemption_count=0, score=30295.264728, test/accuracy=0.479000, test/loss=2.427742, test/num_examples=10000, total_duration=32573.997120, train/accuracy=0.642598, train/loss=1.592528, validation/accuracy=0.601380, validation/loss=1.785371, validation/num_examples=50000
I0130 22:46:01.418583 139990491195136 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3707388639450073, loss=4.198886394500732
I0130 22:46:45.213289 139990499587840 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.480739951133728, loss=3.666788101196289
I0130 22:47:30.887416 139990491195136 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3821392059326172, loss=5.062699317932129
I0130 22:48:16.766231 139990499587840 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3930004835128784, loss=4.760077476501465
I0130 22:49:02.216901 139990491195136 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.8487305641174316, loss=3.2490901947021484
I0130 22:49:48.120462 139990499587840 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.5783874988555908, loss=3.7940726280212402
I0130 22:50:33.782039 139990491195136 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.5975358486175537, loss=3.174178123474121
I0130 22:51:19.483566 139990499587840 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.6127995252609253, loss=3.304877519607544
I0130 22:52:05.502953 139990491195136 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.4310274124145508, loss=5.18120002746582
I0130 22:52:40.504824 140184451094336 spec.py:321] Evaluating on the training split.
I0130 22:52:51.180418 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 22:53:10.221538 140184451094336 spec.py:349] Evaluating on the test split.
I0130 22:53:11.836495 140184451094336 submission_runner.py:408] Time since start: 33025.66s, 	Step: 67979, 	{'train/accuracy': 0.6479296684265137, 'train/loss': 1.6087180376052856, 'validation/accuracy': 0.5970799922943115, 'validation/loss': 1.8276971578598022, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.4544215202331543, 'test/num_examples': 10000, 'score': 30715.514212608337, 'total_duration': 33025.66438269615, 'accumulated_submission_time': 30715.514212608337, 'accumulated_eval_time': 2304.2412803173065, 'accumulated_logging_time': 2.480074882507324}
I0130 22:53:11.864631 139990499587840 logging_writer.py:48] [67979] accumulated_eval_time=2304.241280, accumulated_logging_time=2.480075, accumulated_submission_time=30715.514213, global_step=67979, preemption_count=0, score=30715.514213, test/accuracy=0.483200, test/loss=2.454422, test/num_examples=10000, total_duration=33025.664383, train/accuracy=0.647930, train/loss=1.608718, validation/accuracy=0.597080, validation/loss=1.827697, validation/num_examples=50000
I0130 22:53:20.674042 139990491195136 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.6148231029510498, loss=3.200721263885498
I0130 22:54:02.994696 139990499587840 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.5369452238082886, loss=3.3135385513305664
I0130 22:54:48.906117 139990491195136 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.5274848937988281, loss=3.4302544593811035
I0130 22:55:35.018860 139990499587840 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.5880595445632935, loss=3.319138288497925
I0130 22:56:20.955022 139990491195136 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.715928077697754, loss=3.2398691177368164
I0130 22:57:06.829751 139990499587840 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.627238154411316, loss=3.3878908157348633
I0130 22:57:52.740267 139990491195136 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.4372175931930542, loss=3.764995813369751
I0130 22:58:38.723275 139990499587840 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.565564513206482, loss=3.1892850399017334
I0130 22:59:24.773694 139990491195136 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.624294400215149, loss=3.25644588470459
I0130 23:00:10.421369 139990499587840 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.5608465671539307, loss=3.1745715141296387
I0130 23:00:11.966125 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:00:22.367395 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:00:43.230854 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:00:44.829564 140184451094336 submission_runner.py:408] Time since start: 33478.66s, 	Step: 68905, 	{'train/accuracy': 0.6702734231948853, 'train/loss': 1.4906214475631714, 'validation/accuracy': 0.6045799851417542, 'validation/loss': 1.7795348167419434, 'validation/num_examples': 50000, 'test/accuracy': 0.49320003390312195, 'test/loss': 2.402475595474243, 'test/num_examples': 10000, 'score': 31135.559599637985, 'total_duration': 33478.6574652195, 'accumulated_submission_time': 31135.559599637985, 'accumulated_eval_time': 2337.104706287384, 'accumulated_logging_time': 2.5181806087493896}
I0130 23:00:44.858252 139990491195136 logging_writer.py:48] [68905] accumulated_eval_time=2337.104706, accumulated_logging_time=2.518181, accumulated_submission_time=31135.559600, global_step=68905, preemption_count=0, score=31135.559600, test/accuracy=0.493200, test/loss=2.402476, test/num_examples=10000, total_duration=33478.657465, train/accuracy=0.670273, train/loss=1.490621, validation/accuracy=0.604580, validation/loss=1.779535, validation/num_examples=50000
I0130 23:01:23.966156 139990499587840 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3903778791427612, loss=4.606886863708496
I0130 23:02:09.861856 139990491195136 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.6528778076171875, loss=3.2972803115844727
I0130 23:02:55.796069 139990499587840 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4397259950637817, loss=3.4398410320281982
I0130 23:03:41.356848 139990491195136 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2006937265396118, loss=4.370053291320801
I0130 23:04:27.240483 139990499587840 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.5830715894699097, loss=3.19651198387146
I0130 23:05:12.762479 139990491195136 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.6197866201400757, loss=3.3453240394592285
I0130 23:05:58.689655 139990499587840 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.563385009765625, loss=4.564303874969482
I0130 23:06:44.430495 139990491195136 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.57925546169281, loss=3.293287754058838
I0130 23:07:30.059770 139990499587840 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.671431303024292, loss=3.303813934326172
I0130 23:07:45.026638 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:07:55.148264 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:08:15.724156 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:08:17.342343 140184451094336 submission_runner.py:408] Time since start: 33931.17s, 	Step: 69834, 	{'train/accuracy': 0.6407226324081421, 'train/loss': 1.5906224250793457, 'validation/accuracy': 0.6022399663925171, 'validation/loss': 1.764672040939331, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.3967788219451904, 'test/num_examples': 10000, 'score': 31555.447756052017, 'total_duration': 33931.170246362686, 'accumulated_submission_time': 31555.447756052017, 'accumulated_eval_time': 2369.4204025268555, 'accumulated_logging_time': 2.779877185821533}
I0130 23:08:17.367884 139990491195136 logging_writer.py:48] [69834] accumulated_eval_time=2369.420403, accumulated_logging_time=2.779877, accumulated_submission_time=31555.447756, global_step=69834, preemption_count=0, score=31555.447756, test/accuracy=0.481200, test/loss=2.396779, test/num_examples=10000, total_duration=33931.170246, train/accuracy=0.640723, train/loss=1.590622, validation/accuracy=0.602240, validation/loss=1.764672, validation/num_examples=50000
I0130 23:08:44.165539 139990499587840 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.4442532062530518, loss=3.385446071624756
I0130 23:09:28.560617 139990491195136 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.4485080242156982, loss=3.5910911560058594
I0130 23:10:14.836712 139990499587840 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.6007843017578125, loss=3.2848727703094482
I0130 23:11:01.085286 139990491195136 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.838413953781128, loss=3.4026639461517334
I0130 23:11:46.988134 139990499587840 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.5043805837631226, loss=3.5207862854003906
I0130 23:12:32.757180 139990491195136 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.381382703781128, loss=4.326760292053223
I0130 23:13:18.501600 139990499587840 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.5056861639022827, loss=3.2320735454559326
I0130 23:14:04.490171 139990491195136 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3857280015945435, loss=3.941305637359619
I0130 23:14:50.253282 139990499587840 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.095198154449463, loss=3.176274538040161
I0130 23:15:17.454399 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:15:28.079908 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:15:49.263132 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:15:50.874296 140184451094336 submission_runner.py:408] Time since start: 34384.70s, 	Step: 70761, 	{'train/accuracy': 0.6514452695846558, 'train/loss': 1.564841866493225, 'validation/accuracy': 0.6045599579811096, 'validation/loss': 1.7766313552856445, 'validation/num_examples': 50000, 'test/accuracy': 0.4848000109195709, 'test/loss': 2.404025077819824, 'test/num_examples': 10000, 'score': 31975.477875947952, 'total_duration': 34384.702178001404, 'accumulated_submission_time': 31975.477875947952, 'accumulated_eval_time': 2402.840269088745, 'accumulated_logging_time': 2.8157126903533936}
I0130 23:15:50.903080 139990491195136 logging_writer.py:48] [70761] accumulated_eval_time=2402.840269, accumulated_logging_time=2.815713, accumulated_submission_time=31975.477876, global_step=70761, preemption_count=0, score=31975.477876, test/accuracy=0.484800, test/loss=2.404025, test/num_examples=10000, total_duration=34384.702178, train/accuracy=0.651445, train/loss=1.564842, validation/accuracy=0.604560, validation/loss=1.776631, validation/num_examples=50000
I0130 23:16:06.889887 139990499587840 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.593677282333374, loss=3.2348010540008545
I0130 23:16:50.132933 139990491195136 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.7506487369537354, loss=3.238210678100586
I0130 23:17:35.961824 139990499587840 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.7640917301177979, loss=3.2366456985473633
I0130 23:18:22.061926 139990491195136 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.6573129892349243, loss=3.1464521884918213
I0130 23:19:07.785067 139990499587840 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.421056866645813, loss=3.94012188911438
I0130 23:19:53.565306 139990491195136 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.6936204433441162, loss=3.252139091491699
I0130 23:20:39.381503 139990499587840 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.6777400970458984, loss=3.202449321746826
I0130 23:21:25.267903 139990491195136 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.4512419700622559, loss=3.7494606971740723
I0130 23:22:11.351515 139990499587840 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.7187905311584473, loss=3.2548940181732178
I0130 23:22:51.091632 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:23:01.635066 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:23:22.944698 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:23:24.546449 140184451094336 submission_runner.py:408] Time since start: 34838.37s, 	Step: 71688, 	{'train/accuracy': 0.6606835722923279, 'train/loss': 1.5274606943130493, 'validation/accuracy': 0.6034600138664246, 'validation/loss': 1.7711652517318726, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.400637149810791, 'test/num_examples': 10000, 'score': 32395.610315799713, 'total_duration': 34838.37434220314, 'accumulated_submission_time': 32395.610315799713, 'accumulated_eval_time': 2436.2950756549835, 'accumulated_logging_time': 2.854088068008423}
I0130 23:23:24.577384 139990491195136 logging_writer.py:48] [71688] accumulated_eval_time=2436.295076, accumulated_logging_time=2.854088, accumulated_submission_time=32395.610316, global_step=71688, preemption_count=0, score=32395.610316, test/accuracy=0.484000, test/loss=2.400637, test/num_examples=10000, total_duration=34838.374342, train/accuracy=0.660684, train/loss=1.527461, validation/accuracy=0.603460, validation/loss=1.771165, validation/num_examples=50000
I0130 23:23:29.783994 139990499587840 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.7913072109222412, loss=3.189908742904663
I0130 23:24:11.494211 139990491195136 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.4532272815704346, loss=3.7219796180725098
I0130 23:24:57.479909 139990499587840 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.631445050239563, loss=3.436915159225464
I0130 23:25:43.680241 139990491195136 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2416963577270508, loss=4.944159507751465
I0130 23:26:29.463974 139990499587840 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.7419085502624512, loss=3.2505509853363037
I0130 23:27:15.201361 139990491195136 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.6736056804656982, loss=3.1732280254364014
I0130 23:28:00.934914 139990499587840 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.45328950881958, loss=3.828141689300537
I0130 23:28:46.789860 139990491195136 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.4523897171020508, loss=3.8116979598999023
I0130 23:29:32.484694 139990499587840 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.450600266456604, loss=4.504411697387695
I0130 23:30:18.419133 139990491195136 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.969974160194397, loss=3.3077101707458496
I0130 23:30:24.568924 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:30:34.834829 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:30:54.600012 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:30:56.206090 140184451094336 submission_runner.py:408] Time since start: 35290.03s, 	Step: 72615, 	{'train/accuracy': 0.6470116972923279, 'train/loss': 1.589882493019104, 'validation/accuracy': 0.6062799692153931, 'validation/loss': 1.779320240020752, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.4277710914611816, 'test/num_examples': 10000, 'score': 32815.5456404686, 'total_duration': 35290.03398799896, 'accumulated_submission_time': 32815.5456404686, 'accumulated_eval_time': 2467.9322276115417, 'accumulated_logging_time': 2.8944272994995117}
I0130 23:30:56.233400 139990499587840 logging_writer.py:48] [72615] accumulated_eval_time=2467.932228, accumulated_logging_time=2.894427, accumulated_submission_time=32815.545640, global_step=72615, preemption_count=0, score=32815.545640, test/accuracy=0.486800, test/loss=2.427771, test/num_examples=10000, total_duration=35290.033988, train/accuracy=0.647012, train/loss=1.589882, validation/accuracy=0.606280, validation/loss=1.779320, validation/num_examples=50000
I0130 23:31:31.385725 139990491195136 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4749648571014404, loss=3.6162219047546387
I0130 23:32:17.284551 139990499587840 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.6038942337036133, loss=3.13268780708313
I0130 23:33:03.512148 139990491195136 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.6549307107925415, loss=3.1884608268737793
I0130 23:33:49.164920 139990499587840 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.6519721746444702, loss=3.175769567489624
I0130 23:34:35.474305 139990491195136 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.5007628202438354, loss=3.563603401184082
I0130 23:35:21.360839 139990499587840 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.3204705715179443, loss=5.0611467361450195
I0130 23:36:07.106251 139990491195136 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.5272583961486816, loss=4.2289228439331055
I0130 23:36:53.046404 139990499587840 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4967612028121948, loss=4.340714931488037
I0130 23:37:38.554146 139990491195136 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.4140312671661377, loss=3.650402545928955
I0130 23:37:56.552683 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:38:07.047256 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:38:28.284896 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:38:29.890927 140184451094336 submission_runner.py:408] Time since start: 35743.72s, 	Step: 73541, 	{'train/accuracy': 0.6502929329872131, 'train/loss': 1.571126103401184, 'validation/accuracy': 0.6121199727058411, 'validation/loss': 1.750990629196167, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.3877408504486084, 'test/num_examples': 10000, 'score': 33235.80899262428, 'total_duration': 35743.71882414818, 'accumulated_submission_time': 33235.80899262428, 'accumulated_eval_time': 2501.2704651355743, 'accumulated_logging_time': 2.9318900108337402}
I0130 23:38:29.918041 139990499587840 logging_writer.py:48] [73541] accumulated_eval_time=2501.270465, accumulated_logging_time=2.931890, accumulated_submission_time=33235.808993, global_step=73541, preemption_count=0, score=33235.808993, test/accuracy=0.492300, test/loss=2.387741, test/num_examples=10000, total_duration=35743.718824, train/accuracy=0.650293, train/loss=1.571126, validation/accuracy=0.612120, validation/loss=1.750991, validation/num_examples=50000
I0130 23:38:53.929329 139990491195136 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.371941089630127, loss=4.685179710388184
I0130 23:39:38.305054 139990499587840 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.7635225057601929, loss=3.204186201095581
I0130 23:40:24.058242 139990491195136 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.6749933958053589, loss=3.3052496910095215
I0130 23:41:09.842549 139990499587840 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.2199419736862183, loss=4.710917949676514
I0130 23:41:55.269532 139990491195136 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.8259460926055908, loss=3.194335460662842
I0130 23:42:41.303935 139990499587840 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2808955907821655, loss=4.33225154876709
I0130 23:43:27.212372 139990491195136 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.7016621828079224, loss=3.400423765182495
I0130 23:44:12.870037 139990499587840 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.2914050817489624, loss=4.88950252532959
I0130 23:44:58.666155 139990491195136 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.5114260911941528, loss=4.671307563781738
I0130 23:45:30.013024 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:45:40.276994 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:46:01.921777 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:46:03.528809 140184451094336 submission_runner.py:408] Time since start: 36197.36s, 	Step: 74470, 	{'train/accuracy': 0.6590234041213989, 'train/loss': 1.541833758354187, 'validation/accuracy': 0.6091399788856506, 'validation/loss': 1.7622010707855225, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.3940000534057617, 'test/num_examples': 10000, 'score': 33655.84841275215, 'total_duration': 36197.356711387634, 'accumulated_submission_time': 33655.84841275215, 'accumulated_eval_time': 2534.7862520217896, 'accumulated_logging_time': 2.968616485595703}
I0130 23:46:03.557181 139990499587840 logging_writer.py:48] [74470] accumulated_eval_time=2534.786252, accumulated_logging_time=2.968616, accumulated_submission_time=33655.848413, global_step=74470, preemption_count=0, score=33655.848413, test/accuracy=0.487900, test/loss=2.394000, test/num_examples=10000, total_duration=36197.356711, train/accuracy=0.659023, train/loss=1.541834, validation/accuracy=0.609140, validation/loss=1.762201, validation/num_examples=50000
I0130 23:46:15.978604 139990491195136 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.9069430828094482, loss=3.242577314376831
I0130 23:46:58.218297 139990499587840 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.6921805143356323, loss=3.2763404846191406
I0130 23:47:44.006901 139990491195136 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.7603110074996948, loss=3.243692398071289
I0130 23:48:30.001888 139990499587840 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.3555686473846436, loss=4.940134048461914
I0130 23:49:15.631563 139990491195136 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.5590219497680664, loss=3.4955623149871826
I0130 23:50:01.238860 139990499587840 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3474078178405762, loss=5.414615154266357
I0130 23:50:47.165480 139990491195136 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.7852011919021606, loss=3.0841922760009766
I0130 23:51:32.878371 139990499587840 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.6969705820083618, loss=3.09568452835083
I0130 23:52:18.816455 139990491195136 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.9136900901794434, loss=3.2560019493103027
I0130 23:53:03.857037 140184451094336 spec.py:321] Evaluating on the training split.
I0130 23:53:14.225586 140184451094336 spec.py:333] Evaluating on the validation split.
I0130 23:53:35.680299 140184451094336 spec.py:349] Evaluating on the test split.
I0130 23:53:37.286285 140184451094336 submission_runner.py:408] Time since start: 36651.11s, 	Step: 75400, 	{'train/accuracy': 0.6696093678474426, 'train/loss': 1.4805610179901123, 'validation/accuracy': 0.6134999990463257, 'validation/loss': 1.732527732849121, 'validation/num_examples': 50000, 'test/accuracy': 0.4926000237464905, 'test/loss': 2.362853527069092, 'test/num_examples': 10000, 'score': 34076.092334747314, 'total_duration': 36651.11418533325, 'accumulated_submission_time': 34076.092334747314, 'accumulated_eval_time': 2568.215485572815, 'accumulated_logging_time': 3.006901502609253}
I0130 23:53:37.316790 139990499587840 logging_writer.py:48] [75400] accumulated_eval_time=2568.215486, accumulated_logging_time=3.006902, accumulated_submission_time=34076.092335, global_step=75400, preemption_count=0, score=34076.092335, test/accuracy=0.492600, test/loss=2.362854, test/num_examples=10000, total_duration=36651.114185, train/accuracy=0.669609, train/loss=1.480561, validation/accuracy=0.613500, validation/loss=1.732528, validation/num_examples=50000
I0130 23:53:37.719077 139990491195136 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.6466106176376343, loss=3.445283889770508
I0130 23:54:18.847532 139990499587840 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.5155795812606812, loss=3.412675380706787
I0130 23:55:04.624957 139990491195136 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.5657554864883423, loss=3.314293146133423
I0130 23:55:50.406736 139990499587840 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.535934567451477, loss=3.799448013305664
I0130 23:56:36.147254 139990491195136 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.618759274482727, loss=3.1789863109588623
I0130 23:57:21.815605 139990499587840 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.4478862285614014, loss=4.198824405670166
I0130 23:58:07.342045 139990491195136 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3584414720535278, loss=4.949307918548584
I0130 23:58:53.100291 139990499587840 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.5320574045181274, loss=4.132721424102783
I0130 23:59:38.859952 139990491195136 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.5623406171798706, loss=3.127554178237915
I0131 00:00:24.670037 139990499587840 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.6147111654281616, loss=5.050058364868164
I0131 00:00:37.659276 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:00:47.695375 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:01:10.602373 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:01:12.209918 140184451094336 submission_runner.py:408] Time since start: 37106.04s, 	Step: 76330, 	{'train/accuracy': 0.65869140625, 'train/loss': 1.5325862169265747, 'validation/accuracy': 0.614300012588501, 'validation/loss': 1.7345703840255737, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.3532207012176514, 'test/num_examples': 10000, 'score': 34496.37813591957, 'total_duration': 37106.03781723976, 'accumulated_submission_time': 34496.37813591957, 'accumulated_eval_time': 2602.766112804413, 'accumulated_logging_time': 3.0473546981811523}
I0131 00:01:12.240430 139990491195136 logging_writer.py:48] [76330] accumulated_eval_time=2602.766113, accumulated_logging_time=3.047355, accumulated_submission_time=34496.378136, global_step=76330, preemption_count=0, score=34496.378136, test/accuracy=0.497400, test/loss=2.353221, test/num_examples=10000, total_duration=37106.037817, train/accuracy=0.658691, train/loss=1.532586, validation/accuracy=0.614300, validation/loss=1.734570, validation/num_examples=50000
I0131 00:01:40.645296 139990499587840 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.5495569705963135, loss=5.383623123168945
I0131 00:02:25.676829 139990491195136 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.656561017036438, loss=3.4753735065460205
I0131 00:03:11.881324 139990499587840 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.4812971353530884, loss=5.338188171386719
I0131 00:03:57.530858 139990491195136 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4522181749343872, loss=4.154951572418213
I0131 00:04:43.658252 139990499587840 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.6850244998931885, loss=3.2024340629577637
I0131 00:05:29.276272 139990491195136 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.460076093673706, loss=3.4783761501312256
I0131 00:06:15.283463 139990499587840 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.5874290466308594, loss=3.1696677207946777
I0131 00:07:00.916511 139990491195136 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.6373155117034912, loss=3.05753231048584
I0131 00:07:46.685506 139990499587840 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.819743037223816, loss=3.0992584228515625
I0131 00:08:12.540827 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:08:22.948756 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:08:43.698562 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:08:45.297700 140184451094336 submission_runner.py:408] Time since start: 37559.13s, 	Step: 77258, 	{'train/accuracy': 0.6606054306030273, 'train/loss': 1.5113722085952759, 'validation/accuracy': 0.6152399778366089, 'validation/loss': 1.7202774286270142, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.351444721221924, 'test/num_examples': 10000, 'score': 34916.623254299164, 'total_duration': 37559.12559890747, 'accumulated_submission_time': 34916.623254299164, 'accumulated_eval_time': 2635.522970676422, 'accumulated_logging_time': 3.0869641304016113}
I0131 00:08:45.326349 139990491195136 logging_writer.py:48] [77258] accumulated_eval_time=2635.522971, accumulated_logging_time=3.086964, accumulated_submission_time=34916.623254, global_step=77258, preemption_count=0, score=34916.623254, test/accuracy=0.493700, test/loss=2.351445, test/num_examples=10000, total_duration=37559.125599, train/accuracy=0.660605, train/loss=1.511372, validation/accuracy=0.615240, validation/loss=1.720277, validation/num_examples=50000
I0131 00:09:02.538871 139990499587840 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.8877928256988525, loss=3.29093599319458
I0131 00:09:46.213703 139990491195136 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4008225202560425, loss=4.841910362243652
I0131 00:10:32.408156 139990499587840 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.6534112691879272, loss=3.2519564628601074
I0131 00:11:18.639671 139990491195136 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.7911096811294556, loss=3.247864007949829
I0131 00:12:04.599827 139990499587840 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.5751973390579224, loss=5.15125846862793
I0131 00:12:50.349507 139990491195136 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.5881946086883545, loss=5.238654136657715
I0131 00:13:36.001863 139990499587840 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.542730689048767, loss=4.7825751304626465
I0131 00:14:21.868898 139990491195136 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.4051713943481445, loss=5.458395957946777
I0131 00:15:07.897794 139990499587840 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.8262847661972046, loss=3.081289291381836
I0131 00:15:45.551591 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:15:55.833886 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:16:15.325199 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:16:16.927097 140184451094336 submission_runner.py:408] Time since start: 38010.75s, 	Step: 78184, 	{'train/accuracy': 0.6808788776397705, 'train/loss': 1.4191409349441528, 'validation/accuracy': 0.6181600093841553, 'validation/loss': 1.7033716440200806, 'validation/num_examples': 50000, 'test/accuracy': 0.5019000172615051, 'test/loss': 2.3303937911987305, 'test/num_examples': 10000, 'score': 35336.7923810482, 'total_duration': 38010.754996299744, 'accumulated_submission_time': 35336.7923810482, 'accumulated_eval_time': 2666.8984639644623, 'accumulated_logging_time': 3.1255970001220703}
I0131 00:16:16.958124 139990491195136 logging_writer.py:48] [78184] accumulated_eval_time=2666.898464, accumulated_logging_time=3.125597, accumulated_submission_time=35336.792381, global_step=78184, preemption_count=0, score=35336.792381, test/accuracy=0.501900, test/loss=2.330394, test/num_examples=10000, total_duration=38010.754996, train/accuracy=0.680879, train/loss=1.419141, validation/accuracy=0.618160, validation/loss=1.703372, validation/num_examples=50000
I0131 00:16:23.772766 139990499587840 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.6579331159591675, loss=3.17476749420166
I0131 00:17:05.521479 139990491195136 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.345241904258728, loss=4.074296951293945
I0131 00:17:51.432732 139990499587840 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.3059865236282349, loss=5.205759048461914
I0131 00:18:37.396498 139990491195136 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.6232908964157104, loss=3.1769721508026123
I0131 00:19:23.029956 139990499587840 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.622240424156189, loss=3.4397172927856445
I0131 00:20:09.123706 139990491195136 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.6593573093414307, loss=3.5346739292144775
I0131 00:20:54.766143 139990499587840 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.8825695514678955, loss=3.0849552154541016
I0131 00:21:40.850576 139990491195136 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.099790096282959, loss=3.286252498626709
I0131 00:22:26.600717 139990499587840 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.6253668069839478, loss=3.1340532302856445
I0131 00:23:12.684590 139990491195136 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.7473593950271606, loss=3.242802143096924
I0131 00:23:17.368742 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:23:27.470047 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:23:49.080776 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:23:50.688304 140184451094336 submission_runner.py:408] Time since start: 38464.52s, 	Step: 79112, 	{'train/accuracy': 0.661816418170929, 'train/loss': 1.5143336057662964, 'validation/accuracy': 0.6148399710655212, 'validation/loss': 1.7183010578155518, 'validation/num_examples': 50000, 'test/accuracy': 0.49240002036094666, 'test/loss': 2.3449583053588867, 'test/num_examples': 10000, 'score': 35757.1461520195, 'total_duration': 38464.51617407799, 'accumulated_submission_time': 35757.1461520195, 'accumulated_eval_time': 2700.217987060547, 'accumulated_logging_time': 3.1663384437561035}
I0131 00:23:50.720615 139990499587840 logging_writer.py:48] [79112] accumulated_eval_time=2700.217987, accumulated_logging_time=3.166338, accumulated_submission_time=35757.146152, global_step=79112, preemption_count=0, score=35757.146152, test/accuracy=0.492400, test/loss=2.344958, test/num_examples=10000, total_duration=38464.516174, train/accuracy=0.661816, train/loss=1.514334, validation/accuracy=0.614840, validation/loss=1.718301, validation/num_examples=50000
I0131 00:24:26.331564 139990491195136 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.6289942264556885, loss=3.294715166091919
I0131 00:25:11.950393 139990499587840 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.8546233177185059, loss=3.144015073776245
I0131 00:25:57.889215 139990491195136 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.6059825420379639, loss=4.265484809875488
I0131 00:26:43.401014 139990499587840 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.6344966888427734, loss=4.161993980407715
I0131 00:27:29.374163 139990491195136 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.8374072313308716, loss=3.128674268722534
I0131 00:28:14.876837 139990499587840 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.7390741109848022, loss=3.200061798095703
I0131 00:29:00.391628 139990491195136 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.3757579326629639, loss=5.255627155303955
I0131 00:29:46.496695 139990499587840 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.7851475477218628, loss=3.244473695755005
I0131 00:30:32.154378 139990491195136 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.490326166152954, loss=5.282710075378418
I0131 00:30:50.875836 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:31:01.149640 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:31:22.687519 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:31:24.290283 140184451094336 submission_runner.py:408] Time since start: 38918.12s, 	Step: 80043, 	{'train/accuracy': 0.664746105670929, 'train/loss': 1.5105516910552979, 'validation/accuracy': 0.615339994430542, 'validation/loss': 1.7205344438552856, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.373055934906006, 'test/num_examples': 10000, 'score': 36177.24590039253, 'total_duration': 38918.11818599701, 'accumulated_submission_time': 36177.24590039253, 'accumulated_eval_time': 2733.6324348449707, 'accumulated_logging_time': 3.2079083919525146}
I0131 00:31:24.320505 139990499587840 logging_writer.py:48] [80043] accumulated_eval_time=2733.632435, accumulated_logging_time=3.207908, accumulated_submission_time=36177.245900, global_step=80043, preemption_count=0, score=36177.245900, test/accuracy=0.497000, test/loss=2.373056, test/num_examples=10000, total_duration=38918.118186, train/accuracy=0.664746, train/loss=1.510552, validation/accuracy=0.615340, validation/loss=1.720534, validation/num_examples=50000
I0131 00:31:47.510319 139990491195136 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.6651451587677002, loss=3.0998761653900146
I0131 00:32:31.720154 139990499587840 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.3631683588027954, loss=5.316041469573975
I0131 00:33:17.710992 139990491195136 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.8297607898712158, loss=3.236248016357422
I0131 00:34:03.546756 139990499587840 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.7094058990478516, loss=3.151090145111084
I0131 00:34:49.168247 139990491195136 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.6876952648162842, loss=3.0763986110687256
I0131 00:35:35.066382 139990499587840 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.5740571022033691, loss=4.405403137207031
I0131 00:36:20.823860 139990491195136 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.7491081953048706, loss=3.3438010215759277
I0131 00:37:06.816080 139990499587840 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.8345359563827515, loss=3.2352776527404785
I0131 00:37:52.301924 139990491195136 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.8045783042907715, loss=3.21860933303833
I0131 00:38:24.404582 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:38:34.919695 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:38:56.218775 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:38:57.818531 140184451094336 submission_runner.py:408] Time since start: 39371.65s, 	Step: 80972, 	{'train/accuracy': 0.6741796731948853, 'train/loss': 1.4529361724853516, 'validation/accuracy': 0.6193599700927734, 'validation/loss': 1.702526330947876, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.3454010486602783, 'test/num_examples': 10000, 'score': 36597.27389025688, 'total_duration': 39371.64637541771, 'accumulated_submission_time': 36597.27389025688, 'accumulated_eval_time': 2767.0463218688965, 'accumulated_logging_time': 3.2479307651519775}
I0131 00:38:57.850791 139990499587840 logging_writer.py:48] [80972] accumulated_eval_time=2767.046322, accumulated_logging_time=3.247931, accumulated_submission_time=36597.273890, global_step=80972, preemption_count=0, score=36597.273890, test/accuracy=0.497400, test/loss=2.345401, test/num_examples=10000, total_duration=39371.646375, train/accuracy=0.674180, train/loss=1.452936, validation/accuracy=0.619360, validation/loss=1.702526, validation/num_examples=50000
I0131 00:39:09.467043 139990491195136 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.6154509782791138, loss=3.384843349456787
I0131 00:39:51.924416 139990499587840 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.7501436471939087, loss=3.1512770652770996
I0131 00:40:37.846677 139990491195136 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.980209231376648, loss=3.1219120025634766
I0131 00:41:23.976669 139990499587840 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.8484777212142944, loss=3.035069465637207
I0131 00:42:10.125441 139990491195136 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.3585561513900757, loss=4.056299209594727
I0131 00:42:55.802575 139990499587840 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.6764838695526123, loss=3.2895565032958984
I0131 00:43:41.679353 139990491195136 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.7358134984970093, loss=3.1861178874969482
I0131 00:44:27.351988 139990499587840 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.7495129108428955, loss=3.404297113418579
I0131 00:45:13.229588 139990491195136 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.4898127317428589, loss=4.887390613555908
I0131 00:45:57.835184 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:46:08.199183 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:46:29.501949 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:46:31.112027 140184451094336 submission_runner.py:408] Time since start: 39824.94s, 	Step: 81899, 	{'train/accuracy': 0.667773425579071, 'train/loss': 1.5025302171707153, 'validation/accuracy': 0.622759997844696, 'validation/loss': 1.6995172500610352, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.336276054382324, 'test/num_examples': 10000, 'score': 37017.20155906677, 'total_duration': 39824.93992829323, 'accumulated_submission_time': 37017.20155906677, 'accumulated_eval_time': 2800.32315158844, 'accumulated_logging_time': 3.2903239727020264}
I0131 00:46:31.146295 139990499587840 logging_writer.py:48] [81899] accumulated_eval_time=2800.323152, accumulated_logging_time=3.290324, accumulated_submission_time=37017.201559, global_step=81899, preemption_count=0, score=37017.201559, test/accuracy=0.502700, test/loss=2.336276, test/num_examples=10000, total_duration=39824.939928, train/accuracy=0.667773, train/loss=1.502530, validation/accuracy=0.622760, validation/loss=1.699517, validation/num_examples=50000
I0131 00:46:31.944226 139990491195136 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.7903954982757568, loss=3.1299076080322266
I0131 00:47:13.107904 139990499587840 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.6636993885040283, loss=3.1665048599243164
I0131 00:47:58.479975 139990491195136 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.8436683416366577, loss=3.1272454261779785
I0131 00:48:44.313604 139990499587840 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.6144907474517822, loss=3.4891719818115234
I0131 00:49:29.974922 139990491195136 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.834306240081787, loss=3.170431137084961
I0131 00:50:15.816759 139990499587840 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.65787935256958, loss=3.1764638423919678
I0131 00:51:01.266168 139990491195136 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.041116714477539, loss=3.1847589015960693
I0131 00:51:47.262942 139990499587840 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.7834558486938477, loss=3.1329526901245117
I0131 00:52:33.145402 139990491195136 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.7933286428451538, loss=3.1554195880889893
I0131 00:53:18.872881 139990499587840 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.4506257772445679, loss=4.330113887786865
I0131 00:53:31.303498 140184451094336 spec.py:321] Evaluating on the training split.
I0131 00:53:41.509891 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 00:54:01.706107 140184451094336 spec.py:349] Evaluating on the test split.
I0131 00:54:03.314606 140184451094336 submission_runner.py:408] Time since start: 40277.14s, 	Step: 82829, 	{'train/accuracy': 0.6694530844688416, 'train/loss': 1.4893957376480103, 'validation/accuracy': 0.6229199767112732, 'validation/loss': 1.6935948133468628, 'validation/num_examples': 50000, 'test/accuracy': 0.5026000142097473, 'test/loss': 2.331036329269409, 'test/num_examples': 10000, 'score': 37437.302568912506, 'total_duration': 40277.142501831055, 'accumulated_submission_time': 37437.302568912506, 'accumulated_eval_time': 2832.334250688553, 'accumulated_logging_time': 3.334193706512451}
I0131 00:54:03.344423 139990491195136 logging_writer.py:48] [82829] accumulated_eval_time=2832.334251, accumulated_logging_time=3.334194, accumulated_submission_time=37437.302569, global_step=82829, preemption_count=0, score=37437.302569, test/accuracy=0.502600, test/loss=2.331036, test/num_examples=10000, total_duration=40277.142502, train/accuracy=0.669453, train/loss=1.489396, validation/accuracy=0.622920, validation/loss=1.693595, validation/num_examples=50000
I0131 00:54:32.305734 139990499587840 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.6393771171569824, loss=3.222590923309326
I0131 00:55:17.443008 139990491195136 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.6167863607406616, loss=3.410090208053589
I0131 00:56:03.464830 139990499587840 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.721692442893982, loss=3.101503610610962
I0131 00:56:49.119589 139990491195136 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.7896149158477783, loss=3.114746332168579
I0131 00:57:35.083645 139990499587840 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.871853232383728, loss=3.1141324043273926
I0131 00:58:20.941553 139990491195136 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.4884705543518066, loss=3.996516704559326
I0131 00:59:06.422869 139990499587840 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.6702340841293335, loss=2.948857545852661
I0131 00:59:52.218577 139990491195136 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.970680594444275, loss=3.418361186981201
I0131 01:00:37.918507 139990499587840 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.6529686450958252, loss=3.058694839477539
I0131 01:01:03.618473 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:01:14.000168 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:01:35.126369 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:01:36.722921 140184451094336 submission_runner.py:408] Time since start: 40730.55s, 	Step: 83758, 	{'train/accuracy': 0.6728710532188416, 'train/loss': 1.4168633222579956, 'validation/accuracy': 0.626800000667572, 'validation/loss': 1.644349455833435, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.2819135189056396, 'test/num_examples': 10000, 'score': 37857.52192592621, 'total_duration': 40730.55080986023, 'accumulated_submission_time': 37857.52192592621, 'accumulated_eval_time': 2865.438676595688, 'accumulated_logging_time': 3.372976303100586}
I0131 01:01:36.751650 139990491195136 logging_writer.py:48] [83758] accumulated_eval_time=2865.438677, accumulated_logging_time=3.372976, accumulated_submission_time=37857.521926, global_step=83758, preemption_count=0, score=37857.521926, test/accuracy=0.498000, test/loss=2.281914, test/num_examples=10000, total_duration=40730.550810, train/accuracy=0.672871, train/loss=1.416863, validation/accuracy=0.626800, validation/loss=1.644349, validation/num_examples=50000
I0131 01:01:53.947556 139990499587840 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.9149866104125977, loss=3.1426985263824463
I0131 01:02:37.084591 139990491195136 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.6569302082061768, loss=3.487682342529297
I0131 01:03:22.743750 139990499587840 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.6937967538833618, loss=3.101254463195801
I0131 01:04:08.617688 139990491195136 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.5275590419769287, loss=5.097152233123779
I0131 01:04:54.458599 139990499587840 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.7135083675384521, loss=3.2102184295654297
I0131 01:05:40.305425 139990491195136 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.4157378673553467, loss=5.296128273010254
I0131 01:06:26.220619 139990499587840 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.5580954551696777, loss=3.5607008934020996
I0131 01:07:12.197597 139990491195136 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.3668655157089233, loss=5.048293590545654
I0131 01:07:57.972171 139990499587840 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.4219319820404053, loss=5.233435153961182
I0131 01:08:36.927259 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:08:47.413248 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:09:10.179842 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:09:11.776887 140184451094336 submission_runner.py:408] Time since start: 41185.60s, 	Step: 84687, 	{'train/accuracy': 0.695019543170929, 'train/loss': 1.3510807752609253, 'validation/accuracy': 0.6265199780464172, 'validation/loss': 1.650166630744934, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.274543285369873, 'test/num_examples': 10000, 'score': 38277.640315294266, 'total_duration': 41185.6047809124, 'accumulated_submission_time': 38277.640315294266, 'accumulated_eval_time': 2900.2883038520813, 'accumulated_logging_time': 3.4122555255889893}
I0131 01:09:11.806120 139990491195136 logging_writer.py:48] [84687] accumulated_eval_time=2900.288304, accumulated_logging_time=3.412256, accumulated_submission_time=38277.640315, global_step=84687, preemption_count=0, score=38277.640315, test/accuracy=0.508400, test/loss=2.274543, test/num_examples=10000, total_duration=41185.604781, train/accuracy=0.695020, train/loss=1.351081, validation/accuracy=0.626520, validation/loss=1.650167, validation/num_examples=50000
I0131 01:09:17.409132 139990499587840 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.5355910062789917, loss=3.9704344272613525
I0131 01:09:58.868119 139990491195136 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.7113282680511475, loss=3.5572400093078613
I0131 01:10:44.442064 139990499587840 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.4672257900238037, loss=3.9425675868988037
I0131 01:11:30.410649 139990491195136 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.9341813325881958, loss=3.052095651626587
I0131 01:12:16.434585 139990499587840 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.5132719278335571, loss=5.341762542724609
I0131 01:13:01.775462 139990491195136 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.877886176109314, loss=3.1686208248138428
I0131 01:13:47.555978 139990499587840 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.649645447731018, loss=3.562648057937622
I0131 01:14:33.329963 139990491195136 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.8992847204208374, loss=3.138479709625244
I0131 01:15:19.207323 139990499587840 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.7442904710769653, loss=3.0861756801605225
I0131 01:16:04.685443 139990491195136 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.670296311378479, loss=4.161130428314209
I0131 01:16:11.803374 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:16:22.773941 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:16:44.242617 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:16:45.846238 140184451094336 submission_runner.py:408] Time since start: 41639.67s, 	Step: 85617, 	{'train/accuracy': 0.6769140362739563, 'train/loss': 1.4378875494003296, 'validation/accuracy': 0.6297799944877625, 'validation/loss': 1.655285120010376, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.277604818344116, 'test/num_examples': 10000, 'score': 38697.576645851135, 'total_duration': 41639.674124240875, 'accumulated_submission_time': 38697.576645851135, 'accumulated_eval_time': 2934.331175804138, 'accumulated_logging_time': 3.451533555984497}
I0131 01:16:45.878837 139990499587840 logging_writer.py:48] [85617] accumulated_eval_time=2934.331176, accumulated_logging_time=3.451534, accumulated_submission_time=38697.576646, global_step=85617, preemption_count=0, score=38697.576646, test/accuracy=0.510400, test/loss=2.277605, test/num_examples=10000, total_duration=41639.674124, train/accuracy=0.676914, train/loss=1.437888, validation/accuracy=0.629780, validation/loss=1.655285, validation/num_examples=50000
I0131 01:17:19.529885 139990491195136 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.6100192070007324, loss=3.0412745475769043
I0131 01:18:05.401563 139990499587840 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.5728195905685425, loss=3.5960731506347656
I0131 01:18:51.107617 139990491195136 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.7445210218429565, loss=3.1001009941101074
I0131 01:19:36.615554 139990499587840 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.8137881755828857, loss=3.086089849472046
I0131 01:20:22.430602 139990491195136 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.0418026447296143, loss=3.2327489852905273
I0131 01:21:08.203906 139990499587840 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.6075551509857178, loss=3.518298625946045
I0131 01:21:53.938091 139990491195136 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.705349326133728, loss=3.203214645385742
I0131 01:22:40.204479 139990499587840 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.5450845956802368, loss=4.945645809173584
I0131 01:23:26.072919 139990491195136 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.5269200801849365, loss=4.331631183624268
I0131 01:23:45.893340 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:23:56.582820 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:24:16.134500 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:24:17.735994 140184451094336 submission_runner.py:408] Time since start: 42091.56s, 	Step: 86545, 	{'train/accuracy': 0.676074206829071, 'train/loss': 1.442645788192749, 'validation/accuracy': 0.6266199946403503, 'validation/loss': 1.6573635339736938, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2886831760406494, 'test/num_examples': 10000, 'score': 39117.53278756142, 'total_duration': 42091.563891649246, 'accumulated_submission_time': 39117.53278756142, 'accumulated_eval_time': 2966.1738238334656, 'accumulated_logging_time': 3.496487617492676}
I0131 01:24:17.767538 139990499587840 logging_writer.py:48] [86545] accumulated_eval_time=2966.173824, accumulated_logging_time=3.496488, accumulated_submission_time=39117.532788, global_step=86545, preemption_count=0, score=39117.532788, test/accuracy=0.506500, test/loss=2.288683, test/num_examples=10000, total_duration=42091.563892, train/accuracy=0.676074, train/loss=1.442646, validation/accuracy=0.626620, validation/loss=1.657364, validation/num_examples=50000
I0131 01:24:40.174163 139990491195136 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.6979520320892334, loss=2.9985833168029785
I0131 01:25:24.177446 139990499587840 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.872362732887268, loss=3.097193717956543
I0131 01:26:10.231764 139990491195136 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.8170840740203857, loss=3.2282161712646484
I0131 01:26:56.016422 139990499587840 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.7495630979537964, loss=3.4161603450775146
I0131 01:27:41.935880 139990491195136 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.510933756828308, loss=4.941319942474365
I0131 01:28:27.884648 139990499587840 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.8200207948684692, loss=3.1798863410949707
I0131 01:29:13.568221 139990491195136 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.8457725048065186, loss=3.276759386062622
I0131 01:29:59.408089 139990499587840 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.431174397468567, loss=5.071269512176514
I0131 01:30:45.320498 139990491195136 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.5981779098510742, loss=3.764768600463867
I0131 01:31:18.044123 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:31:28.260121 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:31:50.663560 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:31:52.263117 140184451094336 submission_runner.py:408] Time since start: 42546.09s, 	Step: 87473, 	{'train/accuracy': 0.6855859160423279, 'train/loss': 1.3951412439346313, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.6594972610473633, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.292130470275879, 'test/num_examples': 10000, 'score': 39537.75274658203, 'total_duration': 42546.0910179615, 'accumulated_submission_time': 39537.75274658203, 'accumulated_eval_time': 3000.3928265571594, 'accumulated_logging_time': 3.5384793281555176}
I0131 01:31:52.291215 139990499587840 logging_writer.py:48] [87473] accumulated_eval_time=3000.392827, accumulated_logging_time=3.538479, accumulated_submission_time=39537.752747, global_step=87473, preemption_count=0, score=39537.752747, test/accuracy=0.506400, test/loss=2.292130, test/num_examples=10000, total_duration=42546.091018, train/accuracy=0.685586, train/loss=1.395141, validation/accuracy=0.627200, validation/loss=1.659497, validation/num_examples=50000
I0131 01:32:03.502682 139990491195136 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.464445948600769, loss=4.316200256347656
I0131 01:32:46.313011 139990499587840 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.8498636484146118, loss=3.099661111831665
I0131 01:33:31.895062 139990491195136 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.905592441558838, loss=3.1561639308929443
I0131 01:34:17.879182 139990499587840 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.8505518436431885, loss=3.1044323444366455
I0131 01:35:03.641878 139990491195136 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.763409972190857, loss=3.021378993988037
I0131 01:35:49.276270 139990499587840 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.9426088333129883, loss=3.1387078762054443
I0131 01:36:34.834933 139990491195136 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.678168535232544, loss=3.2492942810058594
I0131 01:37:20.589628 139990499587840 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.8213047981262207, loss=3.0949437618255615
I0131 01:38:06.644776 139990491195136 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.6606661081314087, loss=4.835484027862549
I0131 01:38:52.468717 139990499587840 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.6744258403778076, loss=5.262879371643066
I0131 01:38:52.482671 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:39:03.086259 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:39:24.619188 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:39:26.219793 140184451094336 submission_runner.py:408] Time since start: 43000.05s, 	Step: 88401, 	{'train/accuracy': 0.6751171946525574, 'train/loss': 1.4573235511779785, 'validation/accuracy': 0.630299985408783, 'validation/loss': 1.667418122291565, 'validation/num_examples': 50000, 'test/accuracy': 0.5100000500679016, 'test/loss': 2.2937583923339844, 'test/num_examples': 10000, 'score': 39957.88946032524, 'total_duration': 43000.04769778252, 'accumulated_submission_time': 39957.88946032524, 'accumulated_eval_time': 3034.1299324035645, 'accumulated_logging_time': 3.575552463531494}
I0131 01:39:26.252055 139990491195136 logging_writer.py:48] [88401] accumulated_eval_time=3034.129932, accumulated_logging_time=3.575552, accumulated_submission_time=39957.889460, global_step=88401, preemption_count=0, score=39957.889460, test/accuracy=0.510000, test/loss=2.293758, test/num_examples=10000, total_duration=43000.047698, train/accuracy=0.675117, train/loss=1.457324, validation/accuracy=0.630300, validation/loss=1.667418, validation/num_examples=50000
I0131 01:40:07.139387 139990499587840 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.7645394802093506, loss=3.079218626022339
I0131 01:40:52.600612 139990491195136 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.7714049816131592, loss=3.017970085144043
I0131 01:41:38.867378 139990499587840 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4631839990615845, loss=4.014209747314453
I0131 01:42:24.653193 139990491195136 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.750990867614746, loss=3.0268375873565674
I0131 01:43:10.682402 139990499587840 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.884307622909546, loss=3.0897910594940186
I0131 01:43:56.515672 139990491195136 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.7166224718093872, loss=3.462523937225342
I0131 01:44:42.386090 139990499587840 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.7618579864501953, loss=3.087233543395996
I0131 01:45:28.602831 139990491195136 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.9143050909042358, loss=3.159334182739258
I0131 01:46:14.666023 139990499587840 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.3766509294509888, loss=4.267751216888428
I0131 01:46:26.353009 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:46:36.695184 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:46:58.392812 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:47:00.014799 140184451094336 submission_runner.py:408] Time since start: 43453.84s, 	Step: 89327, 	{'train/accuracy': 0.6839843392372131, 'train/loss': 1.394673466682434, 'validation/accuracy': 0.6331200003623962, 'validation/loss': 1.625125527381897, 'validation/num_examples': 50000, 'test/accuracy': 0.5110000371932983, 'test/loss': 2.2482283115386963, 'test/num_examples': 10000, 'score': 40377.935428380966, 'total_duration': 43453.84268569946, 'accumulated_submission_time': 40377.935428380966, 'accumulated_eval_time': 3067.7917091846466, 'accumulated_logging_time': 3.6167099475860596}
I0131 01:47:00.044213 139990491195136 logging_writer.py:48] [89327] accumulated_eval_time=3067.791709, accumulated_logging_time=3.616710, accumulated_submission_time=40377.935428, global_step=89327, preemption_count=0, score=40377.935428, test/accuracy=0.511000, test/loss=2.248228, test/num_examples=10000, total_duration=43453.842686, train/accuracy=0.683984, train/loss=1.394673, validation/accuracy=0.633120, validation/loss=1.625126, validation/num_examples=50000
I0131 01:47:29.741941 139990499587840 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.6028110980987549, loss=5.067263603210449
I0131 01:48:15.203695 139990491195136 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.4808026552200317, loss=4.468639373779297
I0131 01:49:01.141634 139990499587840 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.5969114303588867, loss=3.659677743911743
I0131 01:49:46.954920 139990491195136 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.4905381202697754, loss=4.729588508605957
I0131 01:50:32.754642 139990499587840 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.8199836015701294, loss=3.143183946609497
I0131 01:51:18.734510 139990491195136 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.6121997833251953, loss=5.177048683166504
I0131 01:52:04.763753 139990499587840 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.5619920492172241, loss=3.768420457839966
I0131 01:52:50.391098 139990491195136 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.5945971012115479, loss=3.641472339630127
I0131 01:53:36.228811 139990499587840 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.5336613655090332, loss=3.753572940826416
I0131 01:54:00.190363 140184451094336 spec.py:321] Evaluating on the training split.
I0131 01:54:10.968826 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 01:54:33.881346 140184451094336 spec.py:349] Evaluating on the test split.
I0131 01:54:35.487745 140184451094336 submission_runner.py:408] Time since start: 43909.32s, 	Step: 90254, 	{'train/accuracy': 0.6922265291213989, 'train/loss': 1.4032502174377441, 'validation/accuracy': 0.6358599662780762, 'validation/loss': 1.6580744981765747, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.281223773956299, 'test/num_examples': 10000, 'score': 40798.026288986206, 'total_duration': 43909.31564474106, 'accumulated_submission_time': 40798.026288986206, 'accumulated_eval_time': 3103.089093208313, 'accumulated_logging_time': 3.6548655033111572}
I0131 01:54:35.519255 139990491195136 logging_writer.py:48] [90254] accumulated_eval_time=3103.089093, accumulated_logging_time=3.654866, accumulated_submission_time=40798.026289, global_step=90254, preemption_count=0, score=40798.026289, test/accuracy=0.516100, test/loss=2.281224, test/num_examples=10000, total_duration=43909.315645, train/accuracy=0.692227, train/loss=1.403250, validation/accuracy=0.635860, validation/loss=1.658074, validation/num_examples=50000
I0131 01:54:54.311211 139990499587840 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.5114598274230957, loss=4.0600409507751465
I0131 01:55:38.090628 139990491195136 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.6108366250991821, loss=4.396795749664307
I0131 01:56:24.099477 139990499587840 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.915666103363037, loss=3.0521368980407715
I0131 01:57:09.872435 139990491195136 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.4132689237594604, loss=4.818081378936768
I0131 01:57:55.637663 139990499587840 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.765536904335022, loss=3.0515527725219727
I0131 01:58:41.150057 139990491195136 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.598738431930542, loss=4.608684539794922
I0131 01:59:27.124213 139990499587840 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.4889347553253174, loss=4.74555778503418
I0131 02:00:12.830333 139990491195136 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.915366291999817, loss=3.1496636867523193
I0131 02:00:58.640106 139990499587840 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.6045130491256714, loss=3.5890674591064453
I0131 02:01:35.722294 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:01:46.050387 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:02:06.993979 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:02:08.594902 140184451094336 submission_runner.py:408] Time since start: 44362.42s, 	Step: 91182, 	{'train/accuracy': 0.6755273342132568, 'train/loss': 1.446442723274231, 'validation/accuracy': 0.6324999928474426, 'validation/loss': 1.6372050046920776, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2850115299224854, 'test/num_examples': 10000, 'score': 41218.17198085785, 'total_duration': 44362.4227976799, 'accumulated_submission_time': 41218.17198085785, 'accumulated_eval_time': 3135.961694717407, 'accumulated_logging_time': 3.6973400115966797}
I0131 02:02:08.626836 139990491195136 logging_writer.py:48] [91182] accumulated_eval_time=3135.961695, accumulated_logging_time=3.697340, accumulated_submission_time=41218.171981, global_step=91182, preemption_count=0, score=41218.171981, test/accuracy=0.508500, test/loss=2.285012, test/num_examples=10000, total_duration=44362.422798, train/accuracy=0.675527, train/loss=1.446443, validation/accuracy=0.632500, validation/loss=1.637205, validation/num_examples=50000
I0131 02:02:16.239293 139990499587840 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.0461788177490234, loss=3.021813154220581
I0131 02:02:58.231338 139990491195136 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.6834014654159546, loss=3.51778244972229
I0131 02:03:44.045285 139990499587840 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.928367018699646, loss=2.9622294902801514
I0131 02:04:29.846898 139990491195136 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.7462767362594604, loss=5.223916053771973
I0131 02:05:15.607725 139990499587840 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.8287955522537231, loss=2.9893059730529785
I0131 02:06:01.323378 139990491195136 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.086289405822754, loss=3.124889612197876
I0131 02:06:47.023022 139990499587840 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.8345874547958374, loss=3.1360340118408203
I0131 02:07:32.930853 139990491195136 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.8168575763702393, loss=2.9159657955169678
I0131 02:08:18.443289 139990499587840 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.7752840518951416, loss=3.1382203102111816
I0131 02:09:04.150357 139990491195136 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.74573814868927, loss=5.026241779327393
I0131 02:09:08.791676 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:09:19.389820 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:09:41.440284 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:09:43.042453 140184451094336 submission_runner.py:408] Time since start: 44816.87s, 	Step: 92112, 	{'train/accuracy': 0.6849218606948853, 'train/loss': 1.3958221673965454, 'validation/accuracy': 0.6351799964904785, 'validation/loss': 1.6146154403686523, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.2473669052124023, 'test/num_examples': 10000, 'score': 41638.28119826317, 'total_duration': 44816.870346307755, 'accumulated_submission_time': 41638.28119826317, 'accumulated_eval_time': 3170.2124574184418, 'accumulated_logging_time': 3.7383203506469727}
I0131 02:09:43.072195 139990499587840 logging_writer.py:48] [92112] accumulated_eval_time=3170.212457, accumulated_logging_time=3.738320, accumulated_submission_time=41638.281198, global_step=92112, preemption_count=0, score=41638.281198, test/accuracy=0.511800, test/loss=2.247367, test/num_examples=10000, total_duration=44816.870346, train/accuracy=0.684922, train/loss=1.395822, validation/accuracy=0.635180, validation/loss=1.614615, validation/num_examples=50000
I0131 02:10:19.000972 139990491195136 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.899724006652832, loss=3.362717866897583
I0131 02:11:04.803692 139990499587840 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.5740267038345337, loss=5.136344909667969
I0131 02:11:51.199516 139990491195136 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.607747197151184, loss=4.115575790405273
I0131 02:12:37.129871 139990499587840 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.9549357891082764, loss=3.028834581375122
I0131 02:13:22.907433 139990491195136 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.6578176021575928, loss=3.5526063442230225
I0131 02:14:09.063217 139990499587840 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.8095625638961792, loss=3.08579421043396
I0131 02:14:54.798734 139990491195136 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.903388500213623, loss=3.0499789714813232
I0131 02:15:40.786992 139990499587840 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.6747736930847168, loss=5.315186500549316
I0131 02:16:26.545063 139990491195136 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.7776566743850708, loss=3.2070155143737793
I0131 02:16:43.169686 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:16:53.302817 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:17:13.861760 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:17:15.466124 140184451094336 submission_runner.py:408] Time since start: 45269.29s, 	Step: 93038, 	{'train/accuracy': 0.689160168170929, 'train/loss': 1.3483566045761108, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.5728561878204346, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.2010042667388916, 'test/num_examples': 10000, 'score': 42058.32339930534, 'total_duration': 45269.29402279854, 'accumulated_submission_time': 42058.32339930534, 'accumulated_eval_time': 3202.508903503418, 'accumulated_logging_time': 3.7772610187530518}
I0131 02:17:15.494316 139990499587840 logging_writer.py:48] [93038] accumulated_eval_time=3202.508904, accumulated_logging_time=3.777261, accumulated_submission_time=42058.323399, global_step=93038, preemption_count=0, score=42058.323399, test/accuracy=0.523200, test/loss=2.201004, test/num_examples=10000, total_duration=45269.294023, train/accuracy=0.689160, train/loss=1.348357, validation/accuracy=0.641800, validation/loss=1.572856, validation/num_examples=50000
I0131 02:17:40.690392 139990491195136 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.760427474975586, loss=4.295098304748535
I0131 02:18:25.063266 139990499587840 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.992428183555603, loss=2.9913525581359863
I0131 02:19:11.182106 139990491195136 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.5432437658309937, loss=4.670719623565674
I0131 02:19:56.779383 139990499587840 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.1978864669799805, loss=3.1342592239379883
I0131 02:20:42.308939 139990491195136 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.6571205854415894, loss=3.1445679664611816
I0131 02:21:28.198514 139990499587840 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.7707793712615967, loss=3.9275219440460205
I0131 02:22:14.252679 139990491195136 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.917270302772522, loss=3.0827817916870117
I0131 02:22:59.720283 139990499587840 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.6111199855804443, loss=5.216322422027588
I0131 02:23:45.982572 139990491195136 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.6834990978240967, loss=4.1431732177734375
I0131 02:24:15.530326 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:24:25.984660 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:24:46.309425 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:24:47.913927 140184451094336 submission_runner.py:408] Time since start: 45721.74s, 	Step: 93965, 	{'train/accuracy': 0.7109375, 'train/loss': 1.3136073350906372, 'validation/accuracy': 0.6380199790000916, 'validation/loss': 1.624780297279358, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.255450487136841, 'test/num_examples': 10000, 'score': 42478.304805994034, 'total_duration': 45721.74182486534, 'accumulated_submission_time': 42478.304805994034, 'accumulated_eval_time': 3234.8925092220306, 'accumulated_logging_time': 3.814180850982666}
I0131 02:24:47.944531 139990499587840 logging_writer.py:48] [93965] accumulated_eval_time=3234.892509, accumulated_logging_time=3.814181, accumulated_submission_time=42478.304806, global_step=93965, preemption_count=0, score=42478.304806, test/accuracy=0.516600, test/loss=2.255450, test/num_examples=10000, total_duration=45721.741825, train/accuracy=0.710938, train/loss=1.313607, validation/accuracy=0.638020, validation/loss=1.624780, validation/num_examples=50000
I0131 02:25:02.354132 139990491195136 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.6530959606170654, loss=3.4237515926361084
I0131 02:25:45.691696 139990499587840 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.8665164709091187, loss=3.081631898880005
I0131 02:26:32.207580 139990491195136 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.8336656093597412, loss=3.1505649089813232
I0131 02:27:18.272610 139990499587840 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.6384663581848145, loss=4.721087455749512
I0131 02:28:04.515586 139990491195136 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.953579306602478, loss=2.9184634685516357
I0131 02:28:50.565525 139990499587840 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.9909322261810303, loss=3.3244049549102783
I0131 02:29:36.607184 139990491195136 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.8821145296096802, loss=2.9971983432769775
I0131 02:30:22.726496 139990499587840 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.7786248922348022, loss=3.380262851715088
I0131 02:31:08.707654 139990491195136 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.6190147399902344, loss=4.892704486846924
I0131 02:31:48.157360 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:31:58.277091 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:32:20.320527 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:32:21.927622 140184451094336 submission_runner.py:408] Time since start: 46175.76s, 	Step: 94887, 	{'train/accuracy': 0.6878905892372131, 'train/loss': 1.4085044860839844, 'validation/accuracy': 0.6405199766159058, 'validation/loss': 1.6214958429336548, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.2491910457611084, 'test/num_examples': 10000, 'score': 42898.46243238449, 'total_duration': 46175.75550246239, 'accumulated_submission_time': 42898.46243238449, 'accumulated_eval_time': 3268.6627497673035, 'accumulated_logging_time': 3.8543221950531006}
I0131 02:32:21.964920 139990499587840 logging_writer.py:48] [94887] accumulated_eval_time=3268.662750, accumulated_logging_time=3.854322, accumulated_submission_time=42898.462432, global_step=94887, preemption_count=0, score=42898.462432, test/accuracy=0.522500, test/loss=2.249191, test/num_examples=10000, total_duration=46175.755502, train/accuracy=0.687891, train/loss=1.408504, validation/accuracy=0.640520, validation/loss=1.621496, validation/num_examples=50000
I0131 02:32:27.568271 139990491195136 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.9332581758499146, loss=3.0462474822998047
I0131 02:33:09.239204 139990499587840 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.0374269485473633, loss=3.0064263343811035
I0131 02:33:55.200017 139990491195136 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.8959684371948242, loss=2.9632198810577393
I0131 02:34:41.158447 139990499587840 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.037424087524414, loss=2.9329800605773926
I0131 02:35:27.025065 139990491195136 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.6179139614105225, loss=3.8029065132141113
I0131 02:36:13.078455 139990499587840 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.485831379890442, loss=5.213866233825684
I0131 02:36:58.588617 139990491195136 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.6926891803741455, loss=4.36479377746582
I0131 02:37:44.677725 139990499587840 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.762529730796814, loss=3.252476692199707
I0131 02:38:30.633282 139990491195136 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.9329638481140137, loss=3.0211105346679688
I0131 02:39:16.512004 139990499587840 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.7525618076324463, loss=3.391845464706421
I0131 02:39:22.099960 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:39:32.303091 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:39:54.420987 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:39:56.024158 140184451094336 submission_runner.py:408] Time since start: 46629.85s, 	Step: 95814, 	{'train/accuracy': 0.6914257407188416, 'train/loss': 1.367840051651001, 'validation/accuracy': 0.6401599645614624, 'validation/loss': 1.597143292427063, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.2320969104766846, 'test/num_examples': 10000, 'score': 43318.54101729393, 'total_duration': 46629.85206055641, 'accumulated_submission_time': 43318.54101729393, 'accumulated_eval_time': 3302.586932182312, 'accumulated_logging_time': 3.902494430541992}
I0131 02:39:56.060850 139990491195136 logging_writer.py:48] [95814] accumulated_eval_time=3302.586932, accumulated_logging_time=3.902494, accumulated_submission_time=43318.541017, global_step=95814, preemption_count=0, score=43318.541017, test/accuracy=0.521300, test/loss=2.232097, test/num_examples=10000, total_duration=46629.852061, train/accuracy=0.691426, train/loss=1.367840, validation/accuracy=0.640160, validation/loss=1.597143, validation/num_examples=50000
I0131 02:40:30.944326 139990499587840 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.9418096542358398, loss=3.0324857234954834
I0131 02:41:16.648604 139990491195136 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.7890156507492065, loss=3.79557204246521
I0131 02:42:02.582994 139990499587840 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.994970679283142, loss=5.245757579803467
I0131 02:42:48.319188 139990491195136 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.9833890199661255, loss=2.9986155033111572
I0131 02:43:34.082550 139990499587840 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.8070998191833496, loss=2.901383876800537
I0131 02:44:20.232797 139990491195136 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.9223260879516602, loss=3.0299148559570312
I0131 02:45:05.756905 139990499587840 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.7708007097244263, loss=3.744982957839966
I0131 02:45:51.327256 139990491195136 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.8936634063720703, loss=3.2984797954559326
I0131 02:46:37.183445 139990499587840 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.9344401359558105, loss=2.9577817916870117
I0131 02:46:56.407481 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:47:07.238622 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:47:29.000788 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:47:30.597532 140184451094336 submission_runner.py:408] Time since start: 47084.43s, 	Step: 96744, 	{'train/accuracy': 0.7025195360183716, 'train/loss': 1.342665433883667, 'validation/accuracy': 0.6408999562263489, 'validation/loss': 1.617620587348938, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.2584726810455322, 'test/num_examples': 10000, 'score': 43738.83052825928, 'total_duration': 47084.425433158875, 'accumulated_submission_time': 43738.83052825928, 'accumulated_eval_time': 3336.7769792079926, 'accumulated_logging_time': 3.949820041656494}
I0131 02:47:30.631991 139990491195136 logging_writer.py:48] [96744] accumulated_eval_time=3336.776979, accumulated_logging_time=3.949820, accumulated_submission_time=43738.830528, global_step=96744, preemption_count=0, score=43738.830528, test/accuracy=0.517600, test/loss=2.258473, test/num_examples=10000, total_duration=47084.425433, train/accuracy=0.702520, train/loss=1.342665, validation/accuracy=0.640900, validation/loss=1.617621, validation/num_examples=50000
I0131 02:47:53.430713 139990499587840 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.027952194213867, loss=3.0646626949310303
I0131 02:48:37.634910 139990491195136 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.7776659727096558, loss=5.125217437744141
I0131 02:49:23.637429 139990499587840 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.8582414388656616, loss=2.940416097640991
I0131 02:50:09.566310 139990491195136 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.5857278108596802, loss=5.106667518615723
I0131 02:50:55.108200 139990499587840 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.7489932775497437, loss=5.052432060241699
I0131 02:51:40.748826 139990491195136 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.0041134357452393, loss=2.999824285507202
I0131 02:52:26.797267 139990499587840 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.7414098978042603, loss=4.6603851318359375
I0131 02:53:12.614715 139990491195136 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.051351308822632, loss=3.0044548511505127
I0131 02:53:58.162213 139990499587840 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.2819039821624756, loss=3.0327134132385254
I0131 02:54:30.930108 140184451094336 spec.py:321] Evaluating on the training split.
I0131 02:54:41.453638 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 02:55:02.718968 140184451094336 spec.py:349] Evaluating on the test split.
I0131 02:55:04.320790 140184451094336 submission_runner.py:408] Time since start: 47538.15s, 	Step: 97673, 	{'train/accuracy': 0.68896484375, 'train/loss': 1.3808518648147583, 'validation/accuracy': 0.6461799740791321, 'validation/loss': 1.570910930633545, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.2020792961120605, 'test/num_examples': 10000, 'score': 44159.072498083115, 'total_duration': 47538.14868545532, 'accumulated_submission_time': 44159.072498083115, 'accumulated_eval_time': 3370.167640209198, 'accumulated_logging_time': 3.994457244873047}
I0131 02:55:04.350518 139990491195136 logging_writer.py:48] [97673] accumulated_eval_time=3370.167640, accumulated_logging_time=3.994457, accumulated_submission_time=44159.072498, global_step=97673, preemption_count=0, score=44159.072498, test/accuracy=0.525300, test/loss=2.202079, test/num_examples=10000, total_duration=47538.148685, train/accuracy=0.688965, train/loss=1.380852, validation/accuracy=0.646180, validation/loss=1.570911, validation/num_examples=50000
I0131 02:55:15.537461 139990499587840 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.674850583076477, loss=4.232176303863525
I0131 02:55:57.901745 139990491195136 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.7981117963790894, loss=3.010586738586426
I0131 02:56:44.338912 139990499587840 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.8804734945297241, loss=3.2420754432678223
I0131 02:57:30.345514 139990491195136 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.6880508661270142, loss=4.3531670570373535
I0131 02:58:16.422881 139990499587840 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.6483062505722046, loss=3.6922225952148438
I0131 02:59:02.294840 139990491195136 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.056123733520508, loss=2.9760146141052246
I0131 02:59:48.026543 139990499587840 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.8130956888198853, loss=4.413309574127197
I0131 03:00:34.012056 139990491195136 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.8338147401809692, loss=4.7374043464660645
I0131 03:01:19.949132 139990499587840 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.9626750946044922, loss=2.9854328632354736
I0131 03:02:04.486774 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:02:14.662751 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:02:36.459896 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:02:38.059193 140184451094336 submission_runner.py:408] Time since start: 47991.89s, 	Step: 98598, 	{'train/accuracy': 0.69544917345047, 'train/loss': 1.3356281518936157, 'validation/accuracy': 0.6452599763870239, 'validation/loss': 1.56348717212677, 'validation/num_examples': 50000, 'test/accuracy': 0.5278000235557556, 'test/loss': 2.181706428527832, 'test/num_examples': 10000, 'score': 44579.15256071091, 'total_duration': 47991.88709282875, 'accumulated_submission_time': 44579.15256071091, 'accumulated_eval_time': 3403.740065574646, 'accumulated_logging_time': 4.033020973205566}
I0131 03:02:38.092471 139990491195136 logging_writer.py:48] [98598] accumulated_eval_time=3403.740066, accumulated_logging_time=4.033021, accumulated_submission_time=44579.152561, global_step=98598, preemption_count=0, score=44579.152561, test/accuracy=0.527800, test/loss=2.181706, test/num_examples=10000, total_duration=47991.887093, train/accuracy=0.695449, train/loss=1.335628, validation/accuracy=0.645260, validation/loss=1.563487, validation/num_examples=50000
I0131 03:02:39.295408 139990499587840 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.596044659614563, loss=4.9459028244018555
I0131 03:03:20.713948 139990491195136 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.936668038368225, loss=2.960500717163086
I0131 03:04:06.563936 139990499587840 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.9764801263809204, loss=3.031184434890747
I0131 03:04:52.712721 139990491195136 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.875055193901062, loss=2.926931381225586
I0131 03:05:38.381531 139990499587840 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.9450161457061768, loss=2.959045648574829
I0131 03:06:24.226322 139990491195136 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.6138191223144531, loss=4.171904563903809
I0131 03:07:09.968055 139990499587840 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.864295482635498, loss=2.9857046604156494
I0131 03:07:55.993045 139990491195136 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.738499641418457, loss=3.048722267150879
I0131 03:08:41.700073 139990499587840 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.294891595840454, loss=3.0091335773468018
I0131 03:09:27.518035 139990491195136 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.734279751777649, loss=2.925168514251709
I0131 03:09:38.105494 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:09:48.431923 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:10:10.308699 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:10:11.909679 140184451094336 submission_runner.py:408] Time since start: 48445.74s, 	Step: 99525, 	{'train/accuracy': 0.6994921565055847, 'train/loss': 1.3409777879714966, 'validation/accuracy': 0.6436600089073181, 'validation/loss': 1.5910606384277344, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.2290115356445312, 'test/num_examples': 10000, 'score': 44999.109080553055, 'total_duration': 48445.73757982254, 'accumulated_submission_time': 44999.109080553055, 'accumulated_eval_time': 3437.5442354679108, 'accumulated_logging_time': 4.075830459594727}
I0131 03:10:11.946014 139990499587840 logging_writer.py:48] [99525] accumulated_eval_time=3437.544235, accumulated_logging_time=4.075830, accumulated_submission_time=44999.109081, global_step=99525, preemption_count=0, score=44999.109081, test/accuracy=0.520100, test/loss=2.229012, test/num_examples=10000, total_duration=48445.737580, train/accuracy=0.699492, train/loss=1.340978, validation/accuracy=0.643660, validation/loss=1.591061, validation/num_examples=50000
I0131 03:10:42.328933 139990491195136 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.9865814447402954, loss=2.9807145595550537
I0131 03:11:27.341913 139990499587840 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.9464120864868164, loss=3.0971827507019043
I0131 03:12:13.581779 139990491195136 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7493723630905151, loss=3.030884265899658
I0131 03:12:59.607850 139990499587840 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.9846774339675903, loss=2.9576916694641113
I0131 03:13:45.328278 139990491195136 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.588240623474121, loss=4.162073135375977
I0131 03:14:31.313292 139990499587840 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.0667402744293213, loss=3.0706965923309326
I0131 03:15:17.131250 139990491195136 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.8213070631027222, loss=3.0167958736419678
I0131 03:16:03.004367 139990499587840 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.9199923276901245, loss=2.9817752838134766
I0131 03:16:48.460011 139990499587840 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.9596647024154663, loss=5.044920921325684
I0131 03:17:12.086384 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:17:22.539592 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:17:44.012722 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:17:45.615915 140184451094336 submission_runner.py:408] Time since start: 48899.44s, 	Step: 100453, 	{'train/accuracy': 0.6985937356948853, 'train/loss': 1.3283151388168335, 'validation/accuracy': 0.6507599949836731, 'validation/loss': 1.5489925146102905, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.1810965538024902, 'test/num_examples': 10000, 'score': 45419.194039821625, 'total_duration': 48899.44379377365, 'accumulated_submission_time': 45419.194039821625, 'accumulated_eval_time': 3471.073740005493, 'accumulated_logging_time': 4.120898962020874}
I0131 03:17:45.650792 139990491195136 logging_writer.py:48] [100453] accumulated_eval_time=3471.073740, accumulated_logging_time=4.120899, accumulated_submission_time=45419.194040, global_step=100453, preemption_count=0, score=45419.194040, test/accuracy=0.525400, test/loss=2.181097, test/num_examples=10000, total_duration=48899.443794, train/accuracy=0.698594, train/loss=1.328315, validation/accuracy=0.650760, validation/loss=1.548993, validation/num_examples=50000
I0131 03:18:04.848002 139990499587840 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.9787602424621582, loss=3.4149086475372314
I0131 03:18:48.691346 139990491195136 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.2234342098236084, loss=3.10626482963562
I0131 03:19:34.657168 139990499587840 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.8394439220428467, loss=2.885298490524292
I0131 03:20:20.255223 139990491195136 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.8293030261993408, loss=2.9390463829040527
I0131 03:21:06.106925 139990499587840 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.6961450576782227, loss=4.0051188468933105
I0131 03:21:51.817131 139990491195136 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.167534351348877, loss=2.956338882446289
I0131 03:22:37.662421 139990499587840 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.0050814151763916, loss=3.0331783294677734
I0131 03:23:23.654533 139990491195136 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.054409980773926, loss=3.032837390899658
I0131 03:24:09.318753 139990499587840 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.9721026420593262, loss=3.005044937133789
I0131 03:24:45.805490 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:24:56.128711 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:25:15.920230 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:25:17.527454 140184451094336 submission_runner.py:408] Time since start: 49351.36s, 	Step: 101381, 	{'train/accuracy': 0.7010741829872131, 'train/loss': 1.3405998945236206, 'validation/accuracy': 0.6499399542808533, 'validation/loss': 1.5560424327850342, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.1788530349731445, 'test/num_examples': 10000, 'score': 45839.292186021805, 'total_duration': 49351.35535264015, 'accumulated_submission_time': 45839.292186021805, 'accumulated_eval_time': 3502.795699119568, 'accumulated_logging_time': 4.166559219360352}
I0131 03:25:17.558635 139990491195136 logging_writer.py:48] [101381] accumulated_eval_time=3502.795699, accumulated_logging_time=4.166559, accumulated_submission_time=45839.292186, global_step=101381, preemption_count=0, score=45839.292186, test/accuracy=0.529400, test/loss=2.178853, test/num_examples=10000, total_duration=49351.355353, train/accuracy=0.701074, train/loss=1.340600, validation/accuracy=0.649940, validation/loss=1.556042, validation/num_examples=50000
I0131 03:25:25.557728 139990499587840 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.929745078086853, loss=3.0456717014312744
I0131 03:26:07.578291 139990491195136 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.0216569900512695, loss=2.94916033744812
I0131 03:26:53.559179 139990499587840 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.896874189376831, loss=3.0850419998168945
I0131 03:27:39.621435 139990491195136 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.8939485549926758, loss=3.1811776161193848
I0131 03:28:25.466871 139990499587840 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.6771609783172607, loss=3.7371833324432373
I0131 03:29:11.323309 139990491195136 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.03753662109375, loss=3.3394665718078613
I0131 03:29:57.111025 139990499587840 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.7638357877731323, loss=3.875344753265381
I0131 03:30:42.892231 139990491195136 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.9374547004699707, loss=3.060502529144287
I0131 03:31:28.726677 139990499587840 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.9793492555618286, loss=3.0351827144622803
I0131 03:32:14.868715 139990491195136 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.7973743677139282, loss=3.2190396785736084
I0131 03:32:17.876038 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:32:27.997309 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:32:49.896685 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:32:51.505858 140184451094336 submission_runner.py:408] Time since start: 49805.33s, 	Step: 102308, 	{'train/accuracy': 0.7044335603713989, 'train/loss': 1.303029179573059, 'validation/accuracy': 0.6525200009346008, 'validation/loss': 1.5312974452972412, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.1580193042755127, 'test/num_examples': 10000, 'score': 46259.55302786827, 'total_duration': 49805.33374285698, 'accumulated_submission_time': 46259.55302786827, 'accumulated_eval_time': 3536.4254937171936, 'accumulated_logging_time': 4.207376718521118}
I0131 03:32:51.540576 139990499587840 logging_writer.py:48] [102308] accumulated_eval_time=3536.425494, accumulated_logging_time=4.207377, accumulated_submission_time=46259.553028, global_step=102308, preemption_count=0, score=46259.553028, test/accuracy=0.532400, test/loss=2.158019, test/num_examples=10000, total_duration=49805.333743, train/accuracy=0.704434, train/loss=1.303029, validation/accuracy=0.652520, validation/loss=1.531297, validation/num_examples=50000
I0131 03:33:29.291877 139990491195136 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.8946174383163452, loss=2.944607734680176
I0131 03:34:14.924112 139990499587840 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.9259639978408813, loss=2.916337490081787
I0131 03:35:01.023743 139990491195136 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.3276004791259766, loss=4.664221286773682
I0131 03:35:46.869009 139990499587840 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.036015272140503, loss=2.925208568572998
I0131 03:36:32.572888 139990491195136 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.9279838800430298, loss=2.9574966430664062
I0131 03:37:18.393426 139990499587840 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.0741069316864014, loss=3.0145022869110107
I0131 03:38:04.179609 139990491195136 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.8538658618927002, loss=4.449512481689453
I0131 03:38:49.942581 139990499587840 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.0775811672210693, loss=2.950958251953125
I0131 03:39:35.769151 139990491195136 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.7683323621749878, loss=4.438814640045166
I0131 03:39:51.569207 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:40:01.789757 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:40:23.584048 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:40:25.196738 140184451094336 submission_runner.py:408] Time since start: 50259.02s, 	Step: 103236, 	{'train/accuracy': 0.721386730670929, 'train/loss': 1.2593860626220703, 'validation/accuracy': 0.6481999754905701, 'validation/loss': 1.5756916999816895, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.1878247261047363, 'test/num_examples': 10000, 'score': 46679.525918245316, 'total_duration': 50259.024639844894, 'accumulated_submission_time': 46679.525918245316, 'accumulated_eval_time': 3570.0530354976654, 'accumulated_logging_time': 4.251614093780518}
I0131 03:40:25.227133 139990499587840 logging_writer.py:48] [103236] accumulated_eval_time=3570.053035, accumulated_logging_time=4.251614, accumulated_submission_time=46679.525918, global_step=103236, preemption_count=0, score=46679.525918, test/accuracy=0.530300, test/loss=2.187825, test/num_examples=10000, total_duration=50259.024640, train/accuracy=0.721387, train/loss=1.259386, validation/accuracy=0.648200, validation/loss=1.575692, validation/num_examples=50000
I0131 03:40:51.239582 139990491195136 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.0558812618255615, loss=2.8788158893585205
I0131 03:41:35.696838 139990499587840 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.9730660915374756, loss=5.201809883117676
I0131 03:42:21.704517 139990491195136 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.7356033325195312, loss=3.5373167991638184
I0131 03:43:07.340079 139990499587840 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.0130069255828857, loss=2.8978118896484375
I0131 03:43:52.990080 139990491195136 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.961669683456421, loss=2.960688352584839
I0131 03:44:38.616166 139990499587840 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.0088984966278076, loss=4.529456615447998
I0131 03:45:24.309044 139990491195136 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.124201774597168, loss=3.003591537475586
I0131 03:46:10.319425 139990499587840 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.9218053817749023, loss=2.8650333881378174
I0131 03:46:55.872404 139990491195136 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.2282862663269043, loss=2.9863178730010986
I0131 03:47:25.363840 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:47:35.717121 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:47:58.267295 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:47:59.873962 140184451094336 submission_runner.py:408] Time since start: 50713.70s, 	Step: 104166, 	{'train/accuracy': 0.6998632550239563, 'train/loss': 1.3183801174163818, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.5319174528121948, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.147125005722046, 'test/num_examples': 10000, 'score': 47099.60453367233, 'total_duration': 50713.70186185837, 'accumulated_submission_time': 47099.60453367233, 'accumulated_eval_time': 3604.563158750534, 'accumulated_logging_time': 4.293308973312378}
I0131 03:47:59.907958 139990499587840 logging_writer.py:48] [104166] accumulated_eval_time=3604.563159, accumulated_logging_time=4.293309, accumulated_submission_time=47099.604534, global_step=104166, preemption_count=0, score=47099.604534, test/accuracy=0.526100, test/loss=2.147125, test/num_examples=10000, total_duration=50713.701862, train/accuracy=0.699863, train/loss=1.318380, validation/accuracy=0.652000, validation/loss=1.531917, validation/num_examples=50000
I0131 03:48:13.931336 139990491195136 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.022620916366577, loss=2.928766965866089
I0131 03:48:56.747747 139990499587840 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.2182199954986572, loss=2.9588494300842285
I0131 03:49:42.439416 139990491195136 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.1290640830993652, loss=2.9366226196289062
I0131 03:50:28.090433 139990499587840 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.9430489540100098, loss=2.920072317123413
I0131 03:51:13.771043 139990491195136 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.9820804595947266, loss=2.856541395187378
I0131 03:51:59.536013 139990499587840 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.9084534645080566, loss=2.911317825317383
I0131 03:52:45.168586 139990491195136 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.0189414024353027, loss=5.058874130249023
I0131 03:53:31.167269 139990499587840 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.95277738571167, loss=2.9633989334106445
I0131 03:54:17.000031 139990491195136 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.8241349458694458, loss=5.08338737487793
I0131 03:55:00.077079 140184451094336 spec.py:321] Evaluating on the training split.
I0131 03:55:10.575460 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 03:55:31.678152 140184451094336 spec.py:349] Evaluating on the test split.
I0131 03:55:33.280350 140184451094336 submission_runner.py:408] Time since start: 51167.11s, 	Step: 105096, 	{'train/accuracy': 0.7125781178474426, 'train/loss': 1.2877388000488281, 'validation/accuracy': 0.6566999554634094, 'validation/loss': 1.5229451656341553, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.143164873123169, 'test/num_examples': 10000, 'score': 47519.71596264839, 'total_duration': 51167.10824346542, 'accumulated_submission_time': 47519.71596264839, 'accumulated_eval_time': 3637.7664346694946, 'accumulated_logging_time': 4.338505029678345}
I0131 03:55:33.314345 139990499587840 logging_writer.py:48] [105096] accumulated_eval_time=3637.766435, accumulated_logging_time=4.338505, accumulated_submission_time=47519.715963, global_step=105096, preemption_count=0, score=47519.715963, test/accuracy=0.534900, test/loss=2.143165, test/num_examples=10000, total_duration=51167.108243, train/accuracy=0.712578, train/loss=1.287739, validation/accuracy=0.656700, validation/loss=1.522945, validation/num_examples=50000
I0131 03:55:35.317594 139990491195136 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.107285737991333, loss=2.985198736190796
I0131 03:56:16.707138 139990499587840 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.0832855701446533, loss=3.0128068923950195
I0131 03:57:02.321887 139990491195136 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.097118377685547, loss=3.168830394744873
I0131 03:57:48.191360 139990499587840 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.8850767612457275, loss=3.843977451324463
I0131 03:58:33.771701 139990491195136 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.135246753692627, loss=3.0059685707092285
I0131 03:59:19.643141 139990499587840 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.9038419723510742, loss=3.2839713096618652
I0131 04:00:05.449563 139990491195136 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.063082695007324, loss=2.9069247245788574
I0131 04:00:51.087757 139990499587840 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.7139673233032227, loss=4.148252010345459
I0131 04:01:37.154656 139990491195136 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.156660795211792, loss=3.3316686153411865
I0131 04:02:22.686001 139990499587840 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.0502383708953857, loss=2.9787464141845703
I0131 04:02:33.303022 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:02:43.485156 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:03:05.243345 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:03:06.839497 140184451094336 submission_runner.py:408] Time since start: 51620.67s, 	Step: 106025, 	{'train/accuracy': 0.71156245470047, 'train/loss': 1.2922778129577637, 'validation/accuracy': 0.6466999650001526, 'validation/loss': 1.5727142095565796, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.194082260131836, 'test/num_examples': 10000, 'score': 47939.64816617966, 'total_duration': 51620.6674015522, 'accumulated_submission_time': 47939.64816617966, 'accumulated_eval_time': 3671.302904844284, 'accumulated_logging_time': 4.3822362422943115}
I0131 04:03:06.873087 139990491195136 logging_writer.py:48] [106025] accumulated_eval_time=3671.302905, accumulated_logging_time=4.382236, accumulated_submission_time=47939.648166, global_step=106025, preemption_count=0, score=47939.648166, test/accuracy=0.526300, test/loss=2.194082, test/num_examples=10000, total_duration=51620.667402, train/accuracy=0.711562, train/loss=1.292278, validation/accuracy=0.646700, validation/loss=1.572714, validation/num_examples=50000
I0131 04:03:37.275323 139990499587840 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.0678915977478027, loss=2.9570493698120117
I0131 04:04:22.488357 139990491195136 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.232713222503662, loss=4.997125625610352
I0131 04:05:08.414551 139990499587840 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.9923444986343384, loss=2.8818445205688477
I0131 04:05:54.506432 139990491195136 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.047339916229248, loss=3.047957181930542
I0131 04:06:40.319654 139990499587840 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.0365309715270996, loss=3.0116567611694336
I0131 04:07:26.127399 139990491195136 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.0666496753692627, loss=4.540369033813477
I0131 04:08:11.955752 139990499587840 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.9340914487838745, loss=4.929752826690674
I0131 04:08:57.892993 139990491195136 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.9200514554977417, loss=3.1203508377075195
I0131 04:09:43.764815 139990499587840 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.822862148284912, loss=4.063344478607178
I0131 04:10:07.058727 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:10:17.428122 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:10:38.603547 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:10:40.210190 140184451094336 submission_runner.py:408] Time since start: 52074.04s, 	Step: 106952, 	{'train/accuracy': 0.7048046588897705, 'train/loss': 1.3123520612716675, 'validation/accuracy': 0.6563400030136108, 'validation/loss': 1.5270174741744995, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.1496691703796387, 'test/num_examples': 10000, 'score': 48359.77759027481, 'total_duration': 52074.03806447983, 'accumulated_submission_time': 48359.77759027481, 'accumulated_eval_time': 3704.454334497452, 'accumulated_logging_time': 4.4254231452941895}
I0131 04:10:40.242326 139990491195136 logging_writer.py:48] [106952] accumulated_eval_time=3704.454334, accumulated_logging_time=4.425423, accumulated_submission_time=48359.777590, global_step=106952, preemption_count=0, score=48359.777590, test/accuracy=0.533800, test/loss=2.149669, test/num_examples=10000, total_duration=52074.038064, train/accuracy=0.704805, train/loss=1.312352, validation/accuracy=0.656340, validation/loss=1.527017, validation/num_examples=50000
I0131 04:10:59.854653 139990499587840 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.1879465579986572, loss=2.8451101779937744
I0131 04:11:43.449377 139990491195136 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.203390598297119, loss=2.973090887069702
I0131 04:12:29.402273 139990499587840 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.9689502716064453, loss=2.877718687057495
I0131 04:13:15.591417 139990491195136 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.49381685256958, loss=4.358516693115234
I0131 04:14:01.304010 139990499587840 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.337336540222168, loss=3.0073606967926025
I0131 04:14:47.273889 139990491195136 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.0292656421661377, loss=4.371393203735352
I0131 04:15:33.171666 139990499587840 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.8622376918792725, loss=4.996272087097168
I0131 04:16:18.936028 139990491195136 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.9288266897201538, loss=3.1853249073028564
I0131 04:17:04.708539 139990499587840 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.1812477111816406, loss=2.9809041023254395
I0131 04:17:40.250621 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:17:50.779232 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:18:11.455006 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:18:13.059020 140184451094336 submission_runner.py:408] Time since start: 52526.89s, 	Step: 107880, 	{'train/accuracy': 0.7080664038658142, 'train/loss': 1.2890511751174927, 'validation/accuracy': 0.6545799970626831, 'validation/loss': 1.529636263847351, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.1473042964935303, 'test/num_examples': 10000, 'score': 48779.730001449585, 'total_duration': 52526.886921167374, 'accumulated_submission_time': 48779.730001449585, 'accumulated_eval_time': 3737.262728214264, 'accumulated_logging_time': 4.467191934585571}
I0131 04:18:13.094312 139990491195136 logging_writer.py:48] [107880] accumulated_eval_time=3737.262728, accumulated_logging_time=4.467192, accumulated_submission_time=48779.730001, global_step=107880, preemption_count=0, score=48779.730001, test/accuracy=0.535400, test/loss=2.147304, test/num_examples=10000, total_duration=52526.886921, train/accuracy=0.708066, train/loss=1.289051, validation/accuracy=0.654580, validation/loss=1.529636, validation/num_examples=50000
I0131 04:18:21.490491 139990499587840 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.018045663833618, loss=2.9145190715789795
I0131 04:19:03.571850 139990491195136 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.054525375366211, loss=2.9138269424438477
I0131 04:19:49.154763 139990499587840 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.8004199266433716, loss=3.8903164863586426
I0131 04:20:35.055227 139990491195136 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.1136796474456787, loss=5.204699993133545
I0131 04:21:20.645897 139990499587840 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.9577373266220093, loss=3.072488784790039
I0131 04:22:06.733354 139990491195136 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.910525918006897, loss=4.793239116668701
I0131 04:22:52.252742 139990499587840 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.8830451965332031, loss=3.9147963523864746
I0131 04:23:37.933575 139990491195136 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.125802755355835, loss=2.86973237991333
I0131 04:24:23.698416 139990499587840 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.0847327709198, loss=5.071905136108398
I0131 04:25:09.466732 139990491195136 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.409428119659424, loss=2.927264928817749
I0131 04:25:13.290338 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:25:23.799974 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:25:42.699594 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:25:44.308356 140184451094336 submission_runner.py:408] Time since start: 52978.14s, 	Step: 108810, 	{'train/accuracy': 0.7175390720367432, 'train/loss': 1.221261978149414, 'validation/accuracy': 0.6603599786758423, 'validation/loss': 1.4771640300750732, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.107815742492676, 'test/num_examples': 10000, 'score': 49199.868581056595, 'total_duration': 52978.13624429703, 'accumulated_submission_time': 49199.868581056595, 'accumulated_eval_time': 3768.2807273864746, 'accumulated_logging_time': 4.513633966445923}
I0131 04:25:44.350589 139990499587840 logging_writer.py:48] [108810] accumulated_eval_time=3768.280727, accumulated_logging_time=4.513634, accumulated_submission_time=49199.868581, global_step=108810, preemption_count=0, score=49199.868581, test/accuracy=0.538700, test/loss=2.107816, test/num_examples=10000, total_duration=52978.136244, train/accuracy=0.717539, train/loss=1.221262, validation/accuracy=0.660360, validation/loss=1.477164, validation/num_examples=50000
I0131 04:26:21.852521 139990491195136 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.010544538497925, loss=3.1581571102142334
I0131 04:27:07.402625 139990499587840 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.2915451526641846, loss=3.410785436630249
I0131 04:27:53.097448 139990491195136 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.9159497022628784, loss=2.819915294647217
I0131 04:28:38.788862 139990499587840 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.926235556602478, loss=3.05672550201416
I0131 04:29:24.451165 139990491195136 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.0152485370635986, loss=2.904398202896118
I0131 04:30:10.264302 139990499587840 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.982258915901184, loss=3.0660641193389893
I0131 04:30:56.095875 139990491195136 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.074033498764038, loss=3.164642810821533
I0131 04:31:42.065842 139990499587840 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.9488507509231567, loss=3.239351511001587
I0131 04:32:27.817965 139990491195136 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.9691318273544312, loss=4.11854362487793
I0131 04:32:44.461361 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:32:55.031268 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:33:14.317625 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:33:15.926329 140184451094336 submission_runner.py:408] Time since start: 53429.75s, 	Step: 109738, 	{'train/accuracy': 0.7140429615974426, 'train/loss': 1.2802772521972656, 'validation/accuracy': 0.6586199998855591, 'validation/loss': 1.5189369916915894, 'validation/num_examples': 50000, 'test/accuracy': 0.5404000282287598, 'test/loss': 2.142338991165161, 'test/num_examples': 10000, 'score': 49619.92288994789, 'total_duration': 53429.75419712067, 'accumulated_submission_time': 49619.92288994789, 'accumulated_eval_time': 3799.7456452846527, 'accumulated_logging_time': 4.565981388092041}
I0131 04:33:15.964535 139990499587840 logging_writer.py:48] [109738] accumulated_eval_time=3799.745645, accumulated_logging_time=4.565981, accumulated_submission_time=49619.922890, global_step=109738, preemption_count=0, score=49619.922890, test/accuracy=0.540400, test/loss=2.142339, test/num_examples=10000, total_duration=53429.754197, train/accuracy=0.714043, train/loss=1.280277, validation/accuracy=0.658620, validation/loss=1.518937, validation/num_examples=50000
I0131 04:33:41.153192 139990491195136 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.0820305347442627, loss=5.138200759887695
I0131 04:34:25.439396 139990499587840 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.116718053817749, loss=2.8921735286712646
I0131 04:35:11.292057 139990491195136 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.0597169399261475, loss=2.876190185546875
I0131 04:35:56.991504 139990499587840 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.0974655151367188, loss=2.987295627593994
I0131 04:36:42.454480 139990491195136 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.2748208045959473, loss=5.104216575622559
I0131 04:37:28.241008 139990499587840 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.3403866291046143, loss=3.0076169967651367
I0131 04:38:13.801464 139990491195136 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.1879944801330566, loss=3.2101192474365234
I0131 04:38:59.450028 139990499587840 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.061345338821411, loss=3.6220321655273438
I0131 04:39:45.231813 139990491195136 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.8430064916610718, loss=4.5954437255859375
I0131 04:40:16.048620 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:40:26.375303 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:40:48.213917 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:40:49.816253 140184451094336 submission_runner.py:408] Time since start: 53883.64s, 	Step: 110669, 	{'train/accuracy': 0.7124218344688416, 'train/loss': 1.2984763383865356, 'validation/accuracy': 0.6633599996566772, 'validation/loss': 1.5199729204177856, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.146238327026367, 'test/num_examples': 10000, 'score': 50039.94888544083, 'total_duration': 53883.64413046837, 'accumulated_submission_time': 50039.94888544083, 'accumulated_eval_time': 3833.5132508277893, 'accumulated_logging_time': 4.615039587020874}
I0131 04:40:49.853360 139990499587840 logging_writer.py:48] [110669] accumulated_eval_time=3833.513251, accumulated_logging_time=4.615040, accumulated_submission_time=50039.948885, global_step=110669, preemption_count=0, score=50039.948885, test/accuracy=0.542000, test/loss=2.146238, test/num_examples=10000, total_duration=53883.644130, train/accuracy=0.712422, train/loss=1.298476, validation/accuracy=0.663360, validation/loss=1.519973, validation/num_examples=50000
I0131 04:41:02.651323 139990491195136 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.0942981243133545, loss=2.890195369720459
I0131 04:41:45.232687 139990499587840 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.9677834510803223, loss=2.970707416534424
I0131 04:42:30.961561 139990491195136 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.961795687675476, loss=3.340866804122925
I0131 04:43:16.754592 139990499587840 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.336888313293457, loss=3.114393711090088
I0131 04:44:02.130715 139990491195136 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.2327158451080322, loss=2.8537960052490234
I0131 04:44:47.888550 139990499587840 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.1320602893829346, loss=3.032581090927124
I0131 04:45:33.467168 139990491195136 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.9834426641464233, loss=3.181450843811035
I0131 04:46:19.384207 139990499587840 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.9803162813186646, loss=3.3116910457611084
I0131 04:47:05.014971 139990491195136 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.1445000171661377, loss=2.8338558673858643
I0131 04:47:50.493859 139990499587840 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.208911657333374, loss=2.9284818172454834
I0131 04:47:50.506890 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:48:00.936790 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:48:22.056670 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:48:23.662912 140184451094336 submission_runner.py:408] Time since start: 54337.49s, 	Step: 111601, 	{'train/accuracy': 0.7237108945846558, 'train/loss': 1.2157682180404663, 'validation/accuracy': 0.6658399701118469, 'validation/loss': 1.4762276411056519, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.1115095615386963, 'test/num_examples': 10000, 'score': 50460.5451362133, 'total_duration': 54337.490814208984, 'accumulated_submission_time': 50460.5451362133, 'accumulated_eval_time': 3866.6692838668823, 'accumulated_logging_time': 4.662085294723511}
I0131 04:48:23.695724 139990491195136 logging_writer.py:48] [111601] accumulated_eval_time=3866.669284, accumulated_logging_time=4.662085, accumulated_submission_time=50460.545136, global_step=111601, preemption_count=0, score=50460.545136, test/accuracy=0.543400, test/loss=2.111510, test/num_examples=10000, total_duration=54337.490814, train/accuracy=0.723711, train/loss=1.215768, validation/accuracy=0.665840, validation/loss=1.476228, validation/num_examples=50000
I0131 04:49:04.471514 139990499587840 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.0534536838531494, loss=2.8853960037231445
I0131 04:49:50.032442 139990491195136 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.078238010406494, loss=5.111147880554199
I0131 04:50:36.207279 139990499587840 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.0485799312591553, loss=3.4577600955963135
I0131 04:51:21.773536 139990491195136 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.9507920742034912, loss=4.876171588897705
I0131 04:52:07.643366 139990499587840 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.071401596069336, loss=2.9272968769073486
I0131 04:52:54.257334 139990491195136 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.2353708744049072, loss=2.7834763526916504
I0131 04:53:39.906987 139990499587840 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.3735415935516357, loss=2.8742008209228516
I0131 04:54:25.801346 139990491195136 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.1843771934509277, loss=2.9676308631896973
I0131 04:55:11.488173 139990499587840 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.1438872814178467, loss=2.8806400299072266
I0131 04:55:23.949119 140184451094336 spec.py:321] Evaluating on the training split.
I0131 04:55:34.335363 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 04:55:56.379303 140184451094336 spec.py:349] Evaluating on the test split.
I0131 04:55:57.988239 140184451094336 submission_runner.py:408] Time since start: 54791.82s, 	Step: 112529, 	{'train/accuracy': 0.73876953125, 'train/loss': 1.147725224494934, 'validation/accuracy': 0.6629999876022339, 'validation/loss': 1.471642017364502, 'validation/num_examples': 50000, 'test/accuracy': 0.550000011920929, 'test/loss': 2.0856385231018066, 'test/num_examples': 10000, 'score': 50880.741792201996, 'total_duration': 54791.81612062454, 'accumulated_submission_time': 50880.741792201996, 'accumulated_eval_time': 3900.708383321762, 'accumulated_logging_time': 4.705749273300171}
I0131 04:55:58.029832 139990491195136 logging_writer.py:48] [112529] accumulated_eval_time=3900.708383, accumulated_logging_time=4.705749, accumulated_submission_time=50880.741792, global_step=112529, preemption_count=0, score=50880.741792, test/accuracy=0.550000, test/loss=2.085639, test/num_examples=10000, total_duration=54791.816121, train/accuracy=0.738770, train/loss=1.147725, validation/accuracy=0.663000, validation/loss=1.471642, validation/num_examples=50000
I0131 04:56:26.826313 139990499587840 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.9362739324569702, loss=3.141550064086914
I0131 04:57:11.934429 139990491195136 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.1289117336273193, loss=2.9101555347442627
I0131 04:57:57.487414 139990499587840 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.228823184967041, loss=2.773636817932129
I0131 04:58:43.327136 139990491195136 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.1108357906341553, loss=2.9383544921875
I0131 04:59:28.806980 139990499587840 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.4004480838775635, loss=2.8407645225524902
I0131 05:00:14.830384 139990491195136 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.8470429182052612, loss=3.4569456577301025
I0131 05:01:00.216439 139990499587840 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.0757439136505127, loss=2.919632911682129
I0131 05:01:46.225314 139990491195136 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.007615089416504, loss=4.353389739990234
I0131 05:02:31.660701 139990499587840 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.1237947940826416, loss=3.974815845489502
I0131 05:02:58.412132 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:03:08.964277 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:03:31.449823 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:03:33.050308 140184451094336 submission_runner.py:408] Time since start: 55246.88s, 	Step: 113460, 	{'train/accuracy': 0.7142187356948853, 'train/loss': 1.2561309337615967, 'validation/accuracy': 0.6647799611091614, 'validation/loss': 1.4717066287994385, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.0994019508361816, 'test/num_examples': 10000, 'score': 51301.066935777664, 'total_duration': 55246.878209114075, 'accumulated_submission_time': 51301.066935777664, 'accumulated_eval_time': 3935.3465564250946, 'accumulated_logging_time': 4.757417678833008}
I0131 05:03:33.085450 139990491195136 logging_writer.py:48] [113460] accumulated_eval_time=3935.346556, accumulated_logging_time=4.757418, accumulated_submission_time=51301.066936, global_step=113460, preemption_count=0, score=51301.066936, test/accuracy=0.546900, test/loss=2.099402, test/num_examples=10000, total_duration=55246.878209, train/accuracy=0.714219, train/loss=1.256131, validation/accuracy=0.664780, validation/loss=1.471707, validation/num_examples=50000
I0131 05:03:49.484340 139990499587840 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.0908548831939697, loss=3.482344150543213
I0131 05:04:32.783141 139990491195136 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.2207117080688477, loss=2.824655532836914
I0131 05:05:18.447654 139990499587840 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.2156150341033936, loss=2.912837505340576
I0131 05:06:04.513422 139990491195136 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.9193183183670044, loss=3.285490036010742
I0131 05:06:50.125787 139990499587840 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.2279269695281982, loss=4.088205337524414
I0131 05:07:35.933168 139990491195136 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.3414106369018555, loss=3.0685832500457764
I0131 05:08:21.631765 139990499587840 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.116973876953125, loss=2.861614227294922
I0131 05:09:07.492595 139990491195136 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.3128294944763184, loss=2.864053726196289
I0131 05:09:53.084389 139990499587840 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.1417911052703857, loss=2.8374381065368652
I0131 05:10:33.200793 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:10:43.655304 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:11:05.980351 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:11:07.574788 140184451094336 submission_runner.py:408] Time since start: 55701.40s, 	Step: 114389, 	{'train/accuracy': 0.7233593463897705, 'train/loss': 1.2339673042297363, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.4803937673568726, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.0966713428497314, 'test/num_examples': 10000, 'score': 51721.110796928406, 'total_duration': 55701.402671575546, 'accumulated_submission_time': 51721.110796928406, 'accumulated_eval_time': 3969.720571756363, 'accumulated_logging_time': 4.803069829940796}
I0131 05:11:07.617887 139990491195136 logging_writer.py:48] [114389] accumulated_eval_time=3969.720572, accumulated_logging_time=4.803070, accumulated_submission_time=51721.110797, global_step=114389, preemption_count=0, score=51721.110797, test/accuracy=0.549900, test/loss=2.096671, test/num_examples=10000, total_duration=55701.402672, train/accuracy=0.723359, train/loss=1.233967, validation/accuracy=0.666960, validation/loss=1.480394, validation/num_examples=50000
I0131 05:11:12.412914 139990499587840 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.0335605144500732, loss=3.8636608123779297
I0131 05:11:54.119437 139990491195136 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.042410373687744, loss=3.670745849609375
I0131 05:12:39.957071 139990499587840 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.2833025455474854, loss=2.916487693786621
I0131 05:13:25.749902 139990491195136 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.1757826805114746, loss=2.839801549911499
I0131 05:14:11.271885 139990499587840 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.190678119659424, loss=2.835907459259033
I0131 05:14:56.953742 139990491195136 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.3264107704162598, loss=2.8637654781341553
I0131 05:15:42.653750 139990499587840 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.324113368988037, loss=3.0021097660064697
I0131 05:16:28.886872 139990491195136 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.2839126586914062, loss=2.884687900543213
I0131 05:17:14.854803 139990499587840 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.3315842151641846, loss=2.8628227710723877
I0131 05:18:00.421745 139990491195136 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.5090339183807373, loss=2.946798801422119
I0131 05:18:08.057712 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:18:18.513859 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:18:40.018886 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:18:41.620011 140184451094336 submission_runner.py:408] Time since start: 56155.45s, 	Step: 115318, 	{'train/accuracy': 0.7351366877555847, 'train/loss': 1.1889073848724365, 'validation/accuracy': 0.6723600029945374, 'validation/loss': 1.4659477472305298, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.0902318954467773, 'test/num_examples': 10000, 'score': 52141.49275612831, 'total_duration': 56155.447914361954, 'accumulated_submission_time': 52141.49275612831, 'accumulated_eval_time': 4003.2828526496887, 'accumulated_logging_time': 4.8567054271698}
I0131 05:18:41.652623 139990499587840 logging_writer.py:48] [115318] accumulated_eval_time=4003.282853, accumulated_logging_time=4.856705, accumulated_submission_time=52141.492756, global_step=115318, preemption_count=0, score=52141.492756, test/accuracy=0.549500, test/loss=2.090232, test/num_examples=10000, total_duration=56155.447914, train/accuracy=0.735137, train/loss=1.188907, validation/accuracy=0.672360, validation/loss=1.465948, validation/num_examples=50000
I0131 05:19:14.814630 139990491195136 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.400209903717041, loss=2.8051397800445557
I0131 05:20:00.123188 139990499587840 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.009375810623169, loss=4.458002090454102
I0131 05:20:46.057033 139990491195136 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.4218332767486572, loss=2.8833322525024414
I0131 05:21:31.755459 139990499587840 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.2117514610290527, loss=3.250659227371216
I0131 05:22:17.567755 139990491195136 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.4743564128875732, loss=2.822934865951538
I0131 05:23:03.391832 139990499587840 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.157625675201416, loss=3.1431376934051514
I0131 05:23:48.849919 139990491195136 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.1180686950683594, loss=4.407543182373047
I0131 05:24:34.708402 139990499587840 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.052126169204712, loss=3.0076472759246826
I0131 05:25:20.606704 139990491195136 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.2085156440734863, loss=3.8890583515167236
I0131 05:25:41.987128 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:25:52.530656 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:26:12.658525 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:26:14.259586 140184451094336 submission_runner.py:408] Time since start: 56608.09s, 	Step: 116249, 	{'train/accuracy': 0.7199413776397705, 'train/loss': 1.2665797472000122, 'validation/accuracy': 0.67221999168396, 'validation/loss': 1.4920510053634644, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.1149799823760986, 'test/num_examples': 10000, 'score': 52561.77123785019, 'total_duration': 56608.08748936653, 'accumulated_submission_time': 52561.77123785019, 'accumulated_eval_time': 4035.5553166866302, 'accumulated_logging_time': 4.898540258407593}
I0131 05:26:14.295762 139990499587840 logging_writer.py:48] [116249] accumulated_eval_time=4035.555317, accumulated_logging_time=4.898540, accumulated_submission_time=52561.771238, global_step=116249, preemption_count=0, score=52561.771238, test/accuracy=0.545500, test/loss=2.114980, test/num_examples=10000, total_duration=56608.087489, train/accuracy=0.719941, train/loss=1.266580, validation/accuracy=0.672220, validation/loss=1.492051, validation/num_examples=50000
I0131 05:26:35.110203 139990491195136 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.307551622390747, loss=3.315216302871704
I0131 05:27:18.917330 139990499587840 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.086764335632324, loss=4.687112331390381
I0131 05:28:04.538233 139990491195136 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.2848668098449707, loss=2.8811192512512207
I0131 05:28:50.473101 139990499587840 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.251938819885254, loss=2.977341890335083
I0131 05:29:35.731880 139990491195136 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.8930293321609497, loss=3.8062944412231445
I0131 05:30:21.497545 139990499587840 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.491426944732666, loss=2.8970253467559814
I0131 05:31:07.149400 139990491195136 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.344815492630005, loss=4.543917655944824
I0131 05:31:52.924573 139990499587840 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.3363633155822754, loss=2.896928548812866
I0131 05:32:38.637797 139990491195136 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.8898638486862183, loss=4.086015701293945
I0131 05:33:14.382886 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:33:24.829870 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:33:47.524641 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:33:49.118865 140184451094336 submission_runner.py:408] Time since start: 57062.95s, 	Step: 117180, 	{'train/accuracy': 0.7294726371765137, 'train/loss': 1.2057080268859863, 'validation/accuracy': 0.677299976348877, 'validation/loss': 1.432081937789917, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.0657169818878174, 'test/num_examples': 10000, 'score': 52981.80159282684, 'total_duration': 57062.94676208496, 'accumulated_submission_time': 52981.80159282684, 'accumulated_eval_time': 4070.2912969589233, 'accumulated_logging_time': 4.944955587387085}
I0131 05:33:49.151924 139990499587840 logging_writer.py:48] [117180] accumulated_eval_time=4070.291297, accumulated_logging_time=4.944956, accumulated_submission_time=52981.801593, global_step=117180, preemption_count=0, score=52981.801593, test/accuracy=0.552700, test/loss=2.065717, test/num_examples=10000, total_duration=57062.946762, train/accuracy=0.729473, train/loss=1.205708, validation/accuracy=0.677300, validation/loss=1.432082, validation/num_examples=50000
I0131 05:33:57.553702 139990491195136 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.0457680225372314, loss=2.9100029468536377
I0131 05:34:39.486609 139990499587840 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.39281964302063, loss=2.896981954574585
I0131 05:35:24.917342 139990491195136 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.324490547180176, loss=2.8081846237182617
I0131 05:36:10.890538 139990499587840 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.221788167953491, loss=2.730210304260254
I0131 05:36:56.779137 139990491195136 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.2783381938934326, loss=2.7516064643859863
I0131 05:37:42.706410 139990499587840 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.002450704574585, loss=3.558727264404297
I0131 05:38:28.416790 139990491195136 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.5238425731658936, loss=4.727553367614746
I0131 05:39:14.178764 139990499587840 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.891422986984253, loss=4.078663349151611
I0131 05:39:59.992281 139990491195136 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.086939811706543, loss=3.9010472297668457
I0131 05:40:45.789745 139990499587840 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.0358126163482666, loss=4.898940086364746
I0131 05:40:49.277563 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:40:59.869068 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:41:22.644067 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:41:24.264456 140184451094336 submission_runner.py:408] Time since start: 57518.09s, 	Step: 118109, 	{'train/accuracy': 0.7354491949081421, 'train/loss': 1.1728439331054688, 'validation/accuracy': 0.6750800013542175, 'validation/loss': 1.43381667137146, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.0560216903686523, 'test/num_examples': 10000, 'score': 53401.867626428604, 'total_duration': 57518.092337846756, 'accumulated_submission_time': 53401.867626428604, 'accumulated_eval_time': 4105.278161764145, 'accumulated_logging_time': 4.990721702575684}
I0131 05:41:24.301780 139990491195136 logging_writer.py:48] [118109] accumulated_eval_time=4105.278162, accumulated_logging_time=4.990722, accumulated_submission_time=53401.867626, global_step=118109, preemption_count=0, score=53401.867626, test/accuracy=0.559200, test/loss=2.056022, test/num_examples=10000, total_duration=57518.092338, train/accuracy=0.735449, train/loss=1.172844, validation/accuracy=0.675080, validation/loss=1.433817, validation/num_examples=50000
I0131 05:42:01.741503 139990499587840 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.071349620819092, loss=4.893683433532715
I0131 05:42:47.348970 139990491195136 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.3370559215545654, loss=2.882046937942505
I0131 05:43:33.148791 139990499587840 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.9467122554779053, loss=4.4519124031066895
I0131 05:44:18.827271 139990491195136 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.0895633697509766, loss=4.209304332733154
I0131 05:45:04.464250 139990499587840 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.9598795175552368, loss=3.56105899810791
I0131 05:45:50.250049 139990491195136 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.0676560401916504, loss=3.7660789489746094
I0131 05:46:36.030741 139990499587840 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.4391562938690186, loss=2.860856294631958
I0131 05:47:22.012341 139990491195136 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.146059274673462, loss=4.0483245849609375
I0131 05:48:07.555038 139990499587840 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.1247427463531494, loss=4.673060417175293
I0131 05:48:24.563715 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:48:35.072441 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:48:55.615526 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:48:57.219580 140184451094336 submission_runner.py:408] Time since start: 57971.05s, 	Step: 119039, 	{'train/accuracy': 0.7400195002555847, 'train/loss': 1.14028000831604, 'validation/accuracy': 0.6732800006866455, 'validation/loss': 1.417976975440979, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.041276454925537, 'test/num_examples': 10000, 'score': 53822.07191777229, 'total_duration': 57971.04748129845, 'accumulated_submission_time': 53822.07191777229, 'accumulated_eval_time': 4137.934015035629, 'accumulated_logging_time': 5.03800368309021}
I0131 05:48:57.258353 139990491195136 logging_writer.py:48] [119039] accumulated_eval_time=4137.934015, accumulated_logging_time=5.038004, accumulated_submission_time=53822.071918, global_step=119039, preemption_count=0, score=53822.071918, test/accuracy=0.553500, test/loss=2.041276, test/num_examples=10000, total_duration=57971.047481, train/accuracy=0.740020, train/loss=1.140280, validation/accuracy=0.673280, validation/loss=1.417977, validation/num_examples=50000
I0131 05:49:22.055099 139990499587840 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.3565878868103027, loss=2.8110694885253906
I0131 05:50:06.513191 139990491195136 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.344080924987793, loss=2.7882254123687744
I0131 05:50:52.417578 139990499587840 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.5592520236968994, loss=4.566048622131348
I0131 05:51:38.117413 139990491195136 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.2670953273773193, loss=2.8228538036346436
I0131 05:52:23.749394 139990499587840 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.2359330654144287, loss=2.820580005645752
I0131 05:53:09.526742 139990491195136 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.234274387359619, loss=3.1052138805389404
I0131 05:53:55.096665 139990499587840 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.244178295135498, loss=3.126147747039795
I0131 05:54:40.567730 139990491195136 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.9594262838363647, loss=4.625757217407227
I0131 05:55:26.418070 139990499587840 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.2565183639526367, loss=4.966783046722412
I0131 05:55:57.384434 140184451094336 spec.py:321] Evaluating on the training split.
I0131 05:56:07.938134 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 05:56:29.718490 140184451094336 spec.py:349] Evaluating on the test split.
I0131 05:56:31.319175 140184451094336 submission_runner.py:408] Time since start: 58425.15s, 	Step: 119970, 	{'train/accuracy': 0.7264648079872131, 'train/loss': 1.2079328298568726, 'validation/accuracy': 0.6744799613952637, 'validation/loss': 1.4414364099502563, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.0710713863372803, 'test/num_examples': 10000, 'score': 54242.142776966095, 'total_duration': 58425.14707708359, 'accumulated_submission_time': 54242.142776966095, 'accumulated_eval_time': 4171.868766546249, 'accumulated_logging_time': 5.085484981536865}
I0131 05:56:31.356177 139990491195136 logging_writer.py:48] [119970] accumulated_eval_time=4171.868767, accumulated_logging_time=5.085485, accumulated_submission_time=54242.142777, global_step=119970, preemption_count=0, score=54242.142777, test/accuracy=0.552300, test/loss=2.071071, test/num_examples=10000, total_duration=58425.147077, train/accuracy=0.726465, train/loss=1.207933, validation/accuracy=0.674480, validation/loss=1.441436, validation/num_examples=50000
I0131 05:56:43.767663 139990499587840 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.4597208499908447, loss=2.9285361766815186
I0131 05:57:26.246650 139990491195136 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.2116758823394775, loss=4.748322010040283
I0131 05:58:11.934377 139990499587840 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.320181131362915, loss=2.9759738445281982
I0131 05:58:57.507850 139990491195136 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.091433525085449, loss=4.266979694366455
I0131 05:59:43.139813 139990499587840 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.538541793823242, loss=2.7990565299987793
I0131 06:00:28.719771 139990491195136 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.3323776721954346, loss=2.8250789642333984
I0131 06:01:14.365585 139990499587840 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.1841559410095215, loss=4.360177993774414
I0131 06:02:00.024542 139990491195136 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.35764479637146, loss=3.416903018951416
I0131 06:02:45.727676 139990499587840 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.3993663787841797, loss=4.614931583404541
I0131 06:03:31.377888 139990491195136 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.3457236289978027, loss=2.8785042762756348
I0131 06:03:31.393481 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:03:41.853493 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:04:03.336626 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:04:04.939800 140184451094336 submission_runner.py:408] Time since start: 58878.77s, 	Step: 120901, 	{'train/accuracy': 0.7350195050239563, 'train/loss': 1.1525589227676392, 'validation/accuracy': 0.6802799701690674, 'validation/loss': 1.4023057222366333, 'validation/num_examples': 50000, 'test/accuracy': 0.5651000142097473, 'test/loss': 2.001354694366455, 'test/num_examples': 10000, 'score': 54662.123398303986, 'total_duration': 58878.76770377159, 'accumulated_submission_time': 54662.123398303986, 'accumulated_eval_time': 4205.4150903224945, 'accumulated_logging_time': 5.1322691440582275}
I0131 06:04:04.972764 139990499587840 logging_writer.py:48] [120901] accumulated_eval_time=4205.415090, accumulated_logging_time=5.132269, accumulated_submission_time=54662.123398, global_step=120901, preemption_count=0, score=54662.123398, test/accuracy=0.565100, test/loss=2.001355, test/num_examples=10000, total_duration=58878.767704, train/accuracy=0.735020, train/loss=1.152559, validation/accuracy=0.680280, validation/loss=1.402306, validation/num_examples=50000
I0131 06:04:45.637628 139990491195136 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.558567523956299, loss=2.7613468170166016
I0131 06:05:31.109880 139990499587840 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.3842859268188477, loss=3.21671986579895
I0131 06:06:17.342058 139990491195136 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.3417067527770996, loss=3.695883274078369
I0131 06:07:03.084969 139990499587840 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.1171140670776367, loss=2.957977294921875
I0131 06:07:48.917205 139990491195136 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.28359055519104, loss=3.053637981414795
I0131 06:08:34.854756 139990499587840 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.346201181411743, loss=3.116056203842163
I0131 06:09:20.805936 139990491195136 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.04612398147583, loss=4.297536849975586
I0131 06:10:06.413235 139990499587840 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.049302339553833, loss=3.8959221839904785
I0131 06:10:51.976315 139990491195136 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.4392528533935547, loss=2.9204437732696533
I0131 06:11:05.067705 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:11:15.506796 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:11:35.997207 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:11:37.597447 140184451094336 submission_runner.py:408] Time since start: 59331.43s, 	Step: 121830, 	{'train/accuracy': 0.7547070384025574, 'train/loss': 1.0838943719863892, 'validation/accuracy': 0.683459997177124, 'validation/loss': 1.395894169807434, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 1.9966557025909424, 'test/num_examples': 10000, 'score': 55082.16000986099, 'total_duration': 59331.42533326149, 'accumulated_submission_time': 55082.16000986099, 'accumulated_eval_time': 4237.944813966751, 'accumulated_logging_time': 5.175921440124512}
I0131 06:11:37.638129 139990499587840 logging_writer.py:48] [121830] accumulated_eval_time=4237.944814, accumulated_logging_time=5.175921, accumulated_submission_time=55082.160010, global_step=121830, preemption_count=0, score=55082.160010, test/accuracy=0.562800, test/loss=1.996656, test/num_examples=10000, total_duration=59331.425333, train/accuracy=0.754707, train/loss=1.083894, validation/accuracy=0.683460, validation/loss=1.395894, validation/num_examples=50000
I0131 06:12:06.106333 139990491195136 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.140803337097168, loss=2.9981765747070312
I0131 06:12:51.835003 139990499587840 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.3731048107147217, loss=2.8778066635131836
I0131 06:13:37.783651 139990491195136 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.3622119426727295, loss=4.476782321929932
I0131 06:14:23.713495 139990499587840 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.4994049072265625, loss=2.7381792068481445
I0131 06:15:09.566370 139990491195136 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.2892580032348633, loss=2.8128206729888916
I0131 06:15:55.206542 139990499587840 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.498929023742676, loss=2.767042398452759
I0131 06:16:41.029455 139990491195136 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.180008888244629, loss=4.8685712814331055
I0131 06:17:26.997324 139990499587840 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.2036049365997314, loss=3.5269806385040283
I0131 06:18:12.677032 139990491195136 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.3715271949768066, loss=3.5996365547180176
I0131 06:18:37.894525 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:18:48.311480 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:19:09.850806 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:19:11.452918 140184451094336 submission_runner.py:408] Time since start: 59785.28s, 	Step: 122757, 	{'train/accuracy': 0.7364257574081421, 'train/loss': 1.1707990169525146, 'validation/accuracy': 0.6805199980735779, 'validation/loss': 1.4080075025558472, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.036602735519409, 'test/num_examples': 10000, 'score': 55502.36118531227, 'total_duration': 59785.28081989288, 'accumulated_submission_time': 55502.36118531227, 'accumulated_eval_time': 4271.503207921982, 'accumulated_logging_time': 5.226181507110596}
I0131 06:19:11.486140 139990499587840 logging_writer.py:48] [122757] accumulated_eval_time=4271.503208, accumulated_logging_time=5.226182, accumulated_submission_time=55502.361185, global_step=122757, preemption_count=0, score=55502.361185, test/accuracy=0.554100, test/loss=2.036603, test/num_examples=10000, total_duration=59785.280820, train/accuracy=0.736426, train/loss=1.170799, validation/accuracy=0.680520, validation/loss=1.408008, validation/num_examples=50000
I0131 06:19:29.089649 139990491195136 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.4744038581848145, loss=4.884120941162109
I0131 06:20:12.209950 139990499587840 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.3778843879699707, loss=4.264832973480225
I0131 06:20:58.131120 139990491195136 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.2059028148651123, loss=4.844738960266113
I0131 06:21:44.342022 139990499587840 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.39479923248291, loss=2.829848051071167
I0131 06:22:30.031351 139990491195136 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.18900728225708, loss=3.0259816646575928
I0131 06:23:15.938443 139990499587840 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.5141823291778564, loss=2.8170974254608154
I0131 06:24:01.679848 139990491195136 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.380857229232788, loss=3.9966514110565186
I0131 06:24:47.626402 139990499587840 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.4573147296905518, loss=3.6317200660705566
I0131 06:25:33.334506 139990491195136 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.5264999866485596, loss=2.7092599868774414
I0131 06:26:11.699516 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:26:22.064703 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:26:44.802534 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:26:46.403858 140184451094336 submission_runner.py:408] Time since start: 60240.23s, 	Step: 123685, 	{'train/accuracy': 0.7400780916213989, 'train/loss': 1.1385509967803955, 'validation/accuracy': 0.6856399774551392, 'validation/loss': 1.3829452991485596, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 2.0096795558929443, 'test/num_examples': 10000, 'score': 55922.51798272133, 'total_duration': 60240.23173499107, 'accumulated_submission_time': 55922.51798272133, 'accumulated_eval_time': 4306.2075407505035, 'accumulated_logging_time': 5.269519805908203}
I0131 06:26:46.439729 139990499587840 logging_writer.py:48] [123685] accumulated_eval_time=4306.207541, accumulated_logging_time=5.269520, accumulated_submission_time=55922.517983, global_step=123685, preemption_count=0, score=55922.517983, test/accuracy=0.564200, test/loss=2.009680, test/num_examples=10000, total_duration=60240.231735, train/accuracy=0.740078, train/loss=1.138551, validation/accuracy=0.685640, validation/loss=1.382945, validation/num_examples=50000
I0131 06:26:52.840201 139990491195136 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.485626459121704, loss=4.7449541091918945
I0131 06:27:34.400997 139990499587840 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.3629395961761475, loss=2.9132843017578125
I0131 06:28:20.218603 139990491195136 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.598947525024414, loss=3.30808687210083
I0131 06:29:06.253594 139990499587840 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.417259931564331, loss=2.754476547241211
I0131 06:29:51.898583 139990491195136 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.513152837753296, loss=4.922818183898926
I0131 06:30:37.747575 139990499587840 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.579167127609253, loss=2.7115209102630615
I0131 06:31:23.565997 139990491195136 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.3645262718200684, loss=4.932870864868164
I0131 06:32:09.576916 139990499587840 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.346248149871826, loss=2.917180299758911
I0131 06:32:55.322044 139990491195136 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.159536123275757, loss=3.557997226715088
I0131 06:33:41.067779 139990499587840 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.424490451812744, loss=2.7928378582000732
I0131 06:33:46.657793 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:33:57.009718 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:34:16.993152 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:34:18.591701 140184451094336 submission_runner.py:408] Time since start: 60692.42s, 	Step: 124614, 	{'train/accuracy': 0.7510937452316284, 'train/loss': 1.11006760597229, 'validation/accuracy': 0.6840999722480774, 'validation/loss': 1.3941822052001953, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 2.01263165473938, 'test/num_examples': 10000, 'score': 56342.67802453041, 'total_duration': 60692.419605493546, 'accumulated_submission_time': 56342.67802453041, 'accumulated_eval_time': 4338.141446352005, 'accumulated_logging_time': 5.316270351409912}
I0131 06:34:18.628049 139990491195136 logging_writer.py:48] [124614] accumulated_eval_time=4338.141446, accumulated_logging_time=5.316270, accumulated_submission_time=56342.678025, global_step=124614, preemption_count=0, score=56342.678025, test/accuracy=0.567800, test/loss=2.012632, test/num_examples=10000, total_duration=60692.419605, train/accuracy=0.751094, train/loss=1.110068, validation/accuracy=0.684100, validation/loss=1.394182, validation/num_examples=50000
I0131 06:34:53.881986 139990499587840 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.2247307300567627, loss=3.7352957725524902
I0131 06:35:39.270795 139990491195136 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.2899527549743652, loss=4.0662126541137695
I0131 06:36:25.276963 139990499587840 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.681091070175171, loss=4.894537925720215
I0131 06:37:11.058994 139990491195136 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.3049631118774414, loss=3.854825735092163
I0131 06:37:56.762975 139990499587840 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.6256985664367676, loss=4.457002639770508
I0131 06:38:42.631378 139990491195136 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.1807057857513428, loss=3.386251926422119
I0131 06:39:28.350057 139990499587840 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.2869858741760254, loss=2.7202723026275635
I0131 06:40:14.019752 139990491195136 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.331294059753418, loss=3.016249895095825
I0131 06:40:59.540195 139990499587840 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.4631524085998535, loss=3.619295358657837
I0131 06:41:18.900089 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:41:29.261896 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:41:50.674286 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:41:52.284498 140184451094336 submission_runner.py:408] Time since start: 61146.11s, 	Step: 125544, 	{'train/accuracy': 0.7424218654632568, 'train/loss': 1.1373136043548584, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.3734878301620483, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.991938591003418, 'test/num_examples': 10000, 'score': 56762.89463853836, 'total_duration': 61146.11239314079, 'accumulated_submission_time': 56762.89463853836, 'accumulated_eval_time': 4371.525834798813, 'accumulated_logging_time': 5.361638784408569}
I0131 06:41:52.319579 139990491195136 logging_writer.py:48] [125544] accumulated_eval_time=4371.525835, accumulated_logging_time=5.361639, accumulated_submission_time=56762.894639, global_step=125544, preemption_count=0, score=56762.894639, test/accuracy=0.566100, test/loss=1.991939, test/num_examples=10000, total_duration=61146.112393, train/accuracy=0.742422, train/loss=1.137314, validation/accuracy=0.687380, validation/loss=1.373488, validation/num_examples=50000
I0131 06:42:15.144819 139990499587840 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.668445110321045, loss=2.713121175765991
I0131 06:42:59.133637 139990491195136 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.5178685188293457, loss=2.647658109664917
I0131 06:43:45.219688 139990499587840 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.223860502243042, loss=3.643332004547119
I0131 06:44:31.037108 139990491195136 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.4943654537200928, loss=2.820779800415039
I0131 06:45:16.991619 139990499587840 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.521000862121582, loss=2.7265841960906982
I0131 06:46:02.867889 139990491195136 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.4744679927825928, loss=2.57832932472229
I0131 06:46:48.603420 139990499587840 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.291398525238037, loss=3.0854532718658447
I0131 06:47:34.445841 139990491195136 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.7294154167175293, loss=2.710709571838379
I0131 06:48:20.526387 139990499587840 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.5881259441375732, loss=2.7143218517303467
I0131 06:48:52.668266 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:49:03.311216 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:49:24.190834 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:49:25.799377 140184451094336 submission_runner.py:408] Time since start: 61599.63s, 	Step: 126472, 	{'train/accuracy': 0.74867182970047, 'train/loss': 1.1079342365264893, 'validation/accuracy': 0.6914599537849426, 'validation/loss': 1.3647878170013428, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 1.9838557243347168, 'test/num_examples': 10000, 'score': 57183.18724656105, 'total_duration': 61599.6272623539, 'accumulated_submission_time': 57183.18724656105, 'accumulated_eval_time': 4404.65695309639, 'accumulated_logging_time': 5.406764268875122}
I0131 06:49:25.833329 139990491195136 logging_writer.py:48] [126472] accumulated_eval_time=4404.656953, accumulated_logging_time=5.406764, accumulated_submission_time=57183.187247, global_step=126472, preemption_count=0, score=57183.187247, test/accuracy=0.569200, test/loss=1.983856, test/num_examples=10000, total_duration=61599.627262, train/accuracy=0.748672, train/loss=1.107934, validation/accuracy=0.691460, validation/loss=1.364788, validation/num_examples=50000
I0131 06:49:37.434879 139990499587840 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.482060194015503, loss=2.668959379196167
I0131 06:50:19.896941 139990491195136 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.809217929840088, loss=2.8271284103393555
I0131 06:51:05.796542 139990499587840 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.3326685428619385, loss=3.198868989944458
I0131 06:51:51.615756 139990491195136 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.578019618988037, loss=2.7375760078430176
I0131 06:52:37.581806 139990499587840 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.573868751525879, loss=2.671180248260498
I0131 06:53:23.005638 139990491195136 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.6041147708892822, loss=2.9187965393066406
I0131 06:54:08.853520 139990499587840 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.4286203384399414, loss=2.8569107055664062
I0131 06:54:54.681684 139990491195136 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.262765407562256, loss=3.9859201908111572
I0131 06:55:39.975735 139990499587840 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.2138640880584717, loss=3.2639782428741455
I0131 06:56:25.669760 139990491195136 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.410292148590088, loss=2.7073512077331543
I0131 06:56:25.810971 140184451094336 spec.py:321] Evaluating on the training split.
I0131 06:56:36.238080 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 06:56:58.028986 140184451094336 spec.py:349] Evaluating on the test split.
I0131 06:56:59.647469 140184451094336 submission_runner.py:408] Time since start: 62053.48s, 	Step: 127402, 	{'train/accuracy': 0.7528125047683716, 'train/loss': 1.0917577743530273, 'validation/accuracy': 0.6893599629402161, 'validation/loss': 1.3566315174102783, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 1.9800525903701782, 'test/num_examples': 10000, 'score': 57603.10753917694, 'total_duration': 62053.47536659241, 'accumulated_submission_time': 57603.10753917694, 'accumulated_eval_time': 4438.493448495865, 'accumulated_logging_time': 5.450902700424194}
I0131 06:56:59.682267 139990499587840 logging_writer.py:48] [127402] accumulated_eval_time=4438.493448, accumulated_logging_time=5.450903, accumulated_submission_time=57603.107539, global_step=127402, preemption_count=0, score=57603.107539, test/accuracy=0.567900, test/loss=1.980053, test/num_examples=10000, total_duration=62053.475367, train/accuracy=0.752813, train/loss=1.091758, validation/accuracy=0.689360, validation/loss=1.356632, validation/num_examples=50000
I0131 06:57:40.246393 139990491195136 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.549312114715576, loss=2.7756288051605225
I0131 06:58:26.174599 139990499587840 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.709747791290283, loss=2.857372760772705
I0131 06:59:26.473856 139990491195136 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.5612590312957764, loss=2.6637468338012695
I0131 07:00:11.794836 139990499587840 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.6110172271728516, loss=2.9247257709503174
I0131 07:00:57.584788 139990491195136 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.8805415630340576, loss=4.481154441833496
I0131 07:01:43.456775 139990499587840 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.1908810138702393, loss=2.703819751739502
I0131 07:02:29.185803 139990491195136 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.947997570037842, loss=2.685547113418579
I0131 07:03:14.877974 139990499587840 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.560009241104126, loss=3.0407533645629883
I0131 07:04:00.375881 139990491195136 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.4536595344543457, loss=4.0192437171936035
I0131 07:04:00.390284 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:04:10.794739 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:04:32.749798 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:04:34.346293 140184451094336 submission_runner.py:408] Time since start: 62508.17s, 	Step: 128301, 	{'train/accuracy': 0.7537695169448853, 'train/loss': 1.098175287246704, 'validation/accuracy': 0.6912999749183655, 'validation/loss': 1.3770054578781128, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.9985442161560059, 'test/num_examples': 10000, 'score': 58023.76083302498, 'total_duration': 62508.17419576645, 'accumulated_submission_time': 58023.76083302498, 'accumulated_eval_time': 4472.449482917786, 'accumulated_logging_time': 5.4948413372039795}
I0131 07:04:34.382074 139990499587840 logging_writer.py:48] [128301] accumulated_eval_time=4472.449483, accumulated_logging_time=5.494841, accumulated_submission_time=58023.760833, global_step=128301, preemption_count=0, score=58023.760833, test/accuracy=0.569600, test/loss=1.998544, test/num_examples=10000, total_duration=62508.174196, train/accuracy=0.753770, train/loss=1.098175, validation/accuracy=0.691300, validation/loss=1.377005, validation/num_examples=50000
I0131 07:05:15.200695 139990491195136 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.685502529144287, loss=4.80037784576416
I0131 07:06:00.683347 139990499587840 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.335813522338867, loss=3.6793675422668457
I0131 07:06:46.624876 139990491195136 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.5404200553894043, loss=4.707512855529785
I0131 07:07:32.428356 139990499587840 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.42685604095459, loss=2.8536360263824463
I0131 07:08:18.155188 139990491195136 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.6117255687713623, loss=2.869394540786743
I0131 07:09:04.084697 139990499587840 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.7172529697418213, loss=2.7269272804260254
I0131 07:09:49.574233 139990491195136 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.398735761642456, loss=3.0314812660217285
I0131 07:10:35.359940 139990499587840 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.281437873840332, loss=3.064271926879883
I0131 07:11:20.958283 139990491195136 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.630884885787964, loss=2.67526912689209
I0131 07:11:34.372559 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:11:44.865426 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:12:06.669348 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:12:08.279838 140184451094336 submission_runner.py:408] Time since start: 62962.11s, 	Step: 129231, 	{'train/accuracy': 0.7473828196525574, 'train/loss': 1.1209533214569092, 'validation/accuracy': 0.6952799558639526, 'validation/loss': 1.3586920499801636, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.974564552307129, 'test/num_examples': 10000, 'score': 58443.69563674927, 'total_duration': 62962.107741594315, 'accumulated_submission_time': 58443.69563674927, 'accumulated_eval_time': 4506.356766462326, 'accumulated_logging_time': 5.540136098861694}
I0131 07:12:08.316687 139990499587840 logging_writer.py:48] [129231] accumulated_eval_time=4506.356766, accumulated_logging_time=5.540136, accumulated_submission_time=58443.695637, global_step=129231, preemption_count=0, score=58443.695637, test/accuracy=0.570900, test/loss=1.974565, test/num_examples=10000, total_duration=62962.107742, train/accuracy=0.747383, train/loss=1.120953, validation/accuracy=0.695280, validation/loss=1.358692, validation/num_examples=50000
I0131 07:12:36.314372 139990491195136 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.786356210708618, loss=2.707200765609741
I0131 07:13:21.206423 139990499587840 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.9626832008361816, loss=4.775599479675293
I0131 07:14:07.221961 139990491195136 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.469357490539551, loss=4.526825428009033
I0131 07:14:52.746275 139990499587840 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.5817923545837402, loss=2.7619175910949707
I0131 07:15:38.322461 139990491195136 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.6009163856506348, loss=2.7563982009887695
I0131 07:16:23.883167 139990499587840 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.383403778076172, loss=3.521580696105957
I0131 07:17:09.622766 139990491195136 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.707531690597534, loss=2.7545979022979736
I0131 07:17:55.212866 139990499587840 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.6156420707702637, loss=2.8905029296875
I0131 07:18:41.018521 139990491195136 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.84542179107666, loss=2.661357879638672
I0131 07:19:08.458051 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:19:18.618825 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:19:38.904900 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:19:40.514074 140184451094336 submission_runner.py:408] Time since start: 63414.34s, 	Step: 130161, 	{'train/accuracy': 0.7574023008346558, 'train/loss': 1.075601577758789, 'validation/accuracy': 0.6932399868965149, 'validation/loss': 1.3420850038528442, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 1.9617393016815186, 'test/num_examples': 10000, 'score': 58863.781465768814, 'total_duration': 63414.34195232391, 'accumulated_submission_time': 58863.781465768814, 'accumulated_eval_time': 4538.41277050972, 'accumulated_logging_time': 5.585582971572876}
I0131 07:19:40.555562 139990499587840 logging_writer.py:48] [130161] accumulated_eval_time=4538.412771, accumulated_logging_time=5.585583, accumulated_submission_time=58863.781466, global_step=130161, preemption_count=0, score=58863.781466, test/accuracy=0.575300, test/loss=1.961739, test/num_examples=10000, total_duration=63414.341952, train/accuracy=0.757402, train/loss=1.075602, validation/accuracy=0.693240, validation/loss=1.342085, validation/num_examples=50000
I0131 07:19:56.579576 139990491195136 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.8797738552093506, loss=2.768717050552368
I0131 07:20:40.184208 139990499587840 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.2950265407562256, loss=3.6147375106811523
I0131 07:21:26.123646 139990491195136 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.5676321983337402, loss=3.338744878768921
I0131 07:22:12.211553 139990499587840 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.6369030475616455, loss=3.024506092071533
I0131 07:22:57.933602 139990491195136 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.690711259841919, loss=2.8547863960266113
I0131 07:23:43.750807 139990499587840 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.7145087718963623, loss=4.150346279144287
I0131 07:24:29.432012 139990491195136 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.443864107131958, loss=3.242947578430176
I0131 07:25:15.268752 139990499587840 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.9791600704193115, loss=2.853961944580078
I0131 07:26:01.020343 139990491195136 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.758364677429199, loss=2.763113021850586
I0131 07:26:40.560449 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:26:51.068761 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:27:12.982939 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:27:14.580123 140184451094336 submission_runner.py:408] Time since start: 63868.41s, 	Step: 131088, 	{'train/accuracy': 0.7674218416213989, 'train/loss': 1.0314123630523682, 'validation/accuracy': 0.6955599784851074, 'validation/loss': 1.343638300895691, 'validation/num_examples': 50000, 'test/accuracy': 0.5745000243186951, 'test/loss': 1.9722157716751099, 'test/num_examples': 10000, 'score': 59283.73093700409, 'total_duration': 63868.40799832344, 'accumulated_submission_time': 59283.73093700409, 'accumulated_eval_time': 4572.432414054871, 'accumulated_logging_time': 5.636627674102783}
I0131 07:27:14.621254 139990499587840 logging_writer.py:48] [131088] accumulated_eval_time=4572.432414, accumulated_logging_time=5.636628, accumulated_submission_time=59283.730937, global_step=131088, preemption_count=0, score=59283.730937, test/accuracy=0.574500, test/loss=1.972216, test/num_examples=10000, total_duration=63868.407998, train/accuracy=0.767422, train/loss=1.031412, validation/accuracy=0.695560, validation/loss=1.343638, validation/num_examples=50000
I0131 07:27:19.812414 139990491195136 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.7213134765625, loss=2.755051612854004
I0131 07:28:01.576627 139990499587840 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.851166248321533, loss=4.880421161651611
I0131 07:28:47.094954 139990491195136 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.5890448093414307, loss=3.3157997131347656
I0131 07:29:33.435680 139990499587840 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.929792642593384, loss=4.161230564117432
I0131 07:30:18.994477 139990491195136 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.4558451175689697, loss=4.567159175872803
I0131 07:31:04.937390 139990499587840 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.812262535095215, loss=2.702805519104004
I0131 07:31:50.855031 139990491195136 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.682526111602783, loss=2.7522153854370117
I0131 07:32:36.532574 139990499587840 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.16121506690979, loss=4.7389631271362305
I0131 07:33:22.480875 139990491195136 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.545750617980957, loss=4.682590484619141
I0131 07:34:08.051133 139990499587840 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.6982951164245605, loss=3.667649507522583
I0131 07:34:14.996646 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:34:25.130022 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:34:46.861819 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:34:48.452264 140184451094336 submission_runner.py:408] Time since start: 64322.28s, 	Step: 132017, 	{'train/accuracy': 0.7551367282867432, 'train/loss': 1.0511291027069092, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.3008636236190796, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 1.91862154006958, 'test/num_examples': 10000, 'score': 59704.04973602295, 'total_duration': 64322.280165433884, 'accumulated_submission_time': 59704.04973602295, 'accumulated_eval_time': 4605.888027429581, 'accumulated_logging_time': 5.6878721714019775}
I0131 07:34:48.489320 139990491195136 logging_writer.py:48] [132017] accumulated_eval_time=4605.888027, accumulated_logging_time=5.687872, accumulated_submission_time=59704.049736, global_step=132017, preemption_count=0, score=59704.049736, test/accuracy=0.575900, test/loss=1.918622, test/num_examples=10000, total_duration=64322.280165, train/accuracy=0.755137, train/loss=1.051129, validation/accuracy=0.698000, validation/loss=1.300864, validation/num_examples=50000
I0131 07:35:22.076239 139990499587840 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.543968439102173, loss=4.045943737030029
I0131 07:36:07.421838 139990491195136 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.6156370639801025, loss=2.7962565422058105
I0131 07:36:53.102442 139990499587840 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.6895716190338135, loss=2.874217987060547
I0131 07:37:39.068336 139990491195136 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.4497523307800293, loss=3.3799569606781006
I0131 07:38:24.862848 139990499587840 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.8365581035614014, loss=2.750169038772583
I0131 07:39:10.559527 139990491195136 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.6750519275665283, loss=2.855105400085449
I0131 07:39:56.554797 139990499587840 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.651517629623413, loss=3.3178963661193848
I0131 07:40:42.342959 139990491195136 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.606200933456421, loss=2.9261269569396973
I0131 07:41:28.208002 139990499587840 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.5774424076080322, loss=2.5213096141815186
I0131 07:41:48.551302 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:41:59.063964 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:42:21.107136 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:42:22.706454 140184451094336 submission_runner.py:408] Time since start: 64776.53s, 	Step: 132946, 	{'train/accuracy': 0.7594921588897705, 'train/loss': 1.0723209381103516, 'validation/accuracy': 0.6966399550437927, 'validation/loss': 1.3334400653839111, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.952175498008728, 'test/num_examples': 10000, 'score': 60124.05582642555, 'total_duration': 64776.534338235855, 'accumulated_submission_time': 60124.05582642555, 'accumulated_eval_time': 4640.043148756027, 'accumulated_logging_time': 5.734477758407593}
I0131 07:42:22.741025 139990491195136 logging_writer.py:48] [132946] accumulated_eval_time=4640.043149, accumulated_logging_time=5.734478, accumulated_submission_time=60124.055826, global_step=132946, preemption_count=0, score=60124.055826, test/accuracy=0.576900, test/loss=1.952175, test/num_examples=10000, total_duration=64776.534338, train/accuracy=0.759492, train/loss=1.072321, validation/accuracy=0.696640, validation/loss=1.333440, validation/num_examples=50000
I0131 07:42:44.726599 139990499587840 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.4158871173858643, loss=2.7834646701812744
I0131 07:43:28.581004 139990491195136 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.570390462875366, loss=3.6398911476135254
I0131 07:44:14.804161 139990499587840 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.4181408882141113, loss=3.1623830795288086
I0131 07:45:00.394320 139990491195136 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.737044334411621, loss=3.343862771987915
I0131 07:45:46.195573 139990499587840 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.720874309539795, loss=2.6936678886413574
I0131 07:46:32.011664 139990491195136 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.752370595932007, loss=2.592468738555908
I0131 07:47:17.587060 139990499587840 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.7983500957489014, loss=2.8995440006256104
I0131 07:48:03.313065 139990491195136 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.9280872344970703, loss=2.81510066986084
I0131 07:48:48.706763 139990499587840 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.7060916423797607, loss=2.648286819458008
I0131 07:49:22.832495 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:49:33.311475 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:49:54.703661 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:49:56.308858 140184451094336 submission_runner.py:408] Time since start: 65230.14s, 	Step: 133876, 	{'train/accuracy': 0.7744726538658142, 'train/loss': 0.9877074360847473, 'validation/accuracy': 0.7017599940299988, 'validation/loss': 1.2961947917938232, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.904213547706604, 'test/num_examples': 10000, 'score': 60544.09117388725, 'total_duration': 65230.13675904274, 'accumulated_submission_time': 60544.09117388725, 'accumulated_eval_time': 4673.519508600235, 'accumulated_logging_time': 5.778345823287964}
I0131 07:49:56.347909 139990491195136 logging_writer.py:48] [133876] accumulated_eval_time=4673.519509, accumulated_logging_time=5.778346, accumulated_submission_time=60544.091174, global_step=133876, preemption_count=0, score=60544.091174, test/accuracy=0.581600, test/loss=1.904214, test/num_examples=10000, total_duration=65230.136759, train/accuracy=0.774473, train/loss=0.987707, validation/accuracy=0.701760, validation/loss=1.296195, validation/num_examples=50000
I0131 07:50:06.360205 139990499587840 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.906268835067749, loss=2.6169610023498535
I0131 07:50:48.687814 139990491195136 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.8368489742279053, loss=3.4823129177093506
I0131 07:51:34.471375 139990499587840 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.441978931427002, loss=3.1934139728546143
I0131 07:52:20.258553 139990491195136 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.9249584674835205, loss=2.6087536811828613
I0131 07:53:05.860707 139990499587840 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.526754140853882, loss=3.879938840866089
I0131 07:53:51.677397 139990491195136 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.764775276184082, loss=4.30708122253418
I0131 07:54:37.436627 139990499587840 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.6946098804473877, loss=4.161077499389648
I0131 07:55:22.933451 139990491195136 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.009566307067871, loss=4.524112701416016
I0131 07:56:08.644711 139990499587840 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.064210891723633, loss=4.681292533874512
I0131 07:56:54.239534 139990491195136 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.846318483352661, loss=2.6117568016052246
I0131 07:56:56.656674 140184451094336 spec.py:321] Evaluating on the training split.
I0131 07:57:07.177118 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 07:57:29.736952 140184451094336 spec.py:349] Evaluating on the test split.
I0131 07:57:31.327896 140184451094336 submission_runner.py:408] Time since start: 65685.16s, 	Step: 134807, 	{'train/accuracy': 0.7617382407188416, 'train/loss': 1.0603655576705933, 'validation/accuracy': 0.701200008392334, 'validation/loss': 1.3200796842575073, 'validation/num_examples': 50000, 'test/accuracy': 0.5774000287055969, 'test/loss': 1.9307143688201904, 'test/num_examples': 10000, 'score': 60964.34392094612, 'total_duration': 65685.15578842163, 'accumulated_submission_time': 60964.34392094612, 'accumulated_eval_time': 4708.190718412399, 'accumulated_logging_time': 5.826894760131836}
I0131 07:57:31.365016 139990499587840 logging_writer.py:48] [134807] accumulated_eval_time=4708.190718, accumulated_logging_time=5.826895, accumulated_submission_time=60964.343921, global_step=134807, preemption_count=0, score=60964.343921, test/accuracy=0.577400, test/loss=1.930714, test/num_examples=10000, total_duration=65685.155788, train/accuracy=0.761738, train/loss=1.060366, validation/accuracy=0.701200, validation/loss=1.320080, validation/num_examples=50000
I0131 07:58:09.510149 139990491195136 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.651686429977417, loss=2.9615774154663086
I0131 07:58:55.078455 139990499587840 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.7386159896850586, loss=2.531733989715576
I0131 07:59:41.008966 139990491195136 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.8162031173706055, loss=2.699483633041382
I0131 08:00:27.036422 139990499587840 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.766274929046631, loss=2.989126205444336
I0131 08:01:12.848152 139990491195136 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.339118242263794, loss=2.618738889694214
I0131 08:01:58.783307 139990499587840 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.7260639667510986, loss=3.9356772899627686
I0131 08:02:44.663824 139990491195136 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.8011927604675293, loss=2.962529182434082
I0131 08:03:30.480483 139990499587840 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.0667202472686768, loss=2.6530377864837646
I0131 08:04:16.293180 139990491195136 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.00087833404541, loss=3.295341730117798
I0131 08:04:31.416751 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:04:41.786211 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:05:00.874569 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:05:02.489671 140184451094336 submission_runner.py:408] Time since start: 66136.32s, 	Step: 135735, 	{'train/accuracy': 0.768847644329071, 'train/loss': 1.01193368434906, 'validation/accuracy': 0.7056999802589417, 'validation/loss': 1.2756710052490234, 'validation/num_examples': 50000, 'test/accuracy': 0.5831000208854675, 'test/loss': 1.8825321197509766, 'test/num_examples': 10000, 'score': 61384.338955163956, 'total_duration': 66136.31755590439, 'accumulated_submission_time': 61384.338955163956, 'accumulated_eval_time': 4739.263605117798, 'accumulated_logging_time': 5.874547243118286}
I0131 08:05:02.535006 139990499587840 logging_writer.py:48] [135735] accumulated_eval_time=4739.263605, accumulated_logging_time=5.874547, accumulated_submission_time=61384.338955, global_step=135735, preemption_count=0, score=61384.338955, test/accuracy=0.583100, test/loss=1.882532, test/num_examples=10000, total_duration=66136.317556, train/accuracy=0.768848, train/loss=1.011934, validation/accuracy=0.705700, validation/loss=1.275671, validation/num_examples=50000
I0131 08:05:28.939343 139990491195136 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.836346387863159, loss=2.703693389892578
I0131 08:06:14.164576 139990499587840 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.7398250102996826, loss=2.600602865219116
I0131 08:06:59.919875 139990491195136 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.8506908416748047, loss=2.8897156715393066
I0131 08:07:45.793542 139990499587840 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.6303250789642334, loss=3.0857982635498047
I0131 08:08:31.599422 139990491195136 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.843294620513916, loss=4.175022602081299
I0131 08:09:17.480465 139990499587840 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.2714474201202393, loss=2.5622000694274902
I0131 08:10:03.326535 139990491195136 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.196927070617676, loss=4.3322649002075195
I0131 08:10:49.144680 139990499587840 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.626063346862793, loss=2.8902029991149902
I0131 08:11:35.077321 139990491195136 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.8666739463806152, loss=3.0795199871063232
I0131 08:12:02.776815 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:12:13.291765 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:12:34.829838 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:12:36.435640 140184451094336 submission_runner.py:408] Time since start: 66590.26s, 	Step: 136662, 	{'train/accuracy': 0.7686523199081421, 'train/loss': 1.024351954460144, 'validation/accuracy': 0.7022199630737305, 'validation/loss': 1.3176020383834839, 'validation/num_examples': 50000, 'test/accuracy': 0.5806000232696533, 'test/loss': 1.923264741897583, 'test/num_examples': 10000, 'score': 61804.52350068092, 'total_duration': 66590.26353669167, 'accumulated_submission_time': 61804.52350068092, 'accumulated_eval_time': 4772.92241859436, 'accumulated_logging_time': 5.9302287101745605}
I0131 08:12:36.474302 139990499587840 logging_writer.py:48] [136662] accumulated_eval_time=4772.922419, accumulated_logging_time=5.930229, accumulated_submission_time=61804.523501, global_step=136662, preemption_count=0, score=61804.523501, test/accuracy=0.580600, test/loss=1.923265, test/num_examples=10000, total_duration=66590.263537, train/accuracy=0.768652, train/loss=1.024352, validation/accuracy=0.702220, validation/loss=1.317602, validation/num_examples=50000
I0131 08:12:52.063293 139990491195136 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.7869858741760254, loss=2.6461293697357178
I0131 08:13:34.730766 139990499587840 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.072833776473999, loss=2.636369228363037
I0131 08:14:20.647112 139990491195136 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.725536584854126, loss=4.226263999938965
I0131 08:15:06.503366 139990499587840 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.0626559257507324, loss=2.6417036056518555
I0131 08:15:52.131661 139990491195136 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.8813531398773193, loss=2.554718494415283
I0131 08:16:37.960259 139990499587840 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.9469199180603027, loss=3.1302194595336914
I0131 08:17:23.545946 139990491195136 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.80240535736084, loss=3.8547744750976562
I0131 08:18:09.531296 139990499587840 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.9778363704681396, loss=2.827491044998169
I0131 08:18:55.355082 139990491195136 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.901505947113037, loss=2.588531970977783
I0131 08:19:36.457899 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:19:46.569181 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:20:09.209503 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:20:10.828915 140184451094336 submission_runner.py:408] Time since start: 67044.66s, 	Step: 137592, 	{'train/accuracy': 0.786425769329071, 'train/loss': 0.9460193514823914, 'validation/accuracy': 0.7095999717712402, 'validation/loss': 1.2811696529388428, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.8915445804595947, 'test/num_examples': 10000, 'score': 62224.45097088814, 'total_duration': 67044.65680789948, 'accumulated_submission_time': 62224.45097088814, 'accumulated_eval_time': 4807.293427467346, 'accumulated_logging_time': 5.978266716003418}
I0131 08:20:10.869835 139990499587840 logging_writer.py:48] [137592] accumulated_eval_time=4807.293427, accumulated_logging_time=5.978267, accumulated_submission_time=62224.450971, global_step=137592, preemption_count=0, score=62224.450971, test/accuracy=0.586800, test/loss=1.891545, test/num_examples=10000, total_duration=67044.656808, train/accuracy=0.786426, train/loss=0.946019, validation/accuracy=0.709600, validation/loss=1.281170, validation/num_examples=50000
I0131 08:20:14.474710 139990491195136 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.841989040374756, loss=3.072725772857666
I0131 08:20:55.775176 139990499587840 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.3134255409240723, loss=4.607120990753174
I0131 08:21:41.514631 139990491195136 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.279230833053589, loss=2.653156280517578
I0131 08:22:27.685712 139990499587840 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.221247434616089, loss=4.746706962585449
I0131 08:23:13.446650 139990491195136 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.5705370903015137, loss=3.43184757232666
I0131 08:23:59.094754 139990499587840 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.413616180419922, loss=2.841801166534424
I0131 08:24:44.803030 139990491195136 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.2669942378997803, loss=4.582999229431152
I0131 08:25:30.668419 139990499587840 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.603564739227295, loss=3.563152551651001
I0131 08:26:16.450213 139990491195136 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.0571506023406982, loss=3.276390790939331
I0131 08:27:02.127385 139990499587840 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.8831613063812256, loss=2.8512587547302246
I0131 08:27:10.863456 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:27:21.147176 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:27:44.276112 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:27:45.876678 140184451094336 submission_runner.py:408] Time since start: 67499.70s, 	Step: 138521, 	{'train/accuracy': 0.76917964220047, 'train/loss': 1.0142436027526855, 'validation/accuracy': 0.7066400051116943, 'validation/loss': 1.2850269079208374, 'validation/num_examples': 50000, 'test/accuracy': 0.584600031375885, 'test/loss': 1.8975168466567993, 'test/num_examples': 10000, 'score': 62644.38854908943, 'total_duration': 67499.70457077026, 'accumulated_submission_time': 62644.38854908943, 'accumulated_eval_time': 4842.306634902954, 'accumulated_logging_time': 6.0289857387542725}
I0131 08:27:45.911828 139990491195136 logging_writer.py:48] [138521] accumulated_eval_time=4842.306635, accumulated_logging_time=6.028986, accumulated_submission_time=62644.388549, global_step=138521, preemption_count=0, score=62644.388549, test/accuracy=0.584600, test/loss=1.897517, test/num_examples=10000, total_duration=67499.704571, train/accuracy=0.769180, train/loss=1.014244, validation/accuracy=0.706640, validation/loss=1.285027, validation/num_examples=50000
I0131 08:28:17.900018 139990499587840 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.742079973220825, loss=2.9616358280181885
I0131 08:29:03.360290 139990491195136 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.789588212966919, loss=3.6403422355651855
I0131 08:29:49.111443 139990499587840 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.907681703567505, loss=2.8459694385528564
I0131 08:30:35.067680 139990491195136 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.0424535274505615, loss=4.133519649505615
I0131 08:31:21.038160 139990499587840 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.9544241428375244, loss=2.6086416244506836
I0131 08:32:07.244766 139990491195136 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.345292568206787, loss=3.1742146015167236
I0131 08:32:52.992498 139990499587840 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.032893657684326, loss=2.720729351043701
I0131 08:33:39.024436 139990491195136 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.9254932403564453, loss=2.806020736694336
I0131 08:34:24.684070 139990499587840 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.059506416320801, loss=2.667750358581543
I0131 08:34:45.879209 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:34:56.044241 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:35:16.977915 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:35:18.595903 140184451094336 submission_runner.py:408] Time since start: 67952.42s, 	Step: 139448, 	{'train/accuracy': 0.7764062285423279, 'train/loss': 0.9926196932792664, 'validation/accuracy': 0.7118200063705444, 'validation/loss': 1.268027663230896, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 1.8783414363861084, 'test/num_examples': 10000, 'score': 63064.298337221146, 'total_duration': 67952.42379832268, 'accumulated_submission_time': 63064.298337221146, 'accumulated_eval_time': 4875.023307323456, 'accumulated_logging_time': 6.075516939163208}
I0131 08:35:18.633232 139990491195136 logging_writer.py:48] [139448] accumulated_eval_time=4875.023307, accumulated_logging_time=6.075517, accumulated_submission_time=63064.298337, global_step=139448, preemption_count=0, score=63064.298337, test/accuracy=0.586700, test/loss=1.878341, test/num_examples=10000, total_duration=67952.423798, train/accuracy=0.776406, train/loss=0.992620, validation/accuracy=0.711820, validation/loss=1.268028, validation/num_examples=50000
I0131 08:35:39.830916 139990499587840 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.984605550765991, loss=4.383007526397705
I0131 08:36:23.542489 139990491195136 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.9336941242218018, loss=2.6959335803985596
I0131 08:37:09.182605 139990499587840 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.0392744541168213, loss=2.588644027709961
I0131 08:37:55.038300 139990491195136 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.6638107299804688, loss=3.2248549461364746
I0131 08:38:40.705768 139990499587840 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.1186399459838867, loss=2.643218994140625
I0131 08:39:27.011920 139990491195136 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.070169687271118, loss=2.552178144454956
I0131 08:40:12.761217 139990499587840 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.02641224861145, loss=2.5731897354125977
I0131 08:40:58.674741 139990491195136 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.017728805541992, loss=2.69551944732666
I0131 08:41:44.606492 139990499587840 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.4559261798858643, loss=2.5439484119415283
I0131 08:42:18.750807 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:42:29.520221 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:42:51.649635 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:42:53.248372 140184451094336 submission_runner.py:408] Time since start: 68407.08s, 	Step: 140376, 	{'train/accuracy': 0.7857421636581421, 'train/loss': 0.9634817838668823, 'validation/accuracy': 0.7120800018310547, 'validation/loss': 1.285139560699463, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.8925907611846924, 'test/num_examples': 10000, 'score': 63484.358603954315, 'total_duration': 68407.07604622841, 'accumulated_submission_time': 63484.358603954315, 'accumulated_eval_time': 4909.52064538002, 'accumulated_logging_time': 6.123907089233398}
I0131 08:42:53.284826 139990491195136 logging_writer.py:48] [140376] accumulated_eval_time=4909.520645, accumulated_logging_time=6.123907, accumulated_submission_time=63484.358604, global_step=140376, preemption_count=0, score=63484.358604, test/accuracy=0.590200, test/loss=1.892591, test/num_examples=10000, total_duration=68407.076046, train/accuracy=0.785742, train/loss=0.963482, validation/accuracy=0.712080, validation/loss=1.285140, validation/num_examples=50000
I0131 08:43:03.285428 139990499587840 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.2418477535247803, loss=2.7025084495544434
I0131 08:43:45.296895 139990491195136 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.1109869480133057, loss=2.632890462875366
I0131 08:44:31.153443 139990499587840 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.0044326782226562, loss=3.1358180046081543
I0131 08:45:17.214772 139990491195136 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.9751293659210205, loss=2.58089017868042
I0131 08:46:02.647904 139990499587840 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.0948688983917236, loss=2.741586685180664
I0131 08:46:48.239572 139990491195136 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.0900537967681885, loss=3.4149727821350098
I0131 08:47:33.826583 139990499587840 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.981058359146118, loss=2.587416172027588
I0131 08:48:19.440025 139990491195136 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.112985134124756, loss=2.454345941543579
I0131 08:49:05.308924 139990499587840 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.0694329738616943, loss=2.9965147972106934
I0131 08:49:51.042060 139990491195136 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.133479118347168, loss=2.656536102294922
I0131 08:49:53.434372 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:50:03.939318 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:50:26.545271 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:50:28.149747 140184451094336 submission_runner.py:408] Time since start: 68861.98s, 	Step: 141307, 	{'train/accuracy': 0.7754882574081421, 'train/loss': 0.99040687084198, 'validation/accuracy': 0.7101199626922607, 'validation/loss': 1.26579749584198, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.8663791418075562, 'test/num_examples': 10000, 'score': 63904.452016592026, 'total_duration': 68861.97762393951, 'accumulated_submission_time': 63904.452016592026, 'accumulated_eval_time': 4944.235973596573, 'accumulated_logging_time': 6.169575452804565}
I0131 08:50:28.199565 139990499587840 logging_writer.py:48] [141307] accumulated_eval_time=4944.235974, accumulated_logging_time=6.169575, accumulated_submission_time=63904.452017, global_step=141307, preemption_count=0, score=63904.452017, test/accuracy=0.596200, test/loss=1.866379, test/num_examples=10000, total_duration=68861.977624, train/accuracy=0.775488, train/loss=0.990407, validation/accuracy=0.710120, validation/loss=1.265797, validation/num_examples=50000
I0131 08:51:06.710050 139990491195136 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.861893653869629, loss=3.6171703338623047
I0131 08:51:52.448751 139990499587840 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.997183322906494, loss=4.048111915588379
I0131 08:52:38.605665 139990491195136 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.8704288005828857, loss=2.5930392742156982
I0131 08:53:24.723545 139990499587840 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.0365030765533447, loss=2.668053150177002
I0131 08:54:10.331953 139990491195136 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.0355417728424072, loss=3.195598602294922
I0131 08:54:56.089799 139990499587840 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.444148063659668, loss=2.7009940147399902
I0131 08:55:42.052656 139990491195136 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.942490339279175, loss=3.07047176361084
I0131 08:56:27.919609 139990499587840 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.3560891151428223, loss=2.689471960067749
I0131 08:57:13.715194 139990491195136 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.0478339195251465, loss=2.8348910808563232
I0131 08:57:28.561497 140184451094336 spec.py:321] Evaluating on the training split.
I0131 08:57:38.822117 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 08:58:01.339998 140184451094336 spec.py:349] Evaluating on the test split.
I0131 08:58:02.949639 140184451094336 submission_runner.py:408] Time since start: 69316.78s, 	Step: 142234, 	{'train/accuracy': 0.7803906202316284, 'train/loss': 0.9680672883987427, 'validation/accuracy': 0.7150200009346008, 'validation/loss': 1.2434526681900024, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8614035844802856, 'test/num_examples': 10000, 'score': 64324.757776498795, 'total_duration': 69316.77753448486, 'accumulated_submission_time': 64324.757776498795, 'accumulated_eval_time': 4978.624108314514, 'accumulated_logging_time': 6.229996204376221}
I0131 08:58:02.990775 139990499587840 logging_writer.py:48] [142234] accumulated_eval_time=4978.624108, accumulated_logging_time=6.229996, accumulated_submission_time=64324.757776, global_step=142234, preemption_count=0, score=64324.757776, test/accuracy=0.593300, test/loss=1.861404, test/num_examples=10000, total_duration=69316.777534, train/accuracy=0.780391, train/loss=0.968067, validation/accuracy=0.715020, validation/loss=1.243453, validation/num_examples=50000
I0131 08:58:29.778667 139990491195136 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.2435355186462402, loss=2.7252306938171387
I0131 08:59:14.496193 139990499587840 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.4783527851104736, loss=4.703133583068848
I0131 09:00:00.232742 139990491195136 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.0304291248321533, loss=3.347047805786133
I0131 09:00:46.306761 139990499587840 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.9918129444122314, loss=3.428600311279297
I0131 09:01:32.269363 139990491195136 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.1685643196105957, loss=2.527428388595581
I0131 09:02:18.026752 139990499587840 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.6556615829467773, loss=2.5973455905914307
I0131 09:03:03.903317 139990491195136 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.2469260692596436, loss=2.6049373149871826
I0131 09:03:49.830247 139990499587840 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.102001667022705, loss=3.691446304321289
I0131 09:04:35.657820 139990491195136 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.430549144744873, loss=4.676788330078125
I0131 09:05:03.293223 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:05:13.547438 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:05:35.506047 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:05:37.101303 140184451094336 submission_runner.py:408] Time since start: 69770.93s, 	Step: 143162, 	{'train/accuracy': 0.7883398532867432, 'train/loss': 0.9409700632095337, 'validation/accuracy': 0.7188999652862549, 'validation/loss': 1.2394484281539917, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.8527811765670776, 'test/num_examples': 10000, 'score': 64745.00335741043, 'total_duration': 69770.92920541763, 'accumulated_submission_time': 64745.00335741043, 'accumulated_eval_time': 5012.432188272476, 'accumulated_logging_time': 6.281265020370483}
I0131 09:05:37.138330 139990499587840 logging_writer.py:48] [143162] accumulated_eval_time=5012.432188, accumulated_logging_time=6.281265, accumulated_submission_time=64745.003357, global_step=143162, preemption_count=0, score=64745.003357, test/accuracy=0.600500, test/loss=1.852781, test/num_examples=10000, total_duration=69770.929205, train/accuracy=0.788340, train/loss=0.940970, validation/accuracy=0.718900, validation/loss=1.239448, validation/num_examples=50000
I0131 09:05:52.736147 139990491195136 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.936779737472534, loss=3.275953769683838
I0131 09:06:35.457750 139990499587840 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.554673671722412, loss=4.1944756507873535
I0131 09:07:21.231987 139990491195136 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.194934844970703, loss=2.617300510406494
I0131 09:08:07.024201 139990499587840 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.2310125827789307, loss=3.021376132965088
I0131 09:08:52.677891 139990491195136 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.045358180999756, loss=2.991086959838867
I0131 09:09:38.901707 139990499587840 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.8442420959472656, loss=2.9083666801452637
I0131 09:10:24.488690 139990491195136 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.557337999343872, loss=4.601412773132324
I0131 09:11:10.757001 139990499587840 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.632685661315918, loss=4.008549690246582
I0131 09:11:56.714328 139990491195136 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.3062915802001953, loss=2.5301990509033203
I0131 09:12:37.295411 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:12:47.495494 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:13:09.463921 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:13:11.060498 140184451094336 submission_runner.py:408] Time since start: 70224.89s, 	Step: 144090, 	{'train/accuracy': 0.7860937118530273, 'train/loss': 0.9350039958953857, 'validation/accuracy': 0.7210599780082703, 'validation/loss': 1.209059238433838, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.8121860027313232, 'test/num_examples': 10000, 'score': 65165.10458111763, 'total_duration': 70224.88837790489, 'accumulated_submission_time': 65165.10458111763, 'accumulated_eval_time': 5046.197269678116, 'accumulated_logging_time': 6.327600479125977}
I0131 09:13:11.103767 139990499587840 logging_writer.py:48] [144090] accumulated_eval_time=5046.197270, accumulated_logging_time=6.327600, accumulated_submission_time=65165.104581, global_step=144090, preemption_count=0, score=65165.104581, test/accuracy=0.601000, test/loss=1.812186, test/num_examples=10000, total_duration=70224.888378, train/accuracy=0.786094, train/loss=0.935004, validation/accuracy=0.721060, validation/loss=1.209059, validation/num_examples=50000
I0131 09:13:15.514842 139990491195136 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.305675983428955, loss=4.626325607299805
I0131 09:13:57.043932 139990499587840 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.236268997192383, loss=3.455705165863037
I0131 09:14:42.624759 139990491195136 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.4474148750305176, loss=2.5536277294158936
I0131 09:15:28.716817 139990499587840 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.6476590633392334, loss=2.84135365486145
I0131 09:16:14.283792 139990491195136 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.179727554321289, loss=2.5866856575012207
I0131 09:17:00.080630 139990499587840 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.1891117095947266, loss=2.4811110496520996
I0131 09:17:45.632112 139990491195136 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.251683235168457, loss=2.596789598464966
I0131 09:18:31.213290 139990499587840 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.2056643962860107, loss=4.273756504058838
I0131 09:19:17.066518 139990491195136 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.2352702617645264, loss=2.5139946937561035
I0131 09:20:02.817191 139990499587840 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.068597793579102, loss=4.251528263092041
I0131 09:20:11.119275 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:20:21.433594 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:20:45.300769 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:20:46.893035 140184451094336 submission_runner.py:408] Time since start: 70680.72s, 	Step: 145020, 	{'train/accuracy': 0.7858593463897705, 'train/loss': 0.9343098402023315, 'validation/accuracy': 0.7197799682617188, 'validation/loss': 1.222838282585144, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.830407738685608, 'test/num_examples': 10000, 'score': 65585.06423592567, 'total_duration': 70680.72092986107, 'accumulated_submission_time': 65585.06423592567, 'accumulated_eval_time': 5081.971020698547, 'accumulated_logging_time': 6.380660057067871}
I0131 09:20:46.931666 139990491195136 logging_writer.py:48] [145020] accumulated_eval_time=5081.971021, accumulated_logging_time=6.380660, accumulated_submission_time=65585.064236, global_step=145020, preemption_count=0, score=65585.064236, test/accuracy=0.600500, test/loss=1.830408, test/num_examples=10000, total_duration=70680.720930, train/accuracy=0.785859, train/loss=0.934310, validation/accuracy=0.719780, validation/loss=1.222838, validation/num_examples=50000
I0131 09:21:19.300179 139990499587840 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.2088303565979004, loss=3.980917453765869
I0131 09:22:04.834377 139990491195136 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.828340768814087, loss=3.428755044937134
I0131 09:22:50.450581 139990499587840 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.4799647331237793, loss=4.513672828674316
I0131 09:23:36.194308 139990491195136 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.1423895359039307, loss=2.825258493423462
I0131 09:24:22.046227 139990499587840 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.117494821548462, loss=3.512151002883911
I0131 09:25:07.761158 139990491195136 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.3032867908477783, loss=2.4394798278808594
I0131 09:25:53.257082 139990499587840 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.4351937770843506, loss=3.5286900997161865
I0131 09:26:39.010709 139990491195136 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.0731685161590576, loss=2.9682488441467285
I0131 09:27:24.625336 139990499587840 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.5786678791046143, loss=2.5027458667755127
I0131 09:27:47.161343 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:27:57.561578 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:28:20.077963 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:28:21.676278 140184451094336 submission_runner.py:408] Time since start: 71135.50s, 	Step: 145951, 	{'train/accuracy': 0.7895702719688416, 'train/loss': 0.9390873312950134, 'validation/accuracy': 0.7216399908065796, 'validation/loss': 1.2361648082733154, 'validation/num_examples': 50000, 'test/accuracy': 0.6039000153541565, 'test/loss': 1.830265760421753, 'test/num_examples': 10000, 'score': 66005.23581600189, 'total_duration': 71135.50416016579, 'accumulated_submission_time': 66005.23581600189, 'accumulated_eval_time': 5116.485925197601, 'accumulated_logging_time': 6.430635690689087}
I0131 09:28:21.714654 139990491195136 logging_writer.py:48] [145951] accumulated_eval_time=5116.485925, accumulated_logging_time=6.430636, accumulated_submission_time=66005.235816, global_step=145951, preemption_count=0, score=66005.235816, test/accuracy=0.603900, test/loss=1.830266, test/num_examples=10000, total_duration=71135.504160, train/accuracy=0.789570, train/loss=0.939087, validation/accuracy=0.721640, validation/loss=1.236165, validation/num_examples=50000
I0131 09:28:41.713200 139990499587840 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.574510097503662, loss=4.124846458435059
I0131 09:29:25.013748 139990491195136 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.436549663543701, loss=2.7728724479675293
I0131 09:30:10.921410 139990499587840 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.2762160301208496, loss=2.508833885192871
I0131 09:30:56.872925 139990491195136 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.2572555541992188, loss=3.594456195831299
I0131 09:31:42.687860 139990499587840 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.37669038772583, loss=2.845048189163208
I0131 09:32:28.536263 139990491195136 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.321561574935913, loss=4.161886215209961
I0131 09:33:14.295235 139990499587840 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.684931993484497, loss=3.7916829586029053
I0131 09:33:59.939872 139990491195136 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.4915904998779297, loss=2.6891050338745117
I0131 09:34:45.780006 139990499587840 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.6267449855804443, loss=4.430915355682373
I0131 09:35:21.725159 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:35:31.697157 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:35:55.375676 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:35:56.987233 140184451094336 submission_runner.py:408] Time since start: 71590.82s, 	Step: 146880, 	{'train/accuracy': 0.8043749928474426, 'train/loss': 0.8740522861480713, 'validation/accuracy': 0.7262799739837646, 'validation/loss': 1.2046759128570557, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.8089510202407837, 'test/num_examples': 10000, 'score': 66425.18833255768, 'total_duration': 71590.81511712074, 'accumulated_submission_time': 66425.18833255768, 'accumulated_eval_time': 5151.747972011566, 'accumulated_logging_time': 6.480493545532227}
I0131 09:35:57.042531 139990491195136 logging_writer.py:48] [146880] accumulated_eval_time=5151.747972, accumulated_logging_time=6.480494, accumulated_submission_time=66425.188333, global_step=146880, preemption_count=0, score=66425.188333, test/accuracy=0.603500, test/loss=1.808951, test/num_examples=10000, total_duration=71590.815117, train/accuracy=0.804375, train/loss=0.874052, validation/accuracy=0.726280, validation/loss=1.204676, validation/num_examples=50000
I0131 09:36:05.465476 139990499587840 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.558939218521118, loss=3.751551389694214
I0131 09:36:47.572948 139990491195136 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.6110804080963135, loss=2.5671451091766357
I0131 09:37:33.275546 139990499587840 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.2898495197296143, loss=2.5185632705688477
I0131 09:38:19.085274 139990491195136 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.3116774559020996, loss=2.631504535675049
I0131 09:39:04.501124 139990499587840 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.6543190479278564, loss=2.5985326766967773
I0131 09:39:50.749627 139990491195136 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.2801949977874756, loss=2.7264809608459473
I0131 09:40:36.110858 139990499587840 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.2386772632598877, loss=2.559579372406006
I0131 09:41:21.940015 139990491195136 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.433004856109619, loss=2.7196197509765625
I0131 09:42:08.201849 139990499587840 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.4993038177490234, loss=2.517881393432617
I0131 09:42:53.612224 139990491195136 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.903883934020996, loss=3.1229348182678223
I0131 09:42:57.374232 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:43:07.888248 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:43:29.270366 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:43:30.867299 140184451094336 submission_runner.py:408] Time since start: 72044.70s, 	Step: 147810, 	{'train/accuracy': 0.7898827791213989, 'train/loss': 0.9271060228347778, 'validation/accuracy': 0.7245599627494812, 'validation/loss': 1.2024517059326172, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.8167974948883057, 'test/num_examples': 10000, 'score': 66845.46230769157, 'total_duration': 72044.69520163536, 'accumulated_submission_time': 66845.46230769157, 'accumulated_eval_time': 5185.241056442261, 'accumulated_logging_time': 6.546149730682373}
I0131 09:43:30.907599 139990499587840 logging_writer.py:48] [147810] accumulated_eval_time=5185.241056, accumulated_logging_time=6.546150, accumulated_submission_time=66845.462308, global_step=147810, preemption_count=0, score=66845.462308, test/accuracy=0.600800, test/loss=1.816797, test/num_examples=10000, total_duration=72044.695202, train/accuracy=0.789883, train/loss=0.927106, validation/accuracy=0.724560, validation/loss=1.202452, validation/num_examples=50000
I0131 09:44:07.457406 139990491195136 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.142467975616455, loss=2.542198657989502
I0131 09:44:52.768825 139990499587840 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.5681841373443604, loss=3.4444191455841064
I0131 09:45:38.569344 139990491195136 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.639554977416992, loss=2.616417169570923
I0131 09:46:24.236066 139990499587840 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.3961055278778076, loss=2.474128484725952
I0131 09:47:10.035430 139990491195136 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.4716193675994873, loss=2.63612699508667
I0131 09:47:55.735190 139990499587840 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.71671986579895, loss=2.6132194995880127
I0131 09:48:41.603901 139990491195136 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.4250428676605225, loss=2.5874595642089844
I0131 09:49:27.349253 139990499587840 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.6783504486083984, loss=3.8598239421844482
I0131 09:50:13.034648 139990491195136 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.6685690879821777, loss=2.5188663005828857
I0131 09:50:31.066971 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:50:41.168318 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:51:03.921362 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:51:05.521039 140184451094336 submission_runner.py:408] Time since start: 72499.35s, 	Step: 148741, 	{'train/accuracy': 0.79749995470047, 'train/loss': 0.8806569576263428, 'validation/accuracy': 0.7253400087356567, 'validation/loss': 1.184898853302002, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7757694721221924, 'test/num_examples': 10000, 'score': 67265.56567597389, 'total_duration': 72499.34892606735, 'accumulated_submission_time': 67265.56567597389, 'accumulated_eval_time': 5219.695116758347, 'accumulated_logging_time': 6.595580577850342}
I0131 09:51:05.564154 139990499587840 logging_writer.py:48] [148741] accumulated_eval_time=5219.695117, accumulated_logging_time=6.595581, accumulated_submission_time=67265.565676, global_step=148741, preemption_count=0, score=67265.565676, test/accuracy=0.612100, test/loss=1.775769, test/num_examples=10000, total_duration=72499.348926, train/accuracy=0.797500, train/loss=0.880657, validation/accuracy=0.725340, validation/loss=1.184899, validation/num_examples=50000
I0131 09:51:29.719675 139990491195136 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.670095205307007, loss=2.548048496246338
I0131 09:52:14.153029 139990499587840 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.342590093612671, loss=2.8999836444854736
I0131 09:52:59.644245 139990491195136 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8468844890594482, loss=4.503968238830566
I0131 09:53:45.641467 139990499587840 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.7718284130096436, loss=2.513270616531372
I0131 09:54:31.405789 139990491195136 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.5181002616882324, loss=2.464749336242676
I0131 09:55:17.510227 139990499587840 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.8709425926208496, loss=2.504578113555908
I0131 09:56:03.121019 139990491195136 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.3462183475494385, loss=2.421761989593506
I0131 09:56:49.270952 139990499587840 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.359016180038452, loss=2.579812526702881
I0131 09:57:34.873079 139990491195136 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.1507792472839355, loss=2.5213847160339355
I0131 09:58:05.652607 140184451094336 spec.py:321] Evaluating on the training split.
I0131 09:58:16.136570 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 09:58:38.136853 140184451094336 spec.py:349] Evaluating on the test split.
I0131 09:58:39.735185 140184451094336 submission_runner.py:408] Time since start: 72953.56s, 	Step: 149669, 	{'train/accuracy': 0.8055273294448853, 'train/loss': 0.8653415441513062, 'validation/accuracy': 0.7287799715995789, 'validation/loss': 1.1887600421905518, 'validation/num_examples': 50000, 'test/accuracy': 0.6064000129699707, 'test/loss': 1.7887310981750488, 'test/num_examples': 10000, 'score': 67685.5972161293, 'total_duration': 72953.5630671978, 'accumulated_submission_time': 67685.5972161293, 'accumulated_eval_time': 5253.777672767639, 'accumulated_logging_time': 6.64896035194397}
I0131 09:58:39.773344 139990499587840 logging_writer.py:48] [149669] accumulated_eval_time=5253.777673, accumulated_logging_time=6.648960, accumulated_submission_time=67685.597216, global_step=149669, preemption_count=0, score=67685.597216, test/accuracy=0.606400, test/loss=1.788731, test/num_examples=10000, total_duration=72953.563067, train/accuracy=0.805527, train/loss=0.865342, validation/accuracy=0.728780, validation/loss=1.188760, validation/num_examples=50000
I0131 09:58:52.567524 139990491195136 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.7918431758880615, loss=2.6483154296875
I0131 09:59:34.926465 139990499587840 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.840446949005127, loss=4.598156452178955
I0131 10:00:20.554551 139990491195136 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.3557181358337402, loss=4.004048824310303
I0131 10:01:06.497575 139990499587840 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.692755937576294, loss=3.3616433143615723
I0131 10:01:52.088634 139990491195136 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.6430468559265137, loss=2.56363582611084
I0131 10:02:38.008773 139990499587840 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.6381378173828125, loss=3.965365409851074
I0131 10:03:23.664989 139990491195136 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.559671640396118, loss=3.1023659706115723
I0131 10:04:09.408166 139990499587840 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.967780828475952, loss=2.719939708709717
I0131 10:04:55.151242 139990491195136 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.542151927947998, loss=2.5467114448547363
I0131 10:05:40.044790 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:05:50.342228 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:06:12.569309 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:06:14.161841 140184451094336 submission_runner.py:408] Time since start: 73407.99s, 	Step: 150600, 	{'train/accuracy': 0.7973241806030273, 'train/loss': 0.8826409578323364, 'validation/accuracy': 0.7310000061988831, 'validation/loss': 1.1645584106445312, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7593497037887573, 'test/num_examples': 10000, 'score': 68105.81286787987, 'total_duration': 73407.98974180222, 'accumulated_submission_time': 68105.81286787987, 'accumulated_eval_time': 5287.894725084305, 'accumulated_logging_time': 6.696053504943848}
I0131 10:06:14.204300 139990499587840 logging_writer.py:48] [150600] accumulated_eval_time=5287.894725, accumulated_logging_time=6.696054, accumulated_submission_time=68105.812868, global_step=150600, preemption_count=0, score=68105.812868, test/accuracy=0.612100, test/loss=1.759350, test/num_examples=10000, total_duration=73407.989742, train/accuracy=0.797324, train/loss=0.882641, validation/accuracy=0.731000, validation/loss=1.164558, validation/num_examples=50000
I0131 10:06:14.611121 139990491195136 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.5463476181030273, loss=2.7988481521606445
I0131 10:06:55.215321 139990499587840 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.502514123916626, loss=2.486626148223877
I0131 10:07:40.889219 139990491195136 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.9631776809692383, loss=2.568094253540039
I0131 10:08:26.962263 139990499587840 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.8347814083099365, loss=2.5993783473968506
I0131 10:09:12.534389 139990491195136 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.7016520500183105, loss=2.801917552947998
I0131 10:09:58.099680 139990499587840 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.902625560760498, loss=2.726926803588867
I0131 10:10:43.808965 139990491195136 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.510841369628906, loss=4.451381683349609
I0131 10:11:29.568038 139990499587840 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.8919568061828613, loss=3.7060136795043945
I0131 10:12:15.587081 139990491195136 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.102158546447754, loss=4.390920162200928
I0131 10:13:01.059536 139990499587840 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.549700736999512, loss=4.457158088684082
I0131 10:13:14.482999 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:13:24.882301 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:13:47.210715 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:13:48.815545 140184451094336 submission_runner.py:408] Time since start: 73862.64s, 	Step: 151531, 	{'train/accuracy': 0.8052929639816284, 'train/loss': 0.8601254224777222, 'validation/accuracy': 0.733959972858429, 'validation/loss': 1.1651591062545776, 'validation/num_examples': 50000, 'test/accuracy': 0.6106000542640686, 'test/loss': 1.780478596687317, 'test/num_examples': 10000, 'score': 68526.03539800644, 'total_duration': 73862.64343810081, 'accumulated_submission_time': 68526.03539800644, 'accumulated_eval_time': 5322.22726726532, 'accumulated_logging_time': 6.748147487640381}
I0131 10:13:48.856588 139990491195136 logging_writer.py:48] [151531] accumulated_eval_time=5322.227267, accumulated_logging_time=6.748147, accumulated_submission_time=68526.035398, global_step=151531, preemption_count=0, score=68526.035398, test/accuracy=0.610600, test/loss=1.780479, test/num_examples=10000, total_duration=73862.643438, train/accuracy=0.805293, train/loss=0.860125, validation/accuracy=0.733960, validation/loss=1.165159, validation/num_examples=50000
I0131 10:14:16.873472 139990499587840 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.8341634273529053, loss=2.979506492614746
I0131 10:15:01.933768 139990491195136 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.853355646133423, loss=2.588754653930664
I0131 10:15:47.615194 139990499587840 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.017663955688477, loss=2.4874465465545654
I0131 10:16:33.534602 139990491195136 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.4803831577301025, loss=2.6299941539764404
I0131 10:17:19.392361 139990499587840 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.111099720001221, loss=2.553553342819214
I0131 10:18:05.285211 139990491195136 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.704880714416504, loss=2.384368419647217
I0131 10:18:50.824712 139990499587840 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.8524210453033447, loss=2.744420051574707
I0131 10:19:36.634838 139990491195136 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.8821394443511963, loss=2.39437198638916
I0131 10:20:22.653852 139990499587840 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.971435070037842, loss=3.69724702835083
I0131 10:20:49.118201 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:20:59.416086 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:21:22.410718 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:21:24.020013 140184451094336 submission_runner.py:408] Time since start: 74317.85s, 	Step: 152460, 	{'train/accuracy': 0.8087890148162842, 'train/loss': 0.8668062686920166, 'validation/accuracy': 0.7332599759101868, 'validation/loss': 1.18711519241333, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.789298176765442, 'test/num_examples': 10000, 'score': 68946.24187660217, 'total_duration': 74317.84789443016, 'accumulated_submission_time': 68946.24187660217, 'accumulated_eval_time': 5357.129096031189, 'accumulated_logging_time': 6.798153877258301}
I0131 10:21:24.067847 139990491195136 logging_writer.py:48] [152460] accumulated_eval_time=5357.129096, accumulated_logging_time=6.798154, accumulated_submission_time=68946.241877, global_step=152460, preemption_count=0, score=68946.241877, test/accuracy=0.610200, test/loss=1.789298, test/num_examples=10000, total_duration=74317.847894, train/accuracy=0.808789, train/loss=0.866806, validation/accuracy=0.733260, validation/loss=1.187115, validation/num_examples=50000
I0131 10:21:40.483457 139990499587840 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.8821403980255127, loss=2.6678895950317383
I0131 10:22:23.571852 139990491195136 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.7204766273498535, loss=2.417205572128296
I0131 10:23:09.342686 139990499587840 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.216308116912842, loss=4.179118633270264
I0131 10:23:55.002852 139990491195136 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.9039604663848877, loss=4.051612854003906
I0131 10:24:40.637113 139990499587840 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.187135219573975, loss=2.487668514251709
I0131 10:25:26.411901 139990491195136 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.773216724395752, loss=2.4205241203308105
I0131 10:26:11.917906 139990499587840 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.9559996128082275, loss=2.400667428970337
I0131 10:26:57.460006 139990491195136 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.42915153503418, loss=2.4339232444763184
I0131 10:27:43.374595 139990499587840 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.416930198669434, loss=4.091845512390137
I0131 10:28:24.063365 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:28:34.296345 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:28:56.960764 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:28:58.559148 140184451094336 submission_runner.py:408] Time since start: 74772.39s, 	Step: 153391, 	{'train/accuracy': 0.8062109351158142, 'train/loss': 0.8395585417747498, 'validation/accuracy': 0.7360999584197998, 'validation/loss': 1.139323353767395, 'validation/num_examples': 50000, 'test/accuracy': 0.6176000237464905, 'test/loss': 1.7518985271453857, 'test/num_examples': 10000, 'score': 69366.1802983284, 'total_duration': 74772.3870472908, 'accumulated_submission_time': 69366.1802983284, 'accumulated_eval_time': 5391.624893188477, 'accumulated_logging_time': 6.85608983039856}
I0131 10:28:58.604377 139990491195136 logging_writer.py:48] [153391] accumulated_eval_time=5391.624893, accumulated_logging_time=6.856090, accumulated_submission_time=69366.180298, global_step=153391, preemption_count=0, score=69366.180298, test/accuracy=0.617600, test/loss=1.751899, test/num_examples=10000, total_duration=74772.387047, train/accuracy=0.806211, train/loss=0.839559, validation/accuracy=0.736100, validation/loss=1.139323, validation/num_examples=50000
I0131 10:29:02.605531 139990499587840 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.8344287872314453, loss=2.5603504180908203
I0131 10:29:43.944936 139990491195136 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.997486114501953, loss=2.9422495365142822
I0131 10:30:29.473000 139990499587840 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.7987864017486572, loss=2.431539297103882
I0131 10:31:15.374819 139990491195136 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.031997203826904, loss=2.440540313720703
I0131 10:32:01.164642 139990499587840 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.6666743755340576, loss=2.8048365116119385
I0131 10:32:47.117986 139990491195136 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.242619514465332, loss=2.4550623893737793
I0131 10:33:33.397814 139990499587840 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.7825963497161865, loss=2.448382616043091
I0131 10:34:19.283218 139990491195136 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.831336498260498, loss=3.431109666824341
I0131 10:35:05.142021 139990499587840 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.8500969409942627, loss=2.3369803428649902
I0131 10:35:51.007066 139990491195136 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.3530683517456055, loss=2.532543897628784
I0131 10:35:58.825382 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:36:09.517587 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:36:32.150885 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:36:33.759960 140184451094336 submission_runner.py:408] Time since start: 75227.59s, 	Step: 154319, 	{'train/accuracy': 0.8050194978713989, 'train/loss': 0.8566577434539795, 'validation/accuracy': 0.7374399900436401, 'validation/loss': 1.1484848260879517, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.7600574493408203, 'test/num_examples': 10000, 'score': 69786.34556651115, 'total_duration': 75227.58785867691, 'accumulated_submission_time': 69786.34556651115, 'accumulated_eval_time': 5426.559454441071, 'accumulated_logging_time': 6.911020278930664}
I0131 10:36:33.798569 139990499587840 logging_writer.py:48] [154319] accumulated_eval_time=5426.559454, accumulated_logging_time=6.911020, accumulated_submission_time=69786.345567, global_step=154319, preemption_count=0, score=69786.345567, test/accuracy=0.615300, test/loss=1.760057, test/num_examples=10000, total_duration=75227.587859, train/accuracy=0.805019, train/loss=0.856658, validation/accuracy=0.737440, validation/loss=1.148485, validation/num_examples=50000
I0131 10:37:06.656977 139990491195136 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.353351593017578, loss=4.035383224487305
I0131 10:37:52.320502 139990499587840 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.839353084564209, loss=3.8890304565429688
I0131 10:38:38.652211 139990491195136 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.8580901622772217, loss=2.9786341190338135
I0131 10:39:24.797251 139990499587840 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.156386375427246, loss=3.744959831237793
I0131 10:40:10.482378 139990491195136 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.964329242706299, loss=2.8817710876464844
I0131 10:40:56.556593 139990499587840 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.386153697967529, loss=4.469998836517334
I0131 10:41:42.668211 139990491195136 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.0226874351501465, loss=2.367692470550537
I0131 10:42:28.586220 139990499587840 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.7356643676757812, loss=3.253679037094116
I0131 10:43:14.530819 139990491195136 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.8564717769622803, loss=2.4813342094421387
I0131 10:43:34.180547 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:43:44.905270 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:44:07.300740 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:44:08.896697 140184451094336 submission_runner.py:408] Time since start: 75682.72s, 	Step: 155245, 	{'train/accuracy': 0.8122656345367432, 'train/loss': 0.8312785625457764, 'validation/accuracy': 0.7390999794006348, 'validation/loss': 1.1496641635894775, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.7459418773651123, 'test/num_examples': 10000, 'score': 70206.66819000244, 'total_duration': 75682.72457933426, 'accumulated_submission_time': 70206.66819000244, 'accumulated_eval_time': 5461.275590896606, 'accumulated_logging_time': 6.962958574295044}
I0131 10:44:08.944056 139990499587840 logging_writer.py:48] [155245] accumulated_eval_time=5461.275591, accumulated_logging_time=6.962959, accumulated_submission_time=70206.668190, global_step=155245, preemption_count=0, score=70206.668190, test/accuracy=0.620500, test/loss=1.745942, test/num_examples=10000, total_duration=75682.724579, train/accuracy=0.812266, train/loss=0.831279, validation/accuracy=0.739100, validation/loss=1.149664, validation/num_examples=50000
I0131 10:44:31.576447 139990491195136 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.680842399597168, loss=4.455073356628418
I0131 10:45:16.473507 139990499587840 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.550511837005615, loss=3.7370033264160156
I0131 10:46:02.717216 139990491195136 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.075815677642822, loss=2.4240527153015137
I0131 10:46:48.495672 139990499587840 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.8789052963256836, loss=2.4687342643737793
I0131 10:47:34.536771 139990491195136 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.06077766418457, loss=2.5086264610290527
I0131 10:48:20.295146 139990499587840 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.1479105949401855, loss=3.808539867401123
I0131 10:49:06.022800 139990491195136 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.790186643600464, loss=2.8096680641174316
I0131 10:49:51.701511 139990499587840 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.4697265625, loss=4.091635227203369
I0131 10:50:37.462092 139990491195136 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.788254976272583, loss=2.586482286453247
I0131 10:51:08.906335 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:51:19.393457 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:51:41.381062 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:51:42.990175 140184451094336 submission_runner.py:408] Time since start: 76136.82s, 	Step: 156170, 	{'train/accuracy': 0.8208202719688416, 'train/loss': 0.808429479598999, 'validation/accuracy': 0.7415599822998047, 'validation/loss': 1.147451639175415, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.7455700635910034, 'test/num_examples': 10000, 'score': 70626.57415676117, 'total_duration': 76136.81805038452, 'accumulated_submission_time': 70626.57415676117, 'accumulated_eval_time': 5495.359393119812, 'accumulated_logging_time': 7.020386457443237}
I0131 10:51:43.038389 139990499587840 logging_writer.py:48] [156170] accumulated_eval_time=5495.359393, accumulated_logging_time=7.020386, accumulated_submission_time=70626.574157, global_step=156170, preemption_count=0, score=70626.574157, test/accuracy=0.617000, test/loss=1.745570, test/num_examples=10000, total_duration=76136.818050, train/accuracy=0.820820, train/loss=0.808429, validation/accuracy=0.741560, validation/loss=1.147452, validation/num_examples=50000
I0131 10:51:55.441444 139990491195136 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.102910995483398, loss=2.4528019428253174
I0131 10:52:38.170451 139990499587840 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.1364288330078125, loss=2.993067979812622
I0131 10:53:23.868517 139990491195136 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.59774112701416, loss=2.3141984939575195
I0131 10:54:09.820331 139990499587840 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.157668590545654, loss=3.905611991882324
I0131 10:54:55.513048 139990491195136 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.4375810623168945, loss=3.6022067070007324
I0131 10:55:41.261041 139990499587840 logging_writer.py:48] [156700] global_step=156700, grad_norm=5.229681968688965, loss=4.412339210510254
I0131 10:56:26.870988 139990491195136 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.903381824493408, loss=2.3714494705200195
I0131 10:57:12.671216 139990499587840 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.984405040740967, loss=3.373422145843506
I0131 10:57:58.290862 139990491195136 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.704190254211426, loss=4.3804612159729
I0131 10:58:43.206106 140184451094336 spec.py:321] Evaluating on the training split.
I0131 10:58:53.784365 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 10:59:15.662838 140184451094336 spec.py:349] Evaluating on the test split.
I0131 10:59:17.284503 140184451094336 submission_runner.py:408] Time since start: 76591.11s, 	Step: 157100, 	{'train/accuracy': 0.8145312070846558, 'train/loss': 0.8163199424743652, 'validation/accuracy': 0.7399199604988098, 'validation/loss': 1.1327764987945557, 'validation/num_examples': 50000, 'test/accuracy': 0.6218000054359436, 'test/loss': 1.7313672304153442, 'test/num_examples': 10000, 'score': 71046.6845676899, 'total_duration': 76591.11240267754, 'accumulated_submission_time': 71046.6845676899, 'accumulated_eval_time': 5529.437774181366, 'accumulated_logging_time': 7.078781843185425}
I0131 10:59:17.326856 139990499587840 logging_writer.py:48] [157100] accumulated_eval_time=5529.437774, accumulated_logging_time=7.078782, accumulated_submission_time=71046.684568, global_step=157100, preemption_count=0, score=71046.684568, test/accuracy=0.621800, test/loss=1.731367, test/num_examples=10000, total_duration=76591.112403, train/accuracy=0.814531, train/loss=0.816320, validation/accuracy=0.739920, validation/loss=1.132776, validation/num_examples=50000
I0131 10:59:17.735405 139990491195136 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.222475051879883, loss=2.3870272636413574
I0131 10:59:58.563334 139990499587840 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.126521110534668, loss=2.392216444015503
I0131 11:00:43.988656 139990491195136 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.997830629348755, loss=3.687790632247925
I0131 11:01:30.383527 139990499587840 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.0265116691589355, loss=3.1107945442199707
I0131 11:02:16.049563 139990491195136 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.9344606399536133, loss=3.236030101776123
I0131 11:03:01.674567 139990499587840 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.126699447631836, loss=2.7360267639160156
I0131 11:03:47.813201 139990491195136 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.219270706176758, loss=3.6123785972595215
I0131 11:04:33.363913 139990499587840 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.155362606048584, loss=2.3719587326049805
I0131 11:05:19.293200 139990491195136 logging_writer.py:48] [157900] global_step=157900, grad_norm=5.174490451812744, loss=4.36675500869751
I0131 11:06:05.081723 139990499587840 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.134871006011963, loss=3.3186774253845215
I0131 11:06:17.477457 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:06:27.841728 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:06:50.266027 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:06:51.866387 140184451094336 submission_runner.py:408] Time since start: 77045.69s, 	Step: 158029, 	{'train/accuracy': 0.8168163895606995, 'train/loss': 0.8214466571807861, 'validation/accuracy': 0.7429199814796448, 'validation/loss': 1.1342238187789917, 'validation/num_examples': 50000, 'test/accuracy': 0.6199000477790833, 'test/loss': 1.7298482656478882, 'test/num_examples': 10000, 'score': 71466.77744364738, 'total_duration': 77045.694283247, 'accumulated_submission_time': 71466.77744364738, 'accumulated_eval_time': 5563.826683521271, 'accumulated_logging_time': 7.130944013595581}
I0131 11:06:51.909031 139990491195136 logging_writer.py:48] [158029] accumulated_eval_time=5563.826684, accumulated_logging_time=7.130944, accumulated_submission_time=71466.777444, global_step=158029, preemption_count=0, score=71466.777444, test/accuracy=0.619900, test/loss=1.729848, test/num_examples=10000, total_duration=77045.694283, train/accuracy=0.816816, train/loss=0.821447, validation/accuracy=0.742920, validation/loss=1.134224, validation/num_examples=50000
I0131 11:07:20.686425 139990499587840 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.668417453765869, loss=2.503028631210327
I0131 11:08:05.547411 139990491195136 logging_writer.py:48] [158200] global_step=158200, grad_norm=5.0042724609375, loss=4.416528701782227
I0131 11:08:51.438302 139990499587840 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.486215114593506, loss=2.404064178466797
I0131 11:09:36.882229 139990491195136 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.9748458862304688, loss=3.6112847328186035
I0131 11:10:22.627082 139990499587840 logging_writer.py:48] [158500] global_step=158500, grad_norm=5.250807762145996, loss=4.334778308868408
I0131 11:11:08.693240 139990491195136 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.5475850105285645, loss=2.6251015663146973
I0131 11:11:54.554044 139990499587840 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.8457298278808594, loss=2.485665798187256
I0131 11:12:40.189946 139990491195136 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.9004647731781006, loss=2.3857901096343994
I0131 11:13:26.019121 139990499587840 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.4556660652160645, loss=2.4012444019317627
I0131 11:13:52.094775 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:14:02.539292 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:14:24.624301 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:14:26.223040 140184451094336 submission_runner.py:408] Time since start: 77500.05s, 	Step: 158959, 	{'train/accuracy': 0.823535144329071, 'train/loss': 0.7854181528091431, 'validation/accuracy': 0.7455799579620361, 'validation/loss': 1.114540696144104, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.7001981735229492, 'test/num_examples': 10000, 'score': 71886.9072842598, 'total_duration': 77500.05093717575, 'accumulated_submission_time': 71886.9072842598, 'accumulated_eval_time': 5597.954945802689, 'accumulated_logging_time': 7.182467699050903}
I0131 11:14:26.262580 139990491195136 logging_writer.py:48] [158959] accumulated_eval_time=5597.954946, accumulated_logging_time=7.182468, accumulated_submission_time=71886.907284, global_step=158959, preemption_count=0, score=71886.907284, test/accuracy=0.625500, test/loss=1.700198, test/num_examples=10000, total_duration=77500.050937, train/accuracy=0.823535, train/loss=0.785418, validation/accuracy=0.745580, validation/loss=1.114541, validation/num_examples=50000
I0131 11:14:43.063131 139990499587840 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.959411859512329, loss=2.273609161376953
I0131 11:15:25.898397 139990491195136 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.183597087860107, loss=3.8059167861938477
I0131 11:16:11.551459 139990499587840 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.258070945739746, loss=4.145602703094482
I0131 11:16:57.087138 139990491195136 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.52512264251709, loss=2.4445583820343018
I0131 11:17:42.775764 139990499587840 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.353348255157471, loss=2.494079113006592
I0131 11:18:28.657543 139990491195136 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.580502033233643, loss=2.522879123687744
I0131 11:19:14.487754 139990499587840 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.390028953552246, loss=2.414738178253174
I0131 11:20:00.070545 139990491195136 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.018033504486084, loss=3.2757575511932373
I0131 11:20:45.717578 139990499587840 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.794394016265869, loss=3.315114974975586
I0131 11:21:26.260242 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:21:36.836278 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:21:58.842262 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:22:00.446468 140184451094336 submission_runner.py:408] Time since start: 77954.27s, 	Step: 159890, 	{'train/accuracy': 0.8223828077316284, 'train/loss': 0.7984592914581299, 'validation/accuracy': 0.7449600100517273, 'validation/loss': 1.1174359321594238, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.702431082725525, 'test/num_examples': 10000, 'score': 72306.84834432602, 'total_duration': 77954.27435541153, 'accumulated_submission_time': 72306.84834432602, 'accumulated_eval_time': 5632.141172647476, 'accumulated_logging_time': 7.231976747512817}
I0131 11:22:00.492836 139990491195136 logging_writer.py:48] [159890] accumulated_eval_time=5632.141173, accumulated_logging_time=7.231977, accumulated_submission_time=72306.848344, global_step=159890, preemption_count=0, score=72306.848344, test/accuracy=0.630000, test/loss=1.702431, test/num_examples=10000, total_duration=77954.274355, train/accuracy=0.822383, train/loss=0.798459, validation/accuracy=0.744960, validation/loss=1.117436, validation/num_examples=50000
I0131 11:22:04.897741 139990499587840 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.633045196533203, loss=2.367496967315674
I0131 11:22:46.140243 139990491195136 logging_writer.py:48] [160000] global_step=160000, grad_norm=5.114899158477783, loss=4.089269638061523
I0131 11:23:31.879805 139990499587840 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.528280258178711, loss=2.4150118827819824
I0131 11:24:17.934615 139990491195136 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.8669843673706055, loss=2.3738186359405518
I0131 11:25:03.622332 139990499587840 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.452373504638672, loss=2.390115261077881
I0131 11:25:49.339084 139990491195136 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.119550704956055, loss=2.3941023349761963
I0131 11:26:35.110670 139990499587840 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.360532760620117, loss=2.3826708793640137
I0131 11:27:20.890575 139990491195136 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.239888668060303, loss=2.377441167831421
I0131 11:28:06.670712 139990499587840 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.499963283538818, loss=2.399127721786499
I0131 11:28:52.534823 139990491195136 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.157538414001465, loss=3.436685085296631
I0131 11:29:00.536525 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:29:10.912472 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:29:33.384965 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:29:34.997525 140184451094336 submission_runner.py:408] Time since start: 78408.83s, 	Step: 160819, 	{'train/accuracy': 0.82386714220047, 'train/loss': 0.7823898196220398, 'validation/accuracy': 0.7475000023841858, 'validation/loss': 1.1016745567321777, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.6906828880310059, 'test/num_examples': 10000, 'score': 72726.83613228798, 'total_duration': 78408.82541036606, 'accumulated_submission_time': 72726.83613228798, 'accumulated_eval_time': 5666.60213804245, 'accumulated_logging_time': 7.287751197814941}
I0131 11:29:35.046299 139990499587840 logging_writer.py:48] [160819] accumulated_eval_time=5666.602138, accumulated_logging_time=7.287751, accumulated_submission_time=72726.836132, global_step=160819, preemption_count=0, score=72726.836132, test/accuracy=0.627300, test/loss=1.690683, test/num_examples=10000, total_duration=78408.825410, train/accuracy=0.823867, train/loss=0.782390, validation/accuracy=0.747500, validation/loss=1.101675, validation/num_examples=50000
I0131 11:30:07.837479 139990491195136 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.412084102630615, loss=2.8477585315704346
I0131 11:30:53.286114 139990499587840 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.57852029800415, loss=3.4393386840820312
I0131 11:31:39.156696 139990491195136 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.679264068603516, loss=2.5800938606262207
I0131 11:32:25.036474 139990499587840 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.221089839935303, loss=2.807621955871582
I0131 11:33:10.970211 139990491195136 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.592694282531738, loss=2.3462071418762207
I0131 11:33:56.760029 139990499587840 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.946259021759033, loss=3.2892045974731445
I0131 11:34:42.529209 139990491195136 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.4783406257629395, loss=3.9059691429138184
I0131 11:35:28.419066 139990499587840 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.479285717010498, loss=2.3431200981140137
I0131 11:36:14.215052 139990491195136 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.881128311157227, loss=3.0548205375671387
I0131 11:36:35.387477 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:36:45.971280 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:37:08.430204 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:37:10.044326 140184451094336 submission_runner.py:408] Time since start: 78863.87s, 	Step: 161748, 	{'train/accuracy': 0.8251367211341858, 'train/loss': 0.7654830813407898, 'validation/accuracy': 0.7488999962806702, 'validation/loss': 1.0918009281158447, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.686492681503296, 'test/num_examples': 10000, 'score': 73147.12069392204, 'total_duration': 78863.87222862244, 'accumulated_submission_time': 73147.12069392204, 'accumulated_eval_time': 5701.25897192955, 'accumulated_logging_time': 7.346428871154785}
I0131 11:37:10.089161 139990499587840 logging_writer.py:48] [161748] accumulated_eval_time=5701.258972, accumulated_logging_time=7.346429, accumulated_submission_time=73147.120694, global_step=161748, preemption_count=0, score=73147.120694, test/accuracy=0.629000, test/loss=1.686493, test/num_examples=10000, total_duration=78863.872229, train/accuracy=0.825137, train/loss=0.765483, validation/accuracy=0.748900, validation/loss=1.091801, validation/num_examples=50000
I0131 11:37:31.372590 139990491195136 logging_writer.py:48] [161800] global_step=161800, grad_norm=5.855355262756348, loss=4.404886245727539
I0131 11:38:15.233171 139990499587840 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.020142555236816, loss=2.7883074283599854
I0131 11:39:01.071262 139990491195136 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.852241516113281, loss=3.732577323913574
I0131 11:39:47.060425 139990499587840 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.345895290374756, loss=2.705768346786499
I0131 11:40:32.760335 139990491195136 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.187781810760498, loss=2.4537596702575684
I0131 11:41:18.634999 139990499587840 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.349236011505127, loss=2.7107174396514893
I0131 11:42:04.654976 139990491195136 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.449388027191162, loss=2.2821309566497803
I0131 11:42:50.316454 139990499587840 logging_writer.py:48] [162500] global_step=162500, grad_norm=5.82417106628418, loss=4.345536231994629
I0131 11:43:35.888092 139990491195136 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.110722064971924, loss=2.9181904792785645
I0131 11:44:10.081597 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:44:20.358494 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:44:42.074926 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:44:43.675517 140184451094336 submission_runner.py:408] Time since start: 79317.50s, 	Step: 162676, 	{'train/accuracy': 0.8285351395606995, 'train/loss': 0.7643269896507263, 'validation/accuracy': 0.7503599524497986, 'validation/loss': 1.0934828519821167, 'validation/num_examples': 50000, 'test/accuracy': 0.633400022983551, 'test/loss': 1.6805967092514038, 'test/num_examples': 10000, 'score': 73567.05645251274, 'total_duration': 79317.50340890884, 'accumulated_submission_time': 73567.05645251274, 'accumulated_eval_time': 5734.852890491486, 'accumulated_logging_time': 7.401033163070679}
I0131 11:44:43.718864 139990499587840 logging_writer.py:48] [162676] accumulated_eval_time=5734.852890, accumulated_logging_time=7.401033, accumulated_submission_time=73567.056453, global_step=162676, preemption_count=0, score=73567.056453, test/accuracy=0.633400, test/loss=1.680597, test/num_examples=10000, total_duration=79317.503409, train/accuracy=0.828535, train/loss=0.764327, validation/accuracy=0.750360, validation/loss=1.093483, validation/num_examples=50000
I0131 11:44:53.724600 139990491195136 logging_writer.py:48] [162700] global_step=162700, grad_norm=5.4149651527404785, loss=3.9646596908569336
I0131 11:45:35.803924 139990499587840 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.767482757568359, loss=2.924856424331665
I0131 11:46:21.837181 139990491195136 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.914585590362549, loss=2.3751060962677
I0131 11:47:07.929043 139990499587840 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.705143451690674, loss=3.57027530670166
I0131 11:47:53.698637 139990491195136 logging_writer.py:48] [163100] global_step=163100, grad_norm=6.094576835632324, loss=4.439189910888672
I0131 11:48:39.788039 139990499587840 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.540975570678711, loss=2.333127975463867
I0131 11:49:25.648901 139990491195136 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.5540595054626465, loss=3.172402858734131
I0131 11:50:11.548759 139990499587840 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.649197578430176, loss=2.659557819366455
I0131 11:50:57.023583 139990491195136 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.274932861328125, loss=2.298455238342285
I0131 11:51:43.160764 139990499587840 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.866875648498535, loss=2.2531514167785645
I0131 11:51:43.778459 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:51:54.185592 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:52:15.120184 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:52:16.729848 140184451094336 submission_runner.py:408] Time since start: 79770.56s, 	Step: 163603, 	{'train/accuracy': 0.8294531106948853, 'train/loss': 0.7535117268562317, 'validation/accuracy': 0.7529199719429016, 'validation/loss': 1.0765010118484497, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.6685441732406616, 'test/num_examples': 10000, 'score': 73987.05826282501, 'total_duration': 79770.55769276619, 'accumulated_submission_time': 73987.05826282501, 'accumulated_eval_time': 5767.8042142391205, 'accumulated_logging_time': 7.45538067817688}
I0131 11:52:16.782030 139990491195136 logging_writer.py:48] [163603] accumulated_eval_time=5767.804214, accumulated_logging_time=7.455381, accumulated_submission_time=73987.058263, global_step=163603, preemption_count=0, score=73987.058263, test/accuracy=0.631700, test/loss=1.668544, test/num_examples=10000, total_duration=79770.557693, train/accuracy=0.829453, train/loss=0.753512, validation/accuracy=0.752920, validation/loss=1.076501, validation/num_examples=50000
I0131 11:52:56.813969 139990499587840 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.7539777755737305, loss=2.52193021774292
I0131 11:53:42.481469 139990491195136 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.778669834136963, loss=4.328885555267334
I0131 11:54:28.534243 139990499587840 logging_writer.py:48] [163900] global_step=163900, grad_norm=5.181985855102539, loss=4.120195388793945
I0131 11:55:14.301479 139990491195136 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.514476776123047, loss=2.301089286804199
I0131 11:56:00.102537 139990499587840 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.189593315124512, loss=2.22552227973938
I0131 11:56:46.208917 139990491195136 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.011338233947754, loss=2.544616222381592
I0131 11:57:32.006507 139990499587840 logging_writer.py:48] [164300] global_step=164300, grad_norm=5.341061115264893, loss=3.6525826454162598
I0131 11:58:18.067382 139990491195136 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.88178825378418, loss=2.688124656677246
I0131 11:59:03.984057 139990499587840 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.758171558380127, loss=3.5602149963378906
I0131 11:59:16.798964 140184451094336 spec.py:321] Evaluating on the training split.
I0131 11:59:27.215703 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 11:59:49.503827 140184451094336 spec.py:349] Evaluating on the test split.
I0131 11:59:51.103927 140184451094336 submission_runner.py:408] Time since start: 80224.93s, 	Step: 164530, 	{'train/accuracy': 0.82923823595047, 'train/loss': 0.7566958665847778, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.0820822715759277, 'validation/num_examples': 50000, 'test/accuracy': 0.6342000365257263, 'test/loss': 1.6714333295822144, 'test/num_examples': 10000, 'score': 74407.01922512054, 'total_duration': 80224.93182849884, 'accumulated_submission_time': 74407.01922512054, 'accumulated_eval_time': 5802.109175443649, 'accumulated_logging_time': 7.517110109329224}
I0131 11:59:51.145838 139990491195136 logging_writer.py:48] [164530] accumulated_eval_time=5802.109175, accumulated_logging_time=7.517110, accumulated_submission_time=74407.019225, global_step=164530, preemption_count=0, score=74407.019225, test/accuracy=0.634200, test/loss=1.671433, test/num_examples=10000, total_duration=80224.931828, train/accuracy=0.829238, train/loss=0.756696, validation/accuracy=0.751040, validation/loss=1.082082, validation/num_examples=50000
I0131 12:00:19.521361 139990499587840 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.858669757843018, loss=2.1648247241973877
I0131 12:01:04.322538 139990491195136 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.533074378967285, loss=3.1086137294769287
I0131 12:01:50.565687 139990499587840 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.533170700073242, loss=2.279130697250366
I0131 12:02:36.515972 139990491195136 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.349419593811035, loss=2.848113775253296
I0131 12:03:22.552269 139990499587840 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.612374305725098, loss=2.419182062149048
I0131 12:04:08.415037 139990491195136 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.4862380027771, loss=2.388485908508301
I0131 12:04:54.243501 139990499587840 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.83319616317749, loss=2.376816987991333
I0131 12:05:39.898093 139990491195136 logging_writer.py:48] [165300] global_step=165300, grad_norm=5.140459060668945, loss=2.369637966156006
I0131 12:06:25.663276 139990499587840 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.303359508514404, loss=3.500558614730835
I0131 12:06:51.325719 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:07:01.514208 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:07:25.115955 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:07:26.710068 140184451094336 submission_runner.py:408] Time since start: 80680.54s, 	Step: 165458, 	{'train/accuracy': 0.8375976085662842, 'train/loss': 0.7356262803077698, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0842537879943848, 'validation/num_examples': 50000, 'test/accuracy': 0.6332000494003296, 'test/loss': 1.685017704963684, 'test/num_examples': 10000, 'score': 74827.14331531525, 'total_duration': 80680.53797078133, 'accumulated_submission_time': 74827.14331531525, 'accumulated_eval_time': 5837.49352812767, 'accumulated_logging_time': 7.5688605308532715}
I0131 12:07:26.749704 139990491195136 logging_writer.py:48] [165458] accumulated_eval_time=5837.493528, accumulated_logging_time=7.568861, accumulated_submission_time=74827.143315, global_step=165458, preemption_count=0, score=74827.143315, test/accuracy=0.633200, test/loss=1.685018, test/num_examples=10000, total_duration=80680.537971, train/accuracy=0.837598, train/loss=0.735626, validation/accuracy=0.754860, validation/loss=1.084254, validation/num_examples=50000
I0131 12:07:43.960557 139990499587840 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.258614540100098, loss=2.390932559967041
I0131 12:08:27.025683 139990491195136 logging_writer.py:48] [165600] global_step=165600, grad_norm=5.227793216705322, loss=3.403738021850586
I0131 12:09:12.764071 139990499587840 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.742443561553955, loss=4.138405799865723
I0131 12:09:58.497592 139990491195136 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.98813533782959, loss=2.712667226791382
I0131 12:10:44.306133 139990499587840 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.144749164581299, loss=2.2780251502990723
I0131 12:11:30.092019 139990491195136 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.933440208435059, loss=2.295673370361328
I0131 12:12:15.817901 139990499587840 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.754571914672852, loss=2.3663039207458496
I0131 12:13:01.493526 139990491195136 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.556707382202148, loss=3.0305733680725098
I0131 12:13:47.018152 139990499587840 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.982768535614014, loss=3.1521878242492676
I0131 12:14:27.042721 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:14:37.639137 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:14:59.918230 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:15:01.518530 140184451094336 submission_runner.py:408] Time since start: 81135.35s, 	Step: 166389, 	{'train/accuracy': 0.8311718702316284, 'train/loss': 0.7485712170600891, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0757715702056885, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.6692404747009277, 'test/num_examples': 10000, 'score': 75247.37900185585, 'total_duration': 81135.34643173218, 'accumulated_submission_time': 75247.37900185585, 'accumulated_eval_time': 5871.969348192215, 'accumulated_logging_time': 7.618942499160767}
I0131 12:15:01.558718 139990491195136 logging_writer.py:48] [166389] accumulated_eval_time=5871.969348, accumulated_logging_time=7.618942, accumulated_submission_time=75247.379002, global_step=166389, preemption_count=0, score=75247.379002, test/accuracy=0.638900, test/loss=1.669240, test/num_examples=10000, total_duration=81135.346432, train/accuracy=0.831172, train/loss=0.748571, validation/accuracy=0.755000, validation/loss=1.075772, validation/num_examples=50000
I0131 12:15:06.358953 139990499587840 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.903982162475586, loss=2.5919106006622314
I0131 12:15:47.768636 139990491195136 logging_writer.py:48] [166500] global_step=166500, grad_norm=5.336282730102539, loss=2.231478452682495
I0131 12:16:33.642769 139990499587840 logging_writer.py:48] [166600] global_step=166600, grad_norm=5.396726608276367, loss=4.113094806671143
I0131 12:17:19.589140 139990491195136 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.125961780548096, loss=2.3757989406585693
I0131 12:18:05.456922 139990499587840 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.501397132873535, loss=2.3535845279693604
I0131 12:18:51.390640 139990491195136 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.728916645050049, loss=2.281618118286133
I0131 12:19:37.181066 139990499587840 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.94627046585083, loss=2.28550124168396
I0131 12:20:22.989547 139990491195136 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.108484745025635, loss=2.485382556915283
I0131 12:21:08.674659 139990499587840 logging_writer.py:48] [167200] global_step=167200, grad_norm=5.747127056121826, loss=2.855180501937866
I0131 12:21:54.589865 139990491195136 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.838705539703369, loss=3.525186061859131
I0131 12:22:01.718427 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:22:12.341651 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:22:34.806351 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:22:36.418659 140184451094336 submission_runner.py:408] Time since start: 81590.25s, 	Step: 167317, 	{'train/accuracy': 0.8334569931030273, 'train/loss': 0.7626773715019226, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.091723918914795, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.6732136011123657, 'test/num_examples': 10000, 'score': 75667.48089122772, 'total_duration': 81590.24653863907, 'accumulated_submission_time': 75667.48089122772, 'accumulated_eval_time': 5906.669553279877, 'accumulated_logging_time': 7.6705286502838135}
I0131 12:22:36.469485 139990499587840 logging_writer.py:48] [167317] accumulated_eval_time=5906.669553, accumulated_logging_time=7.670529, accumulated_submission_time=75667.480891, global_step=167317, preemption_count=0, score=75667.480891, test/accuracy=0.638900, test/loss=1.673214, test/num_examples=10000, total_duration=81590.246539, train/accuracy=0.833457, train/loss=0.762677, validation/accuracy=0.755860, validation/loss=1.091724, validation/num_examples=50000
I0131 12:23:10.089593 139990491195136 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.890959739685059, loss=2.3106462955474854
I0131 12:23:55.691056 139990499587840 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.45590353012085, loss=3.467590093612671
I0131 12:24:41.628302 139990491195136 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.236008167266846, loss=2.2398924827575684
I0131 12:25:27.733995 139990499587840 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.958635330200195, loss=2.8890552520751953
I0131 12:26:13.667079 139990491195136 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.320523738861084, loss=2.313811779022217
I0131 12:26:59.418587 139990499587840 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.167618751525879, loss=2.218843460083008
I0131 12:27:44.980965 139990491195136 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.025832176208496, loss=2.1622931957244873
I0131 12:28:30.563535 139990499587840 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.622838973999023, loss=2.3190650939941406
I0131 12:29:16.285320 139990491195136 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.189889430999756, loss=2.268051862716675
I0131 12:29:36.570836 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:29:46.998898 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:30:09.531959 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:30:11.126052 140184451094336 submission_runner.py:408] Time since start: 82044.95s, 	Step: 168246, 	{'train/accuracy': 0.8374999761581421, 'train/loss': 0.7262058258056641, 'validation/accuracy': 0.7564399838447571, 'validation/loss': 1.0680700540542603, 'validation/num_examples': 50000, 'test/accuracy': 0.6396000385284424, 'test/loss': 1.6480201482772827, 'test/num_examples': 10000, 'score': 76087.52441835403, 'total_duration': 82044.95395517349, 'accumulated_submission_time': 76087.52441835403, 'accumulated_eval_time': 5941.224766492844, 'accumulated_logging_time': 7.7321436405181885}
I0131 12:30:11.167210 139990499587840 logging_writer.py:48] [168246] accumulated_eval_time=5941.224766, accumulated_logging_time=7.732144, accumulated_submission_time=76087.524418, global_step=168246, preemption_count=0, score=76087.524418, test/accuracy=0.639600, test/loss=1.648020, test/num_examples=10000, total_duration=82044.953955, train/accuracy=0.837500, train/loss=0.726206, validation/accuracy=0.756440, validation/loss=1.068070, validation/num_examples=50000
I0131 12:30:33.174317 139990491195136 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.970620632171631, loss=2.7691421508789062
I0131 12:31:16.894882 139990499587840 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.059593677520752, loss=3.008131980895996
I0131 12:32:02.978419 139990491195136 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.605887413024902, loss=2.990110397338867
I0131 12:32:48.825029 139990499587840 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.341492652893066, loss=2.2402191162109375
I0131 12:33:34.643366 139990491195136 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.200689315795898, loss=2.273353338241577
I0131 12:34:20.590617 139990499587840 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.299084186553955, loss=2.2793431282043457
I0131 12:35:06.355277 139990491195136 logging_writer.py:48] [168900] global_step=168900, grad_norm=5.072972297668457, loss=2.3197131156921387
I0131 12:35:51.971140 139990499587840 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.995937824249268, loss=2.2824792861938477
I0131 12:36:37.601814 139990491195136 logging_writer.py:48] [169100] global_step=169100, grad_norm=5.75147819519043, loss=3.939493179321289
I0131 12:37:11.351578 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:37:21.858458 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:37:44.088177 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:37:45.687517 140184451094336 submission_runner.py:408] Time since start: 82499.52s, 	Step: 169175, 	{'train/accuracy': 0.83607417345047, 'train/loss': 0.7408673167228699, 'validation/accuracy': 0.7595399618148804, 'validation/loss': 1.0654107332229614, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.6457515954971313, 'test/num_examples': 10000, 'score': 76507.65330696106, 'total_duration': 82499.51538085938, 'accumulated_submission_time': 76507.65330696106, 'accumulated_eval_time': 5975.560669898987, 'accumulated_logging_time': 7.782688856124878}
I0131 12:37:45.738723 139990499587840 logging_writer.py:48] [169175] accumulated_eval_time=5975.560670, accumulated_logging_time=7.782689, accumulated_submission_time=76507.653307, global_step=169175, preemption_count=0, score=76507.653307, test/accuracy=0.642100, test/loss=1.645752, test/num_examples=10000, total_duration=82499.515381, train/accuracy=0.836074, train/loss=0.740867, validation/accuracy=0.759540, validation/loss=1.065411, validation/num_examples=50000
I0131 12:37:56.143628 139990491195136 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.383334159851074, loss=2.4649410247802734
I0131 12:38:38.278587 139990499587840 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.444721698760986, loss=3.085865020751953
I0131 12:39:24.291303 139990491195136 logging_writer.py:48] [169400] global_step=169400, grad_norm=5.494101047515869, loss=3.1967616081237793
I0131 12:40:10.607948 139990499587840 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.182959079742432, loss=2.3596904277801514
I0131 12:40:56.223629 139990491195136 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.542735576629639, loss=2.2665257453918457
I0131 12:41:42.110465 139990499587840 logging_writer.py:48] [169700] global_step=169700, grad_norm=5.137189865112305, loss=2.2538270950317383
I0131 12:42:28.034343 139990491195136 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.998607158660889, loss=2.232100009918213
I0131 12:43:13.876593 139990499587840 logging_writer.py:48] [169900] global_step=169900, grad_norm=5.238073348999023, loss=2.251091957092285
I0131 12:43:59.668798 139990491195136 logging_writer.py:48] [170000] global_step=170000, grad_norm=6.1382155418396, loss=3.8072922229766846
I0131 12:44:45.506765 139990499587840 logging_writer.py:48] [170100] global_step=170100, grad_norm=5.062934398651123, loss=2.3633153438568115
I0131 12:44:46.099275 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:44:56.481538 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:45:16.417032 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:45:18.030021 140184451094336 submission_runner.py:408] Time since start: 82951.86s, 	Step: 170103, 	{'train/accuracy': 0.8408398032188416, 'train/loss': 0.7134418487548828, 'validation/accuracy': 0.7605199813842773, 'validation/loss': 1.0481165647506714, 'validation/num_examples': 50000, 'test/accuracy': 0.6447000503540039, 'test/loss': 1.6202740669250488, 'test/num_examples': 10000, 'score': 76927.95672249794, 'total_duration': 82951.85792160034, 'accumulated_submission_time': 76927.95672249794, 'accumulated_eval_time': 6007.491415500641, 'accumulated_logging_time': 7.8443825244903564}
I0131 12:45:18.075260 139990491195136 logging_writer.py:48] [170103] accumulated_eval_time=6007.491416, accumulated_logging_time=7.844383, accumulated_submission_time=76927.956722, global_step=170103, preemption_count=0, score=76927.956722, test/accuracy=0.644700, test/loss=1.620274, test/num_examples=10000, total_duration=82951.857922, train/accuracy=0.840840, train/loss=0.713442, validation/accuracy=0.760520, validation/loss=1.048117, validation/num_examples=50000
I0131 12:45:57.994168 139990499587840 logging_writer.py:48] [170200] global_step=170200, grad_norm=5.432248115539551, loss=2.2473702430725098
I0131 12:46:43.572618 139990491195136 logging_writer.py:48] [170300] global_step=170300, grad_norm=5.129003047943115, loss=2.350541830062866
I0131 12:47:29.618794 139990499587840 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.763493537902832, loss=3.1422388553619385
I0131 12:48:15.210322 139990491195136 logging_writer.py:48] [170500] global_step=170500, grad_norm=5.741328239440918, loss=2.578456401824951
I0131 12:49:00.879949 139990499587840 logging_writer.py:48] [170600] global_step=170600, grad_norm=6.4601030349731445, loss=3.838563919067383
I0131 12:49:46.561210 139990491195136 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.966878414154053, loss=2.2644472122192383
I0131 12:50:32.281070 139990499587840 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.686822891235352, loss=2.7704288959503174
I0131 12:51:18.083363 139990491195136 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.848903656005859, loss=2.790578842163086
I0131 12:52:04.022939 139990499587840 logging_writer.py:48] [171000] global_step=171000, grad_norm=5.451107978820801, loss=2.3271374702453613
I0131 12:52:18.323530 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:52:28.809257 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:52:49.507620 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:52:51.115510 140184451094336 submission_runner.py:408] Time since start: 83404.94s, 	Step: 171033, 	{'train/accuracy': 0.8424023389816284, 'train/loss': 0.7111977934837341, 'validation/accuracy': 0.7627399563789368, 'validation/loss': 1.051758050918579, 'validation/num_examples': 50000, 'test/accuracy': 0.6460000276565552, 'test/loss': 1.633678674697876, 'test/num_examples': 10000, 'score': 77348.14902710915, 'total_duration': 83404.94340229034, 'accumulated_submission_time': 77348.14902710915, 'accumulated_eval_time': 6040.283388137817, 'accumulated_logging_time': 7.898629903793335}
I0131 12:52:51.160821 139990491195136 logging_writer.py:48] [171033] accumulated_eval_time=6040.283388, accumulated_logging_time=7.898630, accumulated_submission_time=77348.149027, global_step=171033, preemption_count=0, score=77348.149027, test/accuracy=0.646000, test/loss=1.633679, test/num_examples=10000, total_duration=83404.943402, train/accuracy=0.842402, train/loss=0.711198, validation/accuracy=0.762740, validation/loss=1.051758, validation/num_examples=50000
I0131 12:53:18.325634 139990499587840 logging_writer.py:48] [171100] global_step=171100, grad_norm=6.191476345062256, loss=4.187788486480713
I0131 12:54:03.513287 139990491195136 logging_writer.py:48] [171200] global_step=171200, grad_norm=5.422298431396484, loss=2.3475191593170166
I0131 12:54:49.299919 139990499587840 logging_writer.py:48] [171300] global_step=171300, grad_norm=5.0696797370910645, loss=2.375725269317627
I0131 12:55:35.514410 139990491195136 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.805070400238037, loss=2.122511863708496
I0131 12:55:43.416581 139990499587840 logging_writer.py:48] [171419] global_step=171419, preemption_count=0, score=77520.331688
I0131 12:55:44.033454 140184451094336 checkpoints.py:490] Saving checkpoint at step: 171419
I0131 12:55:45.156454 140184451094336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1/checkpoint_171419
I0131 12:55:45.173536 140184451094336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_1/checkpoint_171419.
I0131 12:55:46.100225 140184451094336 submission_runner.py:583] Tuning trial 1/5
I0131 12:55:46.100442 140184451094336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0131 12:55:46.112017 140184451094336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.74321413040161, 'total_duration': 82.96978783607483, 'accumulated_submission_time': 42.74321413040161, 'accumulated_eval_time': 40.2264838218689, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (897, {'train/accuracy': 0.013144531287252903, 'train/loss': 6.433816909790039, 'validation/accuracy': 0.012319999746978283, 'validation/loss': 6.444516181945801, 'validation/num_examples': 50000, 'test/accuracy': 0.009400000795722008, 'test/loss': 6.486231803894043, 'test/num_examples': 10000, 'score': 463.12793254852295, 'total_duration': 525.1827142238617, 'accumulated_submission_time': 463.12793254852295, 'accumulated_eval_time': 61.97929525375366, 'accumulated_logging_time': 0.0289614200592041, 'global_step': 897, 'preemption_count': 0}), (1840, {'train/accuracy': 0.04189452901482582, 'train/loss': 5.843362808227539, 'validation/accuracy': 0.03983999788761139, 'validation/loss': 5.8705878257751465, 'validation/num_examples': 50000, 'test/accuracy': 0.033400002866983414, 'test/loss': 5.988006591796875, 'test/num_examples': 10000, 'score': 883.3025920391083, 'total_duration': 966.8372595310211, 'accumulated_submission_time': 883.3025920391083, 'accumulated_eval_time': 83.38200998306274, 'accumulated_logging_time': 0.056799888610839844, 'global_step': 1840, 'preemption_count': 0}), (2785, {'train/accuracy': 0.07291015237569809, 'train/loss': 5.394949436187744, 'validation/accuracy': 0.06604000180959702, 'validation/loss': 5.446426868438721, 'validation/num_examples': 50000, 'test/accuracy': 0.049400001764297485, 'test/loss': 5.625874996185303, 'test/num_examples': 10000, 'score': 1303.642668247223, 'total_duration': 1409.0454559326172, 'accumulated_submission_time': 1303.642668247223, 'accumulated_eval_time': 105.17406916618347, 'accumulated_logging_time': 0.08444643020629883, 'global_step': 2785, 'preemption_count': 0}), (3730, {'train/accuracy': 0.10576171427965164, 'train/loss': 5.051905155181885, 'validation/accuracy': 0.09595999866724014, 'validation/loss': 5.101688385009766, 'validation/num_examples': 50000, 'test/accuracy': 0.07540000230073929, 'test/loss': 5.3332037925720215, 'test/num_examples': 10000, 'score': 1723.5683901309967, 'total_duration': 1850.8338613510132, 'accumulated_submission_time': 1723.5683901309967, 'accumulated_eval_time': 126.95962452888489, 'accumulated_logging_time': 0.11334896087646484, 'global_step': 3730, 'preemption_count': 0}), (4667, {'train/accuracy': 0.13990233838558197, 'train/loss': 4.692224025726318, 'validation/accuracy': 0.12933999300003052, 'validation/loss': 4.748983860015869, 'validation/num_examples': 50000, 'test/accuracy': 0.09700000286102295, 'test/loss': 5.055785179138184, 'test/num_examples': 10000, 'score': 2143.667026281357, 'total_duration': 2292.9393298625946, 'accumulated_submission_time': 2143.667026281357, 'accumulated_eval_time': 148.88687324523926, 'accumulated_logging_time': 0.14498591423034668, 'global_step': 4667, 'preemption_count': 0}), (5607, {'train/accuracy': 0.19443358480930328, 'train/loss': 4.300422191619873, 'validation/accuracy': 0.17613999545574188, 'validation/loss': 4.393296718597412, 'validation/num_examples': 50000, 'test/accuracy': 0.13379999995231628, 'test/loss': 4.728091716766357, 'test/num_examples': 10000, 'score': 2563.898540019989, 'total_duration': 2735.196244239807, 'accumulated_submission_time': 2563.898540019989, 'accumulated_eval_time': 170.83421778678894, 'accumulated_logging_time': 0.17491602897644043, 'global_step': 5607, 'preemption_count': 0}), (6544, {'train/accuracy': 0.2354101538658142, 'train/loss': 3.982255458831787, 'validation/accuracy': 0.21879999339580536, 'validation/loss': 4.063016891479492, 'validation/num_examples': 50000, 'test/accuracy': 0.16630001366138458, 'test/loss': 4.450675010681152, 'test/num_examples': 10000, 'score': 2984.3140094280243, 'total_duration': 3179.2775950431824, 'accumulated_submission_time': 2984.3140094280243, 'accumulated_eval_time': 194.42507314682007, 'accumulated_logging_time': 0.2019352912902832, 'global_step': 6544, 'preemption_count': 0}), (7483, {'train/accuracy': 0.2684960961341858, 'train/loss': 3.741316556930542, 'validation/accuracy': 0.25015997886657715, 'validation/loss': 3.8385329246520996, 'validation/num_examples': 50000, 'test/accuracy': 0.19110001623630524, 'test/loss': 4.25723123550415, 'test/num_examples': 10000, 'score': 3404.61616230011, 'total_duration': 3630.5774047374725, 'accumulated_submission_time': 3404.61616230011, 'accumulated_eval_time': 225.34209632873535, 'accumulated_logging_time': 0.2349696159362793, 'global_step': 7483, 'preemption_count': 0}), (8423, {'train/accuracy': 0.31019529700279236, 'train/loss': 3.4238476753234863, 'validation/accuracy': 0.2865000069141388, 'validation/loss': 3.551166534423828, 'validation/num_examples': 50000, 'test/accuracy': 0.22040000557899475, 'test/loss': 4.016154766082764, 'test/num_examples': 10000, 'score': 3824.698988676071, 'total_duration': 4075.722831964493, 'accumulated_submission_time': 3824.698988676071, 'accumulated_eval_time': 250.32358050346375, 'accumulated_logging_time': 0.2688426971435547, 'global_step': 8423, 'preemption_count': 0}), (9362, {'train/accuracy': 0.36613279581069946, 'train/loss': 3.119126558303833, 'validation/accuracy': 0.3231000006198883, 'validation/loss': 3.3199212551116943, 'validation/num_examples': 50000, 'test/accuracy': 0.25030001997947693, 'test/loss': 3.823859453201294, 'test/num_examples': 10000, 'score': 4244.6240670681, 'total_duration': 4523.069453001022, 'accumulated_submission_time': 4244.6240670681, 'accumulated_eval_time': 277.6685001850128, 'accumulated_logging_time': 0.29800844192504883, 'global_step': 9362, 'preemption_count': 0}), (10299, {'train/accuracy': 0.3646875023841858, 'train/loss': 3.0610296726226807, 'validation/accuracy': 0.3439199924468994, 'validation/loss': 3.1753134727478027, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.6979408264160156, 'test/num_examples': 10000, 'score': 4664.77720451355, 'total_duration': 4973.9177231788635, 'accumulated_submission_time': 4664.77720451355, 'accumulated_eval_time': 308.2666883468628, 'accumulated_logging_time': 0.3484940528869629, 'global_step': 10299, 'preemption_count': 0}), (11238, {'train/accuracy': 0.3969140648841858, 'train/loss': 2.903132915496826, 'validation/accuracy': 0.3653799891471863, 'validation/loss': 3.0522048473358154, 'validation/num_examples': 50000, 'test/accuracy': 0.2824000120162964, 'test/loss': 3.593581438064575, 'test/num_examples': 10000, 'score': 5085.150776386261, 'total_duration': 5422.149684429169, 'accumulated_submission_time': 5085.150776386261, 'accumulated_eval_time': 336.04639506340027, 'accumulated_logging_time': 0.37902069091796875, 'global_step': 11238, 'preemption_count': 0}), (12177, {'train/accuracy': 0.4346679449081421, 'train/loss': 2.682563304901123, 'validation/accuracy': 0.3948400020599365, 'validation/loss': 2.876403570175171, 'validation/num_examples': 50000, 'test/accuracy': 0.3043000102043152, 'test/loss': 3.4292190074920654, 'test/num_examples': 10000, 'score': 5505.0947265625, 'total_duration': 5870.017268896103, 'accumulated_submission_time': 5505.0947265625, 'accumulated_eval_time': 363.88840103149414, 'accumulated_logging_time': 0.41297030448913574, 'global_step': 12177, 'preemption_count': 0}), (13114, {'train/accuracy': 0.4341992139816284, 'train/loss': 2.6847686767578125, 'validation/accuracy': 0.40498000383377075, 'validation/loss': 2.8233370780944824, 'validation/num_examples': 50000, 'test/accuracy': 0.32040002942085266, 'test/loss': 3.36568021774292, 'test/num_examples': 10000, 'score': 5925.405028104782, 'total_duration': 6320.223189115524, 'accumulated_submission_time': 5925.405028104782, 'accumulated_eval_time': 393.6926965713501, 'accumulated_logging_time': 0.45781588554382324, 'global_step': 13114, 'preemption_count': 0}), (14045, {'train/accuracy': 0.4446093738079071, 'train/loss': 2.6638307571411133, 'validation/accuracy': 0.4107399880886078, 'validation/loss': 2.8218674659729004, 'validation/num_examples': 50000, 'test/accuracy': 0.3206000030040741, 'test/loss': 3.385582685470581, 'test/num_examples': 10000, 'score': 6345.643961429596, 'total_duration': 6769.418182611465, 'accumulated_submission_time': 6345.643961429596, 'accumulated_eval_time': 422.5653555393219, 'accumulated_logging_time': 0.495150089263916, 'global_step': 14045, 'preemption_count': 0}), (14974, {'train/accuracy': 0.4664062261581421, 'train/loss': 2.4795210361480713, 'validation/accuracy': 0.4297599792480469, 'validation/loss': 2.666552782058716, 'validation/num_examples': 50000, 'test/accuracy': 0.33500000834465027, 'test/loss': 3.2387516498565674, 'test/num_examples': 10000, 'score': 6765.623802185059, 'total_duration': 7218.315635204315, 'accumulated_submission_time': 6765.623802185059, 'accumulated_eval_time': 451.4034585952759, 'accumulated_logging_time': 0.5285844802856445, 'global_step': 14974, 'preemption_count': 0}), (15908, {'train/accuracy': 0.4827929437160492, 'train/loss': 2.4374914169311523, 'validation/accuracy': 0.4357599914073944, 'validation/loss': 2.6655783653259277, 'validation/num_examples': 50000, 'test/accuracy': 0.33640000224113464, 'test/loss': 3.2438104152679443, 'test/num_examples': 10000, 'score': 7185.726698637009, 'total_duration': 7667.809241294861, 'accumulated_submission_time': 7185.726698637009, 'accumulated_eval_time': 480.71657729148865, 'accumulated_logging_time': 0.5597198009490967, 'global_step': 15908, 'preemption_count': 0}), (16837, {'train/accuracy': 0.4757421910762787, 'train/loss': 2.441497802734375, 'validation/accuracy': 0.4432799816131592, 'validation/loss': 2.600684881210327, 'validation/num_examples': 50000, 'test/accuracy': 0.34950003027915955, 'test/loss': 3.1816678047180176, 'test/num_examples': 10000, 'score': 7605.864891529083, 'total_duration': 8116.923457622528, 'accumulated_submission_time': 7605.864891529083, 'accumulated_eval_time': 509.61567425727844, 'accumulated_logging_time': 0.5903451442718506, 'global_step': 16837, 'preemption_count': 0}), (17761, {'train/accuracy': 0.48970702290534973, 'train/loss': 2.3479857444763184, 'validation/accuracy': 0.4527199864387512, 'validation/loss': 2.532827854156494, 'validation/num_examples': 50000, 'test/accuracy': 0.3516000211238861, 'test/loss': 3.1301140785217285, 'test/num_examples': 10000, 'score': 8025.931247711182, 'total_duration': 8568.302577495575, 'accumulated_submission_time': 8025.931247711182, 'accumulated_eval_time': 540.8512902259827, 'accumulated_logging_time': 0.6215465068817139, 'global_step': 17761, 'preemption_count': 0}), (18693, {'train/accuracy': 0.5077343583106995, 'train/loss': 2.280947208404541, 'validation/accuracy': 0.46017998456954956, 'validation/loss': 2.506450653076172, 'validation/num_examples': 50000, 'test/accuracy': 0.3596000075340271, 'test/loss': 3.1122446060180664, 'test/num_examples': 10000, 'score': 8445.988456726074, 'total_duration': 9019.213403463364, 'accumulated_submission_time': 8445.988456726074, 'accumulated_eval_time': 571.6251528263092, 'accumulated_logging_time': 0.6539442539215088, 'global_step': 18693, 'preemption_count': 0}), (19624, {'train/accuracy': 0.5035741925239563, 'train/loss': 2.2893991470336914, 'validation/accuracy': 0.47039997577667236, 'validation/loss': 2.448329448699951, 'validation/num_examples': 50000, 'test/accuracy': 0.36660000681877136, 'test/loss': 3.0513882637023926, 'test/num_examples': 10000, 'score': 8866.089724779129, 'total_duration': 9470.51671743393, 'accumulated_submission_time': 8866.089724779129, 'accumulated_eval_time': 602.7466416358948, 'accumulated_logging_time': 0.687971830368042, 'global_step': 19624, 'preemption_count': 0}), (20553, {'train/accuracy': 0.5167187452316284, 'train/loss': 2.226701259613037, 'validation/accuracy': 0.4785199761390686, 'validation/loss': 2.39848256111145, 'validation/num_examples': 50000, 'test/accuracy': 0.37620002031326294, 'test/loss': 2.9945783615112305, 'test/num_examples': 10000, 'score': 9286.414787769318, 'total_duration': 9921.960839271545, 'accumulated_submission_time': 9286.414787769318, 'accumulated_eval_time': 633.781985282898, 'accumulated_logging_time': 0.7244741916656494, 'global_step': 20553, 'preemption_count': 0}), (21480, {'train/accuracy': 0.5309960842132568, 'train/loss': 2.138967514038086, 'validation/accuracy': 0.4841599762439728, 'validation/loss': 2.3563876152038574, 'validation/num_examples': 50000, 'test/accuracy': 0.3792000114917755, 'test/loss': 2.9629838466644287, 'test/num_examples': 10000, 'score': 9706.385151386261, 'total_duration': 10374.46075630188, 'accumulated_submission_time': 9706.385151386261, 'accumulated_eval_time': 666.2311322689056, 'accumulated_logging_time': 0.7581882476806641, 'global_step': 21480, 'preemption_count': 0}), (22413, {'train/accuracy': 0.5240234136581421, 'train/loss': 2.189439535140991, 'validation/accuracy': 0.4843199849128723, 'validation/loss': 2.3730432987213135, 'validation/num_examples': 50000, 'test/accuracy': 0.37860003113746643, 'test/loss': 2.9851624965667725, 'test/num_examples': 10000, 'score': 10126.384377241135, 'total_duration': 10824.917355537415, 'accumulated_submission_time': 10126.384377241135, 'accumulated_eval_time': 696.6076049804688, 'accumulated_logging_time': 0.7909915447235107, 'global_step': 22413, 'preemption_count': 0}), (23351, {'train/accuracy': 0.5264257788658142, 'train/loss': 2.1538989543914795, 'validation/accuracy': 0.4958399832248688, 'validation/loss': 2.312279224395752, 'validation/num_examples': 50000, 'test/accuracy': 0.3881000280380249, 'test/loss': 2.9460103511810303, 'test/num_examples': 10000, 'score': 10546.718402862549, 'total_duration': 11276.222237586975, 'accumulated_submission_time': 10546.718402862549, 'accumulated_eval_time': 727.5011661052704, 'accumulated_logging_time': 0.8213632106781006, 'global_step': 23351, 'preemption_count': 0}), (24285, {'train/accuracy': 0.5478906035423279, 'train/loss': 2.0787417888641357, 'validation/accuracy': 0.5089600086212158, 'validation/loss': 2.2755093574523926, 'validation/num_examples': 50000, 'test/accuracy': 0.3977000117301941, 'test/loss': 2.882901906967163, 'test/num_examples': 10000, 'score': 10966.702552080154, 'total_duration': 11725.926349878311, 'accumulated_submission_time': 10966.702552080154, 'accumulated_eval_time': 757.1414499282837, 'accumulated_logging_time': 0.8539583683013916, 'global_step': 24285, 'preemption_count': 0}), (25218, {'train/accuracy': 0.5692187547683716, 'train/loss': 1.966854214668274, 'validation/accuracy': 0.5087199807167053, 'validation/loss': 2.2506370544433594, 'validation/num_examples': 50000, 'test/accuracy': 0.40050002932548523, 'test/loss': 2.87434720993042, 'test/num_examples': 10000, 'score': 11386.934196472168, 'total_duration': 12178.604896306992, 'accumulated_submission_time': 11386.934196472168, 'accumulated_eval_time': 789.5124342441559, 'accumulated_logging_time': 0.8832418918609619, 'global_step': 25218, 'preemption_count': 0}), (26143, {'train/accuracy': 0.546679675579071, 'train/loss': 2.098712205886841, 'validation/accuracy': 0.5127400159835815, 'validation/loss': 2.264395236968994, 'validation/num_examples': 50000, 'test/accuracy': 0.40050002932548523, 'test/loss': 2.883815288543701, 'test/num_examples': 10000, 'score': 11806.891574144363, 'total_duration': 12629.590085029602, 'accumulated_submission_time': 11806.891574144363, 'accumulated_eval_time': 820.4644253253937, 'accumulated_logging_time': 0.9127271175384521, 'global_step': 26143, 'preemption_count': 0}), (27074, {'train/accuracy': 0.561718761920929, 'train/loss': 2.005185127258301, 'validation/accuracy': 0.5216599702835083, 'validation/loss': 2.1846542358398438, 'validation/num_examples': 50000, 'test/accuracy': 0.40570002794265747, 'test/loss': 2.8111982345581055, 'test/num_examples': 10000, 'score': 12227.0956864357, 'total_duration': 13082.901546955109, 'accumulated_submission_time': 12227.0956864357, 'accumulated_eval_time': 853.4955842494965, 'accumulated_logging_time': 0.9416196346282959, 'global_step': 27074, 'preemption_count': 0}), (28005, {'train/accuracy': 0.5656445026397705, 'train/loss': 1.9743703603744507, 'validation/accuracy': 0.5188599824905396, 'validation/loss': 2.1966774463653564, 'validation/num_examples': 50000, 'test/accuracy': 0.4076000154018402, 'test/loss': 2.8148574829101562, 'test/num_examples': 10000, 'score': 12647.427459478378, 'total_duration': 13536.928488254547, 'accumulated_submission_time': 12647.427459478378, 'accumulated_eval_time': 887.1125380992889, 'accumulated_logging_time': 0.9730579853057861, 'global_step': 28005, 'preemption_count': 0}), (28935, {'train/accuracy': 0.560351550579071, 'train/loss': 2.0256459712982178, 'validation/accuracy': 0.5274800062179565, 'validation/loss': 2.1879539489746094, 'validation/num_examples': 50000, 'test/accuracy': 0.41130003333091736, 'test/loss': 2.815878391265869, 'test/num_examples': 10000, 'score': 13067.464753627777, 'total_duration': 13989.526335477829, 'accumulated_submission_time': 13067.464753627777, 'accumulated_eval_time': 919.5988109111786, 'accumulated_logging_time': 1.001277208328247, 'global_step': 28935, 'preemption_count': 0}), (29863, {'train/accuracy': 0.5651562213897705, 'train/loss': 1.994471549987793, 'validation/accuracy': 0.5298799872398376, 'validation/loss': 2.168131113052368, 'validation/num_examples': 50000, 'test/accuracy': 0.41690000891685486, 'test/loss': 2.7868216037750244, 'test/num_examples': 10000, 'score': 13487.82652425766, 'total_duration': 14442.533663749695, 'accumulated_submission_time': 13487.82652425766, 'accumulated_eval_time': 952.1639549732208, 'accumulated_logging_time': 1.035024881362915, 'global_step': 29863, 'preemption_count': 0}), (30794, {'train/accuracy': 0.5801953077316284, 'train/loss': 1.9031869173049927, 'validation/accuracy': 0.5334599614143372, 'validation/loss': 2.116943836212158, 'validation/num_examples': 50000, 'test/accuracy': 0.4204000234603882, 'test/loss': 2.7451517581939697, 'test/num_examples': 10000, 'score': 13908.166835308075, 'total_duration': 14896.180969238281, 'accumulated_submission_time': 13908.166835308075, 'accumulated_eval_time': 985.389402627945, 'accumulated_logging_time': 1.0702705383300781, 'global_step': 30794, 'preemption_count': 0}), (31726, {'train/accuracy': 0.5763476490974426, 'train/loss': 1.9141044616699219, 'validation/accuracy': 0.5375999808311462, 'validation/loss': 2.1075029373168945, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.7345998287200928, 'test/num_examples': 10000, 'score': 14328.138955116272, 'total_duration': 15349.214524507523, 'accumulated_submission_time': 14328.138955116272, 'accumulated_eval_time': 1018.3643229007721, 'accumulated_logging_time': 1.1095900535583496, 'global_step': 31726, 'preemption_count': 0}), (32657, {'train/accuracy': 0.5758984088897705, 'train/loss': 1.9533084630966187, 'validation/accuracy': 0.5400399565696716, 'validation/loss': 2.1200501918792725, 'validation/num_examples': 50000, 'test/accuracy': 0.421500027179718, 'test/loss': 2.763683319091797, 'test/num_examples': 10000, 'score': 14748.117035627365, 'total_duration': 15801.935980558395, 'accumulated_submission_time': 14748.117035627365, 'accumulated_eval_time': 1051.026505947113, 'accumulated_logging_time': 1.14223051071167, 'global_step': 32657, 'preemption_count': 0}), (33589, {'train/accuracy': 0.5841405987739563, 'train/loss': 1.894763708114624, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 2.0790975093841553, 'validation/num_examples': 50000, 'test/accuracy': 0.42670002579689026, 'test/loss': 2.706271171569824, 'test/num_examples': 10000, 'score': 15168.483533620834, 'total_duration': 16256.044842720032, 'accumulated_submission_time': 15168.483533620834, 'accumulated_eval_time': 1084.6919131278992, 'accumulated_logging_time': 1.1724753379821777, 'global_step': 33589, 'preemption_count': 0}), (34518, {'train/accuracy': 0.6061328053474426, 'train/loss': 1.7760976552963257, 'validation/accuracy': 0.545799970626831, 'validation/loss': 2.0625627040863037, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.6870129108428955, 'test/num_examples': 10000, 'score': 15588.672712087631, 'total_duration': 16708.648688793182, 'accumulated_submission_time': 15588.672712087631, 'accumulated_eval_time': 1117.0305380821228, 'accumulated_logging_time': 1.202235460281372, 'global_step': 34518, 'preemption_count': 0}), (35448, {'train/accuracy': 0.584667980670929, 'train/loss': 1.8941220045089722, 'validation/accuracy': 0.5425599813461304, 'validation/loss': 2.086238145828247, 'validation/num_examples': 50000, 'test/accuracy': 0.4293000102043152, 'test/loss': 2.7089946269989014, 'test/num_examples': 10000, 'score': 16009.01386475563, 'total_duration': 17163.030723571777, 'accumulated_submission_time': 16009.01386475563, 'accumulated_eval_time': 1150.9917540550232, 'accumulated_logging_time': 1.2357745170593262, 'global_step': 35448, 'preemption_count': 0}), (36379, {'train/accuracy': 0.5904101133346558, 'train/loss': 1.8927258253097534, 'validation/accuracy': 0.5471999645233154, 'validation/loss': 2.0837810039520264, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.709493398666382, 'test/num_examples': 10000, 'score': 16429.294507026672, 'total_duration': 17616.62527346611, 'accumulated_submission_time': 16429.294507026672, 'accumulated_eval_time': 1184.2244091033936, 'accumulated_logging_time': 1.2700214385986328, 'global_step': 36379, 'preemption_count': 0}), (37309, {'train/accuracy': 0.6047655940055847, 'train/loss': 1.7837172746658325, 'validation/accuracy': 0.556119978427887, 'validation/loss': 2.017679214477539, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.6612629890441895, 'test/num_examples': 10000, 'score': 16849.5801718235, 'total_duration': 18069.852862596512, 'accumulated_submission_time': 16849.5801718235, 'accumulated_eval_time': 1217.0834302902222, 'accumulated_logging_time': 1.3062067031860352, 'global_step': 37309, 'preemption_count': 0}), (38237, {'train/accuracy': 0.592089831829071, 'train/loss': 1.8625539541244507, 'validation/accuracy': 0.5530399680137634, 'validation/loss': 2.0462992191314697, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.677217721939087, 'test/num_examples': 10000, 'score': 17269.676019191742, 'total_duration': 18523.4055621624, 'accumulated_submission_time': 17269.676019191742, 'accumulated_eval_time': 1250.4606716632843, 'accumulated_logging_time': 1.3389804363250732, 'global_step': 38237, 'preemption_count': 0}), (39165, {'train/accuracy': 0.5917382836341858, 'train/loss': 1.8414897918701172, 'validation/accuracy': 0.5532400012016296, 'validation/loss': 2.0242159366607666, 'validation/num_examples': 50000, 'test/accuracy': 0.43560001254081726, 'test/loss': 2.6622133255004883, 'test/num_examples': 10000, 'score': 17689.861362457275, 'total_duration': 18976.674981355667, 'accumulated_submission_time': 17689.861362457275, 'accumulated_eval_time': 1283.460999250412, 'accumulated_logging_time': 1.3763537406921387, 'global_step': 39165, 'preemption_count': 0}), (40095, {'train/accuracy': 0.6004687547683716, 'train/loss': 1.7826272249221802, 'validation/accuracy': 0.5520200133323669, 'validation/loss': 2.0079150199890137, 'validation/num_examples': 50000, 'test/accuracy': 0.4385000169277191, 'test/loss': 2.6310582160949707, 'test/num_examples': 10000, 'score': 18110.095131635666, 'total_duration': 19429.93856573105, 'accumulated_submission_time': 18110.095131635666, 'accumulated_eval_time': 1316.411651134491, 'accumulated_logging_time': 1.409106969833374, 'global_step': 40095, 'preemption_count': 0}), (41025, {'train/accuracy': 0.6124023199081421, 'train/loss': 1.7426855564117432, 'validation/accuracy': 0.5575399994850159, 'validation/loss': 1.9958285093307495, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.604980707168579, 'test/num_examples': 10000, 'score': 18530.15157198906, 'total_duration': 19882.56823849678, 'accumulated_submission_time': 18530.15157198906, 'accumulated_eval_time': 1348.9026863574982, 'accumulated_logging_time': 1.444082498550415, 'global_step': 41025, 'preemption_count': 0}), (41953, {'train/accuracy': 0.6009570360183716, 'train/loss': 1.8395626544952393, 'validation/accuracy': 0.5607799887657166, 'validation/loss': 2.0273380279541016, 'validation/num_examples': 50000, 'test/accuracy': 0.4416000247001648, 'test/loss': 2.6486172676086426, 'test/num_examples': 10000, 'score': 18950.41338968277, 'total_duration': 20336.153192281723, 'accumulated_submission_time': 18950.41338968277, 'accumulated_eval_time': 1382.1475772857666, 'accumulated_logging_time': 1.474935531616211, 'global_step': 41953, 'preemption_count': 0}), (42883, {'train/accuracy': 0.60546875, 'train/loss': 1.7532869577407837, 'validation/accuracy': 0.5643599629402161, 'validation/loss': 1.944724678993225, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.5693717002868652, 'test/num_examples': 10000, 'score': 19370.686596870422, 'total_duration': 20788.01582312584, 'accumulated_submission_time': 19370.686596870422, 'accumulated_eval_time': 1413.6585688591003, 'accumulated_logging_time': 1.506608247756958, 'global_step': 42883, 'preemption_count': 0}), (43812, {'train/accuracy': 0.6259570121765137, 'train/loss': 1.6537854671478271, 'validation/accuracy': 0.5655800104141235, 'validation/loss': 1.9352000951766968, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.540505886077881, 'test/num_examples': 10000, 'score': 19790.67938184738, 'total_duration': 21241.678597688675, 'accumulated_submission_time': 19790.67938184738, 'accumulated_eval_time': 1447.244621515274, 'accumulated_logging_time': 1.5436184406280518, 'global_step': 43812, 'preemption_count': 0}), (44744, {'train/accuracy': 0.60791015625, 'train/loss': 1.7625596523284912, 'validation/accuracy': 0.565779983997345, 'validation/loss': 1.9503344297409058, 'validation/num_examples': 50000, 'test/accuracy': 0.454800009727478, 'test/loss': 2.5629172325134277, 'test/num_examples': 10000, 'score': 20210.85508942604, 'total_duration': 21694.710247278214, 'accumulated_submission_time': 20210.85508942604, 'accumulated_eval_time': 1480.019334077835, 'accumulated_logging_time': 1.577929973602295, 'global_step': 44744, 'preemption_count': 0}), (45675, {'train/accuracy': 0.6089062094688416, 'train/loss': 1.7846567630767822, 'validation/accuracy': 0.568619966506958, 'validation/loss': 1.9842305183410645, 'validation/num_examples': 50000, 'test/accuracy': 0.45410001277923584, 'test/loss': 2.59224271774292, 'test/num_examples': 10000, 'score': 20630.804674863815, 'total_duration': 22148.282872200012, 'accumulated_submission_time': 20630.804674863815, 'accumulated_eval_time': 1513.5652458667755, 'accumulated_logging_time': 1.6084258556365967, 'global_step': 45675, 'preemption_count': 0}), (46604, {'train/accuracy': 0.619433581829071, 'train/loss': 1.6914805173873901, 'validation/accuracy': 0.5701199769973755, 'validation/loss': 1.9323407411575317, 'validation/num_examples': 50000, 'test/accuracy': 0.4522000253200531, 'test/loss': 2.561401605606079, 'test/num_examples': 10000, 'score': 21051.006894111633, 'total_duration': 22601.638983488083, 'accumulated_submission_time': 21051.006894111633, 'accumulated_eval_time': 1546.635691165924, 'accumulated_logging_time': 1.6452360153198242, 'global_step': 46604, 'preemption_count': 0}), (47533, {'train/accuracy': 0.6141015291213989, 'train/loss': 1.700886845588684, 'validation/accuracy': 0.5776199698448181, 'validation/loss': 1.8775568008422852, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.502983331680298, 'test/num_examples': 10000, 'score': 21470.933007240295, 'total_duration': 23052.105201005936, 'accumulated_submission_time': 21470.933007240295, 'accumulated_eval_time': 1577.0918953418732, 'accumulated_logging_time': 1.681839942932129, 'global_step': 47533, 'preemption_count': 0}), (48459, {'train/accuracy': 0.6114843487739563, 'train/loss': 1.7474989891052246, 'validation/accuracy': 0.570580005645752, 'validation/loss': 1.9303126335144043, 'validation/num_examples': 50000, 'test/accuracy': 0.4547000229358673, 'test/loss': 2.5657715797424316, 'test/num_examples': 10000, 'score': 21891.246530532837, 'total_duration': 23505.38790154457, 'accumulated_submission_time': 21891.246530532837, 'accumulated_eval_time': 1609.9773399829865, 'accumulated_logging_time': 1.718390703201294, 'global_step': 48459, 'preemption_count': 0}), (49388, {'train/accuracy': 0.6169726252555847, 'train/loss': 1.6989887952804565, 'validation/accuracy': 0.5762199759483337, 'validation/loss': 1.9017544984817505, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.5268452167510986, 'test/num_examples': 10000, 'score': 22311.626733779907, 'total_duration': 23959.685015916824, 'accumulated_submission_time': 22311.626733779907, 'accumulated_eval_time': 1643.8127937316895, 'accumulated_logging_time': 1.7531659603118896, 'global_step': 49388, 'preemption_count': 0}), (50318, {'train/accuracy': 0.6458203196525574, 'train/loss': 1.5972343683242798, 'validation/accuracy': 0.5758799910545349, 'validation/loss': 1.9063783884048462, 'validation/num_examples': 50000, 'test/accuracy': 0.4619000256061554, 'test/loss': 2.520209550857544, 'test/num_examples': 10000, 'score': 22731.99612236023, 'total_duration': 24413.784294366837, 'accumulated_submission_time': 22731.99612236023, 'accumulated_eval_time': 1677.461537361145, 'accumulated_logging_time': 1.7863349914550781, 'global_step': 50318, 'preemption_count': 0}), (51246, {'train/accuracy': 0.613476574420929, 'train/loss': 1.7316758632659912, 'validation/accuracy': 0.5759199857711792, 'validation/loss': 1.9049936532974243, 'validation/num_examples': 50000, 'test/accuracy': 0.4612000286579132, 'test/loss': 2.528799057006836, 'test/num_examples': 10000, 'score': 23151.934993743896, 'total_duration': 24866.39386487007, 'accumulated_submission_time': 23151.934993743896, 'accumulated_eval_time': 1710.0534682273865, 'accumulated_logging_time': 1.8191730976104736, 'global_step': 51246, 'preemption_count': 0}), (52175, {'train/accuracy': 0.6240624785423279, 'train/loss': 1.6678780317306519, 'validation/accuracy': 0.5776399970054626, 'validation/loss': 1.8867851495742798, 'validation/num_examples': 50000, 'test/accuracy': 0.465800017118454, 'test/loss': 2.5022594928741455, 'test/num_examples': 10000, 'score': 23571.889559984207, 'total_duration': 25319.559384584427, 'accumulated_submission_time': 23571.889559984207, 'accumulated_eval_time': 1743.183295249939, 'accumulated_logging_time': 1.854111909866333, 'global_step': 52175, 'preemption_count': 0}), (53104, {'train/accuracy': 0.6388866901397705, 'train/loss': 1.6128058433532715, 'validation/accuracy': 0.5816599726676941, 'validation/loss': 1.8688938617706299, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.480672836303711, 'test/num_examples': 10000, 'score': 23992.10686635971, 'total_duration': 25771.13542485237, 'accumulated_submission_time': 23992.10686635971, 'accumulated_eval_time': 1774.4506244659424, 'accumulated_logging_time': 1.8986506462097168, 'global_step': 53104, 'preemption_count': 0}), (54034, {'train/accuracy': 0.6194140315055847, 'train/loss': 1.7486610412597656, 'validation/accuracy': 0.5802599787712097, 'validation/loss': 1.9259588718414307, 'validation/num_examples': 50000, 'test/accuracy': 0.460500031709671, 'test/loss': 2.5516300201416016, 'test/num_examples': 10000, 'score': 24412.24143385887, 'total_duration': 26224.815851688385, 'accumulated_submission_time': 24412.24143385887, 'accumulated_eval_time': 1807.9144456386566, 'accumulated_logging_time': 1.9343111515045166, 'global_step': 54034, 'preemption_count': 0}), (54963, {'train/accuracy': 0.6254101395606995, 'train/loss': 1.6814249753952026, 'validation/accuracy': 0.5814599990844727, 'validation/loss': 1.8866448402404785, 'validation/num_examples': 50000, 'test/accuracy': 0.46650001406669617, 'test/loss': 2.5163378715515137, 'test/num_examples': 10000, 'score': 24832.460919380188, 'total_duration': 26678.31806921959, 'accumulated_submission_time': 24832.460919380188, 'accumulated_eval_time': 1841.1145780086517, 'accumulated_logging_time': 1.9702081680297852, 'global_step': 54963, 'preemption_count': 0}), (55891, {'train/accuracy': 0.6341210603713989, 'train/loss': 1.6259918212890625, 'validation/accuracy': 0.5867399573326111, 'validation/loss': 1.858447790145874, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4755938053131104, 'test/num_examples': 10000, 'score': 25252.844719171524, 'total_duration': 27131.659603118896, 'accumulated_submission_time': 25252.844719171524, 'accumulated_eval_time': 1873.9938821792603, 'accumulated_logging_time': 2.003046751022339, 'global_step': 55891, 'preemption_count': 0}), (56820, {'train/accuracy': 0.6281445026397705, 'train/loss': 1.6840720176696777, 'validation/accuracy': 0.5877199769020081, 'validation/loss': 1.8711938858032227, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.5007386207580566, 'test/num_examples': 10000, 'score': 25672.96810555458, 'total_duration': 27585.487575769424, 'accumulated_submission_time': 25672.96810555458, 'accumulated_eval_time': 1907.6127750873566, 'accumulated_logging_time': 2.042140007019043, 'global_step': 56820, 'preemption_count': 0}), (57752, {'train/accuracy': 0.6265624761581421, 'train/loss': 1.685849905014038, 'validation/accuracy': 0.5827000141143799, 'validation/loss': 1.870428204536438, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.495413303375244, 'test/num_examples': 10000, 'score': 26093.019316911697, 'total_duration': 28038.671048879623, 'accumulated_submission_time': 26093.019316911697, 'accumulated_eval_time': 1940.665373325348, 'accumulated_logging_time': 2.074948310852051, 'global_step': 57752, 'preemption_count': 0}), (58682, {'train/accuracy': 0.6369921565055847, 'train/loss': 1.6104422807693481, 'validation/accuracy': 0.5866000056266785, 'validation/loss': 1.8379977941513062, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.459522008895874, 'test/num_examples': 10000, 'score': 26513.309331178665, 'total_duration': 28492.005694627762, 'accumulated_submission_time': 26513.309331178665, 'accumulated_eval_time': 1973.6229138374329, 'accumulated_logging_time': 2.110410690307617, 'global_step': 58682, 'preemption_count': 0}), (59613, {'train/accuracy': 0.6574023365974426, 'train/loss': 1.5401033163070679, 'validation/accuracy': 0.5890399813652039, 'validation/loss': 1.8447201251983643, 'validation/num_examples': 50000, 'test/accuracy': 0.4741000235080719, 'test/loss': 2.4697728157043457, 'test/num_examples': 10000, 'score': 26933.473502397537, 'total_duration': 28945.743367433548, 'accumulated_submission_time': 26933.473502397537, 'accumulated_eval_time': 2007.1134917736053, 'accumulated_logging_time': 2.146523952484131, 'global_step': 59613, 'preemption_count': 0}), (60543, {'train/accuracy': 0.6248632669448853, 'train/loss': 1.6834291219711304, 'validation/accuracy': 0.589419960975647, 'validation/loss': 1.868920922279358, 'validation/num_examples': 50000, 'test/accuracy': 0.4699000120162964, 'test/loss': 2.503729820251465, 'test/num_examples': 10000, 'score': 27353.815348386765, 'total_duration': 29398.575788736343, 'accumulated_submission_time': 27353.815348386765, 'accumulated_eval_time': 2039.5238630771637, 'accumulated_logging_time': 2.1801421642303467, 'global_step': 60543, 'preemption_count': 0}), (61472, {'train/accuracy': 0.6370507478713989, 'train/loss': 1.664278268814087, 'validation/accuracy': 0.5917400121688843, 'validation/loss': 1.8678573369979858, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.495166063308716, 'test/num_examples': 10000, 'score': 27774.116877794266, 'total_duration': 29849.922494888306, 'accumulated_submission_time': 27774.116877794266, 'accumulated_eval_time': 2070.4793763160706, 'accumulated_logging_time': 2.2234983444213867, 'global_step': 61472, 'preemption_count': 0}), (62401, {'train/accuracy': 0.6454882621765137, 'train/loss': 1.614796757698059, 'validation/accuracy': 0.5929200053215027, 'validation/loss': 1.8566802740097046, 'validation/num_examples': 50000, 'test/accuracy': 0.4774000346660614, 'test/loss': 2.467177152633667, 'test/num_examples': 10000, 'score': 28194.661197185516, 'total_duration': 30304.258091688156, 'accumulated_submission_time': 28194.661197185516, 'accumulated_eval_time': 2104.190548181534, 'accumulated_logging_time': 2.257309675216675, 'global_step': 62401, 'preemption_count': 0}), (63332, {'train/accuracy': 0.63623046875, 'train/loss': 1.6195902824401855, 'validation/accuracy': 0.5971999764442444, 'validation/loss': 1.8102082014083862, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.4436748027801514, 'test/num_examples': 10000, 'score': 28614.799318790436, 'total_duration': 30758.743307828903, 'accumulated_submission_time': 28614.799318790436, 'accumulated_eval_time': 2138.4504079818726, 'accumulated_logging_time': 2.2979867458343506, 'global_step': 63332, 'preemption_count': 0}), (64262, {'train/accuracy': 0.6394140720367432, 'train/loss': 1.6281222105026245, 'validation/accuracy': 0.5967199802398682, 'validation/loss': 1.8196301460266113, 'validation/num_examples': 50000, 'test/accuracy': 0.47840002179145813, 'test/loss': 2.4457361698150635, 'test/num_examples': 10000, 'score': 29035.028126716614, 'total_duration': 31212.194100141525, 'accumulated_submission_time': 29035.028126716614, 'accumulated_eval_time': 2171.592320203781, 'accumulated_logging_time': 2.3315210342407227, 'global_step': 64262, 'preemption_count': 0}), (65192, {'train/accuracy': 0.6471484303474426, 'train/loss': 1.5985087156295776, 'validation/accuracy': 0.5951799750328064, 'validation/loss': 1.8246562480926514, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.464690685272217, 'test/num_examples': 10000, 'score': 29454.9858648777, 'total_duration': 31665.85106754303, 'accumulated_submission_time': 29454.9858648777, 'accumulated_eval_time': 2205.211054801941, 'accumulated_logging_time': 2.3653695583343506, 'global_step': 65192, 'preemption_count': 0}), (66119, {'train/accuracy': 0.6446288824081421, 'train/loss': 1.5758085250854492, 'validation/accuracy': 0.6003199815750122, 'validation/loss': 1.7816158533096313, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.4119138717651367, 'test/num_examples': 10000, 'score': 29875.157160520554, 'total_duration': 32119.37609243393, 'accumulated_submission_time': 29875.157160520554, 'accumulated_eval_time': 2238.4791843891144, 'accumulated_logging_time': 2.404351234436035, 'global_step': 66119, 'preemption_count': 0}), (67048, {'train/accuracy': 0.6425976157188416, 'train/loss': 1.5925278663635254, 'validation/accuracy': 0.6013799905776978, 'validation/loss': 1.785370945930481, 'validation/num_examples': 50000, 'test/accuracy': 0.4790000319480896, 'test/loss': 2.427741765975952, 'test/num_examples': 10000, 'score': 30295.264727830887, 'total_duration': 32573.9971203804, 'accumulated_submission_time': 30295.264727830887, 'accumulated_eval_time': 2272.9096236228943, 'accumulated_logging_time': 2.440931558609009, 'global_step': 67048, 'preemption_count': 0}), (67979, {'train/accuracy': 0.6479296684265137, 'train/loss': 1.6087180376052856, 'validation/accuracy': 0.5970799922943115, 'validation/loss': 1.8276971578598022, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.4544215202331543, 'test/num_examples': 10000, 'score': 30715.514212608337, 'total_duration': 33025.66438269615, 'accumulated_submission_time': 30715.514212608337, 'accumulated_eval_time': 2304.2412803173065, 'accumulated_logging_time': 2.480074882507324, 'global_step': 67979, 'preemption_count': 0}), (68905, {'train/accuracy': 0.6702734231948853, 'train/loss': 1.4906214475631714, 'validation/accuracy': 0.6045799851417542, 'validation/loss': 1.7795348167419434, 'validation/num_examples': 50000, 'test/accuracy': 0.49320003390312195, 'test/loss': 2.402475595474243, 'test/num_examples': 10000, 'score': 31135.559599637985, 'total_duration': 33478.6574652195, 'accumulated_submission_time': 31135.559599637985, 'accumulated_eval_time': 2337.104706287384, 'accumulated_logging_time': 2.5181806087493896, 'global_step': 68905, 'preemption_count': 0}), (69834, {'train/accuracy': 0.6407226324081421, 'train/loss': 1.5906224250793457, 'validation/accuracy': 0.6022399663925171, 'validation/loss': 1.764672040939331, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.3967788219451904, 'test/num_examples': 10000, 'score': 31555.447756052017, 'total_duration': 33931.170246362686, 'accumulated_submission_time': 31555.447756052017, 'accumulated_eval_time': 2369.4204025268555, 'accumulated_logging_time': 2.779877185821533, 'global_step': 69834, 'preemption_count': 0}), (70761, {'train/accuracy': 0.6514452695846558, 'train/loss': 1.564841866493225, 'validation/accuracy': 0.6045599579811096, 'validation/loss': 1.7766313552856445, 'validation/num_examples': 50000, 'test/accuracy': 0.4848000109195709, 'test/loss': 2.404025077819824, 'test/num_examples': 10000, 'score': 31975.477875947952, 'total_duration': 34384.702178001404, 'accumulated_submission_time': 31975.477875947952, 'accumulated_eval_time': 2402.840269088745, 'accumulated_logging_time': 2.8157126903533936, 'global_step': 70761, 'preemption_count': 0}), (71688, {'train/accuracy': 0.6606835722923279, 'train/loss': 1.5274606943130493, 'validation/accuracy': 0.6034600138664246, 'validation/loss': 1.7711652517318726, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.400637149810791, 'test/num_examples': 10000, 'score': 32395.610315799713, 'total_duration': 34838.37434220314, 'accumulated_submission_time': 32395.610315799713, 'accumulated_eval_time': 2436.2950756549835, 'accumulated_logging_time': 2.854088068008423, 'global_step': 71688, 'preemption_count': 0}), (72615, {'train/accuracy': 0.6470116972923279, 'train/loss': 1.589882493019104, 'validation/accuracy': 0.6062799692153931, 'validation/loss': 1.779320240020752, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.4277710914611816, 'test/num_examples': 10000, 'score': 32815.5456404686, 'total_duration': 35290.03398799896, 'accumulated_submission_time': 32815.5456404686, 'accumulated_eval_time': 2467.9322276115417, 'accumulated_logging_time': 2.8944272994995117, 'global_step': 72615, 'preemption_count': 0}), (73541, {'train/accuracy': 0.6502929329872131, 'train/loss': 1.571126103401184, 'validation/accuracy': 0.6121199727058411, 'validation/loss': 1.750990629196167, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.3877408504486084, 'test/num_examples': 10000, 'score': 33235.80899262428, 'total_duration': 35743.71882414818, 'accumulated_submission_time': 33235.80899262428, 'accumulated_eval_time': 2501.2704651355743, 'accumulated_logging_time': 2.9318900108337402, 'global_step': 73541, 'preemption_count': 0}), (74470, {'train/accuracy': 0.6590234041213989, 'train/loss': 1.541833758354187, 'validation/accuracy': 0.6091399788856506, 'validation/loss': 1.7622010707855225, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.3940000534057617, 'test/num_examples': 10000, 'score': 33655.84841275215, 'total_duration': 36197.356711387634, 'accumulated_submission_time': 33655.84841275215, 'accumulated_eval_time': 2534.7862520217896, 'accumulated_logging_time': 2.968616485595703, 'global_step': 74470, 'preemption_count': 0}), (75400, {'train/accuracy': 0.6696093678474426, 'train/loss': 1.4805610179901123, 'validation/accuracy': 0.6134999990463257, 'validation/loss': 1.732527732849121, 'validation/num_examples': 50000, 'test/accuracy': 0.4926000237464905, 'test/loss': 2.362853527069092, 'test/num_examples': 10000, 'score': 34076.092334747314, 'total_duration': 36651.11418533325, 'accumulated_submission_time': 34076.092334747314, 'accumulated_eval_time': 2568.215485572815, 'accumulated_logging_time': 3.006901502609253, 'global_step': 75400, 'preemption_count': 0}), (76330, {'train/accuracy': 0.65869140625, 'train/loss': 1.5325862169265747, 'validation/accuracy': 0.614300012588501, 'validation/loss': 1.7345703840255737, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.3532207012176514, 'test/num_examples': 10000, 'score': 34496.37813591957, 'total_duration': 37106.03781723976, 'accumulated_submission_time': 34496.37813591957, 'accumulated_eval_time': 2602.766112804413, 'accumulated_logging_time': 3.0473546981811523, 'global_step': 76330, 'preemption_count': 0}), (77258, {'train/accuracy': 0.6606054306030273, 'train/loss': 1.5113722085952759, 'validation/accuracy': 0.6152399778366089, 'validation/loss': 1.7202774286270142, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.351444721221924, 'test/num_examples': 10000, 'score': 34916.623254299164, 'total_duration': 37559.12559890747, 'accumulated_submission_time': 34916.623254299164, 'accumulated_eval_time': 2635.522970676422, 'accumulated_logging_time': 3.0869641304016113, 'global_step': 77258, 'preemption_count': 0}), (78184, {'train/accuracy': 0.6808788776397705, 'train/loss': 1.4191409349441528, 'validation/accuracy': 0.6181600093841553, 'validation/loss': 1.7033716440200806, 'validation/num_examples': 50000, 'test/accuracy': 0.5019000172615051, 'test/loss': 2.3303937911987305, 'test/num_examples': 10000, 'score': 35336.7923810482, 'total_duration': 38010.754996299744, 'accumulated_submission_time': 35336.7923810482, 'accumulated_eval_time': 2666.8984639644623, 'accumulated_logging_time': 3.1255970001220703, 'global_step': 78184, 'preemption_count': 0}), (79112, {'train/accuracy': 0.661816418170929, 'train/loss': 1.5143336057662964, 'validation/accuracy': 0.6148399710655212, 'validation/loss': 1.7183010578155518, 'validation/num_examples': 50000, 'test/accuracy': 0.49240002036094666, 'test/loss': 2.3449583053588867, 'test/num_examples': 10000, 'score': 35757.1461520195, 'total_duration': 38464.51617407799, 'accumulated_submission_time': 35757.1461520195, 'accumulated_eval_time': 2700.217987060547, 'accumulated_logging_time': 3.1663384437561035, 'global_step': 79112, 'preemption_count': 0}), (80043, {'train/accuracy': 0.664746105670929, 'train/loss': 1.5105516910552979, 'validation/accuracy': 0.615339994430542, 'validation/loss': 1.7205344438552856, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.373055934906006, 'test/num_examples': 10000, 'score': 36177.24590039253, 'total_duration': 38918.11818599701, 'accumulated_submission_time': 36177.24590039253, 'accumulated_eval_time': 2733.6324348449707, 'accumulated_logging_time': 3.2079083919525146, 'global_step': 80043, 'preemption_count': 0}), (80972, {'train/accuracy': 0.6741796731948853, 'train/loss': 1.4529361724853516, 'validation/accuracy': 0.6193599700927734, 'validation/loss': 1.702526330947876, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.3454010486602783, 'test/num_examples': 10000, 'score': 36597.27389025688, 'total_duration': 39371.64637541771, 'accumulated_submission_time': 36597.27389025688, 'accumulated_eval_time': 2767.0463218688965, 'accumulated_logging_time': 3.2479307651519775, 'global_step': 80972, 'preemption_count': 0}), (81899, {'train/accuracy': 0.667773425579071, 'train/loss': 1.5025302171707153, 'validation/accuracy': 0.622759997844696, 'validation/loss': 1.6995172500610352, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.336276054382324, 'test/num_examples': 10000, 'score': 37017.20155906677, 'total_duration': 39824.93992829323, 'accumulated_submission_time': 37017.20155906677, 'accumulated_eval_time': 2800.32315158844, 'accumulated_logging_time': 3.2903239727020264, 'global_step': 81899, 'preemption_count': 0}), (82829, {'train/accuracy': 0.6694530844688416, 'train/loss': 1.4893957376480103, 'validation/accuracy': 0.6229199767112732, 'validation/loss': 1.6935948133468628, 'validation/num_examples': 50000, 'test/accuracy': 0.5026000142097473, 'test/loss': 2.331036329269409, 'test/num_examples': 10000, 'score': 37437.302568912506, 'total_duration': 40277.142501831055, 'accumulated_submission_time': 37437.302568912506, 'accumulated_eval_time': 2832.334250688553, 'accumulated_logging_time': 3.334193706512451, 'global_step': 82829, 'preemption_count': 0}), (83758, {'train/accuracy': 0.6728710532188416, 'train/loss': 1.4168633222579956, 'validation/accuracy': 0.626800000667572, 'validation/loss': 1.644349455833435, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.2819135189056396, 'test/num_examples': 10000, 'score': 37857.52192592621, 'total_duration': 40730.55080986023, 'accumulated_submission_time': 37857.52192592621, 'accumulated_eval_time': 2865.438676595688, 'accumulated_logging_time': 3.372976303100586, 'global_step': 83758, 'preemption_count': 0}), (84687, {'train/accuracy': 0.695019543170929, 'train/loss': 1.3510807752609253, 'validation/accuracy': 0.6265199780464172, 'validation/loss': 1.650166630744934, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.274543285369873, 'test/num_examples': 10000, 'score': 38277.640315294266, 'total_duration': 41185.6047809124, 'accumulated_submission_time': 38277.640315294266, 'accumulated_eval_time': 2900.2883038520813, 'accumulated_logging_time': 3.4122555255889893, 'global_step': 84687, 'preemption_count': 0}), (85617, {'train/accuracy': 0.6769140362739563, 'train/loss': 1.4378875494003296, 'validation/accuracy': 0.6297799944877625, 'validation/loss': 1.655285120010376, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.277604818344116, 'test/num_examples': 10000, 'score': 38697.576645851135, 'total_duration': 41639.674124240875, 'accumulated_submission_time': 38697.576645851135, 'accumulated_eval_time': 2934.331175804138, 'accumulated_logging_time': 3.451533555984497, 'global_step': 85617, 'preemption_count': 0}), (86545, {'train/accuracy': 0.676074206829071, 'train/loss': 1.442645788192749, 'validation/accuracy': 0.6266199946403503, 'validation/loss': 1.6573635339736938, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2886831760406494, 'test/num_examples': 10000, 'score': 39117.53278756142, 'total_duration': 42091.563891649246, 'accumulated_submission_time': 39117.53278756142, 'accumulated_eval_time': 2966.1738238334656, 'accumulated_logging_time': 3.496487617492676, 'global_step': 86545, 'preemption_count': 0}), (87473, {'train/accuracy': 0.6855859160423279, 'train/loss': 1.3951412439346313, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.6594972610473633, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.292130470275879, 'test/num_examples': 10000, 'score': 39537.75274658203, 'total_duration': 42546.0910179615, 'accumulated_submission_time': 39537.75274658203, 'accumulated_eval_time': 3000.3928265571594, 'accumulated_logging_time': 3.5384793281555176, 'global_step': 87473, 'preemption_count': 0}), (88401, {'train/accuracy': 0.6751171946525574, 'train/loss': 1.4573235511779785, 'validation/accuracy': 0.630299985408783, 'validation/loss': 1.667418122291565, 'validation/num_examples': 50000, 'test/accuracy': 0.5100000500679016, 'test/loss': 2.2937583923339844, 'test/num_examples': 10000, 'score': 39957.88946032524, 'total_duration': 43000.04769778252, 'accumulated_submission_time': 39957.88946032524, 'accumulated_eval_time': 3034.1299324035645, 'accumulated_logging_time': 3.575552463531494, 'global_step': 88401, 'preemption_count': 0}), (89327, {'train/accuracy': 0.6839843392372131, 'train/loss': 1.394673466682434, 'validation/accuracy': 0.6331200003623962, 'validation/loss': 1.625125527381897, 'validation/num_examples': 50000, 'test/accuracy': 0.5110000371932983, 'test/loss': 2.2482283115386963, 'test/num_examples': 10000, 'score': 40377.935428380966, 'total_duration': 43453.84268569946, 'accumulated_submission_time': 40377.935428380966, 'accumulated_eval_time': 3067.7917091846466, 'accumulated_logging_time': 3.6167099475860596, 'global_step': 89327, 'preemption_count': 0}), (90254, {'train/accuracy': 0.6922265291213989, 'train/loss': 1.4032502174377441, 'validation/accuracy': 0.6358599662780762, 'validation/loss': 1.6580744981765747, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.281223773956299, 'test/num_examples': 10000, 'score': 40798.026288986206, 'total_duration': 43909.31564474106, 'accumulated_submission_time': 40798.026288986206, 'accumulated_eval_time': 3103.089093208313, 'accumulated_logging_time': 3.6548655033111572, 'global_step': 90254, 'preemption_count': 0}), (91182, {'train/accuracy': 0.6755273342132568, 'train/loss': 1.446442723274231, 'validation/accuracy': 0.6324999928474426, 'validation/loss': 1.6372050046920776, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2850115299224854, 'test/num_examples': 10000, 'score': 41218.17198085785, 'total_duration': 44362.4227976799, 'accumulated_submission_time': 41218.17198085785, 'accumulated_eval_time': 3135.961694717407, 'accumulated_logging_time': 3.6973400115966797, 'global_step': 91182, 'preemption_count': 0}), (92112, {'train/accuracy': 0.6849218606948853, 'train/loss': 1.3958221673965454, 'validation/accuracy': 0.6351799964904785, 'validation/loss': 1.6146154403686523, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.2473669052124023, 'test/num_examples': 10000, 'score': 41638.28119826317, 'total_duration': 44816.870346307755, 'accumulated_submission_time': 41638.28119826317, 'accumulated_eval_time': 3170.2124574184418, 'accumulated_logging_time': 3.7383203506469727, 'global_step': 92112, 'preemption_count': 0}), (93038, {'train/accuracy': 0.689160168170929, 'train/loss': 1.3483566045761108, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.5728561878204346, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.2010042667388916, 'test/num_examples': 10000, 'score': 42058.32339930534, 'total_duration': 45269.29402279854, 'accumulated_submission_time': 42058.32339930534, 'accumulated_eval_time': 3202.508903503418, 'accumulated_logging_time': 3.7772610187530518, 'global_step': 93038, 'preemption_count': 0}), (93965, {'train/accuracy': 0.7109375, 'train/loss': 1.3136073350906372, 'validation/accuracy': 0.6380199790000916, 'validation/loss': 1.624780297279358, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.255450487136841, 'test/num_examples': 10000, 'score': 42478.304805994034, 'total_duration': 45721.74182486534, 'accumulated_submission_time': 42478.304805994034, 'accumulated_eval_time': 3234.8925092220306, 'accumulated_logging_time': 3.814180850982666, 'global_step': 93965, 'preemption_count': 0}), (94887, {'train/accuracy': 0.6878905892372131, 'train/loss': 1.4085044860839844, 'validation/accuracy': 0.6405199766159058, 'validation/loss': 1.6214958429336548, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.2491910457611084, 'test/num_examples': 10000, 'score': 42898.46243238449, 'total_duration': 46175.75550246239, 'accumulated_submission_time': 42898.46243238449, 'accumulated_eval_time': 3268.6627497673035, 'accumulated_logging_time': 3.8543221950531006, 'global_step': 94887, 'preemption_count': 0}), (95814, {'train/accuracy': 0.6914257407188416, 'train/loss': 1.367840051651001, 'validation/accuracy': 0.6401599645614624, 'validation/loss': 1.597143292427063, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.2320969104766846, 'test/num_examples': 10000, 'score': 43318.54101729393, 'total_duration': 46629.85206055641, 'accumulated_submission_time': 43318.54101729393, 'accumulated_eval_time': 3302.586932182312, 'accumulated_logging_time': 3.902494430541992, 'global_step': 95814, 'preemption_count': 0}), (96744, {'train/accuracy': 0.7025195360183716, 'train/loss': 1.342665433883667, 'validation/accuracy': 0.6408999562263489, 'validation/loss': 1.617620587348938, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.2584726810455322, 'test/num_examples': 10000, 'score': 43738.83052825928, 'total_duration': 47084.425433158875, 'accumulated_submission_time': 43738.83052825928, 'accumulated_eval_time': 3336.7769792079926, 'accumulated_logging_time': 3.949820041656494, 'global_step': 96744, 'preemption_count': 0}), (97673, {'train/accuracy': 0.68896484375, 'train/loss': 1.3808518648147583, 'validation/accuracy': 0.6461799740791321, 'validation/loss': 1.570910930633545, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.2020792961120605, 'test/num_examples': 10000, 'score': 44159.072498083115, 'total_duration': 47538.14868545532, 'accumulated_submission_time': 44159.072498083115, 'accumulated_eval_time': 3370.167640209198, 'accumulated_logging_time': 3.994457244873047, 'global_step': 97673, 'preemption_count': 0}), (98598, {'train/accuracy': 0.69544917345047, 'train/loss': 1.3356281518936157, 'validation/accuracy': 0.6452599763870239, 'validation/loss': 1.56348717212677, 'validation/num_examples': 50000, 'test/accuracy': 0.5278000235557556, 'test/loss': 2.181706428527832, 'test/num_examples': 10000, 'score': 44579.15256071091, 'total_duration': 47991.88709282875, 'accumulated_submission_time': 44579.15256071091, 'accumulated_eval_time': 3403.740065574646, 'accumulated_logging_time': 4.033020973205566, 'global_step': 98598, 'preemption_count': 0}), (99525, {'train/accuracy': 0.6994921565055847, 'train/loss': 1.3409777879714966, 'validation/accuracy': 0.6436600089073181, 'validation/loss': 1.5910606384277344, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.2290115356445312, 'test/num_examples': 10000, 'score': 44999.109080553055, 'total_duration': 48445.73757982254, 'accumulated_submission_time': 44999.109080553055, 'accumulated_eval_time': 3437.5442354679108, 'accumulated_logging_time': 4.075830459594727, 'global_step': 99525, 'preemption_count': 0}), (100453, {'train/accuracy': 0.6985937356948853, 'train/loss': 1.3283151388168335, 'validation/accuracy': 0.6507599949836731, 'validation/loss': 1.5489925146102905, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.1810965538024902, 'test/num_examples': 10000, 'score': 45419.194039821625, 'total_duration': 48899.44379377365, 'accumulated_submission_time': 45419.194039821625, 'accumulated_eval_time': 3471.073740005493, 'accumulated_logging_time': 4.120898962020874, 'global_step': 100453, 'preemption_count': 0}), (101381, {'train/accuracy': 0.7010741829872131, 'train/loss': 1.3405998945236206, 'validation/accuracy': 0.6499399542808533, 'validation/loss': 1.5560424327850342, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.1788530349731445, 'test/num_examples': 10000, 'score': 45839.292186021805, 'total_duration': 49351.35535264015, 'accumulated_submission_time': 45839.292186021805, 'accumulated_eval_time': 3502.795699119568, 'accumulated_logging_time': 4.166559219360352, 'global_step': 101381, 'preemption_count': 0}), (102308, {'train/accuracy': 0.7044335603713989, 'train/loss': 1.303029179573059, 'validation/accuracy': 0.6525200009346008, 'validation/loss': 1.5312974452972412, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.1580193042755127, 'test/num_examples': 10000, 'score': 46259.55302786827, 'total_duration': 49805.33374285698, 'accumulated_submission_time': 46259.55302786827, 'accumulated_eval_time': 3536.4254937171936, 'accumulated_logging_time': 4.207376718521118, 'global_step': 102308, 'preemption_count': 0}), (103236, {'train/accuracy': 0.721386730670929, 'train/loss': 1.2593860626220703, 'validation/accuracy': 0.6481999754905701, 'validation/loss': 1.5756916999816895, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.1878247261047363, 'test/num_examples': 10000, 'score': 46679.525918245316, 'total_duration': 50259.024639844894, 'accumulated_submission_time': 46679.525918245316, 'accumulated_eval_time': 3570.0530354976654, 'accumulated_logging_time': 4.251614093780518, 'global_step': 103236, 'preemption_count': 0}), (104166, {'train/accuracy': 0.6998632550239563, 'train/loss': 1.3183801174163818, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.5319174528121948, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.147125005722046, 'test/num_examples': 10000, 'score': 47099.60453367233, 'total_duration': 50713.70186185837, 'accumulated_submission_time': 47099.60453367233, 'accumulated_eval_time': 3604.563158750534, 'accumulated_logging_time': 4.293308973312378, 'global_step': 104166, 'preemption_count': 0}), (105096, {'train/accuracy': 0.7125781178474426, 'train/loss': 1.2877388000488281, 'validation/accuracy': 0.6566999554634094, 'validation/loss': 1.5229451656341553, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.143164873123169, 'test/num_examples': 10000, 'score': 47519.71596264839, 'total_duration': 51167.10824346542, 'accumulated_submission_time': 47519.71596264839, 'accumulated_eval_time': 3637.7664346694946, 'accumulated_logging_time': 4.338505029678345, 'global_step': 105096, 'preemption_count': 0}), (106025, {'train/accuracy': 0.71156245470047, 'train/loss': 1.2922778129577637, 'validation/accuracy': 0.6466999650001526, 'validation/loss': 1.5727142095565796, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.194082260131836, 'test/num_examples': 10000, 'score': 47939.64816617966, 'total_duration': 51620.6674015522, 'accumulated_submission_time': 47939.64816617966, 'accumulated_eval_time': 3671.302904844284, 'accumulated_logging_time': 4.3822362422943115, 'global_step': 106025, 'preemption_count': 0}), (106952, {'train/accuracy': 0.7048046588897705, 'train/loss': 1.3123520612716675, 'validation/accuracy': 0.6563400030136108, 'validation/loss': 1.5270174741744995, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.1496691703796387, 'test/num_examples': 10000, 'score': 48359.77759027481, 'total_duration': 52074.03806447983, 'accumulated_submission_time': 48359.77759027481, 'accumulated_eval_time': 3704.454334497452, 'accumulated_logging_time': 4.4254231452941895, 'global_step': 106952, 'preemption_count': 0}), (107880, {'train/accuracy': 0.7080664038658142, 'train/loss': 1.2890511751174927, 'validation/accuracy': 0.6545799970626831, 'validation/loss': 1.529636263847351, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.1473042964935303, 'test/num_examples': 10000, 'score': 48779.730001449585, 'total_duration': 52526.886921167374, 'accumulated_submission_time': 48779.730001449585, 'accumulated_eval_time': 3737.262728214264, 'accumulated_logging_time': 4.467191934585571, 'global_step': 107880, 'preemption_count': 0}), (108810, {'train/accuracy': 0.7175390720367432, 'train/loss': 1.221261978149414, 'validation/accuracy': 0.6603599786758423, 'validation/loss': 1.4771640300750732, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.107815742492676, 'test/num_examples': 10000, 'score': 49199.868581056595, 'total_duration': 52978.13624429703, 'accumulated_submission_time': 49199.868581056595, 'accumulated_eval_time': 3768.2807273864746, 'accumulated_logging_time': 4.513633966445923, 'global_step': 108810, 'preemption_count': 0}), (109738, {'train/accuracy': 0.7140429615974426, 'train/loss': 1.2802772521972656, 'validation/accuracy': 0.6586199998855591, 'validation/loss': 1.5189369916915894, 'validation/num_examples': 50000, 'test/accuracy': 0.5404000282287598, 'test/loss': 2.142338991165161, 'test/num_examples': 10000, 'score': 49619.92288994789, 'total_duration': 53429.75419712067, 'accumulated_submission_time': 49619.92288994789, 'accumulated_eval_time': 3799.7456452846527, 'accumulated_logging_time': 4.565981388092041, 'global_step': 109738, 'preemption_count': 0}), (110669, {'train/accuracy': 0.7124218344688416, 'train/loss': 1.2984763383865356, 'validation/accuracy': 0.6633599996566772, 'validation/loss': 1.5199729204177856, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.146238327026367, 'test/num_examples': 10000, 'score': 50039.94888544083, 'total_duration': 53883.64413046837, 'accumulated_submission_time': 50039.94888544083, 'accumulated_eval_time': 3833.5132508277893, 'accumulated_logging_time': 4.615039587020874, 'global_step': 110669, 'preemption_count': 0}), (111601, {'train/accuracy': 0.7237108945846558, 'train/loss': 1.2157682180404663, 'validation/accuracy': 0.6658399701118469, 'validation/loss': 1.4762276411056519, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.1115095615386963, 'test/num_examples': 10000, 'score': 50460.5451362133, 'total_duration': 54337.490814208984, 'accumulated_submission_time': 50460.5451362133, 'accumulated_eval_time': 3866.6692838668823, 'accumulated_logging_time': 4.662085294723511, 'global_step': 111601, 'preemption_count': 0}), (112529, {'train/accuracy': 0.73876953125, 'train/loss': 1.147725224494934, 'validation/accuracy': 0.6629999876022339, 'validation/loss': 1.471642017364502, 'validation/num_examples': 50000, 'test/accuracy': 0.550000011920929, 'test/loss': 2.0856385231018066, 'test/num_examples': 10000, 'score': 50880.741792201996, 'total_duration': 54791.81612062454, 'accumulated_submission_time': 50880.741792201996, 'accumulated_eval_time': 3900.708383321762, 'accumulated_logging_time': 4.705749273300171, 'global_step': 112529, 'preemption_count': 0}), (113460, {'train/accuracy': 0.7142187356948853, 'train/loss': 1.2561309337615967, 'validation/accuracy': 0.6647799611091614, 'validation/loss': 1.4717066287994385, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.0994019508361816, 'test/num_examples': 10000, 'score': 51301.066935777664, 'total_duration': 55246.878209114075, 'accumulated_submission_time': 51301.066935777664, 'accumulated_eval_time': 3935.3465564250946, 'accumulated_logging_time': 4.757417678833008, 'global_step': 113460, 'preemption_count': 0}), (114389, {'train/accuracy': 0.7233593463897705, 'train/loss': 1.2339673042297363, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.4803937673568726, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.0966713428497314, 'test/num_examples': 10000, 'score': 51721.110796928406, 'total_duration': 55701.402671575546, 'accumulated_submission_time': 51721.110796928406, 'accumulated_eval_time': 3969.720571756363, 'accumulated_logging_time': 4.803069829940796, 'global_step': 114389, 'preemption_count': 0}), (115318, {'train/accuracy': 0.7351366877555847, 'train/loss': 1.1889073848724365, 'validation/accuracy': 0.6723600029945374, 'validation/loss': 1.4659477472305298, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.0902318954467773, 'test/num_examples': 10000, 'score': 52141.49275612831, 'total_duration': 56155.447914361954, 'accumulated_submission_time': 52141.49275612831, 'accumulated_eval_time': 4003.2828526496887, 'accumulated_logging_time': 4.8567054271698, 'global_step': 115318, 'preemption_count': 0}), (116249, {'train/accuracy': 0.7199413776397705, 'train/loss': 1.2665797472000122, 'validation/accuracy': 0.67221999168396, 'validation/loss': 1.4920510053634644, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.1149799823760986, 'test/num_examples': 10000, 'score': 52561.77123785019, 'total_duration': 56608.08748936653, 'accumulated_submission_time': 52561.77123785019, 'accumulated_eval_time': 4035.5553166866302, 'accumulated_logging_time': 4.898540258407593, 'global_step': 116249, 'preemption_count': 0}), (117180, {'train/accuracy': 0.7294726371765137, 'train/loss': 1.2057080268859863, 'validation/accuracy': 0.677299976348877, 'validation/loss': 1.432081937789917, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.0657169818878174, 'test/num_examples': 10000, 'score': 52981.80159282684, 'total_duration': 57062.94676208496, 'accumulated_submission_time': 52981.80159282684, 'accumulated_eval_time': 4070.2912969589233, 'accumulated_logging_time': 4.944955587387085, 'global_step': 117180, 'preemption_count': 0}), (118109, {'train/accuracy': 0.7354491949081421, 'train/loss': 1.1728439331054688, 'validation/accuracy': 0.6750800013542175, 'validation/loss': 1.43381667137146, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.0560216903686523, 'test/num_examples': 10000, 'score': 53401.867626428604, 'total_duration': 57518.092337846756, 'accumulated_submission_time': 53401.867626428604, 'accumulated_eval_time': 4105.278161764145, 'accumulated_logging_time': 4.990721702575684, 'global_step': 118109, 'preemption_count': 0}), (119039, {'train/accuracy': 0.7400195002555847, 'train/loss': 1.14028000831604, 'validation/accuracy': 0.6732800006866455, 'validation/loss': 1.417976975440979, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.041276454925537, 'test/num_examples': 10000, 'score': 53822.07191777229, 'total_duration': 57971.04748129845, 'accumulated_submission_time': 53822.07191777229, 'accumulated_eval_time': 4137.934015035629, 'accumulated_logging_time': 5.03800368309021, 'global_step': 119039, 'preemption_count': 0}), (119970, {'train/accuracy': 0.7264648079872131, 'train/loss': 1.2079328298568726, 'validation/accuracy': 0.6744799613952637, 'validation/loss': 1.4414364099502563, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.0710713863372803, 'test/num_examples': 10000, 'score': 54242.142776966095, 'total_duration': 58425.14707708359, 'accumulated_submission_time': 54242.142776966095, 'accumulated_eval_time': 4171.868766546249, 'accumulated_logging_time': 5.085484981536865, 'global_step': 119970, 'preemption_count': 0}), (120901, {'train/accuracy': 0.7350195050239563, 'train/loss': 1.1525589227676392, 'validation/accuracy': 0.6802799701690674, 'validation/loss': 1.4023057222366333, 'validation/num_examples': 50000, 'test/accuracy': 0.5651000142097473, 'test/loss': 2.001354694366455, 'test/num_examples': 10000, 'score': 54662.123398303986, 'total_duration': 58878.76770377159, 'accumulated_submission_time': 54662.123398303986, 'accumulated_eval_time': 4205.4150903224945, 'accumulated_logging_time': 5.1322691440582275, 'global_step': 120901, 'preemption_count': 0}), (121830, {'train/accuracy': 0.7547070384025574, 'train/loss': 1.0838943719863892, 'validation/accuracy': 0.683459997177124, 'validation/loss': 1.395894169807434, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 1.9966557025909424, 'test/num_examples': 10000, 'score': 55082.16000986099, 'total_duration': 59331.42533326149, 'accumulated_submission_time': 55082.16000986099, 'accumulated_eval_time': 4237.944813966751, 'accumulated_logging_time': 5.175921440124512, 'global_step': 121830, 'preemption_count': 0}), (122757, {'train/accuracy': 0.7364257574081421, 'train/loss': 1.1707990169525146, 'validation/accuracy': 0.6805199980735779, 'validation/loss': 1.4080075025558472, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.036602735519409, 'test/num_examples': 10000, 'score': 55502.36118531227, 'total_duration': 59785.28081989288, 'accumulated_submission_time': 55502.36118531227, 'accumulated_eval_time': 4271.503207921982, 'accumulated_logging_time': 5.226181507110596, 'global_step': 122757, 'preemption_count': 0}), (123685, {'train/accuracy': 0.7400780916213989, 'train/loss': 1.1385509967803955, 'validation/accuracy': 0.6856399774551392, 'validation/loss': 1.3829452991485596, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 2.0096795558929443, 'test/num_examples': 10000, 'score': 55922.51798272133, 'total_duration': 60240.23173499107, 'accumulated_submission_time': 55922.51798272133, 'accumulated_eval_time': 4306.2075407505035, 'accumulated_logging_time': 5.269519805908203, 'global_step': 123685, 'preemption_count': 0}), (124614, {'train/accuracy': 0.7510937452316284, 'train/loss': 1.11006760597229, 'validation/accuracy': 0.6840999722480774, 'validation/loss': 1.3941822052001953, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 2.01263165473938, 'test/num_examples': 10000, 'score': 56342.67802453041, 'total_duration': 60692.419605493546, 'accumulated_submission_time': 56342.67802453041, 'accumulated_eval_time': 4338.141446352005, 'accumulated_logging_time': 5.316270351409912, 'global_step': 124614, 'preemption_count': 0}), (125544, {'train/accuracy': 0.7424218654632568, 'train/loss': 1.1373136043548584, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.3734878301620483, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.991938591003418, 'test/num_examples': 10000, 'score': 56762.89463853836, 'total_duration': 61146.11239314079, 'accumulated_submission_time': 56762.89463853836, 'accumulated_eval_time': 4371.525834798813, 'accumulated_logging_time': 5.361638784408569, 'global_step': 125544, 'preemption_count': 0}), (126472, {'train/accuracy': 0.74867182970047, 'train/loss': 1.1079342365264893, 'validation/accuracy': 0.6914599537849426, 'validation/loss': 1.3647878170013428, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 1.9838557243347168, 'test/num_examples': 10000, 'score': 57183.18724656105, 'total_duration': 61599.6272623539, 'accumulated_submission_time': 57183.18724656105, 'accumulated_eval_time': 4404.65695309639, 'accumulated_logging_time': 5.406764268875122, 'global_step': 126472, 'preemption_count': 0}), (127402, {'train/accuracy': 0.7528125047683716, 'train/loss': 1.0917577743530273, 'validation/accuracy': 0.6893599629402161, 'validation/loss': 1.3566315174102783, 'validation/num_examples': 50000, 'test/accuracy': 0.5679000020027161, 'test/loss': 1.9800525903701782, 'test/num_examples': 10000, 'score': 57603.10753917694, 'total_duration': 62053.47536659241, 'accumulated_submission_time': 57603.10753917694, 'accumulated_eval_time': 4438.493448495865, 'accumulated_logging_time': 5.450902700424194, 'global_step': 127402, 'preemption_count': 0}), (128301, {'train/accuracy': 0.7537695169448853, 'train/loss': 1.098175287246704, 'validation/accuracy': 0.6912999749183655, 'validation/loss': 1.3770054578781128, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.9985442161560059, 'test/num_examples': 10000, 'score': 58023.76083302498, 'total_duration': 62508.17419576645, 'accumulated_submission_time': 58023.76083302498, 'accumulated_eval_time': 4472.449482917786, 'accumulated_logging_time': 5.4948413372039795, 'global_step': 128301, 'preemption_count': 0}), (129231, {'train/accuracy': 0.7473828196525574, 'train/loss': 1.1209533214569092, 'validation/accuracy': 0.6952799558639526, 'validation/loss': 1.3586920499801636, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.974564552307129, 'test/num_examples': 10000, 'score': 58443.69563674927, 'total_duration': 62962.107741594315, 'accumulated_submission_time': 58443.69563674927, 'accumulated_eval_time': 4506.356766462326, 'accumulated_logging_time': 5.540136098861694, 'global_step': 129231, 'preemption_count': 0}), (130161, {'train/accuracy': 0.7574023008346558, 'train/loss': 1.075601577758789, 'validation/accuracy': 0.6932399868965149, 'validation/loss': 1.3420850038528442, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 1.9617393016815186, 'test/num_examples': 10000, 'score': 58863.781465768814, 'total_duration': 63414.34195232391, 'accumulated_submission_time': 58863.781465768814, 'accumulated_eval_time': 4538.41277050972, 'accumulated_logging_time': 5.585582971572876, 'global_step': 130161, 'preemption_count': 0}), (131088, {'train/accuracy': 0.7674218416213989, 'train/loss': 1.0314123630523682, 'validation/accuracy': 0.6955599784851074, 'validation/loss': 1.343638300895691, 'validation/num_examples': 50000, 'test/accuracy': 0.5745000243186951, 'test/loss': 1.9722157716751099, 'test/num_examples': 10000, 'score': 59283.73093700409, 'total_duration': 63868.40799832344, 'accumulated_submission_time': 59283.73093700409, 'accumulated_eval_time': 4572.432414054871, 'accumulated_logging_time': 5.636627674102783, 'global_step': 131088, 'preemption_count': 0}), (132017, {'train/accuracy': 0.7551367282867432, 'train/loss': 1.0511291027069092, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.3008636236190796, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 1.91862154006958, 'test/num_examples': 10000, 'score': 59704.04973602295, 'total_duration': 64322.280165433884, 'accumulated_submission_time': 59704.04973602295, 'accumulated_eval_time': 4605.888027429581, 'accumulated_logging_time': 5.6878721714019775, 'global_step': 132017, 'preemption_count': 0}), (132946, {'train/accuracy': 0.7594921588897705, 'train/loss': 1.0723209381103516, 'validation/accuracy': 0.6966399550437927, 'validation/loss': 1.3334400653839111, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.952175498008728, 'test/num_examples': 10000, 'score': 60124.05582642555, 'total_duration': 64776.534338235855, 'accumulated_submission_time': 60124.05582642555, 'accumulated_eval_time': 4640.043148756027, 'accumulated_logging_time': 5.734477758407593, 'global_step': 132946, 'preemption_count': 0}), (133876, {'train/accuracy': 0.7744726538658142, 'train/loss': 0.9877074360847473, 'validation/accuracy': 0.7017599940299988, 'validation/loss': 1.2961947917938232, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.904213547706604, 'test/num_examples': 10000, 'score': 60544.09117388725, 'total_duration': 65230.13675904274, 'accumulated_submission_time': 60544.09117388725, 'accumulated_eval_time': 4673.519508600235, 'accumulated_logging_time': 5.778345823287964, 'global_step': 133876, 'preemption_count': 0}), (134807, {'train/accuracy': 0.7617382407188416, 'train/loss': 1.0603655576705933, 'validation/accuracy': 0.701200008392334, 'validation/loss': 1.3200796842575073, 'validation/num_examples': 50000, 'test/accuracy': 0.5774000287055969, 'test/loss': 1.9307143688201904, 'test/num_examples': 10000, 'score': 60964.34392094612, 'total_duration': 65685.15578842163, 'accumulated_submission_time': 60964.34392094612, 'accumulated_eval_time': 4708.190718412399, 'accumulated_logging_time': 5.826894760131836, 'global_step': 134807, 'preemption_count': 0}), (135735, {'train/accuracy': 0.768847644329071, 'train/loss': 1.01193368434906, 'validation/accuracy': 0.7056999802589417, 'validation/loss': 1.2756710052490234, 'validation/num_examples': 50000, 'test/accuracy': 0.5831000208854675, 'test/loss': 1.8825321197509766, 'test/num_examples': 10000, 'score': 61384.338955163956, 'total_duration': 66136.31755590439, 'accumulated_submission_time': 61384.338955163956, 'accumulated_eval_time': 4739.263605117798, 'accumulated_logging_time': 5.874547243118286, 'global_step': 135735, 'preemption_count': 0}), (136662, {'train/accuracy': 0.7686523199081421, 'train/loss': 1.024351954460144, 'validation/accuracy': 0.7022199630737305, 'validation/loss': 1.3176020383834839, 'validation/num_examples': 50000, 'test/accuracy': 0.5806000232696533, 'test/loss': 1.923264741897583, 'test/num_examples': 10000, 'score': 61804.52350068092, 'total_duration': 66590.26353669167, 'accumulated_submission_time': 61804.52350068092, 'accumulated_eval_time': 4772.92241859436, 'accumulated_logging_time': 5.9302287101745605, 'global_step': 136662, 'preemption_count': 0}), (137592, {'train/accuracy': 0.786425769329071, 'train/loss': 0.9460193514823914, 'validation/accuracy': 0.7095999717712402, 'validation/loss': 1.2811696529388428, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.8915445804595947, 'test/num_examples': 10000, 'score': 62224.45097088814, 'total_duration': 67044.65680789948, 'accumulated_submission_time': 62224.45097088814, 'accumulated_eval_time': 4807.293427467346, 'accumulated_logging_time': 5.978266716003418, 'global_step': 137592, 'preemption_count': 0}), (138521, {'train/accuracy': 0.76917964220047, 'train/loss': 1.0142436027526855, 'validation/accuracy': 0.7066400051116943, 'validation/loss': 1.2850269079208374, 'validation/num_examples': 50000, 'test/accuracy': 0.584600031375885, 'test/loss': 1.8975168466567993, 'test/num_examples': 10000, 'score': 62644.38854908943, 'total_duration': 67499.70457077026, 'accumulated_submission_time': 62644.38854908943, 'accumulated_eval_time': 4842.306634902954, 'accumulated_logging_time': 6.0289857387542725, 'global_step': 138521, 'preemption_count': 0}), (139448, {'train/accuracy': 0.7764062285423279, 'train/loss': 0.9926196932792664, 'validation/accuracy': 0.7118200063705444, 'validation/loss': 1.268027663230896, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 1.8783414363861084, 'test/num_examples': 10000, 'score': 63064.298337221146, 'total_duration': 67952.42379832268, 'accumulated_submission_time': 63064.298337221146, 'accumulated_eval_time': 4875.023307323456, 'accumulated_logging_time': 6.075516939163208, 'global_step': 139448, 'preemption_count': 0}), (140376, {'train/accuracy': 0.7857421636581421, 'train/loss': 0.9634817838668823, 'validation/accuracy': 0.7120800018310547, 'validation/loss': 1.285139560699463, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.8925907611846924, 'test/num_examples': 10000, 'score': 63484.358603954315, 'total_duration': 68407.07604622841, 'accumulated_submission_time': 63484.358603954315, 'accumulated_eval_time': 4909.52064538002, 'accumulated_logging_time': 6.123907089233398, 'global_step': 140376, 'preemption_count': 0}), (141307, {'train/accuracy': 0.7754882574081421, 'train/loss': 0.99040687084198, 'validation/accuracy': 0.7101199626922607, 'validation/loss': 1.26579749584198, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.8663791418075562, 'test/num_examples': 10000, 'score': 63904.452016592026, 'total_duration': 68861.97762393951, 'accumulated_submission_time': 63904.452016592026, 'accumulated_eval_time': 4944.235973596573, 'accumulated_logging_time': 6.169575452804565, 'global_step': 141307, 'preemption_count': 0}), (142234, {'train/accuracy': 0.7803906202316284, 'train/loss': 0.9680672883987427, 'validation/accuracy': 0.7150200009346008, 'validation/loss': 1.2434526681900024, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8614035844802856, 'test/num_examples': 10000, 'score': 64324.757776498795, 'total_duration': 69316.77753448486, 'accumulated_submission_time': 64324.757776498795, 'accumulated_eval_time': 4978.624108314514, 'accumulated_logging_time': 6.229996204376221, 'global_step': 142234, 'preemption_count': 0}), (143162, {'train/accuracy': 0.7883398532867432, 'train/loss': 0.9409700632095337, 'validation/accuracy': 0.7188999652862549, 'validation/loss': 1.2394484281539917, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.8527811765670776, 'test/num_examples': 10000, 'score': 64745.00335741043, 'total_duration': 69770.92920541763, 'accumulated_submission_time': 64745.00335741043, 'accumulated_eval_time': 5012.432188272476, 'accumulated_logging_time': 6.281265020370483, 'global_step': 143162, 'preemption_count': 0}), (144090, {'train/accuracy': 0.7860937118530273, 'train/loss': 0.9350039958953857, 'validation/accuracy': 0.7210599780082703, 'validation/loss': 1.209059238433838, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.8121860027313232, 'test/num_examples': 10000, 'score': 65165.10458111763, 'total_duration': 70224.88837790489, 'accumulated_submission_time': 65165.10458111763, 'accumulated_eval_time': 5046.197269678116, 'accumulated_logging_time': 6.327600479125977, 'global_step': 144090, 'preemption_count': 0}), (145020, {'train/accuracy': 0.7858593463897705, 'train/loss': 0.9343098402023315, 'validation/accuracy': 0.7197799682617188, 'validation/loss': 1.222838282585144, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.830407738685608, 'test/num_examples': 10000, 'score': 65585.06423592567, 'total_duration': 70680.72092986107, 'accumulated_submission_time': 65585.06423592567, 'accumulated_eval_time': 5081.971020698547, 'accumulated_logging_time': 6.380660057067871, 'global_step': 145020, 'preemption_count': 0}), (145951, {'train/accuracy': 0.7895702719688416, 'train/loss': 0.9390873312950134, 'validation/accuracy': 0.7216399908065796, 'validation/loss': 1.2361648082733154, 'validation/num_examples': 50000, 'test/accuracy': 0.6039000153541565, 'test/loss': 1.830265760421753, 'test/num_examples': 10000, 'score': 66005.23581600189, 'total_duration': 71135.50416016579, 'accumulated_submission_time': 66005.23581600189, 'accumulated_eval_time': 5116.485925197601, 'accumulated_logging_time': 6.430635690689087, 'global_step': 145951, 'preemption_count': 0}), (146880, {'train/accuracy': 0.8043749928474426, 'train/loss': 0.8740522861480713, 'validation/accuracy': 0.7262799739837646, 'validation/loss': 1.2046759128570557, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.8089510202407837, 'test/num_examples': 10000, 'score': 66425.18833255768, 'total_duration': 71590.81511712074, 'accumulated_submission_time': 66425.18833255768, 'accumulated_eval_time': 5151.747972011566, 'accumulated_logging_time': 6.480493545532227, 'global_step': 146880, 'preemption_count': 0}), (147810, {'train/accuracy': 0.7898827791213989, 'train/loss': 0.9271060228347778, 'validation/accuracy': 0.7245599627494812, 'validation/loss': 1.2024517059326172, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.8167974948883057, 'test/num_examples': 10000, 'score': 66845.46230769157, 'total_duration': 72044.69520163536, 'accumulated_submission_time': 66845.46230769157, 'accumulated_eval_time': 5185.241056442261, 'accumulated_logging_time': 6.546149730682373, 'global_step': 147810, 'preemption_count': 0}), (148741, {'train/accuracy': 0.79749995470047, 'train/loss': 0.8806569576263428, 'validation/accuracy': 0.7253400087356567, 'validation/loss': 1.184898853302002, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7757694721221924, 'test/num_examples': 10000, 'score': 67265.56567597389, 'total_duration': 72499.34892606735, 'accumulated_submission_time': 67265.56567597389, 'accumulated_eval_time': 5219.695116758347, 'accumulated_logging_time': 6.595580577850342, 'global_step': 148741, 'preemption_count': 0}), (149669, {'train/accuracy': 0.8055273294448853, 'train/loss': 0.8653415441513062, 'validation/accuracy': 0.7287799715995789, 'validation/loss': 1.1887600421905518, 'validation/num_examples': 50000, 'test/accuracy': 0.6064000129699707, 'test/loss': 1.7887310981750488, 'test/num_examples': 10000, 'score': 67685.5972161293, 'total_duration': 72953.5630671978, 'accumulated_submission_time': 67685.5972161293, 'accumulated_eval_time': 5253.777672767639, 'accumulated_logging_time': 6.64896035194397, 'global_step': 149669, 'preemption_count': 0}), (150600, {'train/accuracy': 0.7973241806030273, 'train/loss': 0.8826409578323364, 'validation/accuracy': 0.7310000061988831, 'validation/loss': 1.1645584106445312, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7593497037887573, 'test/num_examples': 10000, 'score': 68105.81286787987, 'total_duration': 73407.98974180222, 'accumulated_submission_time': 68105.81286787987, 'accumulated_eval_time': 5287.894725084305, 'accumulated_logging_time': 6.696053504943848, 'global_step': 150600, 'preemption_count': 0}), (151531, {'train/accuracy': 0.8052929639816284, 'train/loss': 0.8601254224777222, 'validation/accuracy': 0.733959972858429, 'validation/loss': 1.1651591062545776, 'validation/num_examples': 50000, 'test/accuracy': 0.6106000542640686, 'test/loss': 1.780478596687317, 'test/num_examples': 10000, 'score': 68526.03539800644, 'total_duration': 73862.64343810081, 'accumulated_submission_time': 68526.03539800644, 'accumulated_eval_time': 5322.22726726532, 'accumulated_logging_time': 6.748147487640381, 'global_step': 151531, 'preemption_count': 0}), (152460, {'train/accuracy': 0.8087890148162842, 'train/loss': 0.8668062686920166, 'validation/accuracy': 0.7332599759101868, 'validation/loss': 1.18711519241333, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.789298176765442, 'test/num_examples': 10000, 'score': 68946.24187660217, 'total_duration': 74317.84789443016, 'accumulated_submission_time': 68946.24187660217, 'accumulated_eval_time': 5357.129096031189, 'accumulated_logging_time': 6.798153877258301, 'global_step': 152460, 'preemption_count': 0}), (153391, {'train/accuracy': 0.8062109351158142, 'train/loss': 0.8395585417747498, 'validation/accuracy': 0.7360999584197998, 'validation/loss': 1.139323353767395, 'validation/num_examples': 50000, 'test/accuracy': 0.6176000237464905, 'test/loss': 1.7518985271453857, 'test/num_examples': 10000, 'score': 69366.1802983284, 'total_duration': 74772.3870472908, 'accumulated_submission_time': 69366.1802983284, 'accumulated_eval_time': 5391.624893188477, 'accumulated_logging_time': 6.85608983039856, 'global_step': 153391, 'preemption_count': 0}), (154319, {'train/accuracy': 0.8050194978713989, 'train/loss': 0.8566577434539795, 'validation/accuracy': 0.7374399900436401, 'validation/loss': 1.1484848260879517, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.7600574493408203, 'test/num_examples': 10000, 'score': 69786.34556651115, 'total_duration': 75227.58785867691, 'accumulated_submission_time': 69786.34556651115, 'accumulated_eval_time': 5426.559454441071, 'accumulated_logging_time': 6.911020278930664, 'global_step': 154319, 'preemption_count': 0}), (155245, {'train/accuracy': 0.8122656345367432, 'train/loss': 0.8312785625457764, 'validation/accuracy': 0.7390999794006348, 'validation/loss': 1.1496641635894775, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.7459418773651123, 'test/num_examples': 10000, 'score': 70206.66819000244, 'total_duration': 75682.72457933426, 'accumulated_submission_time': 70206.66819000244, 'accumulated_eval_time': 5461.275590896606, 'accumulated_logging_time': 6.962958574295044, 'global_step': 155245, 'preemption_count': 0}), (156170, {'train/accuracy': 0.8208202719688416, 'train/loss': 0.808429479598999, 'validation/accuracy': 0.7415599822998047, 'validation/loss': 1.147451639175415, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.7455700635910034, 'test/num_examples': 10000, 'score': 70626.57415676117, 'total_duration': 76136.81805038452, 'accumulated_submission_time': 70626.57415676117, 'accumulated_eval_time': 5495.359393119812, 'accumulated_logging_time': 7.020386457443237, 'global_step': 156170, 'preemption_count': 0}), (157100, {'train/accuracy': 0.8145312070846558, 'train/loss': 0.8163199424743652, 'validation/accuracy': 0.7399199604988098, 'validation/loss': 1.1327764987945557, 'validation/num_examples': 50000, 'test/accuracy': 0.6218000054359436, 'test/loss': 1.7313672304153442, 'test/num_examples': 10000, 'score': 71046.6845676899, 'total_duration': 76591.11240267754, 'accumulated_submission_time': 71046.6845676899, 'accumulated_eval_time': 5529.437774181366, 'accumulated_logging_time': 7.078781843185425, 'global_step': 157100, 'preemption_count': 0}), (158029, {'train/accuracy': 0.8168163895606995, 'train/loss': 0.8214466571807861, 'validation/accuracy': 0.7429199814796448, 'validation/loss': 1.1342238187789917, 'validation/num_examples': 50000, 'test/accuracy': 0.6199000477790833, 'test/loss': 1.7298482656478882, 'test/num_examples': 10000, 'score': 71466.77744364738, 'total_duration': 77045.694283247, 'accumulated_submission_time': 71466.77744364738, 'accumulated_eval_time': 5563.826683521271, 'accumulated_logging_time': 7.130944013595581, 'global_step': 158029, 'preemption_count': 0}), (158959, {'train/accuracy': 0.823535144329071, 'train/loss': 0.7854181528091431, 'validation/accuracy': 0.7455799579620361, 'validation/loss': 1.114540696144104, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.7001981735229492, 'test/num_examples': 10000, 'score': 71886.9072842598, 'total_duration': 77500.05093717575, 'accumulated_submission_time': 71886.9072842598, 'accumulated_eval_time': 5597.954945802689, 'accumulated_logging_time': 7.182467699050903, 'global_step': 158959, 'preemption_count': 0}), (159890, {'train/accuracy': 0.8223828077316284, 'train/loss': 0.7984592914581299, 'validation/accuracy': 0.7449600100517273, 'validation/loss': 1.1174359321594238, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.702431082725525, 'test/num_examples': 10000, 'score': 72306.84834432602, 'total_duration': 77954.27435541153, 'accumulated_submission_time': 72306.84834432602, 'accumulated_eval_time': 5632.141172647476, 'accumulated_logging_time': 7.231976747512817, 'global_step': 159890, 'preemption_count': 0}), (160819, {'train/accuracy': 0.82386714220047, 'train/loss': 0.7823898196220398, 'validation/accuracy': 0.7475000023841858, 'validation/loss': 1.1016745567321777, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.6906828880310059, 'test/num_examples': 10000, 'score': 72726.83613228798, 'total_duration': 78408.82541036606, 'accumulated_submission_time': 72726.83613228798, 'accumulated_eval_time': 5666.60213804245, 'accumulated_logging_time': 7.287751197814941, 'global_step': 160819, 'preemption_count': 0}), (161748, {'train/accuracy': 0.8251367211341858, 'train/loss': 0.7654830813407898, 'validation/accuracy': 0.7488999962806702, 'validation/loss': 1.0918009281158447, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.686492681503296, 'test/num_examples': 10000, 'score': 73147.12069392204, 'total_duration': 78863.87222862244, 'accumulated_submission_time': 73147.12069392204, 'accumulated_eval_time': 5701.25897192955, 'accumulated_logging_time': 7.346428871154785, 'global_step': 161748, 'preemption_count': 0}), (162676, {'train/accuracy': 0.8285351395606995, 'train/loss': 0.7643269896507263, 'validation/accuracy': 0.7503599524497986, 'validation/loss': 1.0934828519821167, 'validation/num_examples': 50000, 'test/accuracy': 0.633400022983551, 'test/loss': 1.6805967092514038, 'test/num_examples': 10000, 'score': 73567.05645251274, 'total_duration': 79317.50340890884, 'accumulated_submission_time': 73567.05645251274, 'accumulated_eval_time': 5734.852890491486, 'accumulated_logging_time': 7.401033163070679, 'global_step': 162676, 'preemption_count': 0}), (163603, {'train/accuracy': 0.8294531106948853, 'train/loss': 0.7535117268562317, 'validation/accuracy': 0.7529199719429016, 'validation/loss': 1.0765010118484497, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.6685441732406616, 'test/num_examples': 10000, 'score': 73987.05826282501, 'total_duration': 79770.55769276619, 'accumulated_submission_time': 73987.05826282501, 'accumulated_eval_time': 5767.8042142391205, 'accumulated_logging_time': 7.45538067817688, 'global_step': 163603, 'preemption_count': 0}), (164530, {'train/accuracy': 0.82923823595047, 'train/loss': 0.7566958665847778, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.0820822715759277, 'validation/num_examples': 50000, 'test/accuracy': 0.6342000365257263, 'test/loss': 1.6714333295822144, 'test/num_examples': 10000, 'score': 74407.01922512054, 'total_duration': 80224.93182849884, 'accumulated_submission_time': 74407.01922512054, 'accumulated_eval_time': 5802.109175443649, 'accumulated_logging_time': 7.517110109329224, 'global_step': 164530, 'preemption_count': 0}), (165458, {'train/accuracy': 0.8375976085662842, 'train/loss': 0.7356262803077698, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0842537879943848, 'validation/num_examples': 50000, 'test/accuracy': 0.6332000494003296, 'test/loss': 1.685017704963684, 'test/num_examples': 10000, 'score': 74827.14331531525, 'total_duration': 80680.53797078133, 'accumulated_submission_time': 74827.14331531525, 'accumulated_eval_time': 5837.49352812767, 'accumulated_logging_time': 7.5688605308532715, 'global_step': 165458, 'preemption_count': 0}), (166389, {'train/accuracy': 0.8311718702316284, 'train/loss': 0.7485712170600891, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0757715702056885, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.6692404747009277, 'test/num_examples': 10000, 'score': 75247.37900185585, 'total_duration': 81135.34643173218, 'accumulated_submission_time': 75247.37900185585, 'accumulated_eval_time': 5871.969348192215, 'accumulated_logging_time': 7.618942499160767, 'global_step': 166389, 'preemption_count': 0}), (167317, {'train/accuracy': 0.8334569931030273, 'train/loss': 0.7626773715019226, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.091723918914795, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.6732136011123657, 'test/num_examples': 10000, 'score': 75667.48089122772, 'total_duration': 81590.24653863907, 'accumulated_submission_time': 75667.48089122772, 'accumulated_eval_time': 5906.669553279877, 'accumulated_logging_time': 7.6705286502838135, 'global_step': 167317, 'preemption_count': 0}), (168246, {'train/accuracy': 0.8374999761581421, 'train/loss': 0.7262058258056641, 'validation/accuracy': 0.7564399838447571, 'validation/loss': 1.0680700540542603, 'validation/num_examples': 50000, 'test/accuracy': 0.6396000385284424, 'test/loss': 1.6480201482772827, 'test/num_examples': 10000, 'score': 76087.52441835403, 'total_duration': 82044.95395517349, 'accumulated_submission_time': 76087.52441835403, 'accumulated_eval_time': 5941.224766492844, 'accumulated_logging_time': 7.7321436405181885, 'global_step': 168246, 'preemption_count': 0}), (169175, {'train/accuracy': 0.83607417345047, 'train/loss': 0.7408673167228699, 'validation/accuracy': 0.7595399618148804, 'validation/loss': 1.0654107332229614, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.6457515954971313, 'test/num_examples': 10000, 'score': 76507.65330696106, 'total_duration': 82499.51538085938, 'accumulated_submission_time': 76507.65330696106, 'accumulated_eval_time': 5975.560669898987, 'accumulated_logging_time': 7.782688856124878, 'global_step': 169175, 'preemption_count': 0}), (170103, {'train/accuracy': 0.8408398032188416, 'train/loss': 0.7134418487548828, 'validation/accuracy': 0.7605199813842773, 'validation/loss': 1.0481165647506714, 'validation/num_examples': 50000, 'test/accuracy': 0.6447000503540039, 'test/loss': 1.6202740669250488, 'test/num_examples': 10000, 'score': 76927.95672249794, 'total_duration': 82951.85792160034, 'accumulated_submission_time': 76927.95672249794, 'accumulated_eval_time': 6007.491415500641, 'accumulated_logging_time': 7.8443825244903564, 'global_step': 170103, 'preemption_count': 0}), (171033, {'train/accuracy': 0.8424023389816284, 'train/loss': 0.7111977934837341, 'validation/accuracy': 0.7627399563789368, 'validation/loss': 1.051758050918579, 'validation/num_examples': 50000, 'test/accuracy': 0.6460000276565552, 'test/loss': 1.633678674697876, 'test/num_examples': 10000, 'score': 77348.14902710915, 'total_duration': 83404.94340229034, 'accumulated_submission_time': 77348.14902710915, 'accumulated_eval_time': 6040.283388137817, 'accumulated_logging_time': 7.898629903793335, 'global_step': 171033, 'preemption_count': 0})], 'global_step': 171419}
I0131 12:55:46.112947 140184451094336 submission_runner.py:586] Timing: 77520.33168768883
I0131 12:55:46.113053 140184451094336 submission_runner.py:588] Total number of evals: 185
I0131 12:55:46.113096 140184451094336 submission_runner.py:589] ====================
I0131 12:55:46.113141 140184451094336 submission_runner.py:542] Using RNG seed 3390244169
I0131 12:55:46.114518 140184451094336 submission_runner.py:551] --- Tuning run 2/5 ---
I0131 12:55:46.114636 140184451094336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2.
I0131 12:55:46.121436 140184451094336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2/hparams.json.
I0131 12:55:46.123398 140184451094336 submission_runner.py:206] Initializing dataset.
I0131 12:55:46.134269 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0131 12:55:46.144320 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0131 12:55:46.671094 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0131 12:55:50.855784 140184451094336 submission_runner.py:213] Initializing model.
I0131 12:55:57.126806 140184451094336 submission_runner.py:255] Initializing optimizer.
I0131 12:55:57.584473 140184451094336 submission_runner.py:262] Initializing metrics bundle.
I0131 12:55:57.584632 140184451094336 submission_runner.py:280] Initializing checkpoint and logger.
I0131 12:55:57.682395 140184451094336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2 with prefix checkpoint_
I0131 12:55:57.682525 140184451094336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0131 12:56:13.416805 140184451094336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0131 12:56:28.777836 140184451094336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2/flags_0.json.
I0131 12:56:28.791514 140184451094336 submission_runner.py:314] Starting training loop.
I0131 12:57:02.781183 140022493714176 logging_writer.py:48] [0] global_step=0, grad_norm=0.29195529222488403, loss=6.9077534675598145
I0131 12:57:02.791825 140184451094336 spec.py:321] Evaluating on the training split.
I0131 12:57:11.055870 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 12:57:31.331737 140184451094336 spec.py:349] Evaluating on the test split.
I0131 12:57:32.930423 140184451094336 submission_runner.py:408] Time since start: 64.14s, 	Step: 1, 	{'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 34.00022578239441, 'total_duration': 64.13885116577148, 'accumulated_submission_time': 34.00022578239441, 'accumulated_eval_time': 30.138538599014282, 'accumulated_logging_time': 0}
I0131 12:57:32.939813 140022502106880 logging_writer.py:48] [1] accumulated_eval_time=30.138539, accumulated_logging_time=0, accumulated_submission_time=34.000226, global_step=1, preemption_count=0, score=34.000226, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=64.138851, train/accuracy=0.000859, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0131 12:58:38.379182 140023005427456 logging_writer.py:48] [100] global_step=100, grad_norm=0.37659525871276855, loss=6.9043121337890625
I0131 12:59:24.147273 140022518892288 logging_writer.py:48] [200] global_step=200, grad_norm=0.35097068548202515, loss=6.892667770385742
I0131 13:00:10.307379 140023005427456 logging_writer.py:48] [300] global_step=300, grad_norm=0.518757700920105, loss=6.843148708343506
I0131 13:00:57.007365 140022518892288 logging_writer.py:48] [400] global_step=400, grad_norm=0.7402588725090027, loss=6.812590599060059
I0131 13:01:43.888309 140023005427456 logging_writer.py:48] [500] global_step=500, grad_norm=0.5377245545387268, loss=6.813272476196289
I0131 13:02:30.386947 140022518892288 logging_writer.py:48] [600] global_step=600, grad_norm=1.023477554321289, loss=6.75239372253418
I0131 13:03:16.957121 140023005427456 logging_writer.py:48] [700] global_step=700, grad_norm=0.7056362628936768, loss=6.676258563995361
I0131 13:04:03.242769 140022518892288 logging_writer.py:48] [800] global_step=800, grad_norm=0.8845072984695435, loss=6.6315999031066895
I0131 13:04:32.970065 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:04:43.857814 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:05:07.048097 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:05:08.645700 140184451094336 submission_runner.py:408] Time since start: 519.85s, 	Step: 864, 	{'train/accuracy': 0.01708984375, 'train/loss': 6.374186038970947, 'validation/accuracy': 0.016920000314712524, 'validation/loss': 6.388251781463623, 'validation/num_examples': 50000, 'test/accuracy': 0.012700000777840614, 'test/loss': 6.433145999908447, 'test/num_examples': 10000, 'score': 453.9772663116455, 'total_duration': 519.8541326522827, 'accumulated_submission_time': 453.9772663116455, 'accumulated_eval_time': 65.81416869163513, 'accumulated_logging_time': 0.01839923858642578}
I0131 13:05:08.664678 140023005427456 logging_writer.py:48] [864] accumulated_eval_time=65.814169, accumulated_logging_time=0.018399, accumulated_submission_time=453.977266, global_step=864, preemption_count=0, score=453.977266, test/accuracy=0.012700, test/loss=6.433146, test/num_examples=10000, total_duration=519.854133, train/accuracy=0.017090, train/loss=6.374186, validation/accuracy=0.016920, validation/loss=6.388252, validation/num_examples=50000
I0131 13:05:23.443125 140022518892288 logging_writer.py:48] [900] global_step=900, grad_norm=0.7145282030105591, loss=6.583418846130371
I0131 13:06:06.632889 140023005427456 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0284472703933716, loss=6.602406024932861
I0131 13:06:52.878693 140022518892288 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8624967932701111, loss=6.633641242980957
I0131 13:07:39.493728 140023005427456 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.6164495944976807, loss=6.5003662109375
I0131 13:08:26.076062 140022518892288 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9265348315238953, loss=6.677700996398926
I0131 13:09:11.796500 140023005427456 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9964691400527954, loss=6.46776819229126
I0131 13:09:58.113837 140022518892288 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.273640751838684, loss=6.703912734985352
I0131 13:10:44.171257 140023005427456 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.4053990840911865, loss=6.339003562927246
I0131 13:11:30.260233 140022518892288 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0985726118087769, loss=6.329768657684326
I0131 13:12:08.837773 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:12:19.611515 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:12:46.198884 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:12:47.804099 140184451094336 submission_runner.py:408] Time since start: 979.01s, 	Step: 1785, 	{'train/accuracy': 0.05035156011581421, 'train/loss': 5.795556545257568, 'validation/accuracy': 0.04853999987244606, 'validation/loss': 5.824850559234619, 'validation/num_examples': 50000, 'test/accuracy': 0.03680000081658363, 'test/loss': 5.947690486907959, 'test/num_examples': 10000, 'score': 874.0901634693146, 'total_duration': 979.0125117301941, 'accumulated_submission_time': 874.0901634693146, 'accumulated_eval_time': 104.78045845031738, 'accumulated_logging_time': 0.05016160011291504}
I0131 13:12:47.820819 140023005427456 logging_writer.py:48] [1785] accumulated_eval_time=104.780458, accumulated_logging_time=0.050162, accumulated_submission_time=874.090163, global_step=1785, preemption_count=0, score=874.090163, test/accuracy=0.036800, test/loss=5.947690, test/num_examples=10000, total_duration=979.012512, train/accuracy=0.050352, train/loss=5.795557, validation/accuracy=0.048540, validation/loss=5.824851, validation/num_examples=50000
I0131 13:12:54.210495 140022518892288 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0871150493621826, loss=6.313601016998291
I0131 13:13:36.012966 140023005427456 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1947100162506104, loss=6.277873992919922
I0131 13:14:22.187219 140022518892288 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3580102920532227, loss=6.5524373054504395
I0131 13:15:08.276730 140023005427456 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1504956483840942, loss=6.203086853027344
I0131 13:15:54.505317 140022518892288 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.5609761476516724, loss=6.249909400939941
I0131 13:16:40.745821 140023005427456 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.155069351196289, loss=6.233799934387207
I0131 13:17:26.841057 140022518892288 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3042577505111694, loss=6.160651206970215
I0131 13:18:13.023690 140023005427456 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.085023283958435, loss=6.590906620025635
I0131 13:18:59.266367 140022518892288 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.390308141708374, loss=6.167694091796875
I0131 13:19:45.252272 140023005427456 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.104029655456543, loss=6.38242244720459
I0131 13:19:48.189979 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:19:58.893487 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:20:25.861618 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:20:27.457939 140184451094336 submission_runner.py:408] Time since start: 1438.67s, 	Step: 2708, 	{'train/accuracy': 0.07275390625, 'train/loss': 5.425717830657959, 'validation/accuracy': 0.06825999915599823, 'validation/loss': 5.4650092124938965, 'validation/num_examples': 50000, 'test/accuracy': 0.05020000413060188, 'test/loss': 5.638961315155029, 'test/num_examples': 10000, 'score': 1294.3986456394196, 'total_duration': 1438.6663777828217, 'accumulated_submission_time': 1294.3986456394196, 'accumulated_eval_time': 144.04840278625488, 'accumulated_logging_time': 0.07940435409545898}
I0131 13:20:27.472532 140022518892288 logging_writer.py:48] [2708] accumulated_eval_time=144.048403, accumulated_logging_time=0.079404, accumulated_submission_time=1294.398646, global_step=2708, preemption_count=0, score=1294.398646, test/accuracy=0.050200, test/loss=5.638961, test/num_examples=10000, total_duration=1438.666378, train/accuracy=0.072754, train/loss=5.425718, validation/accuracy=0.068260, validation/loss=5.465009, validation/num_examples=50000
I0131 13:21:05.612642 140023005427456 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.182074785232544, loss=6.154318332672119
I0131 13:21:51.725061 140022518892288 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2605969905853271, loss=6.131096839904785
I0131 13:22:37.906884 140023005427456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8041456937789917, loss=6.443828582763672
I0131 13:23:24.117516 140022518892288 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.216735601425171, loss=6.0279669761657715
I0131 13:24:10.318892 140023005427456 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8657085299491882, loss=6.637199401855469
I0131 13:24:56.429797 140022518892288 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1460797786712646, loss=6.030200004577637
I0131 13:25:42.483913 140023005427456 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1979440450668335, loss=6.061205863952637
I0131 13:26:28.686284 140022518892288 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0631755590438843, loss=5.994271278381348
I0131 13:27:14.856745 140023005427456 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1721868515014648, loss=6.055196762084961
I0131 13:27:27.766017 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:27:38.188241 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:28:05.535733 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:28:07.133475 140184451094336 submission_runner.py:408] Time since start: 1898.34s, 	Step: 3630, 	{'train/accuracy': 0.10914061963558197, 'train/loss': 5.063491344451904, 'validation/accuracy': 0.09845999628305435, 'validation/loss': 5.12637186050415, 'validation/num_examples': 50000, 'test/accuracy': 0.07569999992847443, 'test/loss': 5.355987071990967, 'test/num_examples': 10000, 'score': 1714.635325908661, 'total_duration': 1898.3418984413147, 'accumulated_submission_time': 1714.635325908661, 'accumulated_eval_time': 183.41586256027222, 'accumulated_logging_time': 0.10338544845581055}
I0131 13:28:07.149949 140022518892288 logging_writer.py:48] [3630] accumulated_eval_time=183.415863, accumulated_logging_time=0.103385, accumulated_submission_time=1714.635326, global_step=3630, preemption_count=0, score=1714.635326, test/accuracy=0.075700, test/loss=5.355987, test/num_examples=10000, total_duration=1898.341898, train/accuracy=0.109141, train/loss=5.063491, validation/accuracy=0.098460, validation/loss=5.126372, validation/num_examples=50000
I0131 13:28:35.546300 140023005427456 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9261065125465393, loss=6.27821683883667
I0131 13:29:21.129336 140022518892288 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8839000463485718, loss=6.117744445800781
I0131 13:30:07.316097 140023005427456 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.1769442558288574, loss=5.985755920410156
I0131 13:30:53.673353 140022518892288 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0503231287002563, loss=6.443691730499268
I0131 13:31:39.938816 140023005427456 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.1038028001785278, loss=6.048620700836182
I0131 13:32:26.109022 140022518892288 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.069911003112793, loss=5.823292255401611
I0131 13:33:12.291247 140023005427456 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.0877108573913574, loss=5.814473628997803
I0131 13:33:58.181758 140022518892288 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8782280683517456, loss=6.268486499786377
I0131 13:34:44.358499 140023005427456 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.1116664409637451, loss=5.759600639343262
I0131 13:35:07.514203 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:35:18.263875 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:35:40.990415 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:35:42.594917 140184451094336 submission_runner.py:408] Time since start: 2353.80s, 	Step: 4552, 	{'train/accuracy': 0.16447265446186066, 'train/loss': 4.604984760284424, 'validation/accuracy': 0.14311999082565308, 'validation/loss': 4.7212934494018555, 'validation/num_examples': 50000, 'test/accuracy': 0.10650000721216202, 'test/loss': 5.0176777839660645, 'test/num_examples': 10000, 'score': 2134.942580461502, 'total_duration': 2353.8033304214478, 'accumulated_submission_time': 2134.942580461502, 'accumulated_eval_time': 218.49655675888062, 'accumulated_logging_time': 0.12989163398742676}
I0131 13:35:42.613409 140022518892288 logging_writer.py:48] [4552] accumulated_eval_time=218.496557, accumulated_logging_time=0.129892, accumulated_submission_time=2134.942580, global_step=4552, preemption_count=0, score=2134.942580, test/accuracy=0.106500, test/loss=5.017678, test/num_examples=10000, total_duration=2353.803330, train/accuracy=0.164473, train/loss=4.604985, validation/accuracy=0.143120, validation/loss=4.721293, validation/num_examples=50000
I0131 13:36:02.217890 140023005427456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8092257380485535, loss=6.620126247406006
I0131 13:36:45.977847 140022518892288 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1231447458267212, loss=5.814673900604248
I0131 13:37:32.229143 140023005427456 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0549356937408447, loss=5.928411960601807
I0131 13:38:18.402897 140022518892288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8754273056983948, loss=6.546875953674316
I0131 13:39:04.632148 140023005427456 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9114704132080078, loss=6.358409881591797
I0131 13:39:50.838307 140022518892288 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.3197991847991943, loss=5.607156753540039
I0131 13:40:37.667618 140023005427456 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.1260432004928589, loss=5.7886738777160645
I0131 13:41:23.698115 140022518892288 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.1547670364379883, loss=5.589641571044922
I0131 13:42:10.015104 140023005427456 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0770574808120728, loss=6.01561975479126
I0131 13:42:43.003982 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:42:53.689742 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:43:15.299256 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:43:16.910478 140184451094336 submission_runner.py:408] Time since start: 2808.12s, 	Step: 5473, 	{'train/accuracy': 0.19630858302116394, 'train/loss': 4.357907772064209, 'validation/accuracy': 0.17927999794483185, 'validation/loss': 4.431841850280762, 'validation/num_examples': 50000, 'test/accuracy': 0.13819999992847443, 'test/loss': 4.75654411315918, 'test/num_examples': 10000, 'score': 2555.2755966186523, 'total_duration': 2808.118916273117, 'accumulated_submission_time': 2555.2755966186523, 'accumulated_eval_time': 252.4030647277832, 'accumulated_logging_time': 0.15917325019836426}
I0131 13:43:16.926591 140022518892288 logging_writer.py:48] [5473] accumulated_eval_time=252.403065, accumulated_logging_time=0.159173, accumulated_submission_time=2555.275597, global_step=5473, preemption_count=0, score=2555.275597, test/accuracy=0.138200, test/loss=4.756544, test/num_examples=10000, total_duration=2808.118916, train/accuracy=0.196309, train/loss=4.357908, validation/accuracy=0.179280, validation/loss=4.431842, validation/num_examples=50000
I0131 13:43:28.132464 140023005427456 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.3328410387039185, loss=5.692628383636475
I0131 13:44:10.523457 140022518892288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9451590180397034, loss=6.012711524963379
I0131 13:44:56.585538 140023005427456 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.0956878662109375, loss=5.703676700592041
I0131 13:45:42.724356 140022518892288 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.1351709365844727, loss=5.505777835845947
I0131 13:46:28.747420 140023005427456 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9081092476844788, loss=5.779576301574707
I0131 13:47:14.961689 140022518892288 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0812640190124512, loss=5.35504674911499
I0131 13:48:01.126655 140023005427456 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.0388342142105103, loss=6.542250633239746
I0131 13:48:47.104221 140022518892288 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8380175232887268, loss=6.016355514526367
I0131 13:49:33.463556 140023005427456 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8516576290130615, loss=6.336607933044434
I0131 13:50:17.140468 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:50:27.638793 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:50:51.996268 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:50:53.593991 140184451094336 submission_runner.py:408] Time since start: 3264.80s, 	Step: 6396, 	{'train/accuracy': 0.2514062523841858, 'train/loss': 3.91365122795105, 'validation/accuracy': 0.23061999678611755, 'validation/loss': 4.019598484039307, 'validation/num_examples': 50000, 'test/accuracy': 0.1730000078678131, 'test/loss': 4.418845176696777, 'test/num_examples': 10000, 'score': 2975.4280049800873, 'total_duration': 3264.80242729187, 'accumulated_submission_time': 2975.4280049800873, 'accumulated_eval_time': 288.8565876483917, 'accumulated_logging_time': 0.19016075134277344}
I0131 13:50:53.610479 140022518892288 logging_writer.py:48] [6396] accumulated_eval_time=288.856588, accumulated_logging_time=0.190161, accumulated_submission_time=2975.428005, global_step=6396, preemption_count=0, score=2975.428005, test/accuracy=0.173000, test/loss=4.418845, test/num_examples=10000, total_duration=3264.802427, train/accuracy=0.251406, train/loss=3.913651, validation/accuracy=0.230620, validation/loss=4.019598, validation/num_examples=50000
I0131 13:50:55.618494 140023005427456 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.04635488986969, loss=5.3353424072265625
I0131 13:51:36.972684 140022518892288 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.1356792449951172, loss=6.429257869720459
I0131 13:52:23.026400 140023005427456 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.1704479455947876, loss=5.330953121185303
I0131 13:53:09.082074 140022518892288 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.0871269702911377, loss=5.244077205657959
I0131 13:53:55.151867 140023005427456 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7243725657463074, loss=6.395666599273682
I0131 13:54:41.035855 140022518892288 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.0137622356414795, loss=5.361326217651367
I0131 13:55:27.218197 140023005427456 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0365322828292847, loss=5.264532566070557
I0131 13:56:13.437330 140022518892288 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.2397123575210571, loss=5.199209213256836
I0131 13:56:59.175550 140023005427456 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9441169500350952, loss=6.12974739074707
I0131 13:57:45.421108 140022518892288 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.3236589431762695, loss=5.194558143615723
I0131 13:57:53.702053 140184451094336 spec.py:321] Evaluating on the training split.
I0131 13:58:04.865290 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 13:58:25.956270 140184451094336 spec.py:349] Evaluating on the test split.
I0131 13:58:27.554471 140184451094336 submission_runner.py:408] Time since start: 3718.76s, 	Step: 7320, 	{'train/accuracy': 0.2916015684604645, 'train/loss': 3.6628291606903076, 'validation/accuracy': 0.26499998569488525, 'validation/loss': 3.8069074153900146, 'validation/num_examples': 50000, 'test/accuracy': 0.20390000939369202, 'test/loss': 4.221038818359375, 'test/num_examples': 10000, 'score': 3395.463133573532, 'total_duration': 3718.7628943920135, 'accumulated_submission_time': 3395.463133573532, 'accumulated_eval_time': 322.7090003490448, 'accumulated_logging_time': 0.2164924144744873}
I0131 13:58:27.572917 140023005427456 logging_writer.py:48] [7320] accumulated_eval_time=322.709000, accumulated_logging_time=0.216492, accumulated_submission_time=3395.463134, global_step=7320, preemption_count=0, score=3395.463134, test/accuracy=0.203900, test/loss=4.221039, test/num_examples=10000, total_duration=3718.762894, train/accuracy=0.291602, train/loss=3.662829, validation/accuracy=0.265000, validation/loss=3.806907, validation/num_examples=50000
I0131 13:58:59.996352 140022518892288 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7948556542396545, loss=6.338678359985352
I0131 13:59:45.709372 140023005427456 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.034121036529541, loss=5.151541709899902
I0131 14:00:32.002317 140022518892288 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.2494865655899048, loss=5.0778045654296875
I0131 14:01:18.193553 140023005427456 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.157470464706421, loss=5.221728801727295
I0131 14:02:04.441884 140022518892288 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9630364179611206, loss=5.150274276733398
I0131 14:02:50.379321 140023005427456 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7295755743980408, loss=5.879190921783447
I0131 14:03:36.515907 140022518892288 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.1314644813537598, loss=5.119576454162598
I0131 14:04:22.724709 140023005427456 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0183881521224976, loss=5.1796555519104
I0131 14:05:09.169151 140022518892288 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9896371364593506, loss=5.229722023010254
I0131 14:05:27.997078 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:05:38.740760 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:06:01.364612 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:06:02.965738 140184451094336 submission_runner.py:408] Time since start: 4174.17s, 	Step: 8243, 	{'train/accuracy': 0.32517576217651367, 'train/loss': 3.437488555908203, 'validation/accuracy': 0.3050200045108795, 'validation/loss': 3.5397071838378906, 'validation/num_examples': 50000, 'test/accuracy': 0.23070001602172852, 'test/loss': 4.009162425994873, 'test/num_examples': 10000, 'score': 3815.831436395645, 'total_duration': 4174.174175024033, 'accumulated_submission_time': 3815.831436395645, 'accumulated_eval_time': 357.6776604652405, 'accumulated_logging_time': 0.24421310424804688}
I0131 14:06:02.982398 140023005427456 logging_writer.py:48] [8243] accumulated_eval_time=357.677660, accumulated_logging_time=0.244213, accumulated_submission_time=3815.831436, global_step=8243, preemption_count=0, score=3815.831436, test/accuracy=0.230700, test/loss=4.009162, test/num_examples=10000, total_duration=4174.174175, train/accuracy=0.325176, train/loss=3.437489, validation/accuracy=0.305020, validation/loss=3.539707, validation/num_examples=50000
I0131 14:06:26.197097 140022518892288 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0680134296417236, loss=5.021649360656738
I0131 14:07:10.398310 140023005427456 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8680727481842041, loss=6.064217567443848
I0131 14:07:56.584815 140022518892288 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8807717561721802, loss=5.243906497955322
I0131 14:08:43.116255 140023005427456 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.1406748294830322, loss=4.938409328460693
I0131 14:09:29.181521 140022518892288 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8308937549591064, loss=5.963119029998779
I0131 14:10:15.365216 140023005427456 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.964399516582489, loss=4.884458065032959
I0131 14:11:01.633798 140022518892288 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.891046941280365, loss=5.932561874389648
I0131 14:11:47.845340 140023005427456 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7126812934875488, loss=6.295372486114502
I0131 14:12:34.275830 140022518892288 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.8284870982170105, loss=6.343450546264648
I0131 14:13:03.054708 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:13:13.532890 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:13:35.711421 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:13:37.316612 140184451094336 submission_runner.py:408] Time since start: 4628.53s, 	Step: 9164, 	{'train/accuracy': 0.3589257597923279, 'train/loss': 3.2390763759613037, 'validation/accuracy': 0.32979997992515564, 'validation/loss': 3.377380609512329, 'validation/num_examples': 50000, 'test/accuracy': 0.254800021648407, 'test/loss': 3.8746755123138428, 'test/num_examples': 10000, 'score': 4235.847739696503, 'total_duration': 4628.525053501129, 'accumulated_submission_time': 4235.847739696503, 'accumulated_eval_time': 391.93957924842834, 'accumulated_logging_time': 0.2702317237854004}
I0131 14:13:37.331701 140023005427456 logging_writer.py:48] [9164] accumulated_eval_time=391.939579, accumulated_logging_time=0.270232, accumulated_submission_time=4235.847740, global_step=9164, preemption_count=0, score=4235.847740, test/accuracy=0.254800, test/loss=3.874676, test/num_examples=10000, total_duration=4628.525054, train/accuracy=0.358926, train/loss=3.239076, validation/accuracy=0.329800, validation/loss=3.377381, validation/num_examples=50000
I0131 14:13:52.145940 140022518892288 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9899706840515137, loss=4.871068954467773
I0131 14:14:35.495578 140023005427456 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8738852143287659, loss=5.640869140625
I0131 14:15:21.458060 140022518892288 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9909923672676086, loss=5.104598045349121
I0131 14:16:07.928581 140023005427456 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9708733558654785, loss=5.326377868652344
I0131 14:16:53.679946 140022518892288 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.0195046663284302, loss=4.853391170501709
I0131 14:17:39.746541 140023005427456 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7555609941482544, loss=5.538897514343262
I0131 14:18:25.849987 140022518892288 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9266627430915833, loss=4.743160724639893
I0131 14:19:12.120311 140023005427456 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8967714905738831, loss=4.958263397216797
I0131 14:19:58.133170 140022518892288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8026642203330994, loss=5.798431396484375
I0131 14:20:37.538080 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:20:48.161574 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:21:14.490194 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:21:16.089621 140184451094336 submission_runner.py:408] Time since start: 5087.30s, 	Step: 10086, 	{'train/accuracy': 0.3991015553474426, 'train/loss': 3.000910997390747, 'validation/accuracy': 0.3657599985599518, 'validation/loss': 3.1630349159240723, 'validation/num_examples': 50000, 'test/accuracy': 0.281900018453598, 'test/loss': 3.6769216060638428, 'test/num_examples': 10000, 'score': 4655.996357917786, 'total_duration': 5087.298045635223, 'accumulated_submission_time': 4655.996357917786, 'accumulated_eval_time': 430.49109721183777, 'accumulated_logging_time': 0.2951619625091553}
I0131 14:21:16.107487 140023005427456 logging_writer.py:48] [10086] accumulated_eval_time=430.491097, accumulated_logging_time=0.295162, accumulated_submission_time=4655.996358, global_step=10086, preemption_count=0, score=4655.996358, test/accuracy=0.281900, test/loss=3.676922, test/num_examples=10000, total_duration=5087.298046, train/accuracy=0.399102, train/loss=3.000911, validation/accuracy=0.365760, validation/loss=3.163035, validation/num_examples=50000
I0131 14:21:22.109030 140022518892288 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9185150265693665, loss=4.742156028747559
I0131 14:22:04.228167 140023005427456 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.04112708568573, loss=4.930179119110107
I0131 14:22:50.160310 140022518892288 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9331660866737366, loss=4.761439800262451
I0131 14:23:36.339060 140023005427456 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.924524188041687, loss=4.596560478210449
I0131 14:24:22.711891 140022518892288 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9925108551979065, loss=4.669891357421875
I0131 14:25:08.643200 140023005427456 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6134408712387085, loss=5.776593208312988
I0131 14:25:54.584403 140022518892288 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7855848670005798, loss=5.9320478439331055
I0131 14:26:40.450499 140023005427456 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.0558656454086304, loss=4.655591011047363
I0131 14:27:26.428051 140022518892288 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.885527491569519, loss=4.9465227127075195
I0131 14:28:12.946070 140023005427456 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8713772296905518, loss=4.797397136688232
I0131 14:28:16.118134 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:28:26.628942 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:28:49.205132 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:28:50.804111 140184451094336 submission_runner.py:408] Time since start: 5542.01s, 	Step: 11009, 	{'train/accuracy': 0.4095703065395355, 'train/loss': 2.9577596187591553, 'validation/accuracy': 0.3806999921798706, 'validation/loss': 3.086174726486206, 'validation/num_examples': 50000, 'test/accuracy': 0.2939999997615814, 'test/loss': 3.6142802238464355, 'test/num_examples': 10000, 'score': 5075.949893951416, 'total_duration': 5542.012524604797, 'accumulated_submission_time': 5075.949893951416, 'accumulated_eval_time': 465.17704224586487, 'accumulated_logging_time': 0.3227870464324951}
I0131 14:28:50.822783 140022518892288 logging_writer.py:48] [11009] accumulated_eval_time=465.177042, accumulated_logging_time=0.322787, accumulated_submission_time=5075.949894, global_step=11009, preemption_count=0, score=5075.949894, test/accuracy=0.294000, test/loss=3.614280, test/num_examples=10000, total_duration=5542.012525, train/accuracy=0.409570, train/loss=2.957760, validation/accuracy=0.380700, validation/loss=3.086175, validation/num_examples=50000
I0131 14:29:28.222334 140023005427456 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9633627533912659, loss=4.717619895935059
I0131 14:30:14.028438 140022518892288 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7357022166252136, loss=5.611048221588135
I0131 14:31:00.137607 140023005427456 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9245373606681824, loss=4.5691094398498535
I0131 14:31:46.440268 140022518892288 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.845879852771759, loss=5.296807289123535
I0131 14:32:32.894440 140023005427456 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9728795886039734, loss=4.71406364440918
I0131 14:33:19.247684 140022518892288 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9195222854614258, loss=4.664392471313477
I0131 14:34:05.074173 140023005427456 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9809478521347046, loss=4.617241859436035
I0131 14:34:51.042350 140022518892288 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.875779926776886, loss=4.553348064422607
I0131 14:35:37.110431 140023005427456 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0508311986923218, loss=4.55819845199585
I0131 14:35:51.056034 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:36:01.685190 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:36:23.831996 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:36:25.428214 140184451094336 submission_runner.py:408] Time since start: 5996.64s, 	Step: 11932, 	{'train/accuracy': 0.45369139313697815, 'train/loss': 2.704256534576416, 'validation/accuracy': 0.4152999818325043, 'validation/loss': 2.878851890563965, 'validation/num_examples': 50000, 'test/accuracy': 0.32170000672340393, 'test/loss': 3.442434549331665, 'test/num_examples': 10000, 'score': 5496.126168727875, 'total_duration': 5996.636655807495, 'accumulated_submission_time': 5496.126168727875, 'accumulated_eval_time': 499.54923462867737, 'accumulated_logging_time': 0.35118961334228516}
I0131 14:36:25.444987 140022518892288 logging_writer.py:48] [11932] accumulated_eval_time=499.549235, accumulated_logging_time=0.351190, accumulated_submission_time=5496.126169, global_step=11932, preemption_count=0, score=5496.126169, test/accuracy=0.321700, test/loss=3.442435, test/num_examples=10000, total_duration=5996.636656, train/accuracy=0.453691, train/loss=2.704257, validation/accuracy=0.415300, validation/loss=2.878852, validation/num_examples=50000
I0131 14:36:53.061413 140023005427456 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9333935976028442, loss=4.950097560882568
I0131 14:37:38.014621 140022518892288 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9307103157043457, loss=4.528705596923828
I0131 14:38:24.435868 140023005427456 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8964753150939941, loss=4.654054164886475
I0131 14:39:10.914981 140022518892288 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6751731634140015, loss=6.1029181480407715
I0131 14:39:56.885783 140023005427456 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6873469948768616, loss=5.3751068115234375
I0131 14:40:43.364689 140022518892288 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6381921172142029, loss=5.867323875427246
I0131 14:41:29.311817 140023005427456 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9790400862693787, loss=4.444095611572266
I0131 14:42:15.784309 140022518892288 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7913154363632202, loss=5.403439044952393
I0131 14:43:02.062340 140023005427456 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8698022961616516, loss=4.779932022094727
I0131 14:43:25.495844 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:43:36.119142 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:43:57.342138 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:43:58.945031 140184451094336 submission_runner.py:408] Time since start: 6450.15s, 	Step: 12852, 	{'train/accuracy': 0.47328123450279236, 'train/loss': 2.620152473449707, 'validation/accuracy': 0.43347999453544617, 'validation/loss': 2.793245553970337, 'validation/num_examples': 50000, 'test/accuracy': 0.33250001072883606, 'test/loss': 3.3532426357269287, 'test/num_examples': 10000, 'score': 5916.12104177475, 'total_duration': 6450.153452396393, 'accumulated_submission_time': 5916.12104177475, 'accumulated_eval_time': 532.9983906745911, 'accumulated_logging_time': 0.3771047592163086}
I0131 14:43:58.965288 140022518892288 logging_writer.py:48] [12852] accumulated_eval_time=532.998391, accumulated_logging_time=0.377105, accumulated_submission_time=5916.121042, global_step=12852, preemption_count=0, score=5916.121042, test/accuracy=0.332500, test/loss=3.353243, test/num_examples=10000, total_duration=6450.153452, train/accuracy=0.473281, train/loss=2.620152, validation/accuracy=0.433480, validation/loss=2.793246, validation/num_examples=50000
I0131 14:44:18.578229 140023005427456 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.75663161277771, loss=5.876338005065918
I0131 14:45:02.533876 140022518892288 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8272196650505066, loss=5.414770603179932
I0131 14:45:48.559253 140023005427456 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9457383751869202, loss=4.494048595428467
I0131 14:46:34.800760 140022518892288 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.630266010761261, loss=6.088529586791992
I0131 14:47:20.862560 140023005427456 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.04555344581604, loss=4.418983459472656
I0131 14:48:07.216288 140022518892288 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9169455170631409, loss=4.3996968269348145
I0131 14:48:53.418218 140023005427456 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9527700543403625, loss=4.374059677124023
I0131 14:49:39.630776 140022518892288 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0874110460281372, loss=4.420726776123047
I0131 14:50:25.978682 140023005427456 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9660406708717346, loss=4.404197692871094
I0131 14:50:59.430015 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:51:10.265665 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:51:33.716417 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:51:35.319827 140184451094336 submission_runner.py:408] Time since start: 6906.53s, 	Step: 13774, 	{'train/accuracy': 0.4964648187160492, 'train/loss': 2.481999397277832, 'validation/accuracy': 0.4541199803352356, 'validation/loss': 2.691492795944214, 'validation/num_examples': 50000, 'test/accuracy': 0.35630002617836, 'test/loss': 3.2585787773132324, 'test/num_examples': 10000, 'score': 6336.528871059418, 'total_duration': 6906.528246641159, 'accumulated_submission_time': 6336.528871059418, 'accumulated_eval_time': 568.8881900310516, 'accumulated_logging_time': 0.4073190689086914}
I0131 14:51:35.338163 140022518892288 logging_writer.py:48] [13774] accumulated_eval_time=568.888190, accumulated_logging_time=0.407319, accumulated_submission_time=6336.528871, global_step=13774, preemption_count=0, score=6336.528871, test/accuracy=0.356300, test/loss=3.258579, test/num_examples=10000, total_duration=6906.528247, train/accuracy=0.496465, train/loss=2.481999, validation/accuracy=0.454120, validation/loss=2.691493, validation/num_examples=50000
I0131 14:51:46.150048 140023005427456 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6772873401641846, loss=5.749547958374023
I0131 14:52:28.743798 140022518892288 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.1032204627990723, loss=4.3669233322143555
I0131 14:53:14.772505 140023005427456 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5990667343139648, loss=6.007374286651611
I0131 14:54:00.950645 140022518892288 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.856028139591217, loss=4.347743988037109
I0131 14:54:47.212996 140023005427456 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.963493824005127, loss=4.365928649902344
I0131 14:55:33.236233 140022518892288 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8722629547119141, loss=4.367368698120117
I0131 14:56:19.487477 140023005427456 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.8524234294891357, loss=4.392614364624023
I0131 14:57:05.646949 140022518892288 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9351778030395508, loss=4.318496227264404
I0131 14:57:51.348404 140023005427456 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8426782488822937, loss=4.573448657989502
I0131 14:58:35.641556 140184451094336 spec.py:321] Evaluating on the training split.
I0131 14:58:46.271281 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 14:59:04.843332 140184451094336 spec.py:349] Evaluating on the test split.
I0131 14:59:06.454460 140184451094336 submission_runner.py:408] Time since start: 7357.66s, 	Step: 14697, 	{'train/accuracy': 0.5178515315055847, 'train/loss': 2.393428325653076, 'validation/accuracy': 0.47537997364997864, 'validation/loss': 2.5705621242523193, 'validation/num_examples': 50000, 'test/accuracy': 0.37450000643730164, 'test/loss': 3.150657892227173, 'test/num_examples': 10000, 'score': 6756.774499177933, 'total_duration': 7357.662885427475, 'accumulated_submission_time': 6756.774499177933, 'accumulated_eval_time': 599.7011110782623, 'accumulated_logging_time': 0.4356505870819092}
I0131 14:59:06.474594 140022518892288 logging_writer.py:48] [14697] accumulated_eval_time=599.701111, accumulated_logging_time=0.435651, accumulated_submission_time=6756.774499, global_step=14697, preemption_count=0, score=6756.774499, test/accuracy=0.374500, test/loss=3.150658, test/num_examples=10000, total_duration=7357.662885, train/accuracy=0.517852, train/loss=2.393428, validation/accuracy=0.475380, validation/loss=2.570562, validation/num_examples=50000
I0131 14:59:08.083651 140023005427456 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7138874530792236, loss=5.680973529815674
I0131 14:59:50.205836 140022518892288 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7796211242675781, loss=5.617693901062012
I0131 15:00:36.439327 140023005427456 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7369863390922546, loss=5.017094612121582
I0131 15:01:23.105434 140022518892288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7043502330780029, loss=5.194038391113281
I0131 15:02:09.335336 140023005427456 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9552991390228271, loss=4.280689239501953
I0131 15:02:55.112108 140022518892288 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6973598003387451, loss=5.995480537414551
I0131 15:03:41.200187 140023005427456 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9064242839813232, loss=4.264131546020508
I0131 15:04:27.164491 140022518892288 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8571711778640747, loss=4.465042591094971
I0131 15:05:13.184248 140023005427456 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8817058801651001, loss=4.349506855010986
I0131 15:05:59.060759 140022518892288 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6982482075691223, loss=5.163825035095215
I0131 15:06:06.571382 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:06:16.934262 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:06:40.020443 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:06:41.627246 140184451094336 submission_runner.py:408] Time since start: 7812.84s, 	Step: 15618, 	{'train/accuracy': 0.5367968678474426, 'train/loss': 2.243785858154297, 'validation/accuracy': 0.49685999751091003, 'validation/loss': 2.430511713027954, 'validation/num_examples': 50000, 'test/accuracy': 0.3872000277042389, 'test/loss': 3.020982027053833, 'test/num_examples': 10000, 'score': 7176.8117735385895, 'total_duration': 7812.835678100586, 'accumulated_submission_time': 7176.8117735385895, 'accumulated_eval_time': 634.7569868564606, 'accumulated_logging_time': 0.4672560691833496}
I0131 15:06:41.646077 140023005427456 logging_writer.py:48] [15618] accumulated_eval_time=634.756987, accumulated_logging_time=0.467256, accumulated_submission_time=7176.811774, global_step=15618, preemption_count=0, score=7176.811774, test/accuracy=0.387200, test/loss=3.020982, test/num_examples=10000, total_duration=7812.835678, train/accuracy=0.536797, train/loss=2.243786, validation/accuracy=0.496860, validation/loss=2.430512, validation/num_examples=50000
I0131 15:07:14.835253 140022518892288 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9860117435455322, loss=4.188012599945068
I0131 15:08:01.017229 140023005427456 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9661665558815002, loss=4.160508155822754
I0131 15:08:47.108097 140022518892288 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.904959499835968, loss=4.265626907348633
I0131 15:09:33.493391 140023005427456 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0068066120147705, loss=4.243217945098877
I0131 15:10:19.695276 140022518892288 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.736232578754425, loss=5.640514850616455
I0131 15:11:05.708244 140023005427456 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6265028119087219, loss=5.897406578063965
I0131 15:11:52.175015 140022518892288 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8704465627670288, loss=4.621498107910156
I0131 15:12:38.266238 140023005427456 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7727037072181702, loss=5.370848655700684
I0131 15:13:24.281249 140022518892288 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9990496039390564, loss=4.052417755126953
I0131 15:13:41.943054 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:13:52.313257 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:14:15.118171 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:14:16.718343 140184451094336 submission_runner.py:408] Time since start: 8267.93s, 	Step: 16540, 	{'train/accuracy': 0.562207043170929, 'train/loss': 2.1498546600341797, 'validation/accuracy': 0.507319986820221, 'validation/loss': 2.4012911319732666, 'validation/num_examples': 50000, 'test/accuracy': 0.3929000198841095, 'test/loss': 2.9950408935546875, 'test/num_examples': 10000, 'score': 7597.053501844406, 'total_duration': 8267.926774263382, 'accumulated_submission_time': 7597.053501844406, 'accumulated_eval_time': 669.5322709083557, 'accumulated_logging_time': 0.4950077533721924}
I0131 15:14:16.737002 140023005427456 logging_writer.py:48] [16540] accumulated_eval_time=669.532271, accumulated_logging_time=0.495008, accumulated_submission_time=7597.053502, global_step=16540, preemption_count=0, score=7597.053502, test/accuracy=0.392900, test/loss=2.995041, test/num_examples=10000, total_duration=8267.926774, train/accuracy=0.562207, train/loss=2.149855, validation/accuracy=0.507320, validation/loss=2.401291, validation/num_examples=50000
I0131 15:14:41.417831 140022518892288 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.9797995686531067, loss=4.137617588043213
I0131 15:15:25.712859 140023005427456 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9477752447128296, loss=4.244416236877441
I0131 15:16:11.841388 140022518892288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6775542497634888, loss=5.552659511566162
I0131 15:16:58.206536 140023005427456 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8666793704032898, loss=4.086520195007324
I0131 15:17:44.231608 140022518892288 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9365708827972412, loss=4.153855800628662
I0131 15:18:30.292120 140023005427456 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8642768263816833, loss=4.245645523071289
I0131 15:19:16.418786 140022518892288 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8785513043403625, loss=4.305614471435547
I0131 15:20:02.456854 140023005427456 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7957756519317627, loss=4.6106648445129395
I0131 15:20:48.610372 140022518892288 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7193328738212585, loss=5.128032684326172
I0131 15:21:16.908622 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:21:27.565217 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:21:48.670012 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:21:50.278555 140184451094336 submission_runner.py:408] Time since start: 8721.49s, 	Step: 17463, 	{'train/accuracy': 0.553515613079071, 'train/loss': 2.2031924724578857, 'validation/accuracy': 0.5131999850273132, 'validation/loss': 2.3823468685150146, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.9856181144714355, 'test/num_examples': 10000, 'score': 8017.169707298279, 'total_duration': 8721.486993074417, 'accumulated_submission_time': 8017.169707298279, 'accumulated_eval_time': 702.9022083282471, 'accumulated_logging_time': 0.5227203369140625}
I0131 15:21:50.295443 140023005427456 logging_writer.py:48] [17463] accumulated_eval_time=702.902208, accumulated_logging_time=0.522720, accumulated_submission_time=8017.169707, global_step=17463, preemption_count=0, score=8017.169707, test/accuracy=0.398900, test/loss=2.985618, test/num_examples=10000, total_duration=8721.486993, train/accuracy=0.553516, train/loss=2.203192, validation/accuracy=0.513200, validation/loss=2.382347, validation/num_examples=50000
I0131 15:22:05.513614 140022518892288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7767126560211182, loss=5.833597660064697
I0131 15:22:48.755806 140023005427456 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.9325901865959167, loss=4.22292947769165
I0131 15:23:34.809095 140022518892288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9171856045722961, loss=4.0386247634887695
I0131 15:24:21.433506 140023005427456 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.8012897968292236, loss=4.279473781585693
I0131 15:25:07.513325 140022518892288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6869375705718994, loss=4.965242862701416
I0131 15:25:53.458373 140023005427456 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7677486538887024, loss=4.658698081970215
I0131 15:26:39.885665 140022518892288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7638382315635681, loss=5.157606601715088
I0131 15:27:25.515751 140023005427456 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8279144763946533, loss=4.589115619659424
I0131 15:28:11.605683 140022518892288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9182772040367126, loss=4.1941986083984375
I0131 15:28:50.803934 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:29:01.601540 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:29:22.770590 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:29:24.373590 140184451094336 submission_runner.py:408] Time since start: 9175.58s, 	Step: 18381, 	{'train/accuracy': 0.5694921612739563, 'train/loss': 2.0806596279144287, 'validation/accuracy': 0.5267999768257141, 'validation/loss': 2.2643842697143555, 'validation/num_examples': 50000, 'test/accuracy': 0.41690000891685486, 'test/loss': 2.871544122695923, 'test/num_examples': 10000, 'score': 8437.620900630951, 'total_duration': 9175.582031011581, 'accumulated_submission_time': 8437.620900630951, 'accumulated_eval_time': 736.4718625545502, 'accumulated_logging_time': 0.549354076385498}
I0131 15:29:24.391889 140023005427456 logging_writer.py:48] [18381] accumulated_eval_time=736.471863, accumulated_logging_time=0.549354, accumulated_submission_time=8437.620901, global_step=18381, preemption_count=0, score=8437.620901, test/accuracy=0.416900, test/loss=2.871544, test/num_examples=10000, total_duration=9175.582031, train/accuracy=0.569492, train/loss=2.080660, validation/accuracy=0.526800, validation/loss=2.264384, validation/num_examples=50000
I0131 15:29:32.386294 140022518892288 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6877148747444153, loss=5.922727584838867
I0131 15:30:14.787639 140023005427456 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8411238193511963, loss=4.314124584197998
I0131 15:31:01.478661 140022518892288 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9422290921211243, loss=4.314742565155029
I0131 15:31:48.018038 140023005427456 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8788615465164185, loss=4.114300727844238
I0131 15:32:35.433394 140022518892288 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9005705118179321, loss=4.1886887550354
I0131 15:33:21.449845 140023005427456 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9033544063568115, loss=4.208207607269287
I0131 15:34:07.509701 140022518892288 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9057035446166992, loss=3.990021228790283
I0131 15:34:53.562165 140023005427456 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8464705348014832, loss=4.235262393951416
I0131 15:35:39.912552 140022518892288 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7286491990089417, loss=5.614108085632324
I0131 15:36:24.505625 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:36:35.105897 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:36:58.547266 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:37:00.151706 140184451094336 submission_runner.py:408] Time since start: 9631.36s, 	Step: 19298, 	{'train/accuracy': 0.5946288704872131, 'train/loss': 1.9711012840270996, 'validation/accuracy': 0.5412999987602234, 'validation/loss': 2.2097156047821045, 'validation/num_examples': 50000, 'test/accuracy': 0.42190003395080566, 'test/loss': 2.8385367393493652, 'test/num_examples': 10000, 'score': 8857.677038431168, 'total_duration': 9631.36013674736, 'accumulated_submission_time': 8857.677038431168, 'accumulated_eval_time': 772.117954492569, 'accumulated_logging_time': 0.5785338878631592}
I0131 15:37:00.169720 140023005427456 logging_writer.py:48] [19298] accumulated_eval_time=772.117954, accumulated_logging_time=0.578534, accumulated_submission_time=8857.677038, global_step=19298, preemption_count=0, score=8857.677038, test/accuracy=0.421900, test/loss=2.838537, test/num_examples=10000, total_duration=9631.360137, train/accuracy=0.594629, train/loss=1.971101, validation/accuracy=0.541300, validation/loss=2.209716, validation/num_examples=50000
I0131 15:37:01.374729 140022518892288 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.9061631560325623, loss=4.062648296356201
I0131 15:37:42.713206 140023005427456 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9480623006820679, loss=3.880375385284424
I0131 15:38:28.864522 140022518892288 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9394123554229736, loss=4.095715045928955
I0131 15:39:15.414139 140023005427456 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8403751254081726, loss=5.450319766998291
I0131 15:40:01.640563 140022518892288 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8730220198631287, loss=4.011256694793701
I0131 15:40:47.987698 140023005427456 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.9060014486312866, loss=4.1123456954956055
I0131 15:41:34.603288 140022518892288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7887542247772217, loss=5.4336018562316895
I0131 15:42:20.705956 140023005427456 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7818668484687805, loss=5.771610260009766
I0131 15:43:07.133967 140022518892288 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.016305923461914, loss=4.160527229309082
I0131 15:43:53.398056 140023005427456 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8145723938941956, loss=4.944845199584961
I0131 15:44:00.331067 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:44:10.860530 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:44:33.899132 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:44:35.504902 140184451094336 submission_runner.py:408] Time since start: 10086.71s, 	Step: 20217, 	{'train/accuracy': 0.5886132717132568, 'train/loss': 1.9665268659591675, 'validation/accuracy': 0.5514000058174133, 'validation/loss': 2.1377973556518555, 'validation/num_examples': 50000, 'test/accuracy': 0.4321000277996063, 'test/loss': 2.757359027862549, 'test/num_examples': 10000, 'score': 9277.779623508453, 'total_duration': 10086.7133436203, 'accumulated_submission_time': 9277.779623508453, 'accumulated_eval_time': 807.2917928695679, 'accumulated_logging_time': 0.6079757213592529}
I0131 15:44:35.522850 140022518892288 logging_writer.py:48] [20217] accumulated_eval_time=807.291793, accumulated_logging_time=0.607976, accumulated_submission_time=9277.779624, global_step=20217, preemption_count=0, score=9277.779624, test/accuracy=0.432100, test/loss=2.757359, test/num_examples=10000, total_duration=10086.713344, train/accuracy=0.588613, train/loss=1.966527, validation/accuracy=0.551400, validation/loss=2.137797, validation/num_examples=50000
I0131 15:45:09.804893 140023005427456 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8020766377449036, loss=4.426674842834473
I0131 15:45:55.825405 140022518892288 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.9343887567520142, loss=4.066034317016602
I0131 15:46:42.146109 140023005427456 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8893857002258301, loss=4.464103698730469
I0131 15:47:28.419013 140022518892288 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7305508852005005, loss=5.287868499755859
I0131 15:48:14.269679 140023005427456 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9435149431228638, loss=3.9779186248779297
I0131 15:49:00.281780 140022518892288 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0625828504562378, loss=4.037656307220459
I0131 15:49:46.380164 140023005427456 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8439468741416931, loss=4.953967571258545
I0131 15:50:32.339909 140022518892288 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9649806618690491, loss=4.021223545074463
I0131 15:51:18.776696 140023005427456 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8774775862693787, loss=3.863089084625244
I0131 15:51:35.571120 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:51:46.274058 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:52:08.496533 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:52:10.103788 140184451094336 submission_runner.py:408] Time since start: 10541.31s, 	Step: 21138, 	{'train/accuracy': 0.6024804711341858, 'train/loss': 1.9354881048202515, 'validation/accuracy': 0.5561999678611755, 'validation/loss': 2.1444432735443115, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.7603559494018555, 'test/num_examples': 10000, 'score': 9697.771949529648, 'total_duration': 10541.312201499939, 'accumulated_submission_time': 9697.771949529648, 'accumulated_eval_time': 841.8244321346283, 'accumulated_logging_time': 0.6349647045135498}
I0131 15:52:10.125168 140022518892288 logging_writer.py:48] [21138] accumulated_eval_time=841.824432, accumulated_logging_time=0.634965, accumulated_submission_time=9697.771950, global_step=21138, preemption_count=0, score=9697.771950, test/accuracy=0.440900, test/loss=2.760356, test/num_examples=10000, total_duration=10541.312201, train/accuracy=0.602480, train/loss=1.935488, validation/accuracy=0.556200, validation/loss=2.144443, validation/num_examples=50000
I0131 15:52:35.329147 140023005427456 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.0235306024551392, loss=3.9418396949768066
I0131 15:53:20.216458 140022518892288 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8982383012771606, loss=3.9338181018829346
I0131 15:54:06.598311 140023005427456 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6955077648162842, loss=5.409296035766602
I0131 15:54:52.884208 140022518892288 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.9162254333496094, loss=3.983416795730591
I0131 15:55:38.903100 140023005427456 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6752480864524841, loss=5.314433574676514
I0131 15:56:25.035369 140022518892288 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.9060565233230591, loss=4.030285358428955
I0131 15:57:11.607652 140023005427456 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8955711722373962, loss=4.128429412841797
I0131 15:57:57.508598 140022518892288 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.8636882305145264, loss=4.0542426109313965
I0131 15:58:43.727154 140023005427456 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9126127362251282, loss=4.18337345123291
I0131 15:59:10.241709 140184451094336 spec.py:321] Evaluating on the training split.
I0131 15:59:20.977341 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 15:59:43.142961 140184451094336 spec.py:349] Evaluating on the test split.
I0131 15:59:44.728556 140184451094336 submission_runner.py:408] Time since start: 10995.94s, 	Step: 22059, 	{'train/accuracy': 0.6153905987739563, 'train/loss': 1.8635692596435547, 'validation/accuracy': 0.5629599690437317, 'validation/loss': 2.098595142364502, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.710822343826294, 'test/num_examples': 10000, 'score': 10117.83263373375, 'total_duration': 10995.936970949173, 'accumulated_submission_time': 10117.83263373375, 'accumulated_eval_time': 876.3112514019012, 'accumulated_logging_time': 0.6652498245239258}
I0131 15:59:44.747270 140022518892288 logging_writer.py:48] [22059] accumulated_eval_time=876.311251, accumulated_logging_time=0.665250, accumulated_submission_time=10117.832634, global_step=22059, preemption_count=0, score=10117.832634, test/accuracy=0.444800, test/loss=2.710822, test/num_examples=10000, total_duration=10995.936971, train/accuracy=0.615391, train/loss=1.863569, validation/accuracy=0.562960, validation/loss=2.098595, validation/num_examples=50000
I0131 16:00:01.559638 140023005427456 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9847958087921143, loss=4.009212970733643
I0131 16:00:45.182138 140022518892288 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9672073125839233, loss=4.031589508056641
I0131 16:01:31.435594 140023005427456 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8916627764701843, loss=4.251996994018555
I0131 16:02:17.814492 140022518892288 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.974029004573822, loss=3.9160635471343994
I0131 16:03:03.762080 140023005427456 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.923073410987854, loss=3.8389058113098145
I0131 16:03:50.502050 140022518892288 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7607190012931824, loss=4.818745136260986
I0131 16:04:36.798576 140023005427456 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9660524725914001, loss=3.942884922027588
I0131 16:05:23.152087 140022518892288 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7946001291275024, loss=5.771124839782715
I0131 16:06:09.566470 140023005427456 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7299761772155762, loss=5.380032539367676
I0131 16:06:44.873167 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:06:55.223325 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:07:15.605716 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:07:17.204798 140184451094336 submission_runner.py:408] Time since start: 11448.41s, 	Step: 22978, 	{'train/accuracy': 0.6119140386581421, 'train/loss': 1.860370397567749, 'validation/accuracy': 0.568619966506958, 'validation/loss': 2.0590929985046387, 'validation/num_examples': 50000, 'test/accuracy': 0.451200008392334, 'test/loss': 2.6695117950439453, 'test/num_examples': 10000, 'score': 10537.896867275238, 'total_duration': 11448.413235902786, 'accumulated_submission_time': 10537.896867275238, 'accumulated_eval_time': 908.6428663730621, 'accumulated_logging_time': 0.6947681903839111}
I0131 16:07:17.222787 140022518892288 logging_writer.py:48] [22978] accumulated_eval_time=908.642866, accumulated_logging_time=0.694768, accumulated_submission_time=10537.896867, global_step=22978, preemption_count=0, score=10537.896867, test/accuracy=0.451200, test/loss=2.669512, test/num_examples=10000, total_duration=11448.413236, train/accuracy=0.611914, train/loss=1.860370, validation/accuracy=0.568620, validation/loss=2.059093, validation/num_examples=50000
I0131 16:07:26.442653 140023005427456 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.795669436454773, loss=4.888535022735596
I0131 16:08:08.689868 140022518892288 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.8887842297554016, loss=4.2684221267700195
I0131 16:08:54.731437 140023005427456 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.820989727973938, loss=4.495938301086426
I0131 16:09:40.871550 140022518892288 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9219297766685486, loss=3.968256950378418
I0131 16:10:26.734920 140023005427456 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.728325605392456, loss=5.540421962738037
I0131 16:11:12.703826 140022518892288 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8965852856636047, loss=4.128970146179199
I0131 16:11:58.898277 140023005427456 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9866567850112915, loss=3.7866969108581543
I0131 16:12:44.940260 140022518892288 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9765079617500305, loss=3.989609956741333
I0131 16:13:31.452929 140023005427456 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.907454788684845, loss=4.11190128326416
I0131 16:14:17.532815 140022518892288 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8608661890029907, loss=4.550454616546631
I0131 16:14:17.551549 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:14:28.449786 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:14:50.618323 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:14:52.216225 140184451094336 submission_runner.py:408] Time since start: 11903.42s, 	Step: 23901, 	{'train/accuracy': 0.619140625, 'train/loss': 1.8426916599273682, 'validation/accuracy': 0.5765399932861328, 'validation/loss': 2.036699056625366, 'validation/num_examples': 50000, 'test/accuracy': 0.4563000202178955, 'test/loss': 2.6497387886047363, 'test/num_examples': 10000, 'score': 10958.168203353882, 'total_duration': 11903.424647808075, 'accumulated_submission_time': 10958.168203353882, 'accumulated_eval_time': 943.3075177669525, 'accumulated_logging_time': 0.7228202819824219}
I0131 16:14:52.234352 140023005427456 logging_writer.py:48] [23901] accumulated_eval_time=943.307518, accumulated_logging_time=0.722820, accumulated_submission_time=10958.168203, global_step=23901, preemption_count=0, score=10958.168203, test/accuracy=0.456300, test/loss=2.649739, test/num_examples=10000, total_duration=11903.424648, train/accuracy=0.619141, train/loss=1.842692, validation/accuracy=0.576540, validation/loss=2.036699, validation/num_examples=50000
I0131 16:15:33.518529 140022518892288 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9764652848243713, loss=4.050673484802246
I0131 16:16:19.354717 140023005427456 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9716836810112, loss=3.9329991340637207
I0131 16:17:05.233464 140022518892288 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8998027443885803, loss=3.8323700428009033
I0131 16:17:51.470836 140023005427456 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8539586067199707, loss=4.511059761047363
I0131 16:18:37.314041 140022518892288 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8292938470840454, loss=4.738932132720947
I0131 16:19:23.333865 140023005427456 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8323048949241638, loss=4.251836776733398
I0131 16:20:09.079657 140022518892288 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.8751200437545776, loss=4.332034111022949
I0131 16:20:55.070975 140023005427456 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.9325346350669861, loss=3.9682929515838623
I0131 16:21:41.397987 140022518892288 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9484760761260986, loss=3.7486908435821533
I0131 16:21:52.230339 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:22:03.042531 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:22:22.059860 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:22:23.684833 140184451094336 submission_runner.py:408] Time since start: 12354.89s, 	Step: 24825, 	{'train/accuracy': 0.6344335675239563, 'train/loss': 1.8031526803970337, 'validation/accuracy': 0.5821200013160706, 'validation/loss': 2.0396535396575928, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.65883469581604, 'test/num_examples': 10000, 'score': 11378.107141256332, 'total_duration': 12354.893264770508, 'accumulated_submission_time': 11378.107141256332, 'accumulated_eval_time': 974.7620024681091, 'accumulated_logging_time': 0.7505502700805664}
I0131 16:22:23.707008 140023005427456 logging_writer.py:48] [24825] accumulated_eval_time=974.762002, accumulated_logging_time=0.750550, accumulated_submission_time=11378.107141, global_step=24825, preemption_count=0, score=11378.107141, test/accuracy=0.455700, test/loss=2.658835, test/num_examples=10000, total_duration=12354.893265, train/accuracy=0.634434, train/loss=1.803153, validation/accuracy=0.582120, validation/loss=2.039654, validation/num_examples=50000
I0131 16:22:54.358786 140022518892288 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7528194189071655, loss=5.0274505615234375
I0131 16:23:40.452641 140023005427456 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.9423441290855408, loss=3.905179738998413
I0131 16:24:26.852268 140022518892288 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9036048054695129, loss=5.281939506530762
I0131 16:25:13.404449 140023005427456 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0802499055862427, loss=3.9696879386901855
I0131 16:25:59.297219 140022518892288 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8473126888275146, loss=4.286823749542236
I0131 16:26:45.516536 140023005427456 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0094070434570312, loss=3.8138742446899414
I0131 16:27:31.813428 140022518892288 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9702029228210449, loss=3.969379425048828
I0131 16:28:17.959321 140023005427456 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.9491029381752014, loss=3.9372386932373047
I0131 16:29:03.861154 140022518892288 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8933312892913818, loss=4.660826683044434
I0131 16:29:23.724505 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:29:34.092360 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:29:56.506591 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:29:58.105722 140184451094336 submission_runner.py:408] Time since start: 12809.31s, 	Step: 25745, 	{'train/accuracy': 0.6540429592132568, 'train/loss': 1.6929877996444702, 'validation/accuracy': 0.5794999599456787, 'validation/loss': 2.018251895904541, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.632565975189209, 'test/num_examples': 10000, 'score': 11798.067595005035, 'total_duration': 12809.31416130066, 'accumulated_submission_time': 11798.067595005035, 'accumulated_eval_time': 1009.1432175636292, 'accumulated_logging_time': 0.782789945602417}
I0131 16:29:58.125483 140023005427456 logging_writer.py:48] [25745] accumulated_eval_time=1009.143218, accumulated_logging_time=0.782790, accumulated_submission_time=11798.067595, global_step=25745, preemption_count=0, score=11798.067595, test/accuracy=0.461700, test/loss=2.632566, test/num_examples=10000, total_duration=12809.314161, train/accuracy=0.654043, train/loss=1.692988, validation/accuracy=0.579500, validation/loss=2.018252, validation/num_examples=50000
I0131 16:30:20.541061 140022518892288 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8515993356704712, loss=5.798151969909668
I0131 16:31:04.995861 140023005427456 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.9933636784553528, loss=3.9363319873809814
I0131 16:31:51.261063 140022518892288 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8908841013908386, loss=4.235718250274658
I0131 16:32:37.716956 140023005427456 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8528444170951843, loss=4.1431779861450195
I0131 16:33:23.563436 140022518892288 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9989924430847168, loss=3.922962188720703
I0131 16:34:09.947866 140023005427456 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8920692205429077, loss=3.8639116287231445
I0131 16:34:55.848334 140022518892288 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8575305342674255, loss=5.32870626449585
I0131 16:35:41.651534 140023005427456 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7664957642555237, loss=5.637409210205078
I0131 16:36:27.641808 140022518892288 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9975515007972717, loss=3.914170742034912
I0131 16:36:58.233946 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:37:08.481466 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:37:27.526217 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:37:29.134543 140184451094336 submission_runner.py:408] Time since start: 13260.34s, 	Step: 26668, 	{'train/accuracy': 0.6333398222923279, 'train/loss': 1.7327735424041748, 'validation/accuracy': 0.5909799933433533, 'validation/loss': 1.9266051054000854, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.5658607482910156, 'test/num_examples': 10000, 'score': 12218.119409561157, 'total_duration': 13260.34297466278, 'accumulated_submission_time': 12218.119409561157, 'accumulated_eval_time': 1040.0437922477722, 'accumulated_logging_time': 0.8123970031738281}
I0131 16:37:29.156973 140023005427456 logging_writer.py:48] [26668] accumulated_eval_time=1040.043792, accumulated_logging_time=0.812397, accumulated_submission_time=12218.119410, global_step=26668, preemption_count=0, score=12218.119410, test/accuracy=0.468600, test/loss=2.565861, test/num_examples=10000, total_duration=13260.342975, train/accuracy=0.633340, train/loss=1.732774, validation/accuracy=0.590980, validation/loss=1.926605, validation/num_examples=50000
I0131 16:37:42.412213 140022518892288 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9785491228103638, loss=3.8011345863342285
I0131 16:38:25.832584 140023005427456 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7667957544326782, loss=5.694994926452637
I0131 16:39:11.737756 140022518892288 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.9653517007827759, loss=3.8388400077819824
I0131 16:39:57.838163 140023005427456 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.0963021516799927, loss=3.8440842628479004
I0131 16:40:43.893251 140022518892288 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9483147859573364, loss=3.770864248275757
I0131 16:41:30.010910 140023005427456 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9271001219749451, loss=4.212255001068115
I0131 16:42:16.388452 140022518892288 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.7832346558570862, loss=4.581153869628906
I0131 16:43:02.648646 140023005427456 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9900492429733276, loss=3.775038957595825
I0131 16:43:48.708086 140022518892288 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0124508142471313, loss=3.8790457248687744
I0131 16:44:29.352005 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:44:39.688184 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:45:01.959709 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:45:03.559000 140184451094336 submission_runner.py:408] Time since start: 13714.77s, 	Step: 27589, 	{'train/accuracy': 0.6485351324081421, 'train/loss': 1.7430307865142822, 'validation/accuracy': 0.5958600044250488, 'validation/loss': 1.972648024559021, 'validation/num_examples': 50000, 'test/accuracy': 0.47680002450942993, 'test/loss': 2.580284595489502, 'test/num_examples': 10000, 'score': 12638.257354021072, 'total_duration': 13714.76744222641, 'accumulated_submission_time': 12638.257354021072, 'accumulated_eval_time': 1074.2507855892181, 'accumulated_logging_time': 0.8448200225830078}
I0131 16:45:03.578161 140023005427456 logging_writer.py:48] [27589] accumulated_eval_time=1074.250786, accumulated_logging_time=0.844820, accumulated_submission_time=12638.257354, global_step=27589, preemption_count=0, score=12638.257354, test/accuracy=0.476800, test/loss=2.580285, test/num_examples=10000, total_duration=13714.767442, train/accuracy=0.648535, train/loss=1.743031, validation/accuracy=0.595860, validation/loss=1.972648, validation/num_examples=50000
I0131 16:45:08.425542 140022518892288 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7436943650245667, loss=5.336981773376465
I0131 16:45:49.868862 140023005427456 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9692301750183105, loss=3.762554407119751
I0131 16:46:35.916534 140022518892288 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.9478611946105957, loss=3.859595775604248
I0131 16:47:22.109172 140023005427456 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9313099980354309, loss=3.862185478210449
I0131 16:48:08.322384 140022518892288 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.961348831653595, loss=4.049768447875977
I0131 16:48:53.930748 140023005427456 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.9225636124610901, loss=3.832874298095703
I0131 16:49:39.995417 140022518892288 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.8914054036140442, loss=3.991331100463867
I0131 16:50:26.070171 140023005427456 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.9365382194519043, loss=3.9657459259033203
I0131 16:51:12.157006 140022518892288 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.7921332120895386, loss=5.095827102661133
I0131 16:51:58.426254 140023005427456 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.014108419418335, loss=3.800842761993408
I0131 16:52:04.076279 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:52:14.419841 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 16:52:33.136481 140184451094336 spec.py:349] Evaluating on the test split.
I0131 16:52:34.744907 140184451094336 submission_runner.py:408] Time since start: 14165.95s, 	Step: 28513, 	{'train/accuracy': 0.6622265577316284, 'train/loss': 1.6519564390182495, 'validation/accuracy': 0.5966399908065796, 'validation/loss': 1.929701805114746, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.5671634674072266, 'test/num_examples': 10000, 'score': 13058.69831609726, 'total_duration': 14165.95332980156, 'accumulated_submission_time': 13058.69831609726, 'accumulated_eval_time': 1104.9193880558014, 'accumulated_logging_time': 0.8732032775878906}
I0131 16:52:34.768905 140022518892288 logging_writer.py:48] [28513] accumulated_eval_time=1104.919388, accumulated_logging_time=0.873203, accumulated_submission_time=13058.698316, global_step=28513, preemption_count=0, score=13058.698316, test/accuracy=0.470200, test/loss=2.567163, test/num_examples=10000, total_duration=14165.953330, train/accuracy=0.662227, train/loss=1.651956, validation/accuracy=0.596640, validation/loss=1.929702, validation/num_examples=50000
I0131 16:53:10.826804 140023005427456 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.8526155352592468, loss=4.497583866119385
I0131 16:53:56.733765 140022518892288 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.075161337852478, loss=3.7648253440856934
I0131 16:54:42.993661 140023005427456 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8138326406478882, loss=4.983161926269531
I0131 16:55:29.023772 140022518892288 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0180076360702515, loss=3.8148627281188965
I0131 16:56:15.193581 140023005427456 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8506762385368347, loss=5.0235595703125
I0131 16:57:01.267627 140022518892288 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9986339807510376, loss=3.885080575942993
I0131 16:57:47.081747 140023005427456 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.812731921672821, loss=5.571447372436523
I0131 16:58:33.199112 140022518892288 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0132699012756348, loss=3.7853505611419678
I0131 16:59:19.214841 140023005427456 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.822455644607544, loss=5.700489044189453
I0131 16:59:35.074493 140184451094336 spec.py:321] Evaluating on the training split.
I0131 16:59:45.753768 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:00:07.297944 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:00:08.899164 140184451094336 submission_runner.py:408] Time since start: 14620.11s, 	Step: 29436, 	{'train/accuracy': 0.6512890458106995, 'train/loss': 1.7113102674484253, 'validation/accuracy': 0.6077600121498108, 'validation/loss': 1.9051331281661987, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.534058094024658, 'test/num_examples': 10000, 'score': 13478.946665525436, 'total_duration': 14620.107602596283, 'accumulated_submission_time': 13478.946665525436, 'accumulated_eval_time': 1138.7440557479858, 'accumulated_logging_time': 0.9072282314300537}
I0131 17:00:08.921327 140022518892288 logging_writer.py:48] [29436] accumulated_eval_time=1138.744056, accumulated_logging_time=0.907228, accumulated_submission_time=13478.946666, global_step=29436, preemption_count=0, score=13478.946666, test/accuracy=0.480400, test/loss=2.534058, test/num_examples=10000, total_duration=14620.107603, train/accuracy=0.651289, train/loss=1.711310, validation/accuracy=0.607760, validation/loss=1.905133, validation/num_examples=50000
I0131 17:00:34.943046 140023005427456 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9511550664901733, loss=3.7902095317840576
I0131 17:01:19.581934 140022518892288 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.12918221950531, loss=3.8156113624572754
I0131 17:02:05.892753 140023005427456 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0100733041763306, loss=3.745331048965454
I0131 17:02:52.272994 140022518892288 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.9940920472145081, loss=3.7837350368499756
I0131 17:03:38.284767 140023005427456 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.8235611319541931, loss=5.407349109649658
I0131 17:04:24.428349 140022518892288 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9320264458656311, loss=3.78617000579834
I0131 17:05:10.832219 140023005427456 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0388531684875488, loss=3.721024513244629
I0131 17:05:56.889670 140022518892288 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.9298416376113892, loss=3.691540241241455
I0131 17:06:43.011840 140023005427456 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0862246751785278, loss=3.810344934463501
I0131 17:07:08.944623 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:07:19.590481 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:07:42.284421 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:07:43.889491 140184451094336 submission_runner.py:408] Time since start: 15075.10s, 	Step: 30358, 	{'train/accuracy': 0.6586328148841858, 'train/loss': 1.6941852569580078, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.9113799333572388, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.532050609588623, 'test/num_examples': 10000, 'score': 13898.911831855774, 'total_duration': 15075.097905397415, 'accumulated_submission_time': 13898.911831855774, 'accumulated_eval_time': 1173.6889071464539, 'accumulated_logging_time': 0.9404423236846924}
I0131 17:07:43.915407 140022518892288 logging_writer.py:48] [30358] accumulated_eval_time=1173.688907, accumulated_logging_time=0.940442, accumulated_submission_time=13898.911832, global_step=30358, preemption_count=0, score=13898.911832, test/accuracy=0.490500, test/loss=2.532051, test/num_examples=10000, total_duration=15075.097905, train/accuracy=0.658633, train/loss=1.694185, validation/accuracy=0.607800, validation/loss=1.911380, validation/num_examples=50000
I0131 17:08:01.106265 140023005427456 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.989684522151947, loss=3.714047908782959
I0131 17:08:44.591936 140022518892288 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8665403723716736, loss=4.29987096786499
I0131 17:09:30.637290 140023005427456 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0178163051605225, loss=3.691092014312744
I0131 17:10:16.747131 140022518892288 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.005179524421692, loss=3.8952455520629883
I0131 17:11:02.597637 140023005427456 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.055389404296875, loss=3.77640438079834
I0131 17:11:48.752532 140022518892288 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.9736281037330627, loss=3.8332674503326416
I0131 17:12:34.756164 140023005427456 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8077826499938965, loss=5.340073108673096
I0131 17:13:20.734659 140022518892288 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.8608852624893188, loss=5.396392345428467
I0131 17:14:06.766071 140023005427456 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.850075364112854, loss=5.0735979080200195
I0131 17:14:44.243948 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:14:54.628814 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:15:15.006795 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:15:16.645464 140184451094336 submission_runner.py:408] Time since start: 15527.85s, 	Step: 31283, 	{'train/accuracy': 0.6657617092132568, 'train/loss': 1.6142001152038574, 'validation/accuracy': 0.6137599945068359, 'validation/loss': 1.8565632104873657, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.487379312515259, 'test/num_examples': 10000, 'score': 14319.18147277832, 'total_duration': 15527.85389828682, 'accumulated_submission_time': 14319.18147277832, 'accumulated_eval_time': 1206.0904257297516, 'accumulated_logging_time': 0.9788103103637695}
I0131 17:15:16.665135 140022518892288 logging_writer.py:48] [31283] accumulated_eval_time=1206.090426, accumulated_logging_time=0.978810, accumulated_submission_time=14319.181473, global_step=31283, preemption_count=0, score=14319.181473, test/accuracy=0.487400, test/loss=2.487379, test/num_examples=10000, total_duration=15527.853898, train/accuracy=0.665762, train/loss=1.614200, validation/accuracy=0.613760, validation/loss=1.856563, validation/num_examples=50000
I0131 17:15:23.877512 140023005427456 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0930360555648804, loss=3.753786325454712
I0131 17:16:06.021246 140022518892288 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.8570768237113953, loss=4.635313987731934
I0131 17:16:51.806154 140023005427456 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.0962592363357544, loss=3.700526475906372
I0131 17:17:38.060607 140022518892288 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.9359064102172852, loss=3.718045711517334
I0131 17:18:24.333495 140023005427456 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.807105302810669, loss=5.562866687774658
I0131 17:19:10.434544 140022518892288 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0222193002700806, loss=3.7248446941375732
I0131 17:19:56.392230 140023005427456 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.852509081363678, loss=5.4162678718566895
I0131 17:20:42.488060 140022518892288 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.8339154124259949, loss=4.304205894470215
I0131 17:21:28.594734 140023005427456 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0359352827072144, loss=3.782560110092163
I0131 17:22:14.939886 140022518892288 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.9700316786766052, loss=3.8392341136932373
I0131 17:22:16.892865 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:22:27.434849 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:22:49.447154 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:22:51.053812 140184451094336 submission_runner.py:408] Time since start: 15982.26s, 	Step: 32206, 	{'train/accuracy': 0.6634374856948853, 'train/loss': 1.6532877683639526, 'validation/accuracy': 0.6112799644470215, 'validation/loss': 1.8662524223327637, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.5017411708831787, 'test/num_examples': 10000, 'score': 14739.35320687294, 'total_duration': 15982.26224064827, 'accumulated_submission_time': 14739.35320687294, 'accumulated_eval_time': 1240.2513403892517, 'accumulated_logging_time': 1.008094310760498}
I0131 17:22:51.077254 140023005427456 logging_writer.py:48] [32206] accumulated_eval_time=1240.251340, accumulated_logging_time=1.008094, accumulated_submission_time=14739.353207, global_step=32206, preemption_count=0, score=14739.353207, test/accuracy=0.490700, test/loss=2.501741, test/num_examples=10000, total_duration=15982.262241, train/accuracy=0.663437, train/loss=1.653288, validation/accuracy=0.611280, validation/loss=1.866252, validation/num_examples=50000
I0131 17:23:30.222777 140022518892288 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7955597043037415, loss=4.746283054351807
I0131 17:24:16.184601 140023005427456 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.9432596564292908, loss=3.934007167816162
I0131 17:25:02.426920 140022518892288 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.8778190612792969, loss=4.274838447570801
I0131 17:25:48.356977 140023005427456 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0473194122314453, loss=3.8133351802825928
I0131 17:26:34.176054 140022518892288 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9051049947738647, loss=4.266263484954834
I0131 17:27:20.154791 140023005427456 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.8894656300544739, loss=4.744584083557129
I0131 17:28:05.891256 140022518892288 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.8584527969360352, loss=4.665578842163086
I0131 17:28:51.870555 140023005427456 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.9552460312843323, loss=3.813293218612671
I0131 17:29:38.262135 140022518892288 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.9164208769798279, loss=4.2332940101623535
I0131 17:29:51.294322 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:30:01.883630 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:30:23.259716 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:30:24.857104 140184451094336 submission_runner.py:408] Time since start: 16436.07s, 	Step: 33130, 	{'train/accuracy': 0.6699804663658142, 'train/loss': 1.6174613237380981, 'validation/accuracy': 0.6247999668121338, 'validation/loss': 1.8267481327056885, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.448014974594116, 'test/num_examples': 10000, 'score': 15159.512373924255, 'total_duration': 16436.065540075302, 'accumulated_submission_time': 15159.512373924255, 'accumulated_eval_time': 1273.814103603363, 'accumulated_logging_time': 1.0417673587799072}
I0131 17:30:24.879298 140023005427456 logging_writer.py:48] [33130] accumulated_eval_time=1273.814104, accumulated_logging_time=1.041767, accumulated_submission_time=15159.512374, global_step=33130, preemption_count=0, score=15159.512374, test/accuracy=0.500500, test/loss=2.448015, test/num_examples=10000, total_duration=16436.065540, train/accuracy=0.669980, train/loss=1.617461, validation/accuracy=0.624800, validation/loss=1.826748, validation/num_examples=50000
I0131 17:30:53.285492 140022518892288 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0616542100906372, loss=3.7538223266601562
I0131 17:31:38.698350 140023005427456 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.8684394359588623, loss=5.378941535949707
I0131 17:32:24.885251 140022518892288 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.9185275435447693, loss=4.263382911682129
I0131 17:33:11.300201 140023005427456 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.0752637386322021, loss=3.693859577178955
I0131 17:33:57.105583 140022518892288 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.9505062699317932, loss=3.739333391189575
I0131 17:34:43.193683 140023005427456 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.9851764440536499, loss=3.6550540924072266
I0131 17:35:29.438124 140022518892288 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.071053385734558, loss=3.792612075805664
I0131 17:36:15.539806 140023005427456 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9543318748474121, loss=4.103848457336426
I0131 17:37:01.526357 140022518892288 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0035290718078613, loss=3.621774673461914
I0131 17:37:25.268985 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:37:36.037277 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:37:57.840732 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:37:59.453145 140184451094336 submission_runner.py:408] Time since start: 16890.66s, 	Step: 34053, 	{'train/accuracy': 0.6822265386581421, 'train/loss': 1.577724575996399, 'validation/accuracy': 0.6250799894332886, 'validation/loss': 1.833191156387329, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.4420933723449707, 'test/num_examples': 10000, 'score': 15579.846864700317, 'total_duration': 16890.66156053543, 'accumulated_submission_time': 15579.846864700317, 'accumulated_eval_time': 1307.9982221126556, 'accumulated_logging_time': 1.0729899406433105}
I0131 17:37:59.480274 140023005427456 logging_writer.py:48] [34053] accumulated_eval_time=1307.998222, accumulated_logging_time=1.072990, accumulated_submission_time=15579.846865, global_step=34053, preemption_count=0, score=15579.846865, test/accuracy=0.497800, test/loss=2.442093, test/num_examples=10000, total_duration=16890.661561, train/accuracy=0.682227, train/loss=1.577725, validation/accuracy=0.625080, validation/loss=1.833191, validation/num_examples=50000
I0131 17:38:18.697695 140022518892288 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.8545714616775513, loss=4.3398356437683105
I0131 17:39:02.861878 140023005427456 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8683615326881409, loss=5.299602508544922
I0131 17:39:49.033365 140022518892288 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.9287989139556885, loss=4.0008039474487305
I0131 17:40:35.298126 140023005427456 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.829571545124054, loss=4.184893608093262
I0131 17:41:21.123581 140022518892288 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0041600465774536, loss=3.7094593048095703
I0131 17:42:07.342592 140023005427456 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.8473501205444336, loss=4.601019859313965
I0131 17:42:53.512647 140022518892288 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.9493951201438904, loss=3.6972272396087646
I0131 17:43:39.638695 140023005427456 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8231222033500671, loss=5.062965393066406
I0131 17:44:25.879310 140022518892288 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.0028568506240845, loss=3.7320752143859863
I0131 17:44:59.526662 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:45:09.997379 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:45:34.677873 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:45:36.294066 140184451094336 submission_runner.py:408] Time since start: 17347.50s, 	Step: 34975, 	{'train/accuracy': 0.7008007764816284, 'train/loss': 1.4769867658615112, 'validation/accuracy': 0.6222400069236755, 'validation/loss': 1.811497449874878, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.4435598850250244, 'test/num_examples': 10000, 'score': 15999.829869508743, 'total_duration': 17347.502505779266, 'accumulated_submission_time': 15999.829869508743, 'accumulated_eval_time': 1344.7656226158142, 'accumulated_logging_time': 1.1167857646942139}
I0131 17:45:36.315298 140023005427456 logging_writer.py:48] [34975] accumulated_eval_time=1344.765623, accumulated_logging_time=1.116786, accumulated_submission_time=15999.829870, global_step=34975, preemption_count=0, score=15999.829870, test/accuracy=0.497000, test/loss=2.443560, test/num_examples=10000, total_duration=17347.502506, train/accuracy=0.700801, train/loss=1.476987, validation/accuracy=0.622240, validation/loss=1.811497, validation/num_examples=50000
I0131 17:45:46.719322 140022518892288 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.9399505257606506, loss=4.037399768829346
I0131 17:46:29.433114 140023005427456 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9986236691474915, loss=3.7426724433898926
I0131 17:47:15.535600 140022518892288 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0772769451141357, loss=3.8681211471557617
I0131 17:48:01.942732 140023005427456 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0131813287734985, loss=3.688276767730713
I0131 17:48:48.088288 140022518892288 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.9188622236251831, loss=5.159545421600342
I0131 17:49:34.249994 140023005427456 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.055638074874878, loss=3.769683361053467
I0131 17:50:20.189594 140022518892288 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.8838115930557251, loss=4.469931602478027
I0131 17:51:06.112399 140023005427456 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0149729251861572, loss=3.667107582092285
I0131 17:51:52.183320 140022518892288 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.0377193689346313, loss=3.6865556240081787
I0131 17:52:36.614220 140184451094336 spec.py:321] Evaluating on the training split.
I0131 17:52:48.033794 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 17:53:04.656300 140184451094336 spec.py:349] Evaluating on the test split.
I0131 17:53:06.267539 140184451094336 submission_runner.py:408] Time since start: 17797.48s, 	Step: 35898, 	{'train/accuracy': 0.6775780916213989, 'train/loss': 1.5361953973770142, 'validation/accuracy': 0.6276800036430359, 'validation/loss': 1.7578741312026978, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.371567964553833, 'test/num_examples': 10000, 'score': 16420.072093486786, 'total_duration': 17797.47596859932, 'accumulated_submission_time': 16420.072093486786, 'accumulated_eval_time': 1374.4189398288727, 'accumulated_logging_time': 1.147679090499878}
I0131 17:53:06.294803 140023005427456 logging_writer.py:48] [35898] accumulated_eval_time=1374.418940, accumulated_logging_time=1.147679, accumulated_submission_time=16420.072093, global_step=35898, preemption_count=0, score=16420.072093, test/accuracy=0.506200, test/loss=2.371568, test/num_examples=10000, total_duration=17797.475969, train/accuracy=0.677578, train/loss=1.536195, validation/accuracy=0.627680, validation/loss=1.757874, validation/num_examples=50000
I0131 17:53:07.505257 140022518892288 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.97164386510849, loss=3.6519622802734375
I0131 17:53:49.674144 140023005427456 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9727554321289062, loss=3.6138083934783936
I0131 17:54:35.977195 140022518892288 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.0323437452316284, loss=3.8750698566436768
I0131 17:55:22.184832 140023005427456 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.8711677193641663, loss=5.219004154205322
I0131 17:56:08.571171 140022518892288 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.0436406135559082, loss=3.745711326599121
I0131 17:56:54.510091 140023005427456 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.9362679123878479, loss=5.224666595458984
I0131 17:57:40.642599 140022518892288 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1274579763412476, loss=3.769144296646118
I0131 17:58:26.799960 140023005427456 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.9525649547576904, loss=3.6341967582702637
I0131 17:59:12.966766 140022518892288 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.9886392951011658, loss=3.623927354812622
I0131 17:59:58.897999 140023005427456 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.016722321510315, loss=3.5782690048217773
I0131 18:00:06.472294 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:00:16.802747 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:00:40.073758 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:00:41.695781 140184451094336 submission_runner.py:408] Time since start: 18252.90s, 	Step: 36818, 	{'train/accuracy': 0.6763281226158142, 'train/loss': 1.6137640476226807, 'validation/accuracy': 0.6283800005912781, 'validation/loss': 1.842094898223877, 'validation/num_examples': 50000, 'test/accuracy': 0.5077000260353088, 'test/loss': 2.4434211254119873, 'test/num_examples': 10000, 'score': 16840.190497398376, 'total_duration': 18252.90421462059, 'accumulated_submission_time': 16840.190497398376, 'accumulated_eval_time': 1409.6424214839935, 'accumulated_logging_time': 1.1869757175445557}
I0131 18:00:41.719707 140022518892288 logging_writer.py:48] [36818] accumulated_eval_time=1409.642421, accumulated_logging_time=1.186976, accumulated_submission_time=16840.190497, global_step=36818, preemption_count=0, score=16840.190497, test/accuracy=0.507700, test/loss=2.443421, test/num_examples=10000, total_duration=18252.904215, train/accuracy=0.676328, train/loss=1.613764, validation/accuracy=0.628380, validation/loss=1.842095, validation/num_examples=50000
I0131 18:01:14.901859 140023005427456 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.9862275123596191, loss=3.9678409099578857
I0131 18:02:00.840059 140022518892288 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0434364080429077, loss=3.7093544006347656
I0131 18:02:47.078774 140023005427456 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9077364206314087, loss=3.8356773853302
I0131 18:03:33.586706 140022518892288 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0389268398284912, loss=3.7785191535949707
I0131 18:04:19.848235 140023005427456 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.9609768986701965, loss=3.6891350746154785
I0131 18:05:05.772600 140022518892288 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0735814571380615, loss=3.702624797821045
I0131 18:05:51.824770 140023005427456 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.926366925239563, loss=3.989694595336914
I0131 18:06:38.160625 140022518892288 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.8797740936279297, loss=5.541901588439941
I0131 18:07:24.259600 140023005427456 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.9629911780357361, loss=4.00611686706543
I0131 18:07:41.867004 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:07:52.016199 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:08:12.817261 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:08:14.418347 140184451094336 submission_runner.py:408] Time since start: 18705.63s, 	Step: 37740, 	{'train/accuracy': 0.6975976228713989, 'train/loss': 1.4501029253005981, 'validation/accuracy': 0.631060004234314, 'validation/loss': 1.7535679340362549, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.3802409172058105, 'test/num_examples': 10000, 'score': 17260.28178691864, 'total_duration': 18705.626772880554, 'accumulated_submission_time': 17260.28178691864, 'accumulated_eval_time': 1442.1937334537506, 'accumulated_logging_time': 1.2198548316955566}
I0131 18:08:14.441555 140022518892288 logging_writer.py:48] [37740] accumulated_eval_time=1442.193733, accumulated_logging_time=1.219855, accumulated_submission_time=17260.281787, global_step=37740, preemption_count=0, score=17260.281787, test/accuracy=0.508300, test/loss=2.380241, test/num_examples=10000, total_duration=18705.626773, train/accuracy=0.697598, train/loss=1.450103, validation/accuracy=0.631060, validation/loss=1.753568, validation/num_examples=50000
I0131 18:08:39.126140 140023005427456 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.0007144212722778, loss=3.6728222370147705
I0131 18:09:23.226592 140022518892288 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.0474317073822021, loss=3.7030043601989746
I0131 18:10:09.440590 140023005427456 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0232194662094116, loss=3.932830810546875
I0131 18:10:55.697400 140022518892288 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.9506374001502991, loss=3.6084747314453125
I0131 18:11:41.769228 140023005427456 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.9556934237480164, loss=4.185274600982666
I0131 18:12:28.105425 140022518892288 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.0531045198440552, loss=3.657402276992798
I0131 18:13:14.112565 140023005427456 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.950541079044342, loss=4.146686553955078
I0131 18:14:00.092739 140022518892288 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8347968459129333, loss=4.611985206604004
I0131 18:14:46.377498 140023005427456 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.9504583477973938, loss=3.6467604637145996
I0131 18:15:14.652056 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:15:24.938032 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:15:46.763244 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:15:48.367621 140184451094336 submission_runner.py:408] Time since start: 19159.58s, 	Step: 38663, 	{'train/accuracy': 0.6893359422683716, 'train/loss': 1.514500379562378, 'validation/accuracy': 0.6391400098800659, 'validation/loss': 1.7250676155090332, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.346991777420044, 'test/num_examples': 10000, 'score': 17680.43620157242, 'total_duration': 19159.576062202454, 'accumulated_submission_time': 17680.43620157242, 'accumulated_eval_time': 1475.9092907905579, 'accumulated_logging_time': 1.252131700515747}
I0131 18:15:48.388815 140022518892288 logging_writer.py:48] [38663] accumulated_eval_time=1475.909291, accumulated_logging_time=1.252132, accumulated_submission_time=17680.436202, global_step=38663, preemption_count=0, score=17680.436202, test/accuracy=0.520200, test/loss=2.346992, test/num_examples=10000, total_duration=19159.576062, train/accuracy=0.689336, train/loss=1.514500, validation/accuracy=0.639140, validation/loss=1.725068, validation/num_examples=50000
I0131 18:16:03.596260 140023005427456 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.8705800771713257, loss=4.360020637512207
I0131 18:16:47.249618 140022518892288 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.0117748975753784, loss=3.5636534690856934
I0131 18:17:33.565612 140023005427456 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.0456695556640625, loss=3.607938766479492
I0131 18:18:20.108089 140022518892288 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.8990947008132935, loss=4.424860954284668
I0131 18:19:06.167548 140023005427456 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.0685384273529053, loss=3.610288619995117
I0131 18:19:52.207527 140022518892288 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.975560188293457, loss=3.6277451515197754
I0131 18:20:38.531846 140023005427456 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.0649904012680054, loss=3.5594687461853027
I0131 18:21:24.572221 140022518892288 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.8600419759750366, loss=4.688246726989746
I0131 18:22:10.703910 140023005427456 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.034345269203186, loss=5.30768346786499
I0131 18:22:48.498542 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:22:59.009479 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:23:20.318772 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:23:21.917777 140184451094336 submission_runner.py:408] Time since start: 19613.13s, 	Step: 39584, 	{'train/accuracy': 0.6898046731948853, 'train/loss': 1.4997754096984863, 'validation/accuracy': 0.6385999917984009, 'validation/loss': 1.728279948234558, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.3584721088409424, 'test/num_examples': 10000, 'score': 18100.48945403099, 'total_duration': 19613.12621331215, 'accumulated_submission_time': 18100.48945403099, 'accumulated_eval_time': 1509.32852268219, 'accumulated_logging_time': 1.2823808193206787}
I0131 18:23:21.939600 140022518892288 logging_writer.py:48] [39584] accumulated_eval_time=1509.328523, accumulated_logging_time=1.282381, accumulated_submission_time=18100.489454, global_step=39584, preemption_count=0, score=18100.489454, test/accuracy=0.512200, test/loss=2.358472, test/num_examples=10000, total_duration=19613.126213, train/accuracy=0.689805, train/loss=1.499775, validation/accuracy=0.638600, validation/loss=1.728280, validation/num_examples=50000
I0131 18:23:28.750994 140023005427456 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.9338738918304443, loss=3.7776403427124023
I0131 18:24:10.764557 140022518892288 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.9689617156982422, loss=3.8602418899536133
I0131 18:24:57.000561 140023005427456 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.9731023907661438, loss=3.599470615386963
I0131 18:25:43.065308 140022518892288 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.890077531337738, loss=5.248095512390137
I0131 18:26:29.054803 140023005427456 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.1284596920013428, loss=4.149734020233154
I0131 18:27:15.248355 140022518892288 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0178909301757812, loss=3.6518967151641846
I0131 18:28:01.132494 140023005427456 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0314521789550781, loss=4.022075653076172
I0131 18:28:47.291214 140022518892288 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.068289041519165, loss=3.6486072540283203
I0131 18:29:33.495503 140023005427456 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0182652473449707, loss=3.784419536590576
I0131 18:30:19.523157 140022518892288 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.9252857565879822, loss=5.215222358703613
I0131 18:30:21.985766 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:30:32.608137 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:30:51.726375 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:30:53.333241 140184451094336 submission_runner.py:408] Time since start: 20064.54s, 	Step: 40507, 	{'train/accuracy': 0.70068359375, 'train/loss': 1.4647341966629028, 'validation/accuracy': 0.6398599743843079, 'validation/loss': 1.7389329671859741, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.3418326377868652, 'test/num_examples': 10000, 'score': 18520.47150492668, 'total_duration': 20064.541659355164, 'accumulated_submission_time': 18520.47150492668, 'accumulated_eval_time': 1540.6759810447693, 'accumulated_logging_time': 1.316420316696167}
I0131 18:30:53.356832 140023005427456 logging_writer.py:48] [40507] accumulated_eval_time=1540.675981, accumulated_logging_time=1.316420, accumulated_submission_time=18520.471505, global_step=40507, preemption_count=0, score=18520.471505, test/accuracy=0.521700, test/loss=2.341833, test/num_examples=10000, total_duration=20064.541659, train/accuracy=0.700684, train/loss=1.464734, validation/accuracy=0.639860, validation/loss=1.738933, validation/num_examples=50000
I0131 18:31:31.958145 140022518892288 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.9897380471229553, loss=3.7175137996673584
I0131 18:32:18.314431 140023005427456 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.9897365570068359, loss=3.7055721282958984
I0131 18:33:04.534100 140022518892288 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.1896367073059082, loss=3.7598934173583984
I0131 18:33:50.865205 140023005427456 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9054350852966309, loss=4.701445579528809
I0131 18:34:36.977812 140022518892288 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0048216581344604, loss=3.625777244567871
I0131 18:35:23.455497 140023005427456 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.0320526361465454, loss=3.5699596405029297
I0131 18:36:09.583825 140022518892288 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.94999098777771, loss=4.453876495361328
I0131 18:36:55.860110 140023005427456 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.1413030624389648, loss=3.654730796813965
I0131 18:37:42.092433 140022518892288 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.097707748413086, loss=3.6483218669891357
I0131 18:37:53.747416 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:38:04.194640 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:38:27.552780 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:38:29.154285 140184451094336 submission_runner.py:408] Time since start: 20520.36s, 	Step: 41427, 	{'train/accuracy': 0.6917382478713989, 'train/loss': 1.4947209358215332, 'validation/accuracy': 0.6426999568939209, 'validation/loss': 1.7017552852630615, 'validation/num_examples': 50000, 'test/accuracy': 0.5194000005722046, 'test/loss': 2.3289785385131836, 'test/num_examples': 10000, 'score': 18940.805812358856, 'total_duration': 20520.362723588943, 'accumulated_submission_time': 18940.805812358856, 'accumulated_eval_time': 1576.082843542099, 'accumulated_logging_time': 1.3496661186218262}
I0131 18:38:29.180899 140023005427456 logging_writer.py:48] [41427] accumulated_eval_time=1576.082844, accumulated_logging_time=1.349666, accumulated_submission_time=18940.805812, global_step=41427, preemption_count=0, score=18940.805812, test/accuracy=0.519400, test/loss=2.328979, test/num_examples=10000, total_duration=20520.362724, train/accuracy=0.691738, train/loss=1.494721, validation/accuracy=0.642700, validation/loss=1.701755, validation/num_examples=50000
I0131 18:38:58.781466 140022518892288 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.0104401111602783, loss=3.5954978466033936
I0131 18:39:43.845185 140023005427456 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.0560041666030884, loss=3.647876262664795
I0131 18:40:30.027801 140022518892288 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.0149285793304443, loss=5.208366394042969
I0131 18:41:15.986637 140023005427456 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.0490487813949585, loss=3.610799789428711
I0131 18:42:01.952683 140022518892288 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9140058755874634, loss=4.842230319976807
I0131 18:42:48.235666 140023005427456 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.8507956862449646, loss=4.5172271728515625
I0131 18:43:34.379855 140022518892288 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.8814625144004822, loss=3.883976936340332
I0131 18:44:20.130296 140023005427456 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.0032882690429688, loss=3.724665403366089
I0131 18:45:06.044446 140022518892288 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.0149239301681519, loss=3.90400767326355
I0131 18:45:29.541425 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:45:39.753434 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:46:00.073588 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:46:01.673136 140184451094336 submission_runner.py:408] Time since start: 20972.88s, 	Step: 42353, 	{'train/accuracy': 0.70068359375, 'train/loss': 1.465484619140625, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.7100856304168701, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.3286025524139404, 'test/num_examples': 10000, 'score': 19361.10916209221, 'total_duration': 20972.88152360916, 'accumulated_submission_time': 19361.10916209221, 'accumulated_eval_time': 1608.2144901752472, 'accumulated_logging_time': 1.3862807750701904}
I0131 18:46:01.698625 140023005427456 logging_writer.py:48] [42353] accumulated_eval_time=1608.214490, accumulated_logging_time=1.386281, accumulated_submission_time=19361.109162, global_step=42353, preemption_count=0, score=19361.109162, test/accuracy=0.527200, test/loss=2.328603, test/num_examples=10000, total_duration=20972.881524, train/accuracy=0.700684, train/loss=1.465485, validation/accuracy=0.646920, validation/loss=1.710086, validation/num_examples=50000
I0131 18:46:20.924430 140022518892288 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9968194961547852, loss=3.925450086593628
I0131 18:47:05.004686 140023005427456 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9870266914367676, loss=4.277568340301514
I0131 18:47:51.242089 140022518892288 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.016697883605957, loss=3.525135040283203
I0131 18:48:37.638290 140023005427456 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0112062692642212, loss=3.8380117416381836
I0131 18:49:24.007815 140022518892288 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.9925763010978699, loss=3.6673216819763184
I0131 18:50:09.868578 140023005427456 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.0634160041809082, loss=3.5361857414245605
I0131 18:50:55.951342 140022518892288 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0210942029953003, loss=3.6732563972473145
I0131 18:51:42.154082 140023005427456 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.048362374305725, loss=3.5430784225463867
I0131 18:52:28.311261 140022518892288 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.049304485321045, loss=3.5799715518951416
I0131 18:53:01.905065 140184451094336 spec.py:321] Evaluating on the training split.
I0131 18:53:12.413844 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 18:53:34.584607 140184451094336 spec.py:349] Evaluating on the test split.
I0131 18:53:36.202681 140184451094336 submission_runner.py:408] Time since start: 21427.41s, 	Step: 43275, 	{'train/accuracy': 0.7005859017372131, 'train/loss': 1.4444526433944702, 'validation/accuracy': 0.6436799764633179, 'validation/loss': 1.7053855657577515, 'validation/num_examples': 50000, 'test/accuracy': 0.5193000435829163, 'test/loss': 2.333106279373169, 'test/num_examples': 10000, 'score': 19781.25616669655, 'total_duration': 21427.411111593246, 'accumulated_submission_time': 19781.25616669655, 'accumulated_eval_time': 1642.51211977005, 'accumulated_logging_time': 1.4228754043579102}
I0131 18:53:36.228557 140023005427456 logging_writer.py:48] [43275] accumulated_eval_time=1642.512120, accumulated_logging_time=1.422875, accumulated_submission_time=19781.256167, global_step=43275, preemption_count=0, score=19781.256167, test/accuracy=0.519300, test/loss=2.333106, test/num_examples=10000, total_duration=21427.411112, train/accuracy=0.700586, train/loss=1.444453, validation/accuracy=0.643680, validation/loss=1.705386, validation/num_examples=50000
I0131 18:53:46.643357 140022518892288 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.1695586442947388, loss=3.697636604309082
I0131 18:54:29.107077 140023005427456 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.8958576917648315, loss=4.254367828369141
I0131 18:55:15.240658 140022518892288 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.032700777053833, loss=3.54889178276062
I0131 18:56:01.582678 140023005427456 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.9379796981811523, loss=5.320065975189209
I0131 18:56:47.497637 140022518892288 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.034752368927002, loss=3.7131261825561523
I0131 18:57:33.701071 140023005427456 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.0543218851089478, loss=3.6548421382904053
I0131 18:58:19.713122 140022518892288 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.9431631565093994, loss=5.419023036956787
I0131 18:59:05.752158 140023005427456 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9919067621231079, loss=3.5961906909942627
I0131 18:59:51.647885 140022518892288 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.0127593278884888, loss=3.598057746887207
I0131 19:00:36.541089 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:00:47.035478 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:01:07.978896 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:01:09.577505 140184451094336 submission_runner.py:408] Time since start: 21880.79s, 	Step: 44199, 	{'train/accuracy': 0.7070116996765137, 'train/loss': 1.4227125644683838, 'validation/accuracy': 0.6459999680519104, 'validation/loss': 1.6852363348007202, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.296396255493164, 'test/num_examples': 10000, 'score': 20201.509860515594, 'total_duration': 21880.785943746567, 'accumulated_submission_time': 20201.509860515594, 'accumulated_eval_time': 1675.5485351085663, 'accumulated_logging_time': 1.4599545001983643}
I0131 19:01:09.599129 140023005427456 logging_writer.py:48] [44199] accumulated_eval_time=1675.548535, accumulated_logging_time=1.459955, accumulated_submission_time=20201.509861, global_step=44199, preemption_count=0, score=20201.509861, test/accuracy=0.522600, test/loss=2.296396, test/num_examples=10000, total_duration=21880.785944, train/accuracy=0.707012, train/loss=1.422713, validation/accuracy=0.646000, validation/loss=1.685236, validation/num_examples=50000
I0131 19:01:10.408953 140022518892288 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.9604030251502991, loss=3.5092639923095703
I0131 19:01:51.670211 140023005427456 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.905004620552063, loss=4.594358444213867
I0131 19:02:37.755735 140022518892288 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.9579111933708191, loss=4.475121974945068
I0131 19:03:23.905055 140023005427456 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1295243501663208, loss=3.5979855060577393
I0131 19:04:09.979424 140022518892288 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.09626042842865, loss=3.58140230178833
I0131 19:04:56.045889 140023005427456 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.9350129961967468, loss=4.520934104919434
I0131 19:05:42.495820 140022518892288 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.118966817855835, loss=3.5630154609680176
I0131 19:06:28.458929 140023005427456 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.0334810018539429, loss=3.7402360439300537
I0131 19:07:14.314131 140022518892288 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.0132776498794556, loss=5.342587947845459
I0131 19:08:00.545550 140023005427456 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.0802350044250488, loss=3.5973424911499023
I0131 19:08:09.608825 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:08:20.151078 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:08:43.866441 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:08:45.463175 140184451094336 submission_runner.py:408] Time since start: 22336.67s, 	Step: 45121, 	{'train/accuracy': 0.7047460675239563, 'train/loss': 1.4595746994018555, 'validation/accuracy': 0.6520599722862244, 'validation/loss': 1.6880558729171753, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.3047330379486084, 'test/num_examples': 10000, 'score': 20621.46268749237, 'total_duration': 22336.67161679268, 'accumulated_submission_time': 20621.46268749237, 'accumulated_eval_time': 1711.402881860733, 'accumulated_logging_time': 1.4905712604522705}
I0131 19:08:45.488093 140022518892288 logging_writer.py:48] [45121] accumulated_eval_time=1711.402882, accumulated_logging_time=1.490571, accumulated_submission_time=20621.462687, global_step=45121, preemption_count=0, score=20621.462687, test/accuracy=0.526800, test/loss=2.304733, test/num_examples=10000, total_duration=22336.671617, train/accuracy=0.704746, train/loss=1.459575, validation/accuracy=0.652060, validation/loss=1.688056, validation/num_examples=50000
I0131 19:09:17.492651 140023005427456 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.954243004322052, loss=5.396022796630859
I0131 19:10:02.900443 140022518892288 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.0040611028671265, loss=3.8461856842041016
I0131 19:10:48.817083 140023005427456 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.006126046180725, loss=3.641927719116211
I0131 19:11:35.084821 140022518892288 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9496719241142273, loss=4.331817626953125
I0131 19:12:21.184708 140023005427456 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9503613114356995, loss=4.204639434814453
I0131 19:13:07.550736 140022518892288 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0103580951690674, loss=3.7665820121765137
I0131 19:13:53.537857 140023005427456 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.0815006494522095, loss=3.547839879989624
I0131 19:14:39.465039 140022518892288 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.0782768726348877, loss=3.6435036659240723
I0131 19:15:25.700799 140023005427456 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9909168481826782, loss=3.6430091857910156
I0131 19:15:45.611788 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:15:55.877775 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:16:15.006203 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:16:16.618607 140184451094336 submission_runner.py:408] Time since start: 22787.83s, 	Step: 46045, 	{'train/accuracy': 0.703808605670929, 'train/loss': 1.4133350849151611, 'validation/accuracy': 0.649899959564209, 'validation/loss': 1.6546680927276611, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.286877155303955, 'test/num_examples': 10000, 'score': 21041.530358314514, 'total_duration': 22787.827045440674, 'accumulated_submission_time': 21041.530358314514, 'accumulated_eval_time': 1742.409699678421, 'accumulated_logging_time': 1.524674892425537}
I0131 19:16:16.640510 140022518892288 logging_writer.py:48] [46045] accumulated_eval_time=1742.409700, accumulated_logging_time=1.524675, accumulated_submission_time=21041.530358, global_step=46045, preemption_count=0, score=21041.530358, test/accuracy=0.523700, test/loss=2.286877, test/num_examples=10000, total_duration=22787.827045, train/accuracy=0.703809, train/loss=1.413335, validation/accuracy=0.649900, validation/loss=1.654668, validation/num_examples=50000
I0131 19:16:39.078937 140023005427456 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.126157283782959, loss=3.5029115676879883
I0131 19:17:23.462359 140022518892288 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.055676817893982, loss=3.83009672164917
I0131 19:18:09.715615 140023005427456 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0135900974273682, loss=3.638719320297241
I0131 19:18:55.903896 140022518892288 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.8972131609916687, loss=5.279690742492676
I0131 19:19:41.886620 140023005427456 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1752369403839111, loss=3.629107713699341
I0131 19:20:27.803325 140022518892288 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0520265102386475, loss=3.6000845432281494
I0131 19:21:13.789520 140023005427456 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.041998267173767, loss=3.5174641609191895
I0131 19:21:59.885891 140022518892288 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9159421920776367, loss=4.967912673950195
I0131 19:22:46.007495 140023005427456 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.0113247632980347, loss=4.181155204772949
I0131 19:23:17.025207 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:23:27.449200 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:23:49.687055 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:23:51.292089 140184451094336 submission_runner.py:408] Time since start: 23242.50s, 	Step: 46969, 	{'train/accuracy': 0.7266796827316284, 'train/loss': 1.295320987701416, 'validation/accuracy': 0.6565399765968323, 'validation/loss': 1.6153538227081299, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.2345681190490723, 'test/num_examples': 10000, 'score': 21461.85862827301, 'total_duration': 23242.500529289246, 'accumulated_submission_time': 21461.85862827301, 'accumulated_eval_time': 1776.6765806674957, 'accumulated_logging_time': 1.5564830303192139}
I0131 19:23:51.318160 140022518892288 logging_writer.py:48] [46969] accumulated_eval_time=1776.676581, accumulated_logging_time=1.556483, accumulated_submission_time=21461.858628, global_step=46969, preemption_count=0, score=21461.858628, test/accuracy=0.532000, test/loss=2.234568, test/num_examples=10000, total_duration=23242.500529, train/accuracy=0.726680, train/loss=1.295321, validation/accuracy=0.656540, validation/loss=1.615354, validation/num_examples=50000
I0131 19:24:04.127941 140023005427456 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9485939741134644, loss=4.95635986328125
I0131 19:24:47.252792 140022518892288 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.087139368057251, loss=3.696380853652954
I0131 19:25:33.248294 140023005427456 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0312813520431519, loss=5.23723840713501
I0131 19:26:19.536454 140022518892288 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.8650553226470947, loss=4.878848075866699
I0131 19:27:05.559856 140023005427456 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1117448806762695, loss=3.6113579273223877
I0131 19:27:51.519163 140022518892288 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.2051453590393066, loss=5.402503967285156
I0131 19:28:37.992032 140023005427456 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.9907330274581909, loss=3.8600778579711914
I0131 19:29:24.078917 140022518892288 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9983785152435303, loss=5.220490455627441
I0131 19:30:10.224330 140023005427456 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.0435805320739746, loss=3.643822193145752
I0131 19:30:51.591697 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:31:02.154632 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:31:24.222877 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:31:25.828229 140184451094336 submission_runner.py:408] Time since start: 23697.04s, 	Step: 47891, 	{'train/accuracy': 0.7119921445846558, 'train/loss': 1.4000214338302612, 'validation/accuracy': 0.6568399667739868, 'validation/loss': 1.6305649280548096, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.2502574920654297, 'test/num_examples': 10000, 'score': 21882.072820425034, 'total_duration': 23697.036667346954, 'accumulated_submission_time': 21882.072820425034, 'accumulated_eval_time': 1810.9130997657776, 'accumulated_logging_time': 1.5944783687591553}
I0131 19:31:25.851443 140022518892288 logging_writer.py:48] [47891] accumulated_eval_time=1810.913100, accumulated_logging_time=1.594478, accumulated_submission_time=21882.072820, global_step=47891, preemption_count=0, score=21882.072820, test/accuracy=0.530900, test/loss=2.250257, test/num_examples=10000, total_duration=23697.036667, train/accuracy=0.711992, train/loss=1.400021, validation/accuracy=0.656840, validation/loss=1.630565, validation/num_examples=50000
I0131 19:31:29.858637 140023005427456 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0273135900497437, loss=3.699287176132202
I0131 19:32:11.802007 140022518892288 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9066444635391235, loss=5.291618824005127
I0131 19:32:57.667479 140023005427456 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9543376564979553, loss=4.190643787384033
I0131 19:33:43.811133 140022518892288 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9918429851531982, loss=3.791780471801758
I0131 19:34:29.961097 140023005427456 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.9071033596992493, loss=4.872182846069336
I0131 19:35:15.767527 140022518892288 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.033303141593933, loss=3.6796627044677734
I0131 19:36:01.865264 140023005427456 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0089433193206787, loss=3.5123631954193115
I0131 19:36:47.914591 140022518892288 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0377404689788818, loss=3.548658609390259
I0131 19:37:34.010822 140023005427456 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.0368224382400513, loss=3.5566329956054688
I0131 19:38:20.280795 140022518892288 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.0545028448104858, loss=5.110746383666992
I0131 19:38:25.872512 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:38:36.358141 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:38:54.866264 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:38:56.477882 140184451094336 submission_runner.py:408] Time since start: 24147.69s, 	Step: 48814, 	{'train/accuracy': 0.7128710746765137, 'train/loss': 1.3655827045440674, 'validation/accuracy': 0.6576200127601624, 'validation/loss': 1.6067535877227783, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.232346534729004, 'test/num_examples': 10000, 'score': 22302.034429311752, 'total_duration': 24147.68631052971, 'accumulated_submission_time': 22302.034429311752, 'accumulated_eval_time': 1841.5184531211853, 'accumulated_logging_time': 1.6299116611480713}
I0131 19:38:56.507810 140023005427456 logging_writer.py:48] [48814] accumulated_eval_time=1841.518453, accumulated_logging_time=1.629912, accumulated_submission_time=22302.034429, global_step=48814, preemption_count=0, score=22302.034429, test/accuracy=0.533500, test/loss=2.232347, test/num_examples=10000, total_duration=24147.686311, train/accuracy=0.712871, train/loss=1.365583, validation/accuracy=0.657620, validation/loss=1.606754, validation/num_examples=50000
I0131 19:39:32.437179 140022518892288 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0477997064590454, loss=3.5892961025238037
I0131 19:40:18.261021 140023005427456 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0096492767333984, loss=3.500007390975952
I0131 19:41:04.616158 140022518892288 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.08222496509552, loss=3.587763786315918
I0131 19:41:51.069385 140023005427456 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9538394212722778, loss=3.972264289855957
I0131 19:42:37.199077 140022518892288 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9181939363479614, loss=4.95073127746582
I0131 19:43:23.499298 140023005427456 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1710177659988403, loss=3.5632450580596924
I0131 19:44:09.788666 140022518892288 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0684752464294434, loss=3.6330807209014893
I0131 19:44:55.786447 140023005427456 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.1074782609939575, loss=3.6050312519073486
I0131 19:45:42.039178 140022518892288 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.079991102218628, loss=3.5985310077667236
I0131 19:45:56.922848 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:46:07.538973 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:46:28.298137 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:46:29.914046 140184451094336 submission_runner.py:408] Time since start: 24601.12s, 	Step: 49734, 	{'train/accuracy': 0.728808581829071, 'train/loss': 1.3120293617248535, 'validation/accuracy': 0.6609199643135071, 'validation/loss': 1.6072094440460205, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.2364699840545654, 'test/num_examples': 10000, 'score': 22722.3923227787, 'total_duration': 24601.1224694252, 'accumulated_submission_time': 22722.3923227787, 'accumulated_eval_time': 1874.509628534317, 'accumulated_logging_time': 1.6707801818847656}
I0131 19:46:29.939288 140023005427456 logging_writer.py:48] [49734] accumulated_eval_time=1874.509629, accumulated_logging_time=1.670780, accumulated_submission_time=22722.392323, global_step=49734, preemption_count=0, score=22722.392323, test/accuracy=0.534400, test/loss=2.236470, test/num_examples=10000, total_duration=24601.122469, train/accuracy=0.728809, train/loss=1.312029, validation/accuracy=0.660920, validation/loss=1.607209, validation/num_examples=50000
I0131 19:46:56.744040 140022518892288 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.0904316902160645, loss=3.5040156841278076
I0131 19:47:42.180437 140023005427456 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.1599780321121216, loss=3.555514335632324
I0131 19:48:28.562256 140022518892288 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9813105463981628, loss=3.4924724102020264
I0131 19:49:15.554314 140023005427456 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8867160081863403, loss=4.795750617980957
I0131 19:50:01.416551 140022518892288 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0522394180297852, loss=3.471557140350342
I0131 19:50:47.941882 140023005427456 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.060105800628662, loss=3.5500943660736084
I0131 19:51:34.386744 140022518892288 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.1100146770477295, loss=3.5405499935150146
I0131 19:52:20.695575 140023005427456 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0096077919006348, loss=3.732123613357544
I0131 19:53:06.697830 140022518892288 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.0589708089828491, loss=3.6977951526641846
I0131 19:53:29.926408 140184451094336 spec.py:321] Evaluating on the training split.
I0131 19:53:40.296341 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 19:54:02.820379 140184451094336 spec.py:349] Evaluating on the test split.
I0131 19:54:04.428803 140184451094336 submission_runner.py:408] Time since start: 25055.64s, 	Step: 50652, 	{'train/accuracy': 0.7116601467132568, 'train/loss': 1.3645031452178955, 'validation/accuracy': 0.6611599922180176, 'validation/loss': 1.5872159004211426, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.2071101665496826, 'test/num_examples': 10000, 'score': 23142.32176733017, 'total_duration': 25055.637226343155, 'accumulated_submission_time': 23142.32176733017, 'accumulated_eval_time': 1909.0119910240173, 'accumulated_logging_time': 1.7064182758331299}
I0131 19:54:04.456466 140023005427456 logging_writer.py:48] [50652] accumulated_eval_time=1909.011991, accumulated_logging_time=1.706418, accumulated_submission_time=23142.321767, global_step=50652, preemption_count=0, score=23142.321767, test/accuracy=0.537500, test/loss=2.207110, test/num_examples=10000, total_duration=25055.637226, train/accuracy=0.711660, train/loss=1.364503, validation/accuracy=0.661160, validation/loss=1.587216, validation/num_examples=50000
I0131 19:54:24.092342 140022518892288 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.03795325756073, loss=3.761761426925659
I0131 19:55:08.155821 140023005427456 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.1619054079055786, loss=3.5826096534729004
I0131 19:55:53.942115 140022518892288 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.0921956300735474, loss=3.4949491024017334
I0131 19:56:40.471556 140023005427456 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0817209482192993, loss=5.343537330627441
I0131 19:57:26.448432 140022518892288 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.0797356367111206, loss=3.508458375930786
I0131 19:58:12.559404 140023005427456 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0155802965164185, loss=3.948866605758667
I0131 19:58:59.014402 140022518892288 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.1140801906585693, loss=3.522613048553467
I0131 19:59:45.024126 140023005427456 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.1674455404281616, loss=3.5528197288513184
I0131 20:00:31.342875 140022518892288 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.067779302597046, loss=3.529812812805176
I0131 20:01:04.665225 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:01:15.047672 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:01:34.847727 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:01:36.454333 140184451094336 submission_runner.py:408] Time since start: 25507.66s, 	Step: 51574, 	{'train/accuracy': 0.717968761920929, 'train/loss': 1.3651286363601685, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.6173219680786133, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.240694761276245, 'test/num_examples': 10000, 'score': 23562.47232246399, 'total_duration': 25507.66274857521, 'accumulated_submission_time': 23562.47232246399, 'accumulated_eval_time': 1940.8010630607605, 'accumulated_logging_time': 1.7460317611694336}
I0131 20:01:36.483194 140023005427456 logging_writer.py:48] [51574] accumulated_eval_time=1940.801063, accumulated_logging_time=1.746032, accumulated_submission_time=23562.472322, global_step=51574, preemption_count=0, score=23562.472322, test/accuracy=0.533300, test/loss=2.240695, test/num_examples=10000, total_duration=25507.662749, train/accuracy=0.717969, train/loss=1.365129, validation/accuracy=0.662080, validation/loss=1.617322, validation/num_examples=50000
I0131 20:01:47.305816 140022518892288 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.9141488075256348, loss=4.150054931640625
I0131 20:02:30.288003 140023005427456 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.0270206928253174, loss=3.524301528930664
I0131 20:03:16.406851 140022518892288 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0701185464859009, loss=3.6099023818969727
I0131 20:04:02.678359 140023005427456 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.0291212797164917, loss=3.4362239837646484
I0131 20:04:48.630502 140022518892288 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.064650535583496, loss=3.529606819152832
I0131 20:05:34.697487 140023005427456 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.9842766523361206, loss=4.921252250671387
I0131 20:06:20.777600 140022518892288 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.051551342010498, loss=3.5603582859039307
I0131 20:07:06.898814 140023005427456 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9165573120117188, loss=4.049131870269775
I0131 20:07:53.005705 140022518892288 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.0431190729141235, loss=3.4875056743621826
I0131 20:08:36.878517 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:08:47.440215 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:09:09.855643 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:09:11.460015 140184451094336 submission_runner.py:408] Time since start: 25962.67s, 	Step: 52496, 	{'train/accuracy': 0.7241796851158142, 'train/loss': 1.3452785015106201, 'validation/accuracy': 0.6620000004768372, 'validation/loss': 1.6125682592391968, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.2341880798339844, 'test/num_examples': 10000, 'score': 23982.808941841125, 'total_duration': 25962.668434381485, 'accumulated_submission_time': 23982.808941841125, 'accumulated_eval_time': 1975.382539987564, 'accumulated_logging_time': 1.786369800567627}
I0131 20:09:11.486527 140023005427456 logging_writer.py:48] [52496] accumulated_eval_time=1975.382540, accumulated_logging_time=1.786370, accumulated_submission_time=23982.808942, global_step=52496, preemption_count=0, score=23982.808942, test/accuracy=0.540600, test/loss=2.234188, test/num_examples=10000, total_duration=25962.668434, train/accuracy=0.724180, train/loss=1.345279, validation/accuracy=0.662000, validation/loss=1.612568, validation/num_examples=50000
I0131 20:09:13.493090 140022518892288 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0170975923538208, loss=3.4714577198028564
I0131 20:09:54.802657 140023005427456 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.1063134670257568, loss=3.471527338027954
I0131 20:10:40.658291 140022518892288 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.1181238889694214, loss=3.5006043910980225
I0131 20:11:26.841051 140023005427456 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.0108184814453125, loss=4.066586017608643
I0131 20:12:13.086463 140022518892288 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.0446337461471558, loss=3.487018346786499
I0131 20:12:59.285849 140023005427456 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0231313705444336, loss=3.557351589202881
I0131 20:13:45.637166 140022518892288 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.201910138130188, loss=3.4654717445373535
I0131 20:14:31.693369 140023005427456 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.059334635734558, loss=3.5921452045440674
I0131 20:15:17.858473 140022518892288 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9988686442375183, loss=5.326982498168945
I0131 20:16:04.131814 140023005427456 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.9965541362762451, loss=3.547750234603882
I0131 20:16:11.644911 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:16:22.158918 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:16:41.049822 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:16:42.653198 140184451094336 submission_runner.py:408] Time since start: 26413.86s, 	Step: 53418, 	{'train/accuracy': 0.7199804782867432, 'train/loss': 1.3372256755828857, 'validation/accuracy': 0.6657999753952026, 'validation/loss': 1.5774351358413696, 'validation/num_examples': 50000, 'test/accuracy': 0.5430999994277954, 'test/loss': 2.2015929222106934, 'test/num_examples': 10000, 'score': 24402.908334493637, 'total_duration': 26413.86163020134, 'accumulated_submission_time': 24402.908334493637, 'accumulated_eval_time': 2006.3908026218414, 'accumulated_logging_time': 1.824455976486206}
I0131 20:16:42.683879 140022518892288 logging_writer.py:48] [53418] accumulated_eval_time=2006.390803, accumulated_logging_time=1.824456, accumulated_submission_time=24402.908334, global_step=53418, preemption_count=0, score=24402.908334, test/accuracy=0.543100, test/loss=2.201593, test/num_examples=10000, total_duration=26413.861630, train/accuracy=0.719980, train/loss=1.337226, validation/accuracy=0.665800, validation/loss=1.577435, validation/num_examples=50000
I0131 20:17:16.293570 140023005427456 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9961451888084412, loss=3.8922600746154785
I0131 20:18:02.417172 140022518892288 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9929854869842529, loss=4.808758735656738
I0131 20:18:48.483466 140023005427456 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1133956909179688, loss=3.4401891231536865
I0131 20:19:34.939292 140022518892288 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9973291754722595, loss=3.4788174629211426
I0131 20:20:21.059347 140023005427456 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.0581026077270508, loss=3.6266939640045166
I0131 20:21:07.694290 140022518892288 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0385651588439941, loss=4.977720737457275
I0131 20:21:53.640532 140023005427456 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9668257832527161, loss=4.983317852020264
I0131 20:22:39.757872 140022518892288 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.0717496871948242, loss=3.5554862022399902
I0131 20:23:25.898953 140023005427456 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0190812349319458, loss=5.284567832946777
I0131 20:23:43.025421 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:23:53.382287 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:24:14.245069 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:24:15.845185 140184451094336 submission_runner.py:408] Time since start: 26867.05s, 	Step: 54339, 	{'train/accuracy': 0.7254882454872131, 'train/loss': 1.335657000541687, 'validation/accuracy': 0.670699954032898, 'validation/loss': 1.579382300376892, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.186253309249878, 'test/num_examples': 10000, 'score': 24823.192568540573, 'total_duration': 26867.053624868393, 'accumulated_submission_time': 24823.192568540573, 'accumulated_eval_time': 2039.2105541229248, 'accumulated_logging_time': 1.866105318069458}
I0131 20:24:15.871128 140022518892288 logging_writer.py:48] [54339] accumulated_eval_time=2039.210554, accumulated_logging_time=1.866105, accumulated_submission_time=24823.192569, global_step=54339, preemption_count=0, score=24823.192569, test/accuracy=0.544900, test/loss=2.186253, test/num_examples=10000, total_duration=26867.053625, train/accuracy=0.725488, train/loss=1.335657, validation/accuracy=0.670700, validation/loss=1.579382, validation/num_examples=50000
I0131 20:24:40.717081 140023005427456 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.116106390953064, loss=3.4908266067504883
I0131 20:25:25.486311 140022518892288 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.081885576248169, loss=3.5350780487060547
I0131 20:26:11.490168 140023005427456 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.0581262111663818, loss=3.410632610321045
I0131 20:26:57.846530 140022518892288 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.02419114112854, loss=4.294048309326172
I0131 20:27:43.681467 140023005427456 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.0146993398666382, loss=5.370872974395752
I0131 20:28:30.125770 140022518892288 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.102892518043518, loss=3.5850725173950195
I0131 20:29:16.409147 140023005427456 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.08152437210083, loss=3.4428415298461914
I0131 20:30:02.715229 140022518892288 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0710567235946655, loss=4.161717414855957
I0131 20:30:48.750417 140023005427456 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.1119316816329956, loss=4.0133185386657715
I0131 20:31:16.263795 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:31:26.456883 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:31:45.962578 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:31:47.570234 140184451094336 submission_runner.py:408] Time since start: 27318.78s, 	Step: 55261, 	{'train/accuracy': 0.7294921875, 'train/loss': 1.3043168783187866, 'validation/accuracy': 0.667419970035553, 'validation/loss': 1.5707368850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.1910340785980225, 'test/num_examples': 10000, 'score': 25243.527433633804, 'total_duration': 27318.778652668, 'accumulated_submission_time': 25243.527433633804, 'accumulated_eval_time': 2070.5169591903687, 'accumulated_logging_time': 1.901301383972168}
I0131 20:31:47.594647 140022518892288 logging_writer.py:48] [55261] accumulated_eval_time=2070.516959, accumulated_logging_time=1.901301, accumulated_submission_time=25243.527434, global_step=55261, preemption_count=0, score=25243.527434, test/accuracy=0.543000, test/loss=2.191034, test/num_examples=10000, total_duration=27318.778653, train/accuracy=0.729492, train/loss=1.304317, validation/accuracy=0.667420, validation/loss=1.570737, validation/num_examples=50000
I0131 20:32:03.607727 140023005427456 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.064409613609314, loss=5.312300205230713
I0131 20:32:47.345376 140022518892288 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.1225507259368896, loss=3.521221876144409
I0131 20:33:33.674012 140023005427456 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0511035919189453, loss=4.025167942047119
I0131 20:34:20.108194 140022518892288 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.1882522106170654, loss=3.493748426437378
I0131 20:35:06.155813 140023005427456 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0401908159255981, loss=3.7745320796966553
I0131 20:35:52.340881 140022518892288 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0326762199401855, loss=5.092414855957031
I0131 20:36:38.345460 140023005427456 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.0766898393630981, loss=3.4802772998809814
I0131 20:37:24.514507 140022518892288 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9871222972869873, loss=4.790318489074707
I0131 20:38:10.744441 140023005427456 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0569238662719727, loss=3.571218490600586
I0131 20:38:47.844400 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:38:58.209875 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:39:17.923205 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:39:19.518671 140184451094336 submission_runner.py:408] Time since start: 27770.73s, 	Step: 56182, 	{'train/accuracy': 0.7528125047683716, 'train/loss': 1.1971156597137451, 'validation/accuracy': 0.6747999787330627, 'validation/loss': 1.5370960235595703, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.1446516513824463, 'test/num_examples': 10000, 'score': 25663.721523284912, 'total_duration': 27770.727110385895, 'accumulated_submission_time': 25663.721523284912, 'accumulated_eval_time': 2102.191232919693, 'accumulated_logging_time': 1.9347867965698242}
I0131 20:39:19.543383 140022518892288 logging_writer.py:48] [56182] accumulated_eval_time=2102.191233, accumulated_logging_time=1.934787, accumulated_submission_time=25663.721523, global_step=56182, preemption_count=0, score=25663.721523, test/accuracy=0.553300, test/loss=2.144652, test/num_examples=10000, total_duration=27770.727110, train/accuracy=0.752813, train/loss=1.197116, validation/accuracy=0.674800, validation/loss=1.537096, validation/num_examples=50000
I0131 20:39:27.151024 140023005427456 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0632508993148804, loss=3.399592876434326
I0131 20:40:09.697431 140022518892288 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0952153205871582, loss=3.5047740936279297
I0131 20:40:55.679959 140023005427456 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.0589309930801392, loss=3.470393180847168
I0131 20:41:41.976474 140022518892288 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.0224529504776, loss=5.114987850189209
I0131 20:42:28.042114 140023005427456 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.112697720527649, loss=3.7753825187683105
I0131 20:43:14.099911 140022518892288 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.103246808052063, loss=3.4943041801452637
I0131 20:43:59.937642 140023005427456 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.0019248723983765, loss=3.6538596153259277
I0131 20:44:45.882103 140022518892288 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.9529258012771606, loss=4.536787986755371
I0131 20:45:31.907065 140023005427456 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.0171887874603271, loss=4.527849197387695
I0131 20:46:17.950266 140022518892288 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0281505584716797, loss=3.828986167907715
I0131 20:46:19.944245 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:46:30.248904 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:46:52.955719 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:46:54.570306 140184451094336 submission_runner.py:408] Time since start: 28225.78s, 	Step: 57106, 	{'train/accuracy': 0.72314453125, 'train/loss': 1.3708430528640747, 'validation/accuracy': 0.6675199866294861, 'validation/loss': 1.6150413751602173, 'validation/num_examples': 50000, 'test/accuracy': 0.546500027179718, 'test/loss': 2.2251136302948, 'test/num_examples': 10000, 'score': 26084.066102027893, 'total_duration': 28225.778723955154, 'accumulated_submission_time': 26084.066102027893, 'accumulated_eval_time': 2136.8173022270203, 'accumulated_logging_time': 1.9684455394744873}
I0131 20:46:54.597565 140023005427456 logging_writer.py:48] [57106] accumulated_eval_time=2136.817302, accumulated_logging_time=1.968446, accumulated_submission_time=26084.066102, global_step=57106, preemption_count=0, score=26084.066102, test/accuracy=0.546500, test/loss=2.225114, test/num_examples=10000, total_duration=28225.778724, train/accuracy=0.723145, train/loss=1.370843, validation/accuracy=0.667520, validation/loss=1.615041, validation/num_examples=50000
I0131 20:47:33.420654 140022518892288 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9999833703041077, loss=4.9746174812316895
I0131 20:48:19.433324 140023005427456 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.04056715965271, loss=3.867013692855835
I0131 20:49:05.919381 140022518892288 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.1726336479187012, loss=3.6657962799072266
I0131 20:49:52.363042 140023005427456 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0863703489303589, loss=3.64259672164917
I0131 20:50:38.666483 140022518892288 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9753714203834534, loss=4.1682586669921875
I0131 20:51:24.943239 140023005427456 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9554060101509094, loss=4.451590538024902
I0131 20:52:11.209114 140022518892288 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1217939853668213, loss=3.5205442905426025
I0131 20:52:57.179394 140023005427456 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0499935150146484, loss=3.5821385383605957
I0131 20:53:43.381525 140022518892288 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0556323528289795, loss=4.529347896575928
I0131 20:53:54.636631 140184451094336 spec.py:321] Evaluating on the training split.
I0131 20:54:05.410533 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 20:54:25.492507 140184451094336 spec.py:349] Evaluating on the test split.
I0131 20:54:27.095190 140184451094336 submission_runner.py:408] Time since start: 28678.30s, 	Step: 58026, 	{'train/accuracy': 0.7324023246765137, 'train/loss': 1.3181718587875366, 'validation/accuracy': 0.6703199744224548, 'validation/loss': 1.5841935873031616, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.2126083374023438, 'test/num_examples': 10000, 'score': 26504.048363685608, 'total_duration': 28678.303606510162, 'accumulated_submission_time': 26504.048363685608, 'accumulated_eval_time': 2169.2758326530457, 'accumulated_logging_time': 2.005272388458252}
I0131 20:54:27.125903 140023005427456 logging_writer.py:48] [58026] accumulated_eval_time=2169.275833, accumulated_logging_time=2.005272, accumulated_submission_time=26504.048364, global_step=58026, preemption_count=0, score=26504.048364, test/accuracy=0.545700, test/loss=2.212608, test/num_examples=10000, total_duration=28678.303607, train/accuracy=0.732402, train/loss=1.318172, validation/accuracy=0.670320, validation/loss=1.584194, validation/num_examples=50000
I0131 20:54:57.162933 140022518892288 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.175023078918457, loss=3.5405702590942383
I0131 20:55:42.956361 140023005427456 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.0245991945266724, loss=4.830801010131836
I0131 20:56:29.149084 140022518892288 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9539623260498047, loss=4.099793434143066
I0131 20:57:15.372668 140023005427456 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.212020754814148, loss=3.6145272254943848
I0131 20:58:01.168522 140022518892288 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0814698934555054, loss=3.7809438705444336
I0131 20:58:47.364119 140023005427456 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.0355123281478882, loss=4.634083271026611
I0131 20:59:33.430283 140022518892288 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1880375146865845, loss=3.4724628925323486
I0131 21:00:19.716970 140023005427456 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9789729714393616, loss=4.486438274383545
I0131 21:01:05.832983 140022518892288 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.0901775360107422, loss=3.456284999847412
I0131 21:01:27.186258 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:01:38.859006 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:01:56.638252 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:01:58.246426 140184451094336 submission_runner.py:408] Time since start: 29129.45s, 	Step: 58948, 	{'train/accuracy': 0.7443945407867432, 'train/loss': 1.2518020868301392, 'validation/accuracy': 0.6735599637031555, 'validation/loss': 1.5574877262115479, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.179090738296509, 'test/num_examples': 10000, 'score': 26924.052193164825, 'total_duration': 29129.454854011536, 'accumulated_submission_time': 26924.052193164825, 'accumulated_eval_time': 2200.3360035419464, 'accumulated_logging_time': 2.0453832149505615}
I0131 21:01:58.279162 140023005427456 logging_writer.py:48] [58948] accumulated_eval_time=2200.336004, accumulated_logging_time=2.045383, accumulated_submission_time=26924.052193, global_step=58948, preemption_count=0, score=26924.052193, test/accuracy=0.550200, test/loss=2.179091, test/num_examples=10000, total_duration=29129.454854, train/accuracy=0.744395, train/loss=1.251802, validation/accuracy=0.673560, validation/loss=1.557488, validation/num_examples=50000
I0131 21:02:19.534869 140022518892288 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0495251417160034, loss=3.6688427925109863
I0131 21:03:04.325406 140023005427456 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0770320892333984, loss=3.54642915725708
I0131 21:03:50.474062 140022518892288 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1081124544143677, loss=3.4712424278259277
I0131 21:04:36.724725 140023005427456 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1703354120254517, loss=3.5363993644714355
I0131 21:05:22.517321 140022518892288 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.0823391675949097, loss=3.646918535232544
I0131 21:06:08.408621 140023005427456 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.11557137966156, loss=3.8151819705963135
I0131 21:06:54.818428 140022518892288 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.117269515991211, loss=4.30052375793457
I0131 21:07:41.105680 140023005427456 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.1113357543945312, loss=3.4485764503479004
I0131 21:08:27.394550 140022518892288 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.995756983757019, loss=5.24690055847168
I0131 21:08:58.570765 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:09:09.159685 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:09:31.747419 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:09:33.354385 140184451094336 submission_runner.py:408] Time since start: 29584.56s, 	Step: 59869, 	{'train/accuracy': 0.7306249737739563, 'train/loss': 1.3189994096755981, 'validation/accuracy': 0.6790599822998047, 'validation/loss': 1.5507181882858276, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.166768789291382, 'test/num_examples': 10000, 'score': 27344.28582406044, 'total_duration': 29584.562819719315, 'accumulated_submission_time': 27344.28582406044, 'accumulated_eval_time': 2235.119631052017, 'accumulated_logging_time': 2.088909387588501}
I0131 21:09:33.378580 140023005427456 logging_writer.py:48] [59869] accumulated_eval_time=2235.119631, accumulated_logging_time=2.088909, accumulated_submission_time=27344.285824, global_step=59869, preemption_count=0, score=27344.285824, test/accuracy=0.552500, test/loss=2.166769, test/num_examples=10000, total_duration=29584.562820, train/accuracy=0.730625, train/loss=1.318999, validation/accuracy=0.679060, validation/loss=1.550718, validation/num_examples=50000
I0131 21:09:46.200213 140022518892288 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.178135871887207, loss=3.457376480102539
I0131 21:10:28.836170 140023005427456 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.1747066974639893, loss=3.5026378631591797
I0131 21:11:15.112749 140022518892288 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1753171682357788, loss=3.5548644065856934
I0131 21:12:01.703942 140023005427456 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1122267246246338, loss=3.461885929107666
I0131 21:12:47.821305 140022518892288 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.132287621498108, loss=3.4369306564331055
I0131 21:13:34.030340 140023005427456 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1235239505767822, loss=3.406503677368164
I0131 21:14:19.896144 140022518892288 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.0163837671279907, loss=5.094979286193848
I0131 21:15:06.038939 140023005427456 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.0428160429000854, loss=3.716715097427368
I0131 21:15:52.052254 140022518892288 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.1242170333862305, loss=3.518558979034424
I0131 21:16:33.614928 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:16:43.923625 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:17:05.329645 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:17:06.931131 140184451094336 submission_runner.py:408] Time since start: 30038.14s, 	Step: 60792, 	{'train/accuracy': 0.7305468320846558, 'train/loss': 1.3091228008270264, 'validation/accuracy': 0.6714000105857849, 'validation/loss': 1.5599348545074463, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.1946136951446533, 'test/num_examples': 10000, 'score': 27764.464653491974, 'total_duration': 30038.139572381973, 'accumulated_submission_time': 27764.464653491974, 'accumulated_eval_time': 2268.4358253479004, 'accumulated_logging_time': 2.123471736907959}
I0131 21:17:06.955724 140023005427456 logging_writer.py:48] [60792] accumulated_eval_time=2268.435825, accumulated_logging_time=2.123472, accumulated_submission_time=27764.464653, global_step=60792, preemption_count=0, score=27764.464653, test/accuracy=0.543500, test/loss=2.194614, test/num_examples=10000, total_duration=30038.139572, train/accuracy=0.730547, train/loss=1.309123, validation/accuracy=0.671400, validation/loss=1.559935, validation/num_examples=50000
I0131 21:17:10.568197 140022518892288 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.1750832796096802, loss=3.5063014030456543
I0131 21:17:51.760131 140023005427456 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.1716374158859253, loss=3.6511831283569336
I0131 21:18:37.853384 140022518892288 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0920637845993042, loss=3.4254395961761475
I0131 21:19:24.468907 140023005427456 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.011431336402893, loss=5.116713047027588
I0131 21:20:10.784108 140022518892288 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.2103168964385986, loss=3.505600690841675
I0131 21:20:57.113623 140023005427456 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.0700092315673828, loss=3.7421364784240723
I0131 21:21:43.721977 140022518892288 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.078285574913025, loss=3.6518406867980957
I0131 21:22:29.806803 140023005427456 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9999502897262573, loss=5.0754804611206055
I0131 21:23:18.085474 140022518892288 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.109756350517273, loss=3.5019242763519287
I0131 21:24:04.488008 140023005427456 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.0028773546218872, loss=3.8155441284179688
I0131 21:24:07.011519 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:24:17.437613 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:24:39.415087 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:24:41.016867 140184451094336 submission_runner.py:408] Time since start: 30492.23s, 	Step: 61707, 	{'train/accuracy': 0.7434960603713989, 'train/loss': 1.2444376945495605, 'validation/accuracy': 0.6765999794006348, 'validation/loss': 1.5327082872390747, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.1491177082061768, 'test/num_examples': 10000, 'score': 28184.4619910717, 'total_duration': 30492.225292921066, 'accumulated_submission_time': 28184.4619910717, 'accumulated_eval_time': 2302.4411540031433, 'accumulated_logging_time': 2.160538911819458}
I0131 21:24:41.042343 140022518892288 logging_writer.py:48] [61707] accumulated_eval_time=2302.441154, accumulated_logging_time=2.160539, accumulated_submission_time=28184.461991, global_step=61707, preemption_count=0, score=28184.461991, test/accuracy=0.557300, test/loss=2.149118, test/num_examples=10000, total_duration=30492.225293, train/accuracy=0.743496, train/loss=1.244438, validation/accuracy=0.676600, validation/loss=1.532708, validation/num_examples=50000
I0131 21:25:19.618781 140023005427456 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.103572964668274, loss=3.5635316371917725
I0131 21:26:05.588944 140022518892288 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.2508232593536377, loss=3.771800994873047
I0131 21:26:51.760453 140023005427456 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.1076146364212036, loss=3.4507906436920166
I0131 21:27:37.956238 140022518892288 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0556025505065918, loss=4.750472545623779
I0131 21:28:24.137567 140023005427456 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.9825530052185059, loss=4.312974452972412
I0131 21:29:10.464156 140022518892288 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0202478170394897, loss=3.792295217514038
I0131 21:29:56.604907 140023005427456 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0966252088546753, loss=5.1436543464660645
I0131 21:30:42.812260 140022518892288 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.1748971939086914, loss=3.424654960632324
I0131 21:31:29.248017 140023005427456 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.085371494293213, loss=3.7076683044433594
I0131 21:31:41.073731 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:31:51.637459 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:32:14.779200 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:32:16.389737 140184451094336 submission_runner.py:408] Time since start: 30947.60s, 	Step: 62627, 	{'train/accuracy': 0.7280663847923279, 'train/loss': 1.3498351573944092, 'validation/accuracy': 0.672540009021759, 'validation/loss': 1.5801585912704468, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.195638656616211, 'test/num_examples': 10000, 'score': 28604.436437129974, 'total_duration': 30947.598170518875, 'accumulated_submission_time': 28604.436437129974, 'accumulated_eval_time': 2337.75715136528, 'accumulated_logging_time': 2.1949493885040283}
I0131 21:32:16.415124 140022518892288 logging_writer.py:48] [62627] accumulated_eval_time=2337.757151, accumulated_logging_time=2.194949, accumulated_submission_time=28604.436437, global_step=62627, preemption_count=0, score=28604.436437, test/accuracy=0.550300, test/loss=2.195639, test/num_examples=10000, total_duration=30947.598171, train/accuracy=0.728066, train/loss=1.349835, validation/accuracy=0.672540, validation/loss=1.580159, validation/num_examples=50000
I0131 21:32:46.021272 140023005427456 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.018687129020691, loss=4.215682029724121
I0131 21:33:31.642152 140022518892288 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1935735940933228, loss=3.670901298522949
I0131 21:34:17.961742 140023005427456 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1811658143997192, loss=3.452955484390259
I0131 21:35:04.382638 140022518892288 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0869241952896118, loss=4.925485134124756
I0131 21:35:50.291014 140023005427456 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1467589139938354, loss=3.458404302597046
I0131 21:36:36.391924 140022518892288 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.9786993861198425, loss=4.608179092407227
I0131 21:37:22.709248 140023005427456 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0865124464035034, loss=3.5037689208984375
I0131 21:38:08.767426 140022518892288 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1940780878067017, loss=3.468775987625122
I0131 21:38:54.760839 140023005427456 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.1275850534439087, loss=3.4239678382873535
I0131 21:39:16.744403 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:39:27.626515 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:39:50.595224 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:39:52.198926 140184451094336 submission_runner.py:408] Time since start: 31403.41s, 	Step: 63549, 	{'train/accuracy': 0.7319726347923279, 'train/loss': 1.305245280265808, 'validation/accuracy': 0.6799799799919128, 'validation/loss': 1.5487784147262573, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.1663198471069336, 'test/num_examples': 10000, 'score': 29024.70809817314, 'total_duration': 31403.40734767914, 'accumulated_submission_time': 29024.70809817314, 'accumulated_eval_time': 2373.211641073227, 'accumulated_logging_time': 2.230957508087158}
I0131 21:39:52.230647 140022518892288 logging_writer.py:48] [63549] accumulated_eval_time=2373.211641, accumulated_logging_time=2.230958, accumulated_submission_time=29024.708098, global_step=63549, preemption_count=0, score=29024.708098, test/accuracy=0.555600, test/loss=2.166320, test/num_examples=10000, total_duration=31403.407348, train/accuracy=0.731973, train/loss=1.305245, validation/accuracy=0.679980, validation/loss=1.548778, validation/num_examples=50000
I0131 21:40:13.029664 140023005427456 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.1779050827026367, loss=3.373227119445801
I0131 21:40:57.261002 140022518892288 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1308197975158691, loss=3.5153746604919434
I0131 21:41:43.780449 140023005427456 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0824109315872192, loss=3.6538665294647217
I0131 21:42:30.144274 140022518892288 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.178817629814148, loss=3.520686626434326
I0131 21:43:16.214054 140023005427456 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1550731658935547, loss=3.4802472591400146
I0131 21:44:02.206068 140022518892288 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.9949637651443481, loss=4.184478759765625
I0131 21:44:48.269494 140023005427456 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1191716194152832, loss=4.468137741088867
I0131 21:45:34.517859 140022518892288 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.034285545349121, loss=3.912728786468506
I0131 21:46:20.894256 140023005427456 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.2163383960723877, loss=3.5522890090942383
I0131 21:46:52.566039 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:47:03.276617 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:47:25.165303 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:47:26.775873 140184451094336 submission_runner.py:408] Time since start: 31857.98s, 	Step: 64471, 	{'train/accuracy': 0.7493359446525574, 'train/loss': 1.2207766771316528, 'validation/accuracy': 0.6810799837112427, 'validation/loss': 1.495862603187561, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.1107430458068848, 'test/num_examples': 10000, 'score': 29444.985100269318, 'total_duration': 31857.98429250717, 'accumulated_submission_time': 29444.985100269318, 'accumulated_eval_time': 2407.421452522278, 'accumulated_logging_time': 2.273590564727783}
I0131 21:47:26.804536 140022518892288 logging_writer.py:48] [64471] accumulated_eval_time=2407.421453, accumulated_logging_time=2.273591, accumulated_submission_time=29444.985100, global_step=64471, preemption_count=0, score=29444.985100, test/accuracy=0.558600, test/loss=2.110743, test/num_examples=10000, total_duration=31857.984293, train/accuracy=0.749336, train/loss=1.220777, validation/accuracy=0.681080, validation/loss=1.495863, validation/num_examples=50000
I0131 21:47:39.110367 140023005427456 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0931329727172852, loss=3.3516504764556885
I0131 21:48:21.970329 140022518892288 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.114742636680603, loss=4.704185962677002
I0131 21:49:08.093267 140023005427456 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.0217468738555908, loss=4.082762241363525
I0131 21:49:54.384600 140022518892288 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1551671028137207, loss=3.567135810852051
I0131 21:50:40.285387 140023005427456 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2653528451919556, loss=3.484743118286133
I0131 21:51:26.406884 140022518892288 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1129193305969238, loss=3.4258859157562256
I0131 21:52:13.131679 140023005427456 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0301964282989502, loss=3.968796730041504
I0131 21:52:58.768765 140022518892288 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1322616338729858, loss=3.3150036334991455
I0131 21:53:45.039060 140023005427456 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.9770171046257019, loss=4.1069536209106445
I0131 21:54:26.785610 140184451094336 spec.py:321] Evaluating on the training split.
I0131 21:54:37.274304 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 21:55:00.418118 140184451094336 spec.py:349] Evaluating on the test split.
I0131 21:55:02.020600 140184451094336 submission_runner.py:408] Time since start: 32313.23s, 	Step: 65392, 	{'train/accuracy': 0.7576562166213989, 'train/loss': 1.2009063959121704, 'validation/accuracy': 0.6784200072288513, 'validation/loss': 1.5377628803253174, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 2.167978048324585, 'test/num_examples': 10000, 'score': 29864.908487081528, 'total_duration': 32313.229038715363, 'accumulated_submission_time': 29864.908487081528, 'accumulated_eval_time': 2442.6564412117004, 'accumulated_logging_time': 2.313045024871826}
I0131 21:55:02.049616 140022518892288 logging_writer.py:48] [65392] accumulated_eval_time=2442.656441, accumulated_logging_time=2.313045, accumulated_submission_time=29864.908487, global_step=65392, preemption_count=0, score=29864.908487, test/accuracy=0.556000, test/loss=2.167978, test/num_examples=10000, total_duration=32313.229039, train/accuracy=0.757656, train/loss=1.200906, validation/accuracy=0.678420, validation/loss=1.537763, validation/num_examples=50000
I0131 21:55:05.646047 140023005427456 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1020808219909668, loss=3.4961256980895996
I0131 21:55:47.439452 140022518892288 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0561628341674805, loss=4.593960762023926
I0131 21:56:33.778766 140023005427456 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.1590369939804077, loss=3.5158069133758545
I0131 21:57:19.961649 140022518892288 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.127267599105835, loss=3.4573769569396973
I0131 21:58:06.342458 140023005427456 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1727027893066406, loss=3.4778170585632324
I0131 21:58:52.345656 140022518892288 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0773259401321411, loss=3.675675630569458
I0131 21:59:38.627590 140023005427456 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.035517692565918, loss=3.8733062744140625
I0131 22:00:24.740729 140022518892288 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1956994533538818, loss=5.202332973480225
I0131 22:01:10.626497 140023005427456 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.130445122718811, loss=4.7973737716674805
I0131 22:01:57.103214 140022518892288 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.1281821727752686, loss=3.5913209915161133
I0131 22:02:02.313646 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:02:12.614647 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:02:35.192229 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:02:36.800690 140184451094336 submission_runner.py:408] Time since start: 32768.01s, 	Step: 66313, 	{'train/accuracy': 0.7408398389816284, 'train/loss': 1.2733092308044434, 'validation/accuracy': 0.6830599904060364, 'validation/loss': 1.5249103307724, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.1454310417175293, 'test/num_examples': 10000, 'score': 30285.114988565445, 'total_duration': 32768.00912475586, 'accumulated_submission_time': 30285.114988565445, 'accumulated_eval_time': 2477.143489599228, 'accumulated_logging_time': 2.3523316383361816}
I0131 22:02:36.829942 140023005427456 logging_writer.py:48] [66313] accumulated_eval_time=2477.143490, accumulated_logging_time=2.352332, accumulated_submission_time=30285.114989, global_step=66313, preemption_count=0, score=30285.114989, test/accuracy=0.558200, test/loss=2.145431, test/num_examples=10000, total_duration=32768.009125, train/accuracy=0.740840, train/loss=1.273309, validation/accuracy=0.683060, validation/loss=1.524910, validation/num_examples=50000
I0131 22:03:12.253362 140022518892288 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.9935752749443054, loss=4.511704444885254
I0131 22:03:58.152142 140023005427456 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.0871026515960693, loss=4.714652061462402
I0131 22:04:44.365733 140022518892288 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.2191791534423828, loss=3.4325640201568604
I0131 22:05:30.605646 140023005427456 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.1265009641647339, loss=3.6540048122406006
I0131 22:06:16.901633 140022518892288 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1160216331481934, loss=3.7376296520233154
I0131 22:07:03.032465 140023005427456 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.054948329925537, loss=4.8146538734436035
I0131 22:07:49.286280 140022518892288 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1608644723892212, loss=3.5587544441223145
I0131 22:08:35.272005 140023005427456 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.04510498046875, loss=4.216718673706055
I0131 22:09:21.458615 140022518892288 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0950528383255005, loss=3.7741546630859375
I0131 22:09:36.825802 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:09:47.721517 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:10:09.259875 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:10:10.872094 140184451094336 submission_runner.py:408] Time since start: 33222.08s, 	Step: 67235, 	{'train/accuracy': 0.7444140315055847, 'train/loss': 1.2327913045883179, 'validation/accuracy': 0.6813200116157532, 'validation/loss': 1.510599970817566, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 2.1155412197113037, 'test/num_examples': 10000, 'score': 30705.05445933342, 'total_duration': 33222.08050394058, 'accumulated_submission_time': 30705.05445933342, 'accumulated_eval_time': 2511.189737558365, 'accumulated_logging_time': 2.3908681869506836}
I0131 22:10:10.902254 140023005427456 logging_writer.py:48] [67235] accumulated_eval_time=2511.189738, accumulated_logging_time=2.390868, accumulated_submission_time=30705.054459, global_step=67235, preemption_count=0, score=30705.054459, test/accuracy=0.556000, test/loss=2.115541, test/num_examples=10000, total_duration=33222.080504, train/accuracy=0.744414, train/loss=1.232791, validation/accuracy=0.681320, validation/loss=1.510600, validation/num_examples=50000
I0131 22:10:37.345231 140022518892288 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.0740180015563965, loss=4.9308881759643555
I0131 22:11:22.790218 140023005427456 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.075127363204956, loss=4.630881309509277
I0131 22:12:09.337652 140022518892288 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1769150495529175, loss=3.3898935317993164
I0131 22:12:56.001026 140023005427456 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.0091603994369507, loss=3.9385764598846436
I0131 22:13:41.905661 140022518892288 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1902761459350586, loss=3.37064528465271
I0131 22:14:28.032091 140023005427456 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1686161756515503, loss=3.4205679893493652
I0131 22:15:14.266054 140022518892288 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.1200673580169678, loss=4.967980861663818
I0131 22:16:00.281975 140023005427456 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.10178804397583, loss=3.4328956604003906
I0131 22:16:46.432937 140022518892288 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1442196369171143, loss=3.4377574920654297
I0131 22:17:11.123635 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:17:21.557627 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:17:44.758208 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:17:46.365529 140184451094336 submission_runner.py:408] Time since start: 33677.57s, 	Step: 68155, 	{'train/accuracy': 0.761523425579071, 'train/loss': 1.198433756828308, 'validation/accuracy': 0.6867199540138245, 'validation/loss': 1.5193663835525513, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.129547595977783, 'test/num_examples': 10000, 'score': 31125.21816754341, 'total_duration': 33677.57394862175, 'accumulated_submission_time': 31125.21816754341, 'accumulated_eval_time': 2546.4316437244415, 'accumulated_logging_time': 2.431232452392578}
I0131 22:17:46.399226 140023005427456 logging_writer.py:48] [68155] accumulated_eval_time=2546.431644, accumulated_logging_time=2.431232, accumulated_submission_time=31125.218168, global_step=68155, preemption_count=0, score=31125.218168, test/accuracy=0.561800, test/loss=2.129548, test/num_examples=10000, total_duration=33677.573949, train/accuracy=0.761523, train/loss=1.198434, validation/accuracy=0.686720, validation/loss=1.519366, validation/num_examples=50000
I0131 22:18:04.811998 140022518892288 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1270532608032227, loss=3.6925249099731445
I0131 22:18:48.497308 140023005427456 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.0554674863815308, loss=3.5465238094329834
I0131 22:19:34.830238 140022518892288 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.1965059041976929, loss=3.4932968616485596
I0131 22:20:21.021897 140023005427456 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.257529616355896, loss=3.5790274143218994
I0131 22:21:07.144716 140022518892288 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0658069849014282, loss=3.8895928859710693
I0131 22:21:53.507173 140023005427456 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1839781999588013, loss=3.375384569168091
I0131 22:22:39.835805 140022518892288 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2122280597686768, loss=3.427441120147705
I0131 22:23:25.943189 140023005427456 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.126968502998352, loss=3.4644570350646973
I0131 22:24:12.059796 140022518892288 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.0683972835540771, loss=4.535992622375488
I0131 22:24:46.701795 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:24:57.292709 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:25:17.059730 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:25:18.664214 140184451094336 submission_runner.py:408] Time since start: 34129.87s, 	Step: 69076, 	{'train/accuracy': 0.7444921731948853, 'train/loss': 1.2327656745910645, 'validation/accuracy': 0.6861199736595154, 'validation/loss': 1.484375238418579, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.0891273021698, 'test/num_examples': 10000, 'score': 31545.463525772095, 'total_duration': 34129.872649908066, 'accumulated_submission_time': 31545.463525772095, 'accumulated_eval_time': 2578.39408159256, 'accumulated_logging_time': 2.4752867221832275}
I0131 22:25:18.691586 140023005427456 logging_writer.py:48] [69076] accumulated_eval_time=2578.394082, accumulated_logging_time=2.475287, accumulated_submission_time=31545.463526, global_step=69076, preemption_count=0, score=31545.463526, test/accuracy=0.561100, test/loss=2.089127, test/num_examples=10000, total_duration=34129.872650, train/accuracy=0.744492, train/loss=1.232766, validation/accuracy=0.686120, validation/loss=1.484375, validation/num_examples=50000
I0131 22:25:28.727226 140022518892288 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0926156044006348, loss=3.5070395469665527
I0131 22:26:11.327883 140023005427456 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.052050232887268, loss=3.645941972732544
I0131 22:26:57.484871 140022518892288 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.9852319359779358, loss=4.397398948669434
I0131 22:27:43.821366 140023005427456 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1347053050994873, loss=3.405056953430176
I0131 22:28:29.927287 140022518892288 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0867749452590942, loss=3.477248430252075
I0131 22:29:16.147444 140023005427456 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1437311172485352, loss=4.491621971130371
I0131 22:30:02.222286 140022518892288 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.225091814994812, loss=3.4591012001037598
I0131 22:30:48.185143 140023005427456 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1359106302261353, loss=3.4581704139709473
I0131 22:31:34.312352 140022518892288 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.094188928604126, loss=3.5741519927978516
I0131 22:32:18.805398 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:32:29.131301 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:32:51.681421 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:32:53.286212 140184451094336 submission_runner.py:408] Time since start: 34584.49s, 	Step: 69998, 	{'train/accuracy': 0.7477343678474426, 'train/loss': 1.226297378540039, 'validation/accuracy': 0.6846599578857422, 'validation/loss': 1.4979448318481445, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.113833427429199, 'test/num_examples': 10000, 'score': 31965.51737833023, 'total_duration': 34584.49463844299, 'accumulated_submission_time': 31965.51737833023, 'accumulated_eval_time': 2612.8748741149902, 'accumulated_logging_time': 2.5155715942382812}
I0131 22:32:53.312047 140023005427456 logging_writer.py:48] [69998] accumulated_eval_time=2612.874874, accumulated_logging_time=2.515572, accumulated_submission_time=31965.517378, global_step=69998, preemption_count=0, score=31965.517378, test/accuracy=0.559300, test/loss=2.113833, test/num_examples=10000, total_duration=34584.494638, train/accuracy=0.747734, train/loss=1.226297, validation/accuracy=0.684660, validation/loss=1.497945, validation/num_examples=50000
I0131 22:32:54.518236 140022518892288 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1266155242919922, loss=3.7192065715789795
I0131 22:33:36.124604 140023005427456 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.116904377937317, loss=3.46938419342041
I0131 22:34:22.053904 140022518892288 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.2425302267074585, loss=3.5788650512695312
I0131 22:35:08.398198 140023005427456 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.0535812377929688, loss=3.664982795715332
I0131 22:35:54.546556 140022518892288 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0001622438430786, loss=4.357178211212158
I0131 22:36:40.635245 140023005427456 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.136707067489624, loss=3.486403703689575
I0131 22:37:27.052026 140022518892288 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0592665672302246, loss=4.013739585876465
I0131 22:38:13.032657 140023005427456 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1633625030517578, loss=3.3441271781921387
I0131 22:38:59.056928 140022518892288 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.1850045919418335, loss=3.413797616958618
I0131 22:39:45.377906 140023005427456 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1550078392028809, loss=3.3798770904541016
I0131 22:39:53.427022 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:40:03.967911 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:40:23.027281 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:40:24.637851 140184451094336 submission_runner.py:408] Time since start: 35035.85s, 	Step: 70919, 	{'train/accuracy': 0.761035144329071, 'train/loss': 1.1790474653244019, 'validation/accuracy': 0.6904599666595459, 'validation/loss': 1.4743375778198242, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 2.088679552078247, 'test/num_examples': 10000, 'score': 32385.570518255234, 'total_duration': 35035.8462703228, 'accumulated_submission_time': 32385.570518255234, 'accumulated_eval_time': 2644.0857014656067, 'accumulated_logging_time': 2.554614782333374}
I0131 22:40:24.673086 140022518892288 logging_writer.py:48] [70919] accumulated_eval_time=2644.085701, accumulated_logging_time=2.554615, accumulated_submission_time=32385.570518, global_step=70919, preemption_count=0, score=32385.570518, test/accuracy=0.562400, test/loss=2.088680, test/num_examples=10000, total_duration=35035.846270, train/accuracy=0.761035, train/loss=1.179047, validation/accuracy=0.690460, validation/loss=1.474338, validation/num_examples=50000
I0131 22:40:58.010968 140023005427456 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1829482316970825, loss=3.4379560947418213
I0131 22:41:44.058669 140022518892288 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1808149814605713, loss=3.3573455810546875
I0131 22:42:30.194722 140023005427456 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.134077787399292, loss=4.030436038970947
I0131 22:43:16.587154 140022518892288 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.1787647008895874, loss=3.414921998977661
I0131 22:44:02.619129 140023005427456 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0755059719085693, loss=3.4178216457366943
I0131 22:44:48.921834 140022518892288 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.0346293449401855, loss=3.8538105487823486
I0131 22:45:34.852976 140023005427456 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.1783844232559204, loss=3.427737236022949
I0131 22:46:20.911717 140022518892288 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.2095121145248413, loss=3.373591423034668
I0131 22:47:06.984301 140023005427456 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0929018259048462, loss=3.857306480407715
I0131 22:47:24.969560 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:47:35.536799 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:47:56.492318 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:47:58.096101 140184451094336 submission_runner.py:408] Time since start: 35489.30s, 	Step: 71841, 	{'train/accuracy': 0.7437499761581421, 'train/loss': 1.2982145547866821, 'validation/accuracy': 0.6857199668884277, 'validation/loss': 1.5482041835784912, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 2.14981746673584, 'test/num_examples': 10000, 'score': 32805.80994772911, 'total_duration': 35489.30454015732, 'accumulated_submission_time': 32805.80994772911, 'accumulated_eval_time': 2677.212243080139, 'accumulated_logging_time': 2.599778652191162}
I0131 22:47:58.125631 140022518892288 logging_writer.py:48] [71841] accumulated_eval_time=2677.212243, accumulated_logging_time=2.599779, accumulated_submission_time=32805.809948, global_step=71841, preemption_count=0, score=32805.809948, test/accuracy=0.560400, test/loss=2.149817, test/num_examples=10000, total_duration=35489.304540, train/accuracy=0.743750, train/loss=1.298215, validation/accuracy=0.685720, validation/loss=1.548204, validation/num_examples=50000
I0131 22:48:22.128742 140023005427456 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.0961538553237915, loss=3.5505332946777344
I0131 22:49:06.513022 140022518892288 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.0737218856811523, loss=4.793459892272949
I0131 22:49:52.352528 140023005427456 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.169291615486145, loss=3.3796582221984863
I0131 22:50:38.576764 140022518892288 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.212860345840454, loss=3.417118549346924
I0131 22:51:24.582309 140023005427456 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.0872641801834106, loss=3.91796875
I0131 22:52:11.106975 140022518892288 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.029749870300293, loss=3.926804542541504
I0131 22:52:57.251444 140023005427456 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.0836975574493408, loss=4.472584247589111
I0131 22:53:43.354759 140022518892288 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.1855286359786987, loss=3.454690456390381
I0131 22:54:29.572081 140023005427456 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.1209133863449097, loss=3.8086814880371094
I0131 22:54:58.281392 140184451094336 spec.py:321] Evaluating on the training split.
I0131 22:55:08.880328 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 22:55:28.826956 140184451094336 spec.py:349] Evaluating on the test split.
I0131 22:55:30.440441 140184451094336 submission_runner.py:408] Time since start: 35941.65s, 	Step: 72764, 	{'train/accuracy': 0.7504296898841858, 'train/loss': 1.2068672180175781, 'validation/accuracy': 0.6949999928474426, 'validation/loss': 1.4593466520309448, 'validation/num_examples': 50000, 'test/accuracy': 0.5694000124931335, 'test/loss': 2.0661840438842773, 'test/num_examples': 10000, 'score': 33225.91061067581, 'total_duration': 35941.64887547493, 'accumulated_submission_time': 33225.91061067581, 'accumulated_eval_time': 2709.3712763786316, 'accumulated_logging_time': 2.638214349746704}
I0131 22:55:30.466780 140022518892288 logging_writer.py:48] [72764] accumulated_eval_time=2709.371276, accumulated_logging_time=2.638214, accumulated_submission_time=33225.910611, global_step=72764, preemption_count=0, score=33225.910611, test/accuracy=0.569400, test/loss=2.066184, test/num_examples=10000, total_duration=35941.648875, train/accuracy=0.750430, train/loss=1.206867, validation/accuracy=0.695000, validation/loss=1.459347, validation/num_examples=50000
I0131 22:55:45.287980 140023005427456 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.1842501163482666, loss=3.3002097606658936
I0131 22:56:28.743329 140022518892288 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.1756401062011719, loss=3.37253999710083
I0131 22:57:14.826047 140023005427456 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.1486928462982178, loss=3.378028392791748
I0131 22:58:01.071808 140022518892288 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.1254005432128906, loss=3.7117760181427
I0131 22:58:47.020044 140023005427456 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1599990129470825, loss=4.947061061859131
I0131 22:59:33.041090 140022518892288 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.0847349166870117, loss=4.265219211578369
I0131 23:00:19.387403 140023005427456 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1165995597839355, loss=4.310091972351074
I0131 23:01:05.382166 140022518892288 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.0121275186538696, loss=3.8589634895324707
I0131 23:01:51.940206 140023005427456 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.2050212621688843, loss=4.629806995391846
I0131 23:02:30.871933 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:02:41.661345 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:03:00.019703 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:03:01.632685 140184451094336 submission_runner.py:408] Time since start: 36392.84s, 	Step: 73686, 	{'train/accuracy': 0.7568163871765137, 'train/loss': 1.2269736528396606, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.5098013877868652, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 2.13584303855896, 'test/num_examples': 10000, 'score': 33646.25889086723, 'total_duration': 36392.84107017517, 'accumulated_submission_time': 33646.25889086723, 'accumulated_eval_time': 2740.13196849823, 'accumulated_logging_time': 2.6735827922821045}
I0131 23:03:01.683587 140022518892288 logging_writer.py:48] [73686] accumulated_eval_time=2740.131968, accumulated_logging_time=2.673583, accumulated_submission_time=33646.258891, global_step=73686, preemption_count=0, score=33646.258891, test/accuracy=0.563400, test/loss=2.135843, test/num_examples=10000, total_duration=36392.841070, train/accuracy=0.756816, train/loss=1.226974, validation/accuracy=0.690420, validation/loss=1.509801, validation/num_examples=50000
I0131 23:03:07.698838 140023005427456 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1513259410858154, loss=3.4420650005340576
I0131 23:03:50.031351 140022518892288 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3087291717529297, loss=3.4415597915649414
I0131 23:04:36.244925 140023005427456 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.0350068807601929, loss=4.61871862411499
I0131 23:05:22.583347 140022518892288 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.1521131992340088, loss=3.383162021636963
I0131 23:06:08.785319 140023005427456 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.0964490175247192, loss=4.350633144378662
I0131 23:06:54.931951 140022518892288 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2095723152160645, loss=3.5263962745666504
I0131 23:07:41.134210 140023005427456 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.0994222164154053, loss=4.773001670837402
I0131 23:08:27.079679 140022518892288 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.141628384590149, loss=4.555449485778809
I0131 23:09:13.301214 140023005427456 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2152299880981445, loss=3.409381866455078
I0131 23:09:59.385956 140022518892288 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2256157398223877, loss=3.446073055267334
I0131 23:10:02.002519 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:10:12.364511 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:10:37.384499 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:10:38.983975 140184451094336 submission_runner.py:408] Time since start: 36850.19s, 	Step: 74607, 	{'train/accuracy': 0.7570703029632568, 'train/loss': 1.1745266914367676, 'validation/accuracy': 0.6887800097465515, 'validation/loss': 1.460245132446289, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.070711374282837, 'test/num_examples': 10000, 'score': 34066.51602935791, 'total_duration': 36850.192398786545, 'accumulated_submission_time': 34066.51602935791, 'accumulated_eval_time': 2777.113403081894, 'accumulated_logging_time': 2.73940110206604}
I0131 23:10:39.016026 140023005427456 logging_writer.py:48] [74607] accumulated_eval_time=2777.113403, accumulated_logging_time=2.739401, accumulated_submission_time=34066.516029, global_step=74607, preemption_count=0, score=34066.516029, test/accuracy=0.565300, test/loss=2.070711, test/num_examples=10000, total_duration=36850.192399, train/accuracy=0.757070, train/loss=1.174527, validation/accuracy=0.688780, validation/loss=1.460245, validation/num_examples=50000
I0131 23:11:17.298629 140022518892288 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3106553554534912, loss=3.5345168113708496
I0131 23:12:03.545961 140023005427456 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2144843339920044, loss=4.802308082580566
I0131 23:12:49.660599 140022518892288 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1462005376815796, loss=3.6466569900512695
I0131 23:13:35.861390 140023005427456 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.2280234098434448, loss=5.172563552856445
I0131 23:14:22.034322 140022518892288 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.20624840259552, loss=3.3651533126831055
I0131 23:15:08.094908 140023005427456 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.217244267463684, loss=3.328794479370117
I0131 23:15:53.861820 140022518892288 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2248586416244507, loss=3.4400827884674072
I0131 23:16:40.034734 140023005427456 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.155859112739563, loss=3.5904858112335205
I0131 23:17:26.222752 140022518892288 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.260732650756836, loss=3.547302722930908
I0131 23:17:39.290664 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:17:49.605232 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:18:11.835325 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:18:13.451084 140184451094336 submission_runner.py:408] Time since start: 37304.66s, 	Step: 75530, 	{'train/accuracy': 0.75341796875, 'train/loss': 1.2075634002685547, 'validation/accuracy': 0.6945199966430664, 'validation/loss': 1.4618675708770752, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 2.0816633701324463, 'test/num_examples': 10000, 'score': 34486.733615875244, 'total_duration': 37304.65952205658, 'accumulated_submission_time': 34486.733615875244, 'accumulated_eval_time': 2811.2738075256348, 'accumulated_logging_time': 2.781567335128784}
I0131 23:18:13.477597 140023005427456 logging_writer.py:48] [75530] accumulated_eval_time=2811.273808, accumulated_logging_time=2.781567, accumulated_submission_time=34486.733616, global_step=75530, preemption_count=0, score=34486.733616, test/accuracy=0.567600, test/loss=2.081663, test/num_examples=10000, total_duration=37304.659522, train/accuracy=0.753418, train/loss=1.207563, validation/accuracy=0.694520, validation/loss=1.461868, validation/num_examples=50000
I0131 23:18:41.883455 140022518892288 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.1856436729431152, loss=3.4606990814208984
I0131 23:19:26.838635 140023005427456 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.1232703924179077, loss=3.9176509380340576
I0131 23:20:13.187928 140022518892288 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.2255115509033203, loss=3.342660665512085
I0131 23:20:59.231879 140023005427456 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.0790836811065674, loss=4.264645576477051
I0131 23:21:45.256145 140022518892288 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.1260249614715576, loss=4.823618412017822
I0131 23:22:31.722661 140023005427456 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.0865278244018555, loss=4.1632256507873535
I0131 23:23:17.709487 140022518892288 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.1686185598373413, loss=3.3508048057556152
I0131 23:24:03.846060 140023005427456 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.2347137928009033, loss=4.851900100708008
I0131 23:24:50.026858 140022518892288 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.1860837936401367, loss=5.1405816078186035
I0131 23:25:13.730398 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:25:24.317977 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:25:44.062936 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:25:45.679207 140184451094336 submission_runner.py:408] Time since start: 37756.89s, 	Step: 76453, 	{'train/accuracy': 0.7587304711341858, 'train/loss': 1.1997249126434326, 'validation/accuracy': 0.6966999769210815, 'validation/loss': 1.4641448259353638, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 2.0874454975128174, 'test/num_examples': 10000, 'score': 34906.928926706314, 'total_duration': 37756.88763618469, 'accumulated_submission_time': 34906.928926706314, 'accumulated_eval_time': 2843.222591161728, 'accumulated_logging_time': 2.818364381790161}
I0131 23:25:45.711070 140023005427456 logging_writer.py:48] [76453] accumulated_eval_time=2843.222591, accumulated_logging_time=2.818364, accumulated_submission_time=34906.928927, global_step=76453, preemption_count=0, score=34906.928927, test/accuracy=0.572200, test/loss=2.087445, test/num_examples=10000, total_duration=37756.887636, train/accuracy=0.758730, train/loss=1.199725, validation/accuracy=0.696700, validation/loss=1.464145, validation/num_examples=50000
I0131 23:26:04.922385 140022518892288 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.3225280046463013, loss=3.6664886474609375
I0131 23:26:49.269532 140023005427456 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.223360538482666, loss=5.102215766906738
I0131 23:27:35.780530 140022518892288 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.131866216659546, loss=4.21571683883667
I0131 23:28:22.057284 140023005427456 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.215268611907959, loss=3.4092726707458496
I0131 23:29:07.903841 140022518892288 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1238203048706055, loss=3.6518261432647705
I0131 23:29:54.048239 140023005427456 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.2340264320373535, loss=3.378424644470215
I0131 23:30:40.249592 140022518892288 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1898164749145508, loss=3.3027520179748535
I0131 23:31:26.186618 140023005427456 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.1715338230133057, loss=3.33298397064209
I0131 23:32:12.459916 140022518892288 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.3063873052597046, loss=3.481285333633423
I0131 23:32:46.004989 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:32:56.649131 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:33:15.476198 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:33:17.080647 140184451094336 submission_runner.py:408] Time since start: 38208.29s, 	Step: 77374, 	{'train/accuracy': 0.7772070169448853, 'train/loss': 1.1090781688690186, 'validation/accuracy': 0.694599986076355, 'validation/loss': 1.465245246887207, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 2.073258399963379, 'test/num_examples': 10000, 'score': 35327.165003061295, 'total_duration': 38208.289085149765, 'accumulated_submission_time': 35327.165003061295, 'accumulated_eval_time': 2874.2982473373413, 'accumulated_logging_time': 2.8605735301971436}
I0131 23:33:17.107236 140023005427456 logging_writer.py:48] [77374] accumulated_eval_time=2874.298247, accumulated_logging_time=2.860574, accumulated_submission_time=35327.165003, global_step=77374, preemption_count=0, score=35327.165003, test/accuracy=0.569500, test/loss=2.073258, test/num_examples=10000, total_duration=38208.289085, train/accuracy=0.777207, train/loss=1.109078, validation/accuracy=0.694600, validation/loss=1.465245, validation/num_examples=50000
I0131 23:33:27.923529 140022518892288 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.2291851043701172, loss=4.708460807800293
I0131 23:34:10.395778 140023005427456 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.1601265668869019, loss=3.4592783451080322
I0131 23:34:56.458454 140022518892288 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1977590322494507, loss=3.4290201663970947
I0131 23:35:42.573473 140023005427456 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.1809744834899902, loss=4.956531047821045
I0131 23:36:28.552587 140022518892288 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.2761404514312744, loss=5.048511505126953
I0131 23:37:14.704772 140023005427456 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2245455980300903, loss=4.689659595489502
I0131 23:38:00.682841 140022518892288 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.138840675354004, loss=5.150500297546387
I0131 23:38:46.696206 140023005427456 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.24993097782135, loss=3.3090198040008545
I0131 23:39:32.794236 140022518892288 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.1948130130767822, loss=3.3985342979431152
I0131 23:40:17.402048 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:40:27.975847 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:40:50.319247 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:40:51.929996 140184451094336 submission_runner.py:408] Time since start: 38663.14s, 	Step: 78298, 	{'train/accuracy': 0.7567187547683716, 'train/loss': 1.2263389825820923, 'validation/accuracy': 0.6919599771499634, 'validation/loss': 1.4922540187835693, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 2.110428810119629, 'test/num_examples': 10000, 'score': 35747.40086340904, 'total_duration': 38663.13839507103, 'accumulated_submission_time': 35747.40086340904, 'accumulated_eval_time': 2908.8261551856995, 'accumulated_logging_time': 2.8992533683776855}
I0131 23:40:51.961115 140023005427456 logging_writer.py:48] [78298] accumulated_eval_time=2908.826155, accumulated_logging_time=2.899253, accumulated_submission_time=35747.400863, global_step=78298, preemption_count=0, score=35747.400863, test/accuracy=0.566500, test/loss=2.110429, test/num_examples=10000, total_duration=38663.138395, train/accuracy=0.756719, train/loss=1.226339, validation/accuracy=0.691960, validation/loss=1.492254, validation/num_examples=50000
I0131 23:40:53.166938 140022518892288 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.168584942817688, loss=4.163302898406982
I0131 23:41:34.677455 140023005427456 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.260115385055542, loss=5.033257007598877
I0131 23:42:20.828840 140022518892288 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.262184739112854, loss=3.452787160873413
I0131 23:43:07.118050 140023005427456 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.1304537057876587, loss=3.5733871459960938
I0131 23:43:53.217825 140022518892288 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.207105278968811, loss=3.7278525829315186
I0131 23:44:39.416204 140023005427456 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.2397878170013428, loss=3.324671745300293
I0131 23:45:25.765557 140022518892288 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.2572486400604248, loss=3.457256317138672
I0131 23:46:11.796431 140023005427456 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.1401251554489136, loss=3.4033656120300293
I0131 23:46:57.879242 140022518892288 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.2789270877838135, loss=3.450194835662842
I0131 23:47:44.021234 140023005427456 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.1630815267562866, loss=3.501690626144409
I0131 23:47:51.970042 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:48:02.439198 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:48:19.454911 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:48:21.069857 140184451094336 submission_runner.py:408] Time since start: 39112.28s, 	Step: 79219, 	{'train/accuracy': 0.76429682970047, 'train/loss': 1.1475809812545776, 'validation/accuracy': 0.6978799700737, 'validation/loss': 1.4310686588287354, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 2.0265445709228516, 'test/num_examples': 10000, 'score': 36167.352187633514, 'total_duration': 39112.27828145027, 'accumulated_submission_time': 36167.352187633514, 'accumulated_eval_time': 2937.9259643554688, 'accumulated_logging_time': 2.9404571056365967}
I0131 23:48:21.104233 140022518892288 logging_writer.py:48] [79219] accumulated_eval_time=2937.925964, accumulated_logging_time=2.940457, accumulated_submission_time=36167.352188, global_step=79219, preemption_count=0, score=36167.352188, test/accuracy=0.573400, test/loss=2.026545, test/num_examples=10000, total_duration=39112.278281, train/accuracy=0.764297, train/loss=1.147581, validation/accuracy=0.697880, validation/loss=1.431069, validation/num_examples=50000
I0131 23:48:55.077411 140023005427456 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.3093914985656738, loss=3.388014554977417
I0131 23:49:41.078006 140022518892288 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.132743239402771, loss=4.255255222320557
I0131 23:50:27.160289 140023005427456 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.135047197341919, loss=4.179198265075684
I0131 23:51:13.166708 140022518892288 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.1992019414901733, loss=3.2996621131896973
I0131 23:51:59.240189 140023005427456 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.254069447517395, loss=3.3883254528045654
I0131 23:52:45.323955 140022518892288 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.207509994506836, loss=5.001438617706299
I0131 23:53:31.455108 140023005427456 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.2321830987930298, loss=3.3810601234436035
I0131 23:54:17.386375 140022518892288 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.3017364740371704, loss=5.06057071685791
I0131 23:55:03.728562 140023005427456 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.1317341327667236, loss=3.29703426361084
I0131 23:55:21.324234 140184451094336 spec.py:321] Evaluating on the training split.
I0131 23:55:31.559099 140184451094336 spec.py:333] Evaluating on the validation split.
I0131 23:55:54.697561 140184451094336 spec.py:349] Evaluating on the test split.
I0131 23:55:56.300903 140184451094336 submission_runner.py:408] Time since start: 39567.51s, 	Step: 80140, 	{'train/accuracy': 0.7719921469688416, 'train/loss': 1.1283113956451416, 'validation/accuracy': 0.6975399851799011, 'validation/loss': 1.4419785737991333, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 2.05631685256958, 'test/num_examples': 10000, 'score': 36587.514850854874, 'total_duration': 39567.50933790207, 'accumulated_submission_time': 36587.514850854874, 'accumulated_eval_time': 2972.902609348297, 'accumulated_logging_time': 2.984685182571411}
I0131 23:55:56.330565 140022518892288 logging_writer.py:48] [80140] accumulated_eval_time=2972.902609, accumulated_logging_time=2.984685, accumulated_submission_time=36587.514851, global_step=80140, preemption_count=0, score=36587.514851, test/accuracy=0.570900, test/loss=2.056317, test/num_examples=10000, total_duration=39567.509338, train/accuracy=0.771992, train/loss=1.128311, validation/accuracy=0.697540, validation/loss=1.441979, validation/num_examples=50000
I0131 23:56:20.760953 140023005427456 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.2558555603027344, loss=5.061872959136963
I0131 23:57:04.852688 140022518892288 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4049309492111206, loss=3.3588907718658447
I0131 23:57:51.156217 140023005427456 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.2776854038238525, loss=3.385283946990967
I0131 23:58:37.684641 140022518892288 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.2666914463043213, loss=3.3076038360595703
I0131 23:59:23.798767 140023005427456 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.1385432481765747, loss=4.350012302398682
I0201 00:00:10.114677 140022518892288 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.1683939695358276, loss=3.508617401123047
I0201 00:00:56.514003 140023005427456 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.2066080570220947, loss=3.435647964477539
I0201 00:01:42.694413 140022518892288 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.2316194772720337, loss=3.477208375930786
I0201 00:02:28.914414 140023005427456 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.2127314805984497, loss=3.5317676067352295
I0201 00:02:56.605508 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:03:06.831255 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:03:28.075599 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:03:29.677274 140184451094336 submission_runner.py:408] Time since start: 40020.89s, 	Step: 81062, 	{'train/accuracy': 0.7625781297683716, 'train/loss': 1.1647394895553589, 'validation/accuracy': 0.7009999752044678, 'validation/loss': 1.4354666471481323, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 2.033855438232422, 'test/num_examples': 10000, 'score': 37007.73335170746, 'total_duration': 40020.88571333885, 'accumulated_submission_time': 37007.73335170746, 'accumulated_eval_time': 3005.9743795394897, 'accumulated_logging_time': 3.023676633834839}
I0201 00:03:29.705721 140022518892288 logging_writer.py:48] [81062] accumulated_eval_time=3005.974380, accumulated_logging_time=3.023677, accumulated_submission_time=37007.733352, global_step=81062, preemption_count=0, score=37007.733352, test/accuracy=0.575200, test/loss=2.033855, test/num_examples=10000, total_duration=40020.885713, train/accuracy=0.762578, train/loss=1.164739, validation/accuracy=0.701000, validation/loss=1.435467, validation/num_examples=50000
I0201 00:03:45.319267 140023005427456 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.3673014640808105, loss=3.3829143047332764
I0201 00:04:28.514251 140022518892288 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.2270495891571045, loss=3.235469341278076
I0201 00:05:14.657214 140023005427456 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.2137789726257324, loss=3.2912092208862305
I0201 00:06:00.818804 140022518892288 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.105602741241455, loss=4.102540969848633
I0201 00:06:47.076128 140023005427456 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.1513748168945312, loss=3.4747653007507324
I0201 00:07:33.177830 140022518892288 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.273130178451538, loss=3.3402135372161865
I0201 00:08:19.592785 140023005427456 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.1444625854492188, loss=3.50406813621521
I0201 00:09:05.901009 140022518892288 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.2143856287002563, loss=4.742324352264404
I0201 00:09:51.949531 140023005427456 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.2204581499099731, loss=3.3496828079223633
I0201 00:10:29.782047 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:10:41.068923 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:10:59.902618 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:11:01.522691 140184451094336 submission_runner.py:408] Time since start: 40472.73s, 	Step: 81984, 	{'train/accuracy': 0.7649609446525574, 'train/loss': 1.1457265615463257, 'validation/accuracy': 0.7003799676895142, 'validation/loss': 1.4246442317962646, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 2.0412683486938477, 'test/num_examples': 10000, 'score': 37427.750932216644, 'total_duration': 40472.73112511635, 'accumulated_submission_time': 37427.750932216644, 'accumulated_eval_time': 3037.7150337696075, 'accumulated_logging_time': 3.0635695457458496}
I0201 00:11:01.553730 140022518892288 logging_writer.py:48] [81984] accumulated_eval_time=3037.715034, accumulated_logging_time=3.063570, accumulated_submission_time=37427.750932, global_step=81984, preemption_count=0, score=37427.750932, test/accuracy=0.576000, test/loss=2.041268, test/num_examples=10000, total_duration=40472.731125, train/accuracy=0.764961, train/loss=1.145727, validation/accuracy=0.700380, validation/loss=1.424644, validation/num_examples=50000
I0201 00:11:08.369169 140023005427456 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.4129976034164429, loss=3.3929293155670166
I0201 00:11:50.753593 140022518892288 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2996560335159302, loss=3.4141170978546143
I0201 00:12:36.831563 140023005427456 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.1936945915222168, loss=3.678128719329834
I0201 00:13:23.132249 140022518892288 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3246159553527832, loss=3.405301809310913
I0201 00:14:09.345614 140023005427456 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3410046100616455, loss=3.4138264656066895
I0201 00:14:55.246904 140022518892288 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.2476927042007446, loss=3.40191912651062
I0201 00:15:41.715341 140023005427456 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.2518110275268555, loss=3.3117284774780273
I0201 00:16:27.777566 140022518892288 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.2586525678634644, loss=3.3893380165100098
I0201 00:17:13.683680 140023005427456 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.164110779762268, loss=4.352134704589844
I0201 00:17:59.524385 140022518892288 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2829046249389648, loss=3.4241106510162354
I0201 00:18:01.543445 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:18:12.126317 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:18:33.951158 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:18:35.563585 140184451094336 submission_runner.py:408] Time since start: 40926.77s, 	Step: 82906, 	{'train/accuracy': 0.7728124856948853, 'train/loss': 1.1158013343811035, 'validation/accuracy': 0.7010799646377563, 'validation/loss': 1.41843843460083, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 2.0298521518707275, 'test/num_examples': 10000, 'score': 37847.68333148956, 'total_duration': 40926.77200841904, 'accumulated_submission_time': 37847.68333148956, 'accumulated_eval_time': 3071.735149860382, 'accumulated_logging_time': 3.1044604778289795}
I0201 00:18:35.600746 140023005427456 logging_writer.py:48] [82906] accumulated_eval_time=3071.735150, accumulated_logging_time=3.104460, accumulated_submission_time=37847.683331, global_step=82906, preemption_count=0, score=37847.683331, test/accuracy=0.573600, test/loss=2.029852, test/num_examples=10000, total_duration=40926.772008, train/accuracy=0.772812, train/loss=1.115801, validation/accuracy=0.701080, validation/loss=1.418438, validation/num_examples=50000
I0201 00:19:14.322751 140022518892288 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.2021063566207886, loss=3.5948009490966797
I0201 00:20:00.183185 140023005427456 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.3082060813903809, loss=3.325512409210205
I0201 00:20:46.156960 140022518892288 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.210386872291565, loss=3.329965114593506
I0201 00:21:32.029278 140023005427456 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.2664743661880493, loss=3.3485941886901855
I0201 00:22:18.300293 140022518892288 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.253446340560913, loss=4.074983596801758
I0201 00:23:04.207628 140023005427456 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2424649000167847, loss=3.2577435970306396
I0201 00:23:50.368822 140022518892288 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.2410647869110107, loss=3.527658462524414
I0201 00:24:36.299208 140023005427456 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.201945185661316, loss=3.264803886413574
I0201 00:25:22.256015 140022518892288 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3281487226486206, loss=3.339573621749878
I0201 00:25:35.833783 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:25:46.247931 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:26:10.466448 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:26:12.070080 140184451094336 submission_runner.py:408] Time since start: 41383.28s, 	Step: 83831, 	{'train/accuracy': 0.7667187452316284, 'train/loss': 1.1418139934539795, 'validation/accuracy': 0.7049999833106995, 'validation/loss': 1.4089363813400269, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 2.009699821472168, 'test/num_examples': 10000, 'score': 38267.859589099884, 'total_duration': 41383.278517484665, 'accumulated_submission_time': 38267.859589099884, 'accumulated_eval_time': 3107.9714460372925, 'accumulated_logging_time': 3.151188611984253}
I0201 00:26:12.098413 140023005427456 logging_writer.py:48] [83831] accumulated_eval_time=3107.971446, accumulated_logging_time=3.151189, accumulated_submission_time=38267.859589, global_step=83831, preemption_count=0, score=38267.859589, test/accuracy=0.582200, test/loss=2.009700, test/num_examples=10000, total_duration=41383.278517, train/accuracy=0.766719, train/loss=1.141814, validation/accuracy=0.705000, validation/loss=1.408936, validation/num_examples=50000
I0201 00:26:40.098882 140022518892288 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.245959758758545, loss=3.627347469329834
I0201 00:27:24.830326 140023005427456 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.297061562538147, loss=3.314723014831543
I0201 00:28:10.916085 140022518892288 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.178093433380127, loss=4.885667324066162
I0201 00:28:57.356563 140023005427456 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.2047736644744873, loss=3.350788116455078
I0201 00:29:43.261720 140022518892288 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.3241599798202515, loss=5.065467834472656
I0201 00:30:29.504890 140023005427456 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.1396598815917969, loss=3.6982874870300293
I0201 00:31:15.714944 140022518892288 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.3936134576797485, loss=4.878115653991699
I0201 00:32:01.991544 140023005427456 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.3542201519012451, loss=4.998630523681641
I0201 00:32:48.131069 140022518892288 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.1765575408935547, loss=4.0454864501953125
I0201 00:33:12.318459 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:33:22.831378 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:33:42.147499 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:33:43.761221 140184451094336 submission_runner.py:408] Time since start: 41834.97s, 	Step: 84754, 	{'train/accuracy': 0.7683984041213989, 'train/loss': 1.1525145769119263, 'validation/accuracy': 0.7024199962615967, 'validation/loss': 1.4284731149673462, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 2.029409170150757, 'test/num_examples': 10000, 'score': 38688.022983789444, 'total_duration': 41834.969650030136, 'accumulated_submission_time': 38688.022983789444, 'accumulated_eval_time': 3139.4142003059387, 'accumulated_logging_time': 3.1884803771972656}
I0201 00:33:43.794604 140023005427456 logging_writer.py:48] [84754] accumulated_eval_time=3139.414200, accumulated_logging_time=3.188480, accumulated_submission_time=38688.022984, global_step=84754, preemption_count=0, score=38688.022984, test/accuracy=0.576000, test/loss=2.029409, test/num_examples=10000, total_duration=41834.969650, train/accuracy=0.768398, train/loss=1.152515, validation/accuracy=0.702420, validation/loss=1.428473, validation/num_examples=50000
I0201 00:34:02.637431 140022518892288 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.1952378749847412, loss=3.6807284355163574
I0201 00:34:46.834506 140023005427456 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.1775074005126953, loss=3.999695301055908
I0201 00:35:32.947690 140022518892288 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.3062717914581299, loss=3.2972168922424316
I0201 00:36:19.439619 140023005427456 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.4650133848190308, loss=5.106554985046387
I0201 00:37:05.477494 140022518892288 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.3354889154434204, loss=3.4022233486175537
I0201 00:37:51.582622 140023005427456 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.2894898653030396, loss=3.7466187477111816
I0201 00:38:37.713819 140022518892288 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.2910453081130981, loss=3.376904249191284
I0201 00:39:23.763427 140023005427456 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.219632625579834, loss=3.2817349433898926
I0201 00:40:09.943987 140022518892288 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.2732633352279663, loss=4.157369613647461
I0201 00:40:43.833845 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:40:54.459917 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:41:16.669440 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:41:18.278585 140184451094336 submission_runner.py:408] Time since start: 42289.49s, 	Step: 85675, 	{'train/accuracy': 0.7705664038658142, 'train/loss': 1.1363499164581299, 'validation/accuracy': 0.7039200067520142, 'validation/loss': 1.4214590787887573, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 2.0247151851654053, 'test/num_examples': 10000, 'score': 39108.00399470329, 'total_duration': 42289.48701906204, 'accumulated_submission_time': 39108.00399470329, 'accumulated_eval_time': 3173.8589317798615, 'accumulated_logging_time': 3.232621431350708}
I0201 00:41:18.310017 140023005427456 logging_writer.py:48] [85675] accumulated_eval_time=3173.858932, accumulated_logging_time=3.232621, accumulated_submission_time=39108.003995, global_step=85675, preemption_count=0, score=39108.003995, test/accuracy=0.576700, test/loss=2.024715, test/num_examples=10000, total_duration=42289.487019, train/accuracy=0.770566, train/loss=1.136350, validation/accuracy=0.703920, validation/loss=1.421459, validation/num_examples=50000
I0201 00:41:28.709963 140022518892288 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.356471300125122, loss=3.318166494369507
I0201 00:42:11.749163 140023005427456 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.1464499235153198, loss=3.731027364730835
I0201 00:42:57.750492 140022518892288 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.300980567932129, loss=3.3375773429870605
I0201 00:43:43.903766 140023005427456 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.2605032920837402, loss=3.308065891265869
I0201 00:44:30.010307 140022518892288 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.2468059062957764, loss=3.422905206680298
I0201 00:45:16.339984 140023005427456 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.3505996465682983, loss=3.66054630279541
I0201 00:46:02.679113 140022518892288 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.2427467107772827, loss=3.371002674102783
I0201 00:46:48.863107 140023005427456 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.2724802494049072, loss=4.741859436035156
I0201 00:47:34.942536 140022518892288 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.1524512767791748, loss=4.261178970336914
I0201 00:48:18.686252 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:48:29.396476 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:48:54.323287 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:48:55.928029 140184451094336 submission_runner.py:408] Time since start: 42747.14s, 	Step: 86597, 	{'train/accuracy': 0.7929882407188416, 'train/loss': 1.1039115190505981, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.468793511390686, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 2.075772523880005, 'test/num_examples': 10000, 'score': 39528.32387185097, 'total_duration': 42747.13645219803, 'accumulated_submission_time': 39528.32387185097, 'accumulated_eval_time': 3211.1007244586945, 'accumulated_logging_time': 3.2733957767486572}
I0201 00:48:55.960727 140023005427456 logging_writer.py:48] [86597] accumulated_eval_time=3211.100724, accumulated_logging_time=3.273396, accumulated_submission_time=39528.323872, global_step=86597, preemption_count=0, score=39528.323872, test/accuracy=0.575900, test/loss=2.075773, test/num_examples=10000, total_duration=42747.136452, train/accuracy=0.792988, train/loss=1.103912, validation/accuracy=0.707380, validation/loss=1.468794, validation/num_examples=50000
I0201 00:48:57.572517 140022518892288 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2461621761322021, loss=3.2180213928222656
I0201 00:49:38.885549 140023005427456 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.3173844814300537, loss=3.2734193801879883
I0201 00:50:25.164618 140022518892288 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.232499122619629, loss=3.3747518062591553
I0201 00:51:11.394641 140023005427456 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.1612505912780762, loss=3.620680570602417
I0201 00:51:57.741132 140022518892288 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.2722156047821045, loss=4.776666164398193
I0201 00:52:43.726134 140023005427456 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.2850794792175293, loss=3.3697736263275146
I0201 00:53:29.524888 140022518892288 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.2847133874893188, loss=3.456007719039917
I0201 00:54:15.555408 140023005427456 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.2475690841674805, loss=4.84969425201416
I0201 00:55:01.476375 140022518892288 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.1895018815994263, loss=3.899216413497925
I0201 00:55:47.537951 140023005427456 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.2946113348007202, loss=4.268651962280273
I0201 00:55:56.099746 140184451094336 spec.py:321] Evaluating on the training split.
I0201 00:56:06.603322 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 00:56:30.782025 140184451094336 spec.py:349] Evaluating on the test split.
I0201 00:56:32.391129 140184451094336 submission_runner.py:408] Time since start: 43203.60s, 	Step: 87520, 	{'train/accuracy': 0.7638280987739563, 'train/loss': 1.196157455444336, 'validation/accuracy': 0.7017599940299988, 'validation/loss': 1.4531490802764893, 'validation/num_examples': 50000, 'test/accuracy': 0.575700044631958, 'test/loss': 2.062497854232788, 'test/num_examples': 10000, 'score': 39948.40259766579, 'total_duration': 43203.59956860542, 'accumulated_submission_time': 39948.40259766579, 'accumulated_eval_time': 3247.392087459564, 'accumulated_logging_time': 3.3191680908203125}
I0201 00:56:32.423253 140022518892288 logging_writer.py:48] [87520] accumulated_eval_time=3247.392087, accumulated_logging_time=3.319168, accumulated_submission_time=39948.402598, global_step=87520, preemption_count=0, score=39948.402598, test/accuracy=0.575700, test/loss=2.062498, test/num_examples=10000, total_duration=43203.599569, train/accuracy=0.763828, train/loss=1.196157, validation/accuracy=0.701760, validation/loss=1.453149, validation/num_examples=50000
I0201 00:57:05.023248 140023005427456 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.4315061569213867, loss=3.340959310531616
I0201 00:57:50.922756 140022518892288 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.202963948249817, loss=3.361065626144409
I0201 00:58:37.349709 140023005427456 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.2359365224838257, loss=3.305175304412842
I0201 00:59:23.743988 140022518892288 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.326036810874939, loss=3.2330710887908936
I0201 01:00:09.946800 140023005427456 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.3267732858657837, loss=3.310000419616699
I0201 01:00:56.014195 140022518892288 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.208408236503601, loss=3.4292654991149902
I0201 01:01:42.356575 140023005427456 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2977020740509033, loss=3.2878856658935547
I0201 01:02:28.373792 140022518892288 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.3839833736419678, loss=4.685006141662598
I0201 01:03:14.641241 140023005427456 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.3544435501098633, loss=5.012844085693359
I0201 01:03:32.829371 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:03:43.442434 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:04:05.031009 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:04:06.642108 140184451094336 submission_runner.py:408] Time since start: 43657.85s, 	Step: 88441, 	{'train/accuracy': 0.7773827910423279, 'train/loss': 1.0995545387268066, 'validation/accuracy': 0.7068799734115601, 'validation/loss': 1.3993632793426514, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.993497610092163, 'test/num_examples': 10000, 'score': 40368.75314331055, 'total_duration': 43657.850548028946, 'accumulated_submission_time': 40368.75314331055, 'accumulated_eval_time': 3281.204854249954, 'accumulated_logging_time': 3.3600547313690186}
I0201 01:04:06.672474 140022518892288 logging_writer.py:48] [88441] accumulated_eval_time=3281.204854, accumulated_logging_time=3.360055, accumulated_submission_time=40368.753143, global_step=88441, preemption_count=0, score=40368.753143, test/accuracy=0.583300, test/loss=1.993498, test/num_examples=10000, total_duration=43657.850548, train/accuracy=0.777383, train/loss=1.099555, validation/accuracy=0.706880, validation/loss=1.399363, validation/num_examples=50000
I0201 01:04:30.699685 140023005427456 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.3406418561935425, loss=3.3117496967315674
I0201 01:05:15.432894 140022518892288 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.2714749574661255, loss=3.253166437149048
I0201 01:06:01.553563 140023005427456 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.145208716392517, loss=4.104639530181885
I0201 01:06:48.037922 140022518892288 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4200749397277832, loss=3.285506010055542
I0201 01:07:34.222711 140023005427456 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.314542531967163, loss=3.2972910404205322
I0201 01:08:20.394649 140022518892288 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.261045217514038, loss=3.610346794128418
I0201 01:09:06.542962 140023005427456 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.348250150680542, loss=3.2759928703308105
I0201 01:09:52.652059 140022518892288 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.3862292766571045, loss=3.368680715560913
I0201 01:10:38.708723 140023005427456 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.2077373266220093, loss=4.216963768005371
I0201 01:11:06.683770 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:11:17.160859 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:11:37.839104 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:11:39.449916 140184451094336 submission_runner.py:408] Time since start: 44110.66s, 	Step: 89362, 	{'train/accuracy': 0.7879687547683716, 'train/loss': 1.051804780960083, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.3937915563583374, 'validation/num_examples': 50000, 'test/accuracy': 0.5820000171661377, 'test/loss': 2.0068376064300537, 'test/num_examples': 10000, 'score': 40788.70879864693, 'total_duration': 44110.658349752426, 'accumulated_submission_time': 40788.70879864693, 'accumulated_eval_time': 3313.9709889888763, 'accumulated_logging_time': 3.399597406387329}
I0201 01:11:39.481336 140022518892288 logging_writer.py:48] [89362] accumulated_eval_time=3313.970989, accumulated_logging_time=3.399597, accumulated_submission_time=40788.708799, global_step=89362, preemption_count=0, score=40788.708799, test/accuracy=0.582000, test/loss=2.006838, test/num_examples=10000, total_duration=44110.658350, train/accuracy=0.787969, train/loss=1.051805, validation/accuracy=0.708420, validation/loss=1.393792, validation/num_examples=50000
I0201 01:11:55.106285 140023005427456 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.4204177856445312, loss=4.832315921783447
I0201 01:12:38.863795 140022518892288 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.1805181503295898, loss=4.419756889343262
I0201 01:13:25.059769 140023005427456 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.2225083112716675, loss=3.8094544410705566
I0201 01:14:11.251200 140022518892288 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.220178246498108, loss=4.630609035491943
I0201 01:14:57.109464 140023005427456 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.3291360139846802, loss=3.3791613578796387
I0201 01:15:43.326391 140022518892288 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.4397579431533813, loss=4.9222412109375
I0201 01:16:29.558642 140023005427456 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.2601139545440674, loss=3.864380359649658
I0201 01:17:15.938020 140022518892288 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.1215900182724, loss=3.7860355377197266
I0201 01:18:02.367871 140023005427456 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2445963621139526, loss=3.909700632095337
I0201 01:18:39.515153 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:18:50.182717 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:19:10.706324 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:19:12.313712 140184451094336 submission_runner.py:408] Time since start: 44563.52s, 	Step: 90282, 	{'train/accuracy': 0.7703515291213989, 'train/loss': 1.137851357460022, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.4091787338256836, 'validation/num_examples': 50000, 'test/accuracy': 0.5849000215530396, 'test/loss': 2.013418197631836, 'test/num_examples': 10000, 'score': 41208.68663263321, 'total_duration': 44563.522152900696, 'accumulated_submission_time': 41208.68663263321, 'accumulated_eval_time': 3346.769567489624, 'accumulated_logging_time': 3.440016031265259}
I0201 01:19:12.342341 140022518892288 logging_writer.py:48] [90282] accumulated_eval_time=3346.769567, accumulated_logging_time=3.440016, accumulated_submission_time=41208.686633, global_step=90282, preemption_count=0, score=41208.686633, test/accuracy=0.584900, test/loss=2.013418, test/num_examples=10000, total_duration=44563.522153, train/accuracy=0.770352, train/loss=1.137851, validation/accuracy=0.709080, validation/loss=1.409179, validation/num_examples=50000
I0201 01:19:19.950252 140023005427456 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.2610183954238892, loss=4.162901878356934
I0201 01:20:02.358591 140022518892288 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.1869124174118042, loss=4.398281097412109
I0201 01:20:48.697821 140023005427456 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.3029857873916626, loss=3.2414700984954834
I0201 01:21:35.413581 140022518892288 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.287764072418213, loss=4.61693000793457
I0201 01:22:21.481425 140023005427456 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.2041090726852417, loss=3.2728564739227295
I0201 01:23:07.365629 140022518892288 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.270416259765625, loss=4.519604682922363
I0201 01:23:53.668374 140023005427456 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.3051644563674927, loss=4.616603851318359
I0201 01:24:39.638672 140022518892288 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.3662663698196411, loss=3.3210017681121826
I0201 01:25:25.837311 140023005427456 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.2068589925765991, loss=3.725407361984253
I0201 01:26:12.031959 140022518892288 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.3244065046310425, loss=3.2431530952453613
I0201 01:26:12.707529 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:26:23.023556 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:26:45.094947 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:26:46.695187 140184451094336 submission_runner.py:408] Time since start: 45017.90s, 	Step: 91203, 	{'train/accuracy': 0.779980480670929, 'train/loss': 1.1019827127456665, 'validation/accuracy': 0.7110399603843689, 'validation/loss': 1.3903183937072754, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 1.9904738664627075, 'test/num_examples': 10000, 'score': 41628.995743989944, 'total_duration': 45017.9036052227, 'accumulated_submission_time': 41628.995743989944, 'accumulated_eval_time': 3380.757195711136, 'accumulated_logging_time': 3.477928876876831}
I0201 01:26:46.724426 140023005427456 logging_writer.py:48] [91203] accumulated_eval_time=3380.757196, accumulated_logging_time=3.477929, accumulated_submission_time=41628.995744, global_step=91203, preemption_count=0, score=41628.995744, test/accuracy=0.583500, test/loss=1.990474, test/num_examples=10000, total_duration=45017.903605, train/accuracy=0.779980, train/loss=1.101983, validation/accuracy=0.711040, validation/loss=1.390318, validation/num_examples=50000
I0201 01:27:26.744945 140022518892288 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.2740200757980347, loss=3.662432909011841
I0201 01:28:12.864637 140023005427456 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.3704391717910767, loss=3.255524158477783
I0201 01:28:58.628100 140022518892288 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.3617165088653564, loss=4.998319149017334
I0201 01:29:44.987776 140023005427456 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.2538217306137085, loss=3.2470710277557373
I0201 01:30:30.994025 140022518892288 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.3673313856124878, loss=3.3597910404205322
I0201 01:31:17.084786 140023005427456 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.3283153772354126, loss=3.327122688293457
I0201 01:32:03.506676 140022518892288 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.2442561388015747, loss=3.1183509826660156
I0201 01:32:49.313305 140023005427456 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.198460578918457, loss=3.3623580932617188
I0201 01:33:35.355007 140022518892288 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.3417552709579468, loss=4.812058448791504
I0201 01:33:46.944396 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:33:57.652374 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:34:17.753685 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:34:19.357488 140184451094336 submission_runner.py:408] Time since start: 45470.57s, 	Step: 92127, 	{'train/accuracy': 0.7870507836341858, 'train/loss': 1.0477782487869263, 'validation/accuracy': 0.7113999724388123, 'validation/loss': 1.3689666986465454, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9610776901245117, 'test/num_examples': 10000, 'score': 42049.15962576866, 'total_duration': 45470.5659160614, 'accumulated_submission_time': 42049.15962576866, 'accumulated_eval_time': 3413.170263528824, 'accumulated_logging_time': 3.516024589538574}
I0201 01:34:19.388937 140023005427456 logging_writer.py:48] [92127] accumulated_eval_time=3413.170264, accumulated_logging_time=3.516025, accumulated_submission_time=42049.159626, global_step=92127, preemption_count=0, score=42049.159626, test/accuracy=0.591900, test/loss=1.961078, test/num_examples=10000, total_duration=45470.565916, train/accuracy=0.787051, train/loss=1.047778, validation/accuracy=0.711400, validation/loss=1.368967, validation/num_examples=50000
I0201 01:34:48.990757 140022518892288 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.3022043704986572, loss=3.573782205581665
I0201 01:35:34.465729 140023005427456 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.3430430889129639, loss=4.878212928771973
I0201 01:36:20.485201 140022518892288 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.1495733261108398, loss=4.147334575653076
I0201 01:37:06.700403 140023005427456 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.3331636190414429, loss=3.3005616664886475
I0201 01:37:52.747678 140022518892288 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.2148826122283936, loss=3.6617817878723145
I0201 01:38:38.929168 140023005427456 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.397599458694458, loss=3.3354804515838623
I0201 01:39:24.855384 140022518892288 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.3411866426467896, loss=3.2577342987060547
I0201 01:40:10.774013 140023005427456 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.4613723754882812, loss=4.994439601898193
I0201 01:40:56.855273 140022518892288 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.2507367134094238, loss=3.445596933364868
I0201 01:41:19.480975 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:41:30.152995 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:41:52.681194 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:41:54.280405 140184451094336 submission_runner.py:408] Time since start: 45925.49s, 	Step: 93051, 	{'train/accuracy': 0.7793359160423279, 'train/loss': 1.0956302881240845, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.3716667890548706, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9534903764724731, 'test/num_examples': 10000, 'score': 42469.195261478424, 'total_duration': 45925.48884344101, 'accumulated_submission_time': 42469.195261478424, 'accumulated_eval_time': 3447.9696865081787, 'accumulated_logging_time': 3.5564661026000977}
I0201 01:41:54.312672 140023005427456 logging_writer.py:48] [93051] accumulated_eval_time=3447.969687, accumulated_logging_time=3.556466, accumulated_submission_time=42469.195261, global_step=93051, preemption_count=0, score=42469.195261, test/accuracy=0.596300, test/loss=1.953490, test/num_examples=10000, total_duration=45925.488843, train/accuracy=0.779336, train/loss=1.095630, validation/accuracy=0.713520, validation/loss=1.371667, validation/num_examples=50000
I0201 01:42:14.347790 140022518892288 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.2720317840576172, loss=4.243402481079102
I0201 01:42:58.273823 140023005427456 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.4142022132873535, loss=3.239518880844116
I0201 01:43:44.373163 140022518892288 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.2734947204589844, loss=4.531386852264404
I0201 01:44:30.538474 140023005427456 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.2764816284179688, loss=3.389543294906616
I0201 01:45:16.540429 140022518892288 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.2420787811279297, loss=3.373615264892578
I0201 01:46:02.717924 140023005427456 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.2896039485931396, loss=4.0658392906188965
I0201 01:46:48.828350 140022518892288 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.408375859260559, loss=3.232896327972412
I0201 01:47:34.870467 140023005427456 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.444065809249878, loss=4.983445167541504
I0201 01:48:21.101861 140022518892288 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.2353578805923462, loss=4.171751499176025
I0201 01:48:54.369235 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:49:04.653104 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:49:25.618521 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:49:27.233148 140184451094336 submission_runner.py:408] Time since start: 46378.44s, 	Step: 93974, 	{'train/accuracy': 0.7795116901397705, 'train/loss': 1.0931429862976074, 'validation/accuracy': 0.7133600115776062, 'validation/loss': 1.3716752529144287, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.9728366136550903, 'test/num_examples': 10000, 'score': 42889.19483089447, 'total_duration': 46378.44157075882, 'accumulated_submission_time': 42889.19483089447, 'accumulated_eval_time': 3480.8335807323456, 'accumulated_logging_time': 3.5989716053009033}
I0201 01:49:27.266479 140023005427456 logging_writer.py:48] [93974] accumulated_eval_time=3480.833581, accumulated_logging_time=3.598972, accumulated_submission_time=42889.194831, global_step=93974, preemption_count=0, score=42889.194831, test/accuracy=0.591800, test/loss=1.972837, test/num_examples=10000, total_duration=46378.441571, train/accuracy=0.779512, train/loss=1.093143, validation/accuracy=0.713360, validation/loss=1.371675, validation/num_examples=50000
I0201 01:49:38.068747 140022518892288 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.1882293224334717, loss=3.61704421043396
I0201 01:50:20.820747 140023005427456 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.416508674621582, loss=3.248966693878174
I0201 01:51:07.305320 140022518892288 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.4130456447601318, loss=3.343027114868164
I0201 01:51:54.363609 140023005427456 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.4031603336334229, loss=4.587747097015381
I0201 01:52:40.498224 140022518892288 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.2436202764511108, loss=3.1832475662231445
I0201 01:53:26.538558 140023005427456 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.2618591785430908, loss=3.5322070121765137
I0201 01:54:12.780286 140022518892288 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.3848519325256348, loss=3.293123722076416
I0201 01:54:58.868771 140023005427456 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.4374783039093018, loss=3.6008806228637695
I0201 01:55:45.102073 140022518892288 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.2393702268600464, loss=4.7201313972473145
I0201 01:56:27.530260 140184451094336 spec.py:321] Evaluating on the training split.
I0201 01:56:38.138716 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 01:57:01.257584 140184451094336 spec.py:349] Evaluating on the test split.
I0201 01:57:02.860023 140184451094336 submission_runner.py:408] Time since start: 46834.07s, 	Step: 94893, 	{'train/accuracy': 0.7892773151397705, 'train/loss': 1.0392942428588867, 'validation/accuracy': 0.7130999565124512, 'validation/loss': 1.3569936752319336, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.949466347694397, 'test/num_examples': 10000, 'score': 43309.401195287704, 'total_duration': 46834.06846022606, 'accumulated_submission_time': 43309.401195287704, 'accumulated_eval_time': 3516.163388967514, 'accumulated_logging_time': 3.6427321434020996}
I0201 01:57:02.890611 140023005427456 logging_writer.py:48] [94893] accumulated_eval_time=3516.163389, accumulated_logging_time=3.642732, accumulated_submission_time=43309.401195, global_step=94893, preemption_count=0, score=43309.401195, test/accuracy=0.592500, test/loss=1.949466, test/num_examples=10000, total_duration=46834.068460, train/accuracy=0.789277, train/loss=1.039294, validation/accuracy=0.713100, validation/loss=1.356994, validation/num_examples=50000
I0201 01:57:06.101353 140022518892288 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.2805602550506592, loss=3.274564743041992
I0201 01:57:47.491628 140023005427456 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.2736852169036865, loss=3.204629898071289
I0201 01:58:33.703229 140022518892288 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.344656229019165, loss=3.148928642272949
I0201 01:59:20.087822 140023005427456 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.296437382698059, loss=3.1672823429107666
I0201 02:00:06.194673 140022518892288 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.3183544874191284, loss=3.932046890258789
I0201 02:00:52.217499 140023005427456 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.4623910188674927, loss=4.927210807800293
I0201 02:01:38.771104 140022518892288 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.3247809410095215, loss=4.317566394805908
I0201 02:02:24.984615 140023005427456 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.1992264986038208, loss=3.4173717498779297
I0201 02:03:11.120029 140022518892288 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3900818824768066, loss=3.2582461833953857
I0201 02:03:57.043573 140023005427456 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.2590970993041992, loss=3.555955171585083
I0201 02:04:03.313320 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:04:13.819245 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:04:33.433412 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:04:35.040675 140184451094336 submission_runner.py:408] Time since start: 47286.25s, 	Step: 95815, 	{'train/accuracy': 0.7995898127555847, 'train/loss': 0.9954485297203064, 'validation/accuracy': 0.7177199721336365, 'validation/loss': 1.337254524230957, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.9371448755264282, 'test/num_examples': 10000, 'score': 43729.76480174065, 'total_duration': 47286.249116420746, 'accumulated_submission_time': 43729.76480174065, 'accumulated_eval_time': 3547.8907368183136, 'accumulated_logging_time': 3.6849236488342285}
I0201 02:04:35.072844 140022518892288 logging_writer.py:48] [95815] accumulated_eval_time=3547.890737, accumulated_logging_time=3.684924, accumulated_submission_time=43729.764802, global_step=95815, preemption_count=0, score=43729.764802, test/accuracy=0.592900, test/loss=1.937145, test/num_examples=10000, total_duration=47286.249116, train/accuracy=0.799590, train/loss=0.995449, validation/accuracy=0.717720, validation/loss=1.337255, validation/num_examples=50000
I0201 02:05:09.994177 140023005427456 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.3097671270370483, loss=3.2243223190307617
I0201 02:05:55.739544 140022518892288 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.2486481666564941, loss=3.9217162132263184
I0201 02:06:41.882458 140023005427456 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.4862027168273926, loss=4.942392826080322
I0201 02:07:28.157410 140022518892288 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.3373936414718628, loss=3.2619214057922363
I0201 02:08:14.130969 140023005427456 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.3297209739685059, loss=3.173044443130493
I0201 02:09:00.325130 140022518892288 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.3350377082824707, loss=3.2478809356689453
I0201 02:09:46.312100 140023005427456 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.201439619064331, loss=3.805717706680298
I0201 02:10:32.403138 140022518892288 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.3654717206954956, loss=3.52441143989563
I0201 02:11:18.525115 140023005427456 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.371392846107483, loss=3.267333507537842
I0201 02:11:35.380722 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:11:45.857686 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:12:07.756189 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:12:09.361985 140184451094336 submission_runner.py:408] Time since start: 47740.57s, 	Step: 96738, 	{'train/accuracy': 0.77943354845047, 'train/loss': 1.1019657850265503, 'validation/accuracy': 0.7138400077819824, 'validation/loss': 1.384581208229065, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.9811594486236572, 'test/num_examples': 10000, 'score': 44150.00039052963, 'total_duration': 47740.57042002678, 'accumulated_submission_time': 44150.00039052963, 'accumulated_eval_time': 3581.8719758987427, 'accumulated_logging_time': 3.726364850997925}
I0201 02:12:09.397073 140022518892288 logging_writer.py:48] [96738] accumulated_eval_time=3581.871976, accumulated_logging_time=3.726365, accumulated_submission_time=44150.000391, global_step=96738, preemption_count=0, score=44150.000391, test/accuracy=0.593300, test/loss=1.981159, test/num_examples=10000, total_duration=47740.570420, train/accuracy=0.779434, train/loss=1.101966, validation/accuracy=0.713840, validation/loss=1.384581, validation/num_examples=50000
I0201 02:12:34.610391 140023005427456 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.4799127578735352, loss=3.289156675338745
I0201 02:13:19.201008 140022518892288 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.3753327131271362, loss=4.916571617126465
I0201 02:14:05.265684 140023005427456 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.2690887451171875, loss=3.2128875255584717
I0201 02:14:51.369237 140022518892288 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.4135704040527344, loss=4.876059532165527
I0201 02:15:37.330718 140023005427456 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.4684929847717285, loss=4.812477111816406
I0201 02:16:23.329268 140022518892288 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.3821271657943726, loss=3.2015609741210938
I0201 02:17:09.364003 140023005427456 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.345061182975769, loss=4.558446884155273
I0201 02:17:55.519191 140022518892288 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.292298674583435, loss=3.2003302574157715
I0201 02:18:41.698452 140023005427456 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.3520058393478394, loss=3.2336974143981934
I0201 02:19:09.591526 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:19:19.858414 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:19:43.174270 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:19:44.777031 140184451094336 submission_runner.py:408] Time since start: 48195.99s, 	Step: 97662, 	{'train/accuracy': 0.7866015434265137, 'train/loss': 1.0639996528625488, 'validation/accuracy': 0.7142999768257141, 'validation/loss': 1.3826018571853638, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.9830719232559204, 'test/num_examples': 10000, 'score': 44570.135746240616, 'total_duration': 48195.98546934128, 'accumulated_submission_time': 44570.135746240616, 'accumulated_eval_time': 3617.0574843883514, 'accumulated_logging_time': 3.7728912830352783}
I0201 02:19:44.808684 140022518892288 logging_writer.py:48] [97662] accumulated_eval_time=3617.057484, accumulated_logging_time=3.772891, accumulated_submission_time=44570.135746, global_step=97662, preemption_count=0, score=44570.135746, test/accuracy=0.590700, test/loss=1.983072, test/num_examples=10000, total_duration=48195.985469, train/accuracy=0.786602, train/loss=1.064000, validation/accuracy=0.714300, validation/loss=1.382602, validation/num_examples=50000
I0201 02:20:00.431426 140023005427456 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.272726058959961, loss=4.233139991760254
I0201 02:20:43.513122 140022518892288 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.3065448999404907, loss=3.285717248916626
I0201 02:21:29.343200 140023005427456 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.3902333974838257, loss=3.3946776390075684
I0201 02:22:15.837235 140022518892288 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.2539069652557373, loss=4.341672897338867
I0201 02:23:02.011311 140023005427456 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.1999143362045288, loss=3.824373960494995
I0201 02:23:47.803620 140022518892288 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.4196953773498535, loss=3.237027645111084
I0201 02:24:33.756383 140023005427456 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.4580048322677612, loss=4.3982391357421875
I0201 02:25:19.597887 140022518892288 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.356446385383606, loss=4.548628807067871
I0201 02:26:05.582064 140023005427456 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.4075106382369995, loss=3.250119686126709
I0201 02:26:44.803791 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:26:55.237817 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:27:14.151579 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:27:15.761594 140184451094336 submission_runner.py:408] Time since start: 48646.97s, 	Step: 98587, 	{'train/accuracy': 0.7963476181030273, 'train/loss': 1.0071494579315186, 'validation/accuracy': 0.7149400115013123, 'validation/loss': 1.3564001321792603, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.9637293815612793, 'test/num_examples': 10000, 'score': 44990.07020807266, 'total_duration': 48646.9700319767, 'accumulated_submission_time': 44990.07020807266, 'accumulated_eval_time': 3648.015291452408, 'accumulated_logging_time': 3.817314863204956}
I0201 02:27:15.795498 140022518892288 logging_writer.py:48] [98587] accumulated_eval_time=3648.015291, accumulated_logging_time=3.817315, accumulated_submission_time=44990.070208, global_step=98587, preemption_count=0, score=44990.070208, test/accuracy=0.590300, test/loss=1.963729, test/num_examples=10000, total_duration=48646.970032, train/accuracy=0.796348, train/loss=1.007149, validation/accuracy=0.714940, validation/loss=1.356400, validation/num_examples=50000
I0201 02:27:21.415417 140023005427456 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.3983997106552124, loss=4.744699954986572
I0201 02:28:03.724096 140022518892288 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.297572135925293, loss=3.1960864067077637
I0201 02:28:49.757084 140023005427456 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.3597028255462646, loss=3.252035140991211
I0201 02:29:35.893064 140022518892288 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.361646056175232, loss=3.176028251647949
I0201 02:30:22.244775 140023005427456 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.3638827800750732, loss=3.185842752456665
I0201 02:31:08.286983 140022518892288 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.2460192441940308, loss=4.203943252563477
I0201 02:31:54.440845 140023005427456 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.4868279695510864, loss=3.225065231323242
I0201 02:32:40.603495 140022518892288 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.3027944564819336, loss=3.3040921688079834
I0201 02:33:26.440119 140023005427456 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.3377190828323364, loss=3.1608223915100098
I0201 02:34:12.400379 140022518892288 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.3084436655044556, loss=3.205671548843384
I0201 02:34:15.819608 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:34:26.111638 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:34:49.977359 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:34:51.576666 140184451094336 submission_runner.py:408] Time since start: 49102.79s, 	Step: 99509, 	{'train/accuracy': 0.78919917345047, 'train/loss': 1.0567806959152222, 'validation/accuracy': 0.7200799584388733, 'validation/loss': 1.3418904542922974, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.9461467266082764, 'test/num_examples': 10000, 'score': 45410.03365278244, 'total_duration': 49102.78510594368, 'accumulated_submission_time': 45410.03365278244, 'accumulated_eval_time': 3683.7723546028137, 'accumulated_logging_time': 3.8653643131256104}
I0201 02:34:51.610757 140023005427456 logging_writer.py:48] [99509] accumulated_eval_time=3683.772355, accumulated_logging_time=3.865364, accumulated_submission_time=45410.033653, global_step=99509, preemption_count=0, score=45410.033653, test/accuracy=0.597100, test/loss=1.946147, test/num_examples=10000, total_duration=49102.785106, train/accuracy=0.789199, train/loss=1.056781, validation/accuracy=0.720080, validation/loss=1.341890, validation/num_examples=50000
I0201 02:35:28.953766 140022518892288 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.502549171447754, loss=3.253727436065674
I0201 02:36:14.901192 140023005427456 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.3034698963165283, loss=3.3467819690704346
I0201 02:37:00.861151 140022518892288 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.4151006937026978, loss=3.2840847969055176
I0201 02:37:47.286340 140023005427456 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.5709784030914307, loss=3.2163593769073486
I0201 02:38:33.612139 140022518892288 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.265074610710144, loss=4.173792362213135
I0201 02:39:19.844744 140023005427456 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.4100571870803833, loss=3.271360158920288
I0201 02:40:05.794719 140022518892288 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.2773730754852295, loss=3.241694927215576
I0201 02:40:51.778585 140023005427456 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.5707182884216309, loss=3.2267825603485107
I0201 02:41:37.965762 140022518892288 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.5422626733779907, loss=4.836712837219238
I0201 02:41:51.939888 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:42:02.457736 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:42:27.278722 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:42:28.877896 140184451094336 submission_runner.py:408] Time since start: 49560.09s, 	Step: 100432, 	{'train/accuracy': 0.7888476252555847, 'train/loss': 1.059237003326416, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.3572092056274414, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9448856115341187, 'test/num_examples': 10000, 'score': 45830.30660319328, 'total_duration': 49560.086330890656, 'accumulated_submission_time': 45830.30660319328, 'accumulated_eval_time': 3720.7103447914124, 'accumulated_logging_time': 3.9089343547821045}
I0201 02:42:28.909629 140023005427456 logging_writer.py:48] [100432] accumulated_eval_time=3720.710345, accumulated_logging_time=3.908934, accumulated_submission_time=45830.306603, global_step=100432, preemption_count=0, score=45830.306603, test/accuracy=0.599700, test/loss=1.944886, test/num_examples=10000, total_duration=49560.086331, train/accuracy=0.788848, train/loss=1.059237, validation/accuracy=0.718200, validation/loss=1.357209, validation/num_examples=50000
I0201 02:42:56.511943 140022518892288 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.3265624046325684, loss=3.6204137802124023
I0201 02:43:41.516862 140023005427456 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.3683162927627563, loss=3.3062853813171387
I0201 02:44:27.639302 140022518892288 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.4482306241989136, loss=3.2133171558380127
I0201 02:45:13.826278 140023005427456 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.3067480325698853, loss=3.227165460586548
I0201 02:45:59.639986 140022518892288 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3404844999313354, loss=4.069161891937256
I0201 02:46:45.670640 140023005427456 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.443841576576233, loss=3.214184284210205
I0201 02:47:31.903070 140022518892288 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.463460922241211, loss=3.2964296340942383
I0201 02:48:17.826032 140023005427456 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.4266436100006104, loss=3.2592670917510986
I0201 02:49:03.788658 140022518892288 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.5231108665466309, loss=3.2329800128936768
I0201 02:49:29.296615 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:49:40.179388 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:50:05.899114 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:50:07.519634 140184451094336 submission_runner.py:408] Time since start: 50018.73s, 	Step: 101357, 	{'train/accuracy': 0.8024413585662842, 'train/loss': 0.987043559551239, 'validation/accuracy': 0.7212799787521362, 'validation/loss': 1.3317164182662964, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.923500657081604, 'test/num_examples': 10000, 'score': 46250.637231349945, 'total_duration': 50018.72805285454, 'accumulated_submission_time': 46250.637231349945, 'accumulated_eval_time': 3758.9333407878876, 'accumulated_logging_time': 3.949855089187622}
I0201 02:50:07.558556 140023005427456 logging_writer.py:48] [101357] accumulated_eval_time=3758.933341, accumulated_logging_time=3.949855, accumulated_submission_time=46250.637231, global_step=101357, preemption_count=0, score=46250.637231, test/accuracy=0.597100, test/loss=1.923501, test/num_examples=10000, total_duration=50018.728053, train/accuracy=0.802441, train/loss=0.987044, validation/accuracy=0.721280, validation/loss=1.331716, validation/num_examples=50000
I0201 02:50:25.175625 140022518892288 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.456681728363037, loss=3.3074073791503906
I0201 02:51:08.939007 140023005427456 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.3949791193008423, loss=3.1426877975463867
I0201 02:51:55.168090 140022518892288 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.3862825632095337, loss=3.2789065837860107
I0201 02:52:41.430245 140023005427456 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.2964136600494385, loss=3.3506407737731934
I0201 02:53:27.672513 140022518892288 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.304011583328247, loss=3.857935905456543
I0201 02:54:13.810624 140023005427456 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.4777276515960693, loss=3.566828966140747
I0201 02:54:59.971927 140022518892288 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.3843342065811157, loss=3.9678919315338135
I0201 02:55:46.023253 140023005427456 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.4187248945236206, loss=3.273134708404541
I0201 02:56:32.072813 140022518892288 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.3571009635925293, loss=3.2780990600585938
I0201 02:57:07.883008 140184451094336 spec.py:321] Evaluating on the training split.
I0201 02:57:18.277313 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 02:57:45.156050 140184451094336 spec.py:349] Evaluating on the test split.
I0201 02:57:46.761632 140184451094336 submission_runner.py:408] Time since start: 50477.97s, 	Step: 102279, 	{'train/accuracy': 0.7923827767372131, 'train/loss': 1.034652590751648, 'validation/accuracy': 0.7217400074005127, 'validation/loss': 1.337119460105896, 'validation/num_examples': 50000, 'test/accuracy': 0.5967000126838684, 'test/loss': 1.9396754503250122, 'test/num_examples': 10000, 'score': 46670.904515028, 'total_duration': 50477.970071554184, 'accumulated_submission_time': 46670.904515028, 'accumulated_eval_time': 3797.81196808815, 'accumulated_logging_time': 3.9987268447875977}
I0201 02:57:46.796222 140023005427456 logging_writer.py:48] [102279] accumulated_eval_time=3797.811968, accumulated_logging_time=3.998727, accumulated_submission_time=46670.904515, global_step=102279, preemption_count=0, score=46670.904515, test/accuracy=0.596700, test/loss=1.939675, test/num_examples=10000, total_duration=50477.970072, train/accuracy=0.792383, train/loss=1.034653, validation/accuracy=0.721740, validation/loss=1.337119, validation/num_examples=50000
I0201 02:57:55.604300 140022518892288 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.2723932266235352, loss=3.426382303237915
I0201 02:58:38.364915 140023005427456 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.4206339120864868, loss=3.2149910926818848
I0201 02:59:24.636498 140022518892288 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.34483003616333, loss=3.084287166595459
I0201 03:00:10.987161 140023005427456 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.3744702339172363, loss=4.5543084144592285
I0201 03:00:56.798097 140022518892288 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.4090461730957031, loss=3.1545448303222656
I0201 03:01:43.380688 140023005427456 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.410520315170288, loss=3.2025952339172363
I0201 03:02:29.889045 140022518892288 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.4615377187728882, loss=3.214090347290039
I0201 03:03:15.999810 140023005427456 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.4683589935302734, loss=4.376424789428711
I0201 03:04:02.204225 140022518892288 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.4366916418075562, loss=3.219715118408203
I0201 03:04:47.033531 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:04:57.304233 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:05:19.880696 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:05:21.475622 140184451094336 submission_runner.py:408] Time since start: 50932.68s, 	Step: 103199, 	{'train/accuracy': 0.7949413657188416, 'train/loss': 1.0258285999298096, 'validation/accuracy': 0.7251799702644348, 'validation/loss': 1.329066514968872, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9212018251419067, 'test/num_examples': 10000, 'score': 47091.08183169365, 'total_duration': 50932.684057712555, 'accumulated_submission_time': 47091.08183169365, 'accumulated_eval_time': 3832.2540550231934, 'accumulated_logging_time': 4.04592490196228}
I0201 03:05:21.510170 140023005427456 logging_writer.py:48] [103199] accumulated_eval_time=3832.254055, accumulated_logging_time=4.045925, accumulated_submission_time=47091.081832, global_step=103199, preemption_count=0, score=47091.081832, test/accuracy=0.599700, test/loss=1.921202, test/num_examples=10000, total_duration=50932.684058, train/accuracy=0.794941, train/loss=1.025829, validation/accuracy=0.725180, validation/loss=1.329067, validation/num_examples=50000
I0201 03:05:22.316422 140022518892288 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.3600356578826904, loss=4.317960739135742
I0201 03:06:03.675743 140023005427456 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.5361113548278809, loss=3.1628074645996094
I0201 03:06:49.684871 140022518892288 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.572970986366272, loss=4.913235187530518
I0201 03:07:36.261967 140023005427456 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.3080763816833496, loss=3.6478781700134277
I0201 03:08:22.673905 140022518892288 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.3313887119293213, loss=3.1813900470733643
I0201 03:09:08.744278 140023005427456 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.3859047889709473, loss=3.1633102893829346
I0201 03:09:54.915531 140022518892288 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.3401265144348145, loss=4.376898288726807
I0201 03:10:41.097360 140023005427456 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.3947876691818237, loss=3.1953492164611816
I0201 03:11:27.484932 140022518892288 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.3527448177337646, loss=3.1887099742889404
I0201 03:12:13.929281 140023005427456 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.3394440412521362, loss=3.2218875885009766
I0201 03:12:21.911368 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:12:32.292546 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:12:53.533430 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:12:55.132335 140184451094336 submission_runner.py:408] Time since start: 51386.34s, 	Step: 104119, 	{'train/accuracy': 0.8043944835662842, 'train/loss': 0.9767816662788391, 'validation/accuracy': 0.7267799973487854, 'validation/loss': 1.3111376762390137, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.8960027694702148, 'test/num_examples': 10000, 'score': 47511.4247674942, 'total_duration': 51386.340764284134, 'accumulated_submission_time': 47511.4247674942, 'accumulated_eval_time': 3865.4750039577484, 'accumulated_logging_time': 4.091261148452759}
I0201 03:12:55.167162 140022518892288 logging_writer.py:48] [104119] accumulated_eval_time=3865.475004, accumulated_logging_time=4.091261, accumulated_submission_time=47511.424767, global_step=104119, preemption_count=0, score=47511.424767, test/accuracy=0.600700, test/loss=1.896003, test/num_examples=10000, total_duration=51386.340764, train/accuracy=0.804394, train/loss=0.976782, validation/accuracy=0.726780, validation/loss=1.311138, validation/num_examples=50000
I0201 03:13:28.041532 140023005427456 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.483444333076477, loss=3.2224173545837402
I0201 03:14:13.890323 140022518892288 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.4575111865997314, loss=3.2022910118103027
I0201 03:14:59.950483 140023005427456 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.3608602285385132, loss=3.125436305999756
I0201 03:15:46.149003 140022518892288 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.4377726316452026, loss=3.160120725631714
I0201 03:16:32.093464 140023005427456 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.361436367034912, loss=3.135850191116333
I0201 03:17:23.186144 140022518892288 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.4198073148727417, loss=3.17976975440979
I0201 03:18:10.733815 140023005427456 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.4240647554397583, loss=4.7762322425842285
I0201 03:18:57.788627 140022518892288 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.4562737941741943, loss=3.2416038513183594
I0201 03:19:44.008593 140023005427456 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.5909448862075806, loss=4.851204872131348
I0201 03:19:55.249018 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:20:06.670778 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:20:30.442384 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:20:32.047692 140184451094336 submission_runner.py:408] Time since start: 51843.26s, 	Step: 105026, 	{'train/accuracy': 0.8005077838897705, 'train/loss': 0.9992862343788147, 'validation/accuracy': 0.7254199981689453, 'validation/loss': 1.3189843893051147, 'validation/num_examples': 50000, 'test/accuracy': 0.6068000197410583, 'test/loss': 1.9124714136123657, 'test/num_examples': 10000, 'score': 47931.448093891144, 'total_duration': 51843.25613093376, 'accumulated_submission_time': 47931.448093891144, 'accumulated_eval_time': 3902.273670196533, 'accumulated_logging_time': 4.136998176574707}
I0201 03:20:32.083232 140022518892288 logging_writer.py:48] [105026] accumulated_eval_time=3902.273670, accumulated_logging_time=4.136998, accumulated_submission_time=47931.448094, global_step=105026, preemption_count=0, score=47931.448094, test/accuracy=0.606800, test/loss=1.912471, test/num_examples=10000, total_duration=51843.256131, train/accuracy=0.800508, train/loss=0.999286, validation/accuracy=0.725420, validation/loss=1.318984, validation/num_examples=50000
I0201 03:21:02.082147 140023005427456 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.406570553779602, loss=3.1753008365631104
I0201 03:21:47.767530 140022518892288 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.4722471237182617, loss=3.2027881145477295
I0201 03:22:33.750414 140023005427456 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.3954411745071411, loss=3.4225363731384277
I0201 03:23:19.828335 140022518892288 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.332927942276001, loss=3.8650529384613037
I0201 03:24:05.952186 140023005427456 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.3741817474365234, loss=3.2435481548309326
I0201 03:24:52.069855 140022518892288 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.3998470306396484, loss=3.5058445930480957
I0201 03:25:38.035806 140023005427456 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.3862413167953491, loss=3.1657190322875977
I0201 03:26:23.995438 140022518892288 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.3500144481658936, loss=4.1643290519714355
I0201 03:27:10.162900 140023005427456 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.4080610275268555, loss=3.5561814308166504
I0201 03:27:32.300768 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:27:42.605782 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:28:06.947688 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:28:08.545850 140184451094336 submission_runner.py:408] Time since start: 52299.75s, 	Step: 105950, 	{'train/accuracy': 0.7973827719688416, 'train/loss': 0.9926227927207947, 'validation/accuracy': 0.7240599989891052, 'validation/loss': 1.3056074380874634, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.899614930152893, 'test/num_examples': 10000, 'score': 48351.60951066017, 'total_duration': 52299.75429058075, 'accumulated_submission_time': 48351.60951066017, 'accumulated_eval_time': 3938.51873588562, 'accumulated_logging_time': 4.1818811893463135}
I0201 03:28:08.576991 140022518892288 logging_writer.py:48] [105950] accumulated_eval_time=3938.518736, accumulated_logging_time=4.181881, accumulated_submission_time=48351.609511, global_step=105950, preemption_count=0, score=48351.609511, test/accuracy=0.603400, test/loss=1.899615, test/num_examples=10000, total_duration=52299.754291, train/accuracy=0.797383, train/loss=0.992623, validation/accuracy=0.724060, validation/loss=1.305607, validation/num_examples=50000
I0201 03:28:28.990471 140023005427456 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.5934540033340454, loss=3.2113564014434814
I0201 03:29:13.353893 140022518892288 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.5552676916122437, loss=3.1911418437957764
I0201 03:29:59.521038 140023005427456 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.5382581949234009, loss=4.723667144775391
I0201 03:30:45.996624 140022518892288 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.3189810514450073, loss=3.1842288970947266
I0201 03:31:32.203296 140023005427456 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.5595736503601074, loss=3.2699427604675293
I0201 03:32:18.203857 140022518892288 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.4933871030807495, loss=3.298752546310425
I0201 03:33:04.365862 140023005427456 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.3906975984573364, loss=4.46318244934082
I0201 03:33:50.308327 140022518892288 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.5299365520477295, loss=4.711010456085205
I0201 03:34:36.882288 140023005427456 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.3083184957504272, loss=3.3348774909973145
I0201 03:35:08.900060 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:35:19.372659 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:35:41.235681 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:35:42.833385 140184451094336 submission_runner.py:408] Time since start: 52754.04s, 	Step: 106871, 	{'train/accuracy': 0.8055273294448853, 'train/loss': 0.9919548630714417, 'validation/accuracy': 0.7270799875259399, 'validation/loss': 1.3199224472045898, 'validation/num_examples': 50000, 'test/accuracy': 0.6069000363349915, 'test/loss': 1.9079723358154297, 'test/num_examples': 10000, 'score': 48771.873259305954, 'total_duration': 52754.04180908203, 'accumulated_submission_time': 48771.873259305954, 'accumulated_eval_time': 3972.452043533325, 'accumulated_logging_time': 4.225016117095947}
I0201 03:35:42.868218 140022518892288 logging_writer.py:48] [106871] accumulated_eval_time=3972.452044, accumulated_logging_time=4.225016, accumulated_submission_time=48771.873259, global_step=106871, preemption_count=0, score=48771.873259, test/accuracy=0.606900, test/loss=1.907972, test/num_examples=10000, total_duration=52754.041809, train/accuracy=0.805527, train/loss=0.991955, validation/accuracy=0.727080, validation/loss=1.319922, validation/num_examples=50000
I0201 03:35:54.884792 140023005427456 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.533434510231018, loss=4.115335464477539
I0201 03:36:37.908633 140022518892288 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.5074037313461304, loss=3.115943193435669
I0201 03:37:24.145379 140023005427456 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.4561257362365723, loss=3.2344465255737305
I0201 03:38:10.559358 140022518892288 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.4091603755950928, loss=3.1792855262756348
I0201 03:38:56.510069 140023005427456 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.4619355201721191, loss=4.325263500213623
I0201 03:39:42.611158 140022518892288 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.4085179567337036, loss=3.1790077686309814
I0201 03:40:28.760236 140023005427456 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4138239622116089, loss=4.319354057312012
I0201 03:41:15.060444 140022518892288 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.609076976776123, loss=4.767916679382324
I0201 03:42:01.169913 140023005427456 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.411134958267212, loss=3.361621856689453
I0201 03:42:42.860671 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:42:53.207052 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:43:17.054615 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:43:18.659838 140184451094336 submission_runner.py:408] Time since start: 53209.87s, 	Step: 107792, 	{'train/accuracy': 0.8181054592132568, 'train/loss': 0.9642824530601501, 'validation/accuracy': 0.7266599535942078, 'validation/loss': 1.341126561164856, 'validation/num_examples': 50000, 'test/accuracy': 0.6049000024795532, 'test/loss': 1.9311014413833618, 'test/num_examples': 10000, 'score': 49191.80590748787, 'total_duration': 53209.868277311325, 'accumulated_submission_time': 49191.80590748787, 'accumulated_eval_time': 4008.251214504242, 'accumulated_logging_time': 4.2725653648376465}
I0201 03:43:18.691966 140022518892288 logging_writer.py:48] [107792] accumulated_eval_time=4008.251215, accumulated_logging_time=4.272565, accumulated_submission_time=49191.805907, global_step=107792, preemption_count=0, score=49191.805907, test/accuracy=0.604900, test/loss=1.931101, test/num_examples=10000, total_duration=53209.868277, train/accuracy=0.818105, train/loss=0.964282, validation/accuracy=0.726660, validation/loss=1.341127, validation/num_examples=50000
I0201 03:43:22.300210 140023005427456 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.4987765550613403, loss=3.1592493057250977
I0201 03:44:03.992193 140022518892288 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.4970474243164062, loss=3.1592931747436523
I0201 03:44:50.141834 140023005427456 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.4957016706466675, loss=3.184516668319702
I0201 03:45:36.519628 140022518892288 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.4781848192214966, loss=3.947542190551758
I0201 03:46:22.533913 140023005427456 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.6660231351852417, loss=4.947967529296875
I0201 03:47:08.443291 140022518892288 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.42613685131073, loss=3.334923267364502
I0201 03:47:54.589279 140023005427456 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.5955736637115479, loss=4.575923919677734
I0201 03:48:40.599623 140022518892288 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.4679995775222778, loss=3.9910855293273926
I0201 03:49:26.765527 140023005427456 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.4003419876098633, loss=3.1160011291503906
I0201 03:50:13.101269 140022518892288 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.5563480854034424, loss=4.844203472137451
I0201 03:50:18.693053 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:50:29.133660 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:50:51.573390 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:50:53.180775 140184451094336 submission_runner.py:408] Time since start: 53664.39s, 	Step: 108714, 	{'train/accuracy': 0.8000195026397705, 'train/loss': 0.9913946390151978, 'validation/accuracy': 0.7280600070953369, 'validation/loss': 1.2932459115982056, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.8781938552856445, 'test/num_examples': 10000, 'score': 49611.74863290787, 'total_duration': 53664.38921499252, 'accumulated_submission_time': 49611.74863290787, 'accumulated_eval_time': 4042.7389369010925, 'accumulated_logging_time': 4.315826892852783}
I0201 03:50:53.213264 140023005427456 logging_writer.py:48] [108714] accumulated_eval_time=4042.738937, accumulated_logging_time=4.315827, accumulated_submission_time=49611.748633, global_step=108714, preemption_count=0, score=49611.748633, test/accuracy=0.609700, test/loss=1.878194, test/num_examples=10000, total_duration=53664.389215, train/accuracy=0.800020, train/loss=0.991395, validation/accuracy=0.728060, validation/loss=1.293246, validation/num_examples=50000
I0201 03:51:28.341043 140022518892288 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.5763754844665527, loss=3.1535918712615967
I0201 03:52:14.720733 140023005427456 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.4899438619613647, loss=3.3485381603240967
I0201 03:53:00.892952 140022518892288 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.5046861171722412, loss=3.5527584552764893
I0201 03:53:47.274254 140023005427456 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.463197112083435, loss=3.1293082237243652
I0201 03:54:33.288795 140022518892288 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.4105786085128784, loss=3.314964532852173
I0201 03:55:19.430204 140023005427456 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.468787670135498, loss=3.1478099822998047
I0201 03:56:05.699473 140022518892288 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.4000312089920044, loss=3.274562120437622
I0201 03:56:51.722531 140023005427456 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.5439136028289795, loss=3.43792986869812
I0201 03:57:37.957830 140022518892288 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.331087589263916, loss=3.446364164352417
I0201 03:57:53.396028 140184451094336 spec.py:321] Evaluating on the training split.
I0201 03:58:03.843411 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 03:58:29.009883 140184451094336 spec.py:349] Evaluating on the test split.
I0201 03:58:30.615929 140184451094336 submission_runner.py:408] Time since start: 54121.82s, 	Step: 109635, 	{'train/accuracy': 0.8060351610183716, 'train/loss': 0.9905210733413696, 'validation/accuracy': 0.7256999611854553, 'validation/loss': 1.3248872756958008, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.9160008430480957, 'test/num_examples': 10000, 'score': 50031.87528467178, 'total_duration': 54121.824350357056, 'accumulated_submission_time': 50031.87528467178, 'accumulated_eval_time': 4079.9588174819946, 'accumulated_logging_time': 4.35719108581543}
I0201 03:58:30.653386 140023005427456 logging_writer.py:48] [109635] accumulated_eval_time=4079.958817, accumulated_logging_time=4.357191, accumulated_submission_time=50031.875285, global_step=109635, preemption_count=0, score=50031.875285, test/accuracy=0.609500, test/loss=1.916001, test/num_examples=10000, total_duration=54121.824350, train/accuracy=0.806035, train/loss=0.990521, validation/accuracy=0.725700, validation/loss=1.324887, validation/num_examples=50000
I0201 03:58:57.075174 140022518892288 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.5102641582489014, loss=4.123416423797607
I0201 03:59:42.343083 140023005427456 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.743530511856079, loss=4.861759185791016
I0201 04:00:28.543754 140022518892288 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.5464001893997192, loss=3.107666492462158
I0201 04:01:14.963437 140023005427456 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.579390048980713, loss=3.145313024520874
I0201 04:02:01.002166 140022518892288 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.558590292930603, loss=3.2064132690429688
I0201 04:02:47.232419 140023005427456 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.8184592723846436, loss=4.80618143081665
I0201 04:03:33.139127 140022518892288 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.511492133140564, loss=3.272531509399414
I0201 04:04:19.153704 140023005427456 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.4605116844177246, loss=3.3627779483795166
I0201 04:05:05.436019 140022518892288 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.3236585855484009, loss=3.7341344356536865
I0201 04:05:30.813286 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:05:41.366338 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:06:05.201371 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:06:06.801439 140184451094336 submission_runner.py:408] Time since start: 54578.01s, 	Step: 110557, 	{'train/accuracy': 0.8092968463897705, 'train/loss': 0.9955326914787292, 'validation/accuracy': 0.7274199724197388, 'validation/loss': 1.3348983526229858, 'validation/num_examples': 50000, 'test/accuracy': 0.6116000413894653, 'test/loss': 1.9224961996078491, 'test/num_examples': 10000, 'score': 50451.978063583374, 'total_duration': 54578.00987672806, 'accumulated_submission_time': 50451.978063583374, 'accumulated_eval_time': 4115.946965456009, 'accumulated_logging_time': 4.4040772914886475}
I0201 04:06:06.836918 140023005427456 logging_writer.py:48] [110557] accumulated_eval_time=4115.946965, accumulated_logging_time=4.404077, accumulated_submission_time=50451.978064, global_step=110557, preemption_count=0, score=50451.978064, test/accuracy=0.611600, test/loss=1.922496, test/num_examples=10000, total_duration=54578.009877, train/accuracy=0.809297, train/loss=0.995533, validation/accuracy=0.727420, validation/loss=1.334898, validation/num_examples=50000
I0201 04:06:24.456012 140022518892288 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.592827558517456, loss=4.483580112457275
I0201 04:07:08.492257 140023005427456 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.4812601804733276, loss=3.16267466545105
I0201 04:07:54.688276 140022518892288 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.4235388040542603, loss=3.22774338722229
I0201 04:08:41.156615 140023005427456 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.3599340915679932, loss=3.528958320617676
I0201 04:09:27.560567 140022518892288 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.3889440298080444, loss=3.3315892219543457
I0201 04:10:13.967085 140023005427456 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.4398789405822754, loss=3.1075782775878906
I0201 04:11:00.309725 140022518892288 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.462619423866272, loss=3.1903839111328125
I0201 04:11:46.584358 140023005427456 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.4814212322235107, loss=3.4061079025268555
I0201 04:12:32.961536 140022518892288 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.423751711845398, loss=3.4857866764068604
I0201 04:13:06.873833 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:13:17.456230 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:13:39.584530 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:13:41.191912 140184451094336 submission_runner.py:408] Time since start: 55032.40s, 	Step: 111475, 	{'train/accuracy': 0.8045703172683716, 'train/loss': 0.9676870703697205, 'validation/accuracy': 0.7338599562644958, 'validation/loss': 1.2687467336654663, 'validation/num_examples': 50000, 'test/accuracy': 0.6123000383377075, 'test/loss': 1.863784909248352, 'test/num_examples': 10000, 'score': 50871.95857954025, 'total_duration': 55032.40033340454, 'accumulated_submission_time': 50871.95857954025, 'accumulated_eval_time': 4150.26503443718, 'accumulated_logging_time': 4.449316024780273}
I0201 04:13:41.231384 140023005427456 logging_writer.py:48] [111475] accumulated_eval_time=4150.265034, accumulated_logging_time=4.449316, accumulated_submission_time=50871.958580, global_step=111475, preemption_count=0, score=50871.958580, test/accuracy=0.612300, test/loss=1.863785, test/num_examples=10000, total_duration=55032.400333, train/accuracy=0.804570, train/loss=0.967687, validation/accuracy=0.733860, validation/loss=1.268747, validation/num_examples=50000
I0201 04:13:51.637953 140022518892288 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.4869639873504639, loss=3.14713978767395
I0201 04:14:33.931570 140023005427456 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.6194844245910645, loss=3.168372392654419
I0201 04:15:20.251661 140022518892288 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.4846001863479614, loss=3.086451768875122
I0201 04:16:06.748784 140023005427456 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.708674430847168, loss=4.831699371337891
I0201 04:16:52.943355 140022518892288 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.3591645956039429, loss=3.6558992862701416
I0201 04:17:39.091808 140023005427456 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.640433669090271, loss=4.664268493652344
I0201 04:18:25.509109 140022518892288 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.5791915655136108, loss=3.137570858001709
I0201 04:19:11.656938 140023005427456 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.6166439056396484, loss=3.162524461746216
I0201 04:19:57.487779 140022518892288 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.434235692024231, loss=3.101661205291748
I0201 04:20:41.559921 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:20:52.001757 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:21:15.671000 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:21:17.271638 140184451094336 submission_runner.py:408] Time since start: 55488.48s, 	Step: 112397, 	{'train/accuracy': 0.8095507621765137, 'train/loss': 0.9677566885948181, 'validation/accuracy': 0.7329199910163879, 'validation/loss': 1.2998954057693481, 'validation/num_examples': 50000, 'test/accuracy': 0.6091000437736511, 'test/loss': 1.8859690427780151, 'test/num_examples': 10000, 'score': 51292.22880363464, 'total_duration': 55488.480078697205, 'accumulated_submission_time': 51292.22880363464, 'accumulated_eval_time': 4185.976754188538, 'accumulated_logging_time': 4.499913215637207}
I0201 04:21:17.307309 140023005427456 logging_writer.py:48] [112397] accumulated_eval_time=4185.976754, accumulated_logging_time=4.499913, accumulated_submission_time=51292.228804, global_step=112397, preemption_count=0, score=51292.228804, test/accuracy=0.609100, test/loss=1.885969, test/num_examples=10000, total_duration=55488.480079, train/accuracy=0.809551, train/loss=0.967757, validation/accuracy=0.732920, validation/loss=1.299895, validation/num_examples=50000
I0201 04:21:18.912423 140022518892288 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.4504942893981934, loss=3.170253276824951
I0201 04:22:00.540668 140023005427456 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.6130560636520386, loss=3.1331660747528076
I0201 04:22:46.939480 140022518892288 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.3531818389892578, loss=3.412916898727417
I0201 04:23:33.245204 140023005427456 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.574529767036438, loss=3.2014715671539307
I0201 04:24:19.492434 140022518892288 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.4623570442199707, loss=3.054896354675293
I0201 04:25:05.787565 140023005427456 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.5524075031280518, loss=3.2546441555023193
I0201 04:25:51.964246 140022518892288 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.4991464614868164, loss=3.1218321323394775
I0201 04:26:38.182617 140023005427456 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.2934997081756592, loss=3.6552014350891113
I0201 04:27:24.163214 140022518892288 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.538644552230835, loss=3.128147602081299
I0201 04:28:10.123861 140023005427456 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.5131733417510986, loss=4.2463059425354
I0201 04:28:17.303711 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:28:27.709100 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:28:50.371672 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:28:51.981930 140184451094336 submission_runner.py:408] Time since start: 55943.19s, 	Step: 113317, 	{'train/accuracy': 0.8145117163658142, 'train/loss': 0.9527512192726135, 'validation/accuracy': 0.7319999933242798, 'validation/loss': 1.2896184921264648, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.8763359785079956, 'test/num_examples': 10000, 'score': 51712.16885471344, 'total_duration': 55943.190348148346, 'accumulated_submission_time': 51712.16885471344, 'accumulated_eval_time': 4220.654976129532, 'accumulated_logging_time': 4.545567512512207}
I0201 04:28:52.025616 140022518892288 logging_writer.py:48] [113317] accumulated_eval_time=4220.654976, accumulated_logging_time=4.545568, accumulated_submission_time=51712.168855, global_step=113317, preemption_count=0, score=51712.168855, test/accuracy=0.615600, test/loss=1.876336, test/num_examples=10000, total_duration=55943.190348, train/accuracy=0.814512, train/loss=0.952751, validation/accuracy=0.732000, validation/loss=1.289618, validation/num_examples=50000
I0201 04:29:25.807369 140023005427456 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.4414892196655273, loss=4.029597282409668
I0201 04:30:11.863341 140022518892288 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.5122400522232056, loss=3.6548750400543213
I0201 04:30:57.775284 140023005427456 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.5195411443710327, loss=3.1200315952301025
I0201 04:31:44.342038 140022518892288 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.517446756362915, loss=3.192390203475952
I0201 04:32:30.504879 140023005427456 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.3731619119644165, loss=3.5070998668670654
I0201 04:33:16.722585 140022518892288 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.3687502145767212, loss=4.135687828063965
I0201 04:34:02.841431 140023005427456 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.6407034397125244, loss=3.2919301986694336
I0201 04:34:48.686343 140022518892288 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.5017001628875732, loss=3.1835267543792725
I0201 04:35:34.785584 140023005427456 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.6021219491958618, loss=3.1337244510650635
I0201 04:35:52.468679 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:36:03.241479 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:36:27.684830 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:36:29.286877 140184451094336 submission_runner.py:408] Time since start: 56400.50s, 	Step: 114240, 	{'train/accuracy': 0.8056835532188416, 'train/loss': 0.9622820615768433, 'validation/accuracy': 0.7326399683952332, 'validation/loss': 1.2778819799423218, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.8607583045959473, 'test/num_examples': 10000, 'score': 52132.5546040535, 'total_duration': 56400.49531674385, 'accumulated_submission_time': 52132.5546040535, 'accumulated_eval_time': 4257.473162174225, 'accumulated_logging_time': 4.599227428436279}
I0201 04:36:29.323014 140022518892288 logging_writer.py:48] [114240] accumulated_eval_time=4257.473162, accumulated_logging_time=4.599227, accumulated_submission_time=52132.554604, global_step=114240, preemption_count=0, score=52132.554604, test/accuracy=0.613200, test/loss=1.860758, test/num_examples=10000, total_duration=56400.495317, train/accuracy=0.805684, train/loss=0.962282, validation/accuracy=0.732640, validation/loss=1.277882, validation/num_examples=50000
I0201 04:36:53.724748 140023005427456 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.5670535564422607, loss=3.1064887046813965
I0201 04:37:38.368426 140022518892288 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.5224475860595703, loss=3.942744731903076
I0201 04:38:24.627799 140023005427456 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.4249850511550903, loss=3.7440881729125977
I0201 04:39:11.313618 140022518892288 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.536917805671692, loss=3.125391721725464
I0201 04:39:57.193653 140023005427456 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.412516474723816, loss=3.113145351409912
I0201 04:40:43.352618 140022518892288 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.4838064908981323, loss=3.124108076095581
I0201 04:41:29.586864 140023005427456 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.4602879285812378, loss=3.1064226627349854
I0201 04:42:15.644288 140022518892288 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.4965769052505493, loss=3.189976215362549
I0201 04:43:01.921305 140023005427456 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.5475082397460938, loss=3.159926414489746
I0201 04:43:29.398796 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:43:39.763413 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:44:02.666138 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:44:04.263534 140184451094336 submission_runner.py:408] Time since start: 56855.47s, 	Step: 115161, 	{'train/accuracy': 0.8109765648841858, 'train/loss': 0.9565854668617249, 'validation/accuracy': 0.7332599759101868, 'validation/loss': 1.2811367511749268, 'validation/num_examples': 50000, 'test/accuracy': 0.6152000427246094, 'test/loss': 1.8726716041564941, 'test/num_examples': 10000, 'score': 52552.574162483215, 'total_duration': 56855.47197389603, 'accumulated_submission_time': 52552.574162483215, 'accumulated_eval_time': 4292.337894201279, 'accumulated_logging_time': 4.644402265548706}
I0201 04:44:04.296079 140022518892288 logging_writer.py:48] [115161] accumulated_eval_time=4292.337894, accumulated_logging_time=4.644402, accumulated_submission_time=52552.574162, global_step=115161, preemption_count=0, score=52552.574162, test/accuracy=0.615200, test/loss=1.872672, test/num_examples=10000, total_duration=56855.471974, train/accuracy=0.810977, train/loss=0.956585, validation/accuracy=0.733260, validation/loss=1.281137, validation/num_examples=50000
I0201 04:44:20.298082 140023005427456 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.5545843839645386, loss=3.1478991508483887
I0201 04:45:03.676667 140022518892288 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.545477271080017, loss=3.2234256267547607
I0201 04:45:49.936218 140023005427456 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.6598994731903076, loss=3.0747971534729004
I0201 04:46:36.428243 140022518892288 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.536192536354065, loss=4.373494625091553
I0201 04:47:22.544073 140023005427456 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.5563462972640991, loss=3.1312272548675537
I0201 04:48:08.840852 140022518892288 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.5484541654586792, loss=3.474884033203125
I0201 04:48:55.057605 140023005427456 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.61704421043396, loss=3.0928750038146973
I0201 04:49:41.067883 140022518892288 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.445671796798706, loss=3.358631134033203
I0201 04:50:27.328907 140023005427456 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.5601551532745361, loss=4.328184127807617
I0201 04:51:04.563535 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:51:14.946845 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:51:37.725757 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:51:39.344897 140184451094336 submission_runner.py:408] Time since start: 57310.55s, 	Step: 116082, 	{'train/accuracy': 0.8161913752555847, 'train/loss': 0.9610245823860168, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.299484133720398, 'validation/num_examples': 50000, 'test/accuracy': 0.6181000471115112, 'test/loss': 1.8877934217453003, 'test/num_examples': 10000, 'score': 52972.78386044502, 'total_duration': 57310.55331468582, 'accumulated_submission_time': 52972.78386044502, 'accumulated_eval_time': 4327.119237422943, 'accumulated_logging_time': 4.6875598430633545}
I0201 04:51:39.385272 140022518892288 logging_writer.py:48] [116082] accumulated_eval_time=4327.119237, accumulated_logging_time=4.687560, accumulated_submission_time=52972.783860, global_step=116082, preemption_count=0, score=52972.783860, test/accuracy=0.618100, test/loss=1.887793, test/num_examples=10000, total_duration=57310.553315, train/accuracy=0.816191, train/loss=0.961025, validation/accuracy=0.735840, validation/loss=1.299484, validation/num_examples=50000
I0201 04:51:46.999810 140023005427456 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.6279455423355103, loss=3.292084217071533
I0201 04:52:29.423988 140022518892288 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.6069694757461548, loss=3.958735466003418
I0201 04:53:15.526163 140023005427456 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.5429962873458862, loss=3.494109630584717
I0201 04:54:01.759415 140022518892288 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.6851495504379272, loss=4.502145290374756
I0201 04:54:47.817000 140023005427456 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.455928087234497, loss=3.1674418449401855
I0201 04:55:34.054152 140022518892288 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.618004322052002, loss=3.23359751701355
I0201 04:56:19.884705 140023005427456 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.4720168113708496, loss=3.9024658203125
I0201 04:57:05.903800 140022518892288 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.6051071882247925, loss=3.109074354171753
I0201 04:57:51.949774 140023005427456 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.666712760925293, loss=4.428534984588623
I0201 04:58:38.099362 140022518892288 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.573610782623291, loss=3.130854368209839
I0201 04:58:39.653741 140184451094336 spec.py:321] Evaluating on the training split.
I0201 04:58:49.937646 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 04:59:16.240621 140184451094336 spec.py:349] Evaluating on the test split.
I0201 04:59:17.835977 140184451094336 submission_runner.py:408] Time since start: 57769.04s, 	Step: 117005, 	{'train/accuracy': 0.8302538990974426, 'train/loss': 0.8830304741859436, 'validation/accuracy': 0.7373999953269958, 'validation/loss': 1.2599748373031616, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8561517000198364, 'test/num_examples': 10000, 'score': 53392.99377536774, 'total_duration': 57769.04441308975, 'accumulated_submission_time': 53392.99377536774, 'accumulated_eval_time': 4365.301455259323, 'accumulated_logging_time': 4.738725185394287}
I0201 04:59:17.869254 140023005427456 logging_writer.py:48] [117005] accumulated_eval_time=4365.301455, accumulated_logging_time=4.738725, accumulated_submission_time=53392.993775, global_step=117005, preemption_count=0, score=53392.993775, test/accuracy=0.613800, test/loss=1.856152, test/num_examples=10000, total_duration=57769.044413, train/accuracy=0.830254, train/loss=0.883030, validation/accuracy=0.737400, validation/loss=1.259975, validation/num_examples=50000
I0201 04:59:57.515218 140022518892288 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.6936721801757812, loss=4.069247722625732
I0201 05:00:43.421680 140023005427456 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.5542595386505127, loss=3.1823787689208984
I0201 05:01:29.776295 140022518892288 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.6700575351715088, loss=3.117166757583618
I0201 05:02:16.277683 140023005427456 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.548895001411438, loss=3.0955052375793457
I0201 05:03:02.476847 140022518892288 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.4636961221694946, loss=3.0380358695983887
I0201 05:03:48.983828 140023005427456 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.534139633178711, loss=3.0561940670013428
I0201 05:04:35.416930 140022518892288 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.5105156898498535, loss=3.7006430625915527
I0201 05:05:21.672505 140023005427456 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.749211311340332, loss=4.5769453048706055
I0201 05:06:08.049047 140022518892288 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.6143790483474731, loss=4.098424911499023
I0201 05:06:18.239311 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:06:28.449596 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:06:57.135846 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:06:58.724300 140184451094336 submission_runner.py:408] Time since start: 58229.93s, 	Step: 117924, 	{'train/accuracy': 0.8176171779632568, 'train/loss': 0.9192262291908264, 'validation/accuracy': 0.7388799786567688, 'validation/loss': 1.245247483253479, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.8357372283935547, 'test/num_examples': 10000, 'score': 53813.30598425865, 'total_duration': 58229.93274021149, 'accumulated_submission_time': 53813.30598425865, 'accumulated_eval_time': 4405.786436319351, 'accumulated_logging_time': 4.782990217208862}
I0201 05:06:58.761581 140023005427456 logging_writer.py:48] [117924] accumulated_eval_time=4405.786436, accumulated_logging_time=4.782990, accumulated_submission_time=53813.305984, global_step=117924, preemption_count=0, score=53813.305984, test/accuracy=0.621700, test/loss=1.835737, test/num_examples=10000, total_duration=58229.932740, train/accuracy=0.817617, train/loss=0.919226, validation/accuracy=0.738880, validation/loss=1.245247, validation/num_examples=50000
I0201 05:07:29.766586 140022518892288 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.6007132530212402, loss=3.9881224632263184
I0201 05:08:15.779207 140023005427456 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.721039056777954, loss=4.70904541015625
I0201 05:09:01.676831 140022518892288 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.7382171154022217, loss=4.711663246154785
I0201 05:09:47.889690 140023005427456 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.707373023033142, loss=3.2103986740112305
I0201 05:10:33.660910 140022518892288 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.6640303134918213, loss=4.354334831237793
I0201 05:11:19.722135 140023005427456 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.5781612396240234, loss=4.224739074707031
I0201 05:12:05.944946 140022518892288 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.5506134033203125, loss=3.71101713180542
I0201 05:12:51.774031 140023005427456 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.5349323749542236, loss=3.854283332824707
I0201 05:13:37.988489 140022518892288 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.6596007347106934, loss=3.2118477821350098
I0201 05:13:59.024660 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:14:09.479465 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:14:32.445749 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:14:34.053320 140184451094336 submission_runner.py:408] Time since start: 58685.26s, 	Step: 118847, 	{'train/accuracy': 0.8188671469688416, 'train/loss': 0.9099279642105103, 'validation/accuracy': 0.7407199740409851, 'validation/loss': 1.2410085201263428, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.840397596359253, 'test/num_examples': 10000, 'score': 54233.51160264015, 'total_duration': 58685.261761426926, 'accumulated_submission_time': 54233.51160264015, 'accumulated_eval_time': 4440.815090417862, 'accumulated_logging_time': 4.830764055252075}
I0201 05:14:34.089502 140023005427456 logging_writer.py:48] [118847] accumulated_eval_time=4440.815090, accumulated_logging_time=4.830764, accumulated_submission_time=54233.511603, global_step=118847, preemption_count=0, score=54233.511603, test/accuracy=0.616200, test/loss=1.840398, test/num_examples=10000, total_duration=58685.261761, train/accuracy=0.818867, train/loss=0.909928, validation/accuracy=0.740720, validation/loss=1.241009, validation/num_examples=50000
I0201 05:14:55.696758 140022518892288 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.522500991821289, loss=4.093520164489746
I0201 05:15:39.833521 140023005427456 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.7431522607803345, loss=4.540595054626465
I0201 05:16:26.020301 140022518892288 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.686165690422058, loss=3.079550266265869
I0201 05:17:12.258369 140023005427456 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.6196547746658325, loss=3.0614683628082275
I0201 05:17:58.190682 140022518892288 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.7223409414291382, loss=4.411455154418945
I0201 05:18:44.163739 140023005427456 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.5738277435302734, loss=3.1027605533599854
I0201 05:19:30.215253 140022518892288 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.5759434700012207, loss=3.1074862480163574
I0201 05:20:16.164451 140023005427456 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.5754457712173462, loss=3.326815128326416
I0201 05:21:01.835493 140022518892288 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.6231926679611206, loss=3.3894667625427246
I0201 05:21:34.341138 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:21:44.805994 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:22:09.187250 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:22:10.785850 140184451094336 submission_runner.py:408] Time since start: 59141.99s, 	Step: 119772, 	{'train/accuracy': 0.8258007764816284, 'train/loss': 0.8815270662307739, 'validation/accuracy': 0.7394799590110779, 'validation/loss': 1.2439427375793457, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.8456861972808838, 'test/num_examples': 10000, 'score': 54653.70579338074, 'total_duration': 59141.99428868294, 'accumulated_submission_time': 54653.70579338074, 'accumulated_eval_time': 4477.259788036346, 'accumulated_logging_time': 4.876175165176392}
I0201 05:22:10.819455 140023005427456 logging_writer.py:48] [119772] accumulated_eval_time=4477.259788, accumulated_logging_time=4.876175, accumulated_submission_time=54653.705793, global_step=119772, preemption_count=0, score=54653.705793, test/accuracy=0.614900, test/loss=1.845686, test/num_examples=10000, total_duration=59141.994289, train/accuracy=0.825801, train/loss=0.881527, validation/accuracy=0.739480, validation/loss=1.243943, validation/num_examples=50000
I0201 05:22:22.423979 140022518892288 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.7267767190933228, loss=4.47235631942749
I0201 05:23:05.610832 140023005427456 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.8258137702941895, loss=4.693211078643799
I0201 05:23:51.452234 140022518892288 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.5296343564987183, loss=3.226273775100708
I0201 05:24:37.708522 140023005427456 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.736376404762268, loss=4.5888519287109375
I0201 05:25:23.647392 140022518892288 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.5450619459152222, loss=3.2314183712005615
I0201 05:26:09.609434 140023005427456 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.6077014207839966, loss=4.215633869171143
I0201 05:26:55.875438 140022518892288 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.6598471403121948, loss=3.1043386459350586
I0201 05:27:42.000816 140023005427456 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.6741019487380981, loss=3.123685359954834
I0201 05:28:28.018738 140022518892288 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.639542818069458, loss=4.319793224334717
I0201 05:29:10.936659 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:29:21.454939 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:29:45.214106 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:29:46.819794 140184451094336 submission_runner.py:408] Time since start: 59598.03s, 	Step: 120694, 	{'train/accuracy': 0.81947261095047, 'train/loss': 0.9173012971878052, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.251138687133789, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.8443632125854492, 'test/num_examples': 10000, 'score': 55073.763964653015, 'total_duration': 59598.028196811676, 'accumulated_submission_time': 55073.763964653015, 'accumulated_eval_time': 4513.142883777618, 'accumulated_logging_time': 4.922154426574707}
I0201 05:29:46.858154 140023005427456 logging_writer.py:48] [120694] accumulated_eval_time=4513.142884, accumulated_logging_time=4.922154, accumulated_submission_time=55073.763965, global_step=120694, preemption_count=0, score=55073.763965, test/accuracy=0.621700, test/loss=1.844363, test/num_examples=10000, total_duration=59598.028197, train/accuracy=0.819473, train/loss=0.917301, validation/accuracy=0.741580, validation/loss=1.251139, validation/num_examples=50000
I0201 05:29:49.661436 140022518892288 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.542838215827942, loss=3.591463088989258
I0201 05:30:31.097470 140023005427456 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.7541606426239014, loss=4.487982749938965
I0201 05:31:17.045329 140022518892288 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.6061697006225586, loss=3.1994028091430664
I0201 05:32:03.472748 140023005427456 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.6474871635437012, loss=3.040468215942383
I0201 05:32:49.522721 140022518892288 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.6307100057601929, loss=3.455242395401001
I0201 05:33:35.520357 140023005427456 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.6743360757827759, loss=3.8312509059906006
I0201 05:34:21.571683 140022518892288 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.5019999742507935, loss=3.261300563812256
I0201 05:35:07.789491 140023005427456 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.6027300357818604, loss=3.3628957271575928
I0201 05:35:53.593448 140022518892288 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.5923513174057007, loss=3.4138023853302
I0201 05:36:39.714280 140023005427456 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.696543574333191, loss=4.2785210609436035
I0201 05:36:47.126992 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:36:57.504442 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:37:18.380616 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:37:19.995994 140184451094336 submission_runner.py:408] Time since start: 60051.20s, 	Step: 121618, 	{'train/accuracy': 0.8227343559265137, 'train/loss': 0.9045091867446899, 'validation/accuracy': 0.7416200041770935, 'validation/loss': 1.2397983074188232, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.8253264427185059, 'test/num_examples': 10000, 'score': 55493.97620844841, 'total_duration': 60051.2044301033, 'accumulated_submission_time': 55493.97620844841, 'accumulated_eval_time': 4546.011871576309, 'accumulated_logging_time': 4.970205307006836}
I0201 05:37:20.030091 140022518892288 logging_writer.py:48] [121618] accumulated_eval_time=4546.011872, accumulated_logging_time=4.970205, accumulated_submission_time=55493.976208, global_step=121618, preemption_count=0, score=55493.976208, test/accuracy=0.620100, test/loss=1.825326, test/num_examples=10000, total_duration=60051.204430, train/accuracy=0.822734, train/loss=0.904509, validation/accuracy=0.741620, validation/loss=1.239798, validation/num_examples=50000
I0201 05:37:53.470352 140023005427456 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.5638504028320312, loss=3.9384829998016357
I0201 05:38:39.488656 140022518892288 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.826423168182373, loss=3.2167017459869385
I0201 05:39:25.567525 140023005427456 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.531076431274414, loss=3.296349048614502
I0201 05:40:11.627278 140022518892288 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.7221070528030396, loss=3.127519369125366
I0201 05:40:57.487036 140023005427456 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.7673975229263306, loss=4.3462090492248535
I0201 05:41:43.540844 140022518892288 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.6432111263275146, loss=3.0895121097564697
I0201 05:42:29.620365 140023005427456 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.6292659044265747, loss=3.1093268394470215
I0201 05:43:15.799530 140022518892288 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.694280743598938, loss=3.026962995529175
I0201 05:44:01.742693 140023005427456 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.801801085472107, loss=4.62319278717041
I0201 05:44:20.336264 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:44:30.549525 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:44:55.942283 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:44:57.542153 140184451094336 submission_runner.py:408] Time since start: 60508.75s, 	Step: 122542, 	{'train/accuracy': 0.828417956829071, 'train/loss': 0.8838768601417542, 'validation/accuracy': 0.7440599799156189, 'validation/loss': 1.24087393283844, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8244028091430664, 'test/num_examples': 10000, 'score': 55914.22261214256, 'total_duration': 60508.75056910515, 'accumulated_submission_time': 55914.22261214256, 'accumulated_eval_time': 4583.217739343643, 'accumulated_logging_time': 5.016493797302246}
I0201 05:44:57.576145 140022518892288 logging_writer.py:48] [122542] accumulated_eval_time=4583.217739, accumulated_logging_time=5.016494, accumulated_submission_time=55914.222612, global_step=122542, preemption_count=0, score=55914.222612, test/accuracy=0.627700, test/loss=1.824403, test/num_examples=10000, total_duration=60508.750569, train/accuracy=0.828418, train/loss=0.883877, validation/accuracy=0.744060, validation/loss=1.240874, validation/num_examples=50000
I0201 05:45:21.174392 140023005427456 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.6231677532196045, loss=3.6542584896087646
I0201 05:46:05.902600 140022518892288 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.7094553709030151, loss=3.676093339920044
I0201 05:46:51.970012 140023005427456 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.7589975595474243, loss=4.668926239013672
I0201 05:47:38.206890 140022518892288 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.6686148643493652, loss=4.27504825592041
I0201 05:48:24.129964 140023005427456 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.9415713548660278, loss=4.627645492553711
I0201 05:49:10.230464 140022518892288 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.650134563446045, loss=3.0698864459991455
I0201 05:49:56.255109 140023005427456 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.5599576234817505, loss=3.309866428375244
I0201 05:50:42.481468 140022518892288 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.546595573425293, loss=3.1100196838378906
I0201 05:51:28.679059 140023005427456 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.6631450653076172, loss=4.044795989990234
I0201 05:51:57.934233 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:52:08.450634 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 05:52:32.893991 140184451094336 spec.py:349] Evaluating on the test split.
I0201 05:52:34.488823 140184451094336 submission_runner.py:408] Time since start: 60965.70s, 	Step: 123465, 	{'train/accuracy': 0.8238476514816284, 'train/loss': 0.9152378439903259, 'validation/accuracy': 0.746399998664856, 'validation/loss': 1.2385286092758179, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.8137975931167603, 'test/num_examples': 10000, 'score': 56334.522699832916, 'total_duration': 60965.69726276398, 'accumulated_submission_time': 56334.522699832916, 'accumulated_eval_time': 4619.772355079651, 'accumulated_logging_time': 5.061162710189819}
I0201 05:52:34.525908 140022518892288 logging_writer.py:48] [123465] accumulated_eval_time=4619.772355, accumulated_logging_time=5.061163, accumulated_submission_time=56334.522700, global_step=123465, preemption_count=0, score=56334.522700, test/accuracy=0.628500, test/loss=1.813798, test/num_examples=10000, total_duration=60965.697263, train/accuracy=0.823848, train/loss=0.915238, validation/accuracy=0.746400, validation/loss=1.238529, validation/num_examples=50000
I0201 05:52:48.948051 140023005427456 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.6372913122177124, loss=3.7365059852600098
I0201 05:53:32.405635 140022518892288 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.7708430290222168, loss=3.055946111679077
I0201 05:54:18.517149 140023005427456 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.791869878768921, loss=4.559253692626953
I0201 05:55:05.072724 140022518892288 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.6581284999847412, loss=3.199387788772583
I0201 05:55:51.314240 140023005427456 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.6760329008102417, loss=3.485158681869507
I0201 05:56:37.567425 140022518892288 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.7831621170043945, loss=3.1038694381713867
I0201 05:57:23.977037 140023005427456 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.057605504989624, loss=4.758734703063965
I0201 05:58:10.034391 140022518892288 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.818924903869629, loss=2.9916112422943115
I0201 05:58:56.114719 140023005427456 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.9007935523986816, loss=4.691672325134277
I0201 05:59:34.678251 140184451094336 spec.py:321] Evaluating on the training split.
I0201 05:59:45.129370 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:00:12.152804 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:00:13.756695 140184451094336 submission_runner.py:408] Time since start: 61424.97s, 	Step: 124385, 	{'train/accuracy': 0.8286913633346558, 'train/loss': 0.886573851108551, 'validation/accuracy': 0.7448999881744385, 'validation/loss': 1.2313250303268433, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8222373723983765, 'test/num_examples': 10000, 'score': 56754.61694145203, 'total_duration': 61424.96513009071, 'accumulated_submission_time': 56754.61694145203, 'accumulated_eval_time': 4658.850819826126, 'accumulated_logging_time': 5.109199523925781}
I0201 06:00:13.791561 140022518892288 logging_writer.py:48] [124385] accumulated_eval_time=4658.850820, accumulated_logging_time=5.109200, accumulated_submission_time=56754.616941, global_step=124385, preemption_count=0, score=56754.616941, test/accuracy=0.627000, test/loss=1.822237, test/num_examples=10000, total_duration=61424.965130, train/accuracy=0.828691, train/loss=0.886574, validation/accuracy=0.744900, validation/loss=1.231325, validation/num_examples=50000
I0201 06:00:20.194455 140023005427456 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.552953839302063, loss=3.2013278007507324
I0201 06:01:02.383001 140022518892288 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.6121853590011597, loss=3.6965270042419434
I0201 06:01:48.451524 140023005427456 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.7702410221099854, loss=3.046207904815674
I0201 06:02:34.932872 140022518892288 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.569708228111267, loss=3.852607250213623
I0201 06:03:21.015044 140023005427456 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.7155383825302124, loss=4.071357727050781
I0201 06:04:07.370343 140022518892288 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.9755229949951172, loss=4.658719539642334
I0201 06:04:53.634896 140023005427456 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.598615288734436, loss=3.932095527648926
I0201 06:05:39.869035 140022518892288 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.8787329196929932, loss=4.395163059234619
I0201 06:06:25.979187 140023005427456 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.65061354637146, loss=3.587082862854004
I0201 06:07:12.266608 140022518892288 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.6435489654541016, loss=3.084702491760254
I0201 06:07:13.858024 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:07:24.307296 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:07:45.850690 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:07:47.457848 140184451094336 submission_runner.py:408] Time since start: 61878.67s, 	Step: 125305, 	{'train/accuracy': 0.8369140625, 'train/loss': 0.8582568168640137, 'validation/accuracy': 0.7482799887657166, 'validation/loss': 1.2224678993225098, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.801285982131958, 'test/num_examples': 10000, 'score': 57174.62446951866, 'total_duration': 61878.66628551483, 'accumulated_submission_time': 57174.62446951866, 'accumulated_eval_time': 4692.450619220734, 'accumulated_logging_time': 5.156161308288574}
I0201 06:07:47.493777 140023005427456 logging_writer.py:48] [125305] accumulated_eval_time=4692.450619, accumulated_logging_time=5.156161, accumulated_submission_time=57174.624470, global_step=125305, preemption_count=0, score=57174.624470, test/accuracy=0.631400, test/loss=1.801286, test/num_examples=10000, total_duration=61878.666286, train/accuracy=0.836914, train/loss=0.858257, validation/accuracy=0.748280, validation/loss=1.222468, validation/num_examples=50000
I0201 06:08:27.008790 140022518892288 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.6338889598846436, loss=3.330355167388916
I0201 06:09:13.051783 140023005427456 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.5954002141952515, loss=3.7050576210021973
I0201 06:09:59.073579 140022518892288 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.6025505065917969, loss=3.0447802543640137
I0201 06:10:45.299432 140023005427456 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.5476804971694946, loss=2.991992712020874
I0201 06:11:31.408059 140022518892288 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.5883314609527588, loss=3.7623538970947266
I0201 06:12:17.753309 140023005427456 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.6663180589675903, loss=3.1024489402770996
I0201 06:13:03.925453 140022518892288 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.733759880065918, loss=3.074685573577881
I0201 06:13:49.951910 140023005427456 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.7473598718643188, loss=2.967284917831421
I0201 06:14:36.246759 140022518892288 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.665081262588501, loss=3.3103256225585938
I0201 06:14:47.793715 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:14:58.099324 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:15:19.684584 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:15:21.287607 140184451094336 submission_runner.py:408] Time since start: 62332.50s, 	Step: 126227, 	{'train/accuracy': 0.8360546827316284, 'train/loss': 0.8389551639556885, 'validation/accuracy': 0.7470200061798096, 'validation/loss': 1.2079110145568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.8007813692092896, 'test/num_examples': 10000, 'score': 57594.86729764938, 'total_duration': 62332.49602270126, 'accumulated_submission_time': 57594.86729764938, 'accumulated_eval_time': 4725.944480419159, 'accumulated_logging_time': 5.202749729156494}
I0201 06:15:21.324875 140023005427456 logging_writer.py:48] [126227] accumulated_eval_time=4725.944480, accumulated_logging_time=5.202750, accumulated_submission_time=57594.867298, global_step=126227, preemption_count=0, score=57594.867298, test/accuracy=0.623400, test/loss=1.800781, test/num_examples=10000, total_duration=62332.496023, train/accuracy=0.836055, train/loss=0.838955, validation/accuracy=0.747020, validation/loss=1.207911, validation/num_examples=50000
I0201 06:15:50.919330 140022518892288 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.6677806377410889, loss=3.0193088054656982
I0201 06:16:36.780973 140023005427456 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.6806020736694336, loss=3.0559892654418945
I0201 06:17:22.949501 140022518892288 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.7211438417434692, loss=3.009273052215576
I0201 06:18:09.297702 140023005427456 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.855987787246704, loss=3.0386409759521484
I0201 06:18:55.378874 140022518892288 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.855962872505188, loss=3.4298009872436523
I0201 06:19:41.331621 140023005427456 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.8518810272216797, loss=3.038857936859131
I0201 06:20:27.713704 140022518892288 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.7386674880981445, loss=3.0230531692504883
I0201 06:21:13.566229 140023005427456 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.6504058837890625, loss=3.1909048557281494
I0201 06:21:59.868529 140022518892288 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.791272759437561, loss=3.1248855590820312
I0201 06:22:21.732579 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:22:32.212943 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:22:56.673657 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:22:58.273940 140184451094336 submission_runner.py:408] Time since start: 62789.48s, 	Step: 127149, 	{'train/accuracy': 0.8318945169448853, 'train/loss': 0.8691099882125854, 'validation/accuracy': 0.7507199645042419, 'validation/loss': 1.216996669769287, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.7968522310256958, 'test/num_examples': 10000, 'score': 58015.21964287758, 'total_duration': 62789.48238158226, 'accumulated_submission_time': 58015.21964287758, 'accumulated_eval_time': 4762.4858384132385, 'accumulated_logging_time': 5.24896764755249}
I0201 06:22:58.311831 140023005427456 logging_writer.py:48] [127149] accumulated_eval_time=4762.485838, accumulated_logging_time=5.248968, accumulated_submission_time=58015.219643, global_step=127149, preemption_count=0, score=58015.219643, test/accuracy=0.630000, test/loss=1.796852, test/num_examples=10000, total_duration=62789.482382, train/accuracy=0.831895, train/loss=0.869110, validation/accuracy=0.750720, validation/loss=1.216997, validation/num_examples=50000
I0201 06:23:19.138633 140022518892288 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.766273021697998, loss=4.028041362762451
I0201 06:24:03.280748 140023005427456 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.765442967414856, loss=3.5304465293884277
I0201 06:24:49.456792 140022518892288 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.7301996946334839, loss=3.080551862716675
I0201 06:25:35.892167 140023005427456 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.8644436597824097, loss=3.0632266998291016
I0201 06:26:22.080431 140022518892288 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.8603037595748901, loss=3.1209824085235596
I0201 06:27:08.013276 140023005427456 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.595531702041626, loss=3.012334108352661
I0201 06:27:54.152775 140022518892288 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.8410567045211792, loss=3.1714062690734863
I0201 06:28:40.188682 140023005427456 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.9899394512176514, loss=4.394932746887207
I0201 06:29:26.246975 140022518892288 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.7646257877349854, loss=2.9972381591796875
I0201 06:29:58.651876 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:30:09.890655 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:30:34.828632 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:30:36.432055 140184451094336 submission_runner.py:408] Time since start: 63247.64s, 	Step: 128072, 	{'train/accuracy': 0.8342577815055847, 'train/loss': 0.880061686038971, 'validation/accuracy': 0.7497999668121338, 'validation/loss': 1.2383127212524414, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.8079801797866821, 'test/num_examples': 10000, 'score': 58435.50190925598, 'total_duration': 63247.640478134155, 'accumulated_submission_time': 58435.50190925598, 'accumulated_eval_time': 4800.266010761261, 'accumulated_logging_time': 5.297834873199463}
I0201 06:30:36.470839 140023005427456 logging_writer.py:48] [128072] accumulated_eval_time=4800.266011, accumulated_logging_time=5.297835, accumulated_submission_time=58435.501909, global_step=128072, preemption_count=0, score=58435.501909, test/accuracy=0.629200, test/loss=1.807980, test/num_examples=10000, total_duration=63247.640478, train/accuracy=0.834258, train/loss=0.880062, validation/accuracy=0.749800, validation/loss=1.238313, validation/num_examples=50000
I0201 06:30:48.070171 140022518892288 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.7485538721084595, loss=3.051063299179077
I0201 06:31:31.205648 140023005427456 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.7702949047088623, loss=3.3234260082244873
I0201 06:32:17.356140 140022518892288 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.8130708932876587, loss=4.053528785705566
I0201 06:33:03.703243 140023005427456 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.9638227224349976, loss=4.594202518463135
I0201 06:33:49.685179 140022518892288 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.6193764209747314, loss=3.7654385566711426
I0201 06:34:35.590492 140023005427456 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.0270307064056396, loss=4.496472358703613
I0201 06:35:21.983884 140022518892288 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.6831408739089966, loss=3.1404123306274414
I0201 06:36:08.042561 140023005427456 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.7207579612731934, loss=3.1248457431793213
I0201 06:36:54.149579 140022518892288 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.7960278987884521, loss=3.0352134704589844
I0201 06:37:36.589446 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:37:46.901168 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:38:18.416973 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:38:20.014023 140184451094336 submission_runner.py:408] Time since start: 63711.22s, 	Step: 128994, 	{'train/accuracy': 0.8449023365974426, 'train/loss': 0.8151799440383911, 'validation/accuracy': 0.7497599720954895, 'validation/loss': 1.2033263444900513, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.784820795059204, 'test/num_examples': 10000, 'score': 58855.55880713463, 'total_duration': 63711.22246456146, 'accumulated_submission_time': 58855.55880713463, 'accumulated_eval_time': 4843.690578460693, 'accumulated_logging_time': 5.350980758666992}
I0201 06:38:20.052408 140023005427456 logging_writer.py:48] [128994] accumulated_eval_time=4843.690578, accumulated_logging_time=5.350981, accumulated_submission_time=58855.558807, global_step=128994, preemption_count=0, score=58855.558807, test/accuracy=0.632300, test/loss=1.784821, test/num_examples=10000, total_duration=63711.222465, train/accuracy=0.844902, train/loss=0.815180, validation/accuracy=0.749760, validation/loss=1.203326, validation/num_examples=50000
I0201 06:38:22.847790 140022518892288 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.575730562210083, loss=3.259356737136841
I0201 06:39:04.585113 140023005427456 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.752584457397461, loss=3.333664655685425
I0201 06:39:50.555282 140022518892288 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.6516205072402954, loss=3.0059125423431396
I0201 06:40:36.610555 140023005427456 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.6964972019195557, loss=2.9929637908935547
I0201 06:41:22.733073 140022518892288 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.2866082191467285, loss=4.611078262329102
I0201 06:42:08.948456 140023005427456 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.9589569568634033, loss=4.458524227142334
I0201 06:42:54.983268 140022518892288 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.8042488098144531, loss=3.1203908920288086
I0201 06:43:40.993303 140023005427456 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.896174669265747, loss=3.1160848140716553
I0201 06:44:27.210016 140022518892288 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.747738242149353, loss=3.645688056945801
I0201 06:45:13.534372 140023005427456 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.6904665231704712, loss=3.0168700218200684
I0201 06:45:20.250276 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:45:30.771008 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:45:51.335236 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:45:52.942199 140184451094336 submission_runner.py:408] Time since start: 64164.15s, 	Step: 129916, 	{'train/accuracy': 0.8347851634025574, 'train/loss': 0.8594391942024231, 'validation/accuracy': 0.7525799870491028, 'validation/loss': 1.2059522867202759, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.800089955329895, 'test/num_examples': 10000, 'score': 59275.69723725319, 'total_duration': 64164.15061426163, 'accumulated_submission_time': 59275.69723725319, 'accumulated_eval_time': 4876.382496595383, 'accumulated_logging_time': 5.40146017074585}
I0201 06:45:52.978082 140022518892288 logging_writer.py:48] [129916] accumulated_eval_time=4876.382497, accumulated_logging_time=5.401460, accumulated_submission_time=59275.697237, global_step=129916, preemption_count=0, score=59275.697237, test/accuracy=0.630500, test/loss=1.800090, test/num_examples=10000, total_duration=64164.150614, train/accuracy=0.834785, train/loss=0.859439, validation/accuracy=0.752580, validation/loss=1.205952, validation/num_examples=50000
I0201 06:46:27.459707 140023005427456 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.8568620681762695, loss=3.156778573989868
I0201 06:47:13.512358 140022518892288 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.878914713859558, loss=2.9583888053894043
I0201 06:47:59.720587 140023005427456 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.856434941291809, loss=3.0456838607788086
I0201 06:48:45.978213 140022518892288 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.6246951818466187, loss=3.6987650394439697
I0201 06:49:31.842069 140023005427456 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.8656080961227417, loss=3.512225866317749
I0201 06:50:17.910883 140022518892288 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.8445518016815186, loss=3.286851406097412
I0201 06:51:03.962525 140023005427456 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.7250609397888184, loss=3.179311990737915
I0201 06:51:50.299083 140022518892288 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.8316465616226196, loss=4.148603439331055
I0201 06:52:36.380440 140023005427456 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.648221731185913, loss=3.4747402667999268
I0201 06:52:53.084851 140184451094336 spec.py:321] Evaluating on the training split.
I0201 06:53:03.633173 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 06:53:29.467895 140184451094336 spec.py:349] Evaluating on the test split.
I0201 06:53:31.069232 140184451094336 submission_runner.py:408] Time since start: 64622.28s, 	Step: 130838, 	{'train/accuracy': 0.8397656083106995, 'train/loss': 0.8464886546134949, 'validation/accuracy': 0.753879964351654, 'validation/loss': 1.204972743988037, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7893548011779785, 'test/num_examples': 10000, 'score': 59695.74828839302, 'total_duration': 64622.2776722908, 'accumulated_submission_time': 59695.74828839302, 'accumulated_eval_time': 4914.366876363754, 'accumulated_logging_time': 5.4463982582092285}
I0201 06:53:31.107550 140022518892288 logging_writer.py:48] [130838] accumulated_eval_time=4914.366876, accumulated_logging_time=5.446398, accumulated_submission_time=59695.748288, global_step=130838, preemption_count=0, score=59695.748288, test/accuracy=0.630500, test/loss=1.789355, test/num_examples=10000, total_duration=64622.277672, train/accuracy=0.839766, train/loss=0.846489, validation/accuracy=0.753880, validation/loss=1.204973, validation/num_examples=50000
I0201 06:53:56.312385 140023005427456 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.9534915685653687, loss=3.0648326873779297
I0201 06:54:41.293003 140022518892288 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.8963508605957031, loss=3.08725643157959
I0201 06:55:27.628388 140023005427456 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.9750276803970337, loss=3.086981773376465
I0201 06:56:13.926806 140022518892288 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.1249468326568604, loss=4.658722877502441
I0201 06:56:59.835358 140023005427456 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.6899279356002808, loss=3.5100619792938232
I0201 06:57:46.116383 140022518892288 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.9085038900375366, loss=4.151129722595215
I0201 06:58:32.328457 140023005427456 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.9187707901000977, loss=4.447354316711426
I0201 06:59:18.578855 140022518892288 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.8413348197937012, loss=3.012263298034668
I0201 07:00:04.713433 140023005427456 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.8605971336364746, loss=3.0147552490234375
I0201 07:00:31.123716 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:00:41.467363 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:01:02.723630 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:01:04.318337 140184451094336 submission_runner.py:408] Time since start: 65075.53s, 	Step: 131759, 	{'train/accuracy': 0.8443359136581421, 'train/loss': 0.7996962666511536, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.184333324432373, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.7539469003677368, 'test/num_examples': 10000, 'score': 60115.708676576614, 'total_duration': 65075.52677679062, 'accumulated_submission_time': 60115.708676576614, 'accumulated_eval_time': 4947.561500549316, 'accumulated_logging_time': 5.493636131286621}
I0201 07:01:04.352947 140022518892288 logging_writer.py:48] [131759] accumulated_eval_time=4947.561501, accumulated_logging_time=5.493636, accumulated_submission_time=60115.708677, global_step=131759, preemption_count=0, score=60115.708677, test/accuracy=0.634500, test/loss=1.753947, test/num_examples=10000, total_duration=65075.526777, train/accuracy=0.844336, train/loss=0.799696, validation/accuracy=0.751040, validation/loss=1.184333, validation/num_examples=50000
I0201 07:01:21.169950 140023005427456 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.408543348312378, loss=4.522770881652832
I0201 07:02:04.736487 140022518892288 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.1051535606384277, loss=4.552371978759766
I0201 07:02:50.569808 140023005427456 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8285013437271118, loss=3.7959587574005127
I0201 07:03:36.936155 140022518892288 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.9124351739883423, loss=4.080240249633789
I0201 07:04:22.863390 140023005427456 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.005148410797119, loss=3.1174871921539307
I0201 07:05:08.912177 140022518892288 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.854902982711792, loss=3.1742303371429443
I0201 07:05:55.032178 140023005427456 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.7529542446136475, loss=3.5982439517974854
I0201 07:06:41.255575 140022518892288 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.8547712564468384, loss=3.0668225288391113
I0201 07:07:27.659801 140023005427456 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.8011595010757446, loss=3.1537158489227295
I0201 07:08:04.452584 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:08:14.777079 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:08:38.669443 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:08:40.271653 140184451094336 submission_runner.py:408] Time since start: 65531.48s, 	Step: 132681, 	{'train/accuracy': 0.83753901720047, 'train/loss': 0.8382362127304077, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.1913368701934814, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.769114375114441, 'test/num_examples': 10000, 'score': 60535.7517850399, 'total_duration': 65531.48008418083, 'accumulated_submission_time': 60535.7517850399, 'accumulated_eval_time': 4983.380564451218, 'accumulated_logging_time': 5.537555456161499}
I0201 07:08:40.307867 140022518892288 logging_writer.py:48] [132681] accumulated_eval_time=4983.380564, accumulated_logging_time=5.537555, accumulated_submission_time=60535.751785, global_step=132681, preemption_count=0, score=60535.751785, test/accuracy=0.637400, test/loss=1.769114, test/num_examples=10000, total_duration=65531.480084, train/accuracy=0.837539, train/loss=0.838236, validation/accuracy=0.755140, validation/loss=1.191337, validation/num_examples=50000
I0201 07:08:48.317307 140023005427456 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.6953890323638916, loss=3.56772780418396
I0201 07:09:30.550871 140022518892288 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.9791003465652466, loss=3.189159870147705
I0201 07:10:16.899613 140023005427456 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.726217269897461, loss=2.941896438598633
I0201 07:11:03.099652 140022518892288 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.7602530717849731, loss=3.0997848510742188
I0201 07:11:49.329226 140023005427456 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.934723138809204, loss=3.7325401306152344
I0201 07:12:35.628416 140022518892288 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.6206448078155518, loss=3.4210805892944336
I0201 07:13:21.702387 140023005427456 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.8071964979171753, loss=3.530637264251709
I0201 07:14:07.704335 140022518892288 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.8644373416900635, loss=2.9871737957000732
I0201 07:14:53.826685 140023005427456 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.8724560737609863, loss=2.9815051555633545
I0201 07:15:39.726543 140022518892288 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.9067182540893555, loss=3.1721115112304688
I0201 07:15:40.345016 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:15:50.742863 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:16:12.785597 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:16:14.379186 140184451094336 submission_runner.py:408] Time since start: 65985.59s, 	Step: 133603, 	{'train/accuracy': 0.84046870470047, 'train/loss': 0.8086775541305542, 'validation/accuracy': 0.7542600035667419, 'validation/loss': 1.1765190362930298, 'validation/num_examples': 50000, 'test/accuracy': 0.638200044631958, 'test/loss': 1.7628462314605713, 'test/num_examples': 10000, 'score': 60955.732286930084, 'total_duration': 65985.58762574196, 'accumulated_submission_time': 60955.732286930084, 'accumulated_eval_time': 5017.414732217789, 'accumulated_logging_time': 5.583152770996094}
I0201 07:16:14.418401 140023005427456 logging_writer.py:48] [133603] accumulated_eval_time=5017.414732, accumulated_logging_time=5.583153, accumulated_submission_time=60955.732287, global_step=133603, preemption_count=0, score=60955.732287, test/accuracy=0.638200, test/loss=1.762846, test/num_examples=10000, total_duration=65985.587626, train/accuracy=0.840469, train/loss=0.808678, validation/accuracy=0.754260, validation/loss=1.176519, validation/num_examples=50000
I0201 07:16:54.549722 140022518892288 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.8590033054351807, loss=3.100356101989746
I0201 07:17:40.667323 140023005427456 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.8014991283416748, loss=2.9457430839538574
I0201 07:18:26.815059 140022518892288 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.8778727054595947, loss=3.0096561908721924
I0201 07:19:12.970703 140023005427456 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.8041095733642578, loss=3.63139009475708
I0201 07:19:58.864735 140022518892288 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.8382110595703125, loss=3.4422149658203125
I0201 07:20:45.237233 140023005427456 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.9203671216964722, loss=2.983675003051758
I0201 07:21:31.388542 140022518892288 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.7543047666549683, loss=3.9120736122131348
I0201 07:22:17.495367 140023005427456 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.0851709842681885, loss=4.263731479644775
I0201 07:23:03.814702 140022518892288 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.0270116329193115, loss=4.172379493713379
I0201 07:23:14.511485 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:23:24.665546 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:23:50.571823 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:23:52.176737 140184451094336 submission_runner.py:408] Time since start: 66443.39s, 	Step: 134525, 	{'train/accuracy': 0.8450585603713989, 'train/loss': 0.8082579374313354, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.1792693138122559, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.7707762718200684, 'test/num_examples': 10000, 'score': 61375.7694671154, 'total_duration': 66443.38515496254, 'accumulated_submission_time': 61375.7694671154, 'accumulated_eval_time': 5055.079945325851, 'accumulated_logging_time': 5.6315295696258545}
I0201 07:23:52.222278 140023005427456 logging_writer.py:48] [134525] accumulated_eval_time=5055.079945, accumulated_logging_time=5.631530, accumulated_submission_time=61375.769467, global_step=134525, preemption_count=0, score=61375.769467, test/accuracy=0.632400, test/loss=1.770776, test/num_examples=10000, total_duration=66443.385155, train/accuracy=0.845059, train/loss=0.808258, validation/accuracy=0.757360, validation/loss=1.179269, validation/num_examples=50000
I0201 07:24:22.630509 140022518892288 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.0047717094421387, loss=4.42009162902832
I0201 07:25:08.294500 140023005427456 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.3484957218170166, loss=4.499300956726074
I0201 07:25:54.253302 140022518892288 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.0605130195617676, loss=3.014962673187256
I0201 07:26:40.473452 140023005427456 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.7677483558654785, loss=3.30595326423645
I0201 07:27:26.687231 140022518892288 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.8839799165725708, loss=2.9320549964904785
I0201 07:28:12.967541 140023005427456 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.9008837938308716, loss=3.005764961242676
I0201 07:29:14.041601 140022518892288 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.7145745754241943, loss=3.2630457878112793
I0201 07:30:00.089208 140023005427456 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.88720703125, loss=2.948780059814453
I0201 07:30:46.238412 140022518892288 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.7843976020812988, loss=3.9889745712280273
I0201 07:30:52.334092 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:31:02.985078 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:31:29.820770 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:31:31.419340 140184451094336 submission_runner.py:408] Time since start: 66902.63s, 	Step: 135415, 	{'train/accuracy': 0.8431445360183716, 'train/loss': 0.7971628308296204, 'validation/accuracy': 0.7569400072097778, 'validation/loss': 1.1679935455322266, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.7401326894760132, 'test/num_examples': 10000, 'score': 61795.82298064232, 'total_duration': 66902.62777686119, 'accumulated_submission_time': 61795.82298064232, 'accumulated_eval_time': 5094.165184736252, 'accumulated_logging_time': 5.689408540725708}
I0201 07:31:31.459976 140023005427456 logging_writer.py:48] [135415] accumulated_eval_time=5094.165185, accumulated_logging_time=5.689409, accumulated_submission_time=61795.822981, global_step=135415, preemption_count=0, score=61795.822981, test/accuracy=0.637000, test/loss=1.740133, test/num_examples=10000, total_duration=66902.627777, train/accuracy=0.843145, train/loss=0.797163, validation/accuracy=0.756940, validation/loss=1.167994, validation/num_examples=50000
I0201 07:32:06.624675 140022518892288 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.8196243047714233, loss=3.2255074977874756
I0201 07:32:52.640894 140023005427456 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.8256046772003174, loss=3.029557704925537
I0201 07:33:38.659104 140022518892288 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.9243873357772827, loss=3.495418071746826
I0201 07:34:24.709491 140023005427456 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.8894606828689575, loss=3.0351009368896484
I0201 07:35:10.674910 140022518892288 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.8449244499206543, loss=2.947237730026245
I0201 07:35:56.607981 140023005427456 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.8151065111160278, loss=3.2021656036376953
I0201 07:36:42.820165 140022518892288 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.8456029891967773, loss=3.3339080810546875
I0201 07:37:28.902535 140023005427456 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.8985847234725952, loss=4.15706205368042
I0201 07:38:15.222714 140022518892288 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.9748955965042114, loss=2.8680341243743896
I0201 07:38:31.461649 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:38:42.239412 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:39:06.893280 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:39:08.502735 140184451094336 submission_runner.py:408] Time since start: 67359.71s, 	Step: 136337, 	{'train/accuracy': 0.8441015481948853, 'train/loss': 0.8239647746086121, 'validation/accuracy': 0.7590799927711487, 'validation/loss': 1.176174521446228, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.7639225721359253, 'test/num_examples': 10000, 'score': 62215.76799035072, 'total_duration': 67359.71117639542, 'accumulated_submission_time': 62215.76799035072, 'accumulated_eval_time': 5131.2062656879425, 'accumulated_logging_time': 5.739239692687988}
I0201 07:39:08.538608 140023005427456 logging_writer.py:48] [136337] accumulated_eval_time=5131.206266, accumulated_logging_time=5.739240, accumulated_submission_time=62215.767990, global_step=136337, preemption_count=0, score=62215.767990, test/accuracy=0.632400, test/loss=1.763923, test/num_examples=10000, total_duration=67359.711176, train/accuracy=0.844102, train/loss=0.823965, validation/accuracy=0.759080, validation/loss=1.176175, validation/num_examples=50000
I0201 07:39:34.144326 140022518892288 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.032594680786133, loss=4.270748138427734
I0201 07:40:19.174548 140023005427456 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.7155890464782715, loss=3.2146029472351074
I0201 07:41:05.437940 140022518892288 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.7961227893829346, loss=3.2982897758483887
I0201 07:41:51.830512 140023005427456 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.870656132698059, loss=2.9709720611572266
I0201 07:42:37.839089 140022518892288 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.8890937566757202, loss=3.0063986778259277
I0201 07:43:24.372957 140023005427456 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.110069751739502, loss=4.169464588165283
I0201 07:44:10.463423 140022518892288 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.993173360824585, loss=2.978839874267578
I0201 07:44:56.244929 140023005427456 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.985371708869934, loss=2.903813362121582
I0201 07:45:42.421472 140022518892288 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.8304500579833984, loss=3.394996166229248
I0201 07:46:08.833471 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:46:19.152638 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:46:40.768711 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:46:42.371436 140184451094336 submission_runner.py:408] Time since start: 67813.58s, 	Step: 137259, 	{'train/accuracy': 0.8472656011581421, 'train/loss': 0.8076205253601074, 'validation/accuracy': 0.7579599618911743, 'validation/loss': 1.1808420419692993, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.7493723630905151, 'test/num_examples': 10000, 'score': 62636.00538110733, 'total_duration': 67813.579870224, 'accumulated_submission_time': 62636.00538110733, 'accumulated_eval_time': 5164.7442235946655, 'accumulated_logging_time': 5.78521990776062}
I0201 07:46:42.406954 140023005427456 logging_writer.py:48] [137259] accumulated_eval_time=5164.744224, accumulated_logging_time=5.785220, accumulated_submission_time=62636.005381, global_step=137259, preemption_count=0, score=62636.005381, test/accuracy=0.637000, test/loss=1.749372, test/num_examples=10000, total_duration=67813.579870, train/accuracy=0.847266, train/loss=0.807621, validation/accuracy=0.757960, validation/loss=1.180842, validation/num_examples=50000
I0201 07:46:59.218142 140022518892288 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.9359723329544067, loss=3.893883228302002
I0201 07:47:42.813834 140023005427456 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.8373558521270752, loss=3.12967586517334
I0201 07:48:29.281849 140022518892288 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.8325520753860474, loss=2.933279275894165
I0201 07:49:15.302098 140023005427456 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.9895213842391968, loss=3.3314321041107178
I0201 07:50:01.600402 140022518892288 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.1749937534332275, loss=4.447066307067871
I0201 07:50:47.653581 140023005427456 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.0251636505126953, loss=3.035794258117676
I0201 07:51:33.805348 140022518892288 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.250056266784668, loss=4.552957534790039
I0201 07:52:20.059612 140023005427456 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.7933381795883179, loss=3.615800380706787
I0201 07:53:06.431272 140022518892288 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.8393534421920776, loss=3.1655895709991455
I0201 07:53:42.418938 140184451094336 spec.py:321] Evaluating on the training split.
I0201 07:53:52.918675 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 07:54:15.116378 140184451094336 spec.py:349] Evaluating on the test split.
I0201 07:54:16.734923 140184451094336 submission_runner.py:408] Time since start: 68267.94s, 	Step: 138180, 	{'train/accuracy': 0.8597851395606995, 'train/loss': 0.7563513517379761, 'validation/accuracy': 0.7586399912834167, 'validation/loss': 1.1718906164169312, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.7638990879058838, 'test/num_examples': 10000, 'score': 63055.95963048935, 'total_duration': 68267.94333863258, 'accumulated_submission_time': 63055.95963048935, 'accumulated_eval_time': 5199.060210227966, 'accumulated_logging_time': 5.831464767456055}
I0201 07:54:16.775658 140023005427456 logging_writer.py:48] [138180] accumulated_eval_time=5199.060210, accumulated_logging_time=5.831465, accumulated_submission_time=63055.959630, global_step=138180, preemption_count=0, score=63055.959630, test/accuracy=0.633600, test/loss=1.763899, test/num_examples=10000, total_duration=68267.943339, train/accuracy=0.859785, train/loss=0.756351, validation/accuracy=0.758640, validation/loss=1.171891, validation/num_examples=50000
I0201 07:54:25.184521 140022518892288 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.548110246658325, loss=4.420735836029053
I0201 07:55:07.504203 140023005427456 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.7797938585281372, loss=3.731598377227783
I0201 07:55:53.509030 140022518892288 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.9404231309890747, loss=3.4787869453430176
I0201 07:56:39.772236 140023005427456 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.8931692838668823, loss=3.1404471397399902
I0201 07:57:25.769692 140022518892288 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.8310470581054688, loss=3.2287330627441406
I0201 07:58:12.017331 140023005427456 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.9248138666152954, loss=3.7818236351013184
I0201 07:58:58.085996 140022518892288 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.799716830253601, loss=3.1894893646240234
I0201 07:59:44.416167 140023005427456 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.1092872619628906, loss=4.0826592445373535
I0201 08:00:30.713114 140022518892288 logging_writer.py:48] [139000] global_step=139000, grad_norm=1.9493920803070068, loss=2.984774112701416
I0201 08:01:16.896829 140023005427456 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.9804091453552246, loss=3.436272621154785
I0201 08:01:16.909565 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:01:27.534624 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:01:51.202300 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:01:52.801439 140184451094336 submission_runner.py:408] Time since start: 68724.01s, 	Step: 139101, 	{'train/accuracy': 0.8470116853713989, 'train/loss': 0.8125196099281311, 'validation/accuracy': 0.7589399814605713, 'validation/loss': 1.179947018623352, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.7559922933578491, 'test/num_examples': 10000, 'score': 63476.035716056824, 'total_duration': 68724.00987887383, 'accumulated_submission_time': 63476.035716056824, 'accumulated_eval_time': 5234.952075719833, 'accumulated_logging_time': 5.88202166557312}
I0201 08:01:52.838783 140022518892288 logging_writer.py:48] [139101] accumulated_eval_time=5234.952076, accumulated_logging_time=5.882022, accumulated_submission_time=63476.035716, global_step=139101, preemption_count=0, score=63476.035716, test/accuracy=0.637000, test/loss=1.755992, test/num_examples=10000, total_duration=68724.009879, train/accuracy=0.847012, train/loss=0.812520, validation/accuracy=0.758940, validation/loss=1.179947, validation/num_examples=50000
I0201 08:02:33.970502 140023005427456 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.901423454284668, loss=3.0496888160705566
I0201 08:03:20.059243 140022518892288 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.738939642906189, loss=3.098003625869751
I0201 08:04:06.307740 140023005427456 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.7572853565216064, loss=2.996872901916504
I0201 08:04:52.516614 140022518892288 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.5564022064208984, loss=4.276977062225342
I0201 08:05:38.348865 140023005427456 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.9088213443756104, loss=3.0241005420684814
I0201 08:06:24.369282 140022518892288 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.0627825260162354, loss=2.898048162460327
I0201 08:07:10.423111 140023005427456 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.7781182527542114, loss=3.465092182159424
I0201 08:07:56.521045 140022518892288 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.0065886974334717, loss=3.0417144298553467
I0201 08:08:42.651089 140023005427456 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.9117698669433594, loss=2.9280953407287598
I0201 08:08:52.840995 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:09:03.411484 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:09:27.200978 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:09:28.804415 140184451094336 submission_runner.py:408] Time since start: 69180.01s, 	Step: 140024, 	{'train/accuracy': 0.8527734279632568, 'train/loss': 0.758125364780426, 'validation/accuracy': 0.762499988079071, 'validation/loss': 1.136866569519043, 'validation/num_examples': 50000, 'test/accuracy': 0.6468000411987305, 'test/loss': 1.713780164718628, 'test/num_examples': 10000, 'score': 63895.9807267189, 'total_duration': 69180.01283836365, 'accumulated_submission_time': 63895.9807267189, 'accumulated_eval_time': 5270.915474653244, 'accumulated_logging_time': 5.92934775352478}
I0201 08:09:28.842970 140022518892288 logging_writer.py:48] [140024] accumulated_eval_time=5270.915475, accumulated_logging_time=5.929348, accumulated_submission_time=63895.980727, global_step=140024, preemption_count=0, score=63895.980727, test/accuracy=0.646800, test/loss=1.713780, test/num_examples=10000, total_duration=69180.012838, train/accuracy=0.852773, train/loss=0.758125, validation/accuracy=0.762500, validation/loss=1.136867, validation/num_examples=50000
I0201 08:09:59.627928 140023005427456 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.895945429801941, loss=2.9766077995300293
I0201 08:10:45.746995 140022518892288 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.1536293029785156, loss=3.074615478515625
I0201 08:11:31.867405 140023005427456 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.970739722251892, loss=2.938061237335205
I0201 08:12:18.411442 140022518892288 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.903407096862793, loss=3.081332206726074
I0201 08:13:04.512264 140023005427456 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.0117275714874268, loss=3.009293794631958
I0201 08:13:50.587498 140022518892288 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.967971682548523, loss=3.391299247741699
I0201 08:14:36.719331 140023005427456 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.1651713848114014, loss=2.927614688873291
I0201 08:15:22.909686 140022518892288 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.0807201862335205, loss=3.147522449493408
I0201 08:16:08.935790 140023005427456 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.9879318475723267, loss=3.6623075008392334
I0201 08:16:28.950429 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:16:39.489068 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:17:00.925333 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:17:02.530615 140184451094336 submission_runner.py:408] Time since start: 69633.74s, 	Step: 140945, 	{'train/accuracy': 0.8581054210662842, 'train/loss': 0.7595937848091125, 'validation/accuracy': 0.7590199708938599, 'validation/loss': 1.1628683805465698, 'validation/num_examples': 50000, 'test/accuracy': 0.6384000182151794, 'test/loss': 1.7504186630249023, 'test/num_examples': 10000, 'score': 64316.03085923195, 'total_duration': 69633.73905014992, 'accumulated_submission_time': 64316.03085923195, 'accumulated_eval_time': 5304.495651721954, 'accumulated_logging_time': 5.978683710098267}
I0201 08:17:02.571440 140022518892288 logging_writer.py:48] [140945] accumulated_eval_time=5304.495652, accumulated_logging_time=5.978684, accumulated_submission_time=64316.030859, global_step=140945, preemption_count=0, score=64316.030859, test/accuracy=0.638400, test/loss=1.750419, test/num_examples=10000, total_duration=69633.739050, train/accuracy=0.858105, train/loss=0.759594, validation/accuracy=0.759020, validation/loss=1.162868, validation/num_examples=50000
I0201 08:17:24.971895 140023005427456 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.0152218341827393, loss=2.9620354175567627
I0201 08:18:09.323700 140022518892288 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.9776135683059692, loss=2.873243808746338
I0201 08:18:55.447891 140023005427456 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.9037153720855713, loss=3.2709388732910156
I0201 08:19:41.751125 140022518892288 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.7934941053390503, loss=2.9306962490081787
I0201 08:20:27.890866 140023005427456 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.927771806716919, loss=3.7998287677764893
I0201 08:21:14.040482 140022518892288 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.2138562202453613, loss=4.063796043395996
I0201 08:22:00.316245 140023005427456 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.879639983177185, loss=2.964284896850586
I0201 08:22:46.359368 140022518892288 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.0015981197357178, loss=2.988004207611084
I0201 08:23:32.514906 140023005427456 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.906051516532898, loss=3.4582152366638184
I0201 08:24:02.679158 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:24:12.864989 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:24:34.795204 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:24:36.390422 140184451094336 submission_runner.py:408] Time since start: 70087.60s, 	Step: 141867, 	{'train/accuracy': 0.8505077958106995, 'train/loss': 0.8011537790298462, 'validation/accuracy': 0.7633000016212463, 'validation/loss': 1.1656519174575806, 'validation/num_examples': 50000, 'test/accuracy': 0.641800045967102, 'test/loss': 1.7470555305480957, 'test/num_examples': 10000, 'score': 64736.08189225197, 'total_duration': 70087.59886169434, 'accumulated_submission_time': 64736.08189225197, 'accumulated_eval_time': 5338.206914901733, 'accumulated_logging_time': 6.028635501861572}
I0201 08:24:36.427028 140022518892288 logging_writer.py:48] [141867] accumulated_eval_time=5338.206915, accumulated_logging_time=6.028636, accumulated_submission_time=64736.081892, global_step=141867, preemption_count=0, score=64736.081892, test/accuracy=0.641800, test/loss=1.747056, test/num_examples=10000, total_duration=70087.598862, train/accuracy=0.850508, train/loss=0.801154, validation/accuracy=0.763300, validation/loss=1.165652, validation/num_examples=50000
I0201 08:24:50.052649 140023005427456 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.046318531036377, loss=3.0346198081970215
I0201 08:25:33.066313 140022518892288 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.962152123451233, loss=3.3666136264801025
I0201 08:26:19.218988 140023005427456 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.0138869285583496, loss=3.048304319381714
I0201 08:27:05.576391 140022518892288 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.8821053504943848, loss=3.191495895385742
I0201 08:27:51.663173 140023005427456 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.9336692094802856, loss=3.097480297088623
I0201 08:28:37.608361 140022518892288 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.5337133407592773, loss=4.4904303550720215
I0201 08:29:23.933484 140023005427456 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.095268726348877, loss=3.577932596206665
I0201 08:30:09.992075 140022518892288 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.9040659666061401, loss=3.6123814582824707
I0201 08:30:56.529358 140023005427456 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.0847835540771484, loss=2.934173107147217
I0201 08:31:36.514828 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:31:47.021158 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:32:08.720464 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:32:10.330726 140184451094336 submission_runner.py:408] Time since start: 70541.54s, 	Step: 142788, 	{'train/accuracy': 0.8565039038658142, 'train/loss': 0.7577779293060303, 'validation/accuracy': 0.7645599842071533, 'validation/loss': 1.1408846378326416, 'validation/num_examples': 50000, 'test/accuracy': 0.6448000073432922, 'test/loss': 1.7246521711349487, 'test/num_examples': 10000, 'score': 65156.10931110382, 'total_duration': 70541.53916501999, 'accumulated_submission_time': 65156.10931110382, 'accumulated_eval_time': 5372.0228152275085, 'accumulated_logging_time': 6.0786073207855225}
I0201 08:32:10.375714 140022518892288 logging_writer.py:48] [142788] accumulated_eval_time=5372.022815, accumulated_logging_time=6.078607, accumulated_submission_time=65156.109311, global_step=142788, preemption_count=0, score=65156.109311, test/accuracy=0.644800, test/loss=1.724652, test/num_examples=10000, total_duration=70541.539165, train/accuracy=0.856504, train/loss=0.757778, validation/accuracy=0.764560, validation/loss=1.140885, validation/num_examples=50000
I0201 08:32:15.581990 140023005427456 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.961516261100769, loss=2.955040454864502
I0201 08:32:57.458605 140022518892288 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.1613974571228027, loss=2.9426119327545166
I0201 08:33:43.477858 140023005427456 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.0078604221343994, loss=3.7847931385040283
I0201 08:34:30.062547 140022518892288 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.39988112449646, loss=4.469518661499023
I0201 08:35:16.251155 140023005427456 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.069016695022583, loss=3.5135107040405273
I0201 08:36:02.219930 140022518892288 logging_writer.py:48] [143300] global_step=143300, grad_norm=4.454261302947998, loss=4.21064567565918
I0201 08:36:48.277544 140023005427456 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.0469484329223633, loss=2.9819600582122803
I0201 08:37:34.425818 140022518892288 logging_writer.py:48] [143500] global_step=143500, grad_norm=1.896376132965088, loss=3.3034496307373047
I0201 08:38:20.643120 140023005427456 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.87938392162323, loss=3.2952990531921387
I0201 08:39:06.974375 140022518892288 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.9031543731689453, loss=3.217625379562378
I0201 08:39:10.353361 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:39:20.903098 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:39:43.554921 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:39:45.161859 140184451094336 submission_runner.py:408] Time since start: 70996.37s, 	Step: 143709, 	{'train/accuracy': 0.85986328125, 'train/loss': 0.7596256732940674, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.166117787361145, 'validation/num_examples': 50000, 'test/accuracy': 0.6426000595092773, 'test/loss': 1.7453765869140625, 'test/num_examples': 10000, 'score': 65576.03127932549, 'total_duration': 70996.37029480934, 'accumulated_submission_time': 65576.03127932549, 'accumulated_eval_time': 5406.831308364868, 'accumulated_logging_time': 6.132803916931152}
I0201 08:39:45.204761 140023005427456 logging_writer.py:48] [143709] accumulated_eval_time=5406.831308, accumulated_logging_time=6.132804, accumulated_submission_time=65576.031279, global_step=143709, preemption_count=0, score=65576.031279, test/accuracy=0.642600, test/loss=1.745377, test/num_examples=10000, total_duration=70996.370295, train/accuracy=0.859863, train/loss=0.759626, validation/accuracy=0.762860, validation/loss=1.166118, validation/num_examples=50000
I0201 08:40:22.906614 140022518892288 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.8092598915100098, loss=4.460422515869141
I0201 08:41:08.908098 140023005427456 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.199962615966797, loss=4.026923179626465
I0201 08:41:55.071408 140022518892288 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.119208335876465, loss=2.904865264892578
I0201 08:42:41.287179 140023005427456 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.5077438354492188, loss=4.482303619384766
I0201 08:43:27.368951 140022518892288 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.296222448348999, loss=3.6192643642425537
I0201 08:44:13.307965 140023005427456 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.9555408954620361, loss=2.9178144931793213
I0201 08:44:59.493172 140022518892288 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.9617702960968018, loss=3.151928424835205
I0201 08:45:45.646545 140023005427456 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.015939474105835, loss=2.914360523223877
I0201 08:46:31.725576 140022518892288 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.997441053390503, loss=2.918745756149292
I0201 08:46:45.319168 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:46:55.970555 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:47:17.133997 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:47:18.727188 140184451094336 submission_runner.py:408] Time since start: 71449.94s, 	Step: 144631, 	{'train/accuracy': 0.8544726371765137, 'train/loss': 0.784826934337616, 'validation/accuracy': 0.7655799984931946, 'validation/loss': 1.1578985452651978, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.7437084913253784, 'test/num_examples': 10000, 'score': 65996.08980154991, 'total_duration': 71449.93562602997, 'accumulated_submission_time': 65996.08980154991, 'accumulated_eval_time': 5440.239306926727, 'accumulated_logging_time': 6.184715032577515}
I0201 08:47:18.765287 140023005427456 logging_writer.py:48] [144631] accumulated_eval_time=5440.239307, accumulated_logging_time=6.184715, accumulated_submission_time=65996.089802, global_step=144631, preemption_count=0, score=65996.089802, test/accuracy=0.645600, test/loss=1.743708, test/num_examples=10000, total_duration=71449.935626, train/accuracy=0.854473, train/loss=0.784827, validation/accuracy=0.765580, validation/loss=1.157899, validation/num_examples=50000
I0201 08:47:46.785053 140022518892288 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.146435499191284, loss=2.9925265312194824
I0201 08:48:31.982436 140023005427456 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.1666274070739746, loss=4.192618370056152
I0201 08:49:18.274489 140022518892288 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.0756478309631348, loss=2.875537157058716
I0201 08:50:04.448014 140023005427456 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.3821091651916504, loss=4.1918253898620605
I0201 08:50:50.404134 140022518892288 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.1694629192352295, loss=4.057633876800537
I0201 08:51:36.585511 140023005427456 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.1158716678619385, loss=3.607998847961426
I0201 08:52:22.693897 140022518892288 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.366508722305298, loss=4.358879566192627
I0201 08:53:08.698683 140023005427456 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.0325684547424316, loss=3.16263747215271
I0201 08:53:54.614074 140022518892288 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.0021679401397705, loss=3.7265567779541016
I0201 08:54:18.801284 140184451094336 spec.py:321] Evaluating on the training split.
I0201 08:54:29.642210 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 08:54:53.847306 140184451094336 spec.py:349] Evaluating on the test split.
I0201 08:54:55.447857 140184451094336 submission_runner.py:408] Time since start: 71906.66s, 	Step: 145554, 	{'train/accuracy': 0.8596875071525574, 'train/loss': 0.7511771321296692, 'validation/accuracy': 0.765999972820282, 'validation/loss': 1.1414402723312378, 'validation/num_examples': 50000, 'test/accuracy': 0.6479000449180603, 'test/loss': 1.710391640663147, 'test/num_examples': 10000, 'score': 66416.06876826286, 'total_duration': 71906.65629696846, 'accumulated_submission_time': 66416.06876826286, 'accumulated_eval_time': 5476.885877370834, 'accumulated_logging_time': 6.231912851333618}
I0201 08:54:55.484432 140023005427456 logging_writer.py:48] [145554] accumulated_eval_time=5476.885877, accumulated_logging_time=6.231913, accumulated_submission_time=66416.068768, global_step=145554, preemption_count=0, score=66416.068768, test/accuracy=0.647900, test/loss=1.710392, test/num_examples=10000, total_duration=71906.656297, train/accuracy=0.859688, train/loss=0.751177, validation/accuracy=0.766000, validation/loss=1.141440, validation/num_examples=50000
I0201 08:55:14.303513 140022518892288 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.044240951538086, loss=2.848987102508545
I0201 08:55:58.425220 140023005427456 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.2626030445098877, loss=3.671342611312866
I0201 08:56:44.489091 140022518892288 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.9675871133804321, loss=3.262843608856201
I0201 08:57:30.741580 140023005427456 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.154926061630249, loss=2.8798980712890625
I0201 08:58:16.723468 140022518892288 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.1799895763397217, loss=4.105067253112793
I0201 08:59:02.875248 140023005427456 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.0113441944122314, loss=3.1132731437683105
I0201 08:59:49.076484 140022518892288 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.193399429321289, loss=2.925844192504883
I0201 09:00:34.965400 140023005427456 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.9586368799209595, loss=3.7560536861419678
I0201 09:01:21.212931 140022518892288 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.0118632316589355, loss=3.1946005821228027
I0201 09:01:55.480105 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:02:06.091901 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:02:27.078634 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:02:28.682659 140184451094336 submission_runner.py:408] Time since start: 72359.89s, 	Step: 146476, 	{'train/accuracy': 0.8626366853713989, 'train/loss': 0.7605917453765869, 'validation/accuracy': 0.7663599848747253, 'validation/loss': 1.1581281423568726, 'validation/num_examples': 50000, 'test/accuracy': 0.650600016117096, 'test/loss': 1.7234017848968506, 'test/num_examples': 10000, 'score': 66836.00802612305, 'total_duration': 72359.89109659195, 'accumulated_submission_time': 66836.00802612305, 'accumulated_eval_time': 5510.088440179825, 'accumulated_logging_time': 6.2781524658203125}
I0201 09:02:28.719800 140023005427456 logging_writer.py:48] [146476] accumulated_eval_time=5510.088440, accumulated_logging_time=6.278152, accumulated_submission_time=66836.008026, global_step=146476, preemption_count=0, score=66836.008026, test/accuracy=0.650600, test/loss=1.723402, test/num_examples=10000, total_duration=72359.891097, train/accuracy=0.862637, train/loss=0.760592, validation/accuracy=0.766360, validation/loss=1.158128, validation/num_examples=50000
I0201 09:02:38.719819 140022518892288 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.2512571811676025, loss=4.158164978027344
I0201 09:03:21.299367 140023005427456 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.1902341842651367, loss=3.8800859451293945
I0201 09:04:07.371538 140022518892288 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.2785913944244385, loss=3.090055227279663
I0201 09:04:53.365362 140023005427456 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.4635274410247803, loss=4.317276477813721
I0201 09:05:39.401636 140022518892288 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.104553699493408, loss=3.8203325271606445
I0201 09:06:25.502214 140023005427456 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.08314847946167, loss=2.941943407058716
I0201 09:07:11.557835 140022518892288 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.115002393722534, loss=2.900150775909424
I0201 09:07:57.305274 140023005427456 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.3162736892700195, loss=3.0206892490386963
I0201 09:08:43.163808 140022518892288 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.1572494506835938, loss=2.9639220237731934
I0201 09:09:28.711638 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:09:39.348964 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:10:02.954869 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:10:04.564071 140184451094336 submission_runner.py:408] Time since start: 72815.77s, 	Step: 147400, 	{'train/accuracy': 0.8684960603713989, 'train/loss': 0.7181340456008911, 'validation/accuracy': 0.7680400013923645, 'validation/loss': 1.130260705947876, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.714258074760437, 'test/num_examples': 10000, 'score': 67255.94326424599, 'total_duration': 72815.77249288559, 'accumulated_submission_time': 67255.94326424599, 'accumulated_eval_time': 5545.940866231918, 'accumulated_logging_time': 6.324589729309082}
I0201 09:10:04.609128 140023005427456 logging_writer.py:48] [147400] accumulated_eval_time=5545.940866, accumulated_logging_time=6.324590, accumulated_submission_time=67255.943264, global_step=147400, preemption_count=0, score=67255.943264, test/accuracy=0.645600, test/loss=1.714258, test/num_examples=10000, total_duration=72815.772493, train/accuracy=0.868496, train/loss=0.718134, validation/accuracy=0.768040, validation/loss=1.130261, validation/num_examples=50000
I0201 09:10:05.014436 140022518892288 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.079404354095459, loss=3.106330394744873
I0201 09:10:46.174491 140023005427456 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.4050064086914062, loss=2.9022724628448486
I0201 09:11:32.451223 140022518892288 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.0305593013763428, loss=3.048677444458008
I0201 09:12:19.257090 140023005427456 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.3129467964172363, loss=2.92061448097229
I0201 09:13:05.466658 140022518892288 logging_writer.py:48] [147800] global_step=147800, grad_norm=1.985787034034729, loss=3.3773157596588135
I0201 09:13:53.151964 140023005427456 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.0225226879119873, loss=2.920477867126465
I0201 09:14:39.597981 140022518892288 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.226318597793579, loss=3.6646063327789307
I0201 09:15:26.085753 140023005427456 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.064828634262085, loss=2.9696476459503174
I0201 09:16:12.514321 140022518892288 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.168635368347168, loss=2.9100615978240967
I0201 09:16:58.965974 140023005427456 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.187645673751831, loss=2.9832186698913574
I0201 09:17:04.802504 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:17:15.487371 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:17:42.332210 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:17:43.934458 140184451094336 submission_runner.py:408] Time since start: 73275.14s, 	Step: 148314, 	{'train/accuracy': 0.8642382621765137, 'train/loss': 0.7342635989189148, 'validation/accuracy': 0.7673199772834778, 'validation/loss': 1.126471996307373, 'validation/num_examples': 50000, 'test/accuracy': 0.6473000049591064, 'test/loss': 1.7193629741668701, 'test/num_examples': 10000, 'score': 67676.07908463478, 'total_duration': 73275.14289355278, 'accumulated_submission_time': 67676.07908463478, 'accumulated_eval_time': 5585.072806835175, 'accumulated_logging_time': 6.381242990493774}
I0201 09:17:43.973871 140022518892288 logging_writer.py:48] [148314] accumulated_eval_time=5585.072807, accumulated_logging_time=6.381243, accumulated_submission_time=67676.079085, global_step=148314, preemption_count=0, score=67676.079085, test/accuracy=0.647300, test/loss=1.719363, test/num_examples=10000, total_duration=73275.142894, train/accuracy=0.864238, train/loss=0.734264, validation/accuracy=0.767320, validation/loss=1.126472, validation/num_examples=50000
I0201 09:18:19.404565 140023005427456 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.1296324729919434, loss=2.935586929321289
I0201 09:19:05.324818 140022518892288 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.1667051315307617, loss=2.941157102584839
I0201 09:19:51.651512 140023005427456 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.324312925338745, loss=3.9638636112213135
I0201 09:20:37.817550 140022518892288 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.2319624423980713, loss=2.850834369659424
I0201 09:21:24.063420 140023005427456 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.1093087196350098, loss=2.939361333847046
I0201 09:22:10.622422 140022518892288 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.0700700283050537, loss=3.249713659286499
I0201 09:22:56.550545 140023005427456 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.873457908630371, loss=4.374427795410156
I0201 09:23:42.665689 140022518892288 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.1822996139526367, loss=2.919846773147583
I0201 09:24:29.016589 140023005427456 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.23228120803833, loss=2.9595413208007812
I0201 09:24:44.305281 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:24:54.835548 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:25:19.943138 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:25:21.540938 140184451094336 submission_runner.py:408] Time since start: 73732.75s, 	Step: 149235, 	{'train/accuracy': 0.8697851300239563, 'train/loss': 0.7271069884300232, 'validation/accuracy': 0.7689200043678284, 'validation/loss': 1.1391234397888184, 'validation/num_examples': 50000, 'test/accuracy': 0.6455000042915344, 'test/loss': 1.7286360263824463, 'test/num_examples': 10000, 'score': 68096.35448336601, 'total_duration': 73732.74937844276, 'accumulated_submission_time': 68096.35448336601, 'accumulated_eval_time': 5622.308450460434, 'accumulated_logging_time': 6.429650783538818}
I0201 09:25:21.581294 140022518892288 logging_writer.py:48] [149235] accumulated_eval_time=5622.308450, accumulated_logging_time=6.429651, accumulated_submission_time=68096.354483, global_step=149235, preemption_count=0, score=68096.354483, test/accuracy=0.645500, test/loss=1.728636, test/num_examples=10000, total_duration=73732.749378, train/accuracy=0.869785, train/loss=0.727107, validation/accuracy=0.768920, validation/loss=1.139123, validation/num_examples=50000
I0201 09:25:47.970088 140023005427456 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.141792058944702, loss=2.834784507751465
I0201 09:26:33.397068 140022518892288 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.1368651390075684, loss=2.8563833236694336
I0201 09:27:19.670165 140023005427456 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.1432044506073, loss=2.954789400100708
I0201 09:28:06.147839 140022518892288 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.023669719696045, loss=2.950423002243042
I0201 09:28:51.995485 140023005427456 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.137197971343994, loss=2.9655673503875732
I0201 09:29:38.348075 140022518892288 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.0477473735809326, loss=4.452085018157959
I0201 09:30:24.355129 140023005427456 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.1705610752105713, loss=4.025277137756348
I0201 09:31:10.321676 140022518892288 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.2245569229125977, loss=3.5959134101867676
I0201 09:31:56.581599 140023005427456 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.1716320514678955, loss=2.924531936645508
I0201 09:32:21.891872 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:32:32.231738 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:33:04.158051 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:33:05.754878 140184451094336 submission_runner.py:408] Time since start: 74196.96s, 	Step: 150156, 	{'train/accuracy': 0.8738671541213989, 'train/loss': 0.7149843573570251, 'validation/accuracy': 0.7697599530220032, 'validation/loss': 1.141154170036316, 'validation/num_examples': 50000, 'test/accuracy': 0.650600016117096, 'test/loss': 1.7134649753570557, 'test/num_examples': 10000, 'score': 68516.60844302177, 'total_duration': 74196.96331691742, 'accumulated_submission_time': 68516.60844302177, 'accumulated_eval_time': 5666.171459674835, 'accumulated_logging_time': 6.480257034301758}
I0201 09:33:05.797738 140022518892288 logging_writer.py:48] [150156] accumulated_eval_time=5666.171460, accumulated_logging_time=6.480257, accumulated_submission_time=68516.608443, global_step=150156, preemption_count=0, score=68516.608443, test/accuracy=0.650600, test/loss=1.713465, test/num_examples=10000, total_duration=74196.963317, train/accuracy=0.873867, train/loss=0.714984, validation/accuracy=0.769760, validation/loss=1.141154, validation/num_examples=50000
I0201 09:33:23.801479 140023005427456 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.32499098777771, loss=4.025824546813965
I0201 09:34:07.643073 140022518892288 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.0404446125030518, loss=3.421093225479126
I0201 09:34:53.765713 140023005427456 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.050722122192383, loss=3.0647470951080322
I0201 09:35:40.016007 140022518892288 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.2724530696868896, loss=2.972712278366089
I0201 09:36:26.152591 140023005427456 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.149275541305542, loss=3.137145519256592
I0201 09:37:12.144000 140022518892288 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.092984199523926, loss=2.8814868927001953
I0201 09:37:58.274657 140023005427456 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.1711525917053223, loss=2.9696950912475586
I0201 09:38:44.347747 140022518892288 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.111898899078369, loss=2.989691972732544
I0201 09:39:30.512834 140023005427456 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.23569917678833, loss=3.1309258937835693
I0201 09:40:05.860949 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:40:17.003322 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:40:37.607078 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:40:39.208837 140184451094336 submission_runner.py:408] Time since start: 74650.42s, 	Step: 151078, 	{'train/accuracy': 0.8681445121765137, 'train/loss': 0.7366388440132141, 'validation/accuracy': 0.7703999876976013, 'validation/loss': 1.1382390260696411, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.7185348272323608, 'test/num_examples': 10000, 'score': 68936.61577987671, 'total_duration': 74650.41727089882, 'accumulated_submission_time': 68936.61577987671, 'accumulated_eval_time': 5699.519348621368, 'accumulated_logging_time': 6.532433748245239}
I0201 09:40:39.247678 140022518892288 logging_writer.py:48] [151078] accumulated_eval_time=5699.519349, accumulated_logging_time=6.532434, accumulated_submission_time=68936.615780, global_step=151078, preemption_count=0, score=68936.615780, test/accuracy=0.649900, test/loss=1.718535, test/num_examples=10000, total_duration=74650.417271, train/accuracy=0.868145, train/loss=0.736639, validation/accuracy=0.770400, validation/loss=1.138239, validation/num_examples=50000
I0201 09:40:48.452860 140023005427456 logging_writer.py:48] [151100] global_step=151100, grad_norm=1.9947799444198608, loss=3.101679801940918
I0201 09:41:30.926325 140022518892288 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.8450255393981934, loss=4.372375011444092
I0201 09:42:17.374734 140023005427456 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.4439334869384766, loss=3.800367832183838
I0201 09:43:04.063663 140022518892288 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.705291748046875, loss=4.316680431365967
I0201 09:43:50.316024 140023005427456 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.6371445655822754, loss=4.366281509399414
I0201 09:44:36.502249 140022518892288 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.1003522872924805, loss=3.2960617542266846
I0201 09:45:22.835602 140023005427456 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.2024521827697754, loss=2.943603754043579
I0201 09:46:09.008790 140022518892288 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.410607099533081, loss=2.935218572616577
I0201 09:46:55.242053 140023005427456 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.1205341815948486, loss=2.996635675430298
I0201 09:47:39.255555 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:47:49.591037 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:48:14.329194 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:48:15.935204 140184451094336 submission_runner.py:408] Time since start: 75107.14s, 	Step: 151997, 	{'train/accuracy': 0.8676952719688416, 'train/loss': 0.7144691348075867, 'validation/accuracy': 0.7712999582290649, 'validation/loss': 1.1138060092926025, 'validation/num_examples': 50000, 'test/accuracy': 0.6539000272750854, 'test/loss': 1.6868129968643188, 'test/num_examples': 10000, 'score': 69356.5645096302, 'total_duration': 75107.14363598824, 'accumulated_submission_time': 69356.5645096302, 'accumulated_eval_time': 5736.198989152908, 'accumulated_logging_time': 6.5838096141815186}
I0201 09:48:15.973818 140022518892288 logging_writer.py:48] [151997] accumulated_eval_time=5736.198989, accumulated_logging_time=6.583810, accumulated_submission_time=69356.564510, global_step=151997, preemption_count=0, score=69356.564510, test/accuracy=0.653900, test/loss=1.686813, test/num_examples=10000, total_duration=75107.143636, train/accuracy=0.867695, train/loss=0.714469, validation/accuracy=0.771300, validation/loss=1.113806, validation/num_examples=50000
I0201 09:48:17.586455 140023005427456 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.2339565753936768, loss=2.963067054748535
I0201 09:48:59.335212 140022518892288 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.1928532123565674, loss=2.840402126312256
I0201 09:49:45.260830 140023005427456 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.188483476638794, loss=3.0721864700317383
I0201 09:50:31.345860 140022518892288 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.147282123565674, loss=2.853522539138794
I0201 09:51:17.843346 140023005427456 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.166138172149658, loss=3.8444764614105225
I0201 09:52:03.892596 140022518892288 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.266357898712158, loss=3.0141711235046387
I0201 09:52:50.136189 140023005427456 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.2573418617248535, loss=2.900404930114746
I0201 09:53:36.199376 140022518892288 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.4542810916900635, loss=4.165727138519287
I0201 09:54:22.220589 140023005427456 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.4064645767211914, loss=4.078508377075195
I0201 09:55:08.528581 140022518892288 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.2924551963806152, loss=2.8682632446289062
I0201 09:55:16.027480 140184451094336 spec.py:321] Evaluating on the training split.
I0201 09:55:26.592819 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 09:55:54.752242 140184451094336 spec.py:349] Evaluating on the test split.
I0201 09:55:56.353306 140184451094336 submission_runner.py:408] Time since start: 75567.56s, 	Step: 152918, 	{'train/accuracy': 0.8729101419448853, 'train/loss': 0.6911082863807678, 'validation/accuracy': 0.772879958152771, 'validation/loss': 1.1157152652740479, 'validation/num_examples': 50000, 'test/accuracy': 0.6516000032424927, 'test/loss': 1.699005126953125, 'test/num_examples': 10000, 'score': 69776.55529689789, 'total_duration': 75567.5617249012, 'accumulated_submission_time': 69776.55529689789, 'accumulated_eval_time': 5776.524785995483, 'accumulated_logging_time': 6.637896299362183}
I0201 09:55:56.401397 140023005427456 logging_writer.py:48] [152918] accumulated_eval_time=5776.524786, accumulated_logging_time=6.637896, accumulated_submission_time=69776.555297, global_step=152918, preemption_count=0, score=69776.555297, test/accuracy=0.651600, test/loss=1.699005, test/num_examples=10000, total_duration=75567.561725, train/accuracy=0.872910, train/loss=0.691108, validation/accuracy=0.772880, validation/loss=1.115715, validation/num_examples=50000
I0201 09:56:30.105910 140022518892288 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.313009023666382, loss=2.8344380855560303
I0201 09:57:16.258624 140023005427456 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.1133432388305664, loss=2.7984347343444824
I0201 09:58:02.623871 140022518892288 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.2414534091949463, loss=2.8648056983947754
I0201 09:58:48.775287 140023005427456 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.5637283325195312, loss=4.114461898803711
I0201 09:59:34.796864 140022518892288 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.184528350830078, loss=2.903475522994995
I0201 10:00:21.279903 140023005427456 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.2246813774108887, loss=3.281651020050049
I0201 10:01:07.574451 140022518892288 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.204375982284546, loss=2.869797468185425
I0201 10:01:53.951080 140023005427456 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.360128879547119, loss=2.8682363033294678
I0201 10:02:40.353371 140022518892288 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.217935562133789, loss=3.165318727493286
I0201 10:02:56.681638 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:03:07.331807 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:03:33.171966 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:03:34.770501 140184451094336 submission_runner.py:408] Time since start: 76025.98s, 	Step: 153837, 	{'train/accuracy': 0.8698632717132568, 'train/loss': 0.7052730321884155, 'validation/accuracy': 0.7731999754905701, 'validation/loss': 1.1103754043579102, 'validation/num_examples': 50000, 'test/accuracy': 0.656000018119812, 'test/loss': 1.6826592683792114, 'test/num_examples': 10000, 'score': 70196.77774477005, 'total_duration': 76025.97893810272, 'accumulated_submission_time': 70196.77774477005, 'accumulated_eval_time': 5814.613633155823, 'accumulated_logging_time': 6.696936845779419}
I0201 10:03:34.813466 140023005427456 logging_writer.py:48] [153837] accumulated_eval_time=5814.613633, accumulated_logging_time=6.696937, accumulated_submission_time=70196.777745, global_step=153837, preemption_count=0, score=70196.777745, test/accuracy=0.656000, test/loss=1.682659, test/num_examples=10000, total_duration=76025.978938, train/accuracy=0.869863, train/loss=0.705273, validation/accuracy=0.773200, validation/loss=1.110375, validation/num_examples=50000
I0201 10:04:00.419717 140022518892288 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.227220296859741, loss=2.860297441482544
I0201 10:04:45.742528 140023005427456 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.3243722915649414, loss=2.8658130168914795
I0201 10:05:32.107704 140022518892288 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.1712169647216797, loss=3.6171274185180664
I0201 10:06:18.396972 140023005427456 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.2847180366516113, loss=2.7779266834259033
I0201 10:07:04.310560 140022518892288 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.247267007827759, loss=2.8925185203552246
I0201 10:07:50.469053 140023005427456 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.5966763496398926, loss=4.02232551574707
I0201 10:08:36.741290 140022518892288 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.5925040245056152, loss=3.9748337268829346
I0201 10:09:22.598549 140023005427456 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.1783621311187744, loss=3.262608051300049
I0201 10:10:08.617781 140022518892288 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.347968578338623, loss=3.8806397914886475
I0201 10:10:34.895407 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:10:45.485402 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:11:08.310325 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:11:09.916495 140184451094336 submission_runner.py:408] Time since start: 76481.12s, 	Step: 154759, 	{'train/accuracy': 0.8730077743530273, 'train/loss': 0.7196352481842041, 'validation/accuracy': 0.7730000019073486, 'validation/loss': 1.1262603998184204, 'validation/num_examples': 50000, 'test/accuracy': 0.6591000556945801, 'test/loss': 1.7006280422210693, 'test/num_examples': 10000, 'score': 70616.80341100693, 'total_duration': 76481.1249115467, 'accumulated_submission_time': 70616.80341100693, 'accumulated_eval_time': 5849.63468337059, 'accumulated_logging_time': 6.748908281326294}
I0201 10:11:09.965341 140023005427456 logging_writer.py:48] [154759] accumulated_eval_time=5849.634683, accumulated_logging_time=6.748908, accumulated_submission_time=70616.803411, global_step=154759, preemption_count=0, score=70616.803411, test/accuracy=0.659100, test/loss=1.700628, test/num_examples=10000, total_duration=76481.124912, train/accuracy=0.873008, train/loss=0.719635, validation/accuracy=0.773000, validation/loss=1.126260, validation/num_examples=50000
I0201 10:11:26.765360 140022518892288 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.199479579925537, loss=3.2457995414733887
I0201 10:12:10.628858 140023005427456 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.7673211097717285, loss=4.400836944580078
I0201 10:12:57.162551 140022518892288 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.222365379333496, loss=2.7978501319885254
I0201 10:13:43.958829 140023005427456 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.46748423576355, loss=3.519819736480713
I0201 10:14:30.139025 140022518892288 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.2266571521759033, loss=2.9429824352264404
I0201 10:15:16.332136 140023005427456 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.9451494216918945, loss=4.371480941772461
I0201 10:16:02.543381 140022518892288 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.617779493331909, loss=3.8578333854675293
I0201 10:16:48.698201 140023005427456 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.2689149379730225, loss=2.8662610054016113
I0201 10:17:34.894669 140022518892288 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.1448051929473877, loss=2.940969705581665
I0201 10:18:10.289669 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:18:20.889998 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:18:45.103225 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:18:46.706547 140184451094336 submission_runner.py:408] Time since start: 76937.91s, 	Step: 155678, 	{'train/accuracy': 0.8753125071525574, 'train/loss': 0.6879866719245911, 'validation/accuracy': 0.773419976234436, 'validation/loss': 1.109251618385315, 'validation/num_examples': 50000, 'test/accuracy': 0.6586000323295593, 'test/loss': 1.6903423070907593, 'test/num_examples': 10000, 'score': 71037.05806207657, 'total_duration': 76937.91496515274, 'accumulated_submission_time': 71037.05806207657, 'accumulated_eval_time': 5886.051543951035, 'accumulated_logging_time': 6.808360576629639}
I0201 10:18:46.752435 140023005427456 logging_writer.py:48] [155678] accumulated_eval_time=5886.051544, accumulated_logging_time=6.808361, accumulated_submission_time=71037.058062, global_step=155678, preemption_count=0, score=71037.058062, test/accuracy=0.658600, test/loss=1.690342, test/num_examples=10000, total_duration=76937.914965, train/accuracy=0.875313, train/loss=0.687987, validation/accuracy=0.773420, validation/loss=1.109252, validation/num_examples=50000
I0201 10:18:55.965125 140022518892288 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.1186461448669434, loss=2.9269065856933594
I0201 10:19:38.587204 140023005427456 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.514216184616089, loss=3.9414303302764893
I0201 10:20:24.970680 140022518892288 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.275602340698242, loss=3.1743524074554443
I0201 10:21:11.519437 140023005427456 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.520601749420166, loss=4.074719429016113
I0201 10:21:57.869745 140022518892288 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.1100497245788574, loss=3.044887065887451
I0201 10:22:44.047644 140023005427456 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.442277193069458, loss=2.878599166870117
I0201 10:23:30.533089 140022518892288 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.2825353145599365, loss=3.3122544288635254
I0201 10:24:16.804356 140023005427456 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.259474515914917, loss=2.7986843585968018
I0201 10:25:02.894113 140022518892288 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.5958852767944336, loss=3.9850380420684814
I0201 10:25:46.855457 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:25:57.239452 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:26:17.684392 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:26:19.286529 140184451094336 submission_runner.py:408] Time since start: 77390.49s, 	Step: 156597, 	{'train/accuracy': 0.87353515625, 'train/loss': 0.7132282257080078, 'validation/accuracy': 0.7741999626159668, 'validation/loss': 1.1228692531585693, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.6865030527114868, 'test/num_examples': 10000, 'score': 71457.10474681854, 'total_duration': 77390.49496340752, 'accumulated_submission_time': 71457.10474681854, 'accumulated_eval_time': 5918.482615470886, 'accumulated_logging_time': 6.863985776901245}
I0201 10:26:19.329870 140023005427456 logging_writer.py:48] [156597] accumulated_eval_time=5918.482615, accumulated_logging_time=6.863986, accumulated_submission_time=71457.104747, global_step=156597, preemption_count=0, score=71457.104747, test/accuracy=0.659800, test/loss=1.686503, test/num_examples=10000, total_duration=77390.494963, train/accuracy=0.873535, train/loss=0.713228, validation/accuracy=0.774200, validation/loss=1.122869, validation/num_examples=50000
I0201 10:26:20.937442 140022518892288 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.356818914413452, loss=3.7817537784576416
I0201 10:27:02.338108 140023005427456 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.1773838996887207, loss=4.373260974884033
I0201 10:27:48.099570 140022518892288 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.242382049560547, loss=2.8456294536590576
I0201 10:28:34.521140 140023005427456 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.246720790863037, loss=3.5971243381500244
I0201 10:29:20.557250 140022518892288 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.7928528785705566, loss=4.365970611572266
I0201 10:30:06.660062 140023005427456 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.1341676712036133, loss=2.7824008464813232
I0201 10:30:52.890210 140022518892288 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.2835419178009033, loss=2.8512349128723145
I0201 10:31:39.100449 140023005427456 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.4506025314331055, loss=3.821469783782959
I0201 10:32:25.959401 140022518892288 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.3067739009857178, loss=3.3906376361846924
I0201 10:33:12.010213 140023005427456 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.4181320667266846, loss=3.4956917762756348
I0201 10:33:19.594691 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:33:29.897647 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:33:51.137506 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:33:52.737772 140184451094336 submission_runner.py:408] Time since start: 77843.95s, 	Step: 157518, 	{'train/accuracy': 0.87548828125, 'train/loss': 0.706649124622345, 'validation/accuracy': 0.774619996547699, 'validation/loss': 1.1308605670928955, 'validation/num_examples': 50000, 'test/accuracy': 0.6622000336647034, 'test/loss': 1.6971155405044556, 'test/num_examples': 10000, 'score': 71877.31110310555, 'total_duration': 77843.94621014595, 'accumulated_submission_time': 71877.31110310555, 'accumulated_eval_time': 5951.625692844391, 'accumulated_logging_time': 6.918470144271851}
I0201 10:33:52.779158 140022518892288 logging_writer.py:48] [157518] accumulated_eval_time=5951.625693, accumulated_logging_time=6.918470, accumulated_submission_time=71877.311103, global_step=157518, preemption_count=0, score=71877.311103, test/accuracy=0.662200, test/loss=1.697116, test/num_examples=10000, total_duration=77843.946210, train/accuracy=0.875488, train/loss=0.706649, validation/accuracy=0.774620, validation/loss=1.130861, validation/num_examples=50000
I0201 10:34:26.226840 140023005427456 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.2307727336883545, loss=3.1227216720581055
I0201 10:35:12.623032 140022518892288 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.3617143630981445, loss=3.7999753952026367
I0201 10:35:58.694267 140023005427456 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.30157470703125, loss=2.8024489879608154
I0201 10:36:45.357962 140022518892288 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.074599027633667, loss=4.344609260559082
I0201 10:37:31.439831 140023005427456 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.131223678588867, loss=3.5473177433013916
I0201 10:38:17.539053 140022518892288 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.321291208267212, loss=2.9176933765411377
I0201 10:39:03.753371 140023005427456 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.168337821960449, loss=4.320278644561768
I0201 10:39:50.086071 140022518892288 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.5026121139526367, loss=2.863194465637207
I0201 10:40:36.271946 140023005427456 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.6046180725097656, loss=3.824448585510254
I0201 10:40:53.009138 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:41:03.715409 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:41:25.707757 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:41:27.312883 140184451094336 submission_runner.py:408] Time since start: 78298.52s, 	Step: 158438, 	{'train/accuracy': 0.8758202791213989, 'train/loss': 0.6879441738128662, 'validation/accuracy': 0.7753399610519409, 'validation/loss': 1.1054383516311646, 'validation/num_examples': 50000, 'test/accuracy': 0.6605000495910645, 'test/loss': 1.6729251146316528, 'test/num_examples': 10000, 'score': 72297.48312687874, 'total_duration': 78298.52132201195, 'accumulated_submission_time': 72297.48312687874, 'accumulated_eval_time': 5985.92941904068, 'accumulated_logging_time': 6.971019268035889}
I0201 10:41:27.352526 140022518892288 logging_writer.py:48] [158438] accumulated_eval_time=5985.929419, accumulated_logging_time=6.971019, accumulated_submission_time=72297.483127, global_step=158438, preemption_count=0, score=72297.483127, test/accuracy=0.660500, test/loss=1.672925, test/num_examples=10000, total_duration=78298.521322, train/accuracy=0.875820, train/loss=0.687944, validation/accuracy=0.775340, validation/loss=1.105438, validation/num_examples=50000
I0201 10:41:52.575380 140023005427456 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.9761061668395996, loss=4.300816535949707
I0201 10:42:37.629762 140022518892288 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.309154510498047, loss=3.0504865646362305
I0201 10:43:24.148002 140023005427456 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.2649152278900146, loss=2.916199207305908
I0201 10:44:10.565256 140022518892288 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.2934818267822266, loss=2.806765556335449
I0201 10:44:56.754059 140023005427456 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.45508074760437, loss=2.840494155883789
I0201 10:45:42.981650 140022518892288 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.2117693424224854, loss=2.787802219390869
I0201 10:46:29.450569 140023005427456 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.612977981567383, loss=3.910552978515625
I0201 10:47:15.556383 140022518892288 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.804253339767456, loss=4.166006088256836
I0201 10:48:01.898866 140023005427456 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.2867894172668457, loss=2.8527097702026367
I0201 10:48:27.508572 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:48:38.023405 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:48:59.561227 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:49:01.162213 140184451094336 submission_runner.py:408] Time since start: 78752.37s, 	Step: 159357, 	{'train/accuracy': 0.8858202695846558, 'train/loss': 0.6633342504501343, 'validation/accuracy': 0.7765199542045593, 'validation/loss': 1.1071991920471191, 'validation/num_examples': 50000, 'test/accuracy': 0.6556000113487244, 'test/loss': 1.6883338689804077, 'test/num_examples': 10000, 'score': 72717.58166861534, 'total_duration': 78752.3706395626, 'accumulated_submission_time': 72717.58166861534, 'accumulated_eval_time': 6019.583042383194, 'accumulated_logging_time': 7.0212483406066895}
I0201 10:49:01.201496 140022518892288 logging_writer.py:48] [159357] accumulated_eval_time=6019.583042, accumulated_logging_time=7.021248, accumulated_submission_time=72717.581669, global_step=159357, preemption_count=0, score=72717.581669, test/accuracy=0.655600, test/loss=1.688334, test/num_examples=10000, total_duration=78752.370640, train/accuracy=0.885820, train/loss=0.663334, validation/accuracy=0.776520, validation/loss=1.107199, validation/num_examples=50000
I0201 10:49:18.956370 140023005427456 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.39742374420166, loss=2.890197277069092
I0201 10:50:02.774069 140022518892288 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.356400966644287, loss=2.9091603755950928
I0201 10:50:48.871298 140023005427456 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.3820343017578125, loss=2.8500828742980957
I0201 10:51:35.607445 140022518892288 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.3971543312072754, loss=3.5424859523773193
I0201 10:52:21.826291 140023005427456 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.340703248977661, loss=3.568218469619751
I0201 10:53:08.134802 140022518892288 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.2969634532928467, loss=2.8020858764648438
I0201 10:53:54.151758 140023005427456 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.9527056217193604, loss=4.130988597869873
I0201 10:54:40.306380 140022518892288 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.3956997394561768, loss=2.8371691703796387
I0201 10:55:26.611054 140023005427456 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.476300001144409, loss=2.8456497192382812
I0201 10:56:01.231431 140184451094336 spec.py:321] Evaluating on the training split.
I0201 10:56:11.669709 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 10:56:36.006932 140184451094336 spec.py:349] Evaluating on the test split.
I0201 10:56:37.610511 140184451094336 submission_runner.py:408] Time since start: 79208.82s, 	Step: 160277, 	{'train/accuracy': 0.8758593797683716, 'train/loss': 0.6875196099281311, 'validation/accuracy': 0.776479959487915, 'validation/loss': 1.1003152132034302, 'validation/num_examples': 50000, 'test/accuracy': 0.6580000519752502, 'test/loss': 1.683222770690918, 'test/num_examples': 10000, 'score': 73137.55405020714, 'total_duration': 79208.81894946098, 'accumulated_submission_time': 73137.55405020714, 'accumulated_eval_time': 6055.962126255035, 'accumulated_logging_time': 7.0706892013549805}
I0201 10:56:37.654619 140022518892288 logging_writer.py:48] [160277] accumulated_eval_time=6055.962126, accumulated_logging_time=7.070689, accumulated_submission_time=73137.554050, global_step=160277, preemption_count=0, score=73137.554050, test/accuracy=0.658000, test/loss=1.683223, test/num_examples=10000, total_duration=79208.818949, train/accuracy=0.875859, train/loss=0.687520, validation/accuracy=0.776480, validation/loss=1.100315, validation/num_examples=50000
I0201 10:56:47.255244 140023005427456 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.4040677547454834, loss=2.860675811767578
I0201 10:57:29.905000 140022518892288 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.3666906356811523, loss=2.839111804962158
I0201 10:58:16.361951 140023005427456 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.357590675354004, loss=2.8325963020324707
I0201 10:59:02.617738 140022518892288 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.3915321826934814, loss=2.877354860305786
I0201 10:59:48.675358 140023005427456 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.4503636360168457, loss=2.835503339767456
I0201 11:00:34.795242 140022518892288 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.3591983318328857, loss=3.6666007041931152
I0201 11:01:20.878428 140023005427456 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.279548168182373, loss=3.2334601879119873
I0201 11:02:07.024099 140022518892288 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.480062484741211, loss=3.680520534515381
I0201 11:02:53.005316 140023005427456 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.3433735370635986, loss=2.976095199584961
I0201 11:03:37.899240 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:03:48.370967 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:04:14.252652 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:04:15.852518 140184451094336 submission_runner.py:408] Time since start: 79667.06s, 	Step: 161199, 	{'train/accuracy': 0.8811913728713989, 'train/loss': 0.671556293964386, 'validation/accuracy': 0.7778199911117554, 'validation/loss': 1.1007194519042969, 'validation/num_examples': 50000, 'test/accuracy': 0.6577000021934509, 'test/loss': 1.6750051975250244, 'test/num_examples': 10000, 'score': 73557.74100780487, 'total_duration': 79667.06095504761, 'accumulated_submission_time': 73557.74100780487, 'accumulated_eval_time': 6093.915406227112, 'accumulated_logging_time': 7.124929904937744}
I0201 11:04:15.893297 140022518892288 logging_writer.py:48] [161199] accumulated_eval_time=6093.915406, accumulated_logging_time=7.124930, accumulated_submission_time=73557.741008, global_step=161199, preemption_count=0, score=73557.741008, test/accuracy=0.657700, test/loss=1.675005, test/num_examples=10000, total_duration=79667.060955, train/accuracy=0.881191, train/loss=0.671556, validation/accuracy=0.777820, validation/loss=1.100719, validation/num_examples=50000
I0201 11:04:16.696202 140023005427456 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.2469069957733154, loss=3.1503889560699463
I0201 11:04:58.314431 140022518892288 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.488588333129883, loss=2.8317277431488037
I0201 11:05:44.885654 140023005427456 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.5027213096618652, loss=3.567850112915039
I0201 11:06:31.486752 140022518892288 logging_writer.py:48] [161500] global_step=161500, grad_norm=3.6338467597961426, loss=3.9965524673461914
I0201 11:07:17.992290 140023005427456 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.189452886581421, loss=2.795701026916504
I0201 11:08:04.070888 140022518892288 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.381131649017334, loss=3.3838906288146973
I0201 11:08:50.560554 140023005427456 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.3304269313812256, loss=4.360560417175293
I0201 11:09:36.688163 140022518892288 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.2958850860595703, loss=3.21250057220459
I0201 11:10:22.788675 140023005427456 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.713603973388672, loss=3.9228997230529785
I0201 11:11:09.134566 140022518892288 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.5428483486175537, loss=3.118927001953125
I0201 11:11:16.213056 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:11:26.674350 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:11:43.943069 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:11:45.547754 140184451094336 submission_runner.py:408] Time since start: 80116.76s, 	Step: 162117, 	{'train/accuracy': 0.8856250047683716, 'train/loss': 0.6517072319984436, 'validation/accuracy': 0.7775999903678894, 'validation/loss': 1.0972752571105957, 'validation/num_examples': 50000, 'test/accuracy': 0.6593000292778015, 'test/loss': 1.6683248281478882, 'test/num_examples': 10000, 'score': 73978.00402450562, 'total_duration': 80116.75618052483, 'accumulated_submission_time': 73978.00402450562, 'accumulated_eval_time': 6123.2500858306885, 'accumulated_logging_time': 7.175060510635376}
I0201 11:11:45.596640 140023005427456 logging_writer.py:48] [162117] accumulated_eval_time=6123.250086, accumulated_logging_time=7.175061, accumulated_submission_time=73978.004025, global_step=162117, preemption_count=0, score=73978.004025, test/accuracy=0.659300, test/loss=1.668325, test/num_examples=10000, total_duration=80116.756181, train/accuracy=0.885625, train/loss=0.651707, validation/accuracy=0.777600, validation/loss=1.097275, validation/num_examples=50000
I0201 11:12:20.486098 140022518892288 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.261105537414551, loss=2.883077621459961
I0201 11:13:06.618164 140023005427456 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.2206814289093018, loss=3.0629451274871826
I0201 11:13:52.946759 140022518892288 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.2293217182159424, loss=2.7558414936065674
I0201 11:14:39.071614 140023005427456 logging_writer.py:48] [162500] global_step=162500, grad_norm=3.299588441848755, loss=4.281852722167969
I0201 11:15:25.255727 140022518892288 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.1688170433044434, loss=3.229539155960083
I0201 11:16:11.727663 140023005427456 logging_writer.py:48] [162700] global_step=162700, grad_norm=3.0462334156036377, loss=4.039996147155762
I0201 11:16:57.826029 140022518892288 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.233389139175415, loss=3.2638213634490967
I0201 11:17:44.020991 140023005427456 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.467107057571411, loss=2.8070573806762695
I0201 11:18:30.447609 140022518892288 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.4489352703094482, loss=3.7979347705841064
I0201 11:18:45.897027 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:18:56.254367 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:19:20.272082 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:19:21.875217 140184451094336 submission_runner.py:408] Time since start: 80573.08s, 	Step: 163035, 	{'train/accuracy': 0.8808007836341858, 'train/loss': 0.6864597797393799, 'validation/accuracy': 0.7800799608230591, 'validation/loss': 1.105568766593933, 'validation/num_examples': 50000, 'test/accuracy': 0.6591000556945801, 'test/loss': 1.6852294206619263, 'test/num_examples': 10000, 'score': 74398.24773645401, 'total_duration': 80573.08365154266, 'accumulated_submission_time': 74398.24773645401, 'accumulated_eval_time': 6159.228252887726, 'accumulated_logging_time': 7.233767747879028}
I0201 11:19:21.919418 140023005427456 logging_writer.py:48] [163035] accumulated_eval_time=6159.228253, accumulated_logging_time=7.233768, accumulated_submission_time=74398.247736, global_step=163035, preemption_count=0, score=74398.247736, test/accuracy=0.659100, test/loss=1.685229, test/num_examples=10000, total_duration=80573.083652, train/accuracy=0.880801, train/loss=0.686460, validation/accuracy=0.780080, validation/loss=1.105569, validation/num_examples=50000
I0201 11:19:48.325221 140022518892288 logging_writer.py:48] [163100] global_step=163100, grad_norm=3.14136004447937, loss=4.375020980834961
I0201 11:20:33.512716 140023005427456 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.4163198471069336, loss=2.835814952850342
I0201 11:21:19.744466 140022518892288 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.3984909057617188, loss=3.475543737411499
I0201 11:22:06.255868 140023005427456 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.473834276199341, loss=3.104891061782837
I0201 11:22:52.096998 140022518892288 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.4555721282958984, loss=2.783278465270996
I0201 11:23:38.533452 140023005427456 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.458448886871338, loss=2.765289068222046
I0201 11:24:24.892304 140022518892288 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.3528196811676025, loss=2.969052314758301
I0201 11:25:11.121542 140023005427456 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.080488443374634, loss=4.26697301864624
I0201 11:25:57.473897 140022518892288 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.0483155250549316, loss=4.170564651489258
I0201 11:26:22.205808 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:26:32.413769 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:27:00.892662 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:27:02.512362 140184451094336 submission_runner.py:408] Time since start: 81033.72s, 	Step: 163955, 	{'train/accuracy': 0.8822265267372131, 'train/loss': 0.6664526462554932, 'validation/accuracy': 0.7804200053215027, 'validation/loss': 1.0926640033721924, 'validation/num_examples': 50000, 'test/accuracy': 0.6650000214576721, 'test/loss': 1.66836416721344, 'test/num_examples': 10000, 'score': 74818.47779989243, 'total_duration': 81033.72079610825, 'accumulated_submission_time': 74818.47779989243, 'accumulated_eval_time': 6199.534796953201, 'accumulated_logging_time': 7.287166118621826}
I0201 11:27:02.552865 140023005427456 logging_writer.py:48] [163955] accumulated_eval_time=6199.534797, accumulated_logging_time=7.287166, accumulated_submission_time=74818.477800, global_step=163955, preemption_count=0, score=74818.477800, test/accuracy=0.665000, test/loss=1.668364, test/num_examples=10000, total_duration=81033.720796, train/accuracy=0.882227, train/loss=0.666453, validation/accuracy=0.780420, validation/loss=1.092664, validation/num_examples=50000
I0201 11:27:20.953248 140022518892288 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.4025766849517822, loss=2.7736599445343018
I0201 11:28:04.390765 140023005427456 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.308128595352173, loss=2.7727205753326416
I0201 11:28:50.727070 140022518892288 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.8564507961273193, loss=3.01363468170166
I0201 11:29:37.200941 140023005427456 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.7337310314178467, loss=3.8172242641448975
I0201 11:30:23.413936 140022518892288 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.509357213973999, loss=3.0821800231933594
I0201 11:31:09.818947 140023005427456 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.898437023162842, loss=3.733367681503296
I0201 11:31:56.007923 140022518892288 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.436964988708496, loss=2.71701979637146
I0201 11:32:41.942881 140023005427456 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.5557172298431396, loss=3.4001810550689697
I0201 11:33:27.997914 140022518892288 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.4817025661468506, loss=2.8296444416046143
I0201 11:34:02.615201 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:34:13.179945 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:34:34.320764 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:34:35.932970 140184451094336 submission_runner.py:408] Time since start: 81487.14s, 	Step: 164877, 	{'train/accuracy': 0.88636714220047, 'train/loss': 0.653562068939209, 'validation/accuracy': 0.7805599570274353, 'validation/loss': 1.0905064344406128, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.6655395030975342, 'test/num_examples': 10000, 'score': 75238.48033547401, 'total_duration': 81487.14139032364, 'accumulated_submission_time': 75238.48033547401, 'accumulated_eval_time': 6232.852535486221, 'accumulated_logging_time': 7.340670824050903}
I0201 11:34:35.979895 140023005427456 logging_writer.py:48] [164877] accumulated_eval_time=6232.852535, accumulated_logging_time=7.340671, accumulated_submission_time=75238.480335, global_step=164877, preemption_count=0, score=75238.480335, test/accuracy=0.659800, test/loss=1.665540, test/num_examples=10000, total_duration=81487.141390, train/accuracy=0.886367, train/loss=0.653562, validation/accuracy=0.780560, validation/loss=1.090506, validation/num_examples=50000
I0201 11:34:45.598645 140022518892288 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.4285643100738525, loss=3.2173190116882324
I0201 11:35:28.034342 140023005427456 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.3653595447540283, loss=2.887333869934082
I0201 11:36:14.319959 140022518892288 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.296797275543213, loss=2.8917713165283203
I0201 11:37:00.812678 140023005427456 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.6631710529327393, loss=2.8915789127349854
I0201 11:37:46.899749 140022518892288 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.4728543758392334, loss=2.8114497661590576
I0201 11:38:33.009241 140023005427456 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.8643651008605957, loss=3.7089011669158936
I0201 11:39:19.248533 140022518892288 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.6662890911102295, loss=2.884459972381592
I0201 11:40:05.369521 140023005427456 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.4942562580108643, loss=3.6446280479431152
I0201 11:40:51.369701 140022518892288 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.002079486846924, loss=4.1678996086120605
I0201 11:41:36.004531 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:41:46.307987 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:42:14.505912 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:42:16.115442 140184451094336 submission_runner.py:408] Time since start: 81947.32s, 	Step: 165798, 	{'train/accuracy': 0.8838866949081421, 'train/loss': 0.6820747256278992, 'validation/accuracy': 0.7806800007820129, 'validation/loss': 1.1070207357406616, 'validation/num_examples': 50000, 'test/accuracy': 0.6610000133514404, 'test/loss': 1.6883653402328491, 'test/num_examples': 10000, 'score': 75658.44529294968, 'total_duration': 81947.32384061813, 'accumulated_submission_time': 75658.44529294968, 'accumulated_eval_time': 6272.963407754898, 'accumulated_logging_time': 7.399810791015625}
I0201 11:42:16.166176 140023005427456 logging_writer.py:48] [165798] accumulated_eval_time=6272.963408, accumulated_logging_time=7.399811, accumulated_submission_time=75658.445293, global_step=165798, preemption_count=0, score=75658.445293, test/accuracy=0.661000, test/loss=1.688365, test/num_examples=10000, total_duration=81947.323841, train/accuracy=0.883887, train/loss=0.682075, validation/accuracy=0.780680, validation/loss=1.107021, validation/num_examples=50000
I0201 11:42:17.379463 140022518892288 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.6038191318511963, loss=3.1341090202331543
I0201 11:42:58.873239 140023005427456 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.5131945610046387, loss=2.785768508911133
I0201 11:43:44.956197 140022518892288 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.4383604526519775, loss=2.8170390129089355
I0201 11:44:31.038567 140023005427456 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.651153326034546, loss=2.828746795654297
I0201 11:45:17.274709 140022518892288 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.3984322547912598, loss=3.425102710723877
I0201 11:46:03.251790 140023005427456 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.8873119354248047, loss=3.477199077606201
I0201 11:46:49.790706 140022518892288 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.410529613494873, loss=3.061403274536133
I0201 11:47:35.904624 140023005427456 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.5698461532592773, loss=2.717191219329834
I0201 11:48:22.119143 140022518892288 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.955843448638916, loss=4.194937229156494
I0201 11:49:08.208050 140023005427456 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.349787473678589, loss=2.829023838043213
I0201 11:49:16.211132 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:49:26.591021 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:49:45.264946 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:49:46.872580 140184451094336 submission_runner.py:408] Time since start: 82398.08s, 	Step: 166719, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.640555739402771, 'validation/accuracy': 0.7835800051689148, 'validation/loss': 1.0742700099945068, 'validation/num_examples': 50000, 'test/accuracy': 0.6648000478744507, 'test/loss': 1.6466233730316162, 'test/num_examples': 10000, 'score': 76078.42911958694, 'total_duration': 82398.08100652695, 'accumulated_submission_time': 76078.42911958694, 'accumulated_eval_time': 6303.624835968018, 'accumulated_logging_time': 7.464045286178589}
I0201 11:49:46.925763 140022518892288 logging_writer.py:48] [166719] accumulated_eval_time=6303.624836, accumulated_logging_time=7.464045, accumulated_submission_time=76078.429120, global_step=166719, preemption_count=0, score=76078.429120, test/accuracy=0.664800, test/loss=1.646623, test/num_examples=10000, total_duration=82398.081007, train/accuracy=0.887422, train/loss=0.640556, validation/accuracy=0.783580, validation/loss=1.074270, validation/num_examples=50000
I0201 11:50:20.437034 140023005427456 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.5177905559539795, loss=2.863175630569458
I0201 11:51:06.300050 140022518892288 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.7023563385009766, loss=2.80324387550354
I0201 11:51:52.547005 140023005427456 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.528022050857544, loss=2.7773489952087402
I0201 11:52:38.814405 140022518892288 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.6195387840270996, loss=2.937690019607544
I0201 11:53:24.704493 140023005427456 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.4185807704925537, loss=3.2156851291656494
I0201 11:54:10.896414 140022518892288 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.8798348903656006, loss=3.7767419815063477
I0201 11:54:56.989088 140023005427456 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.5341033935546875, loss=2.80747127532959
I0201 11:55:43.021291 140022518892288 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.767401695251465, loss=3.6536355018615723
I0201 11:56:29.123639 140023005427456 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.417738199234009, loss=2.7489020824432373
I0201 11:56:47.267656 140184451094336 spec.py:321] Evaluating on the training split.
I0201 11:56:57.821126 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 11:57:19.936699 140184451094336 spec.py:349] Evaluating on the test split.
I0201 11:57:21.541330 140184451094336 submission_runner.py:408] Time since start: 82852.75s, 	Step: 167641, 	{'train/accuracy': 0.8880468606948853, 'train/loss': 0.6393408179283142, 'validation/accuracy': 0.7837599515914917, 'validation/loss': 1.0733433961868286, 'validation/num_examples': 50000, 'test/accuracy': 0.6659000515937805, 'test/loss': 1.6449536085128784, 'test/num_examples': 10000, 'score': 76498.71252512932, 'total_duration': 82852.74976229668, 'accumulated_submission_time': 76498.71252512932, 'accumulated_eval_time': 6337.89848613739, 'accumulated_logging_time': 7.528738975524902}
I0201 11:57:21.582211 140022518892288 logging_writer.py:48] [167641] accumulated_eval_time=6337.898486, accumulated_logging_time=7.528739, accumulated_submission_time=76498.712525, global_step=167641, preemption_count=0, score=76498.712525, test/accuracy=0.665900, test/loss=1.644954, test/num_examples=10000, total_duration=82852.749762, train/accuracy=0.888047, train/loss=0.639341, validation/accuracy=0.783760, validation/loss=1.073343, validation/num_examples=50000
I0201 11:57:45.587929 140023005427456 logging_writer.py:48] [167700] global_step=167700, grad_norm=2.289001226425171, loss=3.2663846015930176
I0201 11:58:30.127696 140022518892288 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.5673165321350098, loss=2.8191370964050293
I0201 11:59:16.408361 140023005427456 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.2727859020233154, loss=2.714435577392578
I0201 12:00:02.953700 140022518892288 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.445223569869995, loss=2.6717031002044678
I0201 12:00:48.816789 140023005427456 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.3803629875183105, loss=2.7827346324920654
I0201 12:01:35.187633 140022518892288 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.544431209564209, loss=2.7897517681121826
I0201 12:02:21.320626 140023005427456 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.503408908843994, loss=3.2173643112182617
I0201 12:03:07.330253 140022518892288 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.569786310195923, loss=3.346142292022705
I0201 12:03:53.440640 140023005427456 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.366640329360962, loss=3.360611915588379
I0201 12:04:21.734227 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:04:32.210597 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:04:55.774242 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:04:57.370102 140184451094336 submission_runner.py:408] Time since start: 83308.58s, 	Step: 168563, 	{'train/accuracy': 0.8868163824081421, 'train/loss': 0.655565083026886, 'validation/accuracy': 0.7830599546432495, 'validation/loss': 1.089125394821167, 'validation/num_examples': 50000, 'test/accuracy': 0.6633000373840332, 'test/loss': 1.6611697673797607, 'test/num_examples': 10000, 'score': 76918.80696439743, 'total_duration': 83308.578540802, 'accumulated_submission_time': 76918.80696439743, 'accumulated_eval_time': 6373.534379959106, 'accumulated_logging_time': 7.579162836074829}
I0201 12:04:57.411326 140022518892288 logging_writer.py:48] [168563] accumulated_eval_time=6373.534380, accumulated_logging_time=7.579163, accumulated_submission_time=76918.806964, global_step=168563, preemption_count=0, score=76918.806964, test/accuracy=0.663300, test/loss=1.661170, test/num_examples=10000, total_duration=83308.578541, train/accuracy=0.886816, train/loss=0.655565, validation/accuracy=0.783060, validation/loss=1.089125, validation/num_examples=50000
I0201 12:05:12.631459 140023005427456 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.4907073974609375, loss=2.7456722259521484
I0201 12:05:55.793938 140022518892288 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.4871442317962646, loss=2.7572999000549316
I0201 12:06:42.206234 140023005427456 logging_writer.py:48] [168800] global_step=168800, grad_norm=2.6055784225463867, loss=2.7635490894317627
I0201 12:07:29.007442 140022518892288 logging_writer.py:48] [168900] global_step=168900, grad_norm=2.6346893310546875, loss=2.7788314819335938
I0201 12:08:14.926732 140023005427456 logging_writer.py:48] [169000] global_step=169000, grad_norm=2.5243067741394043, loss=2.7769291400909424
I0201 12:09:00.981005 140022518892288 logging_writer.py:48] [169100] global_step=169100, grad_norm=2.733947992324829, loss=4.072195529937744
I0201 12:09:47.148892 140023005427456 logging_writer.py:48] [169200] global_step=169200, grad_norm=2.6403982639312744, loss=2.9256608486175537
I0201 12:10:33.035751 140022518892288 logging_writer.py:48] [169300] global_step=169300, grad_norm=2.69435715675354, loss=3.4638729095458984
I0201 12:11:19.246421 140023005427456 logging_writer.py:48] [169400] global_step=169400, grad_norm=2.5149905681610107, loss=3.523782968521118
I0201 12:11:57.469487 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:12:07.966294 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:12:34.603059 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:12:36.206790 140184451094336 submission_runner.py:408] Time since start: 83767.42s, 	Step: 169484, 	{'train/accuracy': 0.8880468606948853, 'train/loss': 0.6424294710159302, 'validation/accuracy': 0.7831000089645386, 'validation/loss': 1.0735559463500977, 'validation/num_examples': 50000, 'test/accuracy': 0.6653000116348267, 'test/loss': 1.646395206451416, 'test/num_examples': 10000, 'score': 77338.80617928505, 'total_duration': 83767.41523122787, 'accumulated_submission_time': 77338.80617928505, 'accumulated_eval_time': 6412.271682739258, 'accumulated_logging_time': 7.63238000869751}
I0201 12:12:36.248150 140022518892288 logging_writer.py:48] [169484] accumulated_eval_time=6412.271683, accumulated_logging_time=7.632380, accumulated_submission_time=77338.806179, global_step=169484, preemption_count=0, score=77338.806179, test/accuracy=0.665300, test/loss=1.646395, test/num_examples=10000, total_duration=83767.415231, train/accuracy=0.888047, train/loss=0.642429, validation/accuracy=0.783100, validation/loss=1.073556, validation/num_examples=50000
I0201 12:12:43.046713 140023005427456 logging_writer.py:48] [169500] global_step=169500, grad_norm=2.6494994163513184, loss=2.839970111846924
I0201 12:13:25.278715 140022518892288 logging_writer.py:48] [169600] global_step=169600, grad_norm=2.6122143268585205, loss=2.816149950027466
I0201 12:14:11.927981 140023005427456 logging_writer.py:48] [169700] global_step=169700, grad_norm=2.4339096546173096, loss=2.7638792991638184
I0201 12:14:58.308859 140022518892288 logging_writer.py:48] [169800] global_step=169800, grad_norm=2.487786293029785, loss=2.7431159019470215
I0201 12:15:37.747898 140023005427456 logging_writer.py:48] [169887] global_step=169887, preemption_count=0, score=77520.216902
I0201 12:15:38.427073 140184451094336 checkpoints.py:490] Saving checkpoint at step: 169887
I0201 12:15:39.725465 140184451094336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2/checkpoint_169887
I0201 12:15:39.746533 140184451094336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_2/checkpoint_169887.
I0201 12:15:40.550479 140184451094336 submission_runner.py:583] Tuning trial 2/5
I0201 12:15:40.550715 140184451094336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0201 12:15:40.558742 140184451094336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008593749953433871, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 34.00022578239441, 'total_duration': 64.13885116577148, 'accumulated_submission_time': 34.00022578239441, 'accumulated_eval_time': 30.138538599014282, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (864, {'train/accuracy': 0.01708984375, 'train/loss': 6.374186038970947, 'validation/accuracy': 0.016920000314712524, 'validation/loss': 6.388251781463623, 'validation/num_examples': 50000, 'test/accuracy': 0.012700000777840614, 'test/loss': 6.433145999908447, 'test/num_examples': 10000, 'score': 453.9772663116455, 'total_duration': 519.8541326522827, 'accumulated_submission_time': 453.9772663116455, 'accumulated_eval_time': 65.81416869163513, 'accumulated_logging_time': 0.01839923858642578, 'global_step': 864, 'preemption_count': 0}), (1785, {'train/accuracy': 0.05035156011581421, 'train/loss': 5.795556545257568, 'validation/accuracy': 0.04853999987244606, 'validation/loss': 5.824850559234619, 'validation/num_examples': 50000, 'test/accuracy': 0.03680000081658363, 'test/loss': 5.947690486907959, 'test/num_examples': 10000, 'score': 874.0901634693146, 'total_duration': 979.0125117301941, 'accumulated_submission_time': 874.0901634693146, 'accumulated_eval_time': 104.78045845031738, 'accumulated_logging_time': 0.05016160011291504, 'global_step': 1785, 'preemption_count': 0}), (2708, {'train/accuracy': 0.07275390625, 'train/loss': 5.425717830657959, 'validation/accuracy': 0.06825999915599823, 'validation/loss': 5.4650092124938965, 'validation/num_examples': 50000, 'test/accuracy': 0.05020000413060188, 'test/loss': 5.638961315155029, 'test/num_examples': 10000, 'score': 1294.3986456394196, 'total_duration': 1438.6663777828217, 'accumulated_submission_time': 1294.3986456394196, 'accumulated_eval_time': 144.04840278625488, 'accumulated_logging_time': 0.07940435409545898, 'global_step': 2708, 'preemption_count': 0}), (3630, {'train/accuracy': 0.10914061963558197, 'train/loss': 5.063491344451904, 'validation/accuracy': 0.09845999628305435, 'validation/loss': 5.12637186050415, 'validation/num_examples': 50000, 'test/accuracy': 0.07569999992847443, 'test/loss': 5.355987071990967, 'test/num_examples': 10000, 'score': 1714.635325908661, 'total_duration': 1898.3418984413147, 'accumulated_submission_time': 1714.635325908661, 'accumulated_eval_time': 183.41586256027222, 'accumulated_logging_time': 0.10338544845581055, 'global_step': 3630, 'preemption_count': 0}), (4552, {'train/accuracy': 0.16447265446186066, 'train/loss': 4.604984760284424, 'validation/accuracy': 0.14311999082565308, 'validation/loss': 4.7212934494018555, 'validation/num_examples': 50000, 'test/accuracy': 0.10650000721216202, 'test/loss': 5.0176777839660645, 'test/num_examples': 10000, 'score': 2134.942580461502, 'total_duration': 2353.8033304214478, 'accumulated_submission_time': 2134.942580461502, 'accumulated_eval_time': 218.49655675888062, 'accumulated_logging_time': 0.12989163398742676, 'global_step': 4552, 'preemption_count': 0}), (5473, {'train/accuracy': 0.19630858302116394, 'train/loss': 4.357907772064209, 'validation/accuracy': 0.17927999794483185, 'validation/loss': 4.431841850280762, 'validation/num_examples': 50000, 'test/accuracy': 0.13819999992847443, 'test/loss': 4.75654411315918, 'test/num_examples': 10000, 'score': 2555.2755966186523, 'total_duration': 2808.118916273117, 'accumulated_submission_time': 2555.2755966186523, 'accumulated_eval_time': 252.4030647277832, 'accumulated_logging_time': 0.15917325019836426, 'global_step': 5473, 'preemption_count': 0}), (6396, {'train/accuracy': 0.2514062523841858, 'train/loss': 3.91365122795105, 'validation/accuracy': 0.23061999678611755, 'validation/loss': 4.019598484039307, 'validation/num_examples': 50000, 'test/accuracy': 0.1730000078678131, 'test/loss': 4.418845176696777, 'test/num_examples': 10000, 'score': 2975.4280049800873, 'total_duration': 3264.80242729187, 'accumulated_submission_time': 2975.4280049800873, 'accumulated_eval_time': 288.8565876483917, 'accumulated_logging_time': 0.19016075134277344, 'global_step': 6396, 'preemption_count': 0}), (7320, {'train/accuracy': 0.2916015684604645, 'train/loss': 3.6628291606903076, 'validation/accuracy': 0.26499998569488525, 'validation/loss': 3.8069074153900146, 'validation/num_examples': 50000, 'test/accuracy': 0.20390000939369202, 'test/loss': 4.221038818359375, 'test/num_examples': 10000, 'score': 3395.463133573532, 'total_duration': 3718.7628943920135, 'accumulated_submission_time': 3395.463133573532, 'accumulated_eval_time': 322.7090003490448, 'accumulated_logging_time': 0.2164924144744873, 'global_step': 7320, 'preemption_count': 0}), (8243, {'train/accuracy': 0.32517576217651367, 'train/loss': 3.437488555908203, 'validation/accuracy': 0.3050200045108795, 'validation/loss': 3.5397071838378906, 'validation/num_examples': 50000, 'test/accuracy': 0.23070001602172852, 'test/loss': 4.009162425994873, 'test/num_examples': 10000, 'score': 3815.831436395645, 'total_duration': 4174.174175024033, 'accumulated_submission_time': 3815.831436395645, 'accumulated_eval_time': 357.6776604652405, 'accumulated_logging_time': 0.24421310424804688, 'global_step': 8243, 'preemption_count': 0}), (9164, {'train/accuracy': 0.3589257597923279, 'train/loss': 3.2390763759613037, 'validation/accuracy': 0.32979997992515564, 'validation/loss': 3.377380609512329, 'validation/num_examples': 50000, 'test/accuracy': 0.254800021648407, 'test/loss': 3.8746755123138428, 'test/num_examples': 10000, 'score': 4235.847739696503, 'total_duration': 4628.525053501129, 'accumulated_submission_time': 4235.847739696503, 'accumulated_eval_time': 391.93957924842834, 'accumulated_logging_time': 0.2702317237854004, 'global_step': 9164, 'preemption_count': 0}), (10086, {'train/accuracy': 0.3991015553474426, 'train/loss': 3.000910997390747, 'validation/accuracy': 0.3657599985599518, 'validation/loss': 3.1630349159240723, 'validation/num_examples': 50000, 'test/accuracy': 0.281900018453598, 'test/loss': 3.6769216060638428, 'test/num_examples': 10000, 'score': 4655.996357917786, 'total_duration': 5087.298045635223, 'accumulated_submission_time': 4655.996357917786, 'accumulated_eval_time': 430.49109721183777, 'accumulated_logging_time': 0.2951619625091553, 'global_step': 10086, 'preemption_count': 0}), (11009, {'train/accuracy': 0.4095703065395355, 'train/loss': 2.9577596187591553, 'validation/accuracy': 0.3806999921798706, 'validation/loss': 3.086174726486206, 'validation/num_examples': 50000, 'test/accuracy': 0.2939999997615814, 'test/loss': 3.6142802238464355, 'test/num_examples': 10000, 'score': 5075.949893951416, 'total_duration': 5542.012524604797, 'accumulated_submission_time': 5075.949893951416, 'accumulated_eval_time': 465.17704224586487, 'accumulated_logging_time': 0.3227870464324951, 'global_step': 11009, 'preemption_count': 0}), (11932, {'train/accuracy': 0.45369139313697815, 'train/loss': 2.704256534576416, 'validation/accuracy': 0.4152999818325043, 'validation/loss': 2.878851890563965, 'validation/num_examples': 50000, 'test/accuracy': 0.32170000672340393, 'test/loss': 3.442434549331665, 'test/num_examples': 10000, 'score': 5496.126168727875, 'total_duration': 5996.636655807495, 'accumulated_submission_time': 5496.126168727875, 'accumulated_eval_time': 499.54923462867737, 'accumulated_logging_time': 0.35118961334228516, 'global_step': 11932, 'preemption_count': 0}), (12852, {'train/accuracy': 0.47328123450279236, 'train/loss': 2.620152473449707, 'validation/accuracy': 0.43347999453544617, 'validation/loss': 2.793245553970337, 'validation/num_examples': 50000, 'test/accuracy': 0.33250001072883606, 'test/loss': 3.3532426357269287, 'test/num_examples': 10000, 'score': 5916.12104177475, 'total_duration': 6450.153452396393, 'accumulated_submission_time': 5916.12104177475, 'accumulated_eval_time': 532.9983906745911, 'accumulated_logging_time': 0.3771047592163086, 'global_step': 12852, 'preemption_count': 0}), (13774, {'train/accuracy': 0.4964648187160492, 'train/loss': 2.481999397277832, 'validation/accuracy': 0.4541199803352356, 'validation/loss': 2.691492795944214, 'validation/num_examples': 50000, 'test/accuracy': 0.35630002617836, 'test/loss': 3.2585787773132324, 'test/num_examples': 10000, 'score': 6336.528871059418, 'total_duration': 6906.528246641159, 'accumulated_submission_time': 6336.528871059418, 'accumulated_eval_time': 568.8881900310516, 'accumulated_logging_time': 0.4073190689086914, 'global_step': 13774, 'preemption_count': 0}), (14697, {'train/accuracy': 0.5178515315055847, 'train/loss': 2.393428325653076, 'validation/accuracy': 0.47537997364997864, 'validation/loss': 2.5705621242523193, 'validation/num_examples': 50000, 'test/accuracy': 0.37450000643730164, 'test/loss': 3.150657892227173, 'test/num_examples': 10000, 'score': 6756.774499177933, 'total_duration': 7357.662885427475, 'accumulated_submission_time': 6756.774499177933, 'accumulated_eval_time': 599.7011110782623, 'accumulated_logging_time': 0.4356505870819092, 'global_step': 14697, 'preemption_count': 0}), (15618, {'train/accuracy': 0.5367968678474426, 'train/loss': 2.243785858154297, 'validation/accuracy': 0.49685999751091003, 'validation/loss': 2.430511713027954, 'validation/num_examples': 50000, 'test/accuracy': 0.3872000277042389, 'test/loss': 3.020982027053833, 'test/num_examples': 10000, 'score': 7176.8117735385895, 'total_duration': 7812.835678100586, 'accumulated_submission_time': 7176.8117735385895, 'accumulated_eval_time': 634.7569868564606, 'accumulated_logging_time': 0.4672560691833496, 'global_step': 15618, 'preemption_count': 0}), (16540, {'train/accuracy': 0.562207043170929, 'train/loss': 2.1498546600341797, 'validation/accuracy': 0.507319986820221, 'validation/loss': 2.4012911319732666, 'validation/num_examples': 50000, 'test/accuracy': 0.3929000198841095, 'test/loss': 2.9950408935546875, 'test/num_examples': 10000, 'score': 7597.053501844406, 'total_duration': 8267.926774263382, 'accumulated_submission_time': 7597.053501844406, 'accumulated_eval_time': 669.5322709083557, 'accumulated_logging_time': 0.4950077533721924, 'global_step': 16540, 'preemption_count': 0}), (17463, {'train/accuracy': 0.553515613079071, 'train/loss': 2.2031924724578857, 'validation/accuracy': 0.5131999850273132, 'validation/loss': 2.3823468685150146, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.9856181144714355, 'test/num_examples': 10000, 'score': 8017.169707298279, 'total_duration': 8721.486993074417, 'accumulated_submission_time': 8017.169707298279, 'accumulated_eval_time': 702.9022083282471, 'accumulated_logging_time': 0.5227203369140625, 'global_step': 17463, 'preemption_count': 0}), (18381, {'train/accuracy': 0.5694921612739563, 'train/loss': 2.0806596279144287, 'validation/accuracy': 0.5267999768257141, 'validation/loss': 2.2643842697143555, 'validation/num_examples': 50000, 'test/accuracy': 0.41690000891685486, 'test/loss': 2.871544122695923, 'test/num_examples': 10000, 'score': 8437.620900630951, 'total_duration': 9175.582031011581, 'accumulated_submission_time': 8437.620900630951, 'accumulated_eval_time': 736.4718625545502, 'accumulated_logging_time': 0.549354076385498, 'global_step': 18381, 'preemption_count': 0}), (19298, {'train/accuracy': 0.5946288704872131, 'train/loss': 1.9711012840270996, 'validation/accuracy': 0.5412999987602234, 'validation/loss': 2.2097156047821045, 'validation/num_examples': 50000, 'test/accuracy': 0.42190003395080566, 'test/loss': 2.8385367393493652, 'test/num_examples': 10000, 'score': 8857.677038431168, 'total_duration': 9631.36013674736, 'accumulated_submission_time': 8857.677038431168, 'accumulated_eval_time': 772.117954492569, 'accumulated_logging_time': 0.5785338878631592, 'global_step': 19298, 'preemption_count': 0}), (20217, {'train/accuracy': 0.5886132717132568, 'train/loss': 1.9665268659591675, 'validation/accuracy': 0.5514000058174133, 'validation/loss': 2.1377973556518555, 'validation/num_examples': 50000, 'test/accuracy': 0.4321000277996063, 'test/loss': 2.757359027862549, 'test/num_examples': 10000, 'score': 9277.779623508453, 'total_duration': 10086.7133436203, 'accumulated_submission_time': 9277.779623508453, 'accumulated_eval_time': 807.2917928695679, 'accumulated_logging_time': 0.6079757213592529, 'global_step': 20217, 'preemption_count': 0}), (21138, {'train/accuracy': 0.6024804711341858, 'train/loss': 1.9354881048202515, 'validation/accuracy': 0.5561999678611755, 'validation/loss': 2.1444432735443115, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.7603559494018555, 'test/num_examples': 10000, 'score': 9697.771949529648, 'total_duration': 10541.312201499939, 'accumulated_submission_time': 9697.771949529648, 'accumulated_eval_time': 841.8244321346283, 'accumulated_logging_time': 0.6349647045135498, 'global_step': 21138, 'preemption_count': 0}), (22059, {'train/accuracy': 0.6153905987739563, 'train/loss': 1.8635692596435547, 'validation/accuracy': 0.5629599690437317, 'validation/loss': 2.098595142364502, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.710822343826294, 'test/num_examples': 10000, 'score': 10117.83263373375, 'total_duration': 10995.936970949173, 'accumulated_submission_time': 10117.83263373375, 'accumulated_eval_time': 876.3112514019012, 'accumulated_logging_time': 0.6652498245239258, 'global_step': 22059, 'preemption_count': 0}), (22978, {'train/accuracy': 0.6119140386581421, 'train/loss': 1.860370397567749, 'validation/accuracy': 0.568619966506958, 'validation/loss': 2.0590929985046387, 'validation/num_examples': 50000, 'test/accuracy': 0.451200008392334, 'test/loss': 2.6695117950439453, 'test/num_examples': 10000, 'score': 10537.896867275238, 'total_duration': 11448.413235902786, 'accumulated_submission_time': 10537.896867275238, 'accumulated_eval_time': 908.6428663730621, 'accumulated_logging_time': 0.6947681903839111, 'global_step': 22978, 'preemption_count': 0}), (23901, {'train/accuracy': 0.619140625, 'train/loss': 1.8426916599273682, 'validation/accuracy': 0.5765399932861328, 'validation/loss': 2.036699056625366, 'validation/num_examples': 50000, 'test/accuracy': 0.4563000202178955, 'test/loss': 2.6497387886047363, 'test/num_examples': 10000, 'score': 10958.168203353882, 'total_duration': 11903.424647808075, 'accumulated_submission_time': 10958.168203353882, 'accumulated_eval_time': 943.3075177669525, 'accumulated_logging_time': 0.7228202819824219, 'global_step': 23901, 'preemption_count': 0}), (24825, {'train/accuracy': 0.6344335675239563, 'train/loss': 1.8031526803970337, 'validation/accuracy': 0.5821200013160706, 'validation/loss': 2.0396535396575928, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.65883469581604, 'test/num_examples': 10000, 'score': 11378.107141256332, 'total_duration': 12354.893264770508, 'accumulated_submission_time': 11378.107141256332, 'accumulated_eval_time': 974.7620024681091, 'accumulated_logging_time': 0.7505502700805664, 'global_step': 24825, 'preemption_count': 0}), (25745, {'train/accuracy': 0.6540429592132568, 'train/loss': 1.6929877996444702, 'validation/accuracy': 0.5794999599456787, 'validation/loss': 2.018251895904541, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.632565975189209, 'test/num_examples': 10000, 'score': 11798.067595005035, 'total_duration': 12809.31416130066, 'accumulated_submission_time': 11798.067595005035, 'accumulated_eval_time': 1009.1432175636292, 'accumulated_logging_time': 0.782789945602417, 'global_step': 25745, 'preemption_count': 0}), (26668, {'train/accuracy': 0.6333398222923279, 'train/loss': 1.7327735424041748, 'validation/accuracy': 0.5909799933433533, 'validation/loss': 1.9266051054000854, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.5658607482910156, 'test/num_examples': 10000, 'score': 12218.119409561157, 'total_duration': 13260.34297466278, 'accumulated_submission_time': 12218.119409561157, 'accumulated_eval_time': 1040.0437922477722, 'accumulated_logging_time': 0.8123970031738281, 'global_step': 26668, 'preemption_count': 0}), (27589, {'train/accuracy': 0.6485351324081421, 'train/loss': 1.7430307865142822, 'validation/accuracy': 0.5958600044250488, 'validation/loss': 1.972648024559021, 'validation/num_examples': 50000, 'test/accuracy': 0.47680002450942993, 'test/loss': 2.580284595489502, 'test/num_examples': 10000, 'score': 12638.257354021072, 'total_duration': 13714.76744222641, 'accumulated_submission_time': 12638.257354021072, 'accumulated_eval_time': 1074.2507855892181, 'accumulated_logging_time': 0.8448200225830078, 'global_step': 27589, 'preemption_count': 0}), (28513, {'train/accuracy': 0.6622265577316284, 'train/loss': 1.6519564390182495, 'validation/accuracy': 0.5966399908065796, 'validation/loss': 1.929701805114746, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.5671634674072266, 'test/num_examples': 10000, 'score': 13058.69831609726, 'total_duration': 14165.95332980156, 'accumulated_submission_time': 13058.69831609726, 'accumulated_eval_time': 1104.9193880558014, 'accumulated_logging_time': 0.8732032775878906, 'global_step': 28513, 'preemption_count': 0}), (29436, {'train/accuracy': 0.6512890458106995, 'train/loss': 1.7113102674484253, 'validation/accuracy': 0.6077600121498108, 'validation/loss': 1.9051331281661987, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.534058094024658, 'test/num_examples': 10000, 'score': 13478.946665525436, 'total_duration': 14620.107602596283, 'accumulated_submission_time': 13478.946665525436, 'accumulated_eval_time': 1138.7440557479858, 'accumulated_logging_time': 0.9072282314300537, 'global_step': 29436, 'preemption_count': 0}), (30358, {'train/accuracy': 0.6586328148841858, 'train/loss': 1.6941852569580078, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.9113799333572388, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.532050609588623, 'test/num_examples': 10000, 'score': 13898.911831855774, 'total_duration': 15075.097905397415, 'accumulated_submission_time': 13898.911831855774, 'accumulated_eval_time': 1173.6889071464539, 'accumulated_logging_time': 0.9404423236846924, 'global_step': 30358, 'preemption_count': 0}), (31283, {'train/accuracy': 0.6657617092132568, 'train/loss': 1.6142001152038574, 'validation/accuracy': 0.6137599945068359, 'validation/loss': 1.8565632104873657, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.487379312515259, 'test/num_examples': 10000, 'score': 14319.18147277832, 'total_duration': 15527.85389828682, 'accumulated_submission_time': 14319.18147277832, 'accumulated_eval_time': 1206.0904257297516, 'accumulated_logging_time': 0.9788103103637695, 'global_step': 31283, 'preemption_count': 0}), (32206, {'train/accuracy': 0.6634374856948853, 'train/loss': 1.6532877683639526, 'validation/accuracy': 0.6112799644470215, 'validation/loss': 1.8662524223327637, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.5017411708831787, 'test/num_examples': 10000, 'score': 14739.35320687294, 'total_duration': 15982.26224064827, 'accumulated_submission_time': 14739.35320687294, 'accumulated_eval_time': 1240.2513403892517, 'accumulated_logging_time': 1.008094310760498, 'global_step': 32206, 'preemption_count': 0}), (33130, {'train/accuracy': 0.6699804663658142, 'train/loss': 1.6174613237380981, 'validation/accuracy': 0.6247999668121338, 'validation/loss': 1.8267481327056885, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.448014974594116, 'test/num_examples': 10000, 'score': 15159.512373924255, 'total_duration': 16436.065540075302, 'accumulated_submission_time': 15159.512373924255, 'accumulated_eval_time': 1273.814103603363, 'accumulated_logging_time': 1.0417673587799072, 'global_step': 33130, 'preemption_count': 0}), (34053, {'train/accuracy': 0.6822265386581421, 'train/loss': 1.577724575996399, 'validation/accuracy': 0.6250799894332886, 'validation/loss': 1.833191156387329, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.4420933723449707, 'test/num_examples': 10000, 'score': 15579.846864700317, 'total_duration': 16890.66156053543, 'accumulated_submission_time': 15579.846864700317, 'accumulated_eval_time': 1307.9982221126556, 'accumulated_logging_time': 1.0729899406433105, 'global_step': 34053, 'preemption_count': 0}), (34975, {'train/accuracy': 0.7008007764816284, 'train/loss': 1.4769867658615112, 'validation/accuracy': 0.6222400069236755, 'validation/loss': 1.811497449874878, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.4435598850250244, 'test/num_examples': 10000, 'score': 15999.829869508743, 'total_duration': 17347.502505779266, 'accumulated_submission_time': 15999.829869508743, 'accumulated_eval_time': 1344.7656226158142, 'accumulated_logging_time': 1.1167857646942139, 'global_step': 34975, 'preemption_count': 0}), (35898, {'train/accuracy': 0.6775780916213989, 'train/loss': 1.5361953973770142, 'validation/accuracy': 0.6276800036430359, 'validation/loss': 1.7578741312026978, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.371567964553833, 'test/num_examples': 10000, 'score': 16420.072093486786, 'total_duration': 17797.47596859932, 'accumulated_submission_time': 16420.072093486786, 'accumulated_eval_time': 1374.4189398288727, 'accumulated_logging_time': 1.147679090499878, 'global_step': 35898, 'preemption_count': 0}), (36818, {'train/accuracy': 0.6763281226158142, 'train/loss': 1.6137640476226807, 'validation/accuracy': 0.6283800005912781, 'validation/loss': 1.842094898223877, 'validation/num_examples': 50000, 'test/accuracy': 0.5077000260353088, 'test/loss': 2.4434211254119873, 'test/num_examples': 10000, 'score': 16840.190497398376, 'total_duration': 18252.90421462059, 'accumulated_submission_time': 16840.190497398376, 'accumulated_eval_time': 1409.6424214839935, 'accumulated_logging_time': 1.1869757175445557, 'global_step': 36818, 'preemption_count': 0}), (37740, {'train/accuracy': 0.6975976228713989, 'train/loss': 1.4501029253005981, 'validation/accuracy': 0.631060004234314, 'validation/loss': 1.7535679340362549, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.3802409172058105, 'test/num_examples': 10000, 'score': 17260.28178691864, 'total_duration': 18705.626772880554, 'accumulated_submission_time': 17260.28178691864, 'accumulated_eval_time': 1442.1937334537506, 'accumulated_logging_time': 1.2198548316955566, 'global_step': 37740, 'preemption_count': 0}), (38663, {'train/accuracy': 0.6893359422683716, 'train/loss': 1.514500379562378, 'validation/accuracy': 0.6391400098800659, 'validation/loss': 1.7250676155090332, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.346991777420044, 'test/num_examples': 10000, 'score': 17680.43620157242, 'total_duration': 19159.576062202454, 'accumulated_submission_time': 17680.43620157242, 'accumulated_eval_time': 1475.9092907905579, 'accumulated_logging_time': 1.252131700515747, 'global_step': 38663, 'preemption_count': 0}), (39584, {'train/accuracy': 0.6898046731948853, 'train/loss': 1.4997754096984863, 'validation/accuracy': 0.6385999917984009, 'validation/loss': 1.728279948234558, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.3584721088409424, 'test/num_examples': 10000, 'score': 18100.48945403099, 'total_duration': 19613.12621331215, 'accumulated_submission_time': 18100.48945403099, 'accumulated_eval_time': 1509.32852268219, 'accumulated_logging_time': 1.2823808193206787, 'global_step': 39584, 'preemption_count': 0}), (40507, {'train/accuracy': 0.70068359375, 'train/loss': 1.4647341966629028, 'validation/accuracy': 0.6398599743843079, 'validation/loss': 1.7389329671859741, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.3418326377868652, 'test/num_examples': 10000, 'score': 18520.47150492668, 'total_duration': 20064.541659355164, 'accumulated_submission_time': 18520.47150492668, 'accumulated_eval_time': 1540.6759810447693, 'accumulated_logging_time': 1.316420316696167, 'global_step': 40507, 'preemption_count': 0}), (41427, {'train/accuracy': 0.6917382478713989, 'train/loss': 1.4947209358215332, 'validation/accuracy': 0.6426999568939209, 'validation/loss': 1.7017552852630615, 'validation/num_examples': 50000, 'test/accuracy': 0.5194000005722046, 'test/loss': 2.3289785385131836, 'test/num_examples': 10000, 'score': 18940.805812358856, 'total_duration': 20520.362723588943, 'accumulated_submission_time': 18940.805812358856, 'accumulated_eval_time': 1576.082843542099, 'accumulated_logging_time': 1.3496661186218262, 'global_step': 41427, 'preemption_count': 0}), (42353, {'train/accuracy': 0.70068359375, 'train/loss': 1.465484619140625, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.7100856304168701, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.3286025524139404, 'test/num_examples': 10000, 'score': 19361.10916209221, 'total_duration': 20972.88152360916, 'accumulated_submission_time': 19361.10916209221, 'accumulated_eval_time': 1608.2144901752472, 'accumulated_logging_time': 1.3862807750701904, 'global_step': 42353, 'preemption_count': 0}), (43275, {'train/accuracy': 0.7005859017372131, 'train/loss': 1.4444526433944702, 'validation/accuracy': 0.6436799764633179, 'validation/loss': 1.7053855657577515, 'validation/num_examples': 50000, 'test/accuracy': 0.5193000435829163, 'test/loss': 2.333106279373169, 'test/num_examples': 10000, 'score': 19781.25616669655, 'total_duration': 21427.411111593246, 'accumulated_submission_time': 19781.25616669655, 'accumulated_eval_time': 1642.51211977005, 'accumulated_logging_time': 1.4228754043579102, 'global_step': 43275, 'preemption_count': 0}), (44199, {'train/accuracy': 0.7070116996765137, 'train/loss': 1.4227125644683838, 'validation/accuracy': 0.6459999680519104, 'validation/loss': 1.6852363348007202, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.296396255493164, 'test/num_examples': 10000, 'score': 20201.509860515594, 'total_duration': 21880.785943746567, 'accumulated_submission_time': 20201.509860515594, 'accumulated_eval_time': 1675.5485351085663, 'accumulated_logging_time': 1.4599545001983643, 'global_step': 44199, 'preemption_count': 0}), (45121, {'train/accuracy': 0.7047460675239563, 'train/loss': 1.4595746994018555, 'validation/accuracy': 0.6520599722862244, 'validation/loss': 1.6880558729171753, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.3047330379486084, 'test/num_examples': 10000, 'score': 20621.46268749237, 'total_duration': 22336.67161679268, 'accumulated_submission_time': 20621.46268749237, 'accumulated_eval_time': 1711.402881860733, 'accumulated_logging_time': 1.4905712604522705, 'global_step': 45121, 'preemption_count': 0}), (46045, {'train/accuracy': 0.703808605670929, 'train/loss': 1.4133350849151611, 'validation/accuracy': 0.649899959564209, 'validation/loss': 1.6546680927276611, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.286877155303955, 'test/num_examples': 10000, 'score': 21041.530358314514, 'total_duration': 22787.827045440674, 'accumulated_submission_time': 21041.530358314514, 'accumulated_eval_time': 1742.409699678421, 'accumulated_logging_time': 1.524674892425537, 'global_step': 46045, 'preemption_count': 0}), (46969, {'train/accuracy': 0.7266796827316284, 'train/loss': 1.295320987701416, 'validation/accuracy': 0.6565399765968323, 'validation/loss': 1.6153538227081299, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.2345681190490723, 'test/num_examples': 10000, 'score': 21461.85862827301, 'total_duration': 23242.500529289246, 'accumulated_submission_time': 21461.85862827301, 'accumulated_eval_time': 1776.6765806674957, 'accumulated_logging_time': 1.5564830303192139, 'global_step': 46969, 'preemption_count': 0}), (47891, {'train/accuracy': 0.7119921445846558, 'train/loss': 1.4000214338302612, 'validation/accuracy': 0.6568399667739868, 'validation/loss': 1.6305649280548096, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.2502574920654297, 'test/num_examples': 10000, 'score': 21882.072820425034, 'total_duration': 23697.036667346954, 'accumulated_submission_time': 21882.072820425034, 'accumulated_eval_time': 1810.9130997657776, 'accumulated_logging_time': 1.5944783687591553, 'global_step': 47891, 'preemption_count': 0}), (48814, {'train/accuracy': 0.7128710746765137, 'train/loss': 1.3655827045440674, 'validation/accuracy': 0.6576200127601624, 'validation/loss': 1.6067535877227783, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.232346534729004, 'test/num_examples': 10000, 'score': 22302.034429311752, 'total_duration': 24147.68631052971, 'accumulated_submission_time': 22302.034429311752, 'accumulated_eval_time': 1841.5184531211853, 'accumulated_logging_time': 1.6299116611480713, 'global_step': 48814, 'preemption_count': 0}), (49734, {'train/accuracy': 0.728808581829071, 'train/loss': 1.3120293617248535, 'validation/accuracy': 0.6609199643135071, 'validation/loss': 1.6072094440460205, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.2364699840545654, 'test/num_examples': 10000, 'score': 22722.3923227787, 'total_duration': 24601.1224694252, 'accumulated_submission_time': 22722.3923227787, 'accumulated_eval_time': 1874.509628534317, 'accumulated_logging_time': 1.6707801818847656, 'global_step': 49734, 'preemption_count': 0}), (50652, {'train/accuracy': 0.7116601467132568, 'train/loss': 1.3645031452178955, 'validation/accuracy': 0.6611599922180176, 'validation/loss': 1.5872159004211426, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.2071101665496826, 'test/num_examples': 10000, 'score': 23142.32176733017, 'total_duration': 25055.637226343155, 'accumulated_submission_time': 23142.32176733017, 'accumulated_eval_time': 1909.0119910240173, 'accumulated_logging_time': 1.7064182758331299, 'global_step': 50652, 'preemption_count': 0}), (51574, {'train/accuracy': 0.717968761920929, 'train/loss': 1.3651286363601685, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.6173219680786133, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.240694761276245, 'test/num_examples': 10000, 'score': 23562.47232246399, 'total_duration': 25507.66274857521, 'accumulated_submission_time': 23562.47232246399, 'accumulated_eval_time': 1940.8010630607605, 'accumulated_logging_time': 1.7460317611694336, 'global_step': 51574, 'preemption_count': 0}), (52496, {'train/accuracy': 0.7241796851158142, 'train/loss': 1.3452785015106201, 'validation/accuracy': 0.6620000004768372, 'validation/loss': 1.6125682592391968, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.2341880798339844, 'test/num_examples': 10000, 'score': 23982.808941841125, 'total_duration': 25962.668434381485, 'accumulated_submission_time': 23982.808941841125, 'accumulated_eval_time': 1975.382539987564, 'accumulated_logging_time': 1.786369800567627, 'global_step': 52496, 'preemption_count': 0}), (53418, {'train/accuracy': 0.7199804782867432, 'train/loss': 1.3372256755828857, 'validation/accuracy': 0.6657999753952026, 'validation/loss': 1.5774351358413696, 'validation/num_examples': 50000, 'test/accuracy': 0.5430999994277954, 'test/loss': 2.2015929222106934, 'test/num_examples': 10000, 'score': 24402.908334493637, 'total_duration': 26413.86163020134, 'accumulated_submission_time': 24402.908334493637, 'accumulated_eval_time': 2006.3908026218414, 'accumulated_logging_time': 1.824455976486206, 'global_step': 53418, 'preemption_count': 0}), (54339, {'train/accuracy': 0.7254882454872131, 'train/loss': 1.335657000541687, 'validation/accuracy': 0.670699954032898, 'validation/loss': 1.579382300376892, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.186253309249878, 'test/num_examples': 10000, 'score': 24823.192568540573, 'total_duration': 26867.053624868393, 'accumulated_submission_time': 24823.192568540573, 'accumulated_eval_time': 2039.2105541229248, 'accumulated_logging_time': 1.866105318069458, 'global_step': 54339, 'preemption_count': 0}), (55261, {'train/accuracy': 0.7294921875, 'train/loss': 1.3043168783187866, 'validation/accuracy': 0.667419970035553, 'validation/loss': 1.5707368850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.1910340785980225, 'test/num_examples': 10000, 'score': 25243.527433633804, 'total_duration': 27318.778652668, 'accumulated_submission_time': 25243.527433633804, 'accumulated_eval_time': 2070.5169591903687, 'accumulated_logging_time': 1.901301383972168, 'global_step': 55261, 'preemption_count': 0}), (56182, {'train/accuracy': 0.7528125047683716, 'train/loss': 1.1971156597137451, 'validation/accuracy': 0.6747999787330627, 'validation/loss': 1.5370960235595703, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.1446516513824463, 'test/num_examples': 10000, 'score': 25663.721523284912, 'total_duration': 27770.727110385895, 'accumulated_submission_time': 25663.721523284912, 'accumulated_eval_time': 2102.191232919693, 'accumulated_logging_time': 1.9347867965698242, 'global_step': 56182, 'preemption_count': 0}), (57106, {'train/accuracy': 0.72314453125, 'train/loss': 1.3708430528640747, 'validation/accuracy': 0.6675199866294861, 'validation/loss': 1.6150413751602173, 'validation/num_examples': 50000, 'test/accuracy': 0.546500027179718, 'test/loss': 2.2251136302948, 'test/num_examples': 10000, 'score': 26084.066102027893, 'total_duration': 28225.778723955154, 'accumulated_submission_time': 26084.066102027893, 'accumulated_eval_time': 2136.8173022270203, 'accumulated_logging_time': 1.9684455394744873, 'global_step': 57106, 'preemption_count': 0}), (58026, {'train/accuracy': 0.7324023246765137, 'train/loss': 1.3181718587875366, 'validation/accuracy': 0.6703199744224548, 'validation/loss': 1.5841935873031616, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.2126083374023438, 'test/num_examples': 10000, 'score': 26504.048363685608, 'total_duration': 28678.303606510162, 'accumulated_submission_time': 26504.048363685608, 'accumulated_eval_time': 2169.2758326530457, 'accumulated_logging_time': 2.005272388458252, 'global_step': 58026, 'preemption_count': 0}), (58948, {'train/accuracy': 0.7443945407867432, 'train/loss': 1.2518020868301392, 'validation/accuracy': 0.6735599637031555, 'validation/loss': 1.5574877262115479, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.179090738296509, 'test/num_examples': 10000, 'score': 26924.052193164825, 'total_duration': 29129.454854011536, 'accumulated_submission_time': 26924.052193164825, 'accumulated_eval_time': 2200.3360035419464, 'accumulated_logging_time': 2.0453832149505615, 'global_step': 58948, 'preemption_count': 0}), (59869, {'train/accuracy': 0.7306249737739563, 'train/loss': 1.3189994096755981, 'validation/accuracy': 0.6790599822998047, 'validation/loss': 1.5507181882858276, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.166768789291382, 'test/num_examples': 10000, 'score': 27344.28582406044, 'total_duration': 29584.562819719315, 'accumulated_submission_time': 27344.28582406044, 'accumulated_eval_time': 2235.119631052017, 'accumulated_logging_time': 2.088909387588501, 'global_step': 59869, 'preemption_count': 0}), (60792, {'train/accuracy': 0.7305468320846558, 'train/loss': 1.3091228008270264, 'validation/accuracy': 0.6714000105857849, 'validation/loss': 1.5599348545074463, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.1946136951446533, 'test/num_examples': 10000, 'score': 27764.464653491974, 'total_duration': 30038.139572381973, 'accumulated_submission_time': 27764.464653491974, 'accumulated_eval_time': 2268.4358253479004, 'accumulated_logging_time': 2.123471736907959, 'global_step': 60792, 'preemption_count': 0}), (61707, {'train/accuracy': 0.7434960603713989, 'train/loss': 1.2444376945495605, 'validation/accuracy': 0.6765999794006348, 'validation/loss': 1.5327082872390747, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.1491177082061768, 'test/num_examples': 10000, 'score': 28184.4619910717, 'total_duration': 30492.225292921066, 'accumulated_submission_time': 28184.4619910717, 'accumulated_eval_time': 2302.4411540031433, 'accumulated_logging_time': 2.160538911819458, 'global_step': 61707, 'preemption_count': 0}), (62627, {'train/accuracy': 0.7280663847923279, 'train/loss': 1.3498351573944092, 'validation/accuracy': 0.672540009021759, 'validation/loss': 1.5801585912704468, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.195638656616211, 'test/num_examples': 10000, 'score': 28604.436437129974, 'total_duration': 30947.598170518875, 'accumulated_submission_time': 28604.436437129974, 'accumulated_eval_time': 2337.75715136528, 'accumulated_logging_time': 2.1949493885040283, 'global_step': 62627, 'preemption_count': 0}), (63549, {'train/accuracy': 0.7319726347923279, 'train/loss': 1.305245280265808, 'validation/accuracy': 0.6799799799919128, 'validation/loss': 1.5487784147262573, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.1663198471069336, 'test/num_examples': 10000, 'score': 29024.70809817314, 'total_duration': 31403.40734767914, 'accumulated_submission_time': 29024.70809817314, 'accumulated_eval_time': 2373.211641073227, 'accumulated_logging_time': 2.230957508087158, 'global_step': 63549, 'preemption_count': 0}), (64471, {'train/accuracy': 0.7493359446525574, 'train/loss': 1.2207766771316528, 'validation/accuracy': 0.6810799837112427, 'validation/loss': 1.495862603187561, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.1107430458068848, 'test/num_examples': 10000, 'score': 29444.985100269318, 'total_duration': 31857.98429250717, 'accumulated_submission_time': 29444.985100269318, 'accumulated_eval_time': 2407.421452522278, 'accumulated_logging_time': 2.273590564727783, 'global_step': 64471, 'preemption_count': 0}), (65392, {'train/accuracy': 0.7576562166213989, 'train/loss': 1.2009063959121704, 'validation/accuracy': 0.6784200072288513, 'validation/loss': 1.5377628803253174, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 2.167978048324585, 'test/num_examples': 10000, 'score': 29864.908487081528, 'total_duration': 32313.229038715363, 'accumulated_submission_time': 29864.908487081528, 'accumulated_eval_time': 2442.6564412117004, 'accumulated_logging_time': 2.313045024871826, 'global_step': 65392, 'preemption_count': 0}), (66313, {'train/accuracy': 0.7408398389816284, 'train/loss': 1.2733092308044434, 'validation/accuracy': 0.6830599904060364, 'validation/loss': 1.5249103307724, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.1454310417175293, 'test/num_examples': 10000, 'score': 30285.114988565445, 'total_duration': 32768.00912475586, 'accumulated_submission_time': 30285.114988565445, 'accumulated_eval_time': 2477.143489599228, 'accumulated_logging_time': 2.3523316383361816, 'global_step': 66313, 'preemption_count': 0}), (67235, {'train/accuracy': 0.7444140315055847, 'train/loss': 1.2327913045883179, 'validation/accuracy': 0.6813200116157532, 'validation/loss': 1.510599970817566, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 2.1155412197113037, 'test/num_examples': 10000, 'score': 30705.05445933342, 'total_duration': 33222.08050394058, 'accumulated_submission_time': 30705.05445933342, 'accumulated_eval_time': 2511.189737558365, 'accumulated_logging_time': 2.3908681869506836, 'global_step': 67235, 'preemption_count': 0}), (68155, {'train/accuracy': 0.761523425579071, 'train/loss': 1.198433756828308, 'validation/accuracy': 0.6867199540138245, 'validation/loss': 1.5193663835525513, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.129547595977783, 'test/num_examples': 10000, 'score': 31125.21816754341, 'total_duration': 33677.57394862175, 'accumulated_submission_time': 31125.21816754341, 'accumulated_eval_time': 2546.4316437244415, 'accumulated_logging_time': 2.431232452392578, 'global_step': 68155, 'preemption_count': 0}), (69076, {'train/accuracy': 0.7444921731948853, 'train/loss': 1.2327656745910645, 'validation/accuracy': 0.6861199736595154, 'validation/loss': 1.484375238418579, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.0891273021698, 'test/num_examples': 10000, 'score': 31545.463525772095, 'total_duration': 34129.872649908066, 'accumulated_submission_time': 31545.463525772095, 'accumulated_eval_time': 2578.39408159256, 'accumulated_logging_time': 2.4752867221832275, 'global_step': 69076, 'preemption_count': 0}), (69998, {'train/accuracy': 0.7477343678474426, 'train/loss': 1.226297378540039, 'validation/accuracy': 0.6846599578857422, 'validation/loss': 1.4979448318481445, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.113833427429199, 'test/num_examples': 10000, 'score': 31965.51737833023, 'total_duration': 34584.49463844299, 'accumulated_submission_time': 31965.51737833023, 'accumulated_eval_time': 2612.8748741149902, 'accumulated_logging_time': 2.5155715942382812, 'global_step': 69998, 'preemption_count': 0}), (70919, {'train/accuracy': 0.761035144329071, 'train/loss': 1.1790474653244019, 'validation/accuracy': 0.6904599666595459, 'validation/loss': 1.4743375778198242, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 2.088679552078247, 'test/num_examples': 10000, 'score': 32385.570518255234, 'total_duration': 35035.8462703228, 'accumulated_submission_time': 32385.570518255234, 'accumulated_eval_time': 2644.0857014656067, 'accumulated_logging_time': 2.554614782333374, 'global_step': 70919, 'preemption_count': 0}), (71841, {'train/accuracy': 0.7437499761581421, 'train/loss': 1.2982145547866821, 'validation/accuracy': 0.6857199668884277, 'validation/loss': 1.5482041835784912, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 2.14981746673584, 'test/num_examples': 10000, 'score': 32805.80994772911, 'total_duration': 35489.30454015732, 'accumulated_submission_time': 32805.80994772911, 'accumulated_eval_time': 2677.212243080139, 'accumulated_logging_time': 2.599778652191162, 'global_step': 71841, 'preemption_count': 0}), (72764, {'train/accuracy': 0.7504296898841858, 'train/loss': 1.2068672180175781, 'validation/accuracy': 0.6949999928474426, 'validation/loss': 1.4593466520309448, 'validation/num_examples': 50000, 'test/accuracy': 0.5694000124931335, 'test/loss': 2.0661840438842773, 'test/num_examples': 10000, 'score': 33225.91061067581, 'total_duration': 35941.64887547493, 'accumulated_submission_time': 33225.91061067581, 'accumulated_eval_time': 2709.3712763786316, 'accumulated_logging_time': 2.638214349746704, 'global_step': 72764, 'preemption_count': 0}), (73686, {'train/accuracy': 0.7568163871765137, 'train/loss': 1.2269736528396606, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.5098013877868652, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 2.13584303855896, 'test/num_examples': 10000, 'score': 33646.25889086723, 'total_duration': 36392.84107017517, 'accumulated_submission_time': 33646.25889086723, 'accumulated_eval_time': 2740.13196849823, 'accumulated_logging_time': 2.6735827922821045, 'global_step': 73686, 'preemption_count': 0}), (74607, {'train/accuracy': 0.7570703029632568, 'train/loss': 1.1745266914367676, 'validation/accuracy': 0.6887800097465515, 'validation/loss': 1.460245132446289, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.070711374282837, 'test/num_examples': 10000, 'score': 34066.51602935791, 'total_duration': 36850.192398786545, 'accumulated_submission_time': 34066.51602935791, 'accumulated_eval_time': 2777.113403081894, 'accumulated_logging_time': 2.73940110206604, 'global_step': 74607, 'preemption_count': 0}), (75530, {'train/accuracy': 0.75341796875, 'train/loss': 1.2075634002685547, 'validation/accuracy': 0.6945199966430664, 'validation/loss': 1.4618675708770752, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 2.0816633701324463, 'test/num_examples': 10000, 'score': 34486.733615875244, 'total_duration': 37304.65952205658, 'accumulated_submission_time': 34486.733615875244, 'accumulated_eval_time': 2811.2738075256348, 'accumulated_logging_time': 2.781567335128784, 'global_step': 75530, 'preemption_count': 0}), (76453, {'train/accuracy': 0.7587304711341858, 'train/loss': 1.1997249126434326, 'validation/accuracy': 0.6966999769210815, 'validation/loss': 1.4641448259353638, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 2.0874454975128174, 'test/num_examples': 10000, 'score': 34906.928926706314, 'total_duration': 37756.88763618469, 'accumulated_submission_time': 34906.928926706314, 'accumulated_eval_time': 2843.222591161728, 'accumulated_logging_time': 2.818364381790161, 'global_step': 76453, 'preemption_count': 0}), (77374, {'train/accuracy': 0.7772070169448853, 'train/loss': 1.1090781688690186, 'validation/accuracy': 0.694599986076355, 'validation/loss': 1.465245246887207, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 2.073258399963379, 'test/num_examples': 10000, 'score': 35327.165003061295, 'total_duration': 38208.289085149765, 'accumulated_submission_time': 35327.165003061295, 'accumulated_eval_time': 2874.2982473373413, 'accumulated_logging_time': 2.8605735301971436, 'global_step': 77374, 'preemption_count': 0}), (78298, {'train/accuracy': 0.7567187547683716, 'train/loss': 1.2263389825820923, 'validation/accuracy': 0.6919599771499634, 'validation/loss': 1.4922540187835693, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 2.110428810119629, 'test/num_examples': 10000, 'score': 35747.40086340904, 'total_duration': 38663.13839507103, 'accumulated_submission_time': 35747.40086340904, 'accumulated_eval_time': 2908.8261551856995, 'accumulated_logging_time': 2.8992533683776855, 'global_step': 78298, 'preemption_count': 0}), (79219, {'train/accuracy': 0.76429682970047, 'train/loss': 1.1475809812545776, 'validation/accuracy': 0.6978799700737, 'validation/loss': 1.4310686588287354, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 2.0265445709228516, 'test/num_examples': 10000, 'score': 36167.352187633514, 'total_duration': 39112.27828145027, 'accumulated_submission_time': 36167.352187633514, 'accumulated_eval_time': 2937.9259643554688, 'accumulated_logging_time': 2.9404571056365967, 'global_step': 79219, 'preemption_count': 0}), (80140, {'train/accuracy': 0.7719921469688416, 'train/loss': 1.1283113956451416, 'validation/accuracy': 0.6975399851799011, 'validation/loss': 1.4419785737991333, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 2.05631685256958, 'test/num_examples': 10000, 'score': 36587.514850854874, 'total_duration': 39567.50933790207, 'accumulated_submission_time': 36587.514850854874, 'accumulated_eval_time': 2972.902609348297, 'accumulated_logging_time': 2.984685182571411, 'global_step': 80140, 'preemption_count': 0}), (81062, {'train/accuracy': 0.7625781297683716, 'train/loss': 1.1647394895553589, 'validation/accuracy': 0.7009999752044678, 'validation/loss': 1.4354666471481323, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 2.033855438232422, 'test/num_examples': 10000, 'score': 37007.73335170746, 'total_duration': 40020.88571333885, 'accumulated_submission_time': 37007.73335170746, 'accumulated_eval_time': 3005.9743795394897, 'accumulated_logging_time': 3.023676633834839, 'global_step': 81062, 'preemption_count': 0}), (81984, {'train/accuracy': 0.7649609446525574, 'train/loss': 1.1457265615463257, 'validation/accuracy': 0.7003799676895142, 'validation/loss': 1.4246442317962646, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 2.0412683486938477, 'test/num_examples': 10000, 'score': 37427.750932216644, 'total_duration': 40472.73112511635, 'accumulated_submission_time': 37427.750932216644, 'accumulated_eval_time': 3037.7150337696075, 'accumulated_logging_time': 3.0635695457458496, 'global_step': 81984, 'preemption_count': 0}), (82906, {'train/accuracy': 0.7728124856948853, 'train/loss': 1.1158013343811035, 'validation/accuracy': 0.7010799646377563, 'validation/loss': 1.41843843460083, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 2.0298521518707275, 'test/num_examples': 10000, 'score': 37847.68333148956, 'total_duration': 40926.77200841904, 'accumulated_submission_time': 37847.68333148956, 'accumulated_eval_time': 3071.735149860382, 'accumulated_logging_time': 3.1044604778289795, 'global_step': 82906, 'preemption_count': 0}), (83831, {'train/accuracy': 0.7667187452316284, 'train/loss': 1.1418139934539795, 'validation/accuracy': 0.7049999833106995, 'validation/loss': 1.4089363813400269, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 2.009699821472168, 'test/num_examples': 10000, 'score': 38267.859589099884, 'total_duration': 41383.278517484665, 'accumulated_submission_time': 38267.859589099884, 'accumulated_eval_time': 3107.9714460372925, 'accumulated_logging_time': 3.151188611984253, 'global_step': 83831, 'preemption_count': 0}), (84754, {'train/accuracy': 0.7683984041213989, 'train/loss': 1.1525145769119263, 'validation/accuracy': 0.7024199962615967, 'validation/loss': 1.4284731149673462, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 2.029409170150757, 'test/num_examples': 10000, 'score': 38688.022983789444, 'total_duration': 41834.969650030136, 'accumulated_submission_time': 38688.022983789444, 'accumulated_eval_time': 3139.4142003059387, 'accumulated_logging_time': 3.1884803771972656, 'global_step': 84754, 'preemption_count': 0}), (85675, {'train/accuracy': 0.7705664038658142, 'train/loss': 1.1363499164581299, 'validation/accuracy': 0.7039200067520142, 'validation/loss': 1.4214590787887573, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 2.0247151851654053, 'test/num_examples': 10000, 'score': 39108.00399470329, 'total_duration': 42289.48701906204, 'accumulated_submission_time': 39108.00399470329, 'accumulated_eval_time': 3173.8589317798615, 'accumulated_logging_time': 3.232621431350708, 'global_step': 85675, 'preemption_count': 0}), (86597, {'train/accuracy': 0.7929882407188416, 'train/loss': 1.1039115190505981, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.468793511390686, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 2.075772523880005, 'test/num_examples': 10000, 'score': 39528.32387185097, 'total_duration': 42747.13645219803, 'accumulated_submission_time': 39528.32387185097, 'accumulated_eval_time': 3211.1007244586945, 'accumulated_logging_time': 3.2733957767486572, 'global_step': 86597, 'preemption_count': 0}), (87520, {'train/accuracy': 0.7638280987739563, 'train/loss': 1.196157455444336, 'validation/accuracy': 0.7017599940299988, 'validation/loss': 1.4531490802764893, 'validation/num_examples': 50000, 'test/accuracy': 0.575700044631958, 'test/loss': 2.062497854232788, 'test/num_examples': 10000, 'score': 39948.40259766579, 'total_duration': 43203.59956860542, 'accumulated_submission_time': 39948.40259766579, 'accumulated_eval_time': 3247.392087459564, 'accumulated_logging_time': 3.3191680908203125, 'global_step': 87520, 'preemption_count': 0}), (88441, {'train/accuracy': 0.7773827910423279, 'train/loss': 1.0995545387268066, 'validation/accuracy': 0.7068799734115601, 'validation/loss': 1.3993632793426514, 'validation/num_examples': 50000, 'test/accuracy': 0.5833000540733337, 'test/loss': 1.993497610092163, 'test/num_examples': 10000, 'score': 40368.75314331055, 'total_duration': 43657.850548028946, 'accumulated_submission_time': 40368.75314331055, 'accumulated_eval_time': 3281.204854249954, 'accumulated_logging_time': 3.3600547313690186, 'global_step': 88441, 'preemption_count': 0}), (89362, {'train/accuracy': 0.7879687547683716, 'train/loss': 1.051804780960083, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.3937915563583374, 'validation/num_examples': 50000, 'test/accuracy': 0.5820000171661377, 'test/loss': 2.0068376064300537, 'test/num_examples': 10000, 'score': 40788.70879864693, 'total_duration': 44110.658349752426, 'accumulated_submission_time': 40788.70879864693, 'accumulated_eval_time': 3313.9709889888763, 'accumulated_logging_time': 3.399597406387329, 'global_step': 89362, 'preemption_count': 0}), (90282, {'train/accuracy': 0.7703515291213989, 'train/loss': 1.137851357460022, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.4091787338256836, 'validation/num_examples': 50000, 'test/accuracy': 0.5849000215530396, 'test/loss': 2.013418197631836, 'test/num_examples': 10000, 'score': 41208.68663263321, 'total_duration': 44563.522152900696, 'accumulated_submission_time': 41208.68663263321, 'accumulated_eval_time': 3346.769567489624, 'accumulated_logging_time': 3.440016031265259, 'global_step': 90282, 'preemption_count': 0}), (91203, {'train/accuracy': 0.779980480670929, 'train/loss': 1.1019827127456665, 'validation/accuracy': 0.7110399603843689, 'validation/loss': 1.3903183937072754, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 1.9904738664627075, 'test/num_examples': 10000, 'score': 41628.995743989944, 'total_duration': 45017.9036052227, 'accumulated_submission_time': 41628.995743989944, 'accumulated_eval_time': 3380.757195711136, 'accumulated_logging_time': 3.477928876876831, 'global_step': 91203, 'preemption_count': 0}), (92127, {'train/accuracy': 0.7870507836341858, 'train/loss': 1.0477782487869263, 'validation/accuracy': 0.7113999724388123, 'validation/loss': 1.3689666986465454, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9610776901245117, 'test/num_examples': 10000, 'score': 42049.15962576866, 'total_duration': 45470.5659160614, 'accumulated_submission_time': 42049.15962576866, 'accumulated_eval_time': 3413.170263528824, 'accumulated_logging_time': 3.516024589538574, 'global_step': 92127, 'preemption_count': 0}), (93051, {'train/accuracy': 0.7793359160423279, 'train/loss': 1.0956302881240845, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.3716667890548706, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9534903764724731, 'test/num_examples': 10000, 'score': 42469.195261478424, 'total_duration': 45925.48884344101, 'accumulated_submission_time': 42469.195261478424, 'accumulated_eval_time': 3447.9696865081787, 'accumulated_logging_time': 3.5564661026000977, 'global_step': 93051, 'preemption_count': 0}), (93974, {'train/accuracy': 0.7795116901397705, 'train/loss': 1.0931429862976074, 'validation/accuracy': 0.7133600115776062, 'validation/loss': 1.3716752529144287, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.9728366136550903, 'test/num_examples': 10000, 'score': 42889.19483089447, 'total_duration': 46378.44157075882, 'accumulated_submission_time': 42889.19483089447, 'accumulated_eval_time': 3480.8335807323456, 'accumulated_logging_time': 3.5989716053009033, 'global_step': 93974, 'preemption_count': 0}), (94893, {'train/accuracy': 0.7892773151397705, 'train/loss': 1.0392942428588867, 'validation/accuracy': 0.7130999565124512, 'validation/loss': 1.3569936752319336, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.949466347694397, 'test/num_examples': 10000, 'score': 43309.401195287704, 'total_duration': 46834.06846022606, 'accumulated_submission_time': 43309.401195287704, 'accumulated_eval_time': 3516.163388967514, 'accumulated_logging_time': 3.6427321434020996, 'global_step': 94893, 'preemption_count': 0}), (95815, {'train/accuracy': 0.7995898127555847, 'train/loss': 0.9954485297203064, 'validation/accuracy': 0.7177199721336365, 'validation/loss': 1.337254524230957, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.9371448755264282, 'test/num_examples': 10000, 'score': 43729.76480174065, 'total_duration': 47286.249116420746, 'accumulated_submission_time': 43729.76480174065, 'accumulated_eval_time': 3547.8907368183136, 'accumulated_logging_time': 3.6849236488342285, 'global_step': 95815, 'preemption_count': 0}), (96738, {'train/accuracy': 0.77943354845047, 'train/loss': 1.1019657850265503, 'validation/accuracy': 0.7138400077819824, 'validation/loss': 1.384581208229065, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.9811594486236572, 'test/num_examples': 10000, 'score': 44150.00039052963, 'total_duration': 47740.57042002678, 'accumulated_submission_time': 44150.00039052963, 'accumulated_eval_time': 3581.8719758987427, 'accumulated_logging_time': 3.726364850997925, 'global_step': 96738, 'preemption_count': 0}), (97662, {'train/accuracy': 0.7866015434265137, 'train/loss': 1.0639996528625488, 'validation/accuracy': 0.7142999768257141, 'validation/loss': 1.3826018571853638, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.9830719232559204, 'test/num_examples': 10000, 'score': 44570.135746240616, 'total_duration': 48195.98546934128, 'accumulated_submission_time': 44570.135746240616, 'accumulated_eval_time': 3617.0574843883514, 'accumulated_logging_time': 3.7728912830352783, 'global_step': 97662, 'preemption_count': 0}), (98587, {'train/accuracy': 0.7963476181030273, 'train/loss': 1.0071494579315186, 'validation/accuracy': 0.7149400115013123, 'validation/loss': 1.3564001321792603, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.9637293815612793, 'test/num_examples': 10000, 'score': 44990.07020807266, 'total_duration': 48646.9700319767, 'accumulated_submission_time': 44990.07020807266, 'accumulated_eval_time': 3648.015291452408, 'accumulated_logging_time': 3.817314863204956, 'global_step': 98587, 'preemption_count': 0}), (99509, {'train/accuracy': 0.78919917345047, 'train/loss': 1.0567806959152222, 'validation/accuracy': 0.7200799584388733, 'validation/loss': 1.3418904542922974, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.9461467266082764, 'test/num_examples': 10000, 'score': 45410.03365278244, 'total_duration': 49102.78510594368, 'accumulated_submission_time': 45410.03365278244, 'accumulated_eval_time': 3683.7723546028137, 'accumulated_logging_time': 3.8653643131256104, 'global_step': 99509, 'preemption_count': 0}), (100432, {'train/accuracy': 0.7888476252555847, 'train/loss': 1.059237003326416, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.3572092056274414, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9448856115341187, 'test/num_examples': 10000, 'score': 45830.30660319328, 'total_duration': 49560.086330890656, 'accumulated_submission_time': 45830.30660319328, 'accumulated_eval_time': 3720.7103447914124, 'accumulated_logging_time': 3.9089343547821045, 'global_step': 100432, 'preemption_count': 0}), (101357, {'train/accuracy': 0.8024413585662842, 'train/loss': 0.987043559551239, 'validation/accuracy': 0.7212799787521362, 'validation/loss': 1.3317164182662964, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.923500657081604, 'test/num_examples': 10000, 'score': 46250.637231349945, 'total_duration': 50018.72805285454, 'accumulated_submission_time': 46250.637231349945, 'accumulated_eval_time': 3758.9333407878876, 'accumulated_logging_time': 3.949855089187622, 'global_step': 101357, 'preemption_count': 0}), (102279, {'train/accuracy': 0.7923827767372131, 'train/loss': 1.034652590751648, 'validation/accuracy': 0.7217400074005127, 'validation/loss': 1.337119460105896, 'validation/num_examples': 50000, 'test/accuracy': 0.5967000126838684, 'test/loss': 1.9396754503250122, 'test/num_examples': 10000, 'score': 46670.904515028, 'total_duration': 50477.970071554184, 'accumulated_submission_time': 46670.904515028, 'accumulated_eval_time': 3797.81196808815, 'accumulated_logging_time': 3.9987268447875977, 'global_step': 102279, 'preemption_count': 0}), (103199, {'train/accuracy': 0.7949413657188416, 'train/loss': 1.0258285999298096, 'validation/accuracy': 0.7251799702644348, 'validation/loss': 1.329066514968872, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9212018251419067, 'test/num_examples': 10000, 'score': 47091.08183169365, 'total_duration': 50932.684057712555, 'accumulated_submission_time': 47091.08183169365, 'accumulated_eval_time': 3832.2540550231934, 'accumulated_logging_time': 4.04592490196228, 'global_step': 103199, 'preemption_count': 0}), (104119, {'train/accuracy': 0.8043944835662842, 'train/loss': 0.9767816662788391, 'validation/accuracy': 0.7267799973487854, 'validation/loss': 1.3111376762390137, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.8960027694702148, 'test/num_examples': 10000, 'score': 47511.4247674942, 'total_duration': 51386.340764284134, 'accumulated_submission_time': 47511.4247674942, 'accumulated_eval_time': 3865.4750039577484, 'accumulated_logging_time': 4.091261148452759, 'global_step': 104119, 'preemption_count': 0}), (105026, {'train/accuracy': 0.8005077838897705, 'train/loss': 0.9992862343788147, 'validation/accuracy': 0.7254199981689453, 'validation/loss': 1.3189843893051147, 'validation/num_examples': 50000, 'test/accuracy': 0.6068000197410583, 'test/loss': 1.9124714136123657, 'test/num_examples': 10000, 'score': 47931.448093891144, 'total_duration': 51843.25613093376, 'accumulated_submission_time': 47931.448093891144, 'accumulated_eval_time': 3902.273670196533, 'accumulated_logging_time': 4.136998176574707, 'global_step': 105026, 'preemption_count': 0}), (105950, {'train/accuracy': 0.7973827719688416, 'train/loss': 0.9926227927207947, 'validation/accuracy': 0.7240599989891052, 'validation/loss': 1.3056074380874634, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.899614930152893, 'test/num_examples': 10000, 'score': 48351.60951066017, 'total_duration': 52299.75429058075, 'accumulated_submission_time': 48351.60951066017, 'accumulated_eval_time': 3938.51873588562, 'accumulated_logging_time': 4.1818811893463135, 'global_step': 105950, 'preemption_count': 0}), (106871, {'train/accuracy': 0.8055273294448853, 'train/loss': 0.9919548630714417, 'validation/accuracy': 0.7270799875259399, 'validation/loss': 1.3199224472045898, 'validation/num_examples': 50000, 'test/accuracy': 0.6069000363349915, 'test/loss': 1.9079723358154297, 'test/num_examples': 10000, 'score': 48771.873259305954, 'total_duration': 52754.04180908203, 'accumulated_submission_time': 48771.873259305954, 'accumulated_eval_time': 3972.452043533325, 'accumulated_logging_time': 4.225016117095947, 'global_step': 106871, 'preemption_count': 0}), (107792, {'train/accuracy': 0.8181054592132568, 'train/loss': 0.9642824530601501, 'validation/accuracy': 0.7266599535942078, 'validation/loss': 1.341126561164856, 'validation/num_examples': 50000, 'test/accuracy': 0.6049000024795532, 'test/loss': 1.9311014413833618, 'test/num_examples': 10000, 'score': 49191.80590748787, 'total_duration': 53209.868277311325, 'accumulated_submission_time': 49191.80590748787, 'accumulated_eval_time': 4008.251214504242, 'accumulated_logging_time': 4.2725653648376465, 'global_step': 107792, 'preemption_count': 0}), (108714, {'train/accuracy': 0.8000195026397705, 'train/loss': 0.9913946390151978, 'validation/accuracy': 0.7280600070953369, 'validation/loss': 1.2932459115982056, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.8781938552856445, 'test/num_examples': 10000, 'score': 49611.74863290787, 'total_duration': 53664.38921499252, 'accumulated_submission_time': 49611.74863290787, 'accumulated_eval_time': 4042.7389369010925, 'accumulated_logging_time': 4.315826892852783, 'global_step': 108714, 'preemption_count': 0}), (109635, {'train/accuracy': 0.8060351610183716, 'train/loss': 0.9905210733413696, 'validation/accuracy': 0.7256999611854553, 'validation/loss': 1.3248872756958008, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.9160008430480957, 'test/num_examples': 10000, 'score': 50031.87528467178, 'total_duration': 54121.824350357056, 'accumulated_submission_time': 50031.87528467178, 'accumulated_eval_time': 4079.9588174819946, 'accumulated_logging_time': 4.35719108581543, 'global_step': 109635, 'preemption_count': 0}), (110557, {'train/accuracy': 0.8092968463897705, 'train/loss': 0.9955326914787292, 'validation/accuracy': 0.7274199724197388, 'validation/loss': 1.3348983526229858, 'validation/num_examples': 50000, 'test/accuracy': 0.6116000413894653, 'test/loss': 1.9224961996078491, 'test/num_examples': 10000, 'score': 50451.978063583374, 'total_duration': 54578.00987672806, 'accumulated_submission_time': 50451.978063583374, 'accumulated_eval_time': 4115.946965456009, 'accumulated_logging_time': 4.4040772914886475, 'global_step': 110557, 'preemption_count': 0}), (111475, {'train/accuracy': 0.8045703172683716, 'train/loss': 0.9676870703697205, 'validation/accuracy': 0.7338599562644958, 'validation/loss': 1.2687467336654663, 'validation/num_examples': 50000, 'test/accuracy': 0.6123000383377075, 'test/loss': 1.863784909248352, 'test/num_examples': 10000, 'score': 50871.95857954025, 'total_duration': 55032.40033340454, 'accumulated_submission_time': 50871.95857954025, 'accumulated_eval_time': 4150.26503443718, 'accumulated_logging_time': 4.449316024780273, 'global_step': 111475, 'preemption_count': 0}), (112397, {'train/accuracy': 0.8095507621765137, 'train/loss': 0.9677566885948181, 'validation/accuracy': 0.7329199910163879, 'validation/loss': 1.2998954057693481, 'validation/num_examples': 50000, 'test/accuracy': 0.6091000437736511, 'test/loss': 1.8859690427780151, 'test/num_examples': 10000, 'score': 51292.22880363464, 'total_duration': 55488.480078697205, 'accumulated_submission_time': 51292.22880363464, 'accumulated_eval_time': 4185.976754188538, 'accumulated_logging_time': 4.499913215637207, 'global_step': 112397, 'preemption_count': 0}), (113317, {'train/accuracy': 0.8145117163658142, 'train/loss': 0.9527512192726135, 'validation/accuracy': 0.7319999933242798, 'validation/loss': 1.2896184921264648, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.8763359785079956, 'test/num_examples': 10000, 'score': 51712.16885471344, 'total_duration': 55943.190348148346, 'accumulated_submission_time': 51712.16885471344, 'accumulated_eval_time': 4220.654976129532, 'accumulated_logging_time': 4.545567512512207, 'global_step': 113317, 'preemption_count': 0}), (114240, {'train/accuracy': 0.8056835532188416, 'train/loss': 0.9622820615768433, 'validation/accuracy': 0.7326399683952332, 'validation/loss': 1.2778819799423218, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.8607583045959473, 'test/num_examples': 10000, 'score': 52132.5546040535, 'total_duration': 56400.49531674385, 'accumulated_submission_time': 52132.5546040535, 'accumulated_eval_time': 4257.473162174225, 'accumulated_logging_time': 4.599227428436279, 'global_step': 114240, 'preemption_count': 0}), (115161, {'train/accuracy': 0.8109765648841858, 'train/loss': 0.9565854668617249, 'validation/accuracy': 0.7332599759101868, 'validation/loss': 1.2811367511749268, 'validation/num_examples': 50000, 'test/accuracy': 0.6152000427246094, 'test/loss': 1.8726716041564941, 'test/num_examples': 10000, 'score': 52552.574162483215, 'total_duration': 56855.47197389603, 'accumulated_submission_time': 52552.574162483215, 'accumulated_eval_time': 4292.337894201279, 'accumulated_logging_time': 4.644402265548706, 'global_step': 115161, 'preemption_count': 0}), (116082, {'train/accuracy': 0.8161913752555847, 'train/loss': 0.9610245823860168, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.299484133720398, 'validation/num_examples': 50000, 'test/accuracy': 0.6181000471115112, 'test/loss': 1.8877934217453003, 'test/num_examples': 10000, 'score': 52972.78386044502, 'total_duration': 57310.55331468582, 'accumulated_submission_time': 52972.78386044502, 'accumulated_eval_time': 4327.119237422943, 'accumulated_logging_time': 4.6875598430633545, 'global_step': 116082, 'preemption_count': 0}), (117005, {'train/accuracy': 0.8302538990974426, 'train/loss': 0.8830304741859436, 'validation/accuracy': 0.7373999953269958, 'validation/loss': 1.2599748373031616, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8561517000198364, 'test/num_examples': 10000, 'score': 53392.99377536774, 'total_duration': 57769.04441308975, 'accumulated_submission_time': 53392.99377536774, 'accumulated_eval_time': 4365.301455259323, 'accumulated_logging_time': 4.738725185394287, 'global_step': 117005, 'preemption_count': 0}), (117924, {'train/accuracy': 0.8176171779632568, 'train/loss': 0.9192262291908264, 'validation/accuracy': 0.7388799786567688, 'validation/loss': 1.245247483253479, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.8357372283935547, 'test/num_examples': 10000, 'score': 53813.30598425865, 'total_duration': 58229.93274021149, 'accumulated_submission_time': 53813.30598425865, 'accumulated_eval_time': 4405.786436319351, 'accumulated_logging_time': 4.782990217208862, 'global_step': 117924, 'preemption_count': 0}), (118847, {'train/accuracy': 0.8188671469688416, 'train/loss': 0.9099279642105103, 'validation/accuracy': 0.7407199740409851, 'validation/loss': 1.2410085201263428, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.840397596359253, 'test/num_examples': 10000, 'score': 54233.51160264015, 'total_duration': 58685.261761426926, 'accumulated_submission_time': 54233.51160264015, 'accumulated_eval_time': 4440.815090417862, 'accumulated_logging_time': 4.830764055252075, 'global_step': 118847, 'preemption_count': 0}), (119772, {'train/accuracy': 0.8258007764816284, 'train/loss': 0.8815270662307739, 'validation/accuracy': 0.7394799590110779, 'validation/loss': 1.2439427375793457, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.8456861972808838, 'test/num_examples': 10000, 'score': 54653.70579338074, 'total_duration': 59141.99428868294, 'accumulated_submission_time': 54653.70579338074, 'accumulated_eval_time': 4477.259788036346, 'accumulated_logging_time': 4.876175165176392, 'global_step': 119772, 'preemption_count': 0}), (120694, {'train/accuracy': 0.81947261095047, 'train/loss': 0.9173012971878052, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.251138687133789, 'validation/num_examples': 50000, 'test/accuracy': 0.6217000484466553, 'test/loss': 1.8443632125854492, 'test/num_examples': 10000, 'score': 55073.763964653015, 'total_duration': 59598.028196811676, 'accumulated_submission_time': 55073.763964653015, 'accumulated_eval_time': 4513.142883777618, 'accumulated_logging_time': 4.922154426574707, 'global_step': 120694, 'preemption_count': 0}), (121618, {'train/accuracy': 0.8227343559265137, 'train/loss': 0.9045091867446899, 'validation/accuracy': 0.7416200041770935, 'validation/loss': 1.2397983074188232, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.8253264427185059, 'test/num_examples': 10000, 'score': 55493.97620844841, 'total_duration': 60051.2044301033, 'accumulated_submission_time': 55493.97620844841, 'accumulated_eval_time': 4546.011871576309, 'accumulated_logging_time': 4.970205307006836, 'global_step': 121618, 'preemption_count': 0}), (122542, {'train/accuracy': 0.828417956829071, 'train/loss': 0.8838768601417542, 'validation/accuracy': 0.7440599799156189, 'validation/loss': 1.24087393283844, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8244028091430664, 'test/num_examples': 10000, 'score': 55914.22261214256, 'total_duration': 60508.75056910515, 'accumulated_submission_time': 55914.22261214256, 'accumulated_eval_time': 4583.217739343643, 'accumulated_logging_time': 5.016493797302246, 'global_step': 122542, 'preemption_count': 0}), (123465, {'train/accuracy': 0.8238476514816284, 'train/loss': 0.9152378439903259, 'validation/accuracy': 0.746399998664856, 'validation/loss': 1.2385286092758179, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.8137975931167603, 'test/num_examples': 10000, 'score': 56334.522699832916, 'total_duration': 60965.69726276398, 'accumulated_submission_time': 56334.522699832916, 'accumulated_eval_time': 4619.772355079651, 'accumulated_logging_time': 5.061162710189819, 'global_step': 123465, 'preemption_count': 0}), (124385, {'train/accuracy': 0.8286913633346558, 'train/loss': 0.886573851108551, 'validation/accuracy': 0.7448999881744385, 'validation/loss': 1.2313250303268433, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8222373723983765, 'test/num_examples': 10000, 'score': 56754.61694145203, 'total_duration': 61424.96513009071, 'accumulated_submission_time': 56754.61694145203, 'accumulated_eval_time': 4658.850819826126, 'accumulated_logging_time': 5.109199523925781, 'global_step': 124385, 'preemption_count': 0}), (125305, {'train/accuracy': 0.8369140625, 'train/loss': 0.8582568168640137, 'validation/accuracy': 0.7482799887657166, 'validation/loss': 1.2224678993225098, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.801285982131958, 'test/num_examples': 10000, 'score': 57174.62446951866, 'total_duration': 61878.66628551483, 'accumulated_submission_time': 57174.62446951866, 'accumulated_eval_time': 4692.450619220734, 'accumulated_logging_time': 5.156161308288574, 'global_step': 125305, 'preemption_count': 0}), (126227, {'train/accuracy': 0.8360546827316284, 'train/loss': 0.8389551639556885, 'validation/accuracy': 0.7470200061798096, 'validation/loss': 1.2079110145568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.8007813692092896, 'test/num_examples': 10000, 'score': 57594.86729764938, 'total_duration': 62332.49602270126, 'accumulated_submission_time': 57594.86729764938, 'accumulated_eval_time': 4725.944480419159, 'accumulated_logging_time': 5.202749729156494, 'global_step': 126227, 'preemption_count': 0}), (127149, {'train/accuracy': 0.8318945169448853, 'train/loss': 0.8691099882125854, 'validation/accuracy': 0.7507199645042419, 'validation/loss': 1.216996669769287, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.7968522310256958, 'test/num_examples': 10000, 'score': 58015.21964287758, 'total_duration': 62789.48238158226, 'accumulated_submission_time': 58015.21964287758, 'accumulated_eval_time': 4762.4858384132385, 'accumulated_logging_time': 5.24896764755249, 'global_step': 127149, 'preemption_count': 0}), (128072, {'train/accuracy': 0.8342577815055847, 'train/loss': 0.880061686038971, 'validation/accuracy': 0.7497999668121338, 'validation/loss': 1.2383127212524414, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.8079801797866821, 'test/num_examples': 10000, 'score': 58435.50190925598, 'total_duration': 63247.640478134155, 'accumulated_submission_time': 58435.50190925598, 'accumulated_eval_time': 4800.266010761261, 'accumulated_logging_time': 5.297834873199463, 'global_step': 128072, 'preemption_count': 0}), (128994, {'train/accuracy': 0.8449023365974426, 'train/loss': 0.8151799440383911, 'validation/accuracy': 0.7497599720954895, 'validation/loss': 1.2033263444900513, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.784820795059204, 'test/num_examples': 10000, 'score': 58855.55880713463, 'total_duration': 63711.22246456146, 'accumulated_submission_time': 58855.55880713463, 'accumulated_eval_time': 4843.690578460693, 'accumulated_logging_time': 5.350980758666992, 'global_step': 128994, 'preemption_count': 0}), (129916, {'train/accuracy': 0.8347851634025574, 'train/loss': 0.8594391942024231, 'validation/accuracy': 0.7525799870491028, 'validation/loss': 1.2059522867202759, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.800089955329895, 'test/num_examples': 10000, 'score': 59275.69723725319, 'total_duration': 64164.15061426163, 'accumulated_submission_time': 59275.69723725319, 'accumulated_eval_time': 4876.382496595383, 'accumulated_logging_time': 5.40146017074585, 'global_step': 129916, 'preemption_count': 0}), (130838, {'train/accuracy': 0.8397656083106995, 'train/loss': 0.8464886546134949, 'validation/accuracy': 0.753879964351654, 'validation/loss': 1.204972743988037, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7893548011779785, 'test/num_examples': 10000, 'score': 59695.74828839302, 'total_duration': 64622.2776722908, 'accumulated_submission_time': 59695.74828839302, 'accumulated_eval_time': 4914.366876363754, 'accumulated_logging_time': 5.4463982582092285, 'global_step': 130838, 'preemption_count': 0}), (131759, {'train/accuracy': 0.8443359136581421, 'train/loss': 0.7996962666511536, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.184333324432373, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.7539469003677368, 'test/num_examples': 10000, 'score': 60115.708676576614, 'total_duration': 65075.52677679062, 'accumulated_submission_time': 60115.708676576614, 'accumulated_eval_time': 4947.561500549316, 'accumulated_logging_time': 5.493636131286621, 'global_step': 131759, 'preemption_count': 0}), (132681, {'train/accuracy': 0.83753901720047, 'train/loss': 0.8382362127304077, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.1913368701934814, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.769114375114441, 'test/num_examples': 10000, 'score': 60535.7517850399, 'total_duration': 65531.48008418083, 'accumulated_submission_time': 60535.7517850399, 'accumulated_eval_time': 4983.380564451218, 'accumulated_logging_time': 5.537555456161499, 'global_step': 132681, 'preemption_count': 0}), (133603, {'train/accuracy': 0.84046870470047, 'train/loss': 0.8086775541305542, 'validation/accuracy': 0.7542600035667419, 'validation/loss': 1.1765190362930298, 'validation/num_examples': 50000, 'test/accuracy': 0.638200044631958, 'test/loss': 1.7628462314605713, 'test/num_examples': 10000, 'score': 60955.732286930084, 'total_duration': 65985.58762574196, 'accumulated_submission_time': 60955.732286930084, 'accumulated_eval_time': 5017.414732217789, 'accumulated_logging_time': 5.583152770996094, 'global_step': 133603, 'preemption_count': 0}), (134525, {'train/accuracy': 0.8450585603713989, 'train/loss': 0.8082579374313354, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.1792693138122559, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.7707762718200684, 'test/num_examples': 10000, 'score': 61375.7694671154, 'total_duration': 66443.38515496254, 'accumulated_submission_time': 61375.7694671154, 'accumulated_eval_time': 5055.079945325851, 'accumulated_logging_time': 5.6315295696258545, 'global_step': 134525, 'preemption_count': 0}), (135415, {'train/accuracy': 0.8431445360183716, 'train/loss': 0.7971628308296204, 'validation/accuracy': 0.7569400072097778, 'validation/loss': 1.1679935455322266, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.7401326894760132, 'test/num_examples': 10000, 'score': 61795.82298064232, 'total_duration': 66902.62777686119, 'accumulated_submission_time': 61795.82298064232, 'accumulated_eval_time': 5094.165184736252, 'accumulated_logging_time': 5.689408540725708, 'global_step': 135415, 'preemption_count': 0}), (136337, {'train/accuracy': 0.8441015481948853, 'train/loss': 0.8239647746086121, 'validation/accuracy': 0.7590799927711487, 'validation/loss': 1.176174521446228, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.7639225721359253, 'test/num_examples': 10000, 'score': 62215.76799035072, 'total_duration': 67359.71117639542, 'accumulated_submission_time': 62215.76799035072, 'accumulated_eval_time': 5131.2062656879425, 'accumulated_logging_time': 5.739239692687988, 'global_step': 136337, 'preemption_count': 0}), (137259, {'train/accuracy': 0.8472656011581421, 'train/loss': 0.8076205253601074, 'validation/accuracy': 0.7579599618911743, 'validation/loss': 1.1808420419692993, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.7493723630905151, 'test/num_examples': 10000, 'score': 62636.00538110733, 'total_duration': 67813.579870224, 'accumulated_submission_time': 62636.00538110733, 'accumulated_eval_time': 5164.7442235946655, 'accumulated_logging_time': 5.78521990776062, 'global_step': 137259, 'preemption_count': 0}), (138180, {'train/accuracy': 0.8597851395606995, 'train/loss': 0.7563513517379761, 'validation/accuracy': 0.7586399912834167, 'validation/loss': 1.1718906164169312, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.7638990879058838, 'test/num_examples': 10000, 'score': 63055.95963048935, 'total_duration': 68267.94333863258, 'accumulated_submission_time': 63055.95963048935, 'accumulated_eval_time': 5199.060210227966, 'accumulated_logging_time': 5.831464767456055, 'global_step': 138180, 'preemption_count': 0}), (139101, {'train/accuracy': 0.8470116853713989, 'train/loss': 0.8125196099281311, 'validation/accuracy': 0.7589399814605713, 'validation/loss': 1.179947018623352, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.7559922933578491, 'test/num_examples': 10000, 'score': 63476.035716056824, 'total_duration': 68724.00987887383, 'accumulated_submission_time': 63476.035716056824, 'accumulated_eval_time': 5234.952075719833, 'accumulated_logging_time': 5.88202166557312, 'global_step': 139101, 'preemption_count': 0}), (140024, {'train/accuracy': 0.8527734279632568, 'train/loss': 0.758125364780426, 'validation/accuracy': 0.762499988079071, 'validation/loss': 1.136866569519043, 'validation/num_examples': 50000, 'test/accuracy': 0.6468000411987305, 'test/loss': 1.713780164718628, 'test/num_examples': 10000, 'score': 63895.9807267189, 'total_duration': 69180.01283836365, 'accumulated_submission_time': 63895.9807267189, 'accumulated_eval_time': 5270.915474653244, 'accumulated_logging_time': 5.92934775352478, 'global_step': 140024, 'preemption_count': 0}), (140945, {'train/accuracy': 0.8581054210662842, 'train/loss': 0.7595937848091125, 'validation/accuracy': 0.7590199708938599, 'validation/loss': 1.1628683805465698, 'validation/num_examples': 50000, 'test/accuracy': 0.6384000182151794, 'test/loss': 1.7504186630249023, 'test/num_examples': 10000, 'score': 64316.03085923195, 'total_duration': 69633.73905014992, 'accumulated_submission_time': 64316.03085923195, 'accumulated_eval_time': 5304.495651721954, 'accumulated_logging_time': 5.978683710098267, 'global_step': 140945, 'preemption_count': 0}), (141867, {'train/accuracy': 0.8505077958106995, 'train/loss': 0.8011537790298462, 'validation/accuracy': 0.7633000016212463, 'validation/loss': 1.1656519174575806, 'validation/num_examples': 50000, 'test/accuracy': 0.641800045967102, 'test/loss': 1.7470555305480957, 'test/num_examples': 10000, 'score': 64736.08189225197, 'total_duration': 70087.59886169434, 'accumulated_submission_time': 64736.08189225197, 'accumulated_eval_time': 5338.206914901733, 'accumulated_logging_time': 6.028635501861572, 'global_step': 141867, 'preemption_count': 0}), (142788, {'train/accuracy': 0.8565039038658142, 'train/loss': 0.7577779293060303, 'validation/accuracy': 0.7645599842071533, 'validation/loss': 1.1408846378326416, 'validation/num_examples': 50000, 'test/accuracy': 0.6448000073432922, 'test/loss': 1.7246521711349487, 'test/num_examples': 10000, 'score': 65156.10931110382, 'total_duration': 70541.53916501999, 'accumulated_submission_time': 65156.10931110382, 'accumulated_eval_time': 5372.0228152275085, 'accumulated_logging_time': 6.0786073207855225, 'global_step': 142788, 'preemption_count': 0}), (143709, {'train/accuracy': 0.85986328125, 'train/loss': 0.7596256732940674, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.166117787361145, 'validation/num_examples': 50000, 'test/accuracy': 0.6426000595092773, 'test/loss': 1.7453765869140625, 'test/num_examples': 10000, 'score': 65576.03127932549, 'total_duration': 70996.37029480934, 'accumulated_submission_time': 65576.03127932549, 'accumulated_eval_time': 5406.831308364868, 'accumulated_logging_time': 6.132803916931152, 'global_step': 143709, 'preemption_count': 0}), (144631, {'train/accuracy': 0.8544726371765137, 'train/loss': 0.784826934337616, 'validation/accuracy': 0.7655799984931946, 'validation/loss': 1.1578985452651978, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.7437084913253784, 'test/num_examples': 10000, 'score': 65996.08980154991, 'total_duration': 71449.93562602997, 'accumulated_submission_time': 65996.08980154991, 'accumulated_eval_time': 5440.239306926727, 'accumulated_logging_time': 6.184715032577515, 'global_step': 144631, 'preemption_count': 0}), (145554, {'train/accuracy': 0.8596875071525574, 'train/loss': 0.7511771321296692, 'validation/accuracy': 0.765999972820282, 'validation/loss': 1.1414402723312378, 'validation/num_examples': 50000, 'test/accuracy': 0.6479000449180603, 'test/loss': 1.710391640663147, 'test/num_examples': 10000, 'score': 66416.06876826286, 'total_duration': 71906.65629696846, 'accumulated_submission_time': 66416.06876826286, 'accumulated_eval_time': 5476.885877370834, 'accumulated_logging_time': 6.231912851333618, 'global_step': 145554, 'preemption_count': 0}), (146476, {'train/accuracy': 0.8626366853713989, 'train/loss': 0.7605917453765869, 'validation/accuracy': 0.7663599848747253, 'validation/loss': 1.1581281423568726, 'validation/num_examples': 50000, 'test/accuracy': 0.650600016117096, 'test/loss': 1.7234017848968506, 'test/num_examples': 10000, 'score': 66836.00802612305, 'total_duration': 72359.89109659195, 'accumulated_submission_time': 66836.00802612305, 'accumulated_eval_time': 5510.088440179825, 'accumulated_logging_time': 6.2781524658203125, 'global_step': 146476, 'preemption_count': 0}), (147400, {'train/accuracy': 0.8684960603713989, 'train/loss': 0.7181340456008911, 'validation/accuracy': 0.7680400013923645, 'validation/loss': 1.130260705947876, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.714258074760437, 'test/num_examples': 10000, 'score': 67255.94326424599, 'total_duration': 72815.77249288559, 'accumulated_submission_time': 67255.94326424599, 'accumulated_eval_time': 5545.940866231918, 'accumulated_logging_time': 6.324589729309082, 'global_step': 147400, 'preemption_count': 0}), (148314, {'train/accuracy': 0.8642382621765137, 'train/loss': 0.7342635989189148, 'validation/accuracy': 0.7673199772834778, 'validation/loss': 1.126471996307373, 'validation/num_examples': 50000, 'test/accuracy': 0.6473000049591064, 'test/loss': 1.7193629741668701, 'test/num_examples': 10000, 'score': 67676.07908463478, 'total_duration': 73275.14289355278, 'accumulated_submission_time': 67676.07908463478, 'accumulated_eval_time': 5585.072806835175, 'accumulated_logging_time': 6.381242990493774, 'global_step': 148314, 'preemption_count': 0}), (149235, {'train/accuracy': 0.8697851300239563, 'train/loss': 0.7271069884300232, 'validation/accuracy': 0.7689200043678284, 'validation/loss': 1.1391234397888184, 'validation/num_examples': 50000, 'test/accuracy': 0.6455000042915344, 'test/loss': 1.7286360263824463, 'test/num_examples': 10000, 'score': 68096.35448336601, 'total_duration': 73732.74937844276, 'accumulated_submission_time': 68096.35448336601, 'accumulated_eval_time': 5622.308450460434, 'accumulated_logging_time': 6.429650783538818, 'global_step': 149235, 'preemption_count': 0}), (150156, {'train/accuracy': 0.8738671541213989, 'train/loss': 0.7149843573570251, 'validation/accuracy': 0.7697599530220032, 'validation/loss': 1.141154170036316, 'validation/num_examples': 50000, 'test/accuracy': 0.650600016117096, 'test/loss': 1.7134649753570557, 'test/num_examples': 10000, 'score': 68516.60844302177, 'total_duration': 74196.96331691742, 'accumulated_submission_time': 68516.60844302177, 'accumulated_eval_time': 5666.171459674835, 'accumulated_logging_time': 6.480257034301758, 'global_step': 150156, 'preemption_count': 0}), (151078, {'train/accuracy': 0.8681445121765137, 'train/loss': 0.7366388440132141, 'validation/accuracy': 0.7703999876976013, 'validation/loss': 1.1382390260696411, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.7185348272323608, 'test/num_examples': 10000, 'score': 68936.61577987671, 'total_duration': 74650.41727089882, 'accumulated_submission_time': 68936.61577987671, 'accumulated_eval_time': 5699.519348621368, 'accumulated_logging_time': 6.532433748245239, 'global_step': 151078, 'preemption_count': 0}), (151997, {'train/accuracy': 0.8676952719688416, 'train/loss': 0.7144691348075867, 'validation/accuracy': 0.7712999582290649, 'validation/loss': 1.1138060092926025, 'validation/num_examples': 50000, 'test/accuracy': 0.6539000272750854, 'test/loss': 1.6868129968643188, 'test/num_examples': 10000, 'score': 69356.5645096302, 'total_duration': 75107.14363598824, 'accumulated_submission_time': 69356.5645096302, 'accumulated_eval_time': 5736.198989152908, 'accumulated_logging_time': 6.5838096141815186, 'global_step': 151997, 'preemption_count': 0}), (152918, {'train/accuracy': 0.8729101419448853, 'train/loss': 0.6911082863807678, 'validation/accuracy': 0.772879958152771, 'validation/loss': 1.1157152652740479, 'validation/num_examples': 50000, 'test/accuracy': 0.6516000032424927, 'test/loss': 1.699005126953125, 'test/num_examples': 10000, 'score': 69776.55529689789, 'total_duration': 75567.5617249012, 'accumulated_submission_time': 69776.55529689789, 'accumulated_eval_time': 5776.524785995483, 'accumulated_logging_time': 6.637896299362183, 'global_step': 152918, 'preemption_count': 0}), (153837, {'train/accuracy': 0.8698632717132568, 'train/loss': 0.7052730321884155, 'validation/accuracy': 0.7731999754905701, 'validation/loss': 1.1103754043579102, 'validation/num_examples': 50000, 'test/accuracy': 0.656000018119812, 'test/loss': 1.6826592683792114, 'test/num_examples': 10000, 'score': 70196.77774477005, 'total_duration': 76025.97893810272, 'accumulated_submission_time': 70196.77774477005, 'accumulated_eval_time': 5814.613633155823, 'accumulated_logging_time': 6.696936845779419, 'global_step': 153837, 'preemption_count': 0}), (154759, {'train/accuracy': 0.8730077743530273, 'train/loss': 0.7196352481842041, 'validation/accuracy': 0.7730000019073486, 'validation/loss': 1.1262603998184204, 'validation/num_examples': 50000, 'test/accuracy': 0.6591000556945801, 'test/loss': 1.7006280422210693, 'test/num_examples': 10000, 'score': 70616.80341100693, 'total_duration': 76481.1249115467, 'accumulated_submission_time': 70616.80341100693, 'accumulated_eval_time': 5849.63468337059, 'accumulated_logging_time': 6.748908281326294, 'global_step': 154759, 'preemption_count': 0}), (155678, {'train/accuracy': 0.8753125071525574, 'train/loss': 0.6879866719245911, 'validation/accuracy': 0.773419976234436, 'validation/loss': 1.109251618385315, 'validation/num_examples': 50000, 'test/accuracy': 0.6586000323295593, 'test/loss': 1.6903423070907593, 'test/num_examples': 10000, 'score': 71037.05806207657, 'total_duration': 76937.91496515274, 'accumulated_submission_time': 71037.05806207657, 'accumulated_eval_time': 5886.051543951035, 'accumulated_logging_time': 6.808360576629639, 'global_step': 155678, 'preemption_count': 0}), (156597, {'train/accuracy': 0.87353515625, 'train/loss': 0.7132282257080078, 'validation/accuracy': 0.7741999626159668, 'validation/loss': 1.1228692531585693, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.6865030527114868, 'test/num_examples': 10000, 'score': 71457.10474681854, 'total_duration': 77390.49496340752, 'accumulated_submission_time': 71457.10474681854, 'accumulated_eval_time': 5918.482615470886, 'accumulated_logging_time': 6.863985776901245, 'global_step': 156597, 'preemption_count': 0}), (157518, {'train/accuracy': 0.87548828125, 'train/loss': 0.706649124622345, 'validation/accuracy': 0.774619996547699, 'validation/loss': 1.1308605670928955, 'validation/num_examples': 50000, 'test/accuracy': 0.6622000336647034, 'test/loss': 1.6971155405044556, 'test/num_examples': 10000, 'score': 71877.31110310555, 'total_duration': 77843.94621014595, 'accumulated_submission_time': 71877.31110310555, 'accumulated_eval_time': 5951.625692844391, 'accumulated_logging_time': 6.918470144271851, 'global_step': 157518, 'preemption_count': 0}), (158438, {'train/accuracy': 0.8758202791213989, 'train/loss': 0.6879441738128662, 'validation/accuracy': 0.7753399610519409, 'validation/loss': 1.1054383516311646, 'validation/num_examples': 50000, 'test/accuracy': 0.6605000495910645, 'test/loss': 1.6729251146316528, 'test/num_examples': 10000, 'score': 72297.48312687874, 'total_duration': 78298.52132201195, 'accumulated_submission_time': 72297.48312687874, 'accumulated_eval_time': 5985.92941904068, 'accumulated_logging_time': 6.971019268035889, 'global_step': 158438, 'preemption_count': 0}), (159357, {'train/accuracy': 0.8858202695846558, 'train/loss': 0.6633342504501343, 'validation/accuracy': 0.7765199542045593, 'validation/loss': 1.1071991920471191, 'validation/num_examples': 50000, 'test/accuracy': 0.6556000113487244, 'test/loss': 1.6883338689804077, 'test/num_examples': 10000, 'score': 72717.58166861534, 'total_duration': 78752.3706395626, 'accumulated_submission_time': 72717.58166861534, 'accumulated_eval_time': 6019.583042383194, 'accumulated_logging_time': 7.0212483406066895, 'global_step': 159357, 'preemption_count': 0}), (160277, {'train/accuracy': 0.8758593797683716, 'train/loss': 0.6875196099281311, 'validation/accuracy': 0.776479959487915, 'validation/loss': 1.1003152132034302, 'validation/num_examples': 50000, 'test/accuracy': 0.6580000519752502, 'test/loss': 1.683222770690918, 'test/num_examples': 10000, 'score': 73137.55405020714, 'total_duration': 79208.81894946098, 'accumulated_submission_time': 73137.55405020714, 'accumulated_eval_time': 6055.962126255035, 'accumulated_logging_time': 7.0706892013549805, 'global_step': 160277, 'preemption_count': 0}), (161199, {'train/accuracy': 0.8811913728713989, 'train/loss': 0.671556293964386, 'validation/accuracy': 0.7778199911117554, 'validation/loss': 1.1007194519042969, 'validation/num_examples': 50000, 'test/accuracy': 0.6577000021934509, 'test/loss': 1.6750051975250244, 'test/num_examples': 10000, 'score': 73557.74100780487, 'total_duration': 79667.06095504761, 'accumulated_submission_time': 73557.74100780487, 'accumulated_eval_time': 6093.915406227112, 'accumulated_logging_time': 7.124929904937744, 'global_step': 161199, 'preemption_count': 0}), (162117, {'train/accuracy': 0.8856250047683716, 'train/loss': 0.6517072319984436, 'validation/accuracy': 0.7775999903678894, 'validation/loss': 1.0972752571105957, 'validation/num_examples': 50000, 'test/accuracy': 0.6593000292778015, 'test/loss': 1.6683248281478882, 'test/num_examples': 10000, 'score': 73978.00402450562, 'total_duration': 80116.75618052483, 'accumulated_submission_time': 73978.00402450562, 'accumulated_eval_time': 6123.2500858306885, 'accumulated_logging_time': 7.175060510635376, 'global_step': 162117, 'preemption_count': 0}), (163035, {'train/accuracy': 0.8808007836341858, 'train/loss': 0.6864597797393799, 'validation/accuracy': 0.7800799608230591, 'validation/loss': 1.105568766593933, 'validation/num_examples': 50000, 'test/accuracy': 0.6591000556945801, 'test/loss': 1.6852294206619263, 'test/num_examples': 10000, 'score': 74398.24773645401, 'total_duration': 80573.08365154266, 'accumulated_submission_time': 74398.24773645401, 'accumulated_eval_time': 6159.228252887726, 'accumulated_logging_time': 7.233767747879028, 'global_step': 163035, 'preemption_count': 0}), (163955, {'train/accuracy': 0.8822265267372131, 'train/loss': 0.6664526462554932, 'validation/accuracy': 0.7804200053215027, 'validation/loss': 1.0926640033721924, 'validation/num_examples': 50000, 'test/accuracy': 0.6650000214576721, 'test/loss': 1.66836416721344, 'test/num_examples': 10000, 'score': 74818.47779989243, 'total_duration': 81033.72079610825, 'accumulated_submission_time': 74818.47779989243, 'accumulated_eval_time': 6199.534796953201, 'accumulated_logging_time': 7.287166118621826, 'global_step': 163955, 'preemption_count': 0}), (164877, {'train/accuracy': 0.88636714220047, 'train/loss': 0.653562068939209, 'validation/accuracy': 0.7805599570274353, 'validation/loss': 1.0905064344406128, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.6655395030975342, 'test/num_examples': 10000, 'score': 75238.48033547401, 'total_duration': 81487.14139032364, 'accumulated_submission_time': 75238.48033547401, 'accumulated_eval_time': 6232.852535486221, 'accumulated_logging_time': 7.340670824050903, 'global_step': 164877, 'preemption_count': 0}), (165798, {'train/accuracy': 0.8838866949081421, 'train/loss': 0.6820747256278992, 'validation/accuracy': 0.7806800007820129, 'validation/loss': 1.1070207357406616, 'validation/num_examples': 50000, 'test/accuracy': 0.6610000133514404, 'test/loss': 1.6883653402328491, 'test/num_examples': 10000, 'score': 75658.44529294968, 'total_duration': 81947.32384061813, 'accumulated_submission_time': 75658.44529294968, 'accumulated_eval_time': 6272.963407754898, 'accumulated_logging_time': 7.399810791015625, 'global_step': 165798, 'preemption_count': 0}), (166719, {'train/accuracy': 0.8874218463897705, 'train/loss': 0.640555739402771, 'validation/accuracy': 0.7835800051689148, 'validation/loss': 1.0742700099945068, 'validation/num_examples': 50000, 'test/accuracy': 0.6648000478744507, 'test/loss': 1.6466233730316162, 'test/num_examples': 10000, 'score': 76078.42911958694, 'total_duration': 82398.08100652695, 'accumulated_submission_time': 76078.42911958694, 'accumulated_eval_time': 6303.624835968018, 'accumulated_logging_time': 7.464045286178589, 'global_step': 166719, 'preemption_count': 0}), (167641, {'train/accuracy': 0.8880468606948853, 'train/loss': 0.6393408179283142, 'validation/accuracy': 0.7837599515914917, 'validation/loss': 1.0733433961868286, 'validation/num_examples': 50000, 'test/accuracy': 0.6659000515937805, 'test/loss': 1.6449536085128784, 'test/num_examples': 10000, 'score': 76498.71252512932, 'total_duration': 82852.74976229668, 'accumulated_submission_time': 76498.71252512932, 'accumulated_eval_time': 6337.89848613739, 'accumulated_logging_time': 7.528738975524902, 'global_step': 167641, 'preemption_count': 0}), (168563, {'train/accuracy': 0.8868163824081421, 'train/loss': 0.655565083026886, 'validation/accuracy': 0.7830599546432495, 'validation/loss': 1.089125394821167, 'validation/num_examples': 50000, 'test/accuracy': 0.6633000373840332, 'test/loss': 1.6611697673797607, 'test/num_examples': 10000, 'score': 76918.80696439743, 'total_duration': 83308.578540802, 'accumulated_submission_time': 76918.80696439743, 'accumulated_eval_time': 6373.534379959106, 'accumulated_logging_time': 7.579162836074829, 'global_step': 168563, 'preemption_count': 0}), (169484, {'train/accuracy': 0.8880468606948853, 'train/loss': 0.6424294710159302, 'validation/accuracy': 0.7831000089645386, 'validation/loss': 1.0735559463500977, 'validation/num_examples': 50000, 'test/accuracy': 0.6653000116348267, 'test/loss': 1.646395206451416, 'test/num_examples': 10000, 'score': 77338.80617928505, 'total_duration': 83767.41523122787, 'accumulated_submission_time': 77338.80617928505, 'accumulated_eval_time': 6412.271682739258, 'accumulated_logging_time': 7.63238000869751, 'global_step': 169484, 'preemption_count': 0})], 'global_step': 169887}
I0201 12:15:40.559405 140184451094336 submission_runner.py:586] Timing: 77520.2169020176
I0201 12:15:40.559484 140184451094336 submission_runner.py:588] Total number of evals: 185
I0201 12:15:40.559533 140184451094336 submission_runner.py:589] ====================
I0201 12:15:40.559578 140184451094336 submission_runner.py:542] Using RNG seed 3390244169
I0201 12:15:40.560911 140184451094336 submission_runner.py:551] --- Tuning run 3/5 ---
I0201 12:15:40.561020 140184451094336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3.
I0201 12:15:40.563848 140184451094336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3/hparams.json.
I0201 12:15:40.564668 140184451094336 submission_runner.py:206] Initializing dataset.
I0201 12:15:40.573610 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0201 12:15:40.583566 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0201 12:15:40.789881 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0201 12:15:44.999223 140184451094336 submission_runner.py:213] Initializing model.
I0201 12:15:51.389532 140184451094336 submission_runner.py:255] Initializing optimizer.
I0201 12:15:51.852442 140184451094336 submission_runner.py:262] Initializing metrics bundle.
I0201 12:15:51.852601 140184451094336 submission_runner.py:280] Initializing checkpoint and logger.
I0201 12:15:51.868322 140184451094336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3 with prefix checkpoint_
I0201 12:15:51.868433 140184451094336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0201 12:16:07.972684 140184451094336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0201 12:16:23.826762 140184451094336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3/flags_0.json.
I0201 12:16:23.832417 140184451094336 submission_runner.py:314] Starting training loop.
I0201 12:17:00.226525 140022493714176 logging_writer.py:48] [0] global_step=0, grad_norm=0.3648875653743744, loss=6.9077558517456055
I0201 12:17:00.240375 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:17:08.602591 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:17:26.035392 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:17:27.628345 140184451094336 submission_runner.py:408] Time since start: 63.80s, 	Step: 1, 	{'train/accuracy': 0.0011914062779396772, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.40782308578491, 'total_duration': 63.7958824634552, 'accumulated_submission_time': 36.40782308578491, 'accumulated_eval_time': 27.387945890426636, 'accumulated_logging_time': 0}
I0201 12:17:27.636708 140022502106880 logging_writer.py:48] [1] accumulated_eval_time=27.387946, accumulated_logging_time=0, accumulated_submission_time=36.407823, global_step=1, preemption_count=0, score=36.407823, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=63.795882, train/accuracy=0.001191, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0201 12:18:33.590898 140023005427456 logging_writer.py:48] [100] global_step=100, grad_norm=0.42844563722610474, loss=6.905877113342285
I0201 12:19:19.341839 140022518892288 logging_writer.py:48] [200] global_step=200, grad_norm=0.42369481921195984, loss=6.897512435913086
I0201 12:20:06.218123 140023005427456 logging_writer.py:48] [300] global_step=300, grad_norm=0.5885602235794067, loss=6.853508949279785
I0201 12:20:53.161238 140022518892288 logging_writer.py:48] [400] global_step=400, grad_norm=0.8148109912872314, loss=6.800468921661377
I0201 12:21:39.752616 140023005427456 logging_writer.py:48] [500] global_step=500, grad_norm=1.2914317846298218, loss=6.805516719818115
I0201 12:22:26.356491 140022518892288 logging_writer.py:48] [600] global_step=600, grad_norm=1.1203699111938477, loss=6.7263336181640625
I0201 12:23:13.100854 140023005427456 logging_writer.py:48] [700] global_step=700, grad_norm=1.0554678440093994, loss=6.646402359008789
I0201 12:23:59.353670 140022518892288 logging_writer.py:48] [800] global_step=800, grad_norm=1.5437915325164795, loss=6.587700843811035
I0201 12:24:27.658871 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:24:39.288476 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:25:04.302327 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:25:05.906958 140184451094336 submission_runner.py:408] Time since start: 522.07s, 	Step: 862, 	{'train/accuracy': 0.011425781063735485, 'train/loss': 6.446063041687012, 'validation/accuracy': 0.011159999296069145, 'validation/loss': 6.462528705596924, 'validation/num_examples': 50000, 'test/accuracy': 0.008100000210106373, 'test/loss': 6.500799655914307, 'test/num_examples': 10000, 'score': 456.3771412372589, 'total_duration': 522.0745017528534, 'accumulated_submission_time': 456.3771412372589, 'accumulated_eval_time': 65.63604092597961, 'accumulated_logging_time': 0.01734304428100586}
I0201 12:25:05.921812 140023005427456 logging_writer.py:48] [862] accumulated_eval_time=65.636041, accumulated_logging_time=0.017343, accumulated_submission_time=456.377141, global_step=862, preemption_count=0, score=456.377141, test/accuracy=0.008100, test/loss=6.500800, test/num_examples=10000, total_duration=522.074502, train/accuracy=0.011426, train/loss=6.446063, validation/accuracy=0.011160, validation/loss=6.462529, validation/num_examples=50000
I0201 12:25:21.503432 140022518892288 logging_writer.py:48] [900] global_step=900, grad_norm=1.003468632698059, loss=6.5030436515808105
I0201 12:26:05.067942 140023005427456 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.2712647914886475, loss=6.5310163497924805
I0201 12:26:51.249563 140022518892288 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.0698843002319336, loss=6.555246353149414
I0201 12:27:37.771202 140023005427456 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.6719379425048828, loss=6.376072406768799
I0201 12:28:24.187538 140022518892288 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.5023773908615112, loss=6.603590488433838
I0201 12:29:10.597276 140023005427456 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.0512425899505615, loss=6.3122334480285645
I0201 12:29:56.597796 140022518892288 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.9628957509994507, loss=6.624142646789551
I0201 12:30:43.030893 140023005427456 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.0302295684814453, loss=6.127065181732178
I0201 12:31:29.168807 140022518892288 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.6598198413848877, loss=6.116994380950928
I0201 12:32:06.297978 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:32:17.042640 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:32:42.479619 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:32:44.088031 140184451094336 submission_runner.py:408] Time since start: 980.26s, 	Step: 1781, 	{'train/accuracy': 0.037773437798023224, 'train/loss': 5.867616653442383, 'validation/accuracy': 0.03659999743103981, 'validation/loss': 5.891701698303223, 'validation/num_examples': 50000, 'test/accuracy': 0.03060000203549862, 'test/loss': 6.009640693664551, 'test/num_examples': 10000, 'score': 876.6863760948181, 'total_duration': 980.255558013916, 'accumulated_submission_time': 876.6863760948181, 'accumulated_eval_time': 103.42611718177795, 'accumulated_logging_time': 0.051421165466308594}
I0201 12:32:44.104770 140023005427456 logging_writer.py:48] [1781] accumulated_eval_time=103.426117, accumulated_logging_time=0.051421, accumulated_submission_time=876.686376, global_step=1781, preemption_count=0, score=876.686376, test/accuracy=0.030600, test/loss=6.009641, test/num_examples=10000, total_duration=980.255558, train/accuracy=0.037773, train/loss=5.867617, validation/accuracy=0.036600, validation/loss=5.891702, validation/num_examples=50000
I0201 12:32:52.110353 140022518892288 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.166696071624756, loss=6.117079734802246
I0201 12:33:35.514902 140023005427456 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.041412115097046, loss=6.04324197769165
I0201 12:34:21.510476 140022518892288 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.0469865798950195, loss=6.364161968231201
I0201 12:35:07.625448 140023005427456 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.2216999530792236, loss=5.9498138427734375
I0201 12:35:53.671222 140022518892288 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.8309465646743774, loss=5.960882186889648
I0201 12:36:39.840508 140023005427456 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.592862367630005, loss=5.9155144691467285
I0201 12:37:26.106615 140022518892288 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.3336451053619385, loss=5.809352397918701
I0201 12:38:12.334274 140023005427456 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.9969584941864014, loss=6.472469806671143
I0201 12:38:58.642703 140022518892288 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.1205763816833496, loss=5.835453510284424
I0201 12:39:44.096683 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:39:54.847853 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:40:26.693384 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:40:28.298128 140184451094336 submission_runner.py:408] Time since start: 1444.47s, 	Step: 2700, 	{'train/accuracy': 0.06560546904802322, 'train/loss': 5.406094074249268, 'validation/accuracy': 0.0608999989926815, 'validation/loss': 5.4456024169921875, 'validation/num_examples': 50000, 'test/accuracy': 0.04620000347495079, 'test/loss': 5.637139320373535, 'test/num_examples': 10000, 'score': 1296.615249156952, 'total_duration': 1444.4656417369843, 'accumulated_submission_time': 1296.615249156952, 'accumulated_eval_time': 147.62753534317017, 'accumulated_logging_time': 0.08417248725891113}
I0201 12:40:28.315448 140023005427456 logging_writer.py:48] [2700] accumulated_eval_time=147.627535, accumulated_logging_time=0.084172, accumulated_submission_time=1296.615249, global_step=2700, preemption_count=0, score=1296.615249, test/accuracy=0.046200, test/loss=5.637139, test/num_examples=10000, total_duration=1444.465642, train/accuracy=0.065605, train/loss=5.406094, validation/accuracy=0.060900, validation/loss=5.445602, validation/num_examples=50000
I0201 12:40:28.715890 140022518892288 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.8646769523620605, loss=6.163456916809082
I0201 12:41:10.172111 140023005427456 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.6671290397644043, loss=5.836321830749512
I0201 12:41:56.238055 140022518892288 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.367629289627075, loss=5.767237663269043
I0201 12:42:42.663964 140023005427456 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.9830049276351929, loss=6.2767486572265625
I0201 12:43:28.912189 140022518892288 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.8657524585723877, loss=5.666056156158447
I0201 12:44:15.030024 140023005427456 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.9263163805007935, loss=6.517635345458984
I0201 12:45:01.242467 140022518892288 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.643890619277954, loss=5.650233745574951
I0201 12:45:47.338548 140023005427456 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.296961784362793, loss=5.670427322387695
I0201 12:46:33.695884 140022518892288 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.2628955841064453, loss=5.608978748321533
I0201 12:47:19.942143 140023005427456 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.2082033157348633, loss=5.705844879150391
I0201 12:47:28.376169 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:47:39.551417 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:48:03.995795 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:48:05.597581 140184451094336 submission_runner.py:408] Time since start: 1901.77s, 	Step: 3620, 	{'train/accuracy': 0.09458984434604645, 'train/loss': 5.085729122161865, 'validation/accuracy': 0.08721999824047089, 'validation/loss': 5.123086929321289, 'validation/num_examples': 50000, 'test/accuracy': 0.06670000404119492, 'test/loss': 5.365293502807617, 'test/num_examples': 10000, 'score': 1716.6178047657013, 'total_duration': 1901.7651226520538, 'accumulated_submission_time': 1716.6178047657013, 'accumulated_eval_time': 184.84893035888672, 'accumulated_logging_time': 0.1116495132446289}
I0201 12:48:05.611876 140022518892288 logging_writer.py:48] [3620] accumulated_eval_time=184.848930, accumulated_logging_time=0.111650, accumulated_submission_time=1716.617805, global_step=3620, preemption_count=0, score=1716.617805, test/accuracy=0.066700, test/loss=5.365294, test/num_examples=10000, total_duration=1901.765123, train/accuracy=0.094590, train/loss=5.085729, validation/accuracy=0.087220, validation/loss=5.123087, validation/num_examples=50000
I0201 12:48:37.975505 140023005427456 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.6349879503250122, loss=6.01783561706543
I0201 12:49:23.938330 140022518892288 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.958746314048767, loss=5.803315162658691
I0201 12:50:10.270315 140023005427456 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.8790209293365479, loss=5.422094821929932
I0201 12:50:56.901829 140022518892288 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.864389181137085, loss=6.245246410369873
I0201 12:51:43.219754 140023005427456 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.242617130279541, loss=5.694705486297607
I0201 12:52:29.621550 140022518892288 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.882217526435852, loss=5.4165849685668945
I0201 12:53:15.676297 140023005427456 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.6424307823181152, loss=5.341920852661133
I0201 12:54:01.862643 140022518892288 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.4598209857940674, loss=6.013138294219971
I0201 12:54:48.003258 140023005427456 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.9468111991882324, loss=5.304676055908203
I0201 12:55:05.626749 140184451094336 spec.py:321] Evaluating on the training split.
I0201 12:55:16.352128 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 12:55:41.958678 140184451094336 spec.py:349] Evaluating on the test split.
I0201 12:55:43.567506 140184451094336 submission_runner.py:408] Time since start: 2359.74s, 	Step: 4540, 	{'train/accuracy': 0.13783203065395355, 'train/loss': 4.668841361999512, 'validation/accuracy': 0.1275399923324585, 'validation/loss': 4.732216835021973, 'validation/num_examples': 50000, 'test/accuracy': 0.09450000524520874, 'test/loss': 5.031833648681641, 'test/num_examples': 10000, 'score': 2136.5751678943634, 'total_duration': 2359.7350487709045, 'accumulated_submission_time': 2136.5751678943634, 'accumulated_eval_time': 222.78968811035156, 'accumulated_logging_time': 0.1356348991394043}
I0201 12:55:43.582196 140022518892288 logging_writer.py:48] [4540] accumulated_eval_time=222.789688, accumulated_logging_time=0.135635, accumulated_submission_time=2136.575168, global_step=4540, preemption_count=0, score=2136.575168, test/accuracy=0.094500, test/loss=5.031834, test/num_examples=10000, total_duration=2359.735049, train/accuracy=0.137832, train/loss=4.668841, validation/accuracy=0.127540, validation/loss=4.732217, validation/num_examples=50000
I0201 12:56:07.971272 140023005427456 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.516974687576294, loss=6.498010158538818
I0201 12:56:53.341051 140022518892288 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.778057336807251, loss=5.359397888183594
I0201 12:57:39.781880 140023005427456 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.929840087890625, loss=5.533620834350586
I0201 12:58:26.165621 140022518892288 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.4513241052627563, loss=6.405085563659668
I0201 12:59:12.547496 140023005427456 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.4610356092453003, loss=6.18523645401001
I0201 12:59:58.765529 140022518892288 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.026383638381958, loss=5.061079502105713
I0201 13:00:45.208487 140023005427456 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.0672237873077393, loss=5.38943338394165
I0201 13:01:31.563270 140022518892288 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.1811258792877197, loss=5.084918975830078
I0201 13:02:17.924610 140023005427456 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.7220748662948608, loss=5.719771385192871
I0201 13:02:43.665913 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:02:54.791314 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:03:16.826393 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:03:18.433713 140184451094336 submission_runner.py:408] Time since start: 2814.60s, 	Step: 5457, 	{'train/accuracy': 0.17052733898162842, 'train/loss': 4.367717742919922, 'validation/accuracy': 0.15661999583244324, 'validation/loss': 4.442732334136963, 'validation/num_examples': 50000, 'test/accuracy': 0.11970000714063644, 'test/loss': 4.782550811767578, 'test/num_examples': 10000, 'score': 2556.6016159057617, 'total_duration': 2814.6012556552887, 'accumulated_submission_time': 2556.6016159057617, 'accumulated_eval_time': 257.5574834346771, 'accumulated_logging_time': 0.16059088706970215}
I0201 13:03:18.452591 140022518892288 logging_writer.py:48] [5457] accumulated_eval_time=257.557483, accumulated_logging_time=0.160591, accumulated_submission_time=2556.601616, global_step=5457, preemption_count=0, score=2556.601616, test/accuracy=0.119700, test/loss=4.782551, test/num_examples=10000, total_duration=2814.601256, train/accuracy=0.170527, train/loss=4.367718, validation/accuracy=0.156620, validation/loss=4.442732, validation/num_examples=50000
I0201 13:03:36.065130 140023005427456 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.302929639816284, loss=5.151622772216797
I0201 13:04:19.927882 140022518892288 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.5697832107543945, loss=5.687156677246094
I0201 13:05:06.365635 140023005427456 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.8154289722442627, loss=5.205816268920898
I0201 13:05:52.923429 140022518892288 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.7885712385177612, loss=4.886396408081055
I0201 13:06:39.172706 140023005427456 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.7109225988388062, loss=5.370505332946777
I0201 13:07:25.315998 140022518892288 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.806579351425171, loss=4.697596549987793
I0201 13:08:11.480102 140023005427456 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.008910894393921, loss=6.329128265380859
I0201 13:08:57.303104 140022518892288 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.8587617874145508, loss=5.750117301940918
I0201 13:09:43.561167 140023005427456 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.4939364194869995, loss=6.103440761566162
I0201 13:10:18.845817 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:10:29.464985 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:10:50.899231 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:10:52.510571 140184451094336 submission_runner.py:408] Time since start: 3268.68s, 	Step: 6378, 	{'train/accuracy': 0.2243359386920929, 'train/loss': 3.9350767135620117, 'validation/accuracy': 0.201339989900589, 'validation/loss': 4.064193248748779, 'validation/num_examples': 50000, 'test/accuracy': 0.15040001273155212, 'test/loss': 4.456921100616455, 'test/num_examples': 10000, 'score': 2976.9379460811615, 'total_duration': 3268.678102016449, 'accumulated_submission_time': 2976.9379460811615, 'accumulated_eval_time': 291.22222685813904, 'accumulated_logging_time': 0.18914508819580078}
I0201 13:10:52.531132 140022518892288 logging_writer.py:48] [6378] accumulated_eval_time=291.222227, accumulated_logging_time=0.189145, accumulated_submission_time=2976.937946, global_step=6378, preemption_count=0, score=2976.937946, test/accuracy=0.150400, test/loss=4.456921, test/num_examples=10000, total_duration=3268.678102, train/accuracy=0.224336, train/loss=3.935077, validation/accuracy=0.201340, validation/loss=4.064193, validation/num_examples=50000
I0201 13:11:01.726346 140023005427456 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.7904560565948486, loss=4.690585136413574
I0201 13:11:45.151185 140022518892288 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.7715696096420288, loss=6.216513156890869
I0201 13:12:31.417460 140023005427456 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.9706593751907349, loss=4.646816253662109
I0201 13:13:18.028451 140022518892288 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.9859760999679565, loss=4.626078128814697
I0201 13:14:04.167370 140023005427456 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.3596367835998535, loss=6.207355499267578
I0201 13:14:50.448961 140022518892288 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.8689998388290405, loss=4.6988654136657715
I0201 13:15:36.966123 140023005427456 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.1461453437805176, loss=4.631404876708984
I0201 13:16:23.182970 140022518892288 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.4219560623168945, loss=4.475419044494629
I0201 13:17:09.366597 140023005427456 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.685634732246399, loss=5.827184200286865
I0201 13:17:52.552988 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:18:03.510073 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:18:31.516305 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:18:33.106228 140184451094336 submission_runner.py:408] Time since start: 3729.27s, 	Step: 7295, 	{'train/accuracy': 0.25927734375, 'train/loss': 3.6954123973846436, 'validation/accuracy': 0.24155999720096588, 'validation/loss': 3.7877326011657715, 'validation/num_examples': 50000, 'test/accuracy': 0.18250000476837158, 'test/loss': 4.207826137542725, 'test/num_examples': 10000, 'score': 3396.901723384857, 'total_duration': 3729.273754119873, 'accumulated_submission_time': 3396.901723384857, 'accumulated_eval_time': 331.77545142173767, 'accumulated_logging_time': 0.2208545207977295}
I0201 13:18:33.121558 140022518892288 logging_writer.py:48] [7295] accumulated_eval_time=331.775451, accumulated_logging_time=0.220855, accumulated_submission_time=3396.901723, global_step=7295, preemption_count=0, score=3396.901723, test/accuracy=0.182500, test/loss=4.207826, test/num_examples=10000, total_duration=3729.273754, train/accuracy=0.259277, train/loss=3.695412, validation/accuracy=0.241560, validation/loss=3.787733, validation/num_examples=50000
I0201 13:18:35.524624 140023005427456 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.109785795211792, loss=4.519277095794678
I0201 13:19:16.917733 140022518892288 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.5854932069778442, loss=6.169069290161133
I0201 13:20:03.136367 140023005427456 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.9004411697387695, loss=4.403145790100098
I0201 13:20:49.595928 140022518892288 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.269602060317993, loss=4.367059707641602
I0201 13:21:35.911933 140023005427456 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.8943135738372803, loss=4.507588863372803
I0201 13:22:22.007522 140022518892288 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.9185724258422852, loss=4.391676902770996
I0201 13:23:08.053419 140023005427456 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.4635043144226074, loss=5.5521721839904785
I0201 13:23:54.001255 140022518892288 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.517918109893799, loss=4.356196403503418
I0201 13:24:40.558128 140023005427456 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.9341309070587158, loss=4.516191482543945
I0201 13:25:27.012359 140022518892288 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.6636512279510498, loss=4.598143577575684
I0201 13:25:33.471388 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:25:43.986747 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:26:07.783213 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:26:09.383529 140184451094336 submission_runner.py:408] Time since start: 4185.55s, 	Step: 8216, 	{'train/accuracy': 0.2934960722923279, 'train/loss': 3.44327449798584, 'validation/accuracy': 0.27337998151779175, 'validation/loss': 3.552814483642578, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.0174102783203125, 'test/num_examples': 10000, 'score': 3817.1914489269257, 'total_duration': 4185.551072597504, 'accumulated_submission_time': 3817.1914489269257, 'accumulated_eval_time': 367.68761444091797, 'accumulated_logging_time': 0.247636079788208}
I0201 13:26:09.398779 140023005427456 logging_writer.py:48] [8216] accumulated_eval_time=367.687614, accumulated_logging_time=0.247636, accumulated_submission_time=3817.191449, global_step=8216, preemption_count=0, score=3817.191449, test/accuracy=0.208200, test/loss=4.017410, test/num_examples=10000, total_duration=4185.551073, train/accuracy=0.293496, train/loss=3.443274, validation/accuracy=0.273380, validation/loss=3.552814, validation/num_examples=50000
I0201 13:26:43.650351 140022518892288 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.785028100013733, loss=4.296189785003662
I0201 13:27:29.643972 140023005427456 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.5548173189163208, loss=5.770413398742676
I0201 13:28:16.004356 140022518892288 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.7346793413162231, loss=4.642345905303955
I0201 13:29:02.519299 140023005427456 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.309262275695801, loss=4.1509013175964355
I0201 13:29:48.691781 140022518892288 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.5465096235275269, loss=5.597967147827148
I0201 13:30:35.226657 140023005427456 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.0949642658233643, loss=4.114829063415527
I0201 13:31:21.513803 140022518892288 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.414341688156128, loss=5.559157371520996
I0201 13:32:07.918454 140023005427456 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4410868883132935, loss=6.065889835357666
I0201 13:32:53.889753 140022518892288 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.37606680393219, loss=6.094435691833496
I0201 13:33:09.691509 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:33:20.677404 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:33:42.557414 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:33:44.159460 140184451094336 submission_runner.py:408] Time since start: 4640.33s, 	Step: 9136, 	{'train/accuracy': 0.3303906321525574, 'train/loss': 3.2213683128356934, 'validation/accuracy': 0.2978399991989136, 'validation/loss': 3.398451089859009, 'validation/num_examples': 50000, 'test/accuracy': 0.23120000958442688, 'test/loss': 3.8833281993865967, 'test/num_examples': 10000, 'score': 4237.426783323288, 'total_duration': 4640.326997518539, 'accumulated_submission_time': 4237.426783323288, 'accumulated_eval_time': 402.15560007095337, 'accumulated_logging_time': 0.27222752571105957}
I0201 13:33:44.177234 140023005427456 logging_writer.py:48] [9136] accumulated_eval_time=402.155600, accumulated_logging_time=0.272228, accumulated_submission_time=4237.426783, global_step=9136, preemption_count=0, score=4237.426783, test/accuracy=0.231200, test/loss=3.883328, test/num_examples=10000, total_duration=4640.326998, train/accuracy=0.330391, train/loss=3.221368, validation/accuracy=0.297840, validation/loss=3.398451, validation/num_examples=50000
I0201 13:34:10.172393 140022518892288 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.9970184564590454, loss=4.010526180267334
I0201 13:34:55.223418 140023005427456 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.42252516746521, loss=5.246721267700195
I0201 13:35:42.081125 140022518892288 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.7758997678756714, loss=4.370087623596191
I0201 13:36:28.737541 140023005427456 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.6850948333740234, loss=4.71330451965332
I0201 13:37:14.936238 140022518892288 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.7821635007858276, loss=3.971266031265259
I0201 13:38:01.062405 140023005427456 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.4010933637619019, loss=5.046052932739258
I0201 13:38:47.345292 140022518892288 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.8685778379440308, loss=3.9343690872192383
I0201 13:39:33.859980 140023005427456 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.8511180877685547, loss=4.2116570472717285
I0201 13:40:20.289936 140022518892288 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4329854249954224, loss=5.436568260192871
I0201 13:40:44.417678 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:40:55.019978 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:41:19.993997 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:41:21.601717 140184451094336 submission_runner.py:408] Time since start: 5097.77s, 	Step: 10054, 	{'train/accuracy': 0.35636717081069946, 'train/loss': 3.065946578979492, 'validation/accuracy': 0.3330000042915344, 'validation/loss': 3.185922622680664, 'validation/num_examples': 50000, 'test/accuracy': 0.2563000023365021, 'test/loss': 3.715104103088379, 'test/num_examples': 10000, 'score': 4657.610203266144, 'total_duration': 5097.769255399704, 'accumulated_submission_time': 4657.610203266144, 'accumulated_eval_time': 439.33963537216187, 'accumulated_logging_time': 0.2994673252105713}
I0201 13:41:21.624496 140023005427456 logging_writer.py:48] [10054] accumulated_eval_time=439.339635, accumulated_logging_time=0.299467, accumulated_submission_time=4657.610203, global_step=10054, preemption_count=0, score=4657.610203, test/accuracy=0.256300, test/loss=3.715104, test/num_examples=10000, total_duration=5097.769255, train/accuracy=0.356367, train/loss=3.065947, validation/accuracy=0.333000, validation/loss=3.185923, validation/num_examples=50000
I0201 13:41:40.436900 140022518892288 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.9229702949523926, loss=3.90846848487854
I0201 13:42:24.468061 140023005427456 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5509287118911743, loss=4.181894302368164
I0201 13:43:11.064519 140022518892288 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.337350606918335, loss=3.8591084480285645
I0201 13:43:57.324487 140023005427456 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.0736019611358643, loss=3.7530901432037354
I0201 13:44:43.658917 140022518892288 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.8733043670654297, loss=3.850602149963379
I0201 13:45:30.044640 140023005427456 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.1453239917755127, loss=5.45632791519165
I0201 13:46:16.064005 140022518892288 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.4007737636566162, loss=5.647778511047363
I0201 13:47:02.223107 140023005427456 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.0019755363464355, loss=3.8323400020599365
I0201 13:47:48.607241 140022518892288 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.3701919317245483, loss=4.2887959480285645
I0201 13:48:21.615954 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:48:32.388005 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:48:58.579326 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:49:00.177551 140184451094336 submission_runner.py:408] Time since start: 5556.35s, 	Step: 10973, 	{'train/accuracy': 0.3862695097923279, 'train/loss': 2.848928213119507, 'validation/accuracy': 0.35711997747421265, 'validation/loss': 2.993165969848633, 'validation/num_examples': 50000, 'test/accuracy': 0.2735000252723694, 'test/loss': 3.5459067821502686, 'test/num_examples': 10000, 'score': 5077.543553113937, 'total_duration': 5556.3450927734375, 'accumulated_submission_time': 5077.543553113937, 'accumulated_eval_time': 477.90121936798096, 'accumulated_logging_time': 0.3332977294921875}
I0201 13:49:00.193636 140023005427456 logging_writer.py:48] [10973] accumulated_eval_time=477.901219, accumulated_logging_time=0.333298, accumulated_submission_time=5077.543553, global_step=10973, preemption_count=0, score=5077.543553, test/accuracy=0.273500, test/loss=3.545907, test/num_examples=10000, total_duration=5556.345093, train/accuracy=0.386270, train/loss=2.848928, validation/accuracy=0.357120, validation/loss=2.993166, validation/num_examples=50000
I0201 13:49:11.404028 140022518892288 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.7048308849334717, loss=3.988330841064453
I0201 13:49:54.685349 140023005427456 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.6430485248565674, loss=3.9904775619506836
I0201 13:50:40.885851 140022518892288 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.557918667793274, loss=5.291518688201904
I0201 13:51:27.554000 140023005427456 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.680662989616394, loss=3.69276762008667
I0201 13:52:13.956121 140022518892288 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.5508142709732056, loss=4.786776542663574
I0201 13:53:00.141386 140023005427456 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.6466320753097534, loss=4.0072431564331055
I0201 13:53:46.353757 140022518892288 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.5874667167663574, loss=3.957193374633789
I0201 13:54:32.572434 140023005427456 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.6646308898925781, loss=3.7726101875305176
I0201 13:55:18.799294 140022518892288 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.876788854598999, loss=3.7248921394348145
I0201 13:56:00.341223 140184451094336 spec.py:321] Evaluating on the training split.
I0201 13:56:10.899842 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 13:56:33.362809 140184451094336 spec.py:349] Evaluating on the test split.
I0201 13:56:34.966245 140184451094336 submission_runner.py:408] Time since start: 6011.13s, 	Step: 11892, 	{'train/accuracy': 0.40437498688697815, 'train/loss': 2.7795512676239014, 'validation/accuracy': 0.367279976606369, 'validation/loss': 2.9624674320220947, 'validation/num_examples': 50000, 'test/accuracy': 0.287200003862381, 'test/loss': 3.5046021938323975, 'test/num_examples': 10000, 'score': 5497.630401134491, 'total_duration': 6011.133774995804, 'accumulated_submission_time': 5497.630401134491, 'accumulated_eval_time': 512.5262434482574, 'accumulated_logging_time': 0.3625199794769287}
I0201 13:56:34.983192 140023005427456 logging_writer.py:48] [11892] accumulated_eval_time=512.526243, accumulated_logging_time=0.362520, accumulated_submission_time=5497.630401, global_step=11892, preemption_count=0, score=5497.630401, test/accuracy=0.287200, test/loss=3.504602, test/num_examples=10000, total_duration=6011.133775, train/accuracy=0.404375, train/loss=2.779551, validation/accuracy=0.367280, validation/loss=2.962467, validation/num_examples=50000
I0201 13:56:38.582202 140022518892288 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.7342493534088135, loss=3.734959602355957
I0201 13:57:21.591615 140023005427456 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4951099157333374, loss=4.286226272583008
I0201 13:58:07.535723 140022518892288 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.770653486251831, loss=3.7365329265594482
I0201 13:58:53.844192 140023005427456 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.7830071449279785, loss=3.9601573944091797
I0201 13:59:40.053480 140022518892288 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.215278148651123, loss=5.884793758392334
I0201 14:00:26.463582 140023005427456 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.3128730058670044, loss=4.9780426025390625
I0201 14:01:13.257746 140022518892288 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.2491905689239502, loss=5.622274398803711
I0201 14:01:59.393088 140023005427456 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.014373302459717, loss=3.618711471557617
I0201 14:02:45.629643 140022518892288 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.256001353263855, loss=4.997179985046387
I0201 14:03:31.654438 140023005427456 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.5935169458389282, loss=4.189069747924805
I0201 14:03:35.039015 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:03:45.917682 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:04:10.898505 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:04:12.508914 140184451094336 submission_runner.py:408] Time since start: 6468.68s, 	Step: 12809, 	{'train/accuracy': 0.42238280177116394, 'train/loss': 2.665945291519165, 'validation/accuracy': 0.3883399963378906, 'validation/loss': 2.8275606632232666, 'validation/num_examples': 50000, 'test/accuracy': 0.3069000244140625, 'test/loss': 3.3749306201934814, 'test/num_examples': 10000, 'score': 5917.629370927811, 'total_duration': 6468.676458835602, 'accumulated_submission_time': 5917.629370927811, 'accumulated_eval_time': 549.9961397647858, 'accumulated_logging_time': 0.38938331604003906}
I0201 14:04:12.525324 140022518892288 logging_writer.py:48] [12809] accumulated_eval_time=549.996140, accumulated_logging_time=0.389383, accumulated_submission_time=5917.629371, global_step=12809, preemption_count=0, score=5917.629371, test/accuracy=0.306900, test/loss=3.374931, test/num_examples=10000, total_duration=6468.676459, train/accuracy=0.422383, train/loss=2.665945, validation/accuracy=0.388340, validation/loss=2.827561, validation/num_examples=50000
I0201 14:04:50.176975 140023005427456 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.1451761722564697, loss=5.5709686279296875
I0201 14:05:36.430978 140022518892288 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.4622271060943604, loss=5.02016019821167
I0201 14:06:23.012028 140023005427456 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.792722225189209, loss=3.690187931060791
I0201 14:07:09.323765 140022518892288 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1647932529449463, loss=5.920319080352783
I0201 14:07:55.552242 140023005427456 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.8152878284454346, loss=3.5833706855773926
I0201 14:08:42.085694 140022518892288 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.6786104440689087, loss=3.63971209526062
I0201 14:09:28.356029 140023005427456 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.0769615173339844, loss=3.5246505737304688
I0201 14:10:14.903261 140022518892288 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.9399805068969727, loss=3.501798152923584
I0201 14:11:01.240475 140023005427456 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.6989232301712036, loss=3.5956499576568604
I0201 14:11:12.606546 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:11:23.221966 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:11:49.251569 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:11:50.861453 140184451094336 submission_runner.py:408] Time since start: 6927.03s, 	Step: 13726, 	{'train/accuracy': 0.44224607944488525, 'train/loss': 2.522275447845459, 'validation/accuracy': 0.4086399972438812, 'validation/loss': 2.700598955154419, 'validation/num_examples': 50000, 'test/accuracy': 0.3149000108242035, 'test/loss': 3.2909696102142334, 'test/num_examples': 10000, 'score': 6337.654529333115, 'total_duration': 6927.02899646759, 'accumulated_submission_time': 6337.654529333115, 'accumulated_eval_time': 588.2510459423065, 'accumulated_logging_time': 0.4151310920715332}
I0201 14:11:50.878092 140022518892288 logging_writer.py:48] [13726] accumulated_eval_time=588.251046, accumulated_logging_time=0.415131, accumulated_submission_time=6337.654529, global_step=13726, preemption_count=0, score=6337.654529, test/accuracy=0.314900, test/loss=3.290970, test/num_examples=10000, total_duration=6927.028996, train/accuracy=0.442246, train/loss=2.522275, validation/accuracy=0.408640, validation/loss=2.700599, validation/num_examples=50000
I0201 14:12:21.094905 140023005427456 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.3008936643600464, loss=5.461483001708984
I0201 14:13:06.901326 140022518892288 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.8254793882369995, loss=3.558490753173828
I0201 14:13:53.024990 140023005427456 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0316144227981567, loss=5.833988189697266
I0201 14:14:39.274527 140022518892288 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.6510170698165894, loss=3.544236660003662
I0201 14:15:25.336513 140023005427456 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.7796951532363892, loss=3.5468876361846924
I0201 14:16:11.640821 140022518892288 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7078322172164917, loss=3.512080192565918
I0201 14:16:57.531592 140023005427456 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.699062705039978, loss=3.6137382984161377
I0201 14:17:43.755482 140022518892288 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.7526891231536865, loss=3.445033073425293
I0201 14:18:29.970335 140023005427456 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.4348700046539307, loss=3.9062411785125732
I0201 14:18:51.391190 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:19:01.698016 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:19:29.167412 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:19:30.779536 140184451094336 submission_runner.py:408] Time since start: 7386.95s, 	Step: 14648, 	{'train/accuracy': 0.445136696100235, 'train/loss': 2.5415337085723877, 'validation/accuracy': 0.41259998083114624, 'validation/loss': 2.7152037620544434, 'validation/num_examples': 50000, 'test/accuracy': 0.32170000672340393, 'test/loss': 3.2877392768859863, 'test/num_examples': 10000, 'score': 6758.110645294189, 'total_duration': 7386.947074890137, 'accumulated_submission_time': 6758.110645294189, 'accumulated_eval_time': 627.6393864154816, 'accumulated_logging_time': 0.4406590461730957}
I0201 14:19:30.799318 140022518892288 logging_writer.py:48] [14648] accumulated_eval_time=627.639386, accumulated_logging_time=0.440659, accumulated_submission_time=6758.110645, global_step=14648, preemption_count=0, score=6758.110645, test/accuracy=0.321700, test/loss=3.287739, test/num_examples=10000, total_duration=7386.947075, train/accuracy=0.445137, train/loss=2.541534, validation/accuracy=0.412600, validation/loss=2.715204, validation/num_examples=50000
I0201 14:19:52.015522 140023005427456 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.1561967134475708, loss=5.419717788696289
I0201 14:20:36.776504 140022518892288 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.218052864074707, loss=5.321479320526123
I0201 14:21:23.126521 140023005427456 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.4985783100128174, loss=4.507943153381348
I0201 14:22:09.716121 140022518892288 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.113435983657837, loss=4.772928714752197
I0201 14:22:55.587727 140023005427456 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.8980536460876465, loss=3.4899916648864746
I0201 14:23:41.914931 140022518892288 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.2150934934616089, loss=5.78388786315918
I0201 14:24:28.123456 140023005427456 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.6387666463851929, loss=3.469273567199707
I0201 14:25:14.070142 140022518892288 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.7092738151550293, loss=3.787508487701416
I0201 14:26:00.200660 140023005427456 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5891910791397095, loss=3.583775043487549
I0201 14:26:30.838629 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:26:41.565914 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:27:11.044748 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:27:12.643116 140184451094336 submission_runner.py:408] Time since start: 7848.81s, 	Step: 15568, 	{'train/accuracy': 0.4643945097923279, 'train/loss': 2.4171652793884277, 'validation/accuracy': 0.42885997891426086, 'validation/loss': 2.588149070739746, 'validation/num_examples': 50000, 'test/accuracy': 0.3345000147819519, 'test/loss': 3.1643946170806885, 'test/num_examples': 10000, 'score': 7178.091734886169, 'total_duration': 7848.8106570243835, 'accumulated_submission_time': 7178.091734886169, 'accumulated_eval_time': 669.4438850879669, 'accumulated_logging_time': 0.47141051292419434}
I0201 14:27:12.659636 140022518892288 logging_writer.py:48] [15568] accumulated_eval_time=669.443885, accumulated_logging_time=0.471411, accumulated_submission_time=7178.091735, global_step=15568, preemption_count=0, score=7178.091735, test/accuracy=0.334500, test/loss=3.164395, test/num_examples=10000, total_duration=7848.810657, train/accuracy=0.464395, train/loss=2.417165, validation/accuracy=0.428860, validation/loss=2.588149, validation/num_examples=50000
I0201 14:27:25.844842 140023005427456 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.2361072301864624, loss=4.784446716308594
I0201 14:28:09.195951 140022518892288 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.9883581399917603, loss=3.384556770324707
I0201 14:28:55.365542 140023005427456 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.522535800933838, loss=3.284986972808838
I0201 14:29:41.979199 140022518892288 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.70881187915802, loss=3.487016439437866
I0201 14:30:28.163276 140023005427456 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.7759746313095093, loss=3.398343563079834
I0201 14:31:14.434037 140022518892288 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.2224807739257812, loss=5.379059314727783
I0201 14:32:00.741874 140023005427456 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.1205236911773682, loss=5.696618556976318
I0201 14:32:47.299693 140022518892288 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.461998462677002, loss=4.011344909667969
I0201 14:33:33.426545 140023005427456 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.10341215133667, loss=5.003661632537842
I0201 14:34:12.909145 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:34:23.613543 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:34:50.009356 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:34:51.622547 140184451094336 submission_runner.py:408] Time since start: 8307.79s, 	Step: 16487, 	{'train/accuracy': 0.4732031226158142, 'train/loss': 2.3529422283172607, 'validation/accuracy': 0.43925997614860535, 'validation/loss': 2.526226758956909, 'validation/num_examples': 50000, 'test/accuracy': 0.3418000042438507, 'test/loss': 3.1294901371002197, 'test/num_examples': 10000, 'score': 7598.284974575043, 'total_duration': 8307.79007267952, 'accumulated_submission_time': 7598.284974575043, 'accumulated_eval_time': 708.1572668552399, 'accumulated_logging_time': 0.49685025215148926}
I0201 14:34:51.641524 140022518892288 logging_writer.py:48] [16487] accumulated_eval_time=708.157267, accumulated_logging_time=0.496850, accumulated_submission_time=7598.284975, global_step=16487, preemption_count=0, score=7598.284975, test/accuracy=0.341800, test/loss=3.129490, test/num_examples=10000, total_duration=8307.790073, train/accuracy=0.473203, train/loss=2.352942, validation/accuracy=0.439260, validation/loss=2.526227, validation/num_examples=50000
I0201 14:34:57.246314 140023005427456 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.055549383163452, loss=3.2427825927734375
I0201 14:35:39.236790 140022518892288 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.6536197662353516, loss=3.2486183643341064
I0201 14:36:25.196002 140023005427456 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.7170809507369995, loss=3.44343638420105
I0201 14:37:11.674702 140022518892288 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.2339876890182495, loss=5.2655534744262695
I0201 14:37:57.772907 140023005427456 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.768549919128418, loss=3.2055296897888184
I0201 14:38:43.847108 140022518892288 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.8315318822860718, loss=3.375462770462036
I0201 14:39:30.307721 140023005427456 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4930862188339233, loss=3.4878993034362793
I0201 14:40:16.361177 140022518892288 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.837404727935791, loss=3.539579391479492
I0201 14:41:02.430451 140023005427456 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.1668623685836792, loss=3.985811471939087
I0201 14:41:48.687831 140022518892288 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.2463345527648926, loss=4.767714023590088
I0201 14:41:52.010060 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:42:02.883839 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:42:27.607764 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:42:29.205691 140184451094336 submission_runner.py:408] Time since start: 8765.37s, 	Step: 17409, 	{'train/accuracy': 0.48097655177116394, 'train/loss': 2.311243772506714, 'validation/accuracy': 0.43967998027801514, 'validation/loss': 2.5017337799072266, 'validation/num_examples': 50000, 'test/accuracy': 0.34550002217292786, 'test/loss': 3.113865852355957, 'test/num_examples': 10000, 'score': 8018.596554040909, 'total_duration': 8765.373228549957, 'accumulated_submission_time': 8018.596554040909, 'accumulated_eval_time': 745.3528969287872, 'accumulated_logging_time': 0.5254116058349609}
I0201 14:42:29.222590 140023005427456 logging_writer.py:48] [17409] accumulated_eval_time=745.352897, accumulated_logging_time=0.525412, accumulated_submission_time=8018.596554, global_step=17409, preemption_count=0, score=8018.596554, test/accuracy=0.345500, test/loss=3.113866, test/num_examples=10000, total_duration=8765.373229, train/accuracy=0.480977, train/loss=2.311244, validation/accuracy=0.439680, validation/loss=2.501734, validation/num_examples=50000
I0201 14:43:07.095909 140022518892288 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.167679786682129, loss=5.6081767082214355
I0201 14:43:52.823302 140023005427456 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.5943067073822021, loss=3.4341788291931152
I0201 14:44:39.340933 140022518892288 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.4734586477279663, loss=3.19020938873291
I0201 14:45:25.524478 140023005427456 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.3864742517471313, loss=3.537813901901245
I0201 14:46:11.685887 140022518892288 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5615229606628418, loss=4.582114219665527
I0201 14:46:57.930590 140023005427456 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.3585526943206787, loss=4.170854091644287
I0201 14:47:43.998345 140022518892288 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.3609743118286133, loss=4.872556209564209
I0201 14:48:29.950488 140023005427456 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.272504448890686, loss=4.020526885986328
I0201 14:49:16.207988 140022518892288 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.7274075746536255, loss=3.4810919761657715
I0201 14:49:29.341442 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:49:40.950132 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:50:06.207682 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:50:07.806596 140184451094336 submission_runner.py:408] Time since start: 9223.97s, 	Step: 18330, 	{'train/accuracy': 0.5154687166213989, 'train/loss': 2.13545823097229, 'validation/accuracy': 0.4569000005722046, 'validation/loss': 2.4236037731170654, 'validation/num_examples': 50000, 'test/accuracy': 0.35440000891685486, 'test/loss': 3.041668176651001, 'test/num_examples': 10000, 'score': 8438.65920996666, 'total_duration': 9223.974129915237, 'accumulated_submission_time': 8438.65920996666, 'accumulated_eval_time': 783.8180267810822, 'accumulated_logging_time': 0.551140308380127}
I0201 14:50:07.823785 140023005427456 logging_writer.py:48] [18330] accumulated_eval_time=783.818027, accumulated_logging_time=0.551140, accumulated_submission_time=8438.659210, global_step=18330, preemption_count=0, score=8438.659210, test/accuracy=0.354400, test/loss=3.041668, test/num_examples=10000, total_duration=9223.974130, train/accuracy=0.515469, train/loss=2.135458, validation/accuracy=0.456900, validation/loss=2.423604, validation/num_examples=50000
I0201 14:50:36.227888 140022518892288 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.2877850532531738, loss=5.808053016662598
I0201 14:51:21.804193 140023005427456 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.4805563688278198, loss=3.7223153114318848
I0201 14:52:08.483215 140022518892288 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.089217185974121, loss=3.6057205200195312
I0201 14:52:54.999609 140023005427456 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.448159098625183, loss=3.281217098236084
I0201 14:53:41.260636 140022518892288 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.5191456079483032, loss=3.407039165496826
I0201 14:54:27.799959 140023005427456 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.565398931503296, loss=3.5254502296447754
I0201 14:55:13.798441 140022518892288 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5347472429275513, loss=3.2029099464416504
I0201 14:55:59.835867 140023005427456 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.3976417779922485, loss=3.461381673812866
I0201 14:56:46.173962 140022518892288 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1270029544830322, loss=5.422269821166992
I0201 14:57:08.161241 140184451094336 spec.py:321] Evaluating on the training split.
I0201 14:57:18.874421 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 14:57:44.418024 140184451094336 spec.py:349] Evaluating on the test split.
I0201 14:57:46.011973 140184451094336 submission_runner.py:408] Time since start: 9682.18s, 	Step: 19249, 	{'train/accuracy': 0.49541014432907104, 'train/loss': 2.2226991653442383, 'validation/accuracy': 0.46125999093055725, 'validation/loss': 2.3966081142425537, 'validation/num_examples': 50000, 'test/accuracy': 0.36180001497268677, 'test/loss': 3.014835834503174, 'test/num_examples': 10000, 'score': 8858.939247369766, 'total_duration': 9682.179508447647, 'accumulated_submission_time': 8858.939247369766, 'accumulated_eval_time': 821.6687545776367, 'accumulated_logging_time': 0.5784051418304443}
I0201 14:57:46.029236 140023005427456 logging_writer.py:48] [19249] accumulated_eval_time=821.668755, accumulated_logging_time=0.578405, accumulated_submission_time=8858.939247, global_step=19249, preemption_count=0, score=8858.939247, test/accuracy=0.361800, test/loss=3.014836, test/num_examples=10000, total_duration=9682.179508, train/accuracy=0.495410, train/loss=2.222699, validation/accuracy=0.461260, validation/loss=2.396608, validation/num_examples=50000
I0201 14:58:06.850519 140022518892288 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.508766770362854, loss=3.293745279312134
I0201 14:58:51.217423 140023005427456 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.462398886680603, loss=2.9986510276794434
I0201 14:59:37.482373 140022518892288 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.6604335308074951, loss=3.22939133644104
I0201 15:00:23.813168 140023005427456 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.1839933395385742, loss=5.210069179534912
I0201 15:01:09.823517 140022518892288 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.643917202949524, loss=3.16633677482605
I0201 15:01:56.117641 140023005427456 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6036492586135864, loss=3.348249912261963
I0201 15:02:42.223496 140022518892288 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.149945616722107, loss=5.2214884757995605
I0201 15:03:28.369693 140023005427456 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.298182487487793, loss=5.5792059898376465
I0201 15:04:14.758917 140022518892288 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.7213068008422852, loss=3.260432004928589
I0201 15:04:46.332886 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:04:57.074978 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:05:17.898437 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:05:19.506078 140184451094336 submission_runner.py:408] Time since start: 10135.67s, 	Step: 20170, 	{'train/accuracy': 0.4988085925579071, 'train/loss': 2.2068932056427, 'validation/accuracy': 0.4660399854183197, 'validation/loss': 2.378439426422119, 'validation/num_examples': 50000, 'test/accuracy': 0.359000027179718, 'test/loss': 3.0155863761901855, 'test/num_examples': 10000, 'score': 9279.185322284698, 'total_duration': 10135.67358827591, 'accumulated_submission_time': 9279.185322284698, 'accumulated_eval_time': 854.8419258594513, 'accumulated_logging_time': 0.6058785915374756}
I0201 15:05:19.525579 140023005427456 logging_writer.py:48] [20170] accumulated_eval_time=854.841926, accumulated_logging_time=0.605879, accumulated_submission_time=9279.185322, global_step=20170, preemption_count=0, score=9279.185322, test/accuracy=0.359000, test/loss=3.015586, test/num_examples=10000, total_duration=10135.673588, train/accuracy=0.498809, train/loss=2.206893, validation/accuracy=0.466040, validation/loss=2.378439, validation/num_examples=50000
I0201 15:05:31.945747 140022518892288 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.2762844562530518, loss=4.547144412994385
I0201 15:06:15.075284 140023005427456 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.5243242979049683, loss=3.8039095401763916
I0201 15:07:00.974755 140022518892288 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.716048240661621, loss=3.291722059249878
I0201 15:07:47.130577 140023005427456 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.4040508270263672, loss=3.8835015296936035
I0201 15:08:33.090345 140022518892288 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.0791723728179932, loss=4.9849395751953125
I0201 15:09:19.407188 140023005427456 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.6110919713974, loss=3.162064552307129
I0201 15:10:05.529275 140022518892288 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.5858056545257568, loss=3.193881034851074
I0201 15:10:51.496146 140023005427456 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.0911184549331665, loss=4.598333835601807
I0201 15:11:37.867444 140022518892288 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.6742709875106812, loss=3.11993145942688
I0201 15:12:19.785898 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:12:30.277165 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:12:53.174686 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:12:54.773328 140184451094336 submission_runner.py:408] Time since start: 10590.94s, 	Step: 21092, 	{'train/accuracy': 0.5242773294448853, 'train/loss': 2.0958750247955322, 'validation/accuracy': 0.4736199975013733, 'validation/loss': 2.349881649017334, 'validation/num_examples': 50000, 'test/accuracy': 0.3703000247478485, 'test/loss': 2.9635870456695557, 'test/num_examples': 10000, 'score': 9699.388586997986, 'total_duration': 10590.940872192383, 'accumulated_submission_time': 9699.388586997986, 'accumulated_eval_time': 889.8293704986572, 'accumulated_logging_time': 0.6350181102752686}
I0201 15:12:54.790889 140023005427456 logging_writer.py:48] [21092] accumulated_eval_time=889.829370, accumulated_logging_time=0.635018, accumulated_submission_time=9699.388587, global_step=21092, preemption_count=0, score=9699.388587, test/accuracy=0.370300, test/loss=2.963587, test/num_examples=10000, total_duration=10590.940872, train/accuracy=0.524277, train/loss=2.095875, validation/accuracy=0.473620, validation/loss=2.349882, validation/num_examples=50000
I0201 15:12:58.395125 140022518892288 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.7471015453338623, loss=3.042952299118042
I0201 15:13:40.202684 140023005427456 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.6338517665863037, loss=3.035257577896118
I0201 15:14:26.528787 140022518892288 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.4201006889343262, loss=3.1002259254455566
I0201 15:15:12.922575 140023005427456 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.3272501230239868, loss=5.195652484893799
I0201 15:15:58.955384 140022518892288 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.6213222742080688, loss=3.137920379638672
I0201 15:16:45.377329 140023005427456 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.0875318050384521, loss=5.0717597007751465
I0201 15:17:31.591836 140022518892288 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.6379717588424683, loss=3.2196528911590576
I0201 15:18:17.922413 140023005427456 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2977880239486694, loss=3.3927292823791504
I0201 15:19:04.165383 140022518892288 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.4205238819122314, loss=3.274095058441162
I0201 15:19:50.214475 140023005427456 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.7424514293670654, loss=3.4297995567321777
I0201 15:19:54.859313 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:20:05.447165 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:20:26.574050 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:20:28.175955 140184451094336 submission_runner.py:408] Time since start: 11044.34s, 	Step: 22012, 	{'train/accuracy': 0.5155078172683716, 'train/loss': 2.124331474304199, 'validation/accuracy': 0.4827999770641327, 'validation/loss': 2.2880043983459473, 'validation/num_examples': 50000, 'test/accuracy': 0.37780001759529114, 'test/loss': 2.893505096435547, 'test/num_examples': 10000, 'score': 10119.395952701569, 'total_duration': 11044.343485832214, 'accumulated_submission_time': 10119.395952701569, 'accumulated_eval_time': 923.1459929943085, 'accumulated_logging_time': 0.6662139892578125}
I0201 15:20:28.197654 140022518892288 logging_writer.py:48] [22012] accumulated_eval_time=923.145993, accumulated_logging_time=0.666214, accumulated_submission_time=10119.395953, global_step=22012, preemption_count=0, score=10119.395953, test/accuracy=0.377800, test/loss=2.893505, test/num_examples=10000, total_duration=11044.343486, train/accuracy=0.515508, train/loss=2.124331, validation/accuracy=0.482800, validation/loss=2.288004, validation/num_examples=50000
I0201 15:21:05.502871 140023005427456 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.7663990259170532, loss=3.3118672370910645
I0201 15:21:51.931526 140022518892288 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.5367428064346313, loss=3.208974838256836
I0201 15:22:38.285366 140023005427456 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.4014818668365479, loss=3.5186214447021484
I0201 15:23:24.371146 140022518892288 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.5842770338058472, loss=3.088087320327759
I0201 15:24:10.816814 140023005427456 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.720146656036377, loss=2.9473047256469727
I0201 15:24:57.411052 140022518892288 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.121967077255249, loss=4.358639240264893
I0201 15:25:43.685804 140023005427456 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.5296711921691895, loss=3.044332504272461
I0201 15:26:30.458045 140022518892288 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1586039066314697, loss=5.601245880126953
I0201 15:27:16.574024 140023005427456 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.151207447052002, loss=5.1239013671875
I0201 15:27:28.218581 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:27:38.692821 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:28:03.728891 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:28:05.337269 140184451094336 submission_runner.py:408] Time since start: 11501.50s, 	Step: 22927, 	{'train/accuracy': 0.5287500023841858, 'train/loss': 2.041822910308838, 'validation/accuracy': 0.4889199733734131, 'validation/loss': 2.2429189682006836, 'validation/num_examples': 50000, 'test/accuracy': 0.38380002975463867, 'test/loss': 2.8525102138519287, 'test/num_examples': 10000, 'score': 10539.358990907669, 'total_duration': 11501.50481057167, 'accumulated_submission_time': 10539.358990907669, 'accumulated_eval_time': 960.2646844387054, 'accumulated_logging_time': 0.6987216472625732}
I0201 15:28:05.355194 140022518892288 logging_writer.py:48] [22927] accumulated_eval_time=960.264684, accumulated_logging_time=0.698722, accumulated_submission_time=10539.358991, global_step=22927, preemption_count=0, score=10539.358991, test/accuracy=0.383800, test/loss=2.852510, test/num_examples=10000, total_duration=11501.504811, train/accuracy=0.528750, train/loss=2.041823, validation/accuracy=0.488920, validation/loss=2.242919, validation/num_examples=50000
I0201 15:28:34.987409 140023005427456 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.1520336866378784, loss=4.5096940994262695
I0201 15:29:21.080017 140022518892288 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.4641684293746948, loss=3.51324725151062
I0201 15:30:07.426085 140023005427456 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.2983945608139038, loss=3.8947315216064453
I0201 15:30:53.641155 140022518892288 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.6271191835403442, loss=3.1294736862182617
I0201 15:31:40.089447 140023005427456 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.078850507736206, loss=5.3653669357299805
I0201 15:32:26.496663 140022518892288 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.764719009399414, loss=3.3643932342529297
I0201 15:33:12.764869 140023005427456 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.6181912422180176, loss=2.9015109539031982
I0201 15:33:58.977199 140022518892288 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.404708981513977, loss=3.176457166671753
I0201 15:34:45.521343 140023005427456 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.449495792388916, loss=3.376026153564453
I0201 15:35:05.449833 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:35:15.416081 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:35:43.506148 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:35:45.109197 140184451094336 submission_runner.py:408] Time since start: 11961.28s, 	Step: 23845, 	{'train/accuracy': 0.5420507788658142, 'train/loss': 2.0036168098449707, 'validation/accuracy': 0.5009599924087524, 'validation/loss': 2.2223894596099854, 'validation/num_examples': 50000, 'test/accuracy': 0.387800008058548, 'test/loss': 2.8514115810394287, 'test/num_examples': 10000, 'score': 10959.397310972214, 'total_duration': 11961.276715993881, 'accumulated_submission_time': 10959.397310972214, 'accumulated_eval_time': 999.9240214824677, 'accumulated_logging_time': 0.7256793975830078}
I0201 15:35:45.132403 140022518892288 logging_writer.py:48] [23845] accumulated_eval_time=999.924021, accumulated_logging_time=0.725679, accumulated_submission_time=10959.397311, global_step=23845, preemption_count=0, score=10959.397311, test/accuracy=0.387800, test/loss=2.851412, test/num_examples=10000, total_duration=11961.276716, train/accuracy=0.542051, train/loss=2.003617, validation/accuracy=0.500960, validation/loss=2.222389, validation/num_examples=50000
I0201 15:36:07.530325 140023005427456 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.3631007671356201, loss=4.0491766929626465
I0201 15:36:51.418085 140022518892288 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.6577945947647095, loss=3.306774854660034
I0201 15:37:37.468160 140023005427456 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.5020629167556763, loss=3.1354146003723145
I0201 15:38:23.596931 140022518892288 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.4848319292068481, loss=2.958833694458008
I0201 15:39:09.663815 140023005427456 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.4174857139587402, loss=3.9623966217041016
I0201 15:39:55.628285 140022518892288 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.2190760374069214, loss=4.327495574951172
I0201 15:40:41.574127 140023005427456 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.4597115516662598, loss=3.538175106048584
I0201 15:41:27.531247 140022518892288 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.273329734802246, loss=3.727365493774414
I0201 15:42:13.640501 140023005427456 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.5865931510925293, loss=3.1017022132873535
I0201 15:42:45.137528 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:42:55.419714 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:43:16.629350 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:43:18.232698 140184451094336 submission_runner.py:408] Time since start: 12414.40s, 	Step: 24770, 	{'train/accuracy': 0.5383593440055847, 'train/loss': 2.017052173614502, 'validation/accuracy': 0.5021600127220154, 'validation/loss': 2.1893248558044434, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 2.818950891494751, 'test/num_examples': 10000, 'score': 11379.342381954193, 'total_duration': 12414.400235176086, 'accumulated_submission_time': 11379.342381954193, 'accumulated_eval_time': 1033.0191838741302, 'accumulated_logging_time': 0.761568546295166}
I0201 15:43:18.261214 140022518892288 logging_writer.py:48] [24770] accumulated_eval_time=1033.019184, accumulated_logging_time=0.761569, accumulated_submission_time=11379.342382, global_step=24770, preemption_count=0, score=11379.342382, test/accuracy=0.393600, test/loss=2.818951, test/num_examples=10000, total_duration=12414.400235, train/accuracy=0.538359, train/loss=2.017052, validation/accuracy=0.502160, validation/loss=2.189325, validation/num_examples=50000
I0201 15:43:30.674750 140023005427456 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.6269499063491821, loss=2.9084599018096924
I0201 15:44:13.504093 140022518892288 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.1495364904403687, loss=4.662280082702637
I0201 15:44:59.707529 140023005427456 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.6193984746932983, loss=3.0079150199890137
I0201 15:45:46.333050 140022518892288 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.3464916944503784, loss=5.0915422439575195
I0201 15:46:32.369428 140023005427456 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.6280244588851929, loss=3.1771607398986816
I0201 15:47:18.649724 140022518892288 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.4353835582733154, loss=3.6278228759765625
I0201 15:48:04.818418 140023005427456 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.5757627487182617, loss=2.9616665840148926
I0201 15:48:50.888431 140022518892288 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.5749940872192383, loss=3.190147876739502
I0201 15:49:37.390627 140023005427456 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.4829915761947632, loss=3.0943398475646973
I0201 15:50:18.468472 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:50:28.859845 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:50:53.878481 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:50:55.471030 140184451094336 submission_runner.py:408] Time since start: 12871.64s, 	Step: 25691, 	{'train/accuracy': 0.5432421565055847, 'train/loss': 1.9928700923919678, 'validation/accuracy': 0.5070199966430664, 'validation/loss': 2.167407989501953, 'validation/num_examples': 50000, 'test/accuracy': 0.39320001006126404, 'test/loss': 2.8182153701782227, 'test/num_examples': 10000, 'score': 11799.493110179901, 'total_duration': 12871.638572454453, 'accumulated_submission_time': 11799.493110179901, 'accumulated_eval_time': 1070.0217413902283, 'accumulated_logging_time': 0.7992439270019531}
I0201 15:50:55.489655 140022518892288 logging_writer.py:48] [25691] accumulated_eval_time=1070.021741, accumulated_logging_time=0.799244, accumulated_submission_time=11799.493110, global_step=25691, preemption_count=0, score=11799.493110, test/accuracy=0.393200, test/loss=2.818215, test/num_examples=10000, total_duration=12871.638572, train/accuracy=0.543242, train/loss=1.992870, validation/accuracy=0.507020, validation/loss=2.167408, validation/num_examples=50000
I0201 15:50:59.496760 140023005427456 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.4057821035385132, loss=4.233142375946045
I0201 15:51:41.581394 140022518892288 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.217419981956482, loss=5.634194850921631
I0201 15:52:27.469944 140023005427456 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.5662903785705566, loss=3.1281487941741943
I0201 15:53:13.773232 140022518892288 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.3312335014343262, loss=3.632589817047119
I0201 15:53:59.872102 140023005427456 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.5366932153701782, loss=3.420766830444336
I0201 15:54:45.864438 140022518892288 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.567334771156311, loss=3.101355791091919
I0201 15:55:32.573668 140023005427456 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7076244354248047, loss=2.9680137634277344
I0201 15:56:18.453171 140022518892288 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.2318745851516724, loss=5.080671310424805
I0201 15:57:04.503423 140023005427456 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1512572765350342, loss=5.491078853607178
I0201 15:57:50.614325 140022518892288 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.545836091041565, loss=3.1028072834014893
I0201 15:57:55.857161 140184451094336 spec.py:321] Evaluating on the training split.
I0201 15:58:06.958946 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 15:58:32.028943 140184451094336 spec.py:349] Evaluating on the test split.
I0201 15:58:33.626690 140184451094336 submission_runner.py:408] Time since start: 13329.79s, 	Step: 26613, 	{'train/accuracy': 0.549023449420929, 'train/loss': 1.963714599609375, 'validation/accuracy': 0.5048800110816956, 'validation/loss': 2.1838018894195557, 'validation/num_examples': 50000, 'test/accuracy': 0.39750000834465027, 'test/loss': 2.823049783706665, 'test/num_examples': 10000, 'score': 12219.802811145782, 'total_duration': 13329.79423236847, 'accumulated_submission_time': 12219.802811145782, 'accumulated_eval_time': 1107.7912635803223, 'accumulated_logging_time': 0.8276638984680176}
I0201 15:58:33.648756 140023005427456 logging_writer.py:48] [26613] accumulated_eval_time=1107.791264, accumulated_logging_time=0.827664, accumulated_submission_time=12219.802811, global_step=26613, preemption_count=0, score=12219.802811, test/accuracy=0.397500, test/loss=2.823050, test/num_examples=10000, total_duration=13329.794232, train/accuracy=0.549023, train/loss=1.963715, validation/accuracy=0.504880, validation/loss=2.183802, validation/num_examples=50000
I0201 15:59:09.444551 140022518892288 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.755263328552246, loss=2.9688448905944824
I0201 15:59:55.455722 140023005427456 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.3510544300079346, loss=5.631646156311035
I0201 16:00:41.880209 140022518892288 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.5818283557891846, loss=2.9557323455810547
I0201 16:01:27.958686 140023005427456 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.8860045671463013, loss=2.8967621326446533
I0201 16:02:14.287084 140022518892288 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.6209803819656372, loss=2.877427577972412
I0201 16:03:00.518226 140023005427456 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.4274179935455322, loss=3.608819007873535
I0201 16:03:46.599075 140022518892288 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.2898902893066406, loss=4.084645748138428
I0201 16:04:32.919111 140023005427456 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.5296268463134766, loss=2.8334579467773438
I0201 16:05:19.153606 140022518892288 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.6767910718917847, loss=3.0519089698791504
I0201 16:05:33.748662 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:05:44.304587 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:06:09.726898 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:06:11.334538 140184451094336 submission_runner.py:408] Time since start: 13787.50s, 	Step: 27533, 	{'train/accuracy': 0.564648449420929, 'train/loss': 1.8632910251617432, 'validation/accuracy': 0.5128799676895142, 'validation/loss': 2.0994911193847656, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.7460947036743164, 'test/num_examples': 10000, 'score': 12639.845764398575, 'total_duration': 13787.502056837082, 'accumulated_submission_time': 12639.845764398575, 'accumulated_eval_time': 1145.3771076202393, 'accumulated_logging_time': 0.859968900680542}
I0201 16:06:11.359199 140023005427456 logging_writer.py:48] [27533] accumulated_eval_time=1145.377108, accumulated_logging_time=0.859969, accumulated_submission_time=12639.845764, global_step=27533, preemption_count=0, score=12639.845764, test/accuracy=0.404700, test/loss=2.746095, test/num_examples=10000, total_duration=13787.502057, train/accuracy=0.564648, train/loss=1.863291, validation/accuracy=0.512880, validation/loss=2.099491, validation/num_examples=50000
I0201 16:06:38.566543 140022518892288 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.208581566810608, loss=5.145970344543457
I0201 16:07:24.063925 140023005427456 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.724971890449524, loss=2.848389148712158
I0201 16:08:10.399929 140022518892288 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.6516578197479248, loss=3.056931495666504
I0201 16:08:56.482446 140023005427456 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.6737464666366577, loss=3.062436580657959
I0201 16:09:42.745216 140022518892288 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.4999730587005615, loss=3.210360527038574
I0201 16:10:29.240812 140023005427456 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.51911461353302, loss=3.034979820251465
I0201 16:11:15.347313 140022518892288 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.5862139463424683, loss=3.1864399909973145
I0201 16:12:01.666793 140023005427456 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.620848298072815, loss=3.1251325607299805
I0201 16:12:48.009089 140022518892288 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.405439853668213, loss=4.794615745544434
I0201 16:13:11.849761 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:13:22.516632 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:13:45.929888 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:13:47.532334 140184451094336 submission_runner.py:408] Time since start: 14243.70s, 	Step: 28453, 	{'train/accuracy': 0.5565234422683716, 'train/loss': 1.9159458875656128, 'validation/accuracy': 0.5190799832344055, 'validation/loss': 2.100757360458374, 'validation/num_examples': 50000, 'test/accuracy': 0.40940001606941223, 'test/loss': 2.7339088916778564, 'test/num_examples': 10000, 'score': 13060.279699325562, 'total_duration': 14243.699850559235, 'accumulated_submission_time': 13060.279699325562, 'accumulated_eval_time': 1181.059654712677, 'accumulated_logging_time': 0.8942258358001709}
I0201 16:13:47.551148 140023005427456 logging_writer.py:48] [28453] accumulated_eval_time=1181.059655, accumulated_logging_time=0.894226, accumulated_submission_time=13060.279699, global_step=28453, preemption_count=0, score=13060.279699, test/accuracy=0.409400, test/loss=2.733909, test/num_examples=10000, total_duration=14243.699851, train/accuracy=0.556523, train/loss=1.915946, validation/accuracy=0.519080, validation/loss=2.100757, validation/num_examples=50000
I0201 16:14:06.749510 140022518892288 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.5999014377593994, loss=2.904766321182251
I0201 16:14:51.186175 140023005427456 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2758376598358154, loss=3.9220125675201416
I0201 16:15:37.542406 140022518892288 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.9733995199203491, loss=2.9405245780944824
I0201 16:16:31.271768 140023005427456 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.2977319955825806, loss=4.640600681304932
I0201 16:17:24.086104 140022518892288 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.7965282201766968, loss=2.927448034286499
I0201 16:18:10.177992 140023005427456 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.2699652910232544, loss=4.70472526550293
I0201 16:18:56.084534 140022518892288 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.6242748498916626, loss=3.107599973678589
I0201 16:19:41.916572 140023005427456 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.3663878440856934, loss=5.407681465148926
I0201 16:20:28.156248 140022518892288 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.750872254371643, loss=2.9786341190338135
I0201 16:20:47.707342 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:20:58.070177 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:21:20.370537 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:21:21.998601 140184451094336 submission_runner.py:408] Time since start: 14698.17s, 	Step: 29344, 	{'train/accuracy': 0.5629491806030273, 'train/loss': 1.8562678098678589, 'validation/accuracy': 0.523580014705658, 'validation/loss': 2.056525230407715, 'validation/num_examples': 50000, 'test/accuracy': 0.4134000241756439, 'test/loss': 2.7119228839874268, 'test/num_examples': 10000, 'score': 13480.380759000778, 'total_duration': 14698.166138410568, 'accumulated_submission_time': 13480.380759000778, 'accumulated_eval_time': 1215.350920677185, 'accumulated_logging_time': 0.921989917755127}
I0201 16:21:22.017947 140023005427456 logging_writer.py:48] [29344] accumulated_eval_time=1215.350921, accumulated_logging_time=0.921990, accumulated_submission_time=13480.380759, global_step=29344, preemption_count=0, score=13480.380759, test/accuracy=0.413400, test/loss=2.711923, test/num_examples=10000, total_duration=14698.166138, train/accuracy=0.562949, train/loss=1.856268, validation/accuracy=0.523580, validation/loss=2.056525, validation/num_examples=50000
I0201 16:21:44.824238 140022518892288 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.3772257566452026, loss=5.6255998611450195
I0201 16:22:30.005979 140023005427456 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.4894107580184937, loss=2.8904244899749756
I0201 16:23:15.999492 140022518892288 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.700235366821289, loss=2.9037351608276367
I0201 16:24:02.328656 140023005427456 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.687544822692871, loss=2.8658699989318848
I0201 16:24:48.078934 140022518892288 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.5899569988250732, loss=2.8655786514282227
I0201 16:25:34.225571 140023005427456 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.5546109676361084, loss=5.2937750816345215
I0201 16:26:20.323274 140022518892288 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.5437541007995605, loss=2.906975746154785
I0201 16:27:06.500841 140023005427456 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.5730311870574951, loss=2.784433364868164
I0201 16:27:52.433900 140022518892288 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.6430288553237915, loss=2.764174461364746
I0201 16:28:22.073665 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:28:32.267642 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:28:58.263312 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:28:59.863063 140184451094336 submission_runner.py:408] Time since start: 15156.03s, 	Step: 30266, 	{'train/accuracy': 0.5864452719688416, 'train/loss': 1.7774485349655151, 'validation/accuracy': 0.5238800048828125, 'validation/loss': 2.0740952491760254, 'validation/num_examples': 50000, 'test/accuracy': 0.4092000126838684, 'test/loss': 2.7224607467651367, 'test/num_examples': 10000, 'score': 13900.379534959793, 'total_duration': 15156.030605793, 'accumulated_submission_time': 13900.379534959793, 'accumulated_eval_time': 1253.140303850174, 'accumulated_logging_time': 0.9507706165313721}
I0201 16:28:59.884242 140023005427456 logging_writer.py:48] [30266] accumulated_eval_time=1253.140304, accumulated_logging_time=0.950771, accumulated_submission_time=13900.379535, global_step=30266, preemption_count=0, score=13900.379535, test/accuracy=0.409200, test/loss=2.722461, test/num_examples=10000, total_duration=15156.030606, train/accuracy=0.586445, train/loss=1.777449, validation/accuracy=0.523880, validation/loss=2.074095, validation/num_examples=50000
I0201 16:29:13.884414 140022518892288 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.630678415298462, loss=3.048659563064575
I0201 16:29:56.933430 140023005427456 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.735134482383728, loss=2.8461620807647705
I0201 16:30:43.431021 140022518892288 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3688246011734009, loss=3.649500846862793
I0201 16:31:29.903941 140023005427456 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.6711314916610718, loss=2.7674922943115234
I0201 16:32:16.087017 140022518892288 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.499875783920288, loss=3.086719036102295
I0201 16:33:02.029809 140023005427456 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7499226331710815, loss=2.933537721633911
I0201 16:33:48.210905 140022518892288 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.548843264579773, loss=2.9969961643218994
I0201 16:34:34.306320 140023005427456 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4353306293487549, loss=5.175732612609863
I0201 16:35:20.574710 140022518892288 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1989771127700806, loss=5.21225643157959
I0201 16:36:00.199612 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:36:10.487947 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:36:36.481254 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:36:38.081824 140184451094336 submission_runner.py:408] Time since start: 15614.25s, 	Step: 31188, 	{'train/accuracy': 0.5709179639816284, 'train/loss': 1.8723945617675781, 'validation/accuracy': 0.5306400060653687, 'validation/loss': 2.058422327041626, 'validation/num_examples': 50000, 'test/accuracy': 0.4148000180721283, 'test/loss': 2.696887731552124, 'test/num_examples': 10000, 'score': 14320.638298034668, 'total_duration': 15614.249346971512, 'accumulated_submission_time': 14320.638298034668, 'accumulated_eval_time': 1291.0224838256836, 'accumulated_logging_time': 0.9816055297851562}
I0201 16:36:38.102954 140023005427456 logging_writer.py:48] [31188] accumulated_eval_time=1291.022484, accumulated_logging_time=0.981606, accumulated_submission_time=14320.638298, global_step=31188, preemption_count=0, score=14320.638298, test/accuracy=0.414800, test/loss=2.696888, test/num_examples=10000, total_duration=15614.249347, train/accuracy=0.570918, train/loss=1.872395, validation/accuracy=0.530640, validation/loss=2.058422, validation/num_examples=50000
I0201 16:36:43.313926 140022518892288 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.4255690574645996, loss=4.806018829345703
I0201 16:37:25.669755 140023005427456 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.7250579595565796, loss=2.9376206398010254
I0201 16:38:11.620646 140022518892288 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.3269588947296143, loss=4.193800449371338
I0201 16:38:57.975290 140023005427456 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7489317655563354, loss=2.873373508453369
I0201 16:39:44.247518 140022518892288 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.957814335823059, loss=2.871739149093628
I0201 16:40:30.443388 140023005427456 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.2177305221557617, loss=5.4361066818237305
I0201 16:41:16.649813 140022518892288 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.669508934020996, loss=2.8942322731018066
I0201 16:42:03.041954 140023005427456 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.195008635520935, loss=5.258017063140869
I0201 16:42:49.064282 140022518892288 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.4047131538391113, loss=3.7340943813323975
I0201 16:43:34.904178 140023005427456 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.7653398513793945, loss=2.907644748687744
I0201 16:43:38.378917 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:43:48.920798 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:44:13.966474 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:44:15.567703 140184451094336 submission_runner.py:408] Time since start: 16071.74s, 	Step: 32109, 	{'train/accuracy': 0.5722070336341858, 'train/loss': 1.8319276571273804, 'validation/accuracy': 0.537339985370636, 'validation/loss': 2.008451223373413, 'validation/num_examples': 50000, 'test/accuracy': 0.42180001735687256, 'test/loss': 2.6582112312316895, 'test/num_examples': 10000, 'score': 14740.856187820435, 'total_duration': 16071.735245227814, 'accumulated_submission_time': 14740.856187820435, 'accumulated_eval_time': 1328.2112641334534, 'accumulated_logging_time': 1.0133063793182373}
I0201 16:44:15.589205 140022518892288 logging_writer.py:48] [32109] accumulated_eval_time=1328.211264, accumulated_logging_time=1.013306, accumulated_submission_time=14740.856188, global_step=32109, preemption_count=0, score=14740.856188, test/accuracy=0.421800, test/loss=2.658211, test/num_examples=10000, total_duration=16071.735245, train/accuracy=0.572207, train/loss=1.831928, validation/accuracy=0.537340, validation/loss=2.008451, validation/num_examples=50000
I0201 16:44:53.427497 140023005427456 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.576225757598877, loss=2.966318130493164
I0201 16:45:39.162543 140022518892288 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.2156429290771484, loss=4.3869476318359375
I0201 16:46:25.515418 140023005427456 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.7011233568191528, loss=3.154569149017334
I0201 16:47:11.926556 140022518892288 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.391892671585083, loss=3.647434949874878
I0201 16:47:57.966120 140023005427456 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.651260495185852, loss=3.0420444011688232
I0201 16:48:44.191735 140022518892288 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.4481313228607178, loss=3.6157383918762207
I0201 16:49:30.361317 140023005427456 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.5838313102722168, loss=4.295177459716797
I0201 16:50:16.512295 140022518892288 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.2544156312942505, loss=4.244151592254639
I0201 16:51:02.662576 140023005427456 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.5737286806106567, loss=2.998746156692505
I0201 16:51:15.793486 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:51:26.670833 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:51:54.008696 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:51:55.607298 140184451094336 submission_runner.py:408] Time since start: 16531.77s, 	Step: 33030, 	{'train/accuracy': 0.591601550579071, 'train/loss': 1.7154150009155273, 'validation/accuracy': 0.5344399809837341, 'validation/loss': 1.969157099723816, 'validation/num_examples': 50000, 'test/accuracy': 0.42650002241134644, 'test/loss': 2.6062676906585693, 'test/num_examples': 10000, 'score': 15160.993295431137, 'total_duration': 16531.774827718735, 'accumulated_submission_time': 15160.993295431137, 'accumulated_eval_time': 1368.0250644683838, 'accumulated_logging_time': 1.0439252853393555}
I0201 16:51:55.631066 140022518892288 logging_writer.py:48] [33030] accumulated_eval_time=1368.025064, accumulated_logging_time=1.043925, accumulated_submission_time=15160.993295, global_step=33030, preemption_count=0, score=15160.993295, test/accuracy=0.426500, test/loss=2.606268, test/num_examples=10000, total_duration=16531.774828, train/accuracy=0.591602, train/loss=1.715415, validation/accuracy=0.534440, validation/loss=1.969157, validation/num_examples=50000
I0201 16:52:24.032977 140023005427456 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.258955478668213, loss=3.618556499481201
I0201 16:53:09.633171 140022518892288 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.5819008350372314, loss=2.8793625831604004
I0201 16:53:55.950661 140023005427456 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.3813520669937134, loss=5.193783760070801
I0201 16:54:42.140036 140022518892288 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.3987486362457275, loss=3.606595039367676
I0201 16:55:28.181026 140023005427456 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.7339401245117188, loss=2.8438448905944824
I0201 16:56:14.450987 140022518892288 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.6642515659332275, loss=2.8171732425689697
I0201 16:57:00.416659 140023005427456 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.8549139499664307, loss=2.7870113849639893
I0201 16:57:46.828711 140022518892288 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.6938047409057617, loss=2.843101978302002
I0201 16:58:32.997685 140023005427456 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.3893448114395142, loss=3.388883352279663
I0201 16:58:55.651783 140184451094336 spec.py:321] Evaluating on the training split.
I0201 16:59:06.167558 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 16:59:34.391855 140184451094336 spec.py:349] Evaluating on the test split.
I0201 16:59:35.993571 140184451094336 submission_runner.py:408] Time since start: 16992.16s, 	Step: 33951, 	{'train/accuracy': 0.580078125, 'train/loss': 1.7822751998901367, 'validation/accuracy': 0.5445799827575684, 'validation/loss': 1.9617338180541992, 'validation/num_examples': 50000, 'test/accuracy': 0.4296000301837921, 'test/loss': 2.6152634620666504, 'test/num_examples': 10000, 'score': 15580.957021474838, 'total_duration': 16992.161110639572, 'accumulated_submission_time': 15580.957021474838, 'accumulated_eval_time': 1408.3668491840363, 'accumulated_logging_time': 1.0768020153045654}
I0201 16:59:36.013506 140022518892288 logging_writer.py:48] [33951] accumulated_eval_time=1408.366849, accumulated_logging_time=1.076802, accumulated_submission_time=15580.957021, global_step=33951, preemption_count=0, score=15580.957021, test/accuracy=0.429600, test/loss=2.615263, test/num_examples=10000, total_duration=16992.161111, train/accuracy=0.580078, train/loss=1.782275, validation/accuracy=0.544580, validation/loss=1.961734, validation/num_examples=50000
I0201 16:59:56.016048 140023005427456 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.6226991415023804, loss=2.703648090362549
I0201 17:00:40.476311 140022518892288 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.5370064973831177, loss=3.812328338623047
I0201 17:01:26.735213 140023005427456 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.3929176330566406, loss=5.171817779541016
I0201 17:02:13.560623 140022518892288 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.672796607017517, loss=3.2959208488464355
I0201 17:02:59.594499 140023005427456 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.3900783061981201, loss=3.5541608333587646
I0201 17:03:45.440742 140022518892288 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.8632845878601074, loss=2.8689606189727783
I0201 17:04:31.446263 140023005427456 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.3223382234573364, loss=4.172724723815918
I0201 17:05:17.696132 140022518892288 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6779545545578003, loss=2.8213162422180176
I0201 17:06:04.138277 140023005427456 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.3239296674728394, loss=4.784111499786377
I0201 17:06:36.437035 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:06:46.815145 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:07:12.327498 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:07:13.932067 140184451094336 submission_runner.py:408] Time since start: 17450.10s, 	Step: 34872, 	{'train/accuracy': 0.5824218392372131, 'train/loss': 1.7896679639816284, 'validation/accuracy': 0.5446400046348572, 'validation/loss': 1.968542456626892, 'validation/num_examples': 50000, 'test/accuracy': 0.4246000349521637, 'test/loss': 2.62258243560791, 'test/num_examples': 10000, 'score': 16001.323278665543, 'total_duration': 17450.09957075119, 'accumulated_submission_time': 16001.323278665543, 'accumulated_eval_time': 1445.8618338108063, 'accumulated_logging_time': 1.105936050415039}
I0201 17:07:13.959083 140022518892288 logging_writer.py:48] [34872] accumulated_eval_time=1445.861834, accumulated_logging_time=1.105936, accumulated_submission_time=16001.323279, global_step=34872, preemption_count=0, score=16001.323279, test/accuracy=0.424600, test/loss=2.622582, test/num_examples=10000, total_duration=17450.099571, train/accuracy=0.582422, train/loss=1.789668, validation/accuracy=0.544640, validation/loss=1.968542, validation/num_examples=50000
I0201 17:07:25.562750 140023005427456 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.5847418308258057, loss=2.857997417449951
I0201 17:08:08.598872 140022518892288 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.5466865301132202, loss=3.344630241394043
I0201 17:08:54.751365 140023005427456 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.5928752422332764, loss=2.8074588775634766
I0201 17:09:40.897259 140022518892288 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.619275689125061, loss=3.038909435272217
I0201 17:10:27.054123 140023005427456 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.797836422920227, loss=2.852550506591797
I0201 17:11:13.597913 140022518892288 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.2216860055923462, loss=4.9560136795043945
I0201 17:11:59.738627 140023005427456 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.591974139213562, loss=2.904228448867798
I0201 17:12:45.842301 140022518892288 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.323242425918579, loss=3.9030489921569824
I0201 17:13:32.085271 140023005427456 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.6136733293533325, loss=2.800917625427246
I0201 17:14:14.317946 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:14:24.942019 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:14:50.206881 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:14:51.812984 140184451094336 submission_runner.py:408] Time since start: 17907.98s, 	Step: 35793, 	{'train/accuracy': 0.5892577767372131, 'train/loss': 1.7423663139343262, 'validation/accuracy': 0.5414800047874451, 'validation/loss': 1.9773989915847778, 'validation/num_examples': 50000, 'test/accuracy': 0.42430001497268677, 'test/loss': 2.6173818111419678, 'test/num_examples': 10000, 'score': 16421.62156009674, 'total_duration': 17907.980507850647, 'accumulated_submission_time': 16421.62156009674, 'accumulated_eval_time': 1483.3568606376648, 'accumulated_logging_time': 1.1460247039794922}
I0201 17:14:51.840277 140022518892288 logging_writer.py:48] [35793] accumulated_eval_time=1483.356861, accumulated_logging_time=1.146025, accumulated_submission_time=16421.621560, global_step=35793, preemption_count=0, score=16421.621560, test/accuracy=0.424300, test/loss=2.617382, test/num_examples=10000, total_duration=17907.980508, train/accuracy=0.589258, train/loss=1.742366, validation/accuracy=0.541480, validation/loss=1.977399, validation/num_examples=50000
I0201 17:14:55.048615 140023005427456 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.6213933229446411, loss=2.7836084365844727
I0201 17:15:36.730613 140022518892288 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.5472580194473267, loss=2.6783413887023926
I0201 17:16:22.860133 140023005427456 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.8707143068313599, loss=2.7582204341888428
I0201 17:17:08.855549 140022518892288 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.5240622758865356, loss=3.0744614601135254
I0201 17:17:54.746891 140023005427456 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1129827499389648, loss=5.015788555145264
I0201 17:18:41.113648 140022518892288 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.738704800605774, loss=2.9612014293670654
I0201 17:19:27.282586 140023005427456 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.3143678903579712, loss=4.97141695022583
I0201 17:20:13.287374 140022518892288 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.7391626834869385, loss=2.8231565952301025
I0201 17:20:59.368391 140023005427456 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.7217984199523926, loss=2.779716730117798
I0201 17:21:45.711651 140022518892288 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.689333200454712, loss=2.756991386413574
I0201 17:21:52.265890 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:22:03.107383 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:22:30.015507 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:22:31.616974 140184451094336 submission_runner.py:408] Time since start: 18367.78s, 	Step: 36716, 	{'train/accuracy': 0.5894140601158142, 'train/loss': 1.7541520595550537, 'validation/accuracy': 0.5504999756813049, 'validation/loss': 1.9289462566375732, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.5739073753356934, 'test/num_examples': 10000, 'score': 16841.98622250557, 'total_duration': 18367.78451514244, 'accumulated_submission_time': 16841.98622250557, 'accumulated_eval_time': 1522.7079238891602, 'accumulated_logging_time': 1.1861801147460938}
I0201 17:22:31.640040 140023005427456 logging_writer.py:48] [36716] accumulated_eval_time=1522.707924, accumulated_logging_time=1.186180, accumulated_submission_time=16841.986223, global_step=36716, preemption_count=0, score=16841.986223, test/accuracy=0.431000, test/loss=2.573907, test/num_examples=10000, total_duration=18367.784515, train/accuracy=0.589414, train/loss=1.754152, validation/accuracy=0.550500, validation/loss=1.928946, validation/num_examples=50000
I0201 17:23:06.258002 140022518892288 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.6013444662094116, loss=2.6848092079162598
I0201 17:23:52.141266 140023005427456 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.6496930122375488, loss=3.2639527320861816
I0201 17:24:38.449163 140022518892288 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.6770130395889282, loss=2.758108139038086
I0201 17:25:24.608958 140023005427456 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.551666259765625, loss=3.047240972518921
I0201 17:26:10.518198 140022518892288 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.7566756010055542, loss=2.903548002243042
I0201 17:26:56.581035 140023005427456 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6016708612442017, loss=2.791485548019409
I0201 17:27:42.868936 140022518892288 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.6621286869049072, loss=2.807924270629883
I0201 17:28:29.149262 140023005427456 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.7937911748886108, loss=3.3189241886138916
I0201 17:29:15.234094 140022518892288 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1667084693908691, loss=5.464835166931152
I0201 17:29:31.829759 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:29:42.486906 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:30:05.407409 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:30:07.036982 140184451094336 submission_runner.py:408] Time since start: 18823.20s, 	Step: 37638, 	{'train/accuracy': 0.5914062261581421, 'train/loss': 1.7134793996810913, 'validation/accuracy': 0.5567799806594849, 'validation/loss': 1.893664002418518, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.56376576423645, 'test/num_examples': 10000, 'score': 17262.115788459778, 'total_duration': 18823.204505443573, 'accumulated_submission_time': 17262.115788459778, 'accumulated_eval_time': 1557.9151480197906, 'accumulated_logging_time': 1.2215921878814697}
I0201 17:30:07.061002 140023005427456 logging_writer.py:48] [37638] accumulated_eval_time=1557.915148, accumulated_logging_time=1.221592, accumulated_submission_time=17262.115788, global_step=37638, preemption_count=0, score=17262.115788, test/accuracy=0.435300, test/loss=2.563766, test/num_examples=10000, total_duration=18823.204505, train/accuracy=0.591406, train/loss=1.713479, validation/accuracy=0.556780, validation/loss=1.893664, validation/num_examples=50000
I0201 17:30:32.248423 140022518892288 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.4522533416748047, loss=3.2959539890289307
I0201 17:31:17.288539 140023005427456 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.593717336654663, loss=2.726797580718994
I0201 17:32:03.790269 140022518892288 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.7178701162338257, loss=2.807743549346924
I0201 17:32:49.899400 140023005427456 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.6165872812271118, loss=3.1579065322875977
I0201 17:33:35.953886 140022518892288 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8384456634521484, loss=2.7157936096191406
I0201 17:34:21.948388 140023005427456 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.4215607643127441, loss=3.557460308074951
I0201 17:35:07.952190 140022518892288 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.8066524267196655, loss=2.806649684906006
I0201 17:35:54.078272 140023005427456 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.6015173196792603, loss=3.5568654537200928
I0201 17:36:40.395528 140022518892288 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.3083555698394775, loss=4.20851993560791
I0201 17:37:07.306535 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:37:17.640187 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:37:42.652552 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:37:44.253031 140184451094336 submission_runner.py:408] Time since start: 19280.42s, 	Step: 38560, 	{'train/accuracy': 0.5970898270606995, 'train/loss': 1.721079707145691, 'validation/accuracy': 0.5518199801445007, 'validation/loss': 1.9465895891189575, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.5917446613311768, 'test/num_examples': 10000, 'score': 17682.302928209305, 'total_duration': 19280.42056465149, 'accumulated_submission_time': 17682.302928209305, 'accumulated_eval_time': 1594.8616213798523, 'accumulated_logging_time': 1.2564432621002197}
I0201 17:37:44.273172 140023005427456 logging_writer.py:48] [38560] accumulated_eval_time=1594.861621, accumulated_logging_time=1.256443, accumulated_submission_time=17682.302928, global_step=38560, preemption_count=0, score=17682.302928, test/accuracy=0.435300, test/loss=2.591745, test/num_examples=10000, total_duration=19280.420565, train/accuracy=0.597090, train/loss=1.721080, validation/accuracy=0.551820, validation/loss=1.946590, validation/num_examples=50000
I0201 17:38:00.681141 140022518892288 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.5765560865402222, loss=2.7636849880218506
I0201 17:38:44.462606 140023005427456 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.5397584438323975, loss=3.8064122200012207
I0201 17:39:30.779262 140022518892288 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.7499841451644897, loss=2.6751303672790527
I0201 17:40:17.086723 140023005427456 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.7075010538101196, loss=2.6939644813537598
I0201 17:41:03.218976 140022518892288 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3512293100357056, loss=3.922396183013916
I0201 17:41:49.498074 140023005427456 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.0507304668426514, loss=2.6820757389068604
I0201 17:42:35.543728 140022518892288 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.6201565265655518, loss=2.7601935863494873
I0201 17:43:21.811217 140023005427456 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7750276327133179, loss=2.6512086391448975
I0201 17:44:07.905815 140022518892288 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.3657112121582031, loss=4.334268569946289
I0201 17:44:44.509450 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:44:55.050983 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:45:22.662252 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:45:24.256743 140184451094336 submission_runner.py:408] Time since start: 19740.42s, 	Step: 39481, 	{'train/accuracy': 0.6230077743530273, 'train/loss': 1.6257997751235962, 'validation/accuracy': 0.5562999844551086, 'validation/loss': 1.9278334379196167, 'validation/num_examples': 50000, 'test/accuracy': 0.44120001792907715, 'test/loss': 2.564246654510498, 'test/num_examples': 10000, 'score': 18102.482609033585, 'total_duration': 19740.42428445816, 'accumulated_submission_time': 18102.482609033585, 'accumulated_eval_time': 1634.6089255809784, 'accumulated_logging_time': 1.2855734825134277}
I0201 17:45:24.277630 140023005427456 logging_writer.py:48] [39481] accumulated_eval_time=1634.608926, accumulated_logging_time=1.285573, accumulated_submission_time=18102.482609, global_step=39481, preemption_count=0, score=18102.482609, test/accuracy=0.441200, test/loss=2.564247, test/num_examples=10000, total_duration=19740.424284, train/accuracy=0.623008, train/loss=1.625800, validation/accuracy=0.556300, validation/loss=1.927833, validation/num_examples=50000
I0201 17:45:32.289931 140022518892288 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.4622077941894531, loss=5.1925740242004395
I0201 17:46:14.872156 140023005427456 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.9670196771621704, loss=2.924715042114258
I0201 17:47:00.955592 140022518892288 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.9252970218658447, loss=3.0302042961120605
I0201 17:47:47.475378 140023005427456 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.7357186079025269, loss=2.697133779525757
I0201 17:48:33.556703 140022518892288 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.3016592264175415, loss=5.067485332489014
I0201 17:49:19.740524 140023005427456 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.4981998205184937, loss=3.43941068649292
I0201 17:50:05.981175 140022518892288 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.9249471426010132, loss=2.7685515880584717
I0201 17:50:51.810943 140023005427456 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.5907175540924072, loss=3.364779233932495
I0201 17:51:38.190033 140022518892288 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.7648335695266724, loss=2.7349095344543457
I0201 17:52:24.644244 140023005427456 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.6740816831588745, loss=2.868152618408203
I0201 17:52:24.658846 140184451094336 spec.py:321] Evaluating on the training split.
I0201 17:52:35.316259 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 17:53:01.360763 140184451094336 spec.py:349] Evaluating on the test split.
I0201 17:53:02.957949 140184451094336 submission_runner.py:408] Time since start: 20199.13s, 	Step: 40401, 	{'train/accuracy': 0.5972656011581421, 'train/loss': 1.7070727348327637, 'validation/accuracy': 0.5578399896621704, 'validation/loss': 1.8923197984695435, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5406343936920166, 'test/num_examples': 10000, 'score': 18522.804244995117, 'total_duration': 20199.12549352646, 'accumulated_submission_time': 18522.804244995117, 'accumulated_eval_time': 1672.908019542694, 'accumulated_logging_time': 1.3188085556030273}
I0201 17:53:02.980226 140022518892288 logging_writer.py:48] [40401] accumulated_eval_time=1672.908020, accumulated_logging_time=1.318809, accumulated_submission_time=18522.804245, global_step=40401, preemption_count=0, score=18522.804245, test/accuracy=0.446200, test/loss=2.540634, test/num_examples=10000, total_duration=20199.125494, train/accuracy=0.597266, train/loss=1.707073, validation/accuracy=0.557840, validation/loss=1.892320, validation/num_examples=50000
I0201 17:53:44.290515 140023005427456 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.319871187210083, loss=5.025637149810791
I0201 17:54:30.410862 140022518892288 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.5041900873184204, loss=2.7790801525115967
I0201 17:55:16.601460 140023005427456 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.6631215810775757, loss=2.8144309520721436
I0201 17:56:02.721302 140022518892288 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7072983980178833, loss=2.9631471633911133
I0201 17:56:48.699115 140023005427456 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.3234577178955078, loss=4.317838191986084
I0201 17:57:34.823045 140022518892288 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.7434617280960083, loss=2.63940691947937
I0201 17:58:20.949104 140023005427456 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.632449984550476, loss=2.6140213012695312
I0201 17:59:07.507843 140022518892288 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5417664051055908, loss=3.971740245819092
I0201 17:59:53.633471 140023005427456 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.116494655609131, loss=2.698981761932373
I0201 18:00:03.154988 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:00:13.817044 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:00:40.330217 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:00:41.940336 140184451094336 submission_runner.py:408] Time since start: 20658.11s, 	Step: 41322, 	{'train/accuracy': 0.6015819907188416, 'train/loss': 1.6829158067703247, 'validation/accuracy': 0.557379961013794, 'validation/loss': 1.892937421798706, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.5214507579803467, 'test/num_examples': 10000, 'score': 18942.922422885895, 'total_duration': 20658.107868433, 'accumulated_submission_time': 18942.922422885895, 'accumulated_eval_time': 1711.6933376789093, 'accumulated_logging_time': 1.3502240180969238}
I0201 18:00:41.964812 140022518892288 logging_writer.py:48] [41322] accumulated_eval_time=1711.693338, accumulated_logging_time=1.350224, accumulated_submission_time=18942.922423, global_step=41322, preemption_count=0, score=18942.922423, test/accuracy=0.444800, test/loss=2.521451, test/num_examples=10000, total_duration=20658.107868, train/accuracy=0.601582, train/loss=1.682916, validation/accuracy=0.557380, validation/loss=1.892937, validation/num_examples=50000
I0201 18:01:13.876917 140023005427456 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.0916178226470947, loss=2.803654670715332
I0201 18:01:59.992626 140022518892288 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.615063190460205, loss=2.671269178390503
I0201 18:02:46.138427 140023005427456 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.6886241436004639, loss=2.8006277084350586
I0201 18:03:32.674615 140022518892288 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.2074978351593018, loss=4.975371837615967
I0201 18:04:18.932124 140023005427456 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.6658295392990112, loss=2.796036958694458
I0201 18:05:05.316336 140022518892288 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.3934649229049683, loss=4.563163757324219
I0201 18:05:51.377293 140023005427456 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.6260970830917358, loss=4.028382301330566
I0201 18:06:37.492649 140022518892288 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.471692681312561, loss=3.192153215408325
I0201 18:07:23.854710 140023005427456 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.6285539865493774, loss=2.8697140216827393
I0201 18:07:41.943177 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:07:52.412875 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:08:18.208568 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:08:19.809239 140184451094336 submission_runner.py:408] Time since start: 21115.98s, 	Step: 42241, 	{'train/accuracy': 0.6206249594688416, 'train/loss': 1.5839260816574097, 'validation/accuracy': 0.5624200105667114, 'validation/loss': 1.8680510520935059, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.519059419631958, 'test/num_examples': 10000, 'score': 19362.841962337494, 'total_duration': 21115.976779937744, 'accumulated_submission_time': 19362.841962337494, 'accumulated_eval_time': 1749.5593955516815, 'accumulated_logging_time': 1.3858461380004883}
I0201 18:08:19.835101 140022518892288 logging_writer.py:48] [42241] accumulated_eval_time=1749.559396, accumulated_logging_time=1.385846, accumulated_submission_time=19362.841962, global_step=42241, preemption_count=0, score=19362.841962, test/accuracy=0.443500, test/loss=2.519059, test/num_examples=10000, total_duration=21115.976780, train/accuracy=0.620625, train/loss=1.583926, validation/accuracy=0.562420, validation/loss=1.868051, validation/num_examples=50000
I0201 18:08:43.835784 140023005427456 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.5897331237792969, loss=3.16287899017334
I0201 18:09:28.813979 140022518892288 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.5636428594589233, loss=3.2517738342285156
I0201 18:10:14.956909 140023005427456 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.5078986883163452, loss=3.6821305751800537
I0201 18:11:01.091406 140022518892288 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.767938256263733, loss=2.60471773147583
I0201 18:11:47.280384 140023005427456 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.5919716358184814, loss=3.0388078689575195
I0201 18:12:33.737999 140022518892288 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5223314762115479, loss=2.7649359703063965
I0201 18:13:19.555795 140023005427456 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.7661864757537842, loss=2.6707987785339355
I0201 18:14:05.557340 140022518892288 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.770318627357483, loss=2.8252971172332764
I0201 18:14:51.557634 140023005427456 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.786288857460022, loss=2.640524387359619
I0201 18:15:19.898355 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:15:30.565726 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:15:58.030723 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:15:59.644021 140184451094336 submission_runner.py:408] Time since start: 21575.81s, 	Step: 43163, 	{'train/accuracy': 0.6048827767372131, 'train/loss': 1.6705154180526733, 'validation/accuracy': 0.5646600127220154, 'validation/loss': 1.8540127277374268, 'validation/num_examples': 50000, 'test/accuracy': 0.45250001549720764, 'test/loss': 2.4949381351470947, 'test/num_examples': 10000, 'score': 19782.84850549698, 'total_duration': 21575.811561346054, 'accumulated_submission_time': 19782.84850549698, 'accumulated_eval_time': 1789.3050591945648, 'accumulated_logging_time': 1.420839786529541}
I0201 18:15:59.665426 140022518892288 logging_writer.py:48] [43163] accumulated_eval_time=1789.305059, accumulated_logging_time=1.420840, accumulated_submission_time=19782.848505, global_step=43163, preemption_count=0, score=19782.848505, test/accuracy=0.452500, test/loss=2.494938, test/num_examples=10000, total_duration=21575.811561, train/accuracy=0.604883, train/loss=1.670515, validation/accuracy=0.564660, validation/loss=1.854013, validation/num_examples=50000
I0201 18:16:14.888226 140023005427456 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7135299444198608, loss=2.767587184906006
I0201 18:16:58.526062 140022518892288 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6734061241149902, loss=2.8026928901672363
I0201 18:17:44.496573 140023005427456 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.5624189376831055, loss=3.648578405380249
I0201 18:18:30.789051 140022518892288 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7030640840530396, loss=2.6381754875183105
I0201 18:19:16.754930 140023005427456 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.440523624420166, loss=5.166271209716797
I0201 18:20:03.104814 140022518892288 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.813677430152893, loss=2.878570556640625
I0201 18:20:49.406910 140023005427456 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.6565918922424316, loss=2.721684455871582
I0201 18:21:35.806035 140022518892288 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2069905996322632, loss=5.360140800476074
I0201 18:22:22.116111 140023005427456 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7691144943237305, loss=2.689189910888672
I0201 18:23:00.072139 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:23:10.668233 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:23:33.279898 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:23:34.874853 140184451094336 submission_runner.py:408] Time since start: 22031.04s, 	Step: 44084, 	{'train/accuracy': 0.6102148294448853, 'train/loss': 1.6581255197525024, 'validation/accuracy': 0.5626199841499329, 'validation/loss': 1.85548996925354, 'validation/num_examples': 50000, 'test/accuracy': 0.44940000772476196, 'test/loss': 2.488030433654785, 'test/num_examples': 10000, 'score': 20203.19352889061, 'total_duration': 22031.04239463806, 'accumulated_submission_time': 20203.19352889061, 'accumulated_eval_time': 1824.1077728271484, 'accumulated_logging_time': 1.4556865692138672}
I0201 18:23:34.895668 140022518892288 logging_writer.py:48] [44084] accumulated_eval_time=1824.107773, accumulated_logging_time=1.455687, accumulated_submission_time=20203.193529, global_step=44084, preemption_count=0, score=20203.193529, test/accuracy=0.449400, test/loss=2.488030, test/num_examples=10000, total_duration=22031.042395, train/accuracy=0.610215, train/loss=1.658126, validation/accuracy=0.562620, validation/loss=1.855490, validation/num_examples=50000
I0201 18:23:41.697378 140023005427456 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6771677732467651, loss=2.6728129386901855
I0201 18:24:23.967235 140022518892288 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.8349140882492065, loss=2.620086193084717
I0201 18:25:10.440419 140023005427456 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.312106966972351, loss=4.173936367034912
I0201 18:25:56.626909 140022518892288 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.340235948562622, loss=3.996713161468506
I0201 18:26:42.577905 140023005427456 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.8712232112884521, loss=2.7057509422302246
I0201 18:27:28.847001 140022518892288 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.81431245803833, loss=2.738522529602051
I0201 18:28:14.792353 140023005427456 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.4101673364639282, loss=4.128667831420898
I0201 18:29:00.916328 140022518892288 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7311177253723145, loss=2.612091541290283
I0201 18:29:47.024647 140023005427456 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7535814046859741, loss=2.912288188934326
I0201 18:30:33.153036 140022518892288 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.3809211254119873, loss=5.209785461425781
I0201 18:30:35.032535 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:30:45.577675 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:31:11.098559 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:31:12.700433 140184451094336 submission_runner.py:408] Time since start: 22488.87s, 	Step: 45006, 	{'train/accuracy': 0.6237499713897705, 'train/loss': 1.5927554368972778, 'validation/accuracy': 0.568619966506958, 'validation/loss': 1.843924641609192, 'validation/num_examples': 50000, 'test/accuracy': 0.4508000314235687, 'test/loss': 2.4984617233276367, 'test/num_examples': 10000, 'score': 20623.269857168198, 'total_duration': 22488.867975711823, 'accumulated_submission_time': 20623.269857168198, 'accumulated_eval_time': 1861.7756645679474, 'accumulated_logging_time': 1.4894487857818604}
I0201 18:31:12.722109 140023005427456 logging_writer.py:48] [45006] accumulated_eval_time=1861.775665, accumulated_logging_time=1.489449, accumulated_submission_time=20623.269857, global_step=45006, preemption_count=0, score=20623.269857, test/accuracy=0.450800, test/loss=2.498462, test/num_examples=10000, total_duration=22488.867976, train/accuracy=0.623750, train/loss=1.592755, validation/accuracy=0.568620, validation/loss=1.843925, validation/num_examples=50000
I0201 18:31:51.883894 140022518892288 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.7643344402313232, loss=2.68558406829834
I0201 18:32:37.981888 140023005427456 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2929707765579224, loss=5.306467056274414
I0201 18:33:24.554064 140022518892288 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.7211897373199463, loss=3.1586992740631104
I0201 18:34:10.837380 140023005427456 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.7601789236068726, loss=2.695307970046997
I0201 18:34:57.132461 140022518892288 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.5168434381484985, loss=3.8420286178588867
I0201 18:35:43.451354 140023005427456 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.4016408920288086, loss=3.563939094543457
I0201 18:36:29.498943 140022518892288 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.6992812156677246, loss=2.9026427268981934
I0201 18:37:15.666756 140023005427456 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.9318996667861938, loss=2.6761817932128906
I0201 18:38:01.821915 140022518892288 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7121388912200928, loss=2.679316997528076
I0201 18:38:13.184181 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:38:23.689622 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:38:46.828883 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:38:48.426007 140184451094336 submission_runner.py:408] Time since start: 22944.59s, 	Step: 45926, 	{'train/accuracy': 0.605175793170929, 'train/loss': 1.6940358877182007, 'validation/accuracy': 0.5661999583244324, 'validation/loss': 1.8739290237426758, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.526916027069092, 'test/num_examples': 10000, 'score': 21043.675313472748, 'total_duration': 22944.59355187416, 'accumulated_submission_time': 21043.675313472748, 'accumulated_eval_time': 1897.01748919487, 'accumulated_logging_time': 1.5201151371002197}
I0201 18:38:48.447216 140023005427456 logging_writer.py:48] [45926] accumulated_eval_time=1897.017489, accumulated_logging_time=1.520115, accumulated_submission_time=21043.675313, global_step=45926, preemption_count=0, score=21043.675313, test/accuracy=0.446200, test/loss=2.526916, test/num_examples=10000, total_duration=22944.593552, train/accuracy=0.605176, train/loss=1.694036, validation/accuracy=0.566200, validation/loss=1.873929, validation/num_examples=50000
I0201 18:39:18.434803 140022518892288 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.8812886476516724, loss=2.826785087585449
I0201 18:40:04.355889 140023005427456 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7488240003585815, loss=2.602363348007202
I0201 18:40:50.634510 140022518892288 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7428523302078247, loss=3.1079492568969727
I0201 18:41:37.296815 140023005427456 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6976330280303955, loss=2.7631659507751465
I0201 18:42:23.153585 140022518892288 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.357653021812439, loss=5.189277648925781
I0201 18:43:09.351215 140023005427456 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.775347352027893, loss=2.7212276458740234
I0201 18:43:55.338589 140022518892288 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.9002596139907837, loss=2.662034034729004
I0201 18:44:41.694092 140023005427456 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7311958074569702, loss=2.5955276489257812
I0201 18:45:27.844888 140022518892288 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.269724726676941, loss=4.69462776184082
I0201 18:45:48.738384 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:45:59.177695 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:46:19.891995 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:46:21.498232 140184451094336 submission_runner.py:408] Time since start: 23397.67s, 	Step: 46847, 	{'train/accuracy': 0.6008007526397705, 'train/loss': 1.7134603261947632, 'validation/accuracy': 0.5633000135421753, 'validation/loss': 1.8950095176696777, 'validation/num_examples': 50000, 'test/accuracy': 0.4472000300884247, 'test/loss': 2.5412986278533936, 'test/num_examples': 10000, 'score': 21463.90951180458, 'total_duration': 23397.665759801865, 'accumulated_submission_time': 21463.90951180458, 'accumulated_eval_time': 1929.7773234844208, 'accumulated_logging_time': 1.5505378246307373}
I0201 18:46:21.522634 140023005427456 logging_writer.py:48] [46847] accumulated_eval_time=1929.777323, accumulated_logging_time=1.550538, accumulated_submission_time=21463.909512, global_step=46847, preemption_count=0, score=21463.909512, test/accuracy=0.447200, test/loss=2.541299, test/num_examples=10000, total_duration=23397.665760, train/accuracy=0.600801, train/loss=1.713460, validation/accuracy=0.563300, validation/loss=1.895010, validation/num_examples=50000
I0201 18:46:43.115971 140022518892288 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.640321135520935, loss=3.633152484893799
I0201 18:47:27.505589 140023005427456 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.2420421838760376, loss=4.762373924255371
I0201 18:48:13.535904 140022518892288 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.8587716817855835, loss=2.8350677490234375
I0201 18:48:59.699091 140023005427456 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.817495584487915, loss=5.199036598205566
I0201 18:49:45.736332 140022518892288 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.3595085144042969, loss=4.591889381408691
I0201 18:50:32.021670 140023005427456 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.199577569961548, loss=2.7456295490264893
I0201 18:51:18.122088 140022518892288 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.2516438961029053, loss=5.340632438659668
I0201 18:52:04.603206 140023005427456 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.5549260377883911, loss=3.0725553035736084
I0201 18:52:50.659324 140022518892288 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.4404734373092651, loss=5.121510982513428
I0201 18:53:21.874922 140184451094336 spec.py:321] Evaluating on the training split.
I0201 18:53:32.411859 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 18:53:59.029929 140184451094336 spec.py:349] Evaluating on the test split.
I0201 18:54:00.628728 140184451094336 submission_runner.py:408] Time since start: 23856.80s, 	Step: 47769, 	{'train/accuracy': 0.6170703172683716, 'train/loss': 1.6229615211486816, 'validation/accuracy': 0.5671600103378296, 'validation/loss': 1.8515721559524536, 'validation/num_examples': 50000, 'test/accuracy': 0.45270001888275146, 'test/loss': 2.5124051570892334, 'test/num_examples': 10000, 'score': 21884.20440530777, 'total_duration': 23856.79626774788, 'accumulated_submission_time': 21884.20440530777, 'accumulated_eval_time': 1968.5311410427094, 'accumulated_logging_time': 1.5848033428192139}
I0201 18:54:00.653534 140023005427456 logging_writer.py:48] [47769] accumulated_eval_time=1968.531141, accumulated_logging_time=1.584803, accumulated_submission_time=21884.204405, global_step=47769, preemption_count=0, score=21884.204405, test/accuracy=0.452700, test/loss=2.512405, test/num_examples=10000, total_duration=23856.796268, train/accuracy=0.617070, train/loss=1.622962, validation/accuracy=0.567160, validation/loss=1.851572, validation/num_examples=50000
I0201 18:54:13.442237 140022518892288 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.7499622106552124, loss=2.782677173614502
I0201 18:54:56.653625 140023005427456 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7490465641021729, loss=2.7712855339050293
I0201 18:55:42.798947 140022518892288 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.362198829650879, loss=5.108335494995117
I0201 18:56:29.143996 140023005427456 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.5381066799163818, loss=3.6498939990997314
I0201 18:57:15.258130 140022518892288 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.8653184175491333, loss=3.0598032474517822
I0201 18:58:01.522581 140023005427456 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.2379045486450195, loss=4.551791667938232
I0201 18:58:47.485370 140022518892288 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.5259895324707031, loss=2.8598926067352295
I0201 18:59:33.674594 140023005427456 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.8424655199050903, loss=2.622811794281006
I0201 19:00:19.881830 140022518892288 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.789421558380127, loss=2.7047441005706787
I0201 19:01:01.016112 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:01:11.942573 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:01:37.907565 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:01:39.505491 140184451094336 submission_runner.py:408] Time since start: 24315.67s, 	Step: 48691, 	{'train/accuracy': 0.6227929592132568, 'train/loss': 1.575631856918335, 'validation/accuracy': 0.5725600123405457, 'validation/loss': 1.808995008468628, 'validation/num_examples': 50000, 'test/accuracy': 0.45740002393722534, 'test/loss': 2.4445207118988037, 'test/num_examples': 10000, 'score': 22304.507378816605, 'total_duration': 24315.673028230667, 'accumulated_submission_time': 22304.507378816605, 'accumulated_eval_time': 2007.020524263382, 'accumulated_logging_time': 1.6213884353637695}
I0201 19:01:39.531303 140023005427456 logging_writer.py:48] [48691] accumulated_eval_time=2007.020524, accumulated_logging_time=1.621388, accumulated_submission_time=22304.507379, global_step=48691, preemption_count=0, score=22304.507379, test/accuracy=0.457400, test/loss=2.444521, test/num_examples=10000, total_duration=24315.673028, train/accuracy=0.622793, train/loss=1.575632, validation/accuracy=0.572560, validation/loss=1.808995, validation/num_examples=50000
I0201 19:01:43.535743 140022518892288 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.9520468711853027, loss=2.6490750312805176
I0201 19:02:25.675905 140023005427456 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.4652081727981567, loss=4.9023613929748535
I0201 19:03:11.674116 140022518892288 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.6319118738174438, loss=2.787970542907715
I0201 19:03:58.028684 140023005427456 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.7615612745285034, loss=2.625373363494873
I0201 19:04:44.131856 140022518892288 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.772318720817566, loss=2.7448220252990723
I0201 19:05:30.375711 140023005427456 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.7691025733947754, loss=3.3376107215881348
I0201 19:06:16.732856 140022518892288 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.5169401168823242, loss=4.763294219970703
I0201 19:07:02.765143 140023005427456 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.753988265991211, loss=2.7727530002593994
I0201 19:07:48.814394 140022518892288 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8249903917312622, loss=2.7576818466186523
I0201 19:08:34.957647 140023005427456 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.0418832302093506, loss=2.7803361415863037
I0201 19:08:39.633912 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:08:50.809463 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:09:17.400778 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:09:19.011267 140184451094336 submission_runner.py:408] Time since start: 24775.18s, 	Step: 49612, 	{'train/accuracy': 0.6174609065055847, 'train/loss': 1.6112401485443115, 'validation/accuracy': 0.573639988899231, 'validation/loss': 1.8005505800247192, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.4524426460266113, 'test/num_examples': 10000, 'score': 22724.552975654602, 'total_duration': 24775.178783893585, 'accumulated_submission_time': 22724.552975654602, 'accumulated_eval_time': 2046.3978426456451, 'accumulated_logging_time': 1.657334804534912}
I0201 19:09:19.039347 140022518892288 logging_writer.py:48] [49612] accumulated_eval_time=2046.397843, accumulated_logging_time=1.657335, accumulated_submission_time=22724.552976, global_step=49612, preemption_count=0, score=22724.552976, test/accuracy=0.461700, test/loss=2.452443, test/num_examples=10000, total_duration=24775.178784, train/accuracy=0.617461, train/loss=1.611240, validation/accuracy=0.573640, validation/loss=1.800551, validation/num_examples=50000
I0201 19:09:55.387876 140023005427456 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.6900215148925781, loss=2.717578649520874
I0201 19:10:41.325052 140022518892288 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.6240006685256958, loss=2.522775888442993
I0201 19:11:27.637202 140023005427456 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.831883192062378, loss=2.636425495147705
I0201 19:12:14.056776 140022518892288 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.699474811553955, loss=2.6059727668762207
I0201 19:13:00.446489 140023005427456 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.2471494674682617, loss=4.45540714263916
I0201 19:13:46.515000 140022518892288 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.7289068698883057, loss=2.5246973037719727
I0201 19:14:32.764834 140023005427456 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7953156232833862, loss=2.6804451942443848
I0201 19:15:18.893035 140022518892288 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.8871411085128784, loss=2.7006444931030273
I0201 19:16:04.983705 140023005427456 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.6336236000061035, loss=2.8709897994995117
I0201 19:16:19.314533 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:16:29.710397 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:16:52.246908 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:16:53.852905 140184451094336 submission_runner.py:408] Time since start: 25230.02s, 	Step: 50533, 	{'train/accuracy': 0.62060546875, 'train/loss': 1.6004736423492432, 'validation/accuracy': 0.5769400000572205, 'validation/loss': 1.7959643602371216, 'validation/num_examples': 50000, 'test/accuracy': 0.46630001068115234, 'test/loss': 2.434957981109619, 'test/num_examples': 10000, 'score': 23144.769639730453, 'total_duration': 25230.020445346832, 'accumulated_submission_time': 23144.769639730453, 'accumulated_eval_time': 2080.936208486557, 'accumulated_logging_time': 1.6951377391815186}
I0201 19:16:53.879706 140022518892288 logging_writer.py:48] [50533] accumulated_eval_time=2080.936208, accumulated_logging_time=1.695138, accumulated_submission_time=23144.769640, global_step=50533, preemption_count=0, score=23144.769640, test/accuracy=0.466300, test/loss=2.434958, test/num_examples=10000, total_duration=25230.020445, train/accuracy=0.620605, train/loss=1.600474, validation/accuracy=0.576940, validation/loss=1.795964, validation/num_examples=50000
I0201 19:17:21.070466 140023005427456 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8029580116271973, loss=2.947631597518921
I0201 19:18:06.677494 140022518892288 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.736201286315918, loss=3.02333664894104
I0201 19:18:52.794603 140023005427456 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8333848714828491, loss=2.7508816719055176
I0201 19:19:39.154951 140022518892288 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8142272233963013, loss=2.586925745010376
I0201 19:20:25.370214 140023005427456 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.3447551727294922, loss=5.21579122543335
I0201 19:21:11.557512 140022518892288 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7336865663528442, loss=2.5723214149475098
I0201 19:21:57.968681 140023005427456 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.6297938823699951, loss=3.17785382270813
I0201 19:22:44.131845 140022518892288 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9491233825683594, loss=2.6658167839050293
I0201 19:23:30.206261 140023005427456 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7962448596954346, loss=2.57686710357666
I0201 19:23:53.899431 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:24:04.585899 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:24:31.605913 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:24:33.200497 140184451094336 submission_runner.py:408] Time since start: 25689.37s, 	Step: 51453, 	{'train/accuracy': 0.6434960961341858, 'train/loss': 1.4826300144195557, 'validation/accuracy': 0.5772199630737305, 'validation/loss': 1.7871181964874268, 'validation/num_examples': 50000, 'test/accuracy': 0.45590001344680786, 'test/loss': 2.4530246257781982, 'test/num_examples': 10000, 'score': 23564.732614278793, 'total_duration': 25689.36802005768, 'accumulated_submission_time': 23564.732614278793, 'accumulated_eval_time': 2120.2372534275055, 'accumulated_logging_time': 1.7319800853729248}
I0201 19:24:33.227413 140022518892288 logging_writer.py:48] [51453] accumulated_eval_time=2120.237253, accumulated_logging_time=1.731980, accumulated_submission_time=23564.732614, global_step=51453, preemption_count=0, score=23564.732614, test/accuracy=0.455900, test/loss=2.453025, test/num_examples=10000, total_duration=25689.368020, train/accuracy=0.643496, train/loss=1.482630, validation/accuracy=0.577220, validation/loss=1.787118, validation/num_examples=50000
I0201 19:24:52.442297 140023005427456 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.9344011545181274, loss=2.662947416305542
I0201 19:25:36.588373 140022518892288 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.4947397708892822, loss=3.529926061630249
I0201 19:26:22.668131 140023005427456 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.7540851831436157, loss=2.6126790046691895
I0201 19:27:08.982795 140022518892288 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8190313577651978, loss=2.7276833057403564
I0201 19:27:54.780658 140023005427456 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.065337896347046, loss=2.491508960723877
I0201 19:28:41.035304 140022518892288 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.6746540069580078, loss=2.5838680267333984
I0201 19:29:26.991075 140023005427456 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.5050835609436035, loss=4.709230422973633
I0201 19:30:13.209774 140022518892288 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8720780611038208, loss=2.6640233993530273
I0201 19:30:59.278385 140023005427456 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.3702542781829834, loss=3.401615619659424
I0201 19:31:33.476957 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:31:44.444865 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:32:11.819293 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:32:13.418105 140184451094336 submission_runner.py:408] Time since start: 26149.59s, 	Step: 52375, 	{'train/accuracy': 0.6228905916213989, 'train/loss': 1.5904203653335571, 'validation/accuracy': 0.5809999704360962, 'validation/loss': 1.7867530584335327, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.4186763763427734, 'test/num_examples': 10000, 'score': 23984.924451828003, 'total_duration': 26149.58564400673, 'accumulated_submission_time': 23984.924451828003, 'accumulated_eval_time': 2160.178407430649, 'accumulated_logging_time': 1.7693891525268555}
I0201 19:32:13.444106 140022518892288 logging_writer.py:48] [52375] accumulated_eval_time=2160.178407, accumulated_logging_time=1.769389, accumulated_submission_time=23984.924452, global_step=52375, preemption_count=0, score=23984.924452, test/accuracy=0.462800, test/loss=2.418676, test/num_examples=10000, total_duration=26149.585644, train/accuracy=0.622891, train/loss=1.590420, validation/accuracy=0.581000, validation/loss=1.786753, validation/num_examples=50000
I0201 19:32:23.840619 140023005427456 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.8341156244277954, loss=2.664634943008423
I0201 19:33:06.727585 140022518892288 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.884484052658081, loss=2.573854446411133
I0201 19:33:53.065420 140023005427456 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.6743229627609253, loss=2.5573551654815674
I0201 19:34:39.313531 140022518892288 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8357595205307007, loss=2.6686112880706787
I0201 19:35:25.367575 140023005427456 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.7053890228271484, loss=3.3891966342926025
I0201 19:36:11.626067 140022518892288 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.7539299726486206, loss=2.626783847808838
I0201 19:36:57.888336 140023005427456 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8252562284469604, loss=2.660815477371216
I0201 19:37:44.143585 140022518892288 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.0286505222320557, loss=2.594630718231201
I0201 19:38:30.280940 140023005427456 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.037278890609741, loss=2.7415390014648438
I0201 19:39:13.497004 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:39:24.036394 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:39:46.752074 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:39:48.355670 140184451094336 submission_runner.py:408] Time since start: 26604.52s, 	Step: 53295, 	{'train/accuracy': 0.6255077719688416, 'train/loss': 1.5740492343902588, 'validation/accuracy': 0.5805599689483643, 'validation/loss': 1.7782132625579834, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.41633677482605, 'test/num_examples': 10000, 'score': 24404.92098903656, 'total_duration': 26604.52316379547, 'accumulated_submission_time': 24404.92098903656, 'accumulated_eval_time': 2195.0370230674744, 'accumulated_logging_time': 1.8044648170471191}
I0201 19:39:48.384719 140022518892288 logging_writer.py:48] [53295] accumulated_eval_time=2195.037023, accumulated_logging_time=1.804465, accumulated_submission_time=24404.920989, global_step=53295, preemption_count=0, score=24404.920989, test/accuracy=0.462900, test/loss=2.416337, test/num_examples=10000, total_duration=26604.523164, train/accuracy=0.625508, train/loss=1.574049, validation/accuracy=0.580560, validation/loss=1.778213, validation/num_examples=50000
I0201 19:39:50.788194 140023005427456 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.4176276922225952, loss=5.294174671173096
I0201 19:40:32.494610 140022518892288 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.6245639324188232, loss=2.7194559574127197
I0201 19:41:18.455853 140023005427456 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.548130750656128, loss=3.279275894165039
I0201 19:42:05.073809 140022518892288 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3332711458206177, loss=4.548147201538086
I0201 19:42:50.852870 140023005427456 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.619183897972107, loss=2.4463744163513184
I0201 19:43:37.114410 140022518892288 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.7635785341262817, loss=2.5249931812286377
I0201 19:44:23.364689 140023005427456 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.815226674079895, loss=2.77799129486084
I0201 19:45:09.392654 140022518892288 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.7192593812942505, loss=4.815122127532959
I0201 19:45:55.286802 140023005427456 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.3580732345581055, loss=4.832911491394043
I0201 19:46:41.311735 140022518892288 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.6750209331512451, loss=2.723846435546875
I0201 19:46:48.799473 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:46:59.296943 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:47:22.455948 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:47:24.057897 140184451094336 submission_runner.py:408] Time since start: 27060.23s, 	Step: 54218, 	{'train/accuracy': 0.6314257383346558, 'train/loss': 1.5377583503723145, 'validation/accuracy': 0.5790599584579468, 'validation/loss': 1.7804241180419922, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.433056354522705, 'test/num_examples': 10000, 'score': 24825.276747226715, 'total_duration': 27060.225431203842, 'accumulated_submission_time': 24825.276747226715, 'accumulated_eval_time': 2230.2954602241516, 'accumulated_logging_time': 1.8441081047058105}
I0201 19:47:24.083452 140023005427456 logging_writer.py:48] [54218] accumulated_eval_time=2230.295460, accumulated_logging_time=1.844108, accumulated_submission_time=24825.276747, global_step=54218, preemption_count=0, score=24825.276747, test/accuracy=0.466900, test/loss=2.433056, test/num_examples=10000, total_duration=27060.225431, train/accuracy=0.631426, train/loss=1.537758, validation/accuracy=0.579060, validation/loss=1.780424, validation/num_examples=50000
I0201 19:47:57.505159 140022518892288 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.2941904067993164, loss=5.243781089782715
I0201 19:48:43.311121 140023005427456 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.064032554626465, loss=2.6510207653045654
I0201 19:49:29.557104 140022518892288 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.7417410612106323, loss=2.638875722885132
I0201 19:50:15.732854 140023005427456 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.6241278648376465, loss=2.421236515045166
I0201 19:51:01.940010 140022518892288 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.674729347229004, loss=3.7689590454101562
I0201 19:51:48.290116 140023005427456 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.5546798706054688, loss=5.386902332305908
I0201 19:52:34.525763 140022518892288 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.794612169265747, loss=2.6617588996887207
I0201 19:53:20.897325 140023005427456 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7773666381835938, loss=2.570176124572754
I0201 19:54:07.181317 140022518892288 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.5657175779342651, loss=3.654822826385498
I0201 19:54:24.084206 140184451094336 spec.py:321] Evaluating on the training split.
I0201 19:54:34.631060 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 19:55:00.463687 140184451094336 spec.py:349] Evaluating on the test split.
I0201 19:55:02.069826 140184451094336 submission_runner.py:408] Time since start: 27518.24s, 	Step: 55138, 	{'train/accuracy': 0.6166015267372131, 'train/loss': 1.6154673099517822, 'validation/accuracy': 0.5781399607658386, 'validation/loss': 1.7982147932052612, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.452510356903076, 'test/num_examples': 10000, 'score': 25245.21862053871, 'total_duration': 27518.237367630005, 'accumulated_submission_time': 25245.21862053871, 'accumulated_eval_time': 2268.2810649871826, 'accumulated_logging_time': 1.8811235427856445}
I0201 19:55:02.096455 140023005427456 logging_writer.py:48] [55138] accumulated_eval_time=2268.281065, accumulated_logging_time=1.881124, accumulated_submission_time=25245.218621, global_step=55138, preemption_count=0, score=25245.218621, test/accuracy=0.455400, test/loss=2.452510, test/num_examples=10000, total_duration=27518.237368, train/accuracy=0.616602, train/loss=1.615467, validation/accuracy=0.578140, validation/loss=1.798215, validation/num_examples=50000
I0201 19:55:27.671147 140022518892288 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.729816198348999, loss=3.3849117755889893
I0201 19:56:12.784174 140023005427456 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.5441601276397705, loss=5.216122627258301
I0201 19:56:59.123565 140022518892288 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8906317949295044, loss=2.612900733947754
I0201 19:57:45.789386 140023005427456 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.6416327953338623, loss=3.32047438621521
I0201 19:58:31.970553 140022518892288 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.8584083318710327, loss=2.5886330604553223
I0201 19:59:18.195129 140023005427456 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.7093772888183594, loss=3.0359864234924316
I0201 20:00:04.440447 140022518892288 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.5524893999099731, loss=4.954461097717285
I0201 20:00:50.308285 140023005427456 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.7249298095703125, loss=2.572155475616455
I0201 20:01:36.798255 140022518892288 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.4776195287704468, loss=4.54558801651001
I0201 20:02:02.403991 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:02:13.081140 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:02:35.267741 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:02:36.869929 140184451094336 submission_runner.py:408] Time since start: 27973.04s, 	Step: 56056, 	{'train/accuracy': 0.6266992092132568, 'train/loss': 1.5533452033996582, 'validation/accuracy': 0.5848999619483948, 'validation/loss': 1.7521530389785767, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.410242795944214, 'test/num_examples': 10000, 'score': 25665.102162361145, 'total_duration': 27973.03744482994, 'accumulated_submission_time': 25665.102162361145, 'accumulated_eval_time': 2302.746966123581, 'accumulated_logging_time': 2.2839317321777344}
I0201 20:02:36.897740 140023005427456 logging_writer.py:48] [56056] accumulated_eval_time=2302.746966, accumulated_logging_time=2.283932, accumulated_submission_time=25665.102162, global_step=56056, preemption_count=0, score=25665.102162, test/accuracy=0.469800, test/loss=2.410243, test/num_examples=10000, total_duration=27973.037445, train/accuracy=0.626699, train/loss=1.553345, validation/accuracy=0.584900, validation/loss=1.752153, validation/num_examples=50000
I0201 20:02:54.906250 140022518892288 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.7835402488708496, loss=2.7205088138580322
I0201 20:03:38.871420 140023005427456 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.7937278747558594, loss=2.463010311126709
I0201 20:04:25.160985 140022518892288 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8743659257888794, loss=2.646914005279541
I0201 20:05:11.455845 140023005427456 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.8998750448226929, loss=2.5819616317749023
I0201 20:05:57.317804 140022518892288 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.4865056276321411, loss=5.019169807434082
I0201 20:06:43.433686 140023005427456 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8677705526351929, loss=3.090226888656616
I0201 20:07:29.701145 140022518892288 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.7346112728118896, loss=2.6328446865081787
I0201 20:08:15.857224 140023005427456 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.5671197175979614, loss=2.895662546157837
I0201 20:09:01.993433 140022518892288 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.4843753576278687, loss=4.160383224487305
I0201 20:09:37.195394 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:09:47.775325 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:10:14.214737 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:10:15.818405 140184451094336 submission_runner.py:408] Time since start: 28431.99s, 	Step: 56978, 	{'train/accuracy': 0.6295312643051147, 'train/loss': 1.5653393268585205, 'validation/accuracy': 0.5822399854660034, 'validation/loss': 1.799446702003479, 'validation/num_examples': 50000, 'test/accuracy': 0.46320003271102905, 'test/loss': 2.456258773803711, 'test/num_examples': 10000, 'score': 26085.34273672104, 'total_duration': 28431.985939502716, 'accumulated_submission_time': 26085.34273672104, 'accumulated_eval_time': 2341.3699703216553, 'accumulated_logging_time': 2.3208351135253906}
I0201 20:10:15.842071 140023005427456 logging_writer.py:48] [56978] accumulated_eval_time=2341.369970, accumulated_logging_time=2.320835, accumulated_submission_time=26085.342737, global_step=56978, preemption_count=0, score=26085.342737, test/accuracy=0.463200, test/loss=2.456259, test/num_examples=10000, total_duration=28431.985940, train/accuracy=0.629531, train/loss=1.565339, validation/accuracy=0.582240, validation/loss=1.799447, validation/num_examples=50000
I0201 20:10:25.058784 140022518892288 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.5301973819732666, loss=4.123593330383301
I0201 20:11:07.740254 140023005427456 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.5412790775299072, loss=3.106971502304077
I0201 20:11:53.739666 140022518892288 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.355464220046997, loss=4.807088375091553
I0201 20:12:40.183420 140023005427456 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.6058639287948608, loss=3.1660585403442383
I0201 20:13:26.343110 140022518892288 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.701414942741394, loss=2.7809178829193115
I0201 20:14:12.726032 140023005427456 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8703584671020508, loss=2.847761631011963
I0201 20:14:59.149373 140022518892288 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.3996535539627075, loss=3.5830862522125244
I0201 20:15:45.166699 140023005427456 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.4024064540863037, loss=4.010512828826904
I0201 20:16:31.492439 140022518892288 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.6616840362548828, loss=2.646117687225342
I0201 20:17:15.908016 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:17:26.646856 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:17:53.040649 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:17:54.640964 140184451094336 submission_runner.py:408] Time since start: 28890.81s, 	Step: 57898, 	{'train/accuracy': 0.6297656297683716, 'train/loss': 1.5668786764144897, 'validation/accuracy': 0.5859599709510803, 'validation/loss': 1.7582910060882568, 'validation/num_examples': 50000, 'test/accuracy': 0.4683000147342682, 'test/loss': 2.4005134105682373, 'test/num_examples': 10000, 'score': 26505.349090337753, 'total_duration': 28890.808502435684, 'accumulated_submission_time': 26505.349090337753, 'accumulated_eval_time': 2380.102923631668, 'accumulated_logging_time': 2.3555450439453125}
I0201 20:17:54.664869 140023005427456 logging_writer.py:48] [57898] accumulated_eval_time=2380.102924, accumulated_logging_time=2.355545, accumulated_submission_time=26505.349090, global_step=57898, preemption_count=0, score=26505.349090, test/accuracy=0.468300, test/loss=2.400513, test/num_examples=10000, total_duration=28890.808502, train/accuracy=0.629766, train/loss=1.566879, validation/accuracy=0.585960, validation/loss=1.758291, validation/num_examples=50000
I0201 20:17:55.868386 140022518892288 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8337504863739014, loss=2.7228524684906006
I0201 20:18:37.765626 140023005427456 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.431497573852539, loss=4.149982452392578
I0201 20:19:23.860575 140022518892288 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.7883471250534058, loss=2.6672780513763428
I0201 20:20:10.216805 140023005427456 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.648782730102539, loss=4.655242919921875
I0201 20:20:56.374459 140022518892288 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.4823501110076904, loss=3.5258398056030273
I0201 20:21:42.779891 140023005427456 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.0324342250823975, loss=2.7888336181640625
I0201 20:22:29.123830 140022518892288 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8230751752853394, loss=3.105027198791504
I0201 20:23:15.214055 140023005427456 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.4162898063659668, loss=4.30086612701416
I0201 20:24:01.485036 140022518892288 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.9352666139602661, loss=2.5958480834960938
I0201 20:24:47.949182 140023005427456 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.5266259908676147, loss=4.076033592224121
I0201 20:24:54.917869 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:25:05.462397 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:25:27.721891 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:25:29.330255 140184451094336 submission_runner.py:408] Time since start: 29345.50s, 	Step: 58817, 	{'train/accuracy': 0.6303515434265137, 'train/loss': 1.5301530361175537, 'validation/accuracy': 0.590399980545044, 'validation/loss': 1.723675012588501, 'validation/num_examples': 50000, 'test/accuracy': 0.46960002183914185, 'test/loss': 2.3985865116119385, 'test/num_examples': 10000, 'score': 26925.5447409153, 'total_duration': 29345.497764587402, 'accumulated_submission_time': 26925.5447409153, 'accumulated_eval_time': 2414.5152776241302, 'accumulated_logging_time': 2.3888587951660156}
I0201 20:25:29.362700 140022518892288 logging_writer.py:48] [58817] accumulated_eval_time=2414.515278, accumulated_logging_time=2.388859, accumulated_submission_time=26925.544741, global_step=58817, preemption_count=0, score=26925.544741, test/accuracy=0.469600, test/loss=2.398587, test/num_examples=10000, total_duration=29345.497765, train/accuracy=0.630352, train/loss=1.530153, validation/accuracy=0.590400, validation/loss=1.723675, validation/num_examples=50000
I0201 20:26:03.277014 140023005427456 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.860695481300354, loss=2.595616579055786
I0201 20:26:49.147132 140022518892288 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.7058078050613403, loss=2.779984951019287
I0201 20:27:35.727448 140023005427456 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.7071161270141602, loss=2.7213997840881348
I0201 20:28:22.164355 140022518892288 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.0323617458343506, loss=2.590609073638916
I0201 20:29:08.491763 140023005427456 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.059924364089966, loss=2.754059076309204
I0201 20:29:54.709779 140022518892288 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.665746808052063, loss=2.786376476287842
I0201 20:30:41.003326 140023005427456 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.9212995767593384, loss=3.078014373779297
I0201 20:31:27.070683 140022518892288 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.7279739379882812, loss=3.810694694519043
I0201 20:32:13.706772 140023005427456 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.036015748977661, loss=2.5396697521209717
I0201 20:32:29.451795 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:32:39.873238 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:33:06.722124 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:33:08.323429 140184451094336 submission_runner.py:408] Time since start: 29804.49s, 	Step: 59735, 	{'train/accuracy': 0.6366991996765137, 'train/loss': 1.510748267173767, 'validation/accuracy': 0.590719997882843, 'validation/loss': 1.7339236736297607, 'validation/num_examples': 50000, 'test/accuracy': 0.4668000340461731, 'test/loss': 2.381608486175537, 'test/num_examples': 10000, 'score': 27345.576851844788, 'total_duration': 29804.49097251892, 'accumulated_submission_time': 27345.576851844788, 'accumulated_eval_time': 2453.3868992328644, 'accumulated_logging_time': 2.431230306625366}
I0201 20:33:08.348041 140022518892288 logging_writer.py:48] [59735] accumulated_eval_time=2453.386899, accumulated_logging_time=2.431230, accumulated_submission_time=27345.576852, global_step=59735, preemption_count=0, score=27345.576852, test/accuracy=0.466800, test/loss=2.381608, test/num_examples=10000, total_duration=29804.490973, train/accuracy=0.636699, train/loss=1.510748, validation/accuracy=0.590720, validation/loss=1.733924, validation/num_examples=50000
I0201 20:33:34.731364 140023005427456 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.592153549194336, loss=5.227319717407227
I0201 20:34:20.392981 140022518892288 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.8803819417953491, loss=2.5169076919555664
I0201 20:35:06.466977 140023005427456 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.0188961029052734, loss=2.556713581085205
I0201 20:35:52.644785 140022518892288 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.750746250152588, loss=2.603987216949463
I0201 20:36:38.679744 140023005427456 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.7390726804733276, loss=2.4873178005218506
I0201 20:37:25.123837 140022518892288 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.8418513536453247, loss=2.481128692626953
I0201 20:38:11.253566 140023005427456 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9473469257354736, loss=2.503729820251465
I0201 20:38:57.401997 140022518892288 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.605866551399231, loss=4.960070610046387
I0201 20:39:43.945255 140023005427456 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.592164397239685, loss=3.084606409072876
I0201 20:40:08.609402 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:40:18.999893 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:40:47.986140 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:40:49.592942 140184451094336 submission_runner.py:408] Time since start: 30265.76s, 	Step: 60655, 	{'train/accuracy': 0.661816418170929, 'train/loss': 1.413130760192871, 'validation/accuracy': 0.5918799638748169, 'validation/loss': 1.732790231704712, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.3704307079315186, 'test/num_examples': 10000, 'score': 27765.77965736389, 'total_duration': 30265.760466575623, 'accumulated_submission_time': 27765.77965736389, 'accumulated_eval_time': 2494.3704164028168, 'accumulated_logging_time': 2.467085599899292}
I0201 20:40:49.621292 140022518892288 logging_writer.py:48] [60655] accumulated_eval_time=2494.370416, accumulated_logging_time=2.467086, accumulated_submission_time=27765.779657, global_step=60655, preemption_count=0, score=27765.779657, test/accuracy=0.471600, test/loss=2.370431, test/num_examples=10000, total_duration=30265.760467, train/accuracy=0.661816, train/loss=1.413131, validation/accuracy=0.591880, validation/loss=1.732790, validation/num_examples=50000
I0201 20:41:08.038219 140023005427456 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.6336780786514282, loss=2.638869285583496
I0201 20:41:52.263367 140022518892288 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.1459758281707764, loss=2.665376901626587
I0201 20:42:38.539616 140023005427456 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8608335256576538, loss=2.853308916091919
I0201 20:43:25.012191 140022518892288 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8071749210357666, loss=2.5425071716308594
I0201 20:44:11.220449 140023005427456 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.4222440719604492, loss=5.100454807281494
I0201 20:44:57.310442 140022518892288 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.978894829750061, loss=2.533064126968384
I0201 20:45:43.692433 140023005427456 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.7699322700500488, loss=3.032170295715332
I0201 20:46:29.796227 140022518892288 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.687829852104187, loss=2.8958067893981934
I0201 20:47:16.079474 140023005427456 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.4481946229934692, loss=5.005134582519531
I0201 20:47:49.969233 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:48:00.430901 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:48:21.967386 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:48:23.594032 140184451094336 submission_runner.py:408] Time since start: 30719.76s, 	Step: 61575, 	{'train/accuracy': 0.6309961080551147, 'train/loss': 1.5477447509765625, 'validation/accuracy': 0.5902799963951111, 'validation/loss': 1.7401505708694458, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.3999736309051514, 'test/num_examples': 10000, 'score': 28186.06973552704, 'total_duration': 30719.761566877365, 'accumulated_submission_time': 28186.06973552704, 'accumulated_eval_time': 2527.9952044487, 'accumulated_logging_time': 2.5055477619171143}
I0201 20:48:23.623092 140022518892288 logging_writer.py:48] [61575] accumulated_eval_time=2527.995204, accumulated_logging_time=2.505548, accumulated_submission_time=28186.069736, global_step=61575, preemption_count=0, score=28186.069736, test/accuracy=0.467800, test/loss=2.399974, test/num_examples=10000, total_duration=30719.761567, train/accuracy=0.630996, train/loss=1.547745, validation/accuracy=0.590280, validation/loss=1.740151, validation/num_examples=50000
I0201 20:48:34.024407 140023005427456 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.0301692485809326, loss=2.610633373260498
I0201 20:49:17.052838 140022518892288 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.6616168022155762, loss=3.069551944732666
I0201 20:50:03.519356 140023005427456 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.6477762460708618, loss=2.741683006286621
I0201 20:50:49.793844 140022518892288 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.7493131160736084, loss=3.090601682662964
I0201 20:51:36.007287 140023005427456 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.780936360359192, loss=2.5038132667541504
I0201 20:52:22.256170 140022518892288 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.4533305168151855, loss=4.497800827026367
I0201 20:53:08.568428 140023005427456 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.427588701248169, loss=3.8416860103607178
I0201 20:53:54.810616 140022518892288 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.645448923110962, loss=2.985224962234497
I0201 20:54:41.145971 140023005427456 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.472105622291565, loss=5.137459754943848
I0201 20:55:23.806875 140184451094336 spec.py:321] Evaluating on the training split.
I0201 20:55:34.271821 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 20:56:02.023703 140184451094336 spec.py:349] Evaluating on the test split.
I0201 20:56:03.624591 140184451094336 submission_runner.py:408] Time since start: 31179.79s, 	Step: 62494, 	{'train/accuracy': 0.6387304663658142, 'train/loss': 1.5181124210357666, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.735752820968628, 'validation/num_examples': 50000, 'test/accuracy': 0.4692000150680542, 'test/loss': 2.3889834880828857, 'test/num_examples': 10000, 'score': 28606.195390701294, 'total_duration': 31179.79212284088, 'accumulated_submission_time': 28606.195390701294, 'accumulated_eval_time': 2567.8129115104675, 'accumulated_logging_time': 2.54514741897583}
I0201 20:56:03.654517 140022518892288 logging_writer.py:48] [62494] accumulated_eval_time=2567.812912, accumulated_logging_time=2.545147, accumulated_submission_time=28606.195391, global_step=62494, preemption_count=0, score=28606.195391, test/accuracy=0.469200, test/loss=2.388983, test/num_examples=10000, total_duration=31179.792123, train/accuracy=0.638730, train/loss=1.518112, validation/accuracy=0.588980, validation/loss=1.735753, validation/num_examples=50000
I0201 20:56:06.458434 140023005427456 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.6906907558441162, loss=2.501865863800049
I0201 20:56:48.388380 140022518892288 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.7119275331497192, loss=2.9192872047424316
I0201 20:57:34.632694 140023005427456 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.5393269062042236, loss=3.7966952323913574
I0201 20:58:21.106054 140022518892288 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.8070043325424194, loss=2.8059191703796387
I0201 20:59:07.318555 140023005427456 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.7624447345733643, loss=2.490203380584717
I0201 20:59:53.376403 140022518892288 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.4419866800308228, loss=4.81578254699707
I0201 21:00:39.599783 140023005427456 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.029700517654419, loss=2.626558303833008
I0201 21:01:25.508759 140022518892288 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.6220061779022217, loss=4.298352241516113
I0201 21:02:12.051645 140023005427456 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.7290067672729492, loss=2.7402536869049072
I0201 21:02:58.156151 140022518892288 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.8156001567840576, loss=2.579957962036133
I0201 21:03:03.955927 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:03:14.554673 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:03:42.777909 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:03:44.379032 140184451094336 submission_runner.py:408] Time since start: 31640.55s, 	Step: 63414, 	{'train/accuracy': 0.6572265625, 'train/loss': 1.420607089996338, 'validation/accuracy': 0.5945599675178528, 'validation/loss': 1.7118746042251587, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.362293004989624, 'test/num_examples': 10000, 'score': 29026.439562797546, 'total_duration': 31640.54657483101, 'accumulated_submission_time': 29026.439562797546, 'accumulated_eval_time': 2608.2360076904297, 'accumulated_logging_time': 2.5849668979644775}
I0201 21:03:44.407731 140023005427456 logging_writer.py:48] [63414] accumulated_eval_time=2608.236008, accumulated_logging_time=2.584967, accumulated_submission_time=29026.439563, global_step=63414, preemption_count=0, score=29026.439563, test/accuracy=0.475000, test/loss=2.362293, test/num_examples=10000, total_duration=31640.546575, train/accuracy=0.657227, train/loss=1.420607, validation/accuracy=0.594560, validation/loss=1.711875, validation/num_examples=50000
I0201 21:04:20.187988 140022518892288 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.868316888809204, loss=2.5587544441223145
I0201 21:05:06.081348 140023005427456 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.782668948173523, loss=2.4193332195281982
I0201 21:05:52.412595 140022518892288 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.924699306488037, loss=2.5569705963134766
I0201 21:06:39.036799 140023005427456 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.8748427629470825, loss=2.771751880645752
I0201 21:07:25.247331 140022518892288 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.0854876041412354, loss=2.5642948150634766
I0201 21:08:11.410792 140023005427456 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.8926383256912231, loss=2.5790486335754395
I0201 21:08:57.460201 140022518892288 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.592725157737732, loss=3.6016592979431152
I0201 21:09:43.573835 140023005427456 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.764966607093811, loss=4.035116672515869
I0201 21:10:29.945755 140022518892288 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.7517913579940796, loss=3.2374801635742188
I0201 21:10:44.803966 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:10:55.176656 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:11:22.137589 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:11:23.755702 140184451094336 submission_runner.py:408] Time since start: 32099.92s, 	Step: 64334, 	{'train/accuracy': 0.6352148056030273, 'train/loss': 1.5222920179367065, 'validation/accuracy': 0.5979799628257751, 'validation/loss': 1.6968483924865723, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.3335988521575928, 'test/num_examples': 10000, 'score': 29446.779339313507, 'total_duration': 32099.923241853714, 'accumulated_submission_time': 29446.779339313507, 'accumulated_eval_time': 2647.1877439022064, 'accumulated_logging_time': 2.6227519512176514}
I0201 21:11:23.780398 140023005427456 logging_writer.py:48] [64334] accumulated_eval_time=2647.187744, accumulated_logging_time=2.622752, accumulated_submission_time=29446.779339, global_step=64334, preemption_count=0, score=29446.779339, test/accuracy=0.480600, test/loss=2.333599, test/num_examples=10000, total_duration=32099.923242, train/accuracy=0.635215, train/loss=1.522292, validation/accuracy=0.597980, validation/loss=1.696848, validation/num_examples=50000
I0201 21:11:50.569460 140022518892288 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8891373872756958, loss=2.620227575302124
I0201 21:12:35.880081 140023005427456 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.8440022468566895, loss=2.467196464538574
I0201 21:13:21.918623 140022518892288 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.5651673078536987, loss=4.456795692443848
I0201 21:14:08.274936 140023005427456 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.45710289478302, loss=3.5264029502868652
I0201 21:14:54.295844 140022518892288 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.8177075386047363, loss=2.7562358379364014
I0201 21:15:40.246791 140023005427456 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.8085886240005493, loss=2.5289053916931152
I0201 21:16:26.336210 140022518892288 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.863854169845581, loss=2.556210994720459
I0201 21:17:12.742844 140023005427456 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.7175391912460327, loss=3.3153247833251953
I0201 21:17:58.547777 140022518892288 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8166530132293701, loss=2.269949436187744
I0201 21:18:24.119826 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:18:34.548261 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:18:59.902923 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:19:01.505348 140184451094336 submission_runner.py:408] Time since start: 32557.67s, 	Step: 65257, 	{'train/accuracy': 0.6451562643051147, 'train/loss': 1.4754486083984375, 'validation/accuracy': 0.5941999554634094, 'validation/loss': 1.7001410722732544, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.3748252391815186, 'test/num_examples': 10000, 'score': 29867.059475898743, 'total_duration': 32557.672868967056, 'accumulated_submission_time': 29867.059475898743, 'accumulated_eval_time': 2684.573234319687, 'accumulated_logging_time': 2.6589736938476562}
I0201 21:19:01.534020 140023005427456 logging_writer.py:48] [65257] accumulated_eval_time=2684.573234, accumulated_logging_time=2.658974, accumulated_submission_time=29867.059476, global_step=65257, preemption_count=0, score=29867.059476, test/accuracy=0.475700, test/loss=2.374825, test/num_examples=10000, total_duration=32557.672869, train/accuracy=0.645156, train/loss=1.475449, validation/accuracy=0.594200, validation/loss=1.700141, validation/num_examples=50000
I0201 21:19:19.147209 140022518892288 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.6677842140197754, loss=3.554497480392456
I0201 21:20:03.186523 140023005427456 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.004676103591919, loss=2.649583339691162
I0201 21:20:49.422359 140022518892288 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.4536622762680054, loss=4.224652290344238
I0201 21:21:36.099449 140023005427456 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.095285654067993, loss=2.6751863956451416
I0201 21:22:21.961119 140022518892288 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.06874942779541, loss=2.656848192214966
I0201 21:23:08.372288 140023005427456 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.085632801055908, loss=2.6183042526245117
I0201 21:23:54.392581 140022518892288 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.5775362253189087, loss=2.878664970397949
I0201 21:24:40.571643 140023005427456 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.65083646774292, loss=3.186650276184082
I0201 21:25:26.573158 140022518892288 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.7054452896118164, loss=5.185794830322266
I0201 21:26:01.829808 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:26:12.131030 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:26:38.471548 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:26:40.067589 140184451094336 submission_runner.py:408] Time since start: 33016.24s, 	Step: 66178, 	{'train/accuracy': 0.6478906273841858, 'train/loss': 1.469024658203125, 'validation/accuracy': 0.5952199697494507, 'validation/loss': 1.713046669960022, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.354539155960083, 'test/num_examples': 10000, 'score': 30287.294951438904, 'total_duration': 33016.23513197899, 'accumulated_submission_time': 30287.294951438904, 'accumulated_eval_time': 2722.811019182205, 'accumulated_logging_time': 2.700793743133545}
I0201 21:26:40.092723 140023005427456 logging_writer.py:48] [66178] accumulated_eval_time=2722.811019, accumulated_logging_time=2.700794, accumulated_submission_time=30287.294951, global_step=66178, preemption_count=0, score=30287.294951, test/accuracy=0.480800, test/loss=2.354539, test/num_examples=10000, total_duration=33016.235132, train/accuracy=0.647891, train/loss=1.469025, validation/accuracy=0.595220, validation/loss=1.713047, validation/num_examples=50000
I0201 21:26:49.315301 140022518892288 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.5618301630020142, loss=4.526864051818848
I0201 21:27:32.203402 140023005427456 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.8679386377334595, loss=2.810002088546753
I0201 21:28:18.068434 140022518892288 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.607277274131775, loss=4.173776626586914
I0201 21:29:04.361395 140023005427456 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.772581696510315, loss=4.413625717163086
I0201 21:29:50.221610 140022518892288 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.782586932182312, loss=2.5070199966430664
I0201 21:30:36.333070 140023005427456 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.751404047012329, loss=2.8712856769561768
I0201 21:31:22.698591 140022518892288 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.6844439506530762, loss=2.9551146030426025
I0201 21:32:09.048940 140023005427456 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.5699740648269653, loss=4.58748197555542
I0201 21:32:55.251813 140022518892288 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.8599735498428345, loss=2.691716432571411
I0201 21:33:40.154512 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:33:50.474764 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:34:20.704146 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:34:22.298447 140184451094336 submission_runner.py:408] Time since start: 33478.47s, 	Step: 67099, 	{'train/accuracy': 0.6410937309265137, 'train/loss': 1.491320013999939, 'validation/accuracy': 0.5977799892425537, 'validation/loss': 1.6954699754714966, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.3312244415283203, 'test/num_examples': 10000, 'score': 30707.293865442276, 'total_duration': 33478.465988874435, 'accumulated_submission_time': 30707.293865442276, 'accumulated_eval_time': 2764.954957485199, 'accumulated_logging_time': 2.7407922744750977}
I0201 21:34:22.327570 140023005427456 logging_writer.py:48] [67099] accumulated_eval_time=2764.954957, accumulated_logging_time=2.740792, accumulated_submission_time=30707.293865, global_step=67099, preemption_count=0, score=30707.293865, test/accuracy=0.484300, test/loss=2.331224, test/num_examples=10000, total_duration=33478.465989, train/accuracy=0.641094, train/loss=1.491320, validation/accuracy=0.597780, validation/loss=1.695470, validation/num_examples=50000
I0201 21:34:23.134826 140022518892288 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.5696812868118286, loss=3.7386834621429443
I0201 21:35:04.769852 140023005427456 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.6688517332077026, loss=3.1506218910217285
I0201 21:35:50.584743 140022518892288 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.5022895336151123, loss=4.71535062789917
I0201 21:36:36.890837 140023005427456 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.4725029468536377, loss=4.362476825714111
I0201 21:37:23.070778 140022518892288 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.8519948720932007, loss=2.5330119132995605
I0201 21:38:09.600863 140023005427456 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.6279486417770386, loss=3.222926616668701
I0201 21:38:55.685051 140022518892288 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0839176177978516, loss=2.455874443054199
I0201 21:39:41.690172 140023005427456 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.069430112838745, loss=2.550215482711792
I0201 21:40:27.896716 140022518892288 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.814713954925537, loss=4.87541389465332
I0201 21:41:13.948823 140023005427456 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.917701005935669, loss=2.479818820953369
I0201 21:41:22.323923 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:41:33.027476 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:41:56.401429 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:41:58.002395 140184451094336 submission_runner.py:408] Time since start: 33934.17s, 	Step: 68020, 	{'train/accuracy': 0.6380273103713989, 'train/loss': 1.4931342601776123, 'validation/accuracy': 0.5981599688529968, 'validation/loss': 1.6907784938812256, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.350700855255127, 'test/num_examples': 10000, 'score': 31127.232821702957, 'total_duration': 33934.169929265976, 'accumulated_submission_time': 31127.232821702957, 'accumulated_eval_time': 2800.633416414261, 'accumulated_logging_time': 2.7796566486358643}
I0201 21:41:58.028647 140022518892288 logging_writer.py:48] [68020] accumulated_eval_time=2800.633416, accumulated_logging_time=2.779657, accumulated_submission_time=31127.232822, global_step=68020, preemption_count=0, score=31127.232822, test/accuracy=0.478600, test/loss=2.350701, test/num_examples=10000, total_duration=33934.169929, train/accuracy=0.638027, train/loss=1.493134, validation/accuracy=0.598160, validation/loss=1.690778, validation/num_examples=50000
I0201 21:42:30.568178 140023005427456 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.006955862045288, loss=2.571286678314209
I0201 21:43:16.696044 140022518892288 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.8214070796966553, loss=2.8221893310546875
I0201 21:44:02.878583 140023005427456 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.9780395030975342, loss=2.6384530067443848
I0201 21:44:48.985136 140022518892288 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.9669146537780762, loss=2.5713770389556885
I0201 21:45:35.190974 140023005427456 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9874707460403442, loss=2.7405171394348145
I0201 21:46:21.488610 140022518892288 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.7152528762817383, loss=3.195145845413208
I0201 21:47:07.634588 140023005427456 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.828075647354126, loss=2.4545674324035645
I0201 21:47:53.553814 140022518892288 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0807833671569824, loss=2.5445144176483154
I0201 21:48:39.837566 140023005427456 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.911683440208435, loss=2.509246349334717
I0201 21:48:58.347991 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:49:08.856036 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:49:38.068875 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:49:39.670578 140184451094336 submission_runner.py:408] Time since start: 34395.84s, 	Step: 68942, 	{'train/accuracy': 0.6507812142372131, 'train/loss': 1.4588117599487305, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6879316568374634, 'validation/num_examples': 50000, 'test/accuracy': 0.4767000079154968, 'test/loss': 2.359503746032715, 'test/num_examples': 10000, 'score': 31547.49372458458, 'total_duration': 34395.838116168976, 'accumulated_submission_time': 31547.49372458458, 'accumulated_eval_time': 2841.955982208252, 'accumulated_logging_time': 2.814785957336426}
I0201 21:49:39.696963 140022518892288 logging_writer.py:48] [68942] accumulated_eval_time=2841.955982, accumulated_logging_time=2.814786, accumulated_submission_time=31547.493725, global_step=68942, preemption_count=0, score=31547.493725, test/accuracy=0.476700, test/loss=2.359504, test/num_examples=10000, total_duration=34395.838116, train/accuracy=0.650781, train/loss=1.458812, validation/accuracy=0.599040, validation/loss=1.687932, validation/num_examples=50000
I0201 21:50:03.293519 140023005427456 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.5426039695739746, loss=4.202791213989258
I0201 21:50:48.452822 140022518892288 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.9703670740127563, loss=2.652167797088623
I0201 21:51:34.489440 140023005427456 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.6774067878723145, loss=2.803694486618042
I0201 21:52:20.777454 140022518892288 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.3618967533111572, loss=3.993412971496582
I0201 21:53:07.012458 140023005427456 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.9549977779388428, loss=2.518326759338379
I0201 21:53:53.127009 140022518892288 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.704408884048462, loss=2.654102087020874
I0201 21:54:39.164496 140023005427456 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.5215425491333008, loss=4.151017189025879
I0201 21:55:25.168068 140022518892288 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.1727511882781982, loss=2.612927198410034
I0201 21:56:11.294113 140023005427456 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.027240514755249, loss=2.616295099258423
I0201 21:56:39.905867 140184451094336 spec.py:321] Evaluating on the training split.
I0201 21:56:50.359679 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 21:57:17.453616 140184451094336 spec.py:349] Evaluating on the test split.
I0201 21:57:19.054240 140184451094336 submission_runner.py:408] Time since start: 34855.22s, 	Step: 69864, 	{'train/accuracy': 0.654589831829071, 'train/loss': 1.434368371963501, 'validation/accuracy': 0.6043199896812439, 'validation/loss': 1.666023850440979, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.3192484378814697, 'test/num_examples': 10000, 'score': 31967.64450263977, 'total_duration': 34855.22176671028, 'accumulated_submission_time': 31967.64450263977, 'accumulated_eval_time': 2881.1043269634247, 'accumulated_logging_time': 2.851184606552124}
I0201 21:57:19.079637 140022518892288 logging_writer.py:48] [69864] accumulated_eval_time=2881.104327, accumulated_logging_time=2.851185, accumulated_submission_time=31967.644503, global_step=69864, preemption_count=0, score=31967.644503, test/accuracy=0.482200, test/loss=2.319248, test/num_examples=10000, total_duration=34855.221767, train/accuracy=0.654590, train/loss=1.434368, validation/accuracy=0.604320, validation/loss=1.666024, validation/num_examples=50000
I0201 21:57:33.879336 140023005427456 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.6566535234451294, loss=2.7144908905029297
I0201 21:58:17.680399 140022518892288 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.7709914445877075, loss=2.969176769256592
I0201 21:59:03.991415 140023005427456 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.037851095199585, loss=2.664299964904785
I0201 21:59:50.223434 140022518892288 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.1154189109802246, loss=2.7129080295562744
I0201 22:00:36.397445 140023005427456 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.7530663013458252, loss=2.894636392593384
I0201 22:01:22.747760 140022518892288 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.4808242321014404, loss=3.9131059646606445
I0201 22:02:09.078034 140023005427456 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.7929322719573975, loss=2.5721020698547363
I0201 22:02:55.146138 140022518892288 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.617810606956482, loss=3.4424667358398438
I0201 22:03:41.630672 140023005427456 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.934761643409729, loss=2.4258508682250977
I0201 22:04:19.421629 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:04:29.813532 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:04:48.639330 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:04:50.246012 140184451094336 submission_runner.py:408] Time since start: 35306.41s, 	Step: 70784, 	{'train/accuracy': 0.6522656083106995, 'train/loss': 1.430999517440796, 'validation/accuracy': 0.6041399836540222, 'validation/loss': 1.6534889936447144, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.3093273639678955, 'test/num_examples': 10000, 'score': 32387.930511713028, 'total_duration': 35306.4135351181, 'accumulated_submission_time': 32387.930511713028, 'accumulated_eval_time': 2911.928690671921, 'accumulated_logging_time': 2.8854973316192627}
I0201 22:04:50.280849 140022518892288 logging_writer.py:48] [70784] accumulated_eval_time=2911.928691, accumulated_logging_time=2.885497, accumulated_submission_time=32387.930512, global_step=70784, preemption_count=0, score=32387.930512, test/accuracy=0.490200, test/loss=2.309327, test/num_examples=10000, total_duration=35306.413535, train/accuracy=0.652266, train/loss=1.431000, validation/accuracy=0.604140, validation/loss=1.653489, validation/num_examples=50000
I0201 22:04:57.084915 140023005427456 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.8862224817276, loss=2.517003297805786
I0201 22:05:39.586879 140022518892288 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.7395391464233398, loss=2.4831559658050537
I0201 22:06:25.902940 140023005427456 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.859843373298645, loss=2.4890143871307373
I0201 22:07:12.422758 140022518892288 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.0369043350219727, loss=2.3844075202941895
I0201 22:07:58.452974 140023005427456 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.6638072729110718, loss=3.491034507751465
I0201 22:08:44.647613 140022518892288 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.8513062000274658, loss=2.51457142829895
I0201 22:09:30.859504 140023005427456 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.834707498550415, loss=2.5216317176818848
I0201 22:10:16.830816 140022518892288 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.6166177988052368, loss=3.147541046142578
I0201 22:11:02.996274 140023005427456 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.2084836959838867, loss=2.530545949935913
I0201 22:11:48.946664 140022518892288 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.9462242126464844, loss=2.43377685546875
I0201 22:11:50.556041 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:12:01.047424 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:12:24.572553 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:12:26.176442 140184451094336 submission_runner.py:408] Time since start: 35762.34s, 	Step: 71705, 	{'train/accuracy': 0.657031238079071, 'train/loss': 1.4209951162338257, 'validation/accuracy': 0.6064199805259705, 'validation/loss': 1.6615710258483887, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.3075990676879883, 'test/num_examples': 10000, 'score': 32808.145381212234, 'total_duration': 35762.34394454956, 'accumulated_submission_time': 32808.145381212234, 'accumulated_eval_time': 2947.54905629158, 'accumulated_logging_time': 2.9329519271850586}
I0201 22:12:26.206874 140023005427456 logging_writer.py:48] [71705] accumulated_eval_time=2947.549056, accumulated_logging_time=2.932952, accumulated_submission_time=32808.145381, global_step=71705, preemption_count=0, score=32808.145381, test/accuracy=0.488600, test/loss=2.307599, test/num_examples=10000, total_duration=35762.343945, train/accuracy=0.657031, train/loss=1.420995, validation/accuracy=0.606420, validation/loss=1.661571, validation/num_examples=50000
I0201 22:13:05.667384 140022518892288 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.706658959388733, loss=3.121519088745117
I0201 22:13:51.616488 140023005427456 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.7957631349563599, loss=2.7251060009002686
I0201 22:14:38.080062 140022518892288 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4683549404144287, loss=4.596314430236816
I0201 22:15:24.078558 140023005427456 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.826175332069397, loss=2.51168155670166
I0201 22:16:10.401983 140022518892288 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.9639188051223755, loss=2.5202319622039795
I0201 22:16:56.757274 140023005427456 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.6630804538726807, loss=3.305661678314209
I0201 22:17:42.953973 140022518892288 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.7736815214157104, loss=3.3217430114746094
I0201 22:18:29.302655 140023005427456 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.6494317054748535, loss=4.113864898681641
I0201 22:19:15.572263 140022518892288 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.1213347911834717, loss=2.6129989624023438
I0201 22:19:26.426010 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:19:37.712387 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:20:01.485633 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:20:03.080405 140184451094336 submission_runner.py:408] Time since start: 36219.25s, 	Step: 72625, 	{'train/accuracy': 0.6685937643051147, 'train/loss': 1.38480544090271, 'validation/accuracy': 0.6049599647521973, 'validation/loss': 1.6850557327270508, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3566348552703857, 'test/num_examples': 10000, 'score': 33228.30744147301, 'total_duration': 36219.24794006348, 'accumulated_submission_time': 33228.30744147301, 'accumulated_eval_time': 2984.2034389972687, 'accumulated_logging_time': 2.973165988922119}
I0201 22:20:03.111214 140023005427456 logging_writer.py:48] [72625] accumulated_eval_time=2984.203439, accumulated_logging_time=2.973166, accumulated_submission_time=33228.307441, global_step=72625, preemption_count=0, score=33228.307441, test/accuracy=0.483400, test/loss=2.356635, test/num_examples=10000, total_duration=36219.247940, train/accuracy=0.668594, train/loss=1.384805, validation/accuracy=0.604960, validation/loss=1.685056, validation/num_examples=50000
I0201 22:20:33.506762 140022518892288 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.8775172233581543, loss=3.089308261871338
I0201 22:21:19.262477 140023005427456 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.8928892612457275, loss=2.305182933807373
I0201 22:22:05.699629 140022518892288 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.9969385862350464, loss=2.4233860969543457
I0201 22:22:51.974944 140023005427456 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.11277174949646, loss=2.4243180751800537
I0201 22:23:38.296350 140022518892288 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.7989493608474731, loss=2.9792487621307373
I0201 22:24:24.765613 140023005427456 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.4856544733047485, loss=4.751699924468994
I0201 22:25:10.888222 140022518892288 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.5011687278747559, loss=3.807539463043213
I0201 22:25:57.312606 140023005427456 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.6503149271011353, loss=3.951913356781006
I0201 22:26:43.656022 140022518892288 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.6817097663879395, loss=3.114393711090088
I0201 22:27:03.239655 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:27:13.854021 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:27:40.914841 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:27:42.522594 140184451094336 submission_runner.py:408] Time since start: 36678.69s, 	Step: 73544, 	{'train/accuracy': 0.6450976133346558, 'train/loss': 1.4667530059814453, 'validation/accuracy': 0.6068800091743469, 'validation/loss': 1.662022590637207, 'validation/num_examples': 50000, 'test/accuracy': 0.4855000376701355, 'test/loss': 2.3125555515289307, 'test/num_examples': 10000, 'score': 33648.37844085693, 'total_duration': 36678.69011569023, 'accumulated_submission_time': 33648.37844085693, 'accumulated_eval_time': 3023.486344099045, 'accumulated_logging_time': 3.0142745971679688}
I0201 22:27:42.551452 140023005427456 logging_writer.py:48] [73544] accumulated_eval_time=3023.486344, accumulated_logging_time=3.014275, accumulated_submission_time=33648.378441, global_step=73544, preemption_count=0, score=33648.378441, test/accuracy=0.485500, test/loss=2.312556, test/num_examples=10000, total_duration=36678.690116, train/accuracy=0.645098, train/loss=1.466753, validation/accuracy=0.606880, validation/loss=1.662023, validation/num_examples=50000
I0201 22:28:05.345093 140022518892288 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.7165040969848633, loss=4.352778911590576
I0201 22:28:50.355579 140023005427456 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0452373027801514, loss=2.5836524963378906
I0201 22:29:36.633887 140022518892288 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.0831868648529053, loss=2.58616304397583
I0201 22:30:22.848766 140023005427456 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.5053293704986572, loss=4.317924499511719
I0201 22:31:08.902622 140022518892288 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.039940118789673, loss=2.4945480823516846
I0201 22:31:55.229517 140023005427456 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.491960048675537, loss=3.9409666061401367
I0201 22:32:41.263248 140022518892288 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.8612908124923706, loss=2.735246181488037
I0201 22:33:27.713883 140023005427456 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.5896488428115845, loss=4.583774089813232
I0201 22:34:13.815044 140022518892288 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.6095019578933716, loss=4.241733551025391
I0201 22:34:42.614504 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:34:53.162028 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:35:21.629549 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:35:23.234327 140184451094336 submission_runner.py:408] Time since start: 37139.40s, 	Step: 74464, 	{'train/accuracy': 0.6503515243530273, 'train/loss': 1.4331283569335938, 'validation/accuracy': 0.6075199842453003, 'validation/loss': 1.6410701274871826, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.3070414066314697, 'test/num_examples': 10000, 'score': 34068.383913517, 'total_duration': 37139.40186858177, 'accumulated_submission_time': 34068.383913517, 'accumulated_eval_time': 3064.1061642169952, 'accumulated_logging_time': 3.052863836288452}
I0201 22:35:23.266160 140023005427456 logging_writer.py:48] [74464] accumulated_eval_time=3064.106164, accumulated_logging_time=3.052864, accumulated_submission_time=34068.383914, global_step=74464, preemption_count=0, score=34068.383914, test/accuracy=0.484500, test/loss=2.307041, test/num_examples=10000, total_duration=37139.401869, train/accuracy=0.650352, train/loss=1.433128, validation/accuracy=0.607520, validation/loss=1.641070, validation/num_examples=50000
I0201 22:35:38.079270 140022518892288 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.2577645778656006, loss=2.5497658252716064
I0201 22:36:21.495516 140023005427456 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.9677129983901978, loss=2.588742971420288
I0201 22:37:07.630332 140022518892288 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.288564920425415, loss=2.6270785331726074
I0201 22:37:54.024450 140023005427456 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.3973350524902344, loss=4.578180313110352
I0201 22:38:40.055429 140022518892288 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.721509337425232, loss=2.8330881595611572
I0201 22:39:26.176163 140023005427456 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.5081251859664917, loss=5.168610572814941
I0201 22:40:12.472917 140022518892288 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.0085461139678955, loss=2.3771615028381348
I0201 22:40:58.233643 140023005427456 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.0065834522247314, loss=2.33303165435791
I0201 22:41:44.454114 140022518892288 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.9939790964126587, loss=2.518270969390869
I0201 22:42:23.546152 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:42:33.943906 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:42:57.576525 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:42:59.182101 140184451094336 submission_runner.py:408] Time since start: 37595.35s, 	Step: 75386, 	{'train/accuracy': 0.6625195145606995, 'train/loss': 1.3859827518463135, 'validation/accuracy': 0.6066799759864807, 'validation/loss': 1.6424832344055176, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.278662919998169, 'test/num_examples': 10000, 'score': 34488.60297369957, 'total_duration': 37595.3496427536, 'accumulated_submission_time': 34488.60297369957, 'accumulated_eval_time': 3099.742102622986, 'accumulated_logging_time': 3.097740411758423}
I0201 22:42:59.213015 140023005427456 logging_writer.py:48] [75386] accumulated_eval_time=3099.742103, accumulated_logging_time=3.097740, accumulated_submission_time=34488.602974, global_step=75386, preemption_count=0, score=34488.602974, test/accuracy=0.487800, test/loss=2.278663, test/num_examples=10000, total_duration=37595.349643, train/accuracy=0.662520, train/loss=1.385983, validation/accuracy=0.606680, validation/loss=1.642483, validation/num_examples=50000
I0201 22:43:05.224052 140022518892288 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.7590744495391846, loss=2.858761787414551
I0201 22:43:47.584471 140023005427456 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.9159510135650635, loss=2.772841453552246
I0201 22:44:33.794301 140022518892288 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.8240256309509277, loss=2.50913405418396
I0201 22:45:20.264298 140023005427456 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.7089778184890747, loss=3.2561898231506348
I0201 22:46:06.459334 140022518892288 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.7650620937347412, loss=2.4140076637268066
I0201 22:46:52.448118 140023005427456 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.7733837366104126, loss=3.7638916969299316
I0201 22:47:38.822293 140022518892288 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.7736377716064453, loss=4.590227127075195
I0201 22:48:25.083119 140023005427456 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.5613253116607666, loss=3.635014533996582
I0201 22:49:11.242467 140022518892288 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.9070667028427124, loss=2.3659567832946777
I0201 22:49:57.098171 140023005427456 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.9533796310424805, loss=4.6952643394470215
I0201 22:49:59.312225 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:50:09.678813 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:50:39.393096 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:50:41.000182 140184451094336 submission_runner.py:408] Time since start: 38057.17s, 	Step: 76306, 	{'train/accuracy': 0.6537500023841858, 'train/loss': 1.4322412014007568, 'validation/accuracy': 0.6104399561882019, 'validation/loss': 1.6254669427871704, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.2743124961853027, 'test/num_examples': 10000, 'score': 34908.645033836365, 'total_duration': 38057.167717695236, 'accumulated_submission_time': 34908.645033836365, 'accumulated_eval_time': 3141.430042743683, 'accumulated_logging_time': 3.137625217437744}
I0201 22:50:41.030143 140022518892288 logging_writer.py:48] [76306] accumulated_eval_time=3141.430043, accumulated_logging_time=3.137625, accumulated_submission_time=34908.645034, global_step=76306, preemption_count=0, score=34908.645034, test/accuracy=0.495800, test/loss=2.274312, test/num_examples=10000, total_duration=38057.167718, train/accuracy=0.653750, train/loss=1.432241, validation/accuracy=0.610440, validation/loss=1.625467, validation/num_examples=50000
I0201 22:51:20.378046 140023005427456 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.6752393245697021, loss=5.134469032287598
I0201 22:52:06.566799 140022518892288 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.9201339483261108, loss=2.8620340824127197
I0201 22:52:52.813995 140023005427456 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.8916200399398804, loss=5.08673095703125
I0201 22:53:39.086719 140022518892288 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.7046846151351929, loss=3.7448081970214844
I0201 22:54:25.321034 140023005427456 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.2174324989318848, loss=2.5140795707702637
I0201 22:55:11.251959 140022518892288 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.9572712182998657, loss=2.8874847888946533
I0201 22:55:57.220643 140023005427456 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.9266709089279175, loss=2.460300922393799
I0201 22:56:43.408824 140022518892288 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.9008029699325562, loss=2.3279943466186523
I0201 22:57:29.367830 140023005427456 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.9942498207092285, loss=2.3915112018585205
I0201 22:57:41.023538 140184451094336 spec.py:321] Evaluating on the training split.
I0201 22:57:51.630688 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 22:58:16.318267 140184451094336 spec.py:349] Evaluating on the test split.
I0201 22:58:17.920788 140184451094336 submission_runner.py:408] Time since start: 38514.09s, 	Step: 77227, 	{'train/accuracy': 0.6592382788658142, 'train/loss': 1.3970091342926025, 'validation/accuracy': 0.615399956703186, 'validation/loss': 1.60317063331604, 'validation/num_examples': 50000, 'test/accuracy': 0.48490002751350403, 'test/loss': 2.277125358581543, 'test/num_examples': 10000, 'score': 35328.58205103874, 'total_duration': 38514.088331222534, 'accumulated_submission_time': 35328.58205103874, 'accumulated_eval_time': 3178.3272848129272, 'accumulated_logging_time': 3.1766517162323}
I0201 22:58:17.947492 140022518892288 logging_writer.py:48] [77227] accumulated_eval_time=3178.327285, accumulated_logging_time=3.176652, accumulated_submission_time=35328.582051, global_step=77227, preemption_count=0, score=35328.582051, test/accuracy=0.484900, test/loss=2.277125, test/num_examples=10000, total_duration=38514.088331, train/accuracy=0.659238, train/loss=1.397009, validation/accuracy=0.615400, validation/loss=1.603171, validation/num_examples=50000
I0201 22:58:47.542265 140023005427456 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.1579716205596924, loss=2.560810089111328
I0201 22:59:33.164417 140022518892288 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.6902120113372803, loss=4.566802978515625
I0201 23:00:19.502517 140023005427456 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.0071048736572266, loss=2.5401456356048584
I0201 23:01:05.775208 140022518892288 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.9768524169921875, loss=2.5738537311553955
I0201 23:01:51.928325 140023005427456 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.6100695133209229, loss=4.813218116760254
I0201 23:02:38.074041 140022518892288 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.7578742504119873, loss=4.915032386779785
I0201 23:03:24.076604 140023005427456 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.7375301122665405, loss=4.418569564819336
I0201 23:04:09.996655 140022518892288 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.538901448249817, loss=5.139377593994141
I0201 23:04:55.886558 140023005427456 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.1638033390045166, loss=2.3559327125549316
I0201 23:05:18.273119 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:05:28.628206 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:05:58.393400 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:06:00.002095 140184451094336 submission_runner.py:408] Time since start: 38976.17s, 	Step: 78150, 	{'train/accuracy': 0.6634374856948853, 'train/loss': 1.390366792678833, 'validation/accuracy': 0.6144999861717224, 'validation/loss': 1.6266072988510132, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.281214952468872, 'test/num_examples': 10000, 'score': 35748.85032916069, 'total_duration': 38976.1696164608, 'accumulated_submission_time': 35748.85032916069, 'accumulated_eval_time': 3220.056258201599, 'accumulated_logging_time': 3.212582588195801}
I0201 23:06:00.038596 140022518892288 logging_writer.py:48] [78150] accumulated_eval_time=3220.056258, accumulated_logging_time=3.212583, accumulated_submission_time=35748.850329, global_step=78150, preemption_count=0, score=35748.850329, test/accuracy=0.493300, test/loss=2.281215, test/num_examples=10000, total_duration=38976.169616, train/accuracy=0.663437, train/loss=1.390367, validation/accuracy=0.614500, validation/loss=1.626607, validation/num_examples=50000
I0201 23:06:20.447468 140023005427456 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.913795828819275, loss=2.4654226303100586
I0201 23:07:05.030538 140022518892288 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.5704964399337769, loss=3.6144940853118896
I0201 23:07:51.025085 140023005427456 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.811445951461792, loss=4.933663845062256
I0201 23:08:37.220362 140022518892288 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.9410954713821411, loss=2.506439685821533
I0201 23:09:23.151121 140023005427456 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.805810570716858, loss=2.7899317741394043
I0201 23:10:09.346024 140022518892288 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.64297616481781, loss=2.9653632640838623
I0201 23:10:55.519538 140023005427456 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.090867519378662, loss=2.3239238262176514
I0201 23:11:41.951530 140022518892288 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.2113614082336426, loss=2.546265125274658
I0201 23:12:28.022774 140023005427456 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.8858343362808228, loss=2.4191462993621826
I0201 23:13:00.256481 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:13:10.895231 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:13:37.855132 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:13:39.450751 140184451094336 submission_runner.py:408] Time since start: 39435.62s, 	Step: 79071, 	{'train/accuracy': 0.6562694907188416, 'train/loss': 1.445751428604126, 'validation/accuracy': 0.614139974117279, 'validation/loss': 1.6321967840194702, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.2790701389312744, 'test/num_examples': 10000, 'score': 36169.0088224411, 'total_duration': 39435.618294239044, 'accumulated_submission_time': 36169.0088224411, 'accumulated_eval_time': 3259.250541448593, 'accumulated_logging_time': 3.260847330093384}
I0201 23:13:39.480952 140022518892288 logging_writer.py:48] [79071] accumulated_eval_time=3259.250541, accumulated_logging_time=3.260847, accumulated_submission_time=36169.008822, global_step=79071, preemption_count=0, score=36169.008822, test/accuracy=0.488500, test/loss=2.279070, test/num_examples=10000, total_duration=39435.618294, train/accuracy=0.656269, train/loss=1.445751, validation/accuracy=0.614140, validation/loss=1.632197, validation/num_examples=50000
I0201 23:13:51.481263 140023005427456 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.887391209602356, loss=2.521522045135498
I0201 23:14:34.621789 140022518892288 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.9302105903625488, loss=2.6020731925964355
I0201 23:15:20.913820 140023005427456 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.7950977087020874, loss=2.4048149585723877
I0201 23:16:07.543058 140022518892288 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.5749510526657104, loss=3.747339963912964
I0201 23:16:53.439798 140023005427456 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.8945379257202148, loss=3.698596715927124
I0201 23:17:39.606451 140022518892288 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.8884245157241821, loss=2.46236252784729
I0201 23:18:25.821947 140023005427456 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.1672253608703613, loss=2.4667723178863525
I0201 23:19:11.902539 140022518892288 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.453733205795288, loss=4.962646484375
I0201 23:19:58.120597 140023005427456 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.0588104724884033, loss=2.5164315700531006
I0201 23:20:39.763701 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:20:50.194608 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:21:13.709005 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:21:15.318331 140184451094336 submission_runner.py:408] Time since start: 39891.49s, 	Step: 79992, 	{'train/accuracy': 0.6576757431030273, 'train/loss': 1.4273301362991333, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.6328654289245605, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.2808785438537598, 'test/num_examples': 10000, 'score': 36589.23210167885, 'total_duration': 39891.485827207565, 'accumulated_submission_time': 36589.23210167885, 'accumulated_eval_time': 3294.8051438331604, 'accumulated_logging_time': 3.3025565147399902}
I0201 23:21:15.352758 140022518892288 logging_writer.py:48] [79992] accumulated_eval_time=3294.805144, accumulated_logging_time=3.302557, accumulated_submission_time=36589.232102, global_step=79992, preemption_count=0, score=36589.232102, test/accuracy=0.492900, test/loss=2.280879, test/num_examples=10000, total_duration=39891.485827, train/accuracy=0.657676, train/loss=1.427330, validation/accuracy=0.612680, validation/loss=1.632865, validation/num_examples=50000
I0201 23:21:18.962517 140023005427456 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.7315703630447388, loss=5.014069557189941
I0201 23:22:01.086657 140022518892288 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.8658794164657593, loss=2.3083953857421875
I0201 23:22:46.933117 140023005427456 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.6163300275802612, loss=5.026559829711914
I0201 23:23:33.391417 140022518892288 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.1006157398223877, loss=2.5180156230926514
I0201 23:24:19.332280 140023005427456 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.9066710472106934, loss=2.393994092941284
I0201 23:25:05.785314 140022518892288 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.061929702758789, loss=2.4234442710876465
I0201 23:25:51.890269 140023005427456 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.618162751197815, loss=4.057301998138428
I0201 23:26:37.879563 140022518892288 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.9628634452819824, loss=2.6905100345611572
I0201 23:27:24.229744 140023005427456 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.074678897857666, loss=2.490454912185669
I0201 23:28:10.328784 140022518892288 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.9399091005325317, loss=2.5216846466064453
I0201 23:28:15.533560 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:28:26.017637 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:28:53.519202 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:28:55.126878 140184451094336 submission_runner.py:408] Time since start: 40351.29s, 	Step: 80913, 	{'train/accuracy': 0.674023449420929, 'train/loss': 1.3578218221664429, 'validation/accuracy': 0.6184200048446655, 'validation/loss': 1.6004390716552734, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.2475414276123047, 'test/num_examples': 10000, 'score': 37009.35448670387, 'total_duration': 40351.29439616203, 'accumulated_submission_time': 37009.35448670387, 'accumulated_eval_time': 3334.398421525955, 'accumulated_logging_time': 3.3474948406219482}
I0201 23:28:55.156901 140023005427456 logging_writer.py:48] [80913] accumulated_eval_time=3334.398422, accumulated_logging_time=3.347495, accumulated_submission_time=37009.354487, global_step=80913, preemption_count=0, score=37009.354487, test/accuracy=0.492700, test/loss=2.247541, test/num_examples=10000, total_duration=40351.294396, train/accuracy=0.674023, train/loss=1.357822, validation/accuracy=0.618420, validation/loss=1.600439, validation/num_examples=50000
I0201 23:29:30.929328 140022518892288 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.9332172870635986, loss=2.7168076038360596
I0201 23:30:16.838478 140023005427456 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.086461305618286, loss=2.4019107818603516
I0201 23:31:03.312694 140022518892288 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.3607239723205566, loss=2.3855488300323486
I0201 23:31:49.653517 140023005427456 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.124450445175171, loss=2.302267074584961
I0201 23:32:35.909899 140022518892288 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.5562012195587158, loss=3.6116578578948975
I0201 23:33:22.172696 140023005427456 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.8604755401611328, loss=2.6113197803497314
I0201 23:34:08.272970 140022518892288 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.9310683012008667, loss=2.3459548950195312
I0201 23:34:54.509537 140023005427456 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.9388415813446045, loss=2.730990171432495
I0201 23:35:40.794676 140022518892288 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.5961179733276367, loss=4.5346174240112305
I0201 23:35:55.200352 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:36:05.831572 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:36:32.204648 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:36:33.798221 140184451094336 submission_runner.py:408] Time since start: 40809.97s, 	Step: 81833, 	{'train/accuracy': 0.6919335722923279, 'train/loss': 1.269242525100708, 'validation/accuracy': 0.6193400025367737, 'validation/loss': 1.5890880823135376, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.232736349105835, 'test/num_examples': 10000, 'score': 37429.34026837349, 'total_duration': 40809.9657497406, 'accumulated_submission_time': 37429.34026837349, 'accumulated_eval_time': 3372.996278524399, 'accumulated_logging_time': 3.3874971866607666}
I0201 23:36:33.830025 140023005427456 logging_writer.py:48] [81833] accumulated_eval_time=3372.996279, accumulated_logging_time=3.387497, accumulated_submission_time=37429.340268, global_step=81833, preemption_count=0, score=37429.340268, test/accuracy=0.497600, test/loss=2.232736, test/num_examples=10000, total_duration=40809.965750, train/accuracy=0.691934, train/loss=1.269243, validation/accuracy=0.619340, validation/loss=1.589088, validation/num_examples=50000
I0201 23:37:01.023701 140022518892288 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.9214937686920166, loss=2.387967586517334
I0201 23:37:46.517924 140023005427456 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.062363624572754, loss=2.456425189971924
I0201 23:38:32.936428 140022518892288 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.1319117546081543, loss=2.4564924240112305
I0201 23:39:19.229751 140023005427456 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.0251901149749756, loss=2.9099202156066895
I0201 23:40:05.555214 140022518892288 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.212991714477539, loss=2.43098783493042
I0201 23:40:51.830994 140023005427456 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.9659945964813232, loss=2.470956802368164
I0201 23:41:38.269497 140022518892288 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.9392589330673218, loss=2.498932123184204
I0201 23:42:24.838363 140023005427456 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.0119149684906006, loss=2.3475563526153564
I0201 23:43:11.409184 140022518892288 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.080627679824829, loss=2.4186291694641113
I0201 23:43:33.813699 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:43:44.425685 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:44:10.092750 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:44:11.691013 140184451094336 submission_runner.py:408] Time since start: 41267.86s, 	Step: 82750, 	{'train/accuracy': 0.6674609184265137, 'train/loss': 1.4016146659851074, 'validation/accuracy': 0.6218599677085876, 'validation/loss': 1.6021045446395874, 'validation/num_examples': 50000, 'test/accuracy': 0.49550002813339233, 'test/loss': 2.2633657455444336, 'test/num_examples': 10000, 'score': 37849.2662627697, 'total_duration': 41267.85855412483, 'accumulated_submission_time': 37849.2662627697, 'accumulated_eval_time': 3410.873600244522, 'accumulated_logging_time': 3.429560661315918}
I0201 23:44:11.718649 140023005427456 logging_writer.py:48] [82750] accumulated_eval_time=3410.873600, accumulated_logging_time=3.429561, accumulated_submission_time=37849.266263, global_step=82750, preemption_count=0, score=37849.266263, test/accuracy=0.495500, test/loss=2.263366, test/num_examples=10000, total_duration=41267.858554, train/accuracy=0.667461, train/loss=1.401615, validation/accuracy=0.621860, validation/loss=1.602105, validation/num_examples=50000
I0201 23:44:32.098485 140022518892288 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.8455688953399658, loss=3.908020257949829
I0201 23:45:16.358427 140023005427456 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.0343079566955566, loss=2.549651861190796
I0201 23:46:02.483010 140022518892288 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.8016533851623535, loss=2.7168831825256348
I0201 23:46:48.868723 140023005427456 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1825144290924072, loss=2.3393704891204834
I0201 23:47:34.821721 140022518892288 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.993430733680725, loss=2.4064652919769287
I0201 23:48:20.906424 140023005427456 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.229335069656372, loss=2.3826751708984375
I0201 23:49:06.996872 140022518892288 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.7482866048812866, loss=3.472829818725586
I0201 23:49:53.037281 140023005427456 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.0475339889526367, loss=2.1598174571990967
I0201 23:50:39.463265 140022518892288 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.8957065343856812, loss=2.678354501724243
I0201 23:51:11.816833 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:51:22.230681 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:51:49.054517 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:51:50.661301 140184451094336 submission_runner.py:408] Time since start: 41726.83s, 	Step: 83672, 	{'train/accuracy': 0.6716406345367432, 'train/loss': 1.3420121669769287, 'validation/accuracy': 0.6239799857139587, 'validation/loss': 1.5581839084625244, 'validation/num_examples': 50000, 'test/accuracy': 0.5044000148773193, 'test/loss': 2.198859214782715, 'test/num_examples': 10000, 'score': 38269.307027578354, 'total_duration': 41726.82884001732, 'accumulated_submission_time': 38269.307027578354, 'accumulated_eval_time': 3449.7180716991425, 'accumulated_logging_time': 3.4665777683258057}
I0201 23:51:50.689302 140023005427456 logging_writer.py:48] [83672] accumulated_eval_time=3449.718072, accumulated_logging_time=3.466578, accumulated_submission_time=38269.307028, global_step=83672, preemption_count=0, score=38269.307028, test/accuracy=0.504400, test/loss=2.198859, test/num_examples=10000, total_duration=41726.828840, train/accuracy=0.671641, train/loss=1.342012, validation/accuracy=0.623980, validation/loss=1.558184, validation/num_examples=50000
I0201 23:52:02.299200 140022518892288 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.7822415828704834, loss=2.265321969985962
I0201 23:52:45.626854 140023005427456 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.0793685913085938, loss=2.3896336555480957
I0201 23:53:32.005142 140022518892288 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.8431570529937744, loss=2.9352149963378906
I0201 23:54:18.567733 140023005427456 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.374527931213379, loss=2.4107937812805176
I0201 23:55:04.666519 140022518892288 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.7416253089904785, loss=4.803265571594238
I0201 23:55:50.894523 140023005427456 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.8888731002807617, loss=2.489851236343384
I0201 23:56:37.236110 140022518892288 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.6463556289672852, loss=4.99655294418335
I0201 23:57:23.363149 140023005427456 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.9167006015777588, loss=3.027862071990967
I0201 23:58:09.438088 140022518892288 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.5811845064163208, loss=4.754266738891602
I0201 23:58:50.852304 140184451094336 spec.py:321] Evaluating on the training split.
I0201 23:59:01.394985 140184451094336 spec.py:333] Evaluating on the validation split.
I0201 23:59:21.960254 140184451094336 spec.py:349] Evaluating on the test split.
I0201 23:59:23.559506 140184451094336 submission_runner.py:408] Time since start: 42179.73s, 	Step: 84591, 	{'train/accuracy': 0.6886523365974426, 'train/loss': 1.2715094089508057, 'validation/accuracy': 0.6290000081062317, 'validation/loss': 1.5482810735702515, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2042882442474365, 'test/num_examples': 10000, 'score': 38689.40639066696, 'total_duration': 42179.727041482925, 'accumulated_submission_time': 38689.40639066696, 'accumulated_eval_time': 3482.425269842148, 'accumulated_logging_time': 3.50958251953125}
I0201 23:59:23.586837 140023005427456 logging_writer.py:48] [84591] accumulated_eval_time=3482.425270, accumulated_logging_time=3.509583, accumulated_submission_time=38689.406391, global_step=84591, preemption_count=0, score=38689.406391, test/accuracy=0.505900, test/loss=2.204288, test/num_examples=10000, total_duration=42179.727041, train/accuracy=0.688652, train/loss=1.271509, validation/accuracy=0.629000, validation/loss=1.548281, validation/num_examples=50000
I0201 23:59:27.587168 140022518892288 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.5568199157714844, loss=4.931179046630859
I0202 00:00:09.637232 140023005427456 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.7090184688568115, loss=3.46165132522583
I0202 00:00:55.587380 140022518892288 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.787996768951416, loss=2.934008836746216
I0202 00:01:41.943336 140023005427456 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.7388478517532349, loss=3.4478745460510254
I0202 00:02:28.106361 140022518892288 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.9624805450439453, loss=2.282857894897461
I0202 00:03:14.576156 140023005427456 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.7757068872451782, loss=5.048961162567139
I0202 00:04:00.691099 140022518892288 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.24759840965271, loss=2.489713430404663
I0202 00:04:46.815728 140023005427456 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.872610330581665, loss=2.9405856132507324
I0202 00:05:32.949526 140022518892288 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.2630996704101562, loss=2.4308016300201416
I0202 00:06:19.139893 140023005427456 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.9947630167007446, loss=2.3502116203308105
I0202 00:06:23.938241 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:06:34.453239 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:07:02.981404 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:07:04.576396 140184451094336 submission_runner.py:408] Time since start: 42640.74s, 	Step: 85512, 	{'train/accuracy': 0.6723827719688416, 'train/loss': 1.3585656881332397, 'validation/accuracy': 0.6247400045394897, 'validation/loss': 1.5667896270751953, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.2121453285217285, 'test/num_examples': 10000, 'score': 39109.69956469536, 'total_duration': 42640.743921756744, 'accumulated_submission_time': 39109.69956469536, 'accumulated_eval_time': 3523.0634427070618, 'accumulated_logging_time': 3.547616958618164}
I0202 00:07:04.608113 140022518892288 logging_writer.py:48] [85512] accumulated_eval_time=3523.063443, accumulated_logging_time=3.547617, accumulated_submission_time=39109.699565, global_step=85512, preemption_count=0, score=39109.699565, test/accuracy=0.502800, test/loss=2.212145, test/num_examples=10000, total_duration=42640.743922, train/accuracy=0.672383, train/loss=1.358566, validation/accuracy=0.624740, validation/loss=1.566790, validation/num_examples=50000
I0202 00:07:41.192095 140023005427456 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.9070743322372437, loss=3.7034645080566406
I0202 00:08:27.209217 140022518892288 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.221874475479126, loss=2.351552724838257
I0202 00:09:13.626406 140023005427456 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.8467453718185425, loss=2.981916904449463
I0202 00:09:59.716705 140022518892288 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.2066211700439453, loss=2.3878960609436035
I0202 00:10:45.700520 140023005427456 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.055042266845703, loss=2.3720104694366455
I0202 00:11:31.848329 140022518892288 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.037175178527832, loss=2.525791645050049
I0202 00:12:18.057622 140023005427456 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.920925498008728, loss=2.922389030456543
I0202 00:13:04.199239 140022518892288 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.8452293872833252, loss=2.475200653076172
I0202 00:13:50.421418 140023005427456 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.85930335521698, loss=4.633113384246826
I0202 00:14:04.944234 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:14:15.448786 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:14:40.984629 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:14:42.588481 140184451094336 submission_runner.py:408] Time since start: 43098.76s, 	Step: 86433, 	{'train/accuracy': 0.6749609112739563, 'train/loss': 1.341333031654358, 'validation/accuracy': 0.6245799660682678, 'validation/loss': 1.5732306241989136, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.2271392345428467, 'test/num_examples': 10000, 'score': 39529.97727918625, 'total_duration': 43098.75600481033, 'accumulated_submission_time': 39529.97727918625, 'accumulated_eval_time': 3560.7076559066772, 'accumulated_logging_time': 3.589759588241577}
I0202 00:14:42.623071 140022518892288 logging_writer.py:48] [86433] accumulated_eval_time=3560.707656, accumulated_logging_time=3.589760, accumulated_submission_time=39529.977279, global_step=86433, preemption_count=0, score=39529.977279, test/accuracy=0.497300, test/loss=2.227139, test/num_examples=10000, total_duration=43098.756005, train/accuracy=0.674961, train/loss=1.341333, validation/accuracy=0.624580, validation/loss=1.573231, validation/num_examples=50000
I0202 00:15:09.810853 140023005427456 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.7567518949508667, loss=3.852529525756836
I0202 00:15:55.176713 140022518892288 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.072446584701538, loss=2.196073532104492
I0202 00:16:41.595735 140023005427456 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.045382499694824, loss=2.3691582679748535
I0202 00:17:27.812799 140022518892288 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.00390362739563, loss=2.4809317588806152
I0202 00:18:13.937245 140023005427456 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.9129786491394043, loss=2.8028664588928223
I0202 00:19:00.009268 140022518892288 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.8922948837280273, loss=4.612267971038818
I0202 00:19:46.145627 140023005427456 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.93373703956604, loss=2.4595885276794434
I0202 00:20:32.242900 140022518892288 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.149549961090088, loss=2.6325039863586426
I0202 00:21:18.487034 140023005427456 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.739683747291565, loss=4.7150397300720215
I0202 00:21:42.620762 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:21:53.127208 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:22:18.453598 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:22:20.040275 140184451094336 submission_runner.py:408] Time since start: 43556.21s, 	Step: 87354, 	{'train/accuracy': 0.6795117259025574, 'train/loss': 1.3415911197662354, 'validation/accuracy': 0.6220999956130981, 'validation/loss': 1.5983110666275024, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.2489187717437744, 'test/num_examples': 10000, 'score': 39949.916645765305, 'total_duration': 43556.20781803131, 'accumulated_submission_time': 39949.916645765305, 'accumulated_eval_time': 3598.127161026001, 'accumulated_logging_time': 3.6342861652374268}
I0202 00:22:20.070943 140022518892288 logging_writer.py:48] [87354] accumulated_eval_time=3598.127161, accumulated_logging_time=3.634286, accumulated_submission_time=39949.916646, global_step=87354, preemption_count=0, score=39949.916646, test/accuracy=0.499900, test/loss=2.248919, test/num_examples=10000, total_duration=43556.207818, train/accuracy=0.679512, train/loss=1.341591, validation/accuracy=0.622100, validation/loss=1.598311, validation/num_examples=50000
I0202 00:22:38.876925 140023005427456 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.7795389890670776, loss=3.267573833465576
I0202 00:23:23.220077 140022518892288 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.7158482074737549, loss=3.873605251312256
I0202 00:24:09.462538 140023005427456 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.145052194595337, loss=2.3327016830444336
I0202 00:24:55.700319 140022518892288 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.214064836502075, loss=2.4805610179901123
I0202 00:25:41.683768 140023005427456 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.0559122562408447, loss=2.3657784461975098
I0202 00:26:28.068299 140022518892288 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.0195705890655518, loss=2.263275623321533
I0202 00:27:14.165675 140023005427456 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.1891121864318848, loss=2.330457925796509
I0202 00:28:00.079608 140022518892288 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.9746891260147095, loss=2.5608978271484375
I0202 00:28:46.355665 140023005427456 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.1143641471862793, loss=2.3313241004943848
I0202 00:29:20.153675 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:29:30.670661 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:29:56.639001 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:29:58.244171 140184451094336 submission_runner.py:408] Time since start: 44014.41s, 	Step: 88275, 	{'train/accuracy': 0.6721289157867432, 'train/loss': 1.3381081819534302, 'validation/accuracy': 0.62909996509552, 'validation/loss': 1.5376700162887573, 'validation/num_examples': 50000, 'test/accuracy': 0.5071000456809998, 'test/loss': 2.2031280994415283, 'test/num_examples': 10000, 'score': 40369.94181585312, 'total_duration': 44014.411709070206, 'accumulated_submission_time': 40369.94181585312, 'accumulated_eval_time': 3636.2176535129547, 'accumulated_logging_time': 3.6755335330963135}
I0202 00:29:58.272978 140022518892288 logging_writer.py:48] [88275] accumulated_eval_time=3636.217654, accumulated_logging_time=3.675534, accumulated_submission_time=40369.941816, global_step=88275, preemption_count=0, score=40369.941816, test/accuracy=0.507100, test/loss=2.203128, test/num_examples=10000, total_duration=44014.411709, train/accuracy=0.672129, train/loss=1.338108, validation/accuracy=0.629100, validation/loss=1.537670, validation/num_examples=50000
I0202 00:30:08.658548 140023005427456 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.7644909620285034, loss=4.499539852142334
I0202 00:30:51.430669 140022518892288 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.006775379180908, loss=4.981389999389648
I0202 00:31:38.049119 140023005427456 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.067028284072876, loss=2.3377578258514404
I0202 00:32:24.562316 140022518892288 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.1272990703582764, loss=2.2422797679901123
I0202 00:33:10.823246 140023005427456 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.9563170671463013, loss=3.567944049835205
I0202 00:33:56.794670 140022518892288 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.1600823402404785, loss=2.293116569519043
I0202 00:34:43.282607 140023005427456 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.9655637741088867, loss=2.3336408138275146
I0202 00:35:29.410754 140022518892288 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.917659878730774, loss=2.8310627937316895
I0202 00:36:15.607753 140023005427456 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.1493279933929443, loss=2.327132225036621
I0202 00:36:58.400761 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:37:09.006819 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:37:36.221335 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:37:37.819373 140184451094336 submission_runner.py:408] Time since start: 44473.99s, 	Step: 89194, 	{'train/accuracy': 0.6786718368530273, 'train/loss': 1.3135088682174683, 'validation/accuracy': 0.6307799816131592, 'validation/loss': 1.5366266965866089, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.1833741664886475, 'test/num_examples': 10000, 'score': 40790.010445833206, 'total_duration': 44473.986904621124, 'accumulated_submission_time': 40790.010445833206, 'accumulated_eval_time': 3675.6362698078156, 'accumulated_logging_time': 3.7161176204681396}
I0202 00:37:37.847711 140022518892288 logging_writer.py:48] [89194] accumulated_eval_time=3675.636270, accumulated_logging_time=3.716118, accumulated_submission_time=40790.010446, global_step=89194, preemption_count=0, score=40790.010446, test/accuracy=0.505300, test/loss=2.183374, test/num_examples=10000, total_duration=44473.986905, train/accuracy=0.678672, train/loss=1.313509, validation/accuracy=0.630780, validation/loss=1.536627, validation/num_examples=50000
I0202 00:37:40.652851 140023005427456 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.0387768745422363, loss=2.346116781234741
I0202 00:38:22.497734 140022518892288 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.6673202514648438, loss=3.767911911010742
I0202 00:39:08.713652 140023005427456 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.0025835037231445, loss=4.667092323303223
I0202 00:39:54.881278 140022518892288 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.7570741176605225, loss=4.077582836151123
I0202 00:40:41.089374 140023005427456 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.7327184677124023, loss=3.0125162601470947
I0202 00:41:27.489543 140022518892288 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.9294843673706055, loss=4.330848217010498
I0202 00:42:14.184079 140023005427456 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.2448019981384277, loss=2.41025972366333
I0202 00:43:00.519792 140022518892288 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.0512561798095703, loss=4.845531463623047
I0202 00:43:46.722884 140023005427456 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.9316401481628418, loss=3.3162765502929688
I0202 00:44:33.412112 140022518892288 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.7443442344665527, loss=3.136274576187134
I0202 00:44:38.168869 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:44:48.568313 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:45:15.652173 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:45:17.258972 140184451094336 submission_runner.py:408] Time since start: 44933.43s, 	Step: 90112, 	{'train/accuracy': 0.6904687285423279, 'train/loss': 1.2547414302825928, 'validation/accuracy': 0.6344999670982361, 'validation/loss': 1.5105417966842651, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.1635007858276367, 'test/num_examples': 10000, 'score': 41210.27362036705, 'total_duration': 44933.42651605606, 'accumulated_submission_time': 41210.27362036705, 'accumulated_eval_time': 3714.726364850998, 'accumulated_logging_time': 3.7552504539489746}
I0202 00:45:17.287800 140023005427456 logging_writer.py:48] [90112] accumulated_eval_time=3714.726365, accumulated_logging_time=3.755250, accumulated_submission_time=41210.273620, global_step=90112, preemption_count=0, score=41210.273620, test/accuracy=0.510200, test/loss=2.163501, test/num_examples=10000, total_duration=44933.426516, train/accuracy=0.690469, train/loss=1.254741, validation/accuracy=0.634500, validation/loss=1.510542, validation/num_examples=50000
I0202 00:45:53.761086 140022518892288 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.91370689868927, loss=3.269002914428711
I0202 00:46:39.753834 140023005427456 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.831600308418274, loss=3.603332996368408
I0202 00:47:26.164157 140022518892288 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.987092137336731, loss=3.9973936080932617
I0202 00:48:12.534123 140023005427456 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.3079700469970703, loss=2.242936611175537
I0202 00:48:58.562988 140022518892288 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.6956534385681152, loss=4.411613464355469
I0202 00:49:44.836074 140023005427456 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.1588268280029297, loss=2.3240745067596436
I0202 00:50:31.083168 140022518892288 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.7394331693649292, loss=4.249900817871094
I0202 00:51:17.425711 140023005427456 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.8239601850509644, loss=4.410039901733398
I0202 00:52:03.536253 140022518892288 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.1805195808410645, loss=2.3910634517669678
I0202 00:52:17.611082 140184451094336 spec.py:321] Evaluating on the training split.
I0202 00:52:28.122972 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 00:52:56.403369 140184451094336 spec.py:349] Evaluating on the test split.
I0202 00:52:57.998550 140184451094336 submission_runner.py:408] Time since start: 45394.17s, 	Step: 91032, 	{'train/accuracy': 0.6882616877555847, 'train/loss': 1.284019112586975, 'validation/accuracy': 0.637939989566803, 'validation/loss': 1.5183864831924438, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.1651880741119385, 'test/num_examples': 10000, 'score': 41630.5404984951, 'total_duration': 45394.16607880592, 'accumulated_submission_time': 41630.5404984951, 'accumulated_eval_time': 3755.1138138771057, 'accumulated_logging_time': 3.7933239936828613}
I0202 00:52:58.032012 140023005427456 logging_writer.py:48] [91032] accumulated_eval_time=3755.113814, accumulated_logging_time=3.793324, accumulated_submission_time=41630.540498, global_step=91032, preemption_count=0, score=41630.540498, test/accuracy=0.518900, test/loss=2.165188, test/num_examples=10000, total_duration=45394.166079, train/accuracy=0.688262, train/loss=1.284019, validation/accuracy=0.637940, validation/loss=1.518386, validation/num_examples=50000
I0202 00:53:25.639665 140022518892288 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.8006621599197388, loss=3.0598678588867188
I0202 00:54:11.490032 140023005427456 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.451209783554077, loss=2.26269268989563
I0202 00:54:57.793416 140022518892288 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.0959599018096924, loss=2.8655202388763428
I0202 00:55:44.232636 140023005427456 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.9012925624847412, loss=2.278350830078125
I0202 00:56:30.337905 140022518892288 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.89556884765625, loss=4.900774002075195
I0202 00:57:16.632201 140023005427456 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.1141116619110107, loss=2.190349578857422
I0202 00:58:02.794914 140022518892288 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.2333874702453613, loss=2.4644603729248047
I0202 00:58:49.007137 140023005427456 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.540374755859375, loss=2.415205240249634
I0202 00:59:35.312391 140022518892288 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.1802897453308105, loss=2.123034715652466
I0202 00:59:58.048568 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:00:08.453239 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:00:30.922470 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:00:32.521859 140184451094336 submission_runner.py:408] Time since start: 45848.69s, 	Step: 91951, 	{'train/accuracy': 0.6794726252555847, 'train/loss': 1.3359767198562622, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.5554149150848389, 'validation/num_examples': 50000, 'test/accuracy': 0.5070000290870667, 'test/loss': 2.2034645080566406, 'test/num_examples': 10000, 'score': 42050.500351428986, 'total_duration': 45848.689403772354, 'accumulated_submission_time': 42050.500351428986, 'accumulated_eval_time': 3789.5871107578278, 'accumulated_logging_time': 3.8358139991760254}
I0202 01:00:32.551499 140023005427456 logging_writer.py:48] [91951] accumulated_eval_time=3789.587111, accumulated_logging_time=3.835814, accumulated_submission_time=42050.500351, global_step=91951, preemption_count=0, score=42050.500351, test/accuracy=0.507000, test/loss=2.203465, test/num_examples=10000, total_duration=45848.689404, train/accuracy=0.679473, train/loss=1.335977, validation/accuracy=0.630680, validation/loss=1.555415, validation/num_examples=50000
I0202 01:00:52.559501 140022518892288 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.178661346435547, loss=2.473632335662842
I0202 01:01:36.979004 140023005427456 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.9715797901153564, loss=4.730377197265625
I0202 01:02:23.179301 140022518892288 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.211949110031128, loss=2.754101276397705
I0202 01:03:09.661985 140023005427456 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.7790288925170898, loss=4.811391830444336
I0202 01:03:55.654284 140022518892288 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.6709893941879272, loss=3.6428608894348145
I0202 01:04:41.962330 140023005427456 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.3621349334716797, loss=2.2665460109710693
I0202 01:05:28.252807 140022518892288 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.8608942031860352, loss=2.875725030899048
I0202 01:06:14.541486 140023005427456 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.128117322921753, loss=2.33726167678833
I0202 01:07:00.547868 140022518892288 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.8681532144546509, loss=2.23774790763855
I0202 01:07:32.857798 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:07:43.193589 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:08:08.200387 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:08:09.798634 140184451094336 submission_runner.py:408] Time since start: 46305.97s, 	Step: 92871, 	{'train/accuracy': 0.6898437142372131, 'train/loss': 1.2812461853027344, 'validation/accuracy': 0.6362199783325195, 'validation/loss': 1.5131189823150635, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.15106463432312, 'test/num_examples': 10000, 'score': 42470.74870181084, 'total_duration': 46305.96616792679, 'accumulated_submission_time': 42470.74870181084, 'accumulated_eval_time': 3826.5279400348663, 'accumulated_logging_time': 3.8750860691070557}
I0202 01:08:09.831377 140023005427456 logging_writer.py:48] [92871] accumulated_eval_time=3826.527940, accumulated_logging_time=3.875086, accumulated_submission_time=42470.748702, global_step=92871, preemption_count=0, score=42470.748702, test/accuracy=0.510900, test/loss=2.151065, test/num_examples=10000, total_duration=46305.966168, train/accuracy=0.689844, train/loss=1.281246, validation/accuracy=0.636220, validation/loss=1.513119, validation/num_examples=50000
I0202 01:08:21.851255 140022518892288 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.7748513221740723, loss=4.98057746887207
I0202 01:09:05.117990 140023005427456 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.1083903312683105, loss=2.539130449295044
I0202 01:09:51.405354 140022518892288 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.684448480606079, loss=3.921386480331421
I0202 01:10:37.790626 140023005427456 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.398257255554199, loss=2.219567060470581
I0202 01:11:23.931240 140022518892288 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7142399549484253, loss=4.263421058654785
I0202 01:12:10.648327 140023005427456 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.1341145038604736, loss=2.4186127185821533
I0202 01:12:56.783265 140022518892288 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.0253756046295166, loss=2.4337124824523926
I0202 01:13:42.993553 140023005427456 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.0463931560516357, loss=3.4261016845703125
I0202 01:14:29.257378 140022518892288 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.0816259384155273, loss=2.276099920272827
I0202 01:15:10.047446 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:15:20.472851 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:15:47.876477 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:15:49.479257 140184451094336 submission_runner.py:408] Time since start: 46765.65s, 	Step: 93790, 	{'train/accuracy': 0.7091601490974426, 'train/loss': 1.1998614072799683, 'validation/accuracy': 0.6394400000572205, 'validation/loss': 1.507826566696167, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.150918960571289, 'test/num_examples': 10000, 'score': 42890.90657186508, 'total_duration': 46765.64679956436, 'accumulated_submission_time': 42890.90657186508, 'accumulated_eval_time': 3865.9597566127777, 'accumulated_logging_time': 3.9189999103546143}
I0202 01:15:49.511520 140023005427456 logging_writer.py:48] [93790] accumulated_eval_time=3865.959757, accumulated_logging_time=3.919000, accumulated_submission_time=42890.906572, global_step=93790, preemption_count=0, score=42890.906572, test/accuracy=0.522500, test/loss=2.150919, test/num_examples=10000, total_duration=46765.646800, train/accuracy=0.709160, train/loss=1.199861, validation/accuracy=0.639440, validation/loss=1.507827, validation/num_examples=50000
I0202 01:15:53.911980 140022518892288 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.8332844972610474, loss=4.940708637237549
I0202 01:16:36.355842 140023005427456 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.1786012649536133, loss=3.715057373046875
I0202 01:17:22.083234 140022518892288 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.954651117324829, loss=2.7805471420288086
I0202 01:18:08.519678 140023005427456 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.2767186164855957, loss=2.245208263397217
I0202 01:18:54.547288 140022518892288 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.2731242179870605, loss=2.4137635231018066
I0202 01:19:40.760381 140023005427456 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.9162015914916992, loss=4.373963832855225
I0202 01:20:26.758921 140022518892288 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.1222822666168213, loss=2.1677982807159424
I0202 01:21:12.802640 140023005427456 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.205263614654541, loss=2.6340179443359375
I0202 01:21:58.940472 140022518892288 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.080728769302368, loss=2.2575390338897705
I0202 01:22:45.264370 140023005427456 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.9051432609558105, loss=2.789736032485962
I0202 01:22:49.599322 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:23:00.119580 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:23:24.179129 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:23:25.800581 140184451094336 submission_runner.py:408] Time since start: 47221.97s, 	Step: 94711, 	{'train/accuracy': 0.6896093487739563, 'train/loss': 1.2857972383499146, 'validation/accuracy': 0.6369999647140503, 'validation/loss': 1.5179483890533447, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.1582961082458496, 'test/num_examples': 10000, 'score': 43310.93693423271, 'total_duration': 47221.96811914444, 'accumulated_submission_time': 43310.93693423271, 'accumulated_eval_time': 3902.16099691391, 'accumulated_logging_time': 3.960517644882202}
I0202 01:23:25.833628 140022518892288 logging_writer.py:48] [94711] accumulated_eval_time=3902.160997, accumulated_logging_time=3.960518, accumulated_submission_time=43310.936934, global_step=94711, preemption_count=0, score=43310.936934, test/accuracy=0.517600, test/loss=2.158296, test/num_examples=10000, total_duration=47221.968119, train/accuracy=0.689609, train/loss=1.285797, validation/accuracy=0.637000, validation/loss=1.517948, validation/num_examples=50000
I0202 01:24:02.690577 140023005427456 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.9030845165252686, loss=4.542183876037598
I0202 01:24:48.680302 140022518892288 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.1151366233825684, loss=2.319962501525879
I0202 01:25:34.858338 140023005427456 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.0645687580108643, loss=2.268306255340576
I0202 01:26:21.066205 140022518892288 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.096688747406006, loss=2.205294132232666
I0202 01:27:07.455307 140023005427456 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.419558525085449, loss=2.124809503555298
I0202 01:27:53.681378 140022518892288 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.7995480298995972, loss=3.240607976913452
I0202 01:28:39.741071 140023005427456 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.8531289100646973, loss=4.833020210266113
I0202 01:29:26.007278 140022518892288 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.91413152217865, loss=3.942152976989746
I0202 01:30:12.438554 140023005427456 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.1310038566589355, loss=2.5686721801757812
I0202 01:30:26.233199 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:30:37.654336 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:31:05.062935 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:31:06.679544 140184451094336 submission_runner.py:408] Time since start: 47682.85s, 	Step: 95632, 	{'train/accuracy': 0.6902929544448853, 'train/loss': 1.3088555335998535, 'validation/accuracy': 0.6377800107002258, 'validation/loss': 1.5345081090927124, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.168625831604004, 'test/num_examples': 10000, 'score': 43731.27998661995, 'total_duration': 47682.84707713127, 'accumulated_submission_time': 43731.27998661995, 'accumulated_eval_time': 3942.607335329056, 'accumulated_logging_time': 4.002686023712158}
I0202 01:31:06.714399 140022518892288 logging_writer.py:48] [95632] accumulated_eval_time=3942.607335, accumulated_logging_time=4.002686, accumulated_submission_time=43731.279987, global_step=95632, preemption_count=0, score=43731.279987, test/accuracy=0.515900, test/loss=2.168626, test/num_examples=10000, total_duration=47682.847077, train/accuracy=0.690293, train/loss=1.308856, validation/accuracy=0.637780, validation/loss=1.534508, validation/num_examples=50000
I0202 01:31:34.289629 140023005427456 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.289403200149536, loss=2.208038568496704
I0202 01:32:19.841889 140022518892288 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.0525832176208496, loss=2.7563469409942627
I0202 01:33:06.380212 140023005427456 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.162031412124634, loss=2.2695624828338623
I0202 01:33:52.668221 140022518892288 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.867864966392517, loss=3.2700088024139404
I0202 01:34:39.017066 140023005427456 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.002267837524414, loss=4.898066997528076
I0202 01:35:25.180152 140022518892288 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.1160833835601807, loss=2.2337732315063477
I0202 01:36:11.346126 140023005427456 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.3575408458709717, loss=2.1218879222869873
I0202 01:36:57.483812 140022518892288 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.1667397022247314, loss=2.2926745414733887
I0202 01:37:43.668606 140023005427456 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.9300910234451294, loss=3.1981093883514404
I0202 01:38:07.030163 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:38:17.562637 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:38:44.400143 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:38:46.005943 140184451094336 submission_runner.py:408] Time since start: 48142.17s, 	Step: 96552, 	{'train/accuracy': 0.7011132836341858, 'train/loss': 1.2075631618499756, 'validation/accuracy': 0.6449399590492249, 'validation/loss': 1.4762593507766724, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.1300876140594482, 'test/num_examples': 10000, 'score': 44151.53822731972, 'total_duration': 48142.17348623276, 'accumulated_submission_time': 44151.53822731972, 'accumulated_eval_time': 3981.5831141471863, 'accumulated_logging_time': 4.047634124755859}
I0202 01:38:46.039391 140022518892288 logging_writer.py:48] [96552] accumulated_eval_time=3981.583114, accumulated_logging_time=4.047634, accumulated_submission_time=44151.538227, global_step=96552, preemption_count=0, score=44151.538227, test/accuracy=0.515800, test/loss=2.130088, test/num_examples=10000, total_duration=48142.173486, train/accuracy=0.701113, train/loss=1.207563, validation/accuracy=0.644940, validation/loss=1.476259, validation/num_examples=50000
I0202 01:39:05.644706 140023005427456 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.9605002403259277, loss=2.707930564880371
I0202 01:39:49.992248 140022518892288 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.588559150695801, loss=2.2414755821228027
I0202 01:40:36.245548 140023005427456 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.408661127090454, loss=2.3355553150177
I0202 01:41:22.697682 140022518892288 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.174107313156128, loss=4.828709602355957
I0202 01:42:08.980675 140023005427456 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.2715892791748047, loss=2.245954990386963
I0202 01:42:55.110617 140022518892288 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.913949966430664, loss=4.759693145751953
I0202 01:43:41.137688 140023005427456 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.9347450733184814, loss=4.77556037902832
I0202 01:44:27.233326 140022518892288 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.4979910850524902, loss=2.2073750495910645
I0202 01:45:13.449204 140023005427456 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.076601028442383, loss=4.279653549194336
I0202 01:45:46.158290 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:45:56.538877 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:46:22.138568 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:46:23.754133 140184451094336 submission_runner.py:408] Time since start: 48599.92s, 	Step: 97473, 	{'train/accuracy': 0.6884570121765137, 'train/loss': 1.2699761390686035, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.4902182817459106, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1305158138275146, 'test/num_examples': 10000, 'score': 44571.59751033783, 'total_duration': 48599.9216735363, 'accumulated_submission_time': 44571.59751033783, 'accumulated_eval_time': 4019.1789577007294, 'accumulated_logging_time': 4.093117952346802}
I0202 01:46:23.784209 140022518892288 logging_writer.py:48] [97473] accumulated_eval_time=4019.178958, accumulated_logging_time=4.093118, accumulated_submission_time=44571.597510, global_step=97473, preemption_count=0, score=44571.597510, test/accuracy=0.520800, test/loss=2.130516, test/num_examples=10000, total_duration=48599.921674, train/accuracy=0.688457, train/loss=1.269976, validation/accuracy=0.642140, validation/loss=1.490218, validation/num_examples=50000
I0202 01:46:34.981450 140023005427456 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.154641628265381, loss=2.2617578506469727
I0202 01:47:18.327448 140022518892288 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.6298828125, loss=2.2995810508728027
I0202 01:48:04.688335 140023005427456 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.000155448913574, loss=3.774029493331909
I0202 01:48:51.155356 140022518892288 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.174348831176758, loss=2.308925151824951
I0202 01:49:37.407580 140023005427456 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.2610058784484863, loss=2.5540103912353516
I0202 01:50:23.650586 140022518892288 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.8647912740707397, loss=3.9221975803375244
I0202 01:51:09.788300 140023005427456 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.9391672611236572, loss=3.1480448246002197
I0202 01:51:55.964093 140022518892288 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.373074531555176, loss=2.1763362884521484
I0202 01:52:42.040478 140023005427456 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.39139723777771, loss=4.08671760559082
I0202 01:53:24.068906 140184451094336 spec.py:321] Evaluating on the training split.
I0202 01:53:34.506915 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 01:54:03.666626 140184451094336 spec.py:349] Evaluating on the test split.
I0202 01:54:05.264211 140184451094336 submission_runner.py:408] Time since start: 49061.43s, 	Step: 98393, 	{'train/accuracy': 0.691699206829071, 'train/loss': 1.2806599140167236, 'validation/accuracy': 0.6419199705123901, 'validation/loss': 1.4992659091949463, 'validation/num_examples': 50000, 'test/accuracy': 0.5187000036239624, 'test/loss': 2.1577887535095215, 'test/num_examples': 10000, 'score': 44991.82571578026, 'total_duration': 49061.43175268173, 'accumulated_submission_time': 44991.82571578026, 'accumulated_eval_time': 4060.374292612076, 'accumulated_logging_time': 4.132027626037598}
I0202 01:54:05.294064 140022518892288 logging_writer.py:48] [98393] accumulated_eval_time=4060.374293, accumulated_logging_time=4.132028, accumulated_submission_time=44991.825716, global_step=98393, preemption_count=0, score=44991.825716, test/accuracy=0.518700, test/loss=2.157789, test/num_examples=10000, total_duration=49061.431753, train/accuracy=0.691699, train/loss=1.280660, validation/accuracy=0.641920, validation/loss=1.499266, validation/num_examples=50000
I0202 01:54:08.891187 140023005427456 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.9392714500427246, loss=4.287482261657715
I0202 01:54:50.657261 140022518892288 logging_writer.py:48] [98500] global_step=98500, grad_norm=3.284844160079956, loss=2.255209445953369
I0202 01:55:36.923398 140023005427456 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.886178970336914, loss=4.595560073852539
I0202 01:56:23.632642 140022518892288 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.2032647132873535, loss=2.187147617340088
I0202 01:57:09.952987 140023005427456 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.190591812133789, loss=2.3194005489349365
I0202 01:57:56.130215 140022518892288 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.2517776489257812, loss=2.111549139022827
I0202 01:58:42.277538 140023005427456 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.1794631481170654, loss=2.1849517822265625
I0202 01:59:28.339952 140022518892288 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.8796980381011963, loss=3.6904282569885254
I0202 02:00:14.391098 140023005427456 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.2676830291748047, loss=2.2303075790405273
I0202 02:01:00.744278 140022518892288 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.071843385696411, loss=2.249990701675415
I0202 02:01:05.358325 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:01:15.972747 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:01:43.660518 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:01:45.257794 140184451094336 submission_runner.py:408] Time since start: 49521.43s, 	Step: 99312, 	{'train/accuracy': 0.7009570002555847, 'train/loss': 1.2121543884277344, 'validation/accuracy': 0.6444999575614929, 'validation/loss': 1.4744832515716553, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.101590633392334, 'test/num_examples': 10000, 'score': 45411.43276429176, 'total_duration': 49521.42532157898, 'accumulated_submission_time': 45411.43276429176, 'accumulated_eval_time': 4100.273753166199, 'accumulated_logging_time': 4.570847034454346}
I0202 02:01:45.289546 140023005427456 logging_writer.py:48] [99312] accumulated_eval_time=4100.273753, accumulated_logging_time=4.570847, accumulated_submission_time=45411.432764, global_step=99312, preemption_count=0, score=45411.432764, test/accuracy=0.525800, test/loss=2.101591, test/num_examples=10000, total_duration=49521.425322, train/accuracy=0.700957, train/loss=1.212154, validation/accuracy=0.644500, validation/loss=1.474483, validation/num_examples=50000
I0202 02:02:21.792003 140022518892288 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.3771812915802, loss=2.147202491760254
I0202 02:03:07.956372 140023005427456 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.1264467239379883, loss=2.170206308364868
I0202 02:03:54.108205 140022518892288 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.3995354175567627, loss=2.2893478870391846
I0202 02:04:40.184551 140023005427456 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.969238519668579, loss=2.3294148445129395
I0202 02:05:26.228931 140022518892288 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.0108141899108887, loss=2.310849189758301
I0202 02:06:12.610926 140023005427456 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.542780637741089, loss=2.1908421516418457
I0202 02:06:58.756296 140022518892288 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.8044722080230713, loss=3.726445436477661
I0202 02:07:45.002051 140023005427456 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.6127498149871826, loss=2.3708386421203613
I0202 02:08:31.129400 140022518892288 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.177807569503784, loss=2.2426297664642334
I0202 02:08:45.570529 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:08:55.868866 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:09:19.538878 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:09:21.151508 140184451094336 submission_runner.py:408] Time since start: 49977.32s, 	Step: 100232, 	{'train/accuracy': 0.6933984160423279, 'train/loss': 1.2418538331985474, 'validation/accuracy': 0.6492399573326111, 'validation/loss': 1.4507449865341187, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.115975856781006, 'test/num_examples': 10000, 'score': 45831.65687251091, 'total_duration': 49977.31903076172, 'accumulated_submission_time': 45831.65687251091, 'accumulated_eval_time': 4135.854706764221, 'accumulated_logging_time': 4.6116437911987305}
I0202 02:09:21.191689 140023005427456 logging_writer.py:48] [100232] accumulated_eval_time=4135.854707, accumulated_logging_time=4.611644, accumulated_submission_time=45831.656873, global_step=100232, preemption_count=0, score=45831.656873, test/accuracy=0.518200, test/loss=2.115976, test/num_examples=10000, total_duration=49977.319031, train/accuracy=0.693398, train/loss=1.241854, validation/accuracy=0.649240, validation/loss=1.450745, validation/num_examples=50000
I0202 02:09:48.792123 140022518892288 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.4853436946868896, loss=2.2435312271118164
I0202 02:10:34.422955 140023005427456 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.021273374557495, loss=4.704431533813477
I0202 02:11:20.490478 140022518892288 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.1052215099334717, loss=2.8204708099365234
I0202 02:12:06.923768 140023005427456 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.3647186756134033, loss=2.3933897018432617
I0202 02:12:53.105033 140022518892288 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.2930288314819336, loss=2.136803150177002
I0202 02:13:39.557995 140023005427456 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.1686110496520996, loss=2.2119522094726562
I0202 02:14:25.413335 140022518892288 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.9012744426727295, loss=3.5369114875793457
I0202 02:15:11.522325 140023005427456 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.4168143272399902, loss=2.189852237701416
I0202 02:15:58.001872 140022518892288 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.2465648651123047, loss=2.269139528274536
I0202 02:16:21.220148 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:16:31.553620 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:16:56.877582 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:16:58.491343 140184451094336 submission_runner.py:408] Time since start: 50434.66s, 	Step: 101152, 	{'train/accuracy': 0.6918163895606995, 'train/loss': 1.2562538385391235, 'validation/accuracy': 0.6451799869537354, 'validation/loss': 1.4713010787963867, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1192281246185303, 'test/num_examples': 10000, 'score': 46251.626306295395, 'total_duration': 50434.658855199814, 'accumulated_submission_time': 46251.626306295395, 'accumulated_eval_time': 4173.125863075256, 'accumulated_logging_time': 4.663263559341431}
I0202 02:16:58.531832 140023005427456 logging_writer.py:48] [101152] accumulated_eval_time=4173.125863, accumulated_logging_time=4.663264, accumulated_submission_time=46251.626306, global_step=101152, preemption_count=0, score=46251.626306, test/accuracy=0.520500, test/loss=2.119228, test/num_examples=10000, total_duration=50434.658855, train/accuracy=0.691816, train/loss=1.256254, validation/accuracy=0.645180, validation/loss=1.471301, validation/num_examples=50000
I0202 02:17:18.139964 140022518892288 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.38968563079834, loss=2.2656054496765137
I0202 02:18:02.452692 140023005427456 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.364396810531616, loss=2.2196590900421143
I0202 02:18:49.015063 140022518892288 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.254349946975708, loss=2.298123359680176
I0202 02:19:35.777507 140023005427456 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.7455248832702637, loss=2.210405111312866
I0202 02:20:21.900373 140022518892288 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.2181355953216553, loss=2.365058422088623
I0202 02:21:08.629016 140023005427456 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.2433571815490723, loss=2.43500018119812
I0202 02:21:55.218605 140022518892288 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.029123067855835, loss=3.311939001083374
I0202 02:22:41.748648 140023005427456 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.3075668811798096, loss=2.7179956436157227
I0202 02:23:27.939174 140022518892288 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.2233946323394775, loss=3.4276533126831055
I0202 02:23:58.979546 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:24:09.896152 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:24:36.512466 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:24:38.108356 140184451094336 submission_runner.py:408] Time since start: 50894.28s, 	Step: 102069, 	{'train/accuracy': 0.7005273103713989, 'train/loss': 1.2236254215240479, 'validation/accuracy': 0.64656001329422, 'validation/loss': 1.4615575075149536, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.0956971645355225, 'test/num_examples': 10000, 'score': 46672.015506744385, 'total_duration': 50894.275889635086, 'accumulated_submission_time': 46672.015506744385, 'accumulated_eval_time': 4212.254663944244, 'accumulated_logging_time': 4.7148377895355225}
I0202 02:24:38.141711 140023005427456 logging_writer.py:48] [102069] accumulated_eval_time=4212.254664, accumulated_logging_time=4.714838, accumulated_submission_time=46672.015507, global_step=102069, preemption_count=0, score=46672.015507, test/accuracy=0.521600, test/loss=2.095697, test/num_examples=10000, total_duration=50894.275890, train/accuracy=0.700527, train/loss=1.223625, validation/accuracy=0.646560, validation/loss=1.461558, validation/num_examples=50000
I0202 02:24:50.950287 140022518892288 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.313138961791992, loss=2.3389554023742676
I0202 02:25:34.391932 140023005427456 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.5526106357574463, loss=2.274681329727173
I0202 02:26:20.744517 140022518892288 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.451653003692627, loss=2.596652030944824
I0202 02:27:07.192924 140023005427456 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.2859714031219482, loss=2.209810495376587
I0202 02:27:53.449149 140022518892288 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.4157049655914307, loss=2.090789318084717
I0202 02:28:40.019891 140023005427456 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.1849451065063477, loss=4.357787609100342
I0202 02:29:26.316748 140022518892288 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.244929552078247, loss=2.1103858947753906
I0202 02:30:12.412157 140023005427456 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.408191204071045, loss=2.19211483001709
I0202 02:30:58.920187 140022518892288 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.4070045948028564, loss=2.232241630554199
I0202 02:31:38.309664 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:31:48.710942 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:32:16.801395 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:32:18.406588 140184451094336 submission_runner.py:408] Time since start: 51354.57s, 	Step: 102986, 	{'train/accuracy': 0.7196484208106995, 'train/loss': 1.1426352262496948, 'validation/accuracy': 0.6477000117301941, 'validation/loss': 1.4675030708312988, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.1090574264526367, 'test/num_examples': 10000, 'score': 47092.12380337715, 'total_duration': 51354.57412791252, 'accumulated_submission_time': 47092.12380337715, 'accumulated_eval_time': 4252.351578950882, 'accumulated_logging_time': 4.760955810546875}
I0202 02:32:18.438291 140023005427456 logging_writer.py:48] [102986] accumulated_eval_time=4252.351579, accumulated_logging_time=4.760956, accumulated_submission_time=47092.123803, global_step=102986, preemption_count=0, score=47092.123803, test/accuracy=0.521700, test/loss=2.109057, test/num_examples=10000, total_duration=51354.574128, train/accuracy=0.719648, train/loss=1.142635, validation/accuracy=0.647700, validation/loss=1.467503, validation/num_examples=50000
I0202 02:32:24.433298 140022518892288 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.9949439764022827, loss=4.014431476593018
I0202 02:33:06.919405 140023005427456 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.5386786460876465, loss=2.19331955909729
I0202 02:33:52.946686 140022518892288 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.916399359703064, loss=3.954298257827759
I0202 02:34:39.325617 140023005427456 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.228878974914551, loss=2.039158344268799
I0202 02:35:25.321351 140022518892288 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.2538034915924072, loss=4.883083343505859
I0202 02:36:11.372536 140023005427456 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.9630203247070312, loss=2.9075944423675537
I0202 02:36:57.504282 140022518892288 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.3356449604034424, loss=2.1458230018615723
I0202 02:37:43.715823 140023005427456 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.2265565395355225, loss=2.194244623184204
I0202 02:38:29.816318 140022518892288 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.2535455226898193, loss=4.120907783508301
I0202 02:39:16.066812 140023005427456 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.176684856414795, loss=2.2665908336639404
I0202 02:39:18.666069 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:39:29.119185 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:39:57.342550 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:39:58.943375 140184451094336 submission_runner.py:408] Time since start: 51815.11s, 	Step: 103907, 	{'train/accuracy': 0.7007421851158142, 'train/loss': 1.230646014213562, 'validation/accuracy': 0.6519399881362915, 'validation/loss': 1.450840711593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.102874994277954, 'test/num_examples': 10000, 'score': 47512.29468727112, 'total_duration': 51815.11089801788, 'accumulated_submission_time': 47512.29468727112, 'accumulated_eval_time': 4292.62885594368, 'accumulated_logging_time': 4.802370309829712}
I0202 02:39:58.979737 140022518892288 logging_writer.py:48] [103907] accumulated_eval_time=4292.628856, accumulated_logging_time=4.802370, accumulated_submission_time=47512.294687, global_step=103907, preemption_count=0, score=47512.294687, test/accuracy=0.521200, test/loss=2.102875, test/num_examples=10000, total_duration=51815.110898, train/accuracy=0.700742, train/loss=1.230646, validation/accuracy=0.651940, validation/loss=1.450841, validation/num_examples=50000
I0202 02:40:37.940609 140023005427456 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.2455708980560303, loss=2.1200778484344482
I0202 02:41:23.807901 140022518892288 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.875788688659668, loss=2.311378002166748
I0202 02:42:10.475404 140023005427456 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.41050124168396, loss=2.1543517112731934
I0202 02:42:56.639843 140022518892288 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.4436817169189453, loss=2.254124164581299
I0202 02:43:42.806361 140023005427456 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.4374959468841553, loss=2.182448387145996
I0202 02:44:29.154496 140022518892288 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.545889377593994, loss=2.0945510864257812
I0202 02:45:15.397975 140023005427456 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.2985198497772217, loss=2.119265079498291
I0202 02:46:01.520259 140022518892288 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.3550548553466797, loss=2.1314282417297363
I0202 02:46:47.722593 140023005427456 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.298614025115967, loss=4.6657257080078125
I0202 02:46:59.033368 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:47:09.671175 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:47:34.169589 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:47:35.778347 140184451094336 submission_runner.py:408] Time since start: 52271.95s, 	Step: 104826, 	{'train/accuracy': 0.7058984041213989, 'train/loss': 1.1845290660858154, 'validation/accuracy': 0.6528800129890442, 'validation/loss': 1.4384924173355103, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.06282377243042, 'test/num_examples': 10000, 'score': 47932.29152727127, 'total_duration': 52271.9458630085, 'accumulated_submission_time': 47932.29152727127, 'accumulated_eval_time': 4329.373802185059, 'accumulated_logging_time': 4.848757028579712}
I0202 02:47:35.813627 140022518892288 logging_writer.py:48] [104826] accumulated_eval_time=4329.373802, accumulated_logging_time=4.848757, accumulated_submission_time=47932.291527, global_step=104826, preemption_count=0, score=47932.291527, test/accuracy=0.530000, test/loss=2.062824, test/num_examples=10000, total_duration=52271.945863, train/accuracy=0.705898, train/loss=1.184529, validation/accuracy=0.652880, validation/loss=1.438492, validation/num_examples=50000
I0202 02:48:05.829646 140023005427456 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.766200542449951, loss=2.1753897666931152
I0202 02:48:51.711551 140022518892288 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.9969854354858398, loss=4.764708995819092
I0202 02:49:38.288980 140023005427456 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.265139102935791, loss=2.2002689838409424
I0202 02:50:24.691563 140022518892288 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.3818728923797607, loss=2.1857237815856934
I0202 02:51:10.731254 140023005427456 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.2778263092041016, loss=2.475872755050659
I0202 02:51:57.411115 140022518892288 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.01880145072937, loss=3.2593889236450195
I0202 02:52:43.557038 140023005427456 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.2381107807159424, loss=2.2322371006011963
I0202 02:53:29.615353 140022518892288 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.201768636703491, loss=2.6175694465637207
I0202 02:54:15.757884 140023005427456 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.2778282165527344, loss=2.154240846633911
I0202 02:54:35.785260 140184451094336 spec.py:321] Evaluating on the training split.
I0202 02:54:46.605770 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 02:55:13.542575 140184451094336 spec.py:349] Evaluating on the test split.
I0202 02:55:15.146806 140184451094336 submission_runner.py:408] Time since start: 52731.31s, 	Step: 105745, 	{'train/accuracy': 0.71728515625, 'train/loss': 1.1596193313598633, 'validation/accuracy': 0.6541799902915955, 'validation/loss': 1.4440714120864868, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.0887348651885986, 'test/num_examples': 10000, 'score': 48352.20537209511, 'total_duration': 52731.31432533264, 'accumulated_submission_time': 48352.20537209511, 'accumulated_eval_time': 4368.735318899155, 'accumulated_logging_time': 4.893977880477905}
I0202 02:55:15.183990 140022518892288 logging_writer.py:48] [105745] accumulated_eval_time=4368.735319, accumulated_logging_time=4.893978, accumulated_submission_time=48352.205372, global_step=105745, preemption_count=0, score=48352.205372, test/accuracy=0.528700, test/loss=2.088735, test/num_examples=10000, total_duration=52731.314325, train/accuracy=0.717285, train/loss=1.159619, validation/accuracy=0.654180, validation/loss=1.444071, validation/num_examples=50000
I0202 02:55:37.583787 140023005427456 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.5881240367889404, loss=3.6897449493408203
I0202 02:56:22.258749 140022518892288 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.2846720218658447, loss=2.745312213897705
I0202 02:57:08.491714 140023005427456 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.725355863571167, loss=2.2725765705108643
I0202 02:57:54.736093 140022518892288 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.402128219604492, loss=2.205634832382202
I0202 02:58:40.721991 140023005427456 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.1867222785949707, loss=4.608023643493652
I0202 02:59:27.018979 140022518892288 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.300567626953125, loss=2.170332431793213
I0202 03:00:13.306066 140023005427456 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.525451183319092, loss=2.3743765354156494
I0202 03:00:59.265278 140022518892288 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.762317657470703, loss=2.2494497299194336
I0202 03:01:45.561622 140023005427456 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.136563777923584, loss=4.130002021789551
I0202 03:02:15.690550 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:02:26.503090 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:02:50.326703 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:02:51.935869 140184451094336 submission_runner.py:408] Time since start: 53188.10s, 	Step: 106667, 	{'train/accuracy': 0.7004296779632568, 'train/loss': 1.221923828125, 'validation/accuracy': 0.6500799655914307, 'validation/loss': 1.4510246515274048, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.0904181003570557, 'test/num_examples': 10000, 'score': 48772.653000593185, 'total_duration': 53188.10340118408, 'accumulated_submission_time': 48772.653000593185, 'accumulated_eval_time': 4404.980627298355, 'accumulated_logging_time': 4.941775798797607}
I0202 03:02:51.967325 140022518892288 logging_writer.py:48] [106667] accumulated_eval_time=4404.980627, accumulated_logging_time=4.941776, accumulated_submission_time=48772.653001, global_step=106667, preemption_count=0, score=48772.653001, test/accuracy=0.525500, test/loss=2.090418, test/num_examples=10000, total_duration=53188.103401, train/accuracy=0.700430, train/loss=1.221924, validation/accuracy=0.650080, validation/loss=1.451025, validation/num_examples=50000
I0202 03:03:05.560023 140023005427456 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.2021472454071045, loss=4.566275596618652
I0202 03:03:48.996791 140022518892288 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.321375608444214, loss=2.451505422592163
I0202 03:04:35.107756 140023005427456 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.2977867126464844, loss=3.6367006301879883
I0202 03:05:21.409578 140022518892288 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.6610524654388428, loss=2.1000571250915527
I0202 03:06:07.376021 140023005427456 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.4991817474365234, loss=2.290870428085327
I0202 03:06:53.449293 140022518892288 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.2535998821258545, loss=2.194612741470337
I0202 03:07:39.526888 140023005427456 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.1028451919555664, loss=4.008024215698242
I0202 03:08:25.710875 140022518892288 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.8145735263824463, loss=2.2486343383789062
I0202 03:09:11.968930 140023005427456 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.0925040245056152, loss=3.912327527999878
I0202 03:09:52.274753 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:10:02.966583 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:10:25.796613 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:10:27.398567 140184451094336 submission_runner.py:408] Time since start: 53643.57s, 	Step: 107589, 	{'train/accuracy': 0.7095702886581421, 'train/loss': 1.1786792278289795, 'validation/accuracy': 0.655239999294281, 'validation/loss': 1.4211721420288086, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.077857255935669, 'test/num_examples': 10000, 'score': 49192.90219426155, 'total_duration': 53643.56610870361, 'accumulated_submission_time': 49192.90219426155, 'accumulated_eval_time': 4440.104451656342, 'accumulated_logging_time': 4.982898235321045}
I0202 03:10:27.430237 140022518892288 logging_writer.py:48] [107589] accumulated_eval_time=4440.104452, accumulated_logging_time=4.982898, accumulated_submission_time=49192.902194, global_step=107589, preemption_count=0, score=49192.902194, test/accuracy=0.535700, test/loss=2.077857, test/num_examples=10000, total_duration=53643.566109, train/accuracy=0.709570, train/loss=1.178679, validation/accuracy=0.655240, validation/loss=1.421172, validation/num_examples=50000
I0202 03:10:32.239007 140023005427456 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.3226430416107178, loss=4.670494079589844
I0202 03:11:14.160697 140022518892288 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.1519832611083984, loss=2.5092215538024902
I0202 03:12:00.444080 140023005427456 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.6528146266937256, loss=2.2575273513793945
I0202 03:12:46.803587 140022518892288 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.447266101837158, loss=2.1567983627319336
I0202 03:13:32.913557 140023005427456 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.3392083644866943, loss=2.202845573425293
I0202 03:14:19.184106 140022518892288 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.1058871746063232, loss=3.412663221359253
I0202 03:15:05.403819 140023005427456 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.4505040645599365, loss=4.895566940307617
I0202 03:15:51.322475 140022518892288 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.6725914478302, loss=2.439042091369629
I0202 03:16:37.573526 140023005427456 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.0876781940460205, loss=4.411613941192627
I0202 03:17:23.597371 140022518892288 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.1470136642456055, loss=3.4461300373077393
I0202 03:17:27.399081 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:17:37.808034 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:18:03.998245 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:18:05.594346 140184451094336 submission_runner.py:408] Time since start: 54101.76s, 	Step: 108510, 	{'train/accuracy': 0.7232226133346558, 'train/loss': 1.1331206560134888, 'validation/accuracy': 0.6595199704170227, 'validation/loss': 1.4153560400009155, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.0454037189483643, 'test/num_examples': 10000, 'score': 49612.81418466568, 'total_duration': 54101.761887550354, 'accumulated_submission_time': 49612.81418466568, 'accumulated_eval_time': 4478.299695491791, 'accumulated_logging_time': 5.023855924606323}
I0202 03:18:05.629614 140023005427456 logging_writer.py:48] [108510] accumulated_eval_time=4478.299695, accumulated_logging_time=5.023856, accumulated_submission_time=49612.814185, global_step=108510, preemption_count=0, score=49612.814185, test/accuracy=0.535400, test/loss=2.045404, test/num_examples=10000, total_duration=54101.761888, train/accuracy=0.723223, train/loss=1.133121, validation/accuracy=0.659520, validation/loss=1.415356, validation/num_examples=50000
I0202 03:18:43.036736 140022518892288 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.4549038410186768, loss=2.120795965194702
I0202 03:19:28.774064 140023005427456 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.1734373569488525, loss=4.7616801261901855
I0202 03:20:14.986674 140022518892288 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.64461612701416, loss=2.169111490249634
I0202 03:21:01.250782 140023005427456 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.5954275131225586, loss=2.493652105331421
I0202 03:21:47.589512 140022518892288 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.3422255516052246, loss=2.8706014156341553
I0202 03:22:33.902663 140023005427456 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.635488271713257, loss=2.0509212017059326
I0202 03:23:20.188606 140022518892288 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.7225871086120605, loss=2.4059624671936035
I0202 03:24:06.400617 140023005427456 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.470203161239624, loss=2.1818697452545166
I0202 03:24:52.564514 140022518892288 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.247521162033081, loss=2.3913800716400146
I0202 03:25:05.770323 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:25:16.266218 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:25:43.290469 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:25:44.888021 140184451094336 submission_runner.py:408] Time since start: 54561.06s, 	Step: 109430, 	{'train/accuracy': 0.7046093344688416, 'train/loss': 1.2074859142303467, 'validation/accuracy': 0.6543200016021729, 'validation/loss': 1.4327102899551392, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.0763421058654785, 'test/num_examples': 10000, 'score': 50032.897715091705, 'total_duration': 54561.05555176735, 'accumulated_submission_time': 50032.897715091705, 'accumulated_eval_time': 4517.4173810482025, 'accumulated_logging_time': 5.06796669960022}
I0202 03:25:44.920695 140023005427456 logging_writer.py:48] [109430] accumulated_eval_time=4517.417381, accumulated_logging_time=5.067967, accumulated_submission_time=50032.897715, global_step=109430, preemption_count=0, score=50032.897715, test/accuracy=0.530600, test/loss=2.076342, test/num_examples=10000, total_duration=54561.055552, train/accuracy=0.704609, train/loss=1.207486, validation/accuracy=0.654320, validation/loss=1.432710, validation/num_examples=50000
I0202 03:26:13.324601 140022518892288 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.247889757156372, loss=2.4683799743652344
I0202 03:26:58.775889 140023005427456 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.2428841590881348, loss=2.61694073677063
I0202 03:27:45.348526 140022518892288 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.12263822555542, loss=3.649756908416748
I0202 03:28:31.614356 140023005427456 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.235025405883789, loss=4.77106237411499
I0202 03:29:17.750527 140022518892288 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.4662559032440186, loss=2.0189456939697266
I0202 03:30:04.186218 140023005427456 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.4770679473876953, loss=2.0586369037628174
I0202 03:30:50.203682 140022518892288 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.6347830295562744, loss=2.2067418098449707
I0202 03:31:36.473248 140023005427456 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.4384348392486572, loss=4.7128424644470215
I0202 03:32:22.794585 140022518892288 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.6370720863342285, loss=2.317171335220337
I0202 03:32:45.053250 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:32:55.403180 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:33:21.266353 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:33:22.880194 140184451094336 submission_runner.py:408] Time since start: 55019.05s, 	Step: 110350, 	{'train/accuracy': 0.711621105670929, 'train/loss': 1.1574349403381348, 'validation/accuracy': 0.6602199673652649, 'validation/loss': 1.392660140991211, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.0360000133514404, 'test/num_examples': 10000, 'score': 50452.97357225418, 'total_duration': 55019.047716379166, 'accumulated_submission_time': 50452.97357225418, 'accumulated_eval_time': 4555.2443215847015, 'accumulated_logging_time': 5.109800100326538}
I0202 03:33:22.921172 140023005427456 logging_writer.py:48] [110350] accumulated_eval_time=4555.244322, accumulated_logging_time=5.109800, accumulated_submission_time=50452.973572, global_step=110350, preemption_count=0, score=50452.973572, test/accuracy=0.533500, test/loss=2.036000, test/num_examples=10000, total_duration=55019.047716, train/accuracy=0.711621, train/loss=1.157435, validation/accuracy=0.660220, validation/loss=1.392660, validation/num_examples=50000
I0202 03:33:43.310511 140022518892288 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.52372407913208, loss=2.514955997467041
I0202 03:34:27.697241 140023005427456 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.1367201805114746, loss=3.0785183906555176
I0202 03:35:13.851862 140022518892288 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.3704519271850586, loss=4.149466514587402
I0202 03:36:00.154093 140023005427456 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.286318063735962, loss=2.155411958694458
I0202 03:36:46.636286 140022518892288 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.2727432250976562, loss=2.308987855911255
I0202 03:37:32.713276 140023005427456 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.2387609481811523, loss=2.7827072143554688
I0202 03:38:18.875209 140022518892288 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.3248066902160645, loss=2.459610939025879
I0202 03:39:05.292451 140023005427456 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.555234909057617, loss=2.05861234664917
I0202 03:39:51.470272 140022518892288 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.7122223377227783, loss=2.2945973873138428
I0202 03:40:23.112656 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:40:33.542745 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:41:00.993087 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:41:02.588882 140184451094336 submission_runner.py:408] Time since start: 55478.76s, 	Step: 111270, 	{'train/accuracy': 0.7218554615974426, 'train/loss': 1.1395964622497559, 'validation/accuracy': 0.6591399908065796, 'validation/loss': 1.4095954895019531, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.0380427837371826, 'test/num_examples': 10000, 'score': 50873.10785269737, 'total_duration': 55478.7564125061, 'accumulated_submission_time': 50873.10785269737, 'accumulated_eval_time': 4594.720537662506, 'accumulated_logging_time': 5.160179615020752}
I0202 03:41:02.622949 140023005427456 logging_writer.py:48] [111270] accumulated_eval_time=4594.720538, accumulated_logging_time=5.160180, accumulated_submission_time=50873.107853, global_step=111270, preemption_count=0, score=50873.107853, test/accuracy=0.538600, test/loss=2.038043, test/num_examples=10000, total_duration=55478.756413, train/accuracy=0.721855, train/loss=1.139596, validation/accuracy=0.659140, validation/loss=1.409595, validation/num_examples=50000
I0202 03:41:15.017940 140022518892288 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.5338990688323975, loss=2.5702736377716064
I0202 03:41:58.585654 140023005427456 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.452165126800537, loss=2.6647675037384033
I0202 03:42:44.994452 140022518892288 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.3336963653564453, loss=2.0728859901428223
I0202 03:43:31.375658 140023005427456 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.5145022869110107, loss=2.1595711708068848
I0202 03:44:17.619102 140022518892288 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.361610174179077, loss=2.0813207626342773
I0202 03:45:04.101824 140023005427456 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.2801907062530518, loss=4.7969183921813965
I0202 03:45:50.270548 140022518892288 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.4348418712615967, loss=2.9149060249328613
I0202 03:46:36.402161 140023005427456 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.298394203186035, loss=4.498447895050049
I0202 03:47:22.521354 140022518892288 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.7557029724121094, loss=2.1872735023498535
I0202 03:48:02.946582 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:48:13.448087 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:48:41.487270 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:48:43.090515 140184451094336 submission_runner.py:408] Time since start: 55939.26s, 	Step: 112189, 	{'train/accuracy': 0.72132807970047, 'train/loss': 1.1169127225875854, 'validation/accuracy': 0.6618199944496155, 'validation/loss': 1.3724278211593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.0063302516937256, 'test/num_examples': 10000, 'score': 51293.37358379364, 'total_duration': 55939.25805449486, 'accumulated_submission_time': 51293.37358379364, 'accumulated_eval_time': 4634.864460945129, 'accumulated_logging_time': 5.204523324966431}
I0202 03:48:43.124660 140023005427456 logging_writer.py:48] [112189] accumulated_eval_time=4634.864461, accumulated_logging_time=5.204523, accumulated_submission_time=51293.373584, global_step=112189, preemption_count=0, score=51293.373584, test/accuracy=0.538800, test/loss=2.006330, test/num_examples=10000, total_duration=55939.258054, train/accuracy=0.721328, train/loss=1.116913, validation/accuracy=0.661820, validation/loss=1.372428, validation/num_examples=50000
I0202 03:48:47.932606 140022518892288 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6301186084747314, loss=2.081486225128174
I0202 03:49:30.119074 140023005427456 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.7132480144500732, loss=2.0946602821350098
I0202 03:50:16.242264 140022518892288 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.5620758533477783, loss=2.2156107425689697
I0202 03:51:02.353391 140023005427456 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.5502798557281494, loss=2.1634318828582764
I0202 03:51:48.835930 140022518892288 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.2783026695251465, loss=2.4769554138183594
I0202 03:52:35.275733 140023005427456 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.4214258193969727, loss=2.1727442741394043
I0202 03:53:21.272353 140022518892288 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.674300193786621, loss=1.9707027673721313
I0202 03:54:07.492613 140023005427456 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.507138252258301, loss=2.229489803314209
I0202 03:54:53.667920 140022518892288 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.5857138633728027, loss=2.1158089637756348
I0202 03:55:40.106341 140023005427456 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.409158945083618, loss=2.9533674716949463
I0202 03:55:43.446479 140184451094336 spec.py:321] Evaluating on the training split.
I0202 03:55:53.867326 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 03:56:21.809298 140184451094336 spec.py:349] Evaluating on the test split.
I0202 03:56:23.407674 140184451094336 submission_runner.py:408] Time since start: 56399.58s, 	Step: 113109, 	{'train/accuracy': 0.7181445360183716, 'train/loss': 1.1483697891235352, 'validation/accuracy': 0.6665199995040894, 'validation/loss': 1.3766143321990967, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.011446714401245, 'test/num_examples': 10000, 'score': 51713.63455915451, 'total_duration': 56399.57521724701, 'accumulated_submission_time': 51713.63455915451, 'accumulated_eval_time': 4674.825638771057, 'accumulated_logging_time': 5.251617193222046}
I0202 03:56:23.440199 140022518892288 logging_writer.py:48] [113109] accumulated_eval_time=4674.825639, accumulated_logging_time=5.251617, accumulated_submission_time=51713.634559, global_step=113109, preemption_count=0, score=51713.634559, test/accuracy=0.542700, test/loss=2.011447, test/num_examples=10000, total_duration=56399.575217, train/accuracy=0.718145, train/loss=1.148370, validation/accuracy=0.666520, validation/loss=1.376614, validation/num_examples=50000
I0202 03:57:01.306644 140023005427456 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.7561655044555664, loss=2.1414341926574707
I0202 03:57:47.322837 140022518892288 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.0388023853302, loss=3.8682990074157715
I0202 03:58:33.718563 140023005427456 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.1341402530670166, loss=3.4669792652130127
I0202 03:59:19.897763 140022518892288 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.7999610900878906, loss=2.9034337997436523
I0202 04:00:06.375308 140023005427456 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8701632022857666, loss=2.0836737155914307
I0202 04:00:52.526062 140022518892288 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.3665592670440674, loss=2.1175687313079834
I0202 04:01:38.543389 140023005427456 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.2893431186676025, loss=2.731529474258423
I0202 04:02:24.867213 140022518892288 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.2803475856781006, loss=3.649430990219116
I0202 04:03:10.959403 140023005427456 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.6691877841949463, loss=2.3268251419067383
I0202 04:03:23.563502 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:03:34.030224 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:04:02.440842 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:04:04.031529 140184451094336 submission_runner.py:408] Time since start: 56860.20s, 	Step: 114029, 	{'train/accuracy': 0.7268164157867432, 'train/loss': 1.0959932804107666, 'validation/accuracy': 0.6672999858856201, 'validation/loss': 1.353961706161499, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.0045273303985596, 'test/num_examples': 10000, 'score': 52133.7013399601, 'total_duration': 56860.199070215225, 'accumulated_submission_time': 52133.7013399601, 'accumulated_eval_time': 4715.29369020462, 'accumulated_logging_time': 5.293383836746216}
I0202 04:04:04.063356 140022518892288 logging_writer.py:48] [114029] accumulated_eval_time=4715.293690, accumulated_logging_time=5.293384, accumulated_submission_time=52133.701340, global_step=114029, preemption_count=0, score=52133.701340, test/accuracy=0.543200, test/loss=2.004527, test/num_examples=10000, total_duration=56860.199070, train/accuracy=0.726816, train/loss=1.095993, validation/accuracy=0.667300, validation/loss=1.353962, validation/num_examples=50000
I0202 04:04:32.872608 140023005427456 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.5065672397613525, loss=2.1393918991088867
I0202 04:05:18.750714 140022518892288 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.5832035541534424, loss=2.069753646850586
I0202 04:06:05.051161 140023005427456 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.478536367416382, loss=1.980027198791504
I0202 04:06:51.011409 140022518892288 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.3020691871643066, loss=3.344235897064209
I0202 04:07:37.156958 140023005427456 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.335170030593872, loss=3.028581142425537
I0202 04:08:23.421698 140022518892288 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.3806512355804443, loss=2.1442971229553223
I0202 04:09:09.389208 140023005427456 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.7828140258789062, loss=2.121703624725342
I0202 04:09:55.356313 140022518892288 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.6291933059692383, loss=2.0695106983184814
I0202 04:10:41.695700 140023005427456 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.6845972537994385, loss=2.1405110359191895
I0202 04:11:04.068628 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:11:14.218697 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:11:39.635670 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:11:41.237330 140184451094336 submission_runner.py:408] Time since start: 57317.40s, 	Step: 114950, 	{'train/accuracy': 0.7395898103713989, 'train/loss': 1.0591590404510498, 'validation/accuracy': 0.6657999753952026, 'validation/loss': 1.3845274448394775, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.0233569145202637, 'test/num_examples': 10000, 'score': 52553.64809894562, 'total_duration': 57317.40486860275, 'accumulated_submission_time': 52553.64809894562, 'accumulated_eval_time': 4752.462374687195, 'accumulated_logging_time': 5.3358001708984375}
I0202 04:11:41.273674 140022518892288 logging_writer.py:48] [114950] accumulated_eval_time=4752.462375, accumulated_logging_time=5.335800, accumulated_submission_time=52553.648099, global_step=114950, preemption_count=0, score=52553.648099, test/accuracy=0.538700, test/loss=2.023357, test/num_examples=10000, total_duration=57317.404869, train/accuracy=0.739590, train/loss=1.059159, validation/accuracy=0.665800, validation/loss=1.384527, validation/num_examples=50000
I0202 04:12:01.686654 140023005427456 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.435358762741089, loss=2.2182295322418213
I0202 04:12:46.321040 140022518892288 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.415616512298584, loss=2.13606595993042
I0202 04:13:32.562248 140023005427456 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.926503896713257, loss=2.0633366107940674
I0202 04:14:18.863392 140022518892288 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.7435624599456787, loss=2.2660181522369385
I0202 04:15:05.088652 140023005427456 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.489041566848755, loss=2.0079338550567627
I0202 04:15:51.207699 140022518892288 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.438918352127075, loss=4.009799957275391
I0202 04:16:37.280436 140023005427456 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.7266783714294434, loss=2.1691224575042725
I0202 04:17:23.345619 140022518892288 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.3929920196533203, loss=2.578036069869995
I0202 04:18:09.316952 140023005427456 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.650933265686035, loss=2.0391488075256348
I0202 04:18:41.624569 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:18:52.192020 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:19:17.792925 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:19:19.395509 140184451094336 submission_runner.py:408] Time since start: 57775.56s, 	Step: 115872, 	{'train/accuracy': 0.72279292345047, 'train/loss': 1.1121561527252197, 'validation/accuracy': 0.670799970626831, 'validation/loss': 1.3483924865722656, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 1.9749292135238647, 'test/num_examples': 10000, 'score': 52973.94215083122, 'total_duration': 57775.563047885895, 'accumulated_submission_time': 52973.94215083122, 'accumulated_eval_time': 4790.233314990997, 'accumulated_logging_time': 5.381194114685059}
I0202 04:19:19.428276 140022518892288 logging_writer.py:48] [115872] accumulated_eval_time=4790.233315, accumulated_logging_time=5.381194, accumulated_submission_time=52973.942151, global_step=115872, preemption_count=0, score=52973.942151, test/accuracy=0.546100, test/loss=1.974929, test/num_examples=10000, total_duration=57775.563048, train/accuracy=0.722793, train/loss=1.112156, validation/accuracy=0.670800, validation/loss=1.348392, validation/num_examples=50000
I0202 04:19:31.025384 140023005427456 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.487429618835449, loss=2.428061008453369
I0202 04:20:14.070742 140022518892288 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.4029409885406494, loss=3.926578998565674
I0202 04:21:00.426699 140023005427456 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.3204455375671387, loss=2.3368191719055176
I0202 04:21:47.020033 140022518892288 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.3335049152374268, loss=3.412262201309204
I0202 04:22:33.012115 140023005427456 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.356863498687744, loss=2.652494430541992
I0202 04:23:19.384531 140022518892288 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.1944730281829834, loss=4.300804615020752
I0202 04:24:05.521767 140023005427456 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.687880277633667, loss=2.0907390117645264
I0202 04:24:52.119790 140022518892288 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.531693935394287, loss=2.207144260406494
I0202 04:25:38.139393 140023005427456 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.2726962566375732, loss=3.302567958831787
I0202 04:26:19.486699 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:26:30.000996 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:26:55.781058 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:26:57.382348 140184451094336 submission_runner.py:408] Time since start: 58233.55s, 	Step: 116791, 	{'train/accuracy': 0.7229296565055847, 'train/loss': 1.1437608003616333, 'validation/accuracy': 0.6652799844741821, 'validation/loss': 1.4038317203521729, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.02569580078125, 'test/num_examples': 10000, 'score': 53393.94225502014, 'total_duration': 58233.54987645149, 'accumulated_submission_time': 53393.94225502014, 'accumulated_eval_time': 4828.128947257996, 'accumulated_logging_time': 5.423955678939819}
I0202 04:26:57.424143 140022518892288 logging_writer.py:48] [116791] accumulated_eval_time=4828.128947, accumulated_logging_time=5.423956, accumulated_submission_time=53393.942255, global_step=116791, preemption_count=0, score=53393.942255, test/accuracy=0.542900, test/loss=2.025696, test/num_examples=10000, total_duration=58233.549876, train/accuracy=0.722930, train/loss=1.143761, validation/accuracy=0.665280, validation/loss=1.403832, validation/num_examples=50000
I0202 04:27:01.423875 140023005427456 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.5707807540893555, loss=2.1032323837280273
I0202 04:27:43.487506 140022518892288 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.6897406578063965, loss=4.172255039215088
I0202 04:28:29.592768 140023005427456 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.628568649291992, loss=2.1058881282806396
I0202 04:29:15.969799 140022518892288 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.1750268936157227, loss=3.60827374458313
I0202 04:30:01.967762 140023005427456 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.553590774536133, loss=2.2234857082366943
I0202 04:30:48.311971 140022518892288 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.612109661102295, loss=2.0308141708374023
I0202 04:31:34.706221 140023005427456 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.5892739295959473, loss=2.0578560829162598
I0202 04:32:21.114373 140022518892288 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.506819248199463, loss=1.8923311233520508
I0202 04:33:07.486058 140023005427456 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.648099660873413, loss=1.930004596710205
I0202 04:33:53.763749 140022518892288 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.292973041534424, loss=3.0227832794189453
I0202 04:33:57.512569 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:34:08.351955 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:34:32.984323 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:34:34.590096 140184451094336 submission_runner.py:408] Time since start: 58690.76s, 	Step: 117710, 	{'train/accuracy': 0.741406261920929, 'train/loss': 1.021639108657837, 'validation/accuracy': 0.6748600006103516, 'validation/loss': 1.3188023567199707, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 1.951604962348938, 'test/num_examples': 10000, 'score': 53813.971108198166, 'total_duration': 58690.757637262344, 'accumulated_submission_time': 53813.971108198166, 'accumulated_eval_time': 4865.206468343735, 'accumulated_logging_time': 5.476790428161621}
I0202 04:34:34.623408 140023005427456 logging_writer.py:48] [117710] accumulated_eval_time=4865.206468, accumulated_logging_time=5.476790, accumulated_submission_time=53813.971108, global_step=117710, preemption_count=0, score=53813.971108, test/accuracy=0.550600, test/loss=1.951605, test/num_examples=10000, total_duration=58690.757637, train/accuracy=0.741406, train/loss=1.021639, validation/accuracy=0.674860, validation/loss=1.318802, validation/num_examples=50000
I0202 04:35:11.847625 140022518892288 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.8464291095733643, loss=4.332779884338379
I0202 04:35:58.014018 140023005427456 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.4238028526306152, loss=3.6444735527038574
I0202 04:36:44.522680 140022518892288 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.371453046798706, loss=3.4044106006622314
I0202 04:37:30.591711 140023005427456 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.3445992469787598, loss=4.568698406219482
I0202 04:38:16.744783 140022518892288 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.45385479927063, loss=4.502734661102295
I0202 04:39:02.657604 140023005427456 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.540956735610962, loss=2.178013324737549
I0202 04:39:48.714557 140022518892288 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.65158748626709, loss=4.065484523773193
I0202 04:40:34.912591 140023005427456 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.2382254600524902, loss=3.8310673236846924
I0202 04:41:20.985374 140022518892288 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.517982244491577, loss=3.0641980171203613
I0202 04:41:34.641774 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:41:45.891958 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:42:10.217309 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:42:11.822359 140184451094336 submission_runner.py:408] Time since start: 59147.99s, 	Step: 118631, 	{'train/accuracy': 0.7268164157867432, 'train/loss': 1.1247150897979736, 'validation/accuracy': 0.6675800085067749, 'validation/loss': 1.379623532295227, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 1.9980874061584473, 'test/num_examples': 10000, 'score': 54233.92829823494, 'total_duration': 59147.98989415169, 'accumulated_submission_time': 54233.92829823494, 'accumulated_eval_time': 4902.3870396614075, 'accumulated_logging_time': 5.5232415199279785}
I0202 04:42:11.855724 140023005427456 logging_writer.py:48] [118631] accumulated_eval_time=4902.387040, accumulated_logging_time=5.523242, accumulated_submission_time=54233.928298, global_step=118631, preemption_count=0, score=54233.928298, test/accuracy=0.551500, test/loss=1.998087, test/num_examples=10000, total_duration=59147.989894, train/accuracy=0.726816, train/loss=1.124715, validation/accuracy=0.667580, validation/loss=1.379624, validation/num_examples=50000
I0202 04:42:39.849537 140022518892288 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7024970054626465, loss=3.2611401081085205
I0202 04:43:25.315783 140023005427456 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.502420425415039, loss=2.1264383792877197
I0202 04:44:12.000135 140022518892288 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.506028890609741, loss=3.582900285720825
I0202 04:44:58.211243 140023005427456 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.50015926361084, loss=4.250554084777832
I0202 04:45:44.322305 140022518892288 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.8551130294799805, loss=2.069952964782715
I0202 04:46:30.654889 140023005427456 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.5998294353485107, loss=1.9860191345214844
I0202 04:47:16.855295 140022518892288 logging_writer.py:48] [119300] global_step=119300, grad_norm=3.074424982070923, loss=4.145559310913086
I0202 04:48:02.822484 140023005427456 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.7648417949676514, loss=2.004943370819092
I0202 04:48:49.017085 140022518892288 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.7586450576782227, loss=1.9814622402191162
I0202 04:49:11.823133 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:49:22.980946 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:49:49.692491 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:49:51.293370 140184451094336 submission_runner.py:408] Time since start: 59607.46s, 	Step: 119551, 	{'train/accuracy': 0.7312304377555847, 'train/loss': 1.0820475816726685, 'validation/accuracy': 0.6734399795532227, 'validation/loss': 1.340196967124939, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 1.9669947624206543, 'test/num_examples': 10000, 'score': 54653.83897304535, 'total_duration': 59607.46091222763, 'accumulated_submission_time': 54653.83897304535, 'accumulated_eval_time': 4941.857255935669, 'accumulated_logging_time': 5.566040754318237}
I0202 04:49:51.328724 140023005427456 logging_writer.py:48] [119551] accumulated_eval_time=4941.857256, accumulated_logging_time=5.566041, accumulated_submission_time=54653.838973, global_step=119551, preemption_count=0, score=54653.838973, test/accuracy=0.553300, test/loss=1.966995, test/num_examples=10000, total_duration=59607.460912, train/accuracy=0.731230, train/loss=1.082048, validation/accuracy=0.673440, validation/loss=1.340197, validation/num_examples=50000
I0202 04:50:11.318794 140022518892288 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.4065980911254883, loss=2.3843541145324707
I0202 04:50:55.637641 140023005427456 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.6654560565948486, loss=2.501962184906006
I0202 04:51:42.287237 140022518892288 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.465193271636963, loss=4.287993431091309
I0202 04:52:28.904110 140023005427456 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.7501189708709717, loss=4.535949230194092
I0202 04:53:15.076678 140022518892288 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.50093936920166, loss=2.133819103240967
I0202 04:54:01.120663 140023005427456 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.356722354888916, loss=4.385015487670898
I0202 04:54:47.163959 140022518892288 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.4152352809906006, loss=2.1982955932617188
I0202 04:55:33.213867 140023005427456 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.4843695163726807, loss=3.8323872089385986
I0202 04:56:19.456087 140022518892288 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.610213279724121, loss=1.9904581308364868
I0202 04:56:51.694526 140184451094336 spec.py:321] Evaluating on the training split.
I0202 04:57:02.095906 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 04:57:28.167426 140184451094336 spec.py:349] Evaluating on the test split.
I0202 04:57:29.780280 140184451094336 submission_runner.py:408] Time since start: 60065.95s, 	Step: 120472, 	{'train/accuracy': 0.7416015267372131, 'train/loss': 1.0313720703125, 'validation/accuracy': 0.6787199974060059, 'validation/loss': 1.3119388818740845, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 1.9518202543258667, 'test/num_examples': 10000, 'score': 55074.146409749985, 'total_duration': 60065.947801828384, 'accumulated_submission_time': 55074.146409749985, 'accumulated_eval_time': 4979.9430141448975, 'accumulated_logging_time': 5.610635757446289}
I0202 04:57:29.817565 140023005427456 logging_writer.py:48] [120472] accumulated_eval_time=4979.943014, accumulated_logging_time=5.610636, accumulated_submission_time=55074.146410, global_step=120472, preemption_count=0, score=55074.146410, test/accuracy=0.554000, test/loss=1.951820, test/num_examples=10000, total_duration=60065.947802, train/accuracy=0.741602, train/loss=1.031372, validation/accuracy=0.678720, validation/loss=1.311939, validation/num_examples=50000
I0202 04:57:41.422306 140022518892288 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.791013717651367, loss=1.988380789756775
I0202 04:58:24.650251 140023005427456 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.5140576362609863, loss=3.9880034923553467
I0202 04:59:10.656198 140022518892288 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.510895252227783, loss=2.8261666297912598
I0202 04:59:56.761761 140023005427456 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.4497082233428955, loss=4.212542533874512
I0202 05:00:42.745414 140022518892288 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.542318344116211, loss=2.162402391433716
I0202 05:01:29.094293 140023005427456 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.778716802597046, loss=2.015645742416382
I0202 05:02:15.543441 140022518892288 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.276249408721924, loss=2.545886993408203
I0202 05:03:01.485644 140023005427456 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.4054195880889893, loss=3.219794273376465
I0202 05:03:47.576311 140022518892288 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.4568076133728027, loss=2.2183001041412354
I0202 05:04:30.002216 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:04:40.605346 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:05:08.302433 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:05:09.898367 140184451094336 submission_runner.py:408] Time since start: 60526.07s, 	Step: 121393, 	{'train/accuracy': 0.7378906011581421, 'train/loss': 1.0619568824768066, 'validation/accuracy': 0.6817599534988403, 'validation/loss': 1.3020442724227905, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 1.9345166683197021, 'test/num_examples': 10000, 'score': 55494.27367591858, 'total_duration': 60526.065910339355, 'accumulated_submission_time': 55494.27367591858, 'accumulated_eval_time': 5019.839179754257, 'accumulated_logging_time': 5.657719373703003}
I0202 05:05:09.932177 140023005427456 logging_writer.py:48] [121393] accumulated_eval_time=5019.839180, accumulated_logging_time=5.657719, accumulated_submission_time=55494.273676, global_step=121393, preemption_count=0, score=55494.273676, test/accuracy=0.555200, test/loss=1.934517, test/num_examples=10000, total_duration=60526.065910, train/accuracy=0.737891, train/loss=1.061957, validation/accuracy=0.681760, validation/loss=1.302044, validation/num_examples=50000
I0202 05:05:13.147129 140022518892288 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.4561102390289307, loss=2.366875648498535
I0202 05:05:55.084908 140023005427456 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.5040433406829834, loss=2.454211711883545
I0202 05:06:41.213620 140022518892288 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.267338752746582, loss=3.8543734550476074
I0202 05:07:27.407748 140023005427456 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.511720895767212, loss=3.3720452785491943
I0202 05:08:13.629897 140022518892288 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.648571491241455, loss=2.181720495223999
I0202 05:08:59.914469 140023005427456 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.636326313018799, loss=2.3597142696380615
I0202 05:09:46.161330 140022518892288 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.8809704780578613, loss=2.084176778793335
I0202 05:10:32.415363 140023005427456 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.824812173843384, loss=4.0834641456604
I0202 05:11:18.428251 140022518892288 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.957458257675171, loss=1.9654994010925293
I0202 05:12:04.757404 140023005427456 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.0177948474884033, loss=2.0464189052581787
I0202 05:12:09.920106 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:12:20.349354 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:12:47.029853 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:12:48.629801 140184451094336 submission_runner.py:408] Time since start: 60984.80s, 	Step: 122313, 	{'train/accuracy': 0.73011714220047, 'train/loss': 1.0760235786437988, 'validation/accuracy': 0.6768199801445007, 'validation/loss': 1.3135195970535278, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 1.960036039352417, 'test/num_examples': 10000, 'score': 55914.204786777496, 'total_duration': 60984.79734659195, 'accumulated_submission_time': 55914.204786777496, 'accumulated_eval_time': 5058.548875808716, 'accumulated_logging_time': 5.700714349746704}
I0202 05:12:48.666604 140022518892288 logging_writer.py:48] [122313] accumulated_eval_time=5058.548876, accumulated_logging_time=5.700714, accumulated_submission_time=55914.204787, global_step=122313, preemption_count=0, score=55914.204787, test/accuracy=0.548300, test/loss=1.960036, test/num_examples=10000, total_duration=60984.797347, train/accuracy=0.730117, train/loss=1.076024, validation/accuracy=0.676820, validation/loss=1.313520, validation/num_examples=50000
I0202 05:13:24.586033 140023005427456 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.2765727043151855, loss=1.9247775077819824
I0202 05:14:10.672565 140022518892288 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.8075807094573975, loss=4.353601455688477
I0202 05:14:56.941772 140023005427456 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.448326587677002, loss=2.8948073387145996
I0202 05:15:43.008890 140022518892288 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.537510871887207, loss=2.990962028503418
I0202 05:16:29.290761 140023005427456 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.8701324462890625, loss=4.482246398925781
I0202 05:17:15.498956 140022518892288 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.4601223468780518, loss=3.82989501953125
I0202 05:18:01.385896 140023005427456 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.729097843170166, loss=4.440224647521973
I0202 05:18:47.855992 140022518892288 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.9226667881011963, loss=2.0944104194641113
I0202 05:19:33.881946 140023005427456 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.4868788719177246, loss=2.2950117588043213
I0202 05:19:48.838271 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:19:59.572641 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:20:23.361432 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:20:24.968621 140184451094336 submission_runner.py:408] Time since start: 61441.14s, 	Step: 123234, 	{'train/accuracy': 0.7426952719688416, 'train/loss': 1.0379911661148071, 'validation/accuracy': 0.6814799904823303, 'validation/loss': 1.3065279722213745, 'validation/num_examples': 50000, 'test/accuracy': 0.5596000552177429, 'test/loss': 1.9329463243484497, 'test/num_examples': 10000, 'score': 56334.31851649284, 'total_duration': 61441.13616466522, 'accumulated_submission_time': 56334.31851649284, 'accumulated_eval_time': 5094.679251432419, 'accumulated_logging_time': 5.7470362186431885}
I0202 05:20:25.002545 140022518892288 logging_writer.py:48] [123234] accumulated_eval_time=5094.679251, accumulated_logging_time=5.747036, accumulated_submission_time=56334.318516, global_step=123234, preemption_count=0, score=56334.318516, test/accuracy=0.559600, test/loss=1.932946, test/num_examples=10000, total_duration=61441.136165, train/accuracy=0.742695, train/loss=1.037991, validation/accuracy=0.681480, validation/loss=1.306528, validation/num_examples=50000
I0202 05:20:51.792956 140023005427456 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.7673747539520264, loss=2.045809030532837
I0202 05:21:37.393935 140022518892288 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.7238361835479736, loss=3.5175740718841553
I0202 05:22:23.747693 140023005427456 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.6877431869506836, loss=3.0249061584472656
I0202 05:23:09.785154 140022518892288 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.262598991394043, loss=1.8860092163085938
I0202 05:23:55.781453 140023005427456 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.4835197925567627, loss=4.376871109008789
I0202 05:24:42.102940 140022518892288 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.6507914066314697, loss=2.1402382850646973
I0202 05:25:28.530808 140023005427456 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.0552477836608887, loss=2.602100133895874
I0202 05:26:14.771084 140022518892288 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.0925562381744385, loss=1.9370856285095215
I0202 05:27:00.905729 140023005427456 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.7660293579101562, loss=4.597789287567139
I0202 05:27:25.089744 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:27:35.502013 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:28:02.828327 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:28:04.429631 140184451094336 submission_runner.py:408] Time since start: 61900.60s, 	Step: 124154, 	{'train/accuracy': 0.7546288967132568, 'train/loss': 0.995815634727478, 'validation/accuracy': 0.6815999746322632, 'validation/loss': 1.3149043321609497, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 1.938615322113037, 'test/num_examples': 10000, 'score': 56754.348090171814, 'total_duration': 61900.59717607498, 'accumulated_submission_time': 56754.348090171814, 'accumulated_eval_time': 5134.01913523674, 'accumulated_logging_time': 5.790019989013672}
I0202 05:28:04.467338 140022518892288 logging_writer.py:48] [124154] accumulated_eval_time=5134.019135, accumulated_logging_time=5.790020, accumulated_submission_time=56754.348090, global_step=124154, preemption_count=0, score=56754.348090, test/accuracy=0.562000, test/loss=1.938615, test/num_examples=10000, total_duration=61900.597176, train/accuracy=0.754629, train/loss=0.995816, validation/accuracy=0.681600, validation/loss=1.314904, validation/num_examples=50000
I0202 05:28:23.270873 140023005427456 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.6614651679992676, loss=1.864555835723877
I0202 05:29:07.465918 140022518892288 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.873108386993408, loss=4.577247619628906
I0202 05:29:53.576606 140023005427456 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.9110405445098877, loss=2.190610408782959
I0202 05:30:39.893371 140022518892288 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.905191421508789, loss=3.034301996231079
I0202 05:31:25.908018 140023005427456 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.0011672973632812, loss=2.0142555236816406
I0202 05:32:12.072406 140022518892288 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.6240203380584717, loss=3.242114543914795
I0202 05:32:58.192299 140023005427456 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.693239688873291, loss=3.5583343505859375
I0202 05:33:44.467347 140022518892288 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.8023784160614014, loss=4.562994003295898
I0202 05:34:30.664906 140023005427456 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.560584545135498, loss=3.355259656906128
I0202 05:35:04.594329 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:35:15.285137 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:35:43.253800 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:35:44.844942 140184451094336 submission_runner.py:408] Time since start: 62361.01s, 	Step: 125075, 	{'train/accuracy': 0.7390820384025574, 'train/loss': 1.068745493888855, 'validation/accuracy': 0.6852999925613403, 'validation/loss': 1.3048429489135742, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 1.9468294382095337, 'test/num_examples': 10000, 'score': 57174.4171538353, 'total_duration': 62361.01248407364, 'accumulated_submission_time': 57174.4171538353, 'accumulated_eval_time': 5174.269753456116, 'accumulated_logging_time': 5.836784839630127}
I0202 05:35:44.883340 140022518892288 logging_writer.py:48] [125075] accumulated_eval_time=5174.269753, accumulated_logging_time=5.836785, accumulated_submission_time=57174.417154, global_step=125075, preemption_count=0, score=57174.417154, test/accuracy=0.552500, test/loss=1.946829, test/num_examples=10000, total_duration=62361.012484, train/accuracy=0.739082, train/loss=1.068745, validation/accuracy=0.685300, validation/loss=1.304843, validation/num_examples=50000
I0202 05:35:55.287427 140023005427456 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.8059659004211426, loss=4.053643703460693
I0202 05:36:38.086399 140022518892288 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.7042112350463867, loss=2.793976306915283
I0202 05:37:24.269263 140023005427456 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.8893537521362305, loss=1.9501616954803467
I0202 05:38:10.858298 140022518892288 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.580264091491699, loss=2.362370491027832
I0202 05:38:56.752410 140023005427456 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.6260502338409424, loss=3.0595412254333496
I0202 05:39:42.960771 140022518892288 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.8766207695007324, loss=1.9438364505767822
I0202 05:40:28.839007 140023005427456 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.8973069190979004, loss=1.8246805667877197
I0202 05:41:14.802671 140022518892288 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.7162489891052246, loss=3.153205394744873
I0202 05:42:00.868639 140023005427456 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.947366237640381, loss=2.1323044300079346
I0202 05:42:45.141495 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:42:55.698108 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:43:18.536575 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:43:20.141189 140184451094336 submission_runner.py:408] Time since start: 62816.31s, 	Step: 125998, 	{'train/accuracy': 0.7458398342132568, 'train/loss': 1.0124694108963013, 'validation/accuracy': 0.6858400106430054, 'validation/loss': 1.2887572050094604, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 1.9115575551986694, 'test/num_examples': 10000, 'score': 57594.61695051193, 'total_duration': 62816.308730363846, 'accumulated_submission_time': 57594.61695051193, 'accumulated_eval_time': 5209.269439458847, 'accumulated_logging_time': 5.885339975357056}
I0202 05:43:20.179483 140022518892288 logging_writer.py:48] [125998] accumulated_eval_time=5209.269439, accumulated_logging_time=5.885340, accumulated_submission_time=57594.616951, global_step=125998, preemption_count=0, score=57594.616951, test/accuracy=0.562300, test/loss=1.911558, test/num_examples=10000, total_duration=62816.308730, train/accuracy=0.745840, train/loss=1.012469, validation/accuracy=0.685840, validation/loss=1.288757, validation/num_examples=50000
I0202 05:43:21.386507 140023005427456 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.879537582397461, loss=1.932498812675476
I0202 05:44:02.782391 140022518892288 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.005161762237549, loss=1.8186641931533813
I0202 05:44:48.904824 140023005427456 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.712437152862549, loss=2.3558669090270996
I0202 05:45:35.227271 140022518892288 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.2445101737976074, loss=1.9847242832183838
I0202 05:46:21.403122 140023005427456 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.8430447578430176, loss=1.9924726486206055
I0202 05:47:07.557194 140022518892288 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.0799171924591064, loss=1.9457061290740967
I0202 05:47:53.997006 140023005427456 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.9847042560577393, loss=1.9924122095108032
I0202 05:48:40.126043 140022518892288 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.7113335132598877, loss=2.5778448581695557
I0202 05:49:26.389709 140023005427456 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.034818410873413, loss=1.9722216129302979
I0202 05:50:12.693895 140022518892288 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.259399175643921, loss=1.9667129516601562
I0202 05:50:20.148406 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:50:30.492541 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:50:57.130594 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:50:58.727114 140184451094336 submission_runner.py:408] Time since start: 63274.89s, 	Step: 126918, 	{'train/accuracy': 0.7591796517372131, 'train/loss': 0.9492820501327515, 'validation/accuracy': 0.6929000020027161, 'validation/loss': 1.2491334676742554, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.88283109664917, 'test/num_examples': 10000, 'score': 58014.5237903595, 'total_duration': 63274.89465546608, 'accumulated_submission_time': 58014.5237903595, 'accumulated_eval_time': 5247.848149061203, 'accumulated_logging_time': 5.937408208847046}
I0202 05:50:58.764057 140023005427456 logging_writer.py:48] [126918] accumulated_eval_time=5247.848149, accumulated_logging_time=5.937408, accumulated_submission_time=58014.523790, global_step=126918, preemption_count=0, score=58014.523790, test/accuracy=0.568100, test/loss=1.882831, test/num_examples=10000, total_duration=63274.894655, train/accuracy=0.759180, train/loss=0.949282, validation/accuracy=0.692900, validation/loss=1.249133, validation/num_examples=50000
I0202 05:51:32.459301 140022518892288 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.8282642364501953, loss=2.1992316246032715
I0202 05:52:18.612287 140023005427456 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.701853036880493, loss=2.0776336193084717
I0202 05:53:05.145005 140022518892288 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.5592329502105713, loss=3.547410488128662
I0202 05:53:51.222248 140023005427456 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.761573553085327, loss=2.7188761234283447
I0202 05:54:37.366919 140022518892288 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.1344783306121826, loss=1.9219731092453003
I0202 05:55:23.545693 140023005427456 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.8787543773651123, loss=1.9492192268371582
I0202 05:56:09.867853 140022518892288 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.387017011642456, loss=2.0705883502960205
I0202 05:56:56.144692 140023005427456 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.8007354736328125, loss=1.9028236865997314
I0202 05:57:42.261758 140022518892288 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.9705264568328857, loss=2.152329444885254
I0202 05:57:59.022274 140184451094336 spec.py:321] Evaluating on the training split.
I0202 05:58:09.428103 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 05:58:36.763157 140184451094336 spec.py:349] Evaluating on the test split.
I0202 05:58:38.375850 140184451094336 submission_runner.py:408] Time since start: 63734.54s, 	Step: 127838, 	{'train/accuracy': 0.7441796660423279, 'train/loss': 1.0422630310058594, 'validation/accuracy': 0.6882599592208862, 'validation/loss': 1.2880897521972656, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9071290493011475, 'test/num_examples': 10000, 'score': 58434.72530388832, 'total_duration': 63734.543369054794, 'accumulated_submission_time': 58434.72530388832, 'accumulated_eval_time': 5287.201727390289, 'accumulated_logging_time': 5.98337721824646}
I0202 05:58:38.415404 140023005427456 logging_writer.py:48] [127838] accumulated_eval_time=5287.201727, accumulated_logging_time=5.983377, accumulated_submission_time=58434.725304, global_step=127838, preemption_count=0, score=58434.725304, test/accuracy=0.568000, test/loss=1.907129, test/num_examples=10000, total_duration=63734.543369, train/accuracy=0.744180, train/loss=1.042263, validation/accuracy=0.688260, validation/loss=1.288090, validation/num_examples=50000
I0202 05:59:03.627521 140022518892288 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.865694046020508, loss=4.138256549835205
I0202 05:59:48.860704 140023005427456 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.058107376098633, loss=1.8507791757583618
I0202 06:00:35.040326 140022518892288 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.849337577819824, loss=1.8901703357696533
I0202 06:01:21.339163 140023005427456 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.749269723892212, loss=2.3236985206604004
I0202 06:02:07.406924 140022518892288 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.598832607269287, loss=3.5601744651794434
I0202 06:02:53.551527 140023005427456 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.3285579681396484, loss=4.45753812789917
I0202 06:03:39.866567 140022518892288 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.546860694885254, loss=3.088139772415161
I0202 06:04:25.800832 140023005427456 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.8042337894439697, loss=4.323930740356445
I0202 06:05:12.134468 140022518892288 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.7560465335845947, loss=2.0804011821746826
I0202 06:05:38.566835 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:05:49.225877 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:06:17.024243 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:06:18.628079 140184451094336 submission_runner.py:408] Time since start: 64194.80s, 	Step: 128759, 	{'train/accuracy': 0.74853515625, 'train/loss': 1.0025569200515747, 'validation/accuracy': 0.6908800005912781, 'validation/loss': 1.2619134187698364, 'validation/num_examples': 50000, 'test/accuracy': 0.5669000148773193, 'test/loss': 1.8810703754425049, 'test/num_examples': 10000, 'score': 58854.81752896309, 'total_duration': 64194.79561638832, 'accumulated_submission_time': 58854.81752896309, 'accumulated_eval_time': 5327.26294875145, 'accumulated_logging_time': 6.034414768218994}
I0202 06:06:18.668487 140023005427456 logging_writer.py:48] [128759] accumulated_eval_time=5327.262949, accumulated_logging_time=6.034415, accumulated_submission_time=58854.817529, global_step=128759, preemption_count=0, score=58854.817529, test/accuracy=0.566900, test/loss=1.881070, test/num_examples=10000, total_duration=64194.795616, train/accuracy=0.748535, train/loss=1.002557, validation/accuracy=0.690880, validation/loss=1.261913, validation/num_examples=50000
I0202 06:06:35.467773 140022518892288 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1069321632385254, loss=2.097848653793335
I0202 06:07:19.637797 140023005427456 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.0464322566986084, loss=1.9549821615219116
I0202 06:08:05.807647 140022518892288 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.8317556381225586, loss=2.3101089000701904
I0202 06:08:52.069657 140023005427456 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.832907199859619, loss=2.365548610687256
I0202 06:09:38.135085 140022518892288 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.895334243774414, loss=1.8426134586334229
I0202 06:10:24.086113 140023005427456 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.1171042919158936, loss=1.8712573051452637
I0202 06:11:10.308531 140022518892288 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.2131567001342773, loss=4.430474281311035
I0202 06:11:56.350929 140023005427456 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.757168769836426, loss=4.153066158294678
I0202 06:12:42.454633 140022518892288 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.0110158920288086, loss=2.0026779174804688
I0202 06:13:18.963874 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:13:29.567189 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:13:53.975591 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:13:55.574147 140184451094336 submission_runner.py:408] Time since start: 64651.74s, 	Step: 129680, 	{'train/accuracy': 0.7611327767372131, 'train/loss': 0.9462226629257202, 'validation/accuracy': 0.6943999528884888, 'validation/loss': 1.2438348531723022, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 1.8652161359786987, 'test/num_examples': 10000, 'score': 59275.053935050964, 'total_duration': 64651.74168539047, 'accumulated_submission_time': 59275.053935050964, 'accumulated_eval_time': 5363.8732233047485, 'accumulated_logging_time': 6.085147142410278}
I0202 06:13:55.610952 140023005427456 logging_writer.py:48] [129680] accumulated_eval_time=5363.873223, accumulated_logging_time=6.085147, accumulated_submission_time=59275.053935, global_step=129680, preemption_count=0, score=59275.053935, test/accuracy=0.571500, test/loss=1.865216, test/num_examples=10000, total_duration=64651.741685, train/accuracy=0.761133, train/loss=0.946223, validation/accuracy=0.694400, validation/loss=1.243835, validation/num_examples=50000
I0202 06:14:04.002553 140022518892288 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.0067689418792725, loss=1.949231743812561
I0202 06:14:46.523334 140023005427456 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.8640968799591064, loss=2.9538612365722656
I0202 06:15:32.759117 140022518892288 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.031611204147339, loss=1.933922290802002
I0202 06:16:19.102815 140023005427456 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.995861530303955, loss=2.0964980125427246
I0202 06:17:05.114291 140022518892288 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.387594223022461, loss=1.918437123298645
I0202 06:17:51.434688 140023005427456 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.094879627227783, loss=2.004446506500244
I0202 06:18:37.487777 140022518892288 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.9552245140075684, loss=3.0947787761688232
I0202 06:19:23.888686 140023005427456 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.1585612297058105, loss=2.696406841278076
I0202 06:20:10.127796 140022518892288 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.7782280445098877, loss=2.28660249710083
I0202 06:20:55.962992 140023005427456 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.029825448989868, loss=2.0926570892333984
I0202 06:20:55.977321 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:21:06.550936 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:21:31.280712 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:21:32.884497 140184451094336 submission_runner.py:408] Time since start: 65109.05s, 	Step: 130601, 	{'train/accuracy': 0.7528710961341858, 'train/loss': 0.9790438413619995, 'validation/accuracy': 0.6981199979782104, 'validation/loss': 1.2228922843933105, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.8345760107040405, 'test/num_examples': 10000, 'score': 59695.36253666878, 'total_duration': 65109.05203604698, 'accumulated_submission_time': 59695.36253666878, 'accumulated_eval_time': 5400.780389070511, 'accumulated_logging_time': 6.132253170013428}
I0202 06:21:32.922720 140022518892288 logging_writer.py:48] [130601] accumulated_eval_time=5400.780389, accumulated_logging_time=6.132253, accumulated_submission_time=59695.362537, global_step=130601, preemption_count=0, score=59695.362537, test/accuracy=0.575600, test/loss=1.834576, test/num_examples=10000, total_duration=65109.052036, train/accuracy=0.752871, train/loss=0.979044, validation/accuracy=0.698120, validation/loss=1.222892, validation/num_examples=50000
I0202 06:22:14.412791 140023005427456 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.923027276992798, loss=3.690908432006836
I0202 06:23:00.392439 140022518892288 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.5683493614196777, loss=2.6155846118927
I0202 06:23:46.701831 140023005427456 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.9969518184661865, loss=1.9955414533615112
I0202 06:24:32.778141 140022518892288 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.2740039825439453, loss=2.036736249923706
I0202 06:25:18.855518 140023005427456 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.2568418979644775, loss=1.9984071254730225
I0202 06:26:05.029520 140022518892288 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.554896354675293, loss=4.5273637771606445
I0202 06:26:50.770336 140023005427456 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.9530317783355713, loss=2.6931211948394775
I0202 06:27:37.046588 140022518892288 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.452066659927368, loss=3.719261884689331
I0202 06:28:22.957284 140023005427456 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.932673692703247, loss=4.207891464233398
I0202 06:28:33.315244 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:28:43.641484 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:29:09.935882 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:29:11.533399 140184451094336 submission_runner.py:408] Time since start: 65567.70s, 	Step: 131524, 	{'train/accuracy': 0.7593945264816284, 'train/loss': 0.9528237581253052, 'validation/accuracy': 0.695639967918396, 'validation/loss': 1.2331786155700684, 'validation/num_examples': 50000, 'test/accuracy': 0.5743000507354736, 'test/loss': 1.8481074571609497, 'test/num_examples': 10000, 'score': 60115.69743990898, 'total_duration': 65567.70094275475, 'accumulated_submission_time': 60115.69743990898, 'accumulated_eval_time': 5438.998530864716, 'accumulated_logging_time': 6.179803848266602}
I0202 06:29:11.573564 140022518892288 logging_writer.py:48] [131524] accumulated_eval_time=5438.998531, accumulated_logging_time=6.179804, accumulated_submission_time=60115.697440, global_step=131524, preemption_count=0, score=60115.697440, test/accuracy=0.574300, test/loss=1.848107, test/num_examples=10000, total_duration=65567.700943, train/accuracy=0.759395, train/loss=0.952824, validation/accuracy=0.695640, validation/loss=1.233179, validation/num_examples=50000
I0202 06:29:42.341221 140023005427456 logging_writer.py:48] [131600] global_step=131600, grad_norm=4.209209442138672, loss=1.9150543212890625
I0202 06:30:28.389069 140022518892288 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.95747971534729, loss=1.9160702228546143
I0202 06:31:14.834995 140023005427456 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.4837870597839355, loss=4.361546039581299
I0202 06:32:01.162565 140022518892288 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.0599164962768555, loss=4.329468727111816
I0202 06:32:47.408561 140023005427456 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.9470832347869873, loss=3.097625255584717
I0202 06:33:33.576124 140022518892288 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.0962560176849365, loss=3.498318910598755
I0202 06:34:19.898538 140023005427456 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.2041776180267334, loss=2.048255443572998
I0202 06:35:06.081151 140022518892288 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.263533115386963, loss=2.045590400695801
I0202 06:35:52.174877 140023005427456 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.9187564849853516, loss=2.8840668201446533
I0202 06:36:11.631421 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:36:22.194422 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:36:49.946899 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:36:51.545842 140184451094336 submission_runner.py:408] Time since start: 66027.71s, 	Step: 132444, 	{'train/accuracy': 0.7639452815055847, 'train/loss': 0.9388325214385986, 'validation/accuracy': 0.6972399950027466, 'validation/loss': 1.2304061651229858, 'validation/num_examples': 50000, 'test/accuracy': 0.5740000009536743, 'test/loss': 1.8517664670944214, 'test/num_examples': 10000, 'score': 60535.69753456116, 'total_duration': 66027.71335840225, 'accumulated_submission_time': 60535.69753456116, 'accumulated_eval_time': 5478.9129366874695, 'accumulated_logging_time': 6.230070352554321}
I0202 06:36:51.583962 140022518892288 logging_writer.py:48] [132444] accumulated_eval_time=5478.912937, accumulated_logging_time=6.230070, accumulated_submission_time=60535.697535, global_step=132444, preemption_count=0, score=60535.697535, test/accuracy=0.574000, test/loss=1.851766, test/num_examples=10000, total_duration=66027.713358, train/accuracy=0.763945, train/loss=0.938833, validation/accuracy=0.697240, validation/loss=1.230406, validation/num_examples=50000
I0202 06:37:14.370625 140023005427456 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.1822357177734375, loss=1.9663841724395752
I0202 06:37:59.248718 140022518892288 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.42053484916687, loss=2.101579189300537
I0202 06:38:45.740525 140023005427456 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.7597272396087646, loss=2.7432663440704346
I0202 06:39:32.094374 140022518892288 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.356828212738037, loss=2.197287082672119
I0202 06:40:18.317532 140023005427456 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.1216788291931152, loss=1.7922368049621582
I0202 06:41:04.531657 140022518892288 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.313915967941284, loss=2.0660176277160645
I0202 06:41:50.879123 140023005427456 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.1267449855804443, loss=3.042104721069336
I0202 06:42:37.079642 140022518892288 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.822596311569214, loss=2.513444662094116
I0202 06:43:23.090559 140023005427456 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.0651021003723145, loss=2.7777371406555176
I0202 06:43:51.765262 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:44:02.143396 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:44:27.613794 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:44:29.210245 140184451094336 submission_runner.py:408] Time since start: 66485.38s, 	Step: 133364, 	{'train/accuracy': 0.7642187476158142, 'train/loss': 0.9470322132110596, 'validation/accuracy': 0.7019000053405762, 'validation/loss': 1.2145510911941528, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 1.8425655364990234, 'test/num_examples': 10000, 'score': 60955.82214999199, 'total_duration': 66485.37777233124, 'accumulated_submission_time': 60955.82214999199, 'accumulated_eval_time': 5516.357885599136, 'accumulated_logging_time': 6.277270317077637}
I0202 06:44:29.249501 140022518892288 logging_writer.py:48] [133364] accumulated_eval_time=5516.357886, accumulated_logging_time=6.277270, accumulated_submission_time=60955.822150, global_step=133364, preemption_count=0, score=60955.822150, test/accuracy=0.573400, test/loss=1.842566, test/num_examples=10000, total_duration=66485.377772, train/accuracy=0.764219, train/loss=0.947032, validation/accuracy=0.701900, validation/loss=1.214551, validation/num_examples=50000
I0202 06:44:44.048078 140023005427456 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.048957347869873, loss=1.9573962688446045
I0202 06:45:27.666530 140022518892288 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3072023391723633, loss=1.7489523887634277
I0202 06:46:13.992090 140023005427456 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.2194299697875977, loss=2.195542573928833
I0202 06:47:00.288326 140022518892288 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.2327263355255127, loss=2.0636329650878906
I0202 06:47:46.459645 140023005427456 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.0982861518859863, loss=1.8351844549179077
I0202 06:48:32.665797 140022518892288 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.405707836151123, loss=1.825535774230957
I0202 06:49:19.146455 140023005427456 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.978313446044922, loss=2.816054105758667
I0202 06:50:05.278555 140022518892288 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.009896993637085, loss=2.570148468017578
I0202 06:50:51.523460 140023005427456 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.239828586578369, loss=1.8006073236465454
I0202 06:51:29.513516 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:51:39.991527 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:52:06.695724 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:52:08.300000 140184451094336 submission_runner.py:408] Time since start: 66944.47s, 	Step: 134284, 	{'train/accuracy': 0.7623632550239563, 'train/loss': 0.9518302083015442, 'validation/accuracy': 0.7013999819755554, 'validation/loss': 1.2288814783096313, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8447034358978271, 'test/num_examples': 10000, 'score': 61376.02754378319, 'total_duration': 66944.4675412178, 'accumulated_submission_time': 61376.02754378319, 'accumulated_eval_time': 5555.144359827042, 'accumulated_logging_time': 6.326719284057617}
I0202 06:52:08.339657 140022518892288 logging_writer.py:48] [134284] accumulated_eval_time=5555.144360, accumulated_logging_time=6.326719, accumulated_submission_time=61376.027544, global_step=134284, preemption_count=0, score=61376.027544, test/accuracy=0.578100, test/loss=1.844703, test/num_examples=10000, total_duration=66944.467541, train/accuracy=0.762363, train/loss=0.951830, validation/accuracy=0.701400, validation/loss=1.228881, validation/num_examples=50000
I0202 06:52:15.157613 140023005427456 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.9061055183410645, loss=3.3545446395874023
I0202 06:52:57.528056 140022518892288 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.555375337600708, loss=3.884687662124634
I0202 06:53:44.025523 140023005427456 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.9599220752716064, loss=3.77925705909729
I0202 06:54:30.924413 140022518892288 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.02060604095459, loss=4.116394519805908
I0202 06:55:17.204479 140023005427456 logging_writer.py:48] [134700] global_step=134700, grad_norm=4.12185525894165, loss=4.335134506225586
I0202 06:56:03.251147 140022518892288 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.303302764892578, loss=1.7947885990142822
I0202 06:56:49.373204 140023005427456 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.823498487472534, loss=2.329360008239746
I0202 06:57:35.694831 140022518892288 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.0740818977355957, loss=1.6998752355575562
I0202 06:58:22.092729 140023005427456 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.1798298358917236, loss=1.8915269374847412
I0202 06:59:08.435665 140022518892288 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.9940366744995117, loss=2.2858262062072754
I0202 06:59:08.449573 140184451094336 spec.py:321] Evaluating on the training split.
I0202 06:59:18.977385 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 06:59:43.903782 140184451094336 spec.py:349] Evaluating on the test split.
I0202 06:59:45.499459 140184451094336 submission_runner.py:408] Time since start: 67401.67s, 	Step: 135201, 	{'train/accuracy': 0.7685546875, 'train/loss': 0.9147396087646484, 'validation/accuracy': 0.7045199871063232, 'validation/loss': 1.204154372215271, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.818007469177246, 'test/num_examples': 10000, 'score': 61796.07824969292, 'total_duration': 67401.66700196266, 'accumulated_submission_time': 61796.07824969292, 'accumulated_eval_time': 5592.194238185883, 'accumulated_logging_time': 6.378962516784668}
I0202 06:59:45.543459 140023005427456 logging_writer.py:48] [135201] accumulated_eval_time=5592.194238, accumulated_logging_time=6.378963, accumulated_submission_time=61796.078250, global_step=135201, preemption_count=0, score=61796.078250, test/accuracy=0.580400, test/loss=1.818007, test/num_examples=10000, total_duration=67401.667002, train/accuracy=0.768555, train/loss=0.914740, validation/accuracy=0.704520, validation/loss=1.204154, validation/num_examples=50000
I0202 07:00:26.843934 140022518892288 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.32307767868042, loss=1.7951436042785645
I0202 07:01:13.030114 140023005427456 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.9860968589782715, loss=3.4715847969055176
I0202 07:01:59.668774 140022518892288 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.980837345123291, loss=2.2871005535125732
I0202 07:02:45.838050 140023005427456 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.555802822113037, loss=1.9301588535308838
I0202 07:03:31.867916 140022518892288 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.9508469104766846, loss=2.7188560962677
I0202 07:04:17.822016 140023005427456 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.3153042793273926, loss=1.91036057472229
I0202 07:05:03.899931 140022518892288 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.3174808025360107, loss=1.787561297416687
I0202 07:05:50.053485 140023005427456 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.5068624019622803, loss=2.225207805633545
I0202 07:06:36.128841 140022518892288 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.231869697570801, loss=2.449897050857544
I0202 07:06:45.597841 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:06:55.936280 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:07:22.531419 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:07:24.129233 140184451094336 submission_runner.py:408] Time since start: 67860.30s, 	Step: 136122, 	{'train/accuracy': 0.7792773246765137, 'train/loss': 0.8840520977973938, 'validation/accuracy': 0.7030199766159058, 'validation/loss': 1.2064038515090942, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.8282631635665894, 'test/num_examples': 10000, 'score': 62216.07617998123, 'total_duration': 67860.29677629471, 'accumulated_submission_time': 62216.07617998123, 'accumulated_eval_time': 5630.725626945496, 'accumulated_logging_time': 6.431999683380127}
I0202 07:07:24.166464 140023005427456 logging_writer.py:48] [136122] accumulated_eval_time=5630.725627, accumulated_logging_time=6.432000, accumulated_submission_time=62216.076180, global_step=136122, preemption_count=0, score=62216.076180, test/accuracy=0.581500, test/loss=1.828263, test/num_examples=10000, total_duration=67860.296776, train/accuracy=0.779277, train/loss=0.884052, validation/accuracy=0.703020, validation/loss=1.206404, validation/num_examples=50000
I0202 07:07:56.134752 140022518892288 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.010709524154663, loss=3.7154808044433594
I0202 07:08:42.266998 140023005427456 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.6620562076568604, loss=1.7791767120361328
I0202 07:09:28.629381 140022518892288 logging_writer.py:48] [136400] global_step=136400, grad_norm=4.089421272277832, loss=3.906433343887329
I0202 07:10:14.972823 140023005427456 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.229005813598633, loss=2.2021796703338623
I0202 07:11:01.113386 140022518892288 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.2918074131011963, loss=2.3843913078308105
I0202 07:11:47.378782 140023005427456 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.2998108863830566, loss=1.8534952402114868
I0202 07:12:33.660937 140022518892288 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.677380084991455, loss=1.8511931896209717
I0202 07:13:19.867824 140023005427456 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.400721788406372, loss=3.771435260772705
I0202 07:14:05.955783 140022518892288 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.5261542797088623, loss=1.8089021444320679
I0202 07:14:24.569383 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:14:35.073436 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:15:03.918034 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:15:05.520583 140184451094336 submission_runner.py:408] Time since start: 68321.69s, 	Step: 137042, 	{'train/accuracy': 0.765625, 'train/loss': 0.9307951331138611, 'validation/accuracy': 0.7046799659729004, 'validation/loss': 1.205407738685608, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 1.8337697982788086, 'test/num_examples': 10000, 'score': 62636.41758394241, 'total_duration': 68321.68812680244, 'accumulated_submission_time': 62636.41758394241, 'accumulated_eval_time': 5671.676826477051, 'accumulated_logging_time': 6.483324766159058}
I0202 07:15:05.556857 140023005427456 logging_writer.py:48] [137042] accumulated_eval_time=5671.676826, accumulated_logging_time=6.483325, accumulated_submission_time=62636.417584, global_step=137042, preemption_count=0, score=62636.417584, test/accuracy=0.577700, test/loss=1.833770, test/num_examples=10000, total_duration=68321.688127, train/accuracy=0.765625, train/loss=0.930795, validation/accuracy=0.704680, validation/loss=1.205408, validation/num_examples=50000
I0202 07:15:29.150719 140022518892288 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.5514566898345947, loss=1.6827571392059326
I0202 07:16:14.355632 140023005427456 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.3283751010894775, loss=2.494577646255493
I0202 07:17:00.585006 140022518892288 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.2755610942840576, loss=3.3944649696350098
I0202 07:17:46.810946 140023005427456 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.2172608375549316, loss=2.0787622928619385
I0202 07:18:32.866877 140022518892288 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.353335380554199, loss=1.7908985614776611
I0202 07:19:18.974086 140023005427456 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2888970375061035, loss=2.42791485786438
I0202 07:20:05.313860 140022518892288 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.6062734127044678, loss=4.232234001159668
I0202 07:20:51.396084 140023005427456 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.487095355987549, loss=1.932288646697998
I0202 07:21:37.823585 140022518892288 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.5749967098236084, loss=4.349858283996582
I0202 07:22:05.826450 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:22:16.200966 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:22:41.504536 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:22:43.098959 140184451094336 submission_runner.py:408] Time since start: 68779.27s, 	Step: 137962, 	{'train/accuracy': 0.7728710770606995, 'train/loss': 0.9098615050315857, 'validation/accuracy': 0.7051399946212769, 'validation/loss': 1.195683240890503, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.822838544845581, 'test/num_examples': 10000, 'score': 63056.63118767738, 'total_duration': 68779.26647734642, 'accumulated_submission_time': 63056.63118767738, 'accumulated_eval_time': 5708.94930100441, 'accumulated_logging_time': 6.528689861297607}
I0202 07:22:43.139373 140023005427456 logging_writer.py:48] [137962] accumulated_eval_time=5708.949301, accumulated_logging_time=6.528690, accumulated_submission_time=63056.631188, global_step=137962, preemption_count=0, score=63056.631188, test/accuracy=0.580900, test/loss=1.822839, test/num_examples=10000, total_duration=68779.266477, train/accuracy=0.772871, train/loss=0.909862, validation/accuracy=0.705140, validation/loss=1.195683, validation/num_examples=50000
I0202 07:22:58.754480 140022518892288 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.2650249004364014, loss=2.8569319248199463
I0202 07:23:42.228511 140023005427456 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.34318208694458, loss=2.1557254791259766
I0202 07:24:28.650870 140022518892288 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.9749715328216553, loss=4.230226516723633
I0202 07:25:14.937277 140023005427456 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.1189725399017334, loss=3.0106098651885986
I0202 07:26:00.664930 140022518892288 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.9920852184295654, loss=2.6906745433807373
I0202 07:26:46.754689 140023005427456 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.4085800647735596, loss=2.1381802558898926
I0202 07:27:32.855666 140022518892288 logging_writer.py:48] [138600] global_step=138600, grad_norm=4.393306732177734, loss=2.223142623901367
I0202 07:28:18.952511 140023005427456 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.2995524406433105, loss=3.1801772117614746
I0202 07:29:05.096031 140022518892288 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.306304454803467, loss=2.142651081085205
I0202 07:29:43.230209 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:29:53.600900 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:30:22.218077 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:30:23.830104 140184451094336 submission_runner.py:408] Time since start: 69240.00s, 	Step: 138884, 	{'train/accuracy': 0.7806445360183716, 'train/loss': 0.8815276026725769, 'validation/accuracy': 0.7076799869537354, 'validation/loss': 1.1955801248550415, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 1.8043678998947144, 'test/num_examples': 10000, 'score': 63476.663846731186, 'total_duration': 69239.99764561653, 'accumulated_submission_time': 63476.663846731186, 'accumulated_eval_time': 5749.549212932587, 'accumulated_logging_time': 6.578404903411865}
I0202 07:30:23.866681 140023005427456 logging_writer.py:48] [138884] accumulated_eval_time=5749.549213, accumulated_logging_time=6.578405, accumulated_submission_time=63476.663847, global_step=138884, preemption_count=0, score=63476.663847, test/accuracy=0.584200, test/loss=1.804368, test/num_examples=10000, total_duration=69239.997646, train/accuracy=0.780645, train/loss=0.881528, validation/accuracy=0.707680, validation/loss=1.195580, validation/num_examples=50000
I0202 07:30:30.671551 140022518892288 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.2258565425872803, loss=3.6317219734191895
I0202 07:31:13.352303 140023005427456 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.42305326461792, loss=1.7824245691299438
I0202 07:31:59.351223 140022518892288 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.5403225421905518, loss=2.521854877471924
I0202 07:32:45.531687 140023005427456 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.3997530937194824, loss=1.9307562112808228
I0202 07:33:31.637967 140022518892288 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.095473289489746, loss=2.00832462310791
I0202 07:34:17.854366 140023005427456 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.2959108352661133, loss=1.8212430477142334
I0202 07:35:04.206435 140022518892288 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.5662310123443604, loss=3.9262478351593018
I0202 07:35:50.097089 140023005427456 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.3810925483703613, loss=1.9384580850601196
I0202 07:36:36.109112 140022518892288 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.510037422180176, loss=1.7533738613128662
I0202 07:37:22.260205 140023005427456 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.2824392318725586, loss=2.6296868324279785
I0202 07:37:23.848614 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:37:34.254078 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:38:03.024828 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:38:04.626083 140184451094336 submission_runner.py:408] Time since start: 69700.79s, 	Step: 139805, 	{'train/accuracy': 0.771289050579071, 'train/loss': 0.9243224859237671, 'validation/accuracy': 0.7076199650764465, 'validation/loss': 1.2005985975265503, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.8228113651275635, 'test/num_examples': 10000, 'score': 63896.58831167221, 'total_duration': 69700.79360175133, 'accumulated_submission_time': 63896.58831167221, 'accumulated_eval_time': 5790.326649427414, 'accumulated_logging_time': 6.624654054641724}
I0202 07:38:04.670373 140022518892288 logging_writer.py:48] [139805] accumulated_eval_time=5790.326649, accumulated_logging_time=6.624654, accumulated_submission_time=63896.588312, global_step=139805, preemption_count=0, score=63896.588312, test/accuracy=0.584800, test/loss=1.822811, test/num_examples=10000, total_duration=69700.793602, train/accuracy=0.771289, train/loss=0.924322, validation/accuracy=0.707620, validation/loss=1.200599, validation/num_examples=50000
I0202 07:38:44.302870 140023005427456 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.6253268718719482, loss=1.9096975326538086
I0202 07:39:30.134137 140022518892288 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.2244420051574707, loss=1.7030357122421265
I0202 07:40:16.358784 140023005427456 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.4807872772216797, loss=1.792068600654602
I0202 07:41:02.436015 140022518892288 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.6693270206451416, loss=1.9557594060897827
I0202 07:41:48.702644 140023005427456 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.427419424057007, loss=1.8019877672195435
I0202 07:42:35.173480 140022518892288 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.3340532779693604, loss=1.9060137271881104
I0202 07:43:21.253351 140023005427456 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.804511547088623, loss=1.8875370025634766
I0202 07:44:07.626446 140022518892288 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.5743277072906494, loss=2.5568134784698486
I0202 07:44:53.727736 140023005427456 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.830754041671753, loss=1.7426459789276123
I0202 07:45:04.979200 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:45:15.463543 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:45:42.034222 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:45:43.635447 140184451094336 submission_runner.py:408] Time since start: 70159.80s, 	Step: 140726, 	{'train/accuracy': 0.777148425579071, 'train/loss': 0.8691675662994385, 'validation/accuracy': 0.7116000056266785, 'validation/loss': 1.1573071479797363, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.7759684324264526, 'test/num_examples': 10000, 'score': 64316.83931660652, 'total_duration': 70159.80298304558, 'accumulated_submission_time': 64316.83931660652, 'accumulated_eval_time': 5828.982895612717, 'accumulated_logging_time': 6.679133176803589}
I0202 07:45:43.674144 140022518892288 logging_writer.py:48] [140726] accumulated_eval_time=5828.982896, accumulated_logging_time=6.679133, accumulated_submission_time=64316.839317, global_step=140726, preemption_count=0, score=64316.839317, test/accuracy=0.587100, test/loss=1.775968, test/num_examples=10000, total_duration=70159.802983, train/accuracy=0.777148, train/loss=0.869168, validation/accuracy=0.711600, validation/loss=1.157307, validation/num_examples=50000
I0202 07:46:13.654355 140023005427456 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.4285335540771484, loss=2.043374538421631
I0202 07:46:59.342230 140022518892288 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.537015199661255, loss=2.8820905685424805
I0202 07:47:45.613844 140023005427456 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.607020854949951, loss=1.81638503074646
I0202 07:48:31.588321 140022518892288 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.574610710144043, loss=1.6288988590240479
I0202 07:49:17.834630 140023005427456 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.3407247066497803, loss=2.3137426376342773
I0202 07:50:04.011156 140022518892288 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.884843349456787, loss=1.7713207006454468
I0202 07:50:50.121199 140023005427456 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.6037423610687256, loss=3.0895907878875732
I0202 07:51:36.484076 140022518892288 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.9539928436279297, loss=3.5798795223236084
I0202 07:52:22.669055 140023005427456 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.5032591819763184, loss=1.8172154426574707
I0202 07:52:44.099297 140184451094336 spec.py:321] Evaluating on the training split.
I0202 07:52:55.456892 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 07:53:23.172397 140184451094336 spec.py:349] Evaluating on the test split.
I0202 07:53:24.774329 140184451094336 submission_runner.py:408] Time since start: 70620.94s, 	Step: 141648, 	{'train/accuracy': 0.7853710651397705, 'train/loss': 0.8723379969596863, 'validation/accuracy': 0.7133600115776062, 'validation/loss': 1.1855190992355347, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.806455373764038, 'test/num_examples': 10000, 'score': 64737.20595860481, 'total_duration': 70620.941873312, 'accumulated_submission_time': 64737.20595860481, 'accumulated_eval_time': 5869.657928228378, 'accumulated_logging_time': 6.726979732513428}
I0202 07:53:24.810658 140022518892288 logging_writer.py:48] [141648] accumulated_eval_time=5869.657928, accumulated_logging_time=6.726980, accumulated_submission_time=64737.205959, global_step=141648, preemption_count=0, score=64737.205959, test/accuracy=0.585000, test/loss=1.806455, test/num_examples=10000, total_duration=70620.941873, train/accuracy=0.785371, train/loss=0.872338, validation/accuracy=0.713360, validation/loss=1.185519, validation/num_examples=50000
I0202 07:53:46.402148 140023005427456 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.7578017711639404, loss=1.8414936065673828
I0202 07:54:30.954657 140022518892288 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.5436148643493652, loss=2.5562946796417236
I0202 07:55:17.164572 140023005427456 logging_writer.py:48] [141900] global_step=141900, grad_norm=4.045658588409424, loss=1.807458519935608
I0202 07:56:03.513792 140022518892288 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.383382558822632, loss=2.4420113563537598
I0202 07:56:49.555340 140023005427456 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.443361282348633, loss=1.856624722480774
I0202 07:57:35.718396 140022518892288 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.3658077716827393, loss=2.1451687812805176
I0202 07:58:22.227530 140023005427456 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.8375072479248047, loss=1.9835642576217651
I0202 07:59:08.542110 140022518892288 logging_writer.py:48] [142400] global_step=142400, grad_norm=4.072844505310059, loss=4.3166093826293945
I0202 07:59:54.598042 140023005427456 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.5903468132019043, loss=2.788719654083252
I0202 08:00:25.294973 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:00:35.771649 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:01:02.102764 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:01:03.698696 140184451094336 submission_runner.py:408] Time since start: 71079.87s, 	Step: 142567, 	{'train/accuracy': 0.7803124785423279, 'train/loss': 0.8587034344673157, 'validation/accuracy': 0.7161999940872192, 'validation/loss': 1.1331450939178467, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 1.7515387535095215, 'test/num_examples': 10000, 'score': 65157.241515636444, 'total_duration': 71079.86623930931, 'accumulated_submission_time': 65157.241515636444, 'accumulated_eval_time': 5908.061641693115, 'accumulated_logging_time': 7.164468288421631}
I0202 08:01:03.738926 140022518892288 logging_writer.py:48] [142567] accumulated_eval_time=5908.061642, accumulated_logging_time=7.164468, accumulated_submission_time=65157.241516, global_step=142567, preemption_count=0, score=65157.241516, test/accuracy=0.596000, test/loss=1.751539, test/num_examples=10000, total_duration=71079.866239, train/accuracy=0.780312, train/loss=0.858703, validation/accuracy=0.716200, validation/loss=1.133145, validation/num_examples=50000
I0202 08:01:17.329641 140023005427456 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.140988826751709, loss=2.797807455062866
I0202 08:02:01.017925 140022518892288 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.680135488510132, loss=1.7206292152404785
I0202 08:02:47.238264 140023005427456 logging_writer.py:48] [142800] global_step=142800, grad_norm=4.462905406951904, loss=1.7906520366668701
I0202 08:03:34.079623 140022518892288 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.774066209793091, loss=1.7224339246749878
I0202 08:04:20.292631 140023005427456 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.4723920822143555, loss=3.1531848907470703
I0202 08:05:06.771893 140022518892288 logging_writer.py:48] [143100] global_step=143100, grad_norm=4.600232124328613, loss=4.24819278717041
I0202 08:05:52.678736 140023005427456 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.3728103637695312, loss=2.6922836303710938
I0202 08:06:38.737685 140022518892288 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.9682624340057373, loss=3.7661526203155518
I0202 08:07:25.059538 140023005427456 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.986673355102539, loss=1.814823865890503
I0202 08:08:04.113586 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:08:14.678939 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:08:40.111626 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:08:41.708637 140184451094336 submission_runner.py:408] Time since start: 71537.88s, 	Step: 143486, 	{'train/accuracy': 0.7825976610183716, 'train/loss': 0.8492751121520996, 'validation/accuracy': 0.7155599594116211, 'validation/loss': 1.143829345703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5928000211715698, 'test/loss': 1.7593907117843628, 'test/num_examples': 10000, 'score': 65577.55893421173, 'total_duration': 71537.87617897987, 'accumulated_submission_time': 65577.55893421173, 'accumulated_eval_time': 5945.656690597534, 'accumulated_logging_time': 7.214048862457275}
I0202 08:08:41.746145 140022518892288 logging_writer.py:48] [143486] accumulated_eval_time=5945.656691, accumulated_logging_time=7.214049, accumulated_submission_time=65577.558934, global_step=143486, preemption_count=0, score=65577.558934, test/accuracy=0.592800, test/loss=1.759391, test/num_examples=10000, total_duration=71537.876179, train/accuracy=0.782598, train/loss=0.849275, validation/accuracy=0.715560, validation/loss=1.143829, validation/num_examples=50000
I0202 08:08:47.760445 140023005427456 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.738632917404175, loss=2.3709940910339355
I0202 08:09:30.208575 140022518892288 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.603764295578003, loss=2.31489634513855
I0202 08:10:16.562336 140023005427456 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.625643014907837, loss=2.2488901615142822
I0202 08:11:02.869231 140022518892288 logging_writer.py:48] [143800] global_step=143800, grad_norm=4.4875617027282715, loss=4.246708869934082
I0202 08:11:49.325518 140023005427456 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.934429883956909, loss=3.57309889793396
I0202 08:12:35.670650 140022518892288 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.8872578144073486, loss=1.7371282577514648
I0202 08:13:22.120694 140023005427456 logging_writer.py:48] [144100] global_step=144100, grad_norm=4.051441192626953, loss=4.213641166687012
I0202 08:14:08.360597 140022518892288 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.811216115951538, loss=2.8844845294952393
I0202 08:14:54.633734 140023005427456 logging_writer.py:48] [144300] global_step=144300, grad_norm=4.204989433288574, loss=1.7739307880401611
I0202 08:15:41.043027 140022518892288 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.702867031097412, loss=2.1688945293426514
I0202 08:15:42.157022 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:15:52.712434 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:16:21.758363 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:16:23.361684 140184451094336 submission_runner.py:408] Time since start: 71999.53s, 	Step: 144404, 	{'train/accuracy': 0.7892382740974426, 'train/loss': 0.8360196352005005, 'validation/accuracy': 0.7186799645423889, 'validation/loss': 1.148780107498169, 'validation/num_examples': 50000, 'test/accuracy': 0.5980000495910645, 'test/loss': 1.7610589265823364, 'test/num_examples': 10000, 'score': 65997.90977883339, 'total_duration': 71999.52923107147, 'accumulated_submission_time': 65997.90977883339, 'accumulated_eval_time': 5986.8613493442535, 'accumulated_logging_time': 7.263937711715698}
I0202 08:16:23.398773 140023005427456 logging_writer.py:48] [144404] accumulated_eval_time=5986.861349, accumulated_logging_time=7.263938, accumulated_submission_time=65997.909779, global_step=144404, preemption_count=0, score=65997.909779, test/accuracy=0.598000, test/loss=1.761059, test/num_examples=10000, total_duration=71999.529231, train/accuracy=0.789238, train/loss=0.836020, validation/accuracy=0.718680, validation/loss=1.148780, validation/num_examples=50000
I0202 08:17:03.686173 140022518892288 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.847027063369751, loss=1.7884153127670288
I0202 08:17:49.449468 140023005427456 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.9293439388275146, loss=1.704985499382019
I0202 08:18:35.713437 140022518892288 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.430388927459717, loss=1.8540315628051758
I0202 08:19:21.894640 140023005427456 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.404946804046631, loss=3.831815481185913
I0202 08:20:08.170784 140022518892288 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.498640775680542, loss=1.7185444831848145
I0202 08:20:54.263906 140023005427456 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.337077617645264, loss=3.760406970977783
I0202 08:21:40.703885 140022518892288 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.9504520893096924, loss=3.5181334018707275
I0202 08:22:26.888967 140023005427456 logging_writer.py:48] [145200] global_step=145200, grad_norm=4.073403358459473, loss=2.8641505241394043
I0202 08:23:13.117015 140022518892288 logging_writer.py:48] [145300] global_step=145300, grad_norm=4.51976203918457, loss=4.122857093811035
I0202 08:23:23.433269 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:23:34.046553 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:23:59.523456 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:24:01.131939 140184451094336 submission_runner.py:408] Time since start: 72457.30s, 	Step: 145324, 	{'train/accuracy': 0.796191394329071, 'train/loss': 0.796824038028717, 'validation/accuracy': 0.7212199568748474, 'validation/loss': 1.1199946403503418, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.755782127380371, 'test/num_examples': 10000, 'score': 66417.887434721, 'total_duration': 72457.2994647026, 'accumulated_submission_time': 66417.887434721, 'accumulated_eval_time': 6024.559996366501, 'accumulated_logging_time': 7.30958104133606}
I0202 08:24:01.188642 140023005427456 logging_writer.py:48] [145324] accumulated_eval_time=6024.559996, accumulated_logging_time=7.309581, accumulated_submission_time=66417.887435, global_step=145324, preemption_count=0, score=66417.887435, test/accuracy=0.595800, test/loss=1.755782, test/num_examples=10000, total_duration=72457.299465, train/accuracy=0.796191, train/loss=0.796824, validation/accuracy=0.721220, validation/loss=1.119995, validation/num_examples=50000
I0202 08:24:32.113010 140022518892288 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.586824417114258, loss=2.071013927459717
I0202 08:25:18.093729 140023005427456 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.860038995742798, loss=2.974576950073242
I0202 08:26:04.498843 140022518892288 logging_writer.py:48] [145600] global_step=145600, grad_norm=4.523338317871094, loss=1.5996489524841309
I0202 08:26:50.558302 140023005427456 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.831753730773926, loss=3.002933979034424
I0202 08:27:36.755126 140022518892288 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.949457883834839, loss=2.2814745903015137
I0202 08:28:22.996649 140023005427456 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.700369358062744, loss=1.678114652633667
I0202 08:29:09.029597 140022518892288 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.402334690093994, loss=3.695173740386963
I0202 08:29:55.216884 140023005427456 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.8287031650543213, loss=2.0749082565307617
I0202 08:30:41.286885 140022518892288 logging_writer.py:48] [146200] global_step=146200, grad_norm=4.1165924072265625, loss=1.739415168762207
I0202 08:31:01.284723 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:31:11.798061 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:31:38.939709 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:31:40.547056 140184451094336 submission_runner.py:408] Time since start: 72916.71s, 	Step: 146245, 	{'train/accuracy': 0.7898046970367432, 'train/loss': 0.825894832611084, 'validation/accuracy': 0.7240999937057495, 'validation/loss': 1.1160138845443726, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7424756288528442, 'test/num_examples': 10000, 'score': 66837.9214565754, 'total_duration': 72916.71459913254, 'accumulated_submission_time': 66837.9214565754, 'accumulated_eval_time': 6063.822338104248, 'accumulated_logging_time': 7.380070686340332}
I0202 08:31:40.588356 140023005427456 logging_writer.py:48] [146245] accumulated_eval_time=6063.822338, accumulated_logging_time=7.380071, accumulated_submission_time=66837.921457, global_step=146245, preemption_count=0, score=66837.921457, test/accuracy=0.601100, test/loss=1.742476, test/num_examples=10000, total_duration=72916.714599, train/accuracy=0.789805, train/loss=0.825895, validation/accuracy=0.724100, validation/loss=1.116014, validation/num_examples=50000
I0202 08:32:03.005037 140022518892288 logging_writer.py:48] [146300] global_step=146300, grad_norm=4.183110237121582, loss=3.0785515308380127
I0202 08:32:48.001101 140023005427456 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.28774881362915, loss=2.1846489906311035
I0202 08:33:34.521934 140022518892288 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.736034631729126, loss=3.747779607772827
I0202 08:34:21.192190 140023005427456 logging_writer.py:48] [146600] global_step=146600, grad_norm=4.317473888397217, loss=3.301084280014038
I0202 08:35:07.401395 140022518892288 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.8874571323394775, loss=1.9654960632324219
I0202 08:35:53.553868 140023005427456 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.192549228668213, loss=4.03377103805542
I0202 08:36:39.858674 140022518892288 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.838865041732788, loss=3.2375802993774414
I0202 08:37:26.156199 140023005427456 logging_writer.py:48] [147000] global_step=147000, grad_norm=4.004907608032227, loss=1.810968279838562
I0202 08:38:12.312814 140022518892288 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.9534659385681152, loss=1.7388665676116943
I0202 08:38:40.662253 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:38:51.317252 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:39:20.107813 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:39:21.706164 140184451094336 submission_runner.py:408] Time since start: 73377.87s, 	Step: 147163, 	{'train/accuracy': 0.7924023270606995, 'train/loss': 0.8204847574234009, 'validation/accuracy': 0.7229200005531311, 'validation/loss': 1.114357590675354, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.7341212034225464, 'test/num_examples': 10000, 'score': 67257.93815946579, 'total_duration': 73377.8737001419, 'accumulated_submission_time': 67257.93815946579, 'accumulated_eval_time': 6104.866254329681, 'accumulated_logging_time': 7.430980443954468}
I0202 08:39:21.747645 140023005427456 logging_writer.py:48] [147163] accumulated_eval_time=6104.866254, accumulated_logging_time=7.430980, accumulated_submission_time=67257.938159, global_step=147163, preemption_count=0, score=67257.938159, test/accuracy=0.596600, test/loss=1.734121, test/num_examples=10000, total_duration=73377.873700, train/accuracy=0.792402, train/loss=0.820485, validation/accuracy=0.722920, validation/loss=1.114358, validation/num_examples=50000
I0202 08:39:37.137393 140022518892288 logging_writer.py:48] [147200] global_step=147200, grad_norm=4.10020637512207, loss=1.8846219778060913
I0202 08:40:20.868990 140023005427456 logging_writer.py:48] [147300] global_step=147300, grad_norm=4.074311256408691, loss=1.8096702098846436
I0202 08:41:07.264369 140022518892288 logging_writer.py:48] [147400] global_step=147400, grad_norm=4.07260274887085, loss=2.000021457672119
I0202 08:41:53.702415 140023005427456 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.9064507484436035, loss=1.7367099523544312
I0202 08:42:39.880764 140022518892288 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.754687547683716, loss=1.9188154935836792
I0202 08:43:26.146770 140023005427456 logging_writer.py:48] [147700] global_step=147700, grad_norm=4.442536354064941, loss=1.7339253425598145
I0202 08:44:12.362691 140022518892288 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.6624414920806885, loss=2.5391085147857666
I0202 08:44:58.305862 140023005427456 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.781493663787842, loss=1.7160634994506836
I0202 08:45:44.633697 140022518892288 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.9330828189849854, loss=2.912761688232422
I0202 08:46:21.805839 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:46:32.314901 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:47:01.661448 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:47:03.262821 140184451094336 submission_runner.py:408] Time since start: 73839.43s, 	Step: 148082, 	{'train/accuracy': 0.7983788847923279, 'train/loss': 0.792547345161438, 'validation/accuracy': 0.7223399877548218, 'validation/loss': 1.120306372642517, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.7311818599700928, 'test/num_examples': 10000, 'score': 67677.93710446358, 'total_duration': 73839.43036198616, 'accumulated_submission_time': 67677.93710446358, 'accumulated_eval_time': 6146.32323050499, 'accumulated_logging_time': 7.481757164001465}
I0202 08:47:03.300704 140023005427456 logging_writer.py:48] [148082] accumulated_eval_time=6146.323231, accumulated_logging_time=7.481757, accumulated_submission_time=67677.937104, global_step=148082, preemption_count=0, score=67677.937104, test/accuracy=0.600700, test/loss=1.731182, test/num_examples=10000, total_duration=73839.430362, train/accuracy=0.798379, train/loss=0.792547, validation/accuracy=0.722340, validation/loss=1.120306, validation/num_examples=50000
I0202 08:47:10.911479 140022518892288 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.9596290588378906, loss=1.8048081398010254
I0202 08:47:53.555374 140023005427456 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.188024044036865, loss=1.6531373262405396
I0202 08:48:39.622726 140022518892288 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.970093250274658, loss=1.831915259361267
I0202 08:49:26.289285 140023005427456 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.488133907318115, loss=1.836606502532959
I0202 08:50:12.527830 140022518892288 logging_writer.py:48] [148500] global_step=148500, grad_norm=4.0262370109558105, loss=1.80263352394104
I0202 08:50:58.616929 140023005427456 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.783773422241211, loss=3.332871437072754
I0202 08:51:45.069567 140022518892288 logging_writer.py:48] [148700] global_step=148700, grad_norm=4.058223724365234, loss=1.6858266592025757
I0202 08:52:31.485950 140023005427456 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.476060390472412, loss=1.7826402187347412
I0202 08:53:17.798311 140022518892288 logging_writer.py:48] [148900] global_step=148900, grad_norm=4.092362403869629, loss=2.21555495262146
I0202 08:54:03.296046 140184451094336 spec.py:321] Evaluating on the training split.
I0202 08:54:13.885773 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 08:54:42.876213 140184451094336 spec.py:349] Evaluating on the test split.
I0202 08:54:44.481674 140184451094336 submission_runner.py:408] Time since start: 74300.65s, 	Step: 149000, 	{'train/accuracy': 0.7928906083106995, 'train/loss': 0.8123624920845032, 'validation/accuracy': 0.7275399565696716, 'validation/loss': 1.1038259267807007, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.7259600162506104, 'test/num_examples': 10000, 'score': 68097.8760201931, 'total_duration': 74300.64919734001, 'accumulated_submission_time': 68097.8760201931, 'accumulated_eval_time': 6187.508858203888, 'accumulated_logging_time': 7.528716564178467}
I0202 08:54:44.529186 140023005427456 logging_writer.py:48] [149000] accumulated_eval_time=6187.508858, accumulated_logging_time=7.528717, accumulated_submission_time=68097.876020, global_step=149000, preemption_count=0, score=68097.876020, test/accuracy=0.604000, test/loss=1.725960, test/num_examples=10000, total_duration=74300.649197, train/accuracy=0.792891, train/loss=0.812362, validation/accuracy=0.727540, validation/loss=1.103826, validation/num_examples=50000
I0202 08:54:44.936931 140022518892288 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.591580867767334, loss=4.103639125823975
I0202 08:55:26.522637 140023005427456 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.214719295501709, loss=1.6888749599456787
I0202 08:56:12.690010 140022518892288 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.295511722564697, loss=1.6881805658340454
I0202 08:56:58.877383 140023005427456 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.9640583992004395, loss=1.6710712909698486
I0202 08:57:44.721024 140022518892288 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.9195289611816406, loss=1.568434715270996
I0202 08:58:30.716643 140023005427456 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.374942302703857, loss=1.7600536346435547
I0202 08:59:16.724238 140022518892288 logging_writer.py:48] [149600] global_step=149600, grad_norm=4.143764019012451, loss=1.7948399782180786
I0202 09:00:02.771794 140023005427456 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.485236167907715, loss=1.8155732154846191
I0202 09:00:48.763543 140022518892288 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.727942943572998, loss=4.183919906616211
I0202 09:01:34.926034 140023005427456 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.037950038909912, loss=3.4835338592529297
I0202 09:01:44.838867 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:01:55.060390 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:02:17.943115 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:02:19.568608 140184451094336 submission_runner.py:408] Time since start: 74755.74s, 	Step: 149923, 	{'train/accuracy': 0.7988671660423279, 'train/loss': 0.78203284740448, 'validation/accuracy': 0.7282399535179138, 'validation/loss': 1.0843335390090942, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.7013198137283325, 'test/num_examples': 10000, 'score': 68518.12698411942, 'total_duration': 74755.73614430428, 'accumulated_submission_time': 68518.12698411942, 'accumulated_eval_time': 6222.238587379456, 'accumulated_logging_time': 7.587629318237305}
I0202 09:02:19.606855 140022518892288 logging_writer.py:48] [149923] accumulated_eval_time=6222.238587, accumulated_logging_time=7.587629, accumulated_submission_time=68518.126984, global_step=149923, preemption_count=0, score=68518.126984, test/accuracy=0.604300, test/loss=1.701320, test/num_examples=10000, total_duration=74755.736144, train/accuracy=0.798867, train/loss=0.782033, validation/accuracy=0.728240, validation/loss=1.084334, validation/num_examples=50000
I0202 09:02:50.977981 140023005427456 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.004479885101318, loss=2.7624130249023438
I0202 09:03:36.791568 140022518892288 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.60018253326416, loss=1.7541718482971191
I0202 09:04:23.261759 140023005427456 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.2714128494262695, loss=3.498389959335327
I0202 09:05:09.503582 140022518892288 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.131317138671875, loss=2.5489180088043213
I0202 09:05:55.452047 140023005427456 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.235267639160156, loss=1.9597084522247314
I0202 09:06:41.662361 140022518892288 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.4922590255737305, loss=1.7340497970581055
I0202 09:07:27.895927 140023005427456 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.498888969421387, loss=2.0734851360321045
I0202 09:08:14.040144 140022518892288 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.217381954193115, loss=1.672903060913086
I0202 09:09:00.090417 140023005427456 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.619822978973389, loss=1.7770211696624756
I0202 09:09:20.044893 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:09:30.463536 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:10:01.629406 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:10:03.232744 140184451094336 submission_runner.py:408] Time since start: 75219.40s, 	Step: 150845, 	{'train/accuracy': 0.8016406297683716, 'train/loss': 0.7594988346099854, 'validation/accuracy': 0.7286399602890015, 'validation/loss': 1.0782972574234009, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.6799514293670654, 'test/num_examples': 10000, 'score': 68938.50674057007, 'total_duration': 75219.40028524399, 'accumulated_submission_time': 68938.50674057007, 'accumulated_eval_time': 6265.426444530487, 'accumulated_logging_time': 7.636116981506348}
I0202 09:10:03.271990 140022518892288 logging_writer.py:48] [150845] accumulated_eval_time=6265.426445, accumulated_logging_time=7.636117, accumulated_submission_time=68938.506741, global_step=150845, preemption_count=0, score=68938.506741, test/accuracy=0.604100, test/loss=1.679951, test/num_examples=10000, total_duration=75219.400285, train/accuracy=0.801641, train/loss=0.759499, validation/accuracy=0.728640, validation/loss=1.078297, validation/num_examples=50000
I0202 09:10:25.682476 140023005427456 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.191631317138672, loss=1.8354222774505615
I0202 09:11:10.125348 140022518892288 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.676916122436523, loss=2.0307345390319824
I0202 09:11:56.088020 140023005427456 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.876495122909546, loss=2.0425853729248047
I0202 09:12:42.397892 140022518892288 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.648720741271973, loss=3.9639463424682617
I0202 09:13:28.459424 140023005427456 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.19614839553833, loss=3.2135438919067383
I0202 09:14:14.637763 140022518892288 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.6053242683410645, loss=3.969709873199463
I0202 09:15:00.564565 140023005427456 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.496904373168945, loss=4.080544948577881
I0202 09:15:47.069678 140022518892288 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.031825065612793, loss=2.33133602142334
I0202 09:16:33.275915 140023005427456 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.286411285400391, loss=1.771621584892273
I0202 09:17:03.504315 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:17:13.849764 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:17:46.161622 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:17:47.758864 140184451094336 submission_runner.py:408] Time since start: 75683.93s, 	Step: 151767, 	{'train/accuracy': 0.7989062070846558, 'train/loss': 0.7856873273849487, 'validation/accuracy': 0.7301799654960632, 'validation/loss': 1.0794763565063477, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.6965135335922241, 'test/num_examples': 10000, 'score': 69358.68147015572, 'total_duration': 75683.92640280724, 'accumulated_submission_time': 69358.68147015572, 'accumulated_eval_time': 6309.68102478981, 'accumulated_logging_time': 7.684762239456177}
I0202 09:17:47.798636 140022518892288 logging_writer.py:48] [151767] accumulated_eval_time=6309.681025, accumulated_logging_time=7.684762, accumulated_submission_time=69358.681470, global_step=151767, preemption_count=0, score=69358.681470, test/accuracy=0.604100, test/loss=1.696514, test/num_examples=10000, total_duration=75683.926403, train/accuracy=0.798906, train/loss=0.785687, validation/accuracy=0.730180, validation/loss=1.079476, validation/num_examples=50000
I0202 09:18:01.589990 140023005427456 logging_writer.py:48] [151800] global_step=151800, grad_norm=5.186537742614746, loss=1.6730655431747437
I0202 09:18:44.824061 140022518892288 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.259785175323486, loss=1.8488032817840576
I0202 09:19:31.086644 140023005427456 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.6382737159729, loss=1.7729650735855103
I0202 09:20:17.426374 140022518892288 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.289571285247803, loss=1.5659122467041016
I0202 09:21:03.203297 140023005427456 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.214573860168457, loss=1.980783462524414
I0202 09:21:49.437978 140022518892288 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.5368452072143555, loss=1.5434482097625732
I0202 09:22:35.629100 140023005427456 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.529606342315674, loss=3.237940549850464
I0202 09:23:21.736064 140022518892288 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.380064487457275, loss=1.8665826320648193
I0202 09:24:07.795711 140023005427456 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.4072489738464355, loss=1.61540687084198
I0202 09:24:47.872335 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:24:58.209594 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:25:24.992400 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:25:26.595155 140184451094336 submission_runner.py:408] Time since start: 76142.76s, 	Step: 152688, 	{'train/accuracy': 0.8006835579872131, 'train/loss': 0.7903160452842712, 'validation/accuracy': 0.7302599549293518, 'validation/loss': 1.0970721244812012, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.7170206308364868, 'test/num_examples': 10000, 'score': 69778.69618415833, 'total_duration': 76142.76267409325, 'accumulated_submission_time': 69778.69618415833, 'accumulated_eval_time': 6348.403820991516, 'accumulated_logging_time': 7.73581075668335}
I0202 09:25:26.639404 140022518892288 logging_writer.py:48] [152688] accumulated_eval_time=6348.403821, accumulated_logging_time=7.735811, accumulated_submission_time=69778.696184, global_step=152688, preemption_count=0, score=69778.696184, test/accuracy=0.607700, test/loss=1.717021, test/num_examples=10000, total_duration=76142.762674, train/accuracy=0.800684, train/loss=0.790316, validation/accuracy=0.730260, validation/loss=1.097072, validation/num_examples=50000
I0202 09:25:31.851799 140023005427456 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.808617115020752, loss=3.803034782409668
I0202 09:26:14.092301 140022518892288 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.392122268676758, loss=3.5651230812072754
I0202 09:27:00.049064 140023005427456 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.385202884674072, loss=1.6102550029754639
I0202 09:27:46.343380 140022518892288 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.276394844055176, loss=1.5178099870681763
I0202 09:28:32.360934 140023005427456 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.488125801086426, loss=1.5105878114700317
I0202 09:29:18.595729 140022518892288 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.771284103393555, loss=1.6568052768707275
I0202 09:30:04.855870 140023005427456 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.74870491027832, loss=3.6323916912078857
I0202 09:30:50.638489 140022518892288 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.349521160125732, loss=1.7784079313278198
I0202 09:31:36.934580 140023005427456 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.320282936096191, loss=2.2284739017486572
I0202 09:32:23.059117 140022518892288 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.528769016265869, loss=1.6682109832763672
I0202 09:32:26.956463 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:32:37.590844 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:33:04.759403 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:33:06.356917 140184451094336 submission_runner.py:408] Time since start: 76602.52s, 	Step: 153610, 	{'train/accuracy': 0.8105077743530273, 'train/loss': 0.7433952689170837, 'validation/accuracy': 0.7332199811935425, 'validation/loss': 1.0680303573608398, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.6835938692092896, 'test/num_examples': 10000, 'score': 70198.95512890816, 'total_duration': 76602.52445936203, 'accumulated_submission_time': 70198.95512890816, 'accumulated_eval_time': 6387.8042669296265, 'accumulated_logging_time': 7.7897889614105225}
I0202 09:33:06.395640 140023005427456 logging_writer.py:48] [153610] accumulated_eval_time=6387.804267, accumulated_logging_time=7.789789, accumulated_submission_time=70198.955129, global_step=153610, preemption_count=0, score=70198.955129, test/accuracy=0.610700, test/loss=1.683594, test/num_examples=10000, total_duration=76602.524459, train/accuracy=0.810508, train/loss=0.743395, validation/accuracy=0.733220, validation/loss=1.068030, validation/num_examples=50000
I0202 09:33:43.583869 140022518892288 logging_writer.py:48] [153700] global_step=153700, grad_norm=5.115653038024902, loss=1.6545196771621704
I0202 09:34:29.483627 140023005427456 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.644935607910156, loss=2.1260154247283936
I0202 09:35:16.069736 140022518892288 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.637459754943848, loss=1.6780037879943848
I0202 09:36:02.342692 140023005427456 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.577785491943359, loss=1.5810737609863281
I0202 09:36:48.436142 140022518892288 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.649953365325928, loss=2.8496222496032715
I0202 09:37:34.663181 140023005427456 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.635721206665039, loss=1.4989718198776245
I0202 09:38:20.854088 140022518892288 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.8185014724731445, loss=1.666390061378479
I0202 09:39:06.892345 140023005427456 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.643862247467041, loss=3.551555871963501
I0202 09:39:52.872833 140022518892288 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.806998252868652, loss=3.4196126461029053
I0202 09:40:06.506055 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:40:16.863411 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:40:45.778220 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:40:47.373317 140184451094336 submission_runner.py:408] Time since start: 77063.54s, 	Step: 154531, 	{'train/accuracy': 0.8120312094688416, 'train/loss': 0.7386379241943359, 'validation/accuracy': 0.7362799644470215, 'validation/loss': 1.0651764869689941, 'validation/num_examples': 50000, 'test/accuracy': 0.6134000420570374, 'test/loss': 1.6804908514022827, 'test/num_examples': 10000, 'score': 70619.00755596161, 'total_duration': 77063.54085183144, 'accumulated_submission_time': 70619.00755596161, 'accumulated_eval_time': 6428.671512365341, 'accumulated_logging_time': 7.8374738693237305}
I0202 09:40:47.413537 140023005427456 logging_writer.py:48] [154531] accumulated_eval_time=6428.671512, accumulated_logging_time=7.837474, accumulated_submission_time=70619.007556, global_step=154531, preemption_count=0, score=70619.007556, test/accuracy=0.613400, test/loss=1.680491, test/num_examples=10000, total_duration=77063.540852, train/accuracy=0.812031, train/loss=0.738638, validation/accuracy=0.736280, validation/loss=1.065176, validation/num_examples=50000
I0202 09:41:15.388302 140022518892288 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.5890793800354, loss=2.269625425338745
I0202 09:42:00.906309 140023005427456 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.4241156578063965, loss=3.2609689235687256
I0202 09:42:46.993089 140022518892288 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.162125587463379, loss=2.18894624710083
I0202 09:43:33.441019 140023005427456 logging_writer.py:48] [154900] global_step=154900, grad_norm=5.125230312347412, loss=4.134580135345459
I0202 09:44:19.562593 140022518892288 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.512636184692383, loss=1.4670796394348145
I0202 09:45:05.831110 140023005427456 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.96126651763916, loss=2.714165449142456
I0202 09:45:51.979022 140022518892288 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.566303253173828, loss=1.6760470867156982
I0202 09:46:38.130481 140023005427456 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.97959041595459, loss=4.056009292602539
I0202 09:47:24.064944 140022518892288 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.614015102386475, loss=3.2342841625213623
I0202 09:47:47.888422 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:47:58.373486 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:48:24.070684 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:48:25.679630 140184451094336 submission_runner.py:408] Time since start: 77521.85s, 	Step: 155453, 	{'train/accuracy': 0.8109374642372131, 'train/loss': 0.7333440780639648, 'validation/accuracy': 0.7377199530601501, 'validation/loss': 1.0530829429626465, 'validation/num_examples': 50000, 'test/accuracy': 0.6100000143051147, 'test/loss': 1.6643873453140259, 'test/num_examples': 10000, 'score': 71039.42457556725, 'total_duration': 77521.84715628624, 'accumulated_submission_time': 71039.42457556725, 'accumulated_eval_time': 6466.462705373764, 'accumulated_logging_time': 7.8881707191467285}
I0202 09:48:25.728334 140023005427456 logging_writer.py:48] [155453] accumulated_eval_time=6466.462705, accumulated_logging_time=7.888171, accumulated_submission_time=71039.424576, global_step=155453, preemption_count=0, score=71039.424576, test/accuracy=0.610000, test/loss=1.664387, test/num_examples=10000, total_duration=77521.847156, train/accuracy=0.810937, train/loss=0.733344, validation/accuracy=0.737720, validation/loss=1.053083, validation/num_examples=50000
I0202 09:48:44.951949 140022518892288 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.974203109741211, loss=1.5855835676193237
I0202 09:49:29.434837 140023005427456 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.720308303833008, loss=1.6943964958190918
I0202 09:50:15.771774 140022518892288 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.683047771453857, loss=1.7431977987289429
I0202 09:51:02.275081 140023005427456 logging_writer.py:48] [155800] global_step=155800, grad_norm=5.162518501281738, loss=3.3675103187561035
I0202 09:51:48.486162 140022518892288 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.478181838989258, loss=2.0649070739746094
I0202 09:52:34.711116 140023005427456 logging_writer.py:48] [156000] global_step=156000, grad_norm=5.146500587463379, loss=3.5958216190338135
I0202 09:53:21.142425 140022518892288 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.615214824676514, loss=1.8647328615188599
I0202 09:54:07.607285 140023005427456 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.930489540100098, loss=1.6560763120651245
I0202 09:54:53.564129 140022518892288 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.519946575164795, loss=2.334397792816162
I0202 09:55:25.725697 140184451094336 spec.py:321] Evaluating on the training split.
I0202 09:55:36.039253 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 09:56:05.130144 140184451094336 spec.py:349] Evaluating on the test split.
I0202 09:56:06.740145 140184451094336 submission_runner.py:408] Time since start: 77982.91s, 	Step: 156371, 	{'train/accuracy': 0.81103515625, 'train/loss': 0.74477618932724, 'validation/accuracy': 0.737060010433197, 'validation/loss': 1.0577281713485718, 'validation/num_examples': 50000, 'test/accuracy': 0.6124000549316406, 'test/loss': 1.6711875200271606, 'test/num_examples': 10000, 'score': 71459.36435127258, 'total_duration': 77982.90768504143, 'accumulated_submission_time': 71459.36435127258, 'accumulated_eval_time': 6507.477152347565, 'accumulated_logging_time': 7.9470250606536865}
I0202 09:56:06.782265 140023005427456 logging_writer.py:48] [156371] accumulated_eval_time=6507.477152, accumulated_logging_time=7.947025, accumulated_submission_time=71459.364351, global_step=156371, preemption_count=0, score=71459.364351, test/accuracy=0.612400, test/loss=1.671188, test/num_examples=10000, total_duration=77982.907685, train/accuracy=0.811035, train/loss=0.744776, validation/accuracy=0.737060, validation/loss=1.057728, validation/num_examples=50000
I0202 09:56:18.936082 140022518892288 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.901337623596191, loss=1.5255831480026245
I0202 09:57:01.790759 140023005427456 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.908048152923584, loss=3.4226324558258057
I0202 09:57:48.195200 140022518892288 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.741331100463867, loss=3.1598541736602783
I0202 09:58:34.408971 140023005427456 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.241622447967529, loss=4.013185501098633
I0202 09:59:20.679127 140022518892288 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.468327045440674, loss=1.5930761098861694
I0202 10:00:07.069422 140023005427456 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.756398677825928, loss=2.7891955375671387
I0202 10:00:53.136968 140022518892288 logging_writer.py:48] [157000] global_step=157000, grad_norm=5.049014568328857, loss=4.019118309020996
I0202 10:01:39.642453 140023005427456 logging_writer.py:48] [157100] global_step=157100, grad_norm=5.135354995727539, loss=1.5144774913787842
I0202 10:02:25.980855 140022518892288 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.973935127258301, loss=1.5838881731033325
I0202 10:03:06.915232 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:03:17.351357 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:03:43.368554 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:03:44.964967 140184451094336 submission_runner.py:408] Time since start: 78441.13s, 	Step: 157290, 	{'train/accuracy': 0.82044917345047, 'train/loss': 0.6939221620559692, 'validation/accuracy': 0.7407599687576294, 'validation/loss': 1.043515920639038, 'validation/num_examples': 50000, 'test/accuracy': 0.6168000102043152, 'test/loss': 1.6577025651931763, 'test/num_examples': 10000, 'score': 71879.43958759308, 'total_duration': 78441.13249969482, 'accumulated_submission_time': 71879.43958759308, 'accumulated_eval_time': 6545.526882886887, 'accumulated_logging_time': 7.999570369720459}
I0202 10:03:45.006566 140023005427456 logging_writer.py:48] [157290] accumulated_eval_time=6545.526883, accumulated_logging_time=7.999570, accumulated_submission_time=71879.439588, global_step=157290, preemption_count=0, score=71879.439588, test/accuracy=0.616800, test/loss=1.657703, test/num_examples=10000, total_duration=78441.132500, train/accuracy=0.820449, train/loss=0.693922, validation/accuracy=0.740760, validation/loss=1.043516, validation/num_examples=50000
I0202 10:03:49.414900 140022518892288 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.958664417266846, loss=3.143470287322998
I0202 10:04:31.613373 140023005427456 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.473292827606201, loss=2.45611834526062
I0202 10:05:17.752364 140022518892288 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.831863880157471, loss=2.6141841411590576
I0202 10:06:04.132702 140023005427456 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.205650329589844, loss=1.9961435794830322
I0202 10:06:50.536593 140022518892288 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.7956929206848145, loss=3.074981689453125
I0202 10:07:36.725095 140023005427456 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.773951530456543, loss=1.5178325176239014
I0202 10:08:22.930666 140022518892288 logging_writer.py:48] [157900] global_step=157900, grad_norm=6.351993560791016, loss=4.033379554748535
I0202 10:09:08.964446 140023005427456 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.606734752655029, loss=2.7182528972625732
I0202 10:09:55.438775 140022518892288 logging_writer.py:48] [158100] global_step=158100, grad_norm=5.102044105529785, loss=1.6808977127075195
I0202 10:10:41.680620 140023005427456 logging_writer.py:48] [158200] global_step=158200, grad_norm=6.118121147155762, loss=4.004695892333984
I0202 10:10:45.007096 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:10:55.628785 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:11:23.151620 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:11:24.752935 140184451094336 submission_runner.py:408] Time since start: 78900.92s, 	Step: 158209, 	{'train/accuracy': 0.8133788704872131, 'train/loss': 0.7271475791931152, 'validation/accuracy': 0.7434799671173096, 'validation/loss': 1.0289294719696045, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.6337950229644775, 'test/num_examples': 10000, 'score': 72299.37812900543, 'total_duration': 78900.9204583168, 'accumulated_submission_time': 72299.37812900543, 'accumulated_eval_time': 6585.272683382034, 'accumulated_logging_time': 8.054925918579102}
I0202 10:11:24.801931 140022518892288 logging_writer.py:48] [158209] accumulated_eval_time=6585.272683, accumulated_logging_time=8.054926, accumulated_submission_time=72299.378129, global_step=158209, preemption_count=0, score=72299.378129, test/accuracy=0.618300, test/loss=1.633795, test/num_examples=10000, total_duration=78900.920458, train/accuracy=0.813379, train/loss=0.727148, validation/accuracy=0.743480, validation/loss=1.028929, validation/num_examples=50000
I0202 10:12:02.799904 140023005427456 logging_writer.py:48] [158300] global_step=158300, grad_norm=5.275259971618652, loss=1.606550931930542
I0202 10:12:48.819929 140022518892288 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.648889541625977, loss=3.101938247680664
I0202 10:13:34.820169 140023005427456 logging_writer.py:48] [158500] global_step=158500, grad_norm=6.145980358123779, loss=3.921416759490967
I0202 10:14:20.996756 140022518892288 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.6092424392700195, loss=1.9095813035964966
I0202 10:15:07.246690 140023005427456 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.601320743560791, loss=1.6680219173431396
I0202 10:15:53.480824 140022518892288 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.733772277832031, loss=1.4912315607070923
I0202 10:16:39.887289 140023005427456 logging_writer.py:48] [158900] global_step=158900, grad_norm=5.005405902862549, loss=1.566863775253296
I0202 10:17:26.103148 140022518892288 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.807583332061768, loss=1.4851322174072266
I0202 10:18:12.278427 140023005427456 logging_writer.py:48] [159100] global_step=159100, grad_norm=5.516959190368652, loss=3.30642032623291
I0202 10:18:24.921548 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:18:35.302899 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:19:05.038753 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:19:06.640972 140184451094336 submission_runner.py:408] Time since start: 79362.81s, 	Step: 159129, 	{'train/accuracy': 0.8163671493530273, 'train/loss': 0.6988460421562195, 'validation/accuracy': 0.745419979095459, 'validation/loss': 1.0122644901275635, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.6151468753814697, 'test/num_examples': 10000, 'score': 72719.4393901825, 'total_duration': 79362.80851197243, 'accumulated_submission_time': 72719.4393901825, 'accumulated_eval_time': 6626.9921362400055, 'accumulated_logging_time': 8.114330053329468}
I0202 10:19:06.681005 140022518892288 logging_writer.py:48] [159129] accumulated_eval_time=6626.992136, accumulated_logging_time=8.114330, accumulated_submission_time=72719.439390, global_step=159129, preemption_count=0, score=72719.439390, test/accuracy=0.619300, test/loss=1.615147, test/num_examples=10000, total_duration=79362.808512, train/accuracy=0.816367, train/loss=0.698846, validation/accuracy=0.745420, validation/loss=1.012264, validation/num_examples=50000
I0202 10:19:35.470645 140023005427456 logging_writer.py:48] [159200] global_step=159200, grad_norm=5.414419651031494, loss=3.65232515335083
I0202 10:20:21.160550 140022518892288 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.99403715133667, loss=1.6709246635437012
I0202 10:21:07.239600 140023005427456 logging_writer.py:48] [159400] global_step=159400, grad_norm=5.12007999420166, loss=1.6922931671142578
I0202 10:21:53.342058 140022518892288 logging_writer.py:48] [159500] global_step=159500, grad_norm=5.411773204803467, loss=1.7696195840835571
I0202 10:22:39.400419 140023005427456 logging_writer.py:48] [159600] global_step=159600, grad_norm=5.037550926208496, loss=1.5378351211547852
I0202 10:23:25.403900 140022518892288 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.997057914733887, loss=2.7663164138793945
I0202 10:24:11.461544 140023005427456 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.6894049644470215, loss=2.6945087909698486
I0202 10:24:57.737219 140022518892288 logging_writer.py:48] [159900] global_step=159900, grad_norm=5.304183483123779, loss=1.4891506433486938
I0202 10:25:43.773458 140023005427456 logging_writer.py:48] [160000] global_step=160000, grad_norm=6.131483554840088, loss=3.6327757835388184
I0202 10:26:07.048881 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:26:17.801376 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:26:46.524211 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:26:48.127528 140184451094336 submission_runner.py:408] Time since start: 79824.30s, 	Step: 160052, 	{'train/accuracy': 0.8247265219688416, 'train/loss': 0.6779054403305054, 'validation/accuracy': 0.7456199526786804, 'validation/loss': 1.016263723373413, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.628363013267517, 'test/num_examples': 10000, 'score': 73139.74932837486, 'total_duration': 79824.29507088661, 'accumulated_submission_time': 73139.74932837486, 'accumulated_eval_time': 6668.070779085159, 'accumulated_logging_time': 8.16342830657959}
I0202 10:26:48.169992 140022518892288 logging_writer.py:48] [160052] accumulated_eval_time=6668.070779, accumulated_logging_time=8.163428, accumulated_submission_time=73139.749328, global_step=160052, preemption_count=0, score=73139.749328, test/accuracy=0.623600, test/loss=1.628363, test/num_examples=10000, total_duration=79824.295071, train/accuracy=0.824727, train/loss=0.677905, validation/accuracy=0.745620, validation/loss=1.016264, validation/num_examples=50000
I0202 10:27:07.779869 140023005427456 logging_writer.py:48] [160100] global_step=160100, grad_norm=6.111697673797607, loss=1.6460288763046265
I0202 10:27:52.203069 140022518892288 logging_writer.py:48] [160200] global_step=160200, grad_norm=5.364865303039551, loss=1.551552176475525
I0202 10:28:38.470966 140023005427456 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.856235027313232, loss=1.5155255794525146
I0202 10:29:24.937285 140022518892288 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.919597625732422, loss=1.51662278175354
I0202 10:30:11.130708 140023005427456 logging_writer.py:48] [160500] global_step=160500, grad_norm=5.451132297515869, loss=1.5073959827423096
I0202 10:30:57.180279 140022518892288 logging_writer.py:48] [160600] global_step=160600, grad_norm=5.2550177574157715, loss=1.604113221168518
I0202 10:31:43.536712 140023005427456 logging_writer.py:48] [160700] global_step=160700, grad_norm=5.146165370941162, loss=1.535785436630249
I0202 10:32:29.777197 140022518892288 logging_writer.py:48] [160800] global_step=160800, grad_norm=5.141114711761475, loss=2.8679049015045166
I0202 10:33:15.921911 140023005427456 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.817629337310791, loss=2.1712217330932617
I0202 10:33:48.281626 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:33:58.732896 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:34:24.114335 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:34:25.716017 140184451094336 submission_runner.py:408] Time since start: 80281.88s, 	Step: 160972, 	{'train/accuracy': 0.8173242211341858, 'train/loss': 0.7082533240318298, 'validation/accuracy': 0.745199978351593, 'validation/loss': 1.0230603218078613, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.6267614364624023, 'test/num_examples': 10000, 'score': 73559.80233550072, 'total_duration': 80281.88353705406, 'accumulated_submission_time': 73559.80233550072, 'accumulated_eval_time': 6705.505133152008, 'accumulated_logging_time': 8.21670150756836}
I0202 10:34:25.765510 140022518892288 logging_writer.py:48] [160972] accumulated_eval_time=6705.505133, accumulated_logging_time=8.216702, accumulated_submission_time=73559.802336, global_step=160972, preemption_count=0, score=73559.802336, test/accuracy=0.621200, test/loss=1.626761, test/num_examples=10000, total_duration=80281.883537, train/accuracy=0.817324, train/loss=0.708253, validation/accuracy=0.745200, validation/loss=1.023060, validation/num_examples=50000
I0202 10:34:37.375463 140023005427456 logging_writer.py:48] [161000] global_step=161000, grad_norm=5.015244007110596, loss=2.936929702758789
I0202 10:35:20.569697 140022518892288 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.927443981170654, loss=1.7541587352752686
I0202 10:36:06.817918 140023005427456 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.890829563140869, loss=2.1343231201171875
I0202 10:36:52.857528 140022518892288 logging_writer.py:48] [161300] global_step=161300, grad_norm=5.545181751251221, loss=1.5531400442123413
I0202 10:37:39.370480 140023005427456 logging_writer.py:48] [161400] global_step=161400, grad_norm=5.173217296600342, loss=2.6949081420898438
I0202 10:38:25.731775 140022518892288 logging_writer.py:48] [161500] global_step=161500, grad_norm=5.326252460479736, loss=3.373444080352783
I0202 10:39:11.971045 140023005427456 logging_writer.py:48] [161600] global_step=161600, grad_norm=5.59737491607666, loss=1.533928632736206
I0202 10:39:58.038733 140022518892288 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.533392906188965, loss=2.4230828285217285
I0202 10:40:44.395462 140023005427456 logging_writer.py:48] [161800] global_step=161800, grad_norm=6.531078815460205, loss=3.9707188606262207
I0202 10:41:25.718608 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:41:36.160894 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:42:06.506737 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:42:08.105910 140184451094336 submission_runner.py:408] Time since start: 80744.27s, 	Step: 161891, 	{'train/accuracy': 0.8249804377555847, 'train/loss': 0.6753319501876831, 'validation/accuracy': 0.7469199895858765, 'validation/loss': 1.015852451324463, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.627989649772644, 'test/num_examples': 10000, 'score': 73979.69502210617, 'total_duration': 80744.27344155312, 'accumulated_submission_time': 73979.69502210617, 'accumulated_eval_time': 6747.892434120178, 'accumulated_logging_time': 8.278788328170776}
I0202 10:42:08.149464 140022518892288 logging_writer.py:48] [161891] accumulated_eval_time=6747.892434, accumulated_logging_time=8.278788, accumulated_submission_time=73979.695022, global_step=161891, preemption_count=0, score=73979.695022, test/accuracy=0.619300, test/loss=1.627990, test/num_examples=10000, total_duration=80744.273442, train/accuracy=0.824980, train/loss=0.675332, validation/accuracy=0.746920, validation/loss=1.015852, validation/num_examples=50000
I0202 10:42:12.147034 140023005427456 logging_writer.py:48] [161900] global_step=161900, grad_norm=5.351461410522461, loss=2.1225786209106445
I0202 10:42:54.081652 140022518892288 logging_writer.py:48] [162000] global_step=162000, grad_norm=5.574371337890625, loss=3.2506496906280518
I0202 10:43:40.082918 140023005427456 logging_writer.py:48] [162100] global_step=162100, grad_norm=5.307766914367676, loss=2.076967716217041
I0202 10:44:26.439556 140022518892288 logging_writer.py:48] [162200] global_step=162200, grad_norm=5.126891136169434, loss=1.6614903211593628
I0202 10:45:12.851243 140023005427456 logging_writer.py:48] [162300] global_step=162300, grad_norm=5.538879871368408, loss=1.9486005306243896
I0202 10:45:59.082506 140022518892288 logging_writer.py:48] [162400] global_step=162400, grad_norm=5.278342247009277, loss=1.3710802793502808
I0202 10:46:45.118490 140023005427456 logging_writer.py:48] [162500] global_step=162500, grad_norm=6.183803558349609, loss=3.904332399368286
I0202 10:47:31.340324 140022518892288 logging_writer.py:48] [162600] global_step=162600, grad_norm=5.076685905456543, loss=2.223712205886841
I0202 10:48:17.491159 140023005427456 logging_writer.py:48] [162700] global_step=162700, grad_norm=6.066912651062012, loss=3.451317310333252
I0202 10:49:03.635507 140022518892288 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.4035162925720215, loss=2.2682430744171143
I0202 10:49:08.386061 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:49:18.740633 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:49:45.754244 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:49:47.365150 140184451094336 submission_runner.py:408] Time since start: 81203.53s, 	Step: 162812, 	{'train/accuracy': 0.8253905773162842, 'train/loss': 0.6590047478675842, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.0020933151245117, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.611265778541565, 'test/num_examples': 10000, 'score': 74399.87388181686, 'total_duration': 81203.53269195557, 'accumulated_submission_time': 74399.87388181686, 'accumulated_eval_time': 6786.871521949768, 'accumulated_logging_time': 8.332364797592163}
I0202 10:49:47.405347 140023005427456 logging_writer.py:48] [162812] accumulated_eval_time=6786.871522, accumulated_logging_time=8.332365, accumulated_submission_time=74399.873882, global_step=162812, preemption_count=0, score=74399.873882, test/accuracy=0.623800, test/loss=1.611266, test/num_examples=10000, total_duration=81203.532692, train/accuracy=0.825391, train/loss=0.659005, validation/accuracy=0.747620, validation/loss=1.002093, validation/num_examples=50000
I0202 10:50:23.595557 140022518892288 logging_writer.py:48] [162900] global_step=162900, grad_norm=5.276670455932617, loss=1.5031614303588867
I0202 10:51:09.659256 140023005427456 logging_writer.py:48] [163000] global_step=163000, grad_norm=5.182979106903076, loss=3.015425205230713
I0202 10:51:56.030894 140022518892288 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.1490397453308105, loss=3.9918744564056396
I0202 10:52:42.413829 140023005427456 logging_writer.py:48] [163200] global_step=163200, grad_norm=5.728311061859131, loss=1.5288211107254028
I0202 10:53:28.436098 140022518892288 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.088558673858643, loss=2.579824686050415
I0202 10:54:14.657128 140023005427456 logging_writer.py:48] [163400] global_step=163400, grad_norm=5.0937323570251465, loss=1.9294068813323975
I0202 10:55:00.758416 140022518892288 logging_writer.py:48] [163500] global_step=163500, grad_norm=5.470736980438232, loss=1.5195705890655518
I0202 10:55:46.924314 140023005427456 logging_writer.py:48] [163600] global_step=163600, grad_norm=5.528410911560059, loss=1.4266752004623413
I0202 10:56:33.315580 140022518892288 logging_writer.py:48] [163700] global_step=163700, grad_norm=5.273909568786621, loss=1.7493054866790771
I0202 10:56:47.819635 140184451094336 spec.py:321] Evaluating on the training split.
I0202 10:56:58.231349 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 10:57:25.189719 140184451094336 spec.py:349] Evaluating on the test split.
I0202 10:57:26.796616 140184451094336 submission_runner.py:408] Time since start: 81662.96s, 	Step: 163733, 	{'train/accuracy': 0.8265624642372131, 'train/loss': 0.6701897382736206, 'validation/accuracy': 0.7502399682998657, 'validation/loss': 0.9970006942749023, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.6085433959960938, 'test/num_examples': 10000, 'score': 74820.23160123825, 'total_duration': 81662.96415829659, 'accumulated_submission_time': 74820.23160123825, 'accumulated_eval_time': 6825.848484277725, 'accumulated_logging_time': 8.381687641143799}
I0202 10:57:26.840377 140023005427456 logging_writer.py:48] [163733] accumulated_eval_time=6825.848484, accumulated_logging_time=8.381688, accumulated_submission_time=74820.231601, global_step=163733, preemption_count=0, score=74820.231601, test/accuracy=0.626700, test/loss=1.608543, test/num_examples=10000, total_duration=81662.964158, train/accuracy=0.826562, train/loss=0.670190, validation/accuracy=0.750240, validation/loss=0.997001, validation/num_examples=50000
I0202 10:57:54.043200 140022518892288 logging_writer.py:48] [163800] global_step=163800, grad_norm=6.158341884613037, loss=3.860759735107422
I0202 10:58:39.866584 140023005427456 logging_writer.py:48] [163900] global_step=163900, grad_norm=6.436734676361084, loss=3.6695663928985596
I0202 10:59:26.464755 140022518892288 logging_writer.py:48] [164000] global_step=164000, grad_norm=5.669109344482422, loss=1.4715161323547363
I0202 11:00:13.052163 140023005427456 logging_writer.py:48] [164100] global_step=164100, grad_norm=5.4740777015686035, loss=1.4421075582504272
I0202 11:00:59.367332 140022518892288 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.502299785614014, loss=1.7836742401123047
I0202 11:01:45.881894 140023005427456 logging_writer.py:48] [164300] global_step=164300, grad_norm=6.437232494354248, loss=3.1524713039398193
I0202 11:02:32.259567 140022518892288 logging_writer.py:48] [164400] global_step=164400, grad_norm=5.621256351470947, loss=1.9391990900039673
I0202 11:03:18.572359 140023005427456 logging_writer.py:48] [164500] global_step=164500, grad_norm=5.6672773361206055, loss=2.9906609058380127
I0202 11:04:04.776929 140022518892288 logging_writer.py:48] [164600] global_step=164600, grad_norm=5.277829170227051, loss=1.2779508829116821
I0202 11:04:27.126745 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:04:38.585813 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:05:06.301271 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:05:07.900403 140184451094336 submission_runner.py:408] Time since start: 82124.07s, 	Step: 164650, 	{'train/accuracy': 0.8306249976158142, 'train/loss': 0.6445387601852417, 'validation/accuracy': 0.7535799741744995, 'validation/loss': 0.9883765578269958, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.5907145738601685, 'test/num_examples': 10000, 'score': 75240.45762014389, 'total_duration': 82124.06794404984, 'accumulated_submission_time': 75240.45762014389, 'accumulated_eval_time': 6866.622145652771, 'accumulated_logging_time': 8.43875503540039}
I0202 11:05:07.943397 140023005427456 logging_writer.py:48] [164650] accumulated_eval_time=6866.622146, accumulated_logging_time=8.438755, accumulated_submission_time=75240.457620, global_step=164650, preemption_count=0, score=75240.457620, test/accuracy=0.626600, test/loss=1.590715, test/num_examples=10000, total_duration=82124.067944, train/accuracy=0.830625, train/loss=0.644539, validation/accuracy=0.753580, validation/loss=0.988377, validation/num_examples=50000
I0202 11:05:28.351070 140022518892288 logging_writer.py:48] [164700] global_step=164700, grad_norm=5.769493579864502, loss=2.4821577072143555
I0202 11:06:12.423159 140023005427456 logging_writer.py:48] [164800] global_step=164800, grad_norm=5.649593353271484, loss=1.474350929260254
I0202 11:06:58.820207 140022518892288 logging_writer.py:48] [164900] global_step=164900, grad_norm=5.056056022644043, loss=2.170928955078125
I0202 11:07:45.039633 140023005427456 logging_writer.py:48] [165000] global_step=165000, grad_norm=5.612279891967773, loss=1.6359238624572754
I0202 11:08:31.117756 140022518892288 logging_writer.py:48] [165100] global_step=165100, grad_norm=5.435202121734619, loss=1.6413856744766235
I0202 11:09:17.515321 140023005427456 logging_writer.py:48] [165200] global_step=165200, grad_norm=5.548544406890869, loss=1.6104096174240112
I0202 11:10:03.376776 140022518892288 logging_writer.py:48] [165300] global_step=165300, grad_norm=5.859115123748779, loss=1.5656688213348389
I0202 11:10:49.447186 140023005427456 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.505248069763184, loss=2.978909969329834
I0202 11:11:35.668852 140022518892288 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.842413902282715, loss=1.6149821281433105
I0202 11:12:07.987488 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:12:18.312709 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:12:45.443247 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:12:47.038694 140184451094336 submission_runner.py:408] Time since start: 82583.21s, 	Step: 165572, 	{'train/accuracy': 0.8291601538658142, 'train/loss': 0.6529331207275391, 'validation/accuracy': 0.7529000043869019, 'validation/loss': 0.9910786747932434, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.5999563932418823, 'test/num_examples': 10000, 'score': 75660.44420909882, 'total_duration': 82583.20623064041, 'accumulated_submission_time': 75660.44420909882, 'accumulated_eval_time': 6905.6733481884, 'accumulated_logging_time': 8.490631818771362}
I0202 11:12:47.079267 140023005427456 logging_writer.py:48] [165572] accumulated_eval_time=6905.673348, accumulated_logging_time=8.490632, accumulated_submission_time=75660.444209, global_step=165572, preemption_count=0, score=75660.444209, test/accuracy=0.629600, test/loss=1.599956, test/num_examples=10000, total_duration=82583.206231, train/accuracy=0.829160, train/loss=0.652933, validation/accuracy=0.752900, validation/loss=0.991079, validation/num_examples=50000
I0202 11:12:58.692155 140022518892288 logging_writer.py:48] [165600] global_step=165600, grad_norm=5.6018805503845215, loss=2.8621861934661865
I0202 11:13:41.535957 140023005427456 logging_writer.py:48] [165700] global_step=165700, grad_norm=6.006351947784424, loss=3.661888599395752
I0202 11:14:27.901125 140022518892288 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.7892961502075195, loss=2.0212180614471436
I0202 11:15:14.283992 140023005427456 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.842295169830322, loss=1.4118106365203857
I0202 11:16:00.173440 140022518892288 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.216778755187988, loss=1.49767005443573
I0202 11:16:46.339195 140023005427456 logging_writer.py:48] [166100] global_step=166100, grad_norm=5.210229396820068, loss=1.5249963998794556
I0202 11:17:32.569326 140022518892288 logging_writer.py:48] [166200] global_step=166200, grad_norm=6.011716842651367, loss=2.421236515045166
I0202 11:18:18.680357 140023005427456 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.272659778594971, loss=2.5966596603393555
I0202 11:19:05.047019 140022518892288 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.379657745361328, loss=1.8528887033462524
I0202 11:19:47.198254 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:19:57.766310 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:20:24.701908 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:20:26.312718 140184451094336 submission_runner.py:408] Time since start: 83042.48s, 	Step: 166493, 	{'train/accuracy': 0.8376562595367432, 'train/loss': 0.6226816177368164, 'validation/accuracy': 0.7539199590682983, 'validation/loss': 0.9782753586769104, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.5768295526504517, 'test/num_examples': 10000, 'score': 76080.50569367409, 'total_duration': 83042.480260849, 'accumulated_submission_time': 76080.50569367409, 'accumulated_eval_time': 6944.787847995758, 'accumulated_logging_time': 8.540288209915161}
I0202 11:20:26.358428 140023005427456 logging_writer.py:48] [166493] accumulated_eval_time=6944.787848, accumulated_logging_time=8.540288, accumulated_submission_time=76080.505694, global_step=166493, preemption_count=0, score=76080.505694, test/accuracy=0.631300, test/loss=1.576830, test/num_examples=10000, total_duration=83042.480261, train/accuracy=0.837656, train/loss=0.622682, validation/accuracy=0.753920, validation/loss=0.978275, validation/num_examples=50000
I0202 11:20:29.560807 140022518892288 logging_writer.py:48] [166500] global_step=166500, grad_norm=5.683835983276367, loss=1.3248987197875977
I0202 11:21:11.749908 140023005427456 logging_writer.py:48] [166600] global_step=166600, grad_norm=6.497203826904297, loss=3.6627447605133057
I0202 11:21:57.567712 140022518892288 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.5805745124816895, loss=1.4663139581680298
I0202 11:22:43.951592 140023005427456 logging_writer.py:48] [166800] global_step=166800, grad_norm=5.3913960456848145, loss=1.545376181602478
I0202 11:23:30.160674 140022518892288 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.7303948402404785, loss=1.4199602603912354
I0202 11:24:15.903201 140023005427456 logging_writer.py:48] [167000] global_step=167000, grad_norm=6.3301801681518555, loss=1.5096979141235352
I0202 11:25:01.751462 140022518892288 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.497582912445068, loss=1.703450083732605
I0202 11:25:47.836842 140023005427456 logging_writer.py:48] [167200] global_step=167200, grad_norm=6.381619930267334, loss=2.2126927375793457
I0202 11:26:33.865180 140022518892288 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.525240898132324, loss=2.975822687149048
I0202 11:27:20.419444 140023005427456 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.680232524871826, loss=1.4692356586456299
I0202 11:27:26.385301 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:27:36.905677 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:28:05.084446 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:28:06.692032 140184451094336 submission_runner.py:408] Time since start: 83502.86s, 	Step: 167415, 	{'train/accuracy': 0.8343359231948853, 'train/loss': 0.6367462277412415, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 0.9818673729896545, 'validation/num_examples': 50000, 'test/accuracy': 0.6365000009536743, 'test/loss': 1.5672297477722168, 'test/num_examples': 10000, 'score': 76500.47437024117, 'total_duration': 83502.85957407951, 'accumulated_submission_time': 76500.47437024117, 'accumulated_eval_time': 6985.094571828842, 'accumulated_logging_time': 8.595833539962769}
I0202 11:28:06.733293 140022518892288 logging_writer.py:48] [167415] accumulated_eval_time=6985.094572, accumulated_logging_time=8.595834, accumulated_submission_time=76500.474370, global_step=167415, preemption_count=0, score=76500.474370, test/accuracy=0.636500, test/loss=1.567230, test/num_examples=10000, total_duration=83502.859574, train/accuracy=0.834336, train/loss=0.636746, validation/accuracy=0.755680, validation/loss=0.981867, validation/num_examples=50000
I0202 11:28:41.599988 140023005427456 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.73462438583374, loss=2.9308338165283203
I0202 11:29:27.586254 140022518892288 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.618396282196045, loss=1.4715070724487305
I0202 11:30:14.325250 140023005427456 logging_writer.py:48] [167700] global_step=167700, grad_norm=5.972532749176025, loss=2.2469730377197266
I0202 11:31:00.643188 140022518892288 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.872138500213623, loss=1.4747107028961182
I0202 11:31:47.328510 140023005427456 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.889883518218994, loss=1.368790626525879
I0202 11:32:33.729657 140022518892288 logging_writer.py:48] [168000] global_step=168000, grad_norm=6.696719646453857, loss=1.331216812133789
I0202 11:33:19.807952 140023005427456 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.858698844909668, loss=1.4971044063568115
I0202 11:34:05.989829 140022518892288 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.925081729888916, loss=1.4236310720443726
I0202 11:34:52.147749 140023005427456 logging_writer.py:48] [168300] global_step=168300, grad_norm=6.74789571762085, loss=2.1485743522644043
I0202 11:35:06.741790 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:35:17.245978 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:35:46.668829 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:35:48.265916 140184451094336 submission_runner.py:408] Time since start: 83964.43s, 	Step: 168333, 	{'train/accuracy': 0.8360351324081421, 'train/loss': 0.6291006207466125, 'validation/accuracy': 0.756659984588623, 'validation/loss': 0.974940836429596, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.5703439712524414, 'test/num_examples': 10000, 'score': 76920.42654371262, 'total_duration': 83964.43345236778, 'accumulated_submission_time': 76920.42654371262, 'accumulated_eval_time': 7026.618679523468, 'accumulated_logging_time': 8.646256685256958}
I0202 11:35:48.306961 140022518892288 logging_writer.py:48] [168333] accumulated_eval_time=7026.618680, accumulated_logging_time=8.646257, accumulated_submission_time=76920.426544, global_step=168333, preemption_count=0, score=76920.426544, test/accuracy=0.631600, test/loss=1.570344, test/num_examples=10000, total_duration=83964.433452, train/accuracy=0.836035, train/loss=0.629101, validation/accuracy=0.756660, validation/loss=0.974941, validation/num_examples=50000
I0202 11:36:15.492798 140023005427456 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.75663948059082, loss=2.3326258659362793
I0202 11:37:01.066976 140022518892288 logging_writer.py:48] [168500] global_step=168500, grad_norm=5.485361576080322, loss=2.3698384761810303
I0202 11:37:47.434823 140023005427456 logging_writer.py:48] [168600] global_step=168600, grad_norm=6.145035266876221, loss=1.3961105346679688
I0202 11:38:34.023984 140022518892288 logging_writer.py:48] [168700] global_step=168700, grad_norm=6.007782936096191, loss=1.4366077184677124
I0202 11:39:20.119782 140023005427456 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.981991767883301, loss=1.4234801530838013
I0202 11:40:06.389690 140022518892288 logging_writer.py:48] [168900] global_step=168900, grad_norm=6.212022304534912, loss=1.4961917400360107
I0202 11:40:52.164012 140023005427456 logging_writer.py:48] [169000] global_step=169000, grad_norm=6.172530174255371, loss=1.4279152154922485
I0202 11:41:38.462604 140022518892288 logging_writer.py:48] [169100] global_step=169100, grad_norm=6.7653632164001465, loss=3.4546897411346436
I0202 11:42:24.653507 140023005427456 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.91595983505249, loss=1.726933240890503
I0202 11:42:48.308310 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:42:58.698985 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:43:23.507696 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:43:25.111333 140184451094336 submission_runner.py:408] Time since start: 84421.28s, 	Step: 169253, 	{'train/accuracy': 0.8400976657867432, 'train/loss': 0.60579913854599, 'validation/accuracy': 0.7610599994659424, 'validation/loss': 0.9543364644050598, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.5526543855667114, 'test/num_examples': 10000, 'score': 77340.37170767784, 'total_duration': 84421.27886939049, 'accumulated_submission_time': 77340.37170767784, 'accumulated_eval_time': 7063.42170381546, 'accumulated_logging_time': 8.696255445480347}
I0202 11:43:25.152289 140022518892288 logging_writer.py:48] [169253] accumulated_eval_time=7063.421704, accumulated_logging_time=8.696255, accumulated_submission_time=77340.371708, global_step=169253, preemption_count=0, score=77340.371708, test/accuracy=0.638900, test/loss=1.552654, test/num_examples=10000, total_duration=84421.278869, train/accuracy=0.840098, train/loss=0.605799, validation/accuracy=0.761060, validation/loss=0.954336, validation/num_examples=50000
I0202 11:43:44.360350 140023005427456 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.936136245727539, loss=2.4707283973693848
I0202 11:44:28.328698 140022518892288 logging_writer.py:48] [169400] global_step=169400, grad_norm=5.845630645751953, loss=2.6312952041625977
I0202 11:45:14.839164 140023005427456 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.802169322967529, loss=1.407907485961914
I0202 11:46:01.205791 140022518892288 logging_writer.py:48] [169600] global_step=169600, grad_norm=6.05207633972168, loss=1.4860364198684692
I0202 11:46:25.370024 140023005427456 logging_writer.py:48] [169654] global_step=169654, preemption_count=0, score=77520.503570
I0202 11:46:26.038121 140184451094336 checkpoints.py:490] Saving checkpoint at step: 169654
I0202 11:46:27.325208 140184451094336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3/checkpoint_169654
I0202 11:46:27.347076 140184451094336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_3/checkpoint_169654.
I0202 11:46:28.082892 140184451094336 submission_runner.py:583] Tuning trial 3/5
I0202 11:46:28.083105 140184451094336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0202 11:46:28.090876 140184451094336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011914062779396772, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.40782308578491, 'total_duration': 63.7958824634552, 'accumulated_submission_time': 36.40782308578491, 'accumulated_eval_time': 27.387945890426636, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (862, {'train/accuracy': 0.011425781063735485, 'train/loss': 6.446063041687012, 'validation/accuracy': 0.011159999296069145, 'validation/loss': 6.462528705596924, 'validation/num_examples': 50000, 'test/accuracy': 0.008100000210106373, 'test/loss': 6.500799655914307, 'test/num_examples': 10000, 'score': 456.3771412372589, 'total_duration': 522.0745017528534, 'accumulated_submission_time': 456.3771412372589, 'accumulated_eval_time': 65.63604092597961, 'accumulated_logging_time': 0.01734304428100586, 'global_step': 862, 'preemption_count': 0}), (1781, {'train/accuracy': 0.037773437798023224, 'train/loss': 5.867616653442383, 'validation/accuracy': 0.03659999743103981, 'validation/loss': 5.891701698303223, 'validation/num_examples': 50000, 'test/accuracy': 0.03060000203549862, 'test/loss': 6.009640693664551, 'test/num_examples': 10000, 'score': 876.6863760948181, 'total_duration': 980.255558013916, 'accumulated_submission_time': 876.6863760948181, 'accumulated_eval_time': 103.42611718177795, 'accumulated_logging_time': 0.051421165466308594, 'global_step': 1781, 'preemption_count': 0}), (2700, {'train/accuracy': 0.06560546904802322, 'train/loss': 5.406094074249268, 'validation/accuracy': 0.0608999989926815, 'validation/loss': 5.4456024169921875, 'validation/num_examples': 50000, 'test/accuracy': 0.04620000347495079, 'test/loss': 5.637139320373535, 'test/num_examples': 10000, 'score': 1296.615249156952, 'total_duration': 1444.4656417369843, 'accumulated_submission_time': 1296.615249156952, 'accumulated_eval_time': 147.62753534317017, 'accumulated_logging_time': 0.08417248725891113, 'global_step': 2700, 'preemption_count': 0}), (3620, {'train/accuracy': 0.09458984434604645, 'train/loss': 5.085729122161865, 'validation/accuracy': 0.08721999824047089, 'validation/loss': 5.123086929321289, 'validation/num_examples': 50000, 'test/accuracy': 0.06670000404119492, 'test/loss': 5.365293502807617, 'test/num_examples': 10000, 'score': 1716.6178047657013, 'total_duration': 1901.7651226520538, 'accumulated_submission_time': 1716.6178047657013, 'accumulated_eval_time': 184.84893035888672, 'accumulated_logging_time': 0.1116495132446289, 'global_step': 3620, 'preemption_count': 0}), (4540, {'train/accuracy': 0.13783203065395355, 'train/loss': 4.668841361999512, 'validation/accuracy': 0.1275399923324585, 'validation/loss': 4.732216835021973, 'validation/num_examples': 50000, 'test/accuracy': 0.09450000524520874, 'test/loss': 5.031833648681641, 'test/num_examples': 10000, 'score': 2136.5751678943634, 'total_duration': 2359.7350487709045, 'accumulated_submission_time': 2136.5751678943634, 'accumulated_eval_time': 222.78968811035156, 'accumulated_logging_time': 0.1356348991394043, 'global_step': 4540, 'preemption_count': 0}), (5457, {'train/accuracy': 0.17052733898162842, 'train/loss': 4.367717742919922, 'validation/accuracy': 0.15661999583244324, 'validation/loss': 4.442732334136963, 'validation/num_examples': 50000, 'test/accuracy': 0.11970000714063644, 'test/loss': 4.782550811767578, 'test/num_examples': 10000, 'score': 2556.6016159057617, 'total_duration': 2814.6012556552887, 'accumulated_submission_time': 2556.6016159057617, 'accumulated_eval_time': 257.5574834346771, 'accumulated_logging_time': 0.16059088706970215, 'global_step': 5457, 'preemption_count': 0}), (6378, {'train/accuracy': 0.2243359386920929, 'train/loss': 3.9350767135620117, 'validation/accuracy': 0.201339989900589, 'validation/loss': 4.064193248748779, 'validation/num_examples': 50000, 'test/accuracy': 0.15040001273155212, 'test/loss': 4.456921100616455, 'test/num_examples': 10000, 'score': 2976.9379460811615, 'total_duration': 3268.678102016449, 'accumulated_submission_time': 2976.9379460811615, 'accumulated_eval_time': 291.22222685813904, 'accumulated_logging_time': 0.18914508819580078, 'global_step': 6378, 'preemption_count': 0}), (7295, {'train/accuracy': 0.25927734375, 'train/loss': 3.6954123973846436, 'validation/accuracy': 0.24155999720096588, 'validation/loss': 3.7877326011657715, 'validation/num_examples': 50000, 'test/accuracy': 0.18250000476837158, 'test/loss': 4.207826137542725, 'test/num_examples': 10000, 'score': 3396.901723384857, 'total_duration': 3729.273754119873, 'accumulated_submission_time': 3396.901723384857, 'accumulated_eval_time': 331.77545142173767, 'accumulated_logging_time': 0.2208545207977295, 'global_step': 7295, 'preemption_count': 0}), (8216, {'train/accuracy': 0.2934960722923279, 'train/loss': 3.44327449798584, 'validation/accuracy': 0.27337998151779175, 'validation/loss': 3.552814483642578, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.0174102783203125, 'test/num_examples': 10000, 'score': 3817.1914489269257, 'total_duration': 4185.551072597504, 'accumulated_submission_time': 3817.1914489269257, 'accumulated_eval_time': 367.68761444091797, 'accumulated_logging_time': 0.247636079788208, 'global_step': 8216, 'preemption_count': 0}), (9136, {'train/accuracy': 0.3303906321525574, 'train/loss': 3.2213683128356934, 'validation/accuracy': 0.2978399991989136, 'validation/loss': 3.398451089859009, 'validation/num_examples': 50000, 'test/accuracy': 0.23120000958442688, 'test/loss': 3.8833281993865967, 'test/num_examples': 10000, 'score': 4237.426783323288, 'total_duration': 4640.326997518539, 'accumulated_submission_time': 4237.426783323288, 'accumulated_eval_time': 402.15560007095337, 'accumulated_logging_time': 0.27222752571105957, 'global_step': 9136, 'preemption_count': 0}), (10054, {'train/accuracy': 0.35636717081069946, 'train/loss': 3.065946578979492, 'validation/accuracy': 0.3330000042915344, 'validation/loss': 3.185922622680664, 'validation/num_examples': 50000, 'test/accuracy': 0.2563000023365021, 'test/loss': 3.715104103088379, 'test/num_examples': 10000, 'score': 4657.610203266144, 'total_duration': 5097.769255399704, 'accumulated_submission_time': 4657.610203266144, 'accumulated_eval_time': 439.33963537216187, 'accumulated_logging_time': 0.2994673252105713, 'global_step': 10054, 'preemption_count': 0}), (10973, {'train/accuracy': 0.3862695097923279, 'train/loss': 2.848928213119507, 'validation/accuracy': 0.35711997747421265, 'validation/loss': 2.993165969848633, 'validation/num_examples': 50000, 'test/accuracy': 0.2735000252723694, 'test/loss': 3.5459067821502686, 'test/num_examples': 10000, 'score': 5077.543553113937, 'total_duration': 5556.3450927734375, 'accumulated_submission_time': 5077.543553113937, 'accumulated_eval_time': 477.90121936798096, 'accumulated_logging_time': 0.3332977294921875, 'global_step': 10973, 'preemption_count': 0}), (11892, {'train/accuracy': 0.40437498688697815, 'train/loss': 2.7795512676239014, 'validation/accuracy': 0.367279976606369, 'validation/loss': 2.9624674320220947, 'validation/num_examples': 50000, 'test/accuracy': 0.287200003862381, 'test/loss': 3.5046021938323975, 'test/num_examples': 10000, 'score': 5497.630401134491, 'total_duration': 6011.133774995804, 'accumulated_submission_time': 5497.630401134491, 'accumulated_eval_time': 512.5262434482574, 'accumulated_logging_time': 0.3625199794769287, 'global_step': 11892, 'preemption_count': 0}), (12809, {'train/accuracy': 0.42238280177116394, 'train/loss': 2.665945291519165, 'validation/accuracy': 0.3883399963378906, 'validation/loss': 2.8275606632232666, 'validation/num_examples': 50000, 'test/accuracy': 0.3069000244140625, 'test/loss': 3.3749306201934814, 'test/num_examples': 10000, 'score': 5917.629370927811, 'total_duration': 6468.676458835602, 'accumulated_submission_time': 5917.629370927811, 'accumulated_eval_time': 549.9961397647858, 'accumulated_logging_time': 0.38938331604003906, 'global_step': 12809, 'preemption_count': 0}), (13726, {'train/accuracy': 0.44224607944488525, 'train/loss': 2.522275447845459, 'validation/accuracy': 0.4086399972438812, 'validation/loss': 2.700598955154419, 'validation/num_examples': 50000, 'test/accuracy': 0.3149000108242035, 'test/loss': 3.2909696102142334, 'test/num_examples': 10000, 'score': 6337.654529333115, 'total_duration': 6927.02899646759, 'accumulated_submission_time': 6337.654529333115, 'accumulated_eval_time': 588.2510459423065, 'accumulated_logging_time': 0.4151310920715332, 'global_step': 13726, 'preemption_count': 0}), (14648, {'train/accuracy': 0.445136696100235, 'train/loss': 2.5415337085723877, 'validation/accuracy': 0.41259998083114624, 'validation/loss': 2.7152037620544434, 'validation/num_examples': 50000, 'test/accuracy': 0.32170000672340393, 'test/loss': 3.2877392768859863, 'test/num_examples': 10000, 'score': 6758.110645294189, 'total_duration': 7386.947074890137, 'accumulated_submission_time': 6758.110645294189, 'accumulated_eval_time': 627.6393864154816, 'accumulated_logging_time': 0.4406590461730957, 'global_step': 14648, 'preemption_count': 0}), (15568, {'train/accuracy': 0.4643945097923279, 'train/loss': 2.4171652793884277, 'validation/accuracy': 0.42885997891426086, 'validation/loss': 2.588149070739746, 'validation/num_examples': 50000, 'test/accuracy': 0.3345000147819519, 'test/loss': 3.1643946170806885, 'test/num_examples': 10000, 'score': 7178.091734886169, 'total_duration': 7848.8106570243835, 'accumulated_submission_time': 7178.091734886169, 'accumulated_eval_time': 669.4438850879669, 'accumulated_logging_time': 0.47141051292419434, 'global_step': 15568, 'preemption_count': 0}), (16487, {'train/accuracy': 0.4732031226158142, 'train/loss': 2.3529422283172607, 'validation/accuracy': 0.43925997614860535, 'validation/loss': 2.526226758956909, 'validation/num_examples': 50000, 'test/accuracy': 0.3418000042438507, 'test/loss': 3.1294901371002197, 'test/num_examples': 10000, 'score': 7598.284974575043, 'total_duration': 8307.79007267952, 'accumulated_submission_time': 7598.284974575043, 'accumulated_eval_time': 708.1572668552399, 'accumulated_logging_time': 0.49685025215148926, 'global_step': 16487, 'preemption_count': 0}), (17409, {'train/accuracy': 0.48097655177116394, 'train/loss': 2.311243772506714, 'validation/accuracy': 0.43967998027801514, 'validation/loss': 2.5017337799072266, 'validation/num_examples': 50000, 'test/accuracy': 0.34550002217292786, 'test/loss': 3.113865852355957, 'test/num_examples': 10000, 'score': 8018.596554040909, 'total_duration': 8765.373228549957, 'accumulated_submission_time': 8018.596554040909, 'accumulated_eval_time': 745.3528969287872, 'accumulated_logging_time': 0.5254116058349609, 'global_step': 17409, 'preemption_count': 0}), (18330, {'train/accuracy': 0.5154687166213989, 'train/loss': 2.13545823097229, 'validation/accuracy': 0.4569000005722046, 'validation/loss': 2.4236037731170654, 'validation/num_examples': 50000, 'test/accuracy': 0.35440000891685486, 'test/loss': 3.041668176651001, 'test/num_examples': 10000, 'score': 8438.65920996666, 'total_duration': 9223.974129915237, 'accumulated_submission_time': 8438.65920996666, 'accumulated_eval_time': 783.8180267810822, 'accumulated_logging_time': 0.551140308380127, 'global_step': 18330, 'preemption_count': 0}), (19249, {'train/accuracy': 0.49541014432907104, 'train/loss': 2.2226991653442383, 'validation/accuracy': 0.46125999093055725, 'validation/loss': 2.3966081142425537, 'validation/num_examples': 50000, 'test/accuracy': 0.36180001497268677, 'test/loss': 3.014835834503174, 'test/num_examples': 10000, 'score': 8858.939247369766, 'total_duration': 9682.179508447647, 'accumulated_submission_time': 8858.939247369766, 'accumulated_eval_time': 821.6687545776367, 'accumulated_logging_time': 0.5784051418304443, 'global_step': 19249, 'preemption_count': 0}), (20170, {'train/accuracy': 0.4988085925579071, 'train/loss': 2.2068932056427, 'validation/accuracy': 0.4660399854183197, 'validation/loss': 2.378439426422119, 'validation/num_examples': 50000, 'test/accuracy': 0.359000027179718, 'test/loss': 3.0155863761901855, 'test/num_examples': 10000, 'score': 9279.185322284698, 'total_duration': 10135.67358827591, 'accumulated_submission_time': 9279.185322284698, 'accumulated_eval_time': 854.8419258594513, 'accumulated_logging_time': 0.6058785915374756, 'global_step': 20170, 'preemption_count': 0}), (21092, {'train/accuracy': 0.5242773294448853, 'train/loss': 2.0958750247955322, 'validation/accuracy': 0.4736199975013733, 'validation/loss': 2.349881649017334, 'validation/num_examples': 50000, 'test/accuracy': 0.3703000247478485, 'test/loss': 2.9635870456695557, 'test/num_examples': 10000, 'score': 9699.388586997986, 'total_duration': 10590.940872192383, 'accumulated_submission_time': 9699.388586997986, 'accumulated_eval_time': 889.8293704986572, 'accumulated_logging_time': 0.6350181102752686, 'global_step': 21092, 'preemption_count': 0}), (22012, {'train/accuracy': 0.5155078172683716, 'train/loss': 2.124331474304199, 'validation/accuracy': 0.4827999770641327, 'validation/loss': 2.2880043983459473, 'validation/num_examples': 50000, 'test/accuracy': 0.37780001759529114, 'test/loss': 2.893505096435547, 'test/num_examples': 10000, 'score': 10119.395952701569, 'total_duration': 11044.343485832214, 'accumulated_submission_time': 10119.395952701569, 'accumulated_eval_time': 923.1459929943085, 'accumulated_logging_time': 0.6662139892578125, 'global_step': 22012, 'preemption_count': 0}), (22927, {'train/accuracy': 0.5287500023841858, 'train/loss': 2.041822910308838, 'validation/accuracy': 0.4889199733734131, 'validation/loss': 2.2429189682006836, 'validation/num_examples': 50000, 'test/accuracy': 0.38380002975463867, 'test/loss': 2.8525102138519287, 'test/num_examples': 10000, 'score': 10539.358990907669, 'total_duration': 11501.50481057167, 'accumulated_submission_time': 10539.358990907669, 'accumulated_eval_time': 960.2646844387054, 'accumulated_logging_time': 0.6987216472625732, 'global_step': 22927, 'preemption_count': 0}), (23845, {'train/accuracy': 0.5420507788658142, 'train/loss': 2.0036168098449707, 'validation/accuracy': 0.5009599924087524, 'validation/loss': 2.2223894596099854, 'validation/num_examples': 50000, 'test/accuracy': 0.387800008058548, 'test/loss': 2.8514115810394287, 'test/num_examples': 10000, 'score': 10959.397310972214, 'total_duration': 11961.276715993881, 'accumulated_submission_time': 10959.397310972214, 'accumulated_eval_time': 999.9240214824677, 'accumulated_logging_time': 0.7256793975830078, 'global_step': 23845, 'preemption_count': 0}), (24770, {'train/accuracy': 0.5383593440055847, 'train/loss': 2.017052173614502, 'validation/accuracy': 0.5021600127220154, 'validation/loss': 2.1893248558044434, 'validation/num_examples': 50000, 'test/accuracy': 0.3936000168323517, 'test/loss': 2.818950891494751, 'test/num_examples': 10000, 'score': 11379.342381954193, 'total_duration': 12414.400235176086, 'accumulated_submission_time': 11379.342381954193, 'accumulated_eval_time': 1033.0191838741302, 'accumulated_logging_time': 0.761568546295166, 'global_step': 24770, 'preemption_count': 0}), (25691, {'train/accuracy': 0.5432421565055847, 'train/loss': 1.9928700923919678, 'validation/accuracy': 0.5070199966430664, 'validation/loss': 2.167407989501953, 'validation/num_examples': 50000, 'test/accuracy': 0.39320001006126404, 'test/loss': 2.8182153701782227, 'test/num_examples': 10000, 'score': 11799.493110179901, 'total_duration': 12871.638572454453, 'accumulated_submission_time': 11799.493110179901, 'accumulated_eval_time': 1070.0217413902283, 'accumulated_logging_time': 0.7992439270019531, 'global_step': 25691, 'preemption_count': 0}), (26613, {'train/accuracy': 0.549023449420929, 'train/loss': 1.963714599609375, 'validation/accuracy': 0.5048800110816956, 'validation/loss': 2.1838018894195557, 'validation/num_examples': 50000, 'test/accuracy': 0.39750000834465027, 'test/loss': 2.823049783706665, 'test/num_examples': 10000, 'score': 12219.802811145782, 'total_duration': 13329.79423236847, 'accumulated_submission_time': 12219.802811145782, 'accumulated_eval_time': 1107.7912635803223, 'accumulated_logging_time': 0.8276638984680176, 'global_step': 26613, 'preemption_count': 0}), (27533, {'train/accuracy': 0.564648449420929, 'train/loss': 1.8632910251617432, 'validation/accuracy': 0.5128799676895142, 'validation/loss': 2.0994911193847656, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.7460947036743164, 'test/num_examples': 10000, 'score': 12639.845764398575, 'total_duration': 13787.502056837082, 'accumulated_submission_time': 12639.845764398575, 'accumulated_eval_time': 1145.3771076202393, 'accumulated_logging_time': 0.859968900680542, 'global_step': 27533, 'preemption_count': 0}), (28453, {'train/accuracy': 0.5565234422683716, 'train/loss': 1.9159458875656128, 'validation/accuracy': 0.5190799832344055, 'validation/loss': 2.100757360458374, 'validation/num_examples': 50000, 'test/accuracy': 0.40940001606941223, 'test/loss': 2.7339088916778564, 'test/num_examples': 10000, 'score': 13060.279699325562, 'total_duration': 14243.699850559235, 'accumulated_submission_time': 13060.279699325562, 'accumulated_eval_time': 1181.059654712677, 'accumulated_logging_time': 0.8942258358001709, 'global_step': 28453, 'preemption_count': 0}), (29344, {'train/accuracy': 0.5629491806030273, 'train/loss': 1.8562678098678589, 'validation/accuracy': 0.523580014705658, 'validation/loss': 2.056525230407715, 'validation/num_examples': 50000, 'test/accuracy': 0.4134000241756439, 'test/loss': 2.7119228839874268, 'test/num_examples': 10000, 'score': 13480.380759000778, 'total_duration': 14698.166138410568, 'accumulated_submission_time': 13480.380759000778, 'accumulated_eval_time': 1215.350920677185, 'accumulated_logging_time': 0.921989917755127, 'global_step': 29344, 'preemption_count': 0}), (30266, {'train/accuracy': 0.5864452719688416, 'train/loss': 1.7774485349655151, 'validation/accuracy': 0.5238800048828125, 'validation/loss': 2.0740952491760254, 'validation/num_examples': 50000, 'test/accuracy': 0.4092000126838684, 'test/loss': 2.7224607467651367, 'test/num_examples': 10000, 'score': 13900.379534959793, 'total_duration': 15156.030605793, 'accumulated_submission_time': 13900.379534959793, 'accumulated_eval_time': 1253.140303850174, 'accumulated_logging_time': 0.9507706165313721, 'global_step': 30266, 'preemption_count': 0}), (31188, {'train/accuracy': 0.5709179639816284, 'train/loss': 1.8723945617675781, 'validation/accuracy': 0.5306400060653687, 'validation/loss': 2.058422327041626, 'validation/num_examples': 50000, 'test/accuracy': 0.4148000180721283, 'test/loss': 2.696887731552124, 'test/num_examples': 10000, 'score': 14320.638298034668, 'total_duration': 15614.249346971512, 'accumulated_submission_time': 14320.638298034668, 'accumulated_eval_time': 1291.0224838256836, 'accumulated_logging_time': 0.9816055297851562, 'global_step': 31188, 'preemption_count': 0}), (32109, {'train/accuracy': 0.5722070336341858, 'train/loss': 1.8319276571273804, 'validation/accuracy': 0.537339985370636, 'validation/loss': 2.008451223373413, 'validation/num_examples': 50000, 'test/accuracy': 0.42180001735687256, 'test/loss': 2.6582112312316895, 'test/num_examples': 10000, 'score': 14740.856187820435, 'total_duration': 16071.735245227814, 'accumulated_submission_time': 14740.856187820435, 'accumulated_eval_time': 1328.2112641334534, 'accumulated_logging_time': 1.0133063793182373, 'global_step': 32109, 'preemption_count': 0}), (33030, {'train/accuracy': 0.591601550579071, 'train/loss': 1.7154150009155273, 'validation/accuracy': 0.5344399809837341, 'validation/loss': 1.969157099723816, 'validation/num_examples': 50000, 'test/accuracy': 0.42650002241134644, 'test/loss': 2.6062676906585693, 'test/num_examples': 10000, 'score': 15160.993295431137, 'total_duration': 16531.774827718735, 'accumulated_submission_time': 15160.993295431137, 'accumulated_eval_time': 1368.0250644683838, 'accumulated_logging_time': 1.0439252853393555, 'global_step': 33030, 'preemption_count': 0}), (33951, {'train/accuracy': 0.580078125, 'train/loss': 1.7822751998901367, 'validation/accuracy': 0.5445799827575684, 'validation/loss': 1.9617338180541992, 'validation/num_examples': 50000, 'test/accuracy': 0.4296000301837921, 'test/loss': 2.6152634620666504, 'test/num_examples': 10000, 'score': 15580.957021474838, 'total_duration': 16992.161110639572, 'accumulated_submission_time': 15580.957021474838, 'accumulated_eval_time': 1408.3668491840363, 'accumulated_logging_time': 1.0768020153045654, 'global_step': 33951, 'preemption_count': 0}), (34872, {'train/accuracy': 0.5824218392372131, 'train/loss': 1.7896679639816284, 'validation/accuracy': 0.5446400046348572, 'validation/loss': 1.968542456626892, 'validation/num_examples': 50000, 'test/accuracy': 0.4246000349521637, 'test/loss': 2.62258243560791, 'test/num_examples': 10000, 'score': 16001.323278665543, 'total_duration': 17450.09957075119, 'accumulated_submission_time': 16001.323278665543, 'accumulated_eval_time': 1445.8618338108063, 'accumulated_logging_time': 1.105936050415039, 'global_step': 34872, 'preemption_count': 0}), (35793, {'train/accuracy': 0.5892577767372131, 'train/loss': 1.7423663139343262, 'validation/accuracy': 0.5414800047874451, 'validation/loss': 1.9773989915847778, 'validation/num_examples': 50000, 'test/accuracy': 0.42430001497268677, 'test/loss': 2.6173818111419678, 'test/num_examples': 10000, 'score': 16421.62156009674, 'total_duration': 17907.980507850647, 'accumulated_submission_time': 16421.62156009674, 'accumulated_eval_time': 1483.3568606376648, 'accumulated_logging_time': 1.1460247039794922, 'global_step': 35793, 'preemption_count': 0}), (36716, {'train/accuracy': 0.5894140601158142, 'train/loss': 1.7541520595550537, 'validation/accuracy': 0.5504999756813049, 'validation/loss': 1.9289462566375732, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.5739073753356934, 'test/num_examples': 10000, 'score': 16841.98622250557, 'total_duration': 18367.78451514244, 'accumulated_submission_time': 16841.98622250557, 'accumulated_eval_time': 1522.7079238891602, 'accumulated_logging_time': 1.1861801147460938, 'global_step': 36716, 'preemption_count': 0}), (37638, {'train/accuracy': 0.5914062261581421, 'train/loss': 1.7134793996810913, 'validation/accuracy': 0.5567799806594849, 'validation/loss': 1.893664002418518, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.56376576423645, 'test/num_examples': 10000, 'score': 17262.115788459778, 'total_duration': 18823.204505443573, 'accumulated_submission_time': 17262.115788459778, 'accumulated_eval_time': 1557.9151480197906, 'accumulated_logging_time': 1.2215921878814697, 'global_step': 37638, 'preemption_count': 0}), (38560, {'train/accuracy': 0.5970898270606995, 'train/loss': 1.721079707145691, 'validation/accuracy': 0.5518199801445007, 'validation/loss': 1.9465895891189575, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.5917446613311768, 'test/num_examples': 10000, 'score': 17682.302928209305, 'total_duration': 19280.42056465149, 'accumulated_submission_time': 17682.302928209305, 'accumulated_eval_time': 1594.8616213798523, 'accumulated_logging_time': 1.2564432621002197, 'global_step': 38560, 'preemption_count': 0}), (39481, {'train/accuracy': 0.6230077743530273, 'train/loss': 1.6257997751235962, 'validation/accuracy': 0.5562999844551086, 'validation/loss': 1.9278334379196167, 'validation/num_examples': 50000, 'test/accuracy': 0.44120001792907715, 'test/loss': 2.564246654510498, 'test/num_examples': 10000, 'score': 18102.482609033585, 'total_duration': 19740.42428445816, 'accumulated_submission_time': 18102.482609033585, 'accumulated_eval_time': 1634.6089255809784, 'accumulated_logging_time': 1.2855734825134277, 'global_step': 39481, 'preemption_count': 0}), (40401, {'train/accuracy': 0.5972656011581421, 'train/loss': 1.7070727348327637, 'validation/accuracy': 0.5578399896621704, 'validation/loss': 1.8923197984695435, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5406343936920166, 'test/num_examples': 10000, 'score': 18522.804244995117, 'total_duration': 20199.12549352646, 'accumulated_submission_time': 18522.804244995117, 'accumulated_eval_time': 1672.908019542694, 'accumulated_logging_time': 1.3188085556030273, 'global_step': 40401, 'preemption_count': 0}), (41322, {'train/accuracy': 0.6015819907188416, 'train/loss': 1.6829158067703247, 'validation/accuracy': 0.557379961013794, 'validation/loss': 1.892937421798706, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.5214507579803467, 'test/num_examples': 10000, 'score': 18942.922422885895, 'total_duration': 20658.107868433, 'accumulated_submission_time': 18942.922422885895, 'accumulated_eval_time': 1711.6933376789093, 'accumulated_logging_time': 1.3502240180969238, 'global_step': 41322, 'preemption_count': 0}), (42241, {'train/accuracy': 0.6206249594688416, 'train/loss': 1.5839260816574097, 'validation/accuracy': 0.5624200105667114, 'validation/loss': 1.8680510520935059, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.519059419631958, 'test/num_examples': 10000, 'score': 19362.841962337494, 'total_duration': 21115.976779937744, 'accumulated_submission_time': 19362.841962337494, 'accumulated_eval_time': 1749.5593955516815, 'accumulated_logging_time': 1.3858461380004883, 'global_step': 42241, 'preemption_count': 0}), (43163, {'train/accuracy': 0.6048827767372131, 'train/loss': 1.6705154180526733, 'validation/accuracy': 0.5646600127220154, 'validation/loss': 1.8540127277374268, 'validation/num_examples': 50000, 'test/accuracy': 0.45250001549720764, 'test/loss': 2.4949381351470947, 'test/num_examples': 10000, 'score': 19782.84850549698, 'total_duration': 21575.811561346054, 'accumulated_submission_time': 19782.84850549698, 'accumulated_eval_time': 1789.3050591945648, 'accumulated_logging_time': 1.420839786529541, 'global_step': 43163, 'preemption_count': 0}), (44084, {'train/accuracy': 0.6102148294448853, 'train/loss': 1.6581255197525024, 'validation/accuracy': 0.5626199841499329, 'validation/loss': 1.85548996925354, 'validation/num_examples': 50000, 'test/accuracy': 0.44940000772476196, 'test/loss': 2.488030433654785, 'test/num_examples': 10000, 'score': 20203.19352889061, 'total_duration': 22031.04239463806, 'accumulated_submission_time': 20203.19352889061, 'accumulated_eval_time': 1824.1077728271484, 'accumulated_logging_time': 1.4556865692138672, 'global_step': 44084, 'preemption_count': 0}), (45006, {'train/accuracy': 0.6237499713897705, 'train/loss': 1.5927554368972778, 'validation/accuracy': 0.568619966506958, 'validation/loss': 1.843924641609192, 'validation/num_examples': 50000, 'test/accuracy': 0.4508000314235687, 'test/loss': 2.4984617233276367, 'test/num_examples': 10000, 'score': 20623.269857168198, 'total_duration': 22488.867975711823, 'accumulated_submission_time': 20623.269857168198, 'accumulated_eval_time': 1861.7756645679474, 'accumulated_logging_time': 1.4894487857818604, 'global_step': 45006, 'preemption_count': 0}), (45926, {'train/accuracy': 0.605175793170929, 'train/loss': 1.6940358877182007, 'validation/accuracy': 0.5661999583244324, 'validation/loss': 1.8739290237426758, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.526916027069092, 'test/num_examples': 10000, 'score': 21043.675313472748, 'total_duration': 22944.59355187416, 'accumulated_submission_time': 21043.675313472748, 'accumulated_eval_time': 1897.01748919487, 'accumulated_logging_time': 1.5201151371002197, 'global_step': 45926, 'preemption_count': 0}), (46847, {'train/accuracy': 0.6008007526397705, 'train/loss': 1.7134603261947632, 'validation/accuracy': 0.5633000135421753, 'validation/loss': 1.8950095176696777, 'validation/num_examples': 50000, 'test/accuracy': 0.4472000300884247, 'test/loss': 2.5412986278533936, 'test/num_examples': 10000, 'score': 21463.90951180458, 'total_duration': 23397.665759801865, 'accumulated_submission_time': 21463.90951180458, 'accumulated_eval_time': 1929.7773234844208, 'accumulated_logging_time': 1.5505378246307373, 'global_step': 46847, 'preemption_count': 0}), (47769, {'train/accuracy': 0.6170703172683716, 'train/loss': 1.6229615211486816, 'validation/accuracy': 0.5671600103378296, 'validation/loss': 1.8515721559524536, 'validation/num_examples': 50000, 'test/accuracy': 0.45270001888275146, 'test/loss': 2.5124051570892334, 'test/num_examples': 10000, 'score': 21884.20440530777, 'total_duration': 23856.79626774788, 'accumulated_submission_time': 21884.20440530777, 'accumulated_eval_time': 1968.5311410427094, 'accumulated_logging_time': 1.5848033428192139, 'global_step': 47769, 'preemption_count': 0}), (48691, {'train/accuracy': 0.6227929592132568, 'train/loss': 1.575631856918335, 'validation/accuracy': 0.5725600123405457, 'validation/loss': 1.808995008468628, 'validation/num_examples': 50000, 'test/accuracy': 0.45740002393722534, 'test/loss': 2.4445207118988037, 'test/num_examples': 10000, 'score': 22304.507378816605, 'total_duration': 24315.673028230667, 'accumulated_submission_time': 22304.507378816605, 'accumulated_eval_time': 2007.020524263382, 'accumulated_logging_time': 1.6213884353637695, 'global_step': 48691, 'preemption_count': 0}), (49612, {'train/accuracy': 0.6174609065055847, 'train/loss': 1.6112401485443115, 'validation/accuracy': 0.573639988899231, 'validation/loss': 1.8005505800247192, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.4524426460266113, 'test/num_examples': 10000, 'score': 22724.552975654602, 'total_duration': 24775.178783893585, 'accumulated_submission_time': 22724.552975654602, 'accumulated_eval_time': 2046.3978426456451, 'accumulated_logging_time': 1.657334804534912, 'global_step': 49612, 'preemption_count': 0}), (50533, {'train/accuracy': 0.62060546875, 'train/loss': 1.6004736423492432, 'validation/accuracy': 0.5769400000572205, 'validation/loss': 1.7959643602371216, 'validation/num_examples': 50000, 'test/accuracy': 0.46630001068115234, 'test/loss': 2.434957981109619, 'test/num_examples': 10000, 'score': 23144.769639730453, 'total_duration': 25230.020445346832, 'accumulated_submission_time': 23144.769639730453, 'accumulated_eval_time': 2080.936208486557, 'accumulated_logging_time': 1.6951377391815186, 'global_step': 50533, 'preemption_count': 0}), (51453, {'train/accuracy': 0.6434960961341858, 'train/loss': 1.4826300144195557, 'validation/accuracy': 0.5772199630737305, 'validation/loss': 1.7871181964874268, 'validation/num_examples': 50000, 'test/accuracy': 0.45590001344680786, 'test/loss': 2.4530246257781982, 'test/num_examples': 10000, 'score': 23564.732614278793, 'total_duration': 25689.36802005768, 'accumulated_submission_time': 23564.732614278793, 'accumulated_eval_time': 2120.2372534275055, 'accumulated_logging_time': 1.7319800853729248, 'global_step': 51453, 'preemption_count': 0}), (52375, {'train/accuracy': 0.6228905916213989, 'train/loss': 1.5904203653335571, 'validation/accuracy': 0.5809999704360962, 'validation/loss': 1.7867530584335327, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.4186763763427734, 'test/num_examples': 10000, 'score': 23984.924451828003, 'total_duration': 26149.58564400673, 'accumulated_submission_time': 23984.924451828003, 'accumulated_eval_time': 2160.178407430649, 'accumulated_logging_time': 1.7693891525268555, 'global_step': 52375, 'preemption_count': 0}), (53295, {'train/accuracy': 0.6255077719688416, 'train/loss': 1.5740492343902588, 'validation/accuracy': 0.5805599689483643, 'validation/loss': 1.7782132625579834, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.41633677482605, 'test/num_examples': 10000, 'score': 24404.92098903656, 'total_duration': 26604.52316379547, 'accumulated_submission_time': 24404.92098903656, 'accumulated_eval_time': 2195.0370230674744, 'accumulated_logging_time': 1.8044648170471191, 'global_step': 53295, 'preemption_count': 0}), (54218, {'train/accuracy': 0.6314257383346558, 'train/loss': 1.5377583503723145, 'validation/accuracy': 0.5790599584579468, 'validation/loss': 1.7804241180419922, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.433056354522705, 'test/num_examples': 10000, 'score': 24825.276747226715, 'total_duration': 27060.225431203842, 'accumulated_submission_time': 24825.276747226715, 'accumulated_eval_time': 2230.2954602241516, 'accumulated_logging_time': 1.8441081047058105, 'global_step': 54218, 'preemption_count': 0}), (55138, {'train/accuracy': 0.6166015267372131, 'train/loss': 1.6154673099517822, 'validation/accuracy': 0.5781399607658386, 'validation/loss': 1.7982147932052612, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.452510356903076, 'test/num_examples': 10000, 'score': 25245.21862053871, 'total_duration': 27518.237367630005, 'accumulated_submission_time': 25245.21862053871, 'accumulated_eval_time': 2268.2810649871826, 'accumulated_logging_time': 1.8811235427856445, 'global_step': 55138, 'preemption_count': 0}), (56056, {'train/accuracy': 0.6266992092132568, 'train/loss': 1.5533452033996582, 'validation/accuracy': 0.5848999619483948, 'validation/loss': 1.7521530389785767, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.410242795944214, 'test/num_examples': 10000, 'score': 25665.102162361145, 'total_duration': 27973.03744482994, 'accumulated_submission_time': 25665.102162361145, 'accumulated_eval_time': 2302.746966123581, 'accumulated_logging_time': 2.2839317321777344, 'global_step': 56056, 'preemption_count': 0}), (56978, {'train/accuracy': 0.6295312643051147, 'train/loss': 1.5653393268585205, 'validation/accuracy': 0.5822399854660034, 'validation/loss': 1.799446702003479, 'validation/num_examples': 50000, 'test/accuracy': 0.46320003271102905, 'test/loss': 2.456258773803711, 'test/num_examples': 10000, 'score': 26085.34273672104, 'total_duration': 28431.985939502716, 'accumulated_submission_time': 26085.34273672104, 'accumulated_eval_time': 2341.3699703216553, 'accumulated_logging_time': 2.3208351135253906, 'global_step': 56978, 'preemption_count': 0}), (57898, {'train/accuracy': 0.6297656297683716, 'train/loss': 1.5668786764144897, 'validation/accuracy': 0.5859599709510803, 'validation/loss': 1.7582910060882568, 'validation/num_examples': 50000, 'test/accuracy': 0.4683000147342682, 'test/loss': 2.4005134105682373, 'test/num_examples': 10000, 'score': 26505.349090337753, 'total_duration': 28890.808502435684, 'accumulated_submission_time': 26505.349090337753, 'accumulated_eval_time': 2380.102923631668, 'accumulated_logging_time': 2.3555450439453125, 'global_step': 57898, 'preemption_count': 0}), (58817, {'train/accuracy': 0.6303515434265137, 'train/loss': 1.5301530361175537, 'validation/accuracy': 0.590399980545044, 'validation/loss': 1.723675012588501, 'validation/num_examples': 50000, 'test/accuracy': 0.46960002183914185, 'test/loss': 2.3985865116119385, 'test/num_examples': 10000, 'score': 26925.5447409153, 'total_duration': 29345.497764587402, 'accumulated_submission_time': 26925.5447409153, 'accumulated_eval_time': 2414.5152776241302, 'accumulated_logging_time': 2.3888587951660156, 'global_step': 58817, 'preemption_count': 0}), (59735, {'train/accuracy': 0.6366991996765137, 'train/loss': 1.510748267173767, 'validation/accuracy': 0.590719997882843, 'validation/loss': 1.7339236736297607, 'validation/num_examples': 50000, 'test/accuracy': 0.4668000340461731, 'test/loss': 2.381608486175537, 'test/num_examples': 10000, 'score': 27345.576851844788, 'total_duration': 29804.49097251892, 'accumulated_submission_time': 27345.576851844788, 'accumulated_eval_time': 2453.3868992328644, 'accumulated_logging_time': 2.431230306625366, 'global_step': 59735, 'preemption_count': 0}), (60655, {'train/accuracy': 0.661816418170929, 'train/loss': 1.413130760192871, 'validation/accuracy': 0.5918799638748169, 'validation/loss': 1.732790231704712, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.3704307079315186, 'test/num_examples': 10000, 'score': 27765.77965736389, 'total_duration': 30265.760466575623, 'accumulated_submission_time': 27765.77965736389, 'accumulated_eval_time': 2494.3704164028168, 'accumulated_logging_time': 2.467085599899292, 'global_step': 60655, 'preemption_count': 0}), (61575, {'train/accuracy': 0.6309961080551147, 'train/loss': 1.5477447509765625, 'validation/accuracy': 0.5902799963951111, 'validation/loss': 1.7401505708694458, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.3999736309051514, 'test/num_examples': 10000, 'score': 28186.06973552704, 'total_duration': 30719.761566877365, 'accumulated_submission_time': 28186.06973552704, 'accumulated_eval_time': 2527.9952044487, 'accumulated_logging_time': 2.5055477619171143, 'global_step': 61575, 'preemption_count': 0}), (62494, {'train/accuracy': 0.6387304663658142, 'train/loss': 1.5181124210357666, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.735752820968628, 'validation/num_examples': 50000, 'test/accuracy': 0.4692000150680542, 'test/loss': 2.3889834880828857, 'test/num_examples': 10000, 'score': 28606.195390701294, 'total_duration': 31179.79212284088, 'accumulated_submission_time': 28606.195390701294, 'accumulated_eval_time': 2567.8129115104675, 'accumulated_logging_time': 2.54514741897583, 'global_step': 62494, 'preemption_count': 0}), (63414, {'train/accuracy': 0.6572265625, 'train/loss': 1.420607089996338, 'validation/accuracy': 0.5945599675178528, 'validation/loss': 1.7118746042251587, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.362293004989624, 'test/num_examples': 10000, 'score': 29026.439562797546, 'total_duration': 31640.54657483101, 'accumulated_submission_time': 29026.439562797546, 'accumulated_eval_time': 2608.2360076904297, 'accumulated_logging_time': 2.5849668979644775, 'global_step': 63414, 'preemption_count': 0}), (64334, {'train/accuracy': 0.6352148056030273, 'train/loss': 1.5222920179367065, 'validation/accuracy': 0.5979799628257751, 'validation/loss': 1.6968483924865723, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.3335988521575928, 'test/num_examples': 10000, 'score': 29446.779339313507, 'total_duration': 32099.923241853714, 'accumulated_submission_time': 29446.779339313507, 'accumulated_eval_time': 2647.1877439022064, 'accumulated_logging_time': 2.6227519512176514, 'global_step': 64334, 'preemption_count': 0}), (65257, {'train/accuracy': 0.6451562643051147, 'train/loss': 1.4754486083984375, 'validation/accuracy': 0.5941999554634094, 'validation/loss': 1.7001410722732544, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.3748252391815186, 'test/num_examples': 10000, 'score': 29867.059475898743, 'total_duration': 32557.672868967056, 'accumulated_submission_time': 29867.059475898743, 'accumulated_eval_time': 2684.573234319687, 'accumulated_logging_time': 2.6589736938476562, 'global_step': 65257, 'preemption_count': 0}), (66178, {'train/accuracy': 0.6478906273841858, 'train/loss': 1.469024658203125, 'validation/accuracy': 0.5952199697494507, 'validation/loss': 1.713046669960022, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.354539155960083, 'test/num_examples': 10000, 'score': 30287.294951438904, 'total_duration': 33016.23513197899, 'accumulated_submission_time': 30287.294951438904, 'accumulated_eval_time': 2722.811019182205, 'accumulated_logging_time': 2.700793743133545, 'global_step': 66178, 'preemption_count': 0}), (67099, {'train/accuracy': 0.6410937309265137, 'train/loss': 1.491320013999939, 'validation/accuracy': 0.5977799892425537, 'validation/loss': 1.6954699754714966, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.3312244415283203, 'test/num_examples': 10000, 'score': 30707.293865442276, 'total_duration': 33478.465988874435, 'accumulated_submission_time': 30707.293865442276, 'accumulated_eval_time': 2764.954957485199, 'accumulated_logging_time': 2.7407922744750977, 'global_step': 67099, 'preemption_count': 0}), (68020, {'train/accuracy': 0.6380273103713989, 'train/loss': 1.4931342601776123, 'validation/accuracy': 0.5981599688529968, 'validation/loss': 1.6907784938812256, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.350700855255127, 'test/num_examples': 10000, 'score': 31127.232821702957, 'total_duration': 33934.169929265976, 'accumulated_submission_time': 31127.232821702957, 'accumulated_eval_time': 2800.633416414261, 'accumulated_logging_time': 2.7796566486358643, 'global_step': 68020, 'preemption_count': 0}), (68942, {'train/accuracy': 0.6507812142372131, 'train/loss': 1.4588117599487305, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6879316568374634, 'validation/num_examples': 50000, 'test/accuracy': 0.4767000079154968, 'test/loss': 2.359503746032715, 'test/num_examples': 10000, 'score': 31547.49372458458, 'total_duration': 34395.838116168976, 'accumulated_submission_time': 31547.49372458458, 'accumulated_eval_time': 2841.955982208252, 'accumulated_logging_time': 2.814785957336426, 'global_step': 68942, 'preemption_count': 0}), (69864, {'train/accuracy': 0.654589831829071, 'train/loss': 1.434368371963501, 'validation/accuracy': 0.6043199896812439, 'validation/loss': 1.666023850440979, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.3192484378814697, 'test/num_examples': 10000, 'score': 31967.64450263977, 'total_duration': 34855.22176671028, 'accumulated_submission_time': 31967.64450263977, 'accumulated_eval_time': 2881.1043269634247, 'accumulated_logging_time': 2.851184606552124, 'global_step': 69864, 'preemption_count': 0}), (70784, {'train/accuracy': 0.6522656083106995, 'train/loss': 1.430999517440796, 'validation/accuracy': 0.6041399836540222, 'validation/loss': 1.6534889936447144, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.3093273639678955, 'test/num_examples': 10000, 'score': 32387.930511713028, 'total_duration': 35306.4135351181, 'accumulated_submission_time': 32387.930511713028, 'accumulated_eval_time': 2911.928690671921, 'accumulated_logging_time': 2.8854973316192627, 'global_step': 70784, 'preemption_count': 0}), (71705, {'train/accuracy': 0.657031238079071, 'train/loss': 1.4209951162338257, 'validation/accuracy': 0.6064199805259705, 'validation/loss': 1.6615710258483887, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.3075990676879883, 'test/num_examples': 10000, 'score': 32808.145381212234, 'total_duration': 35762.34394454956, 'accumulated_submission_time': 32808.145381212234, 'accumulated_eval_time': 2947.54905629158, 'accumulated_logging_time': 2.9329519271850586, 'global_step': 71705, 'preemption_count': 0}), (72625, {'train/accuracy': 0.6685937643051147, 'train/loss': 1.38480544090271, 'validation/accuracy': 0.6049599647521973, 'validation/loss': 1.6850557327270508, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3566348552703857, 'test/num_examples': 10000, 'score': 33228.30744147301, 'total_duration': 36219.24794006348, 'accumulated_submission_time': 33228.30744147301, 'accumulated_eval_time': 2984.2034389972687, 'accumulated_logging_time': 2.973165988922119, 'global_step': 72625, 'preemption_count': 0}), (73544, {'train/accuracy': 0.6450976133346558, 'train/loss': 1.4667530059814453, 'validation/accuracy': 0.6068800091743469, 'validation/loss': 1.662022590637207, 'validation/num_examples': 50000, 'test/accuracy': 0.4855000376701355, 'test/loss': 2.3125555515289307, 'test/num_examples': 10000, 'score': 33648.37844085693, 'total_duration': 36678.69011569023, 'accumulated_submission_time': 33648.37844085693, 'accumulated_eval_time': 3023.486344099045, 'accumulated_logging_time': 3.0142745971679688, 'global_step': 73544, 'preemption_count': 0}), (74464, {'train/accuracy': 0.6503515243530273, 'train/loss': 1.4331283569335938, 'validation/accuracy': 0.6075199842453003, 'validation/loss': 1.6410701274871826, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.3070414066314697, 'test/num_examples': 10000, 'score': 34068.383913517, 'total_duration': 37139.40186858177, 'accumulated_submission_time': 34068.383913517, 'accumulated_eval_time': 3064.1061642169952, 'accumulated_logging_time': 3.052863836288452, 'global_step': 74464, 'preemption_count': 0}), (75386, {'train/accuracy': 0.6625195145606995, 'train/loss': 1.3859827518463135, 'validation/accuracy': 0.6066799759864807, 'validation/loss': 1.6424832344055176, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.278662919998169, 'test/num_examples': 10000, 'score': 34488.60297369957, 'total_duration': 37595.3496427536, 'accumulated_submission_time': 34488.60297369957, 'accumulated_eval_time': 3099.742102622986, 'accumulated_logging_time': 3.097740411758423, 'global_step': 75386, 'preemption_count': 0}), (76306, {'train/accuracy': 0.6537500023841858, 'train/loss': 1.4322412014007568, 'validation/accuracy': 0.6104399561882019, 'validation/loss': 1.6254669427871704, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.2743124961853027, 'test/num_examples': 10000, 'score': 34908.645033836365, 'total_duration': 38057.167717695236, 'accumulated_submission_time': 34908.645033836365, 'accumulated_eval_time': 3141.430042743683, 'accumulated_logging_time': 3.137625217437744, 'global_step': 76306, 'preemption_count': 0}), (77227, {'train/accuracy': 0.6592382788658142, 'train/loss': 1.3970091342926025, 'validation/accuracy': 0.615399956703186, 'validation/loss': 1.60317063331604, 'validation/num_examples': 50000, 'test/accuracy': 0.48490002751350403, 'test/loss': 2.277125358581543, 'test/num_examples': 10000, 'score': 35328.58205103874, 'total_duration': 38514.088331222534, 'accumulated_submission_time': 35328.58205103874, 'accumulated_eval_time': 3178.3272848129272, 'accumulated_logging_time': 3.1766517162323, 'global_step': 77227, 'preemption_count': 0}), (78150, {'train/accuracy': 0.6634374856948853, 'train/loss': 1.390366792678833, 'validation/accuracy': 0.6144999861717224, 'validation/loss': 1.6266072988510132, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.281214952468872, 'test/num_examples': 10000, 'score': 35748.85032916069, 'total_duration': 38976.1696164608, 'accumulated_submission_time': 35748.85032916069, 'accumulated_eval_time': 3220.056258201599, 'accumulated_logging_time': 3.212582588195801, 'global_step': 78150, 'preemption_count': 0}), (79071, {'train/accuracy': 0.6562694907188416, 'train/loss': 1.445751428604126, 'validation/accuracy': 0.614139974117279, 'validation/loss': 1.6321967840194702, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.2790701389312744, 'test/num_examples': 10000, 'score': 36169.0088224411, 'total_duration': 39435.618294239044, 'accumulated_submission_time': 36169.0088224411, 'accumulated_eval_time': 3259.250541448593, 'accumulated_logging_time': 3.260847330093384, 'global_step': 79071, 'preemption_count': 0}), (79992, {'train/accuracy': 0.6576757431030273, 'train/loss': 1.4273301362991333, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.6328654289245605, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.2808785438537598, 'test/num_examples': 10000, 'score': 36589.23210167885, 'total_duration': 39891.485827207565, 'accumulated_submission_time': 36589.23210167885, 'accumulated_eval_time': 3294.8051438331604, 'accumulated_logging_time': 3.3025565147399902, 'global_step': 79992, 'preemption_count': 0}), (80913, {'train/accuracy': 0.674023449420929, 'train/loss': 1.3578218221664429, 'validation/accuracy': 0.6184200048446655, 'validation/loss': 1.6004390716552734, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.2475414276123047, 'test/num_examples': 10000, 'score': 37009.35448670387, 'total_duration': 40351.29439616203, 'accumulated_submission_time': 37009.35448670387, 'accumulated_eval_time': 3334.398421525955, 'accumulated_logging_time': 3.3474948406219482, 'global_step': 80913, 'preemption_count': 0}), (81833, {'train/accuracy': 0.6919335722923279, 'train/loss': 1.269242525100708, 'validation/accuracy': 0.6193400025367737, 'validation/loss': 1.5890880823135376, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.232736349105835, 'test/num_examples': 10000, 'score': 37429.34026837349, 'total_duration': 40809.9657497406, 'accumulated_submission_time': 37429.34026837349, 'accumulated_eval_time': 3372.996278524399, 'accumulated_logging_time': 3.3874971866607666, 'global_step': 81833, 'preemption_count': 0}), (82750, {'train/accuracy': 0.6674609184265137, 'train/loss': 1.4016146659851074, 'validation/accuracy': 0.6218599677085876, 'validation/loss': 1.6021045446395874, 'validation/num_examples': 50000, 'test/accuracy': 0.49550002813339233, 'test/loss': 2.2633657455444336, 'test/num_examples': 10000, 'score': 37849.2662627697, 'total_duration': 41267.85855412483, 'accumulated_submission_time': 37849.2662627697, 'accumulated_eval_time': 3410.873600244522, 'accumulated_logging_time': 3.429560661315918, 'global_step': 82750, 'preemption_count': 0}), (83672, {'train/accuracy': 0.6716406345367432, 'train/loss': 1.3420121669769287, 'validation/accuracy': 0.6239799857139587, 'validation/loss': 1.5581839084625244, 'validation/num_examples': 50000, 'test/accuracy': 0.5044000148773193, 'test/loss': 2.198859214782715, 'test/num_examples': 10000, 'score': 38269.307027578354, 'total_duration': 41726.82884001732, 'accumulated_submission_time': 38269.307027578354, 'accumulated_eval_time': 3449.7180716991425, 'accumulated_logging_time': 3.4665777683258057, 'global_step': 83672, 'preemption_count': 0}), (84591, {'train/accuracy': 0.6886523365974426, 'train/loss': 1.2715094089508057, 'validation/accuracy': 0.6290000081062317, 'validation/loss': 1.5482810735702515, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2042882442474365, 'test/num_examples': 10000, 'score': 38689.40639066696, 'total_duration': 42179.727041482925, 'accumulated_submission_time': 38689.40639066696, 'accumulated_eval_time': 3482.425269842148, 'accumulated_logging_time': 3.50958251953125, 'global_step': 84591, 'preemption_count': 0}), (85512, {'train/accuracy': 0.6723827719688416, 'train/loss': 1.3585656881332397, 'validation/accuracy': 0.6247400045394897, 'validation/loss': 1.5667896270751953, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.2121453285217285, 'test/num_examples': 10000, 'score': 39109.69956469536, 'total_duration': 42640.743921756744, 'accumulated_submission_time': 39109.69956469536, 'accumulated_eval_time': 3523.0634427070618, 'accumulated_logging_time': 3.547616958618164, 'global_step': 85512, 'preemption_count': 0}), (86433, {'train/accuracy': 0.6749609112739563, 'train/loss': 1.341333031654358, 'validation/accuracy': 0.6245799660682678, 'validation/loss': 1.5732306241989136, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.2271392345428467, 'test/num_examples': 10000, 'score': 39529.97727918625, 'total_duration': 43098.75600481033, 'accumulated_submission_time': 39529.97727918625, 'accumulated_eval_time': 3560.7076559066772, 'accumulated_logging_time': 3.589759588241577, 'global_step': 86433, 'preemption_count': 0}), (87354, {'train/accuracy': 0.6795117259025574, 'train/loss': 1.3415911197662354, 'validation/accuracy': 0.6220999956130981, 'validation/loss': 1.5983110666275024, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.2489187717437744, 'test/num_examples': 10000, 'score': 39949.916645765305, 'total_duration': 43556.20781803131, 'accumulated_submission_time': 39949.916645765305, 'accumulated_eval_time': 3598.127161026001, 'accumulated_logging_time': 3.6342861652374268, 'global_step': 87354, 'preemption_count': 0}), (88275, {'train/accuracy': 0.6721289157867432, 'train/loss': 1.3381081819534302, 'validation/accuracy': 0.62909996509552, 'validation/loss': 1.5376700162887573, 'validation/num_examples': 50000, 'test/accuracy': 0.5071000456809998, 'test/loss': 2.2031280994415283, 'test/num_examples': 10000, 'score': 40369.94181585312, 'total_duration': 44014.411709070206, 'accumulated_submission_time': 40369.94181585312, 'accumulated_eval_time': 3636.2176535129547, 'accumulated_logging_time': 3.6755335330963135, 'global_step': 88275, 'preemption_count': 0}), (89194, {'train/accuracy': 0.6786718368530273, 'train/loss': 1.3135088682174683, 'validation/accuracy': 0.6307799816131592, 'validation/loss': 1.5366266965866089, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.1833741664886475, 'test/num_examples': 10000, 'score': 40790.010445833206, 'total_duration': 44473.986904621124, 'accumulated_submission_time': 40790.010445833206, 'accumulated_eval_time': 3675.6362698078156, 'accumulated_logging_time': 3.7161176204681396, 'global_step': 89194, 'preemption_count': 0}), (90112, {'train/accuracy': 0.6904687285423279, 'train/loss': 1.2547414302825928, 'validation/accuracy': 0.6344999670982361, 'validation/loss': 1.5105417966842651, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.1635007858276367, 'test/num_examples': 10000, 'score': 41210.27362036705, 'total_duration': 44933.42651605606, 'accumulated_submission_time': 41210.27362036705, 'accumulated_eval_time': 3714.726364850998, 'accumulated_logging_time': 3.7552504539489746, 'global_step': 90112, 'preemption_count': 0}), (91032, {'train/accuracy': 0.6882616877555847, 'train/loss': 1.284019112586975, 'validation/accuracy': 0.637939989566803, 'validation/loss': 1.5183864831924438, 'validation/num_examples': 50000, 'test/accuracy': 0.5189000368118286, 'test/loss': 2.1651880741119385, 'test/num_examples': 10000, 'score': 41630.5404984951, 'total_duration': 45394.16607880592, 'accumulated_submission_time': 41630.5404984951, 'accumulated_eval_time': 3755.1138138771057, 'accumulated_logging_time': 3.7933239936828613, 'global_step': 91032, 'preemption_count': 0}), (91951, {'train/accuracy': 0.6794726252555847, 'train/loss': 1.3359767198562622, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.5554149150848389, 'validation/num_examples': 50000, 'test/accuracy': 0.5070000290870667, 'test/loss': 2.2034645080566406, 'test/num_examples': 10000, 'score': 42050.500351428986, 'total_duration': 45848.689403772354, 'accumulated_submission_time': 42050.500351428986, 'accumulated_eval_time': 3789.5871107578278, 'accumulated_logging_time': 3.8358139991760254, 'global_step': 91951, 'preemption_count': 0}), (92871, {'train/accuracy': 0.6898437142372131, 'train/loss': 1.2812461853027344, 'validation/accuracy': 0.6362199783325195, 'validation/loss': 1.5131189823150635, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.15106463432312, 'test/num_examples': 10000, 'score': 42470.74870181084, 'total_duration': 46305.96616792679, 'accumulated_submission_time': 42470.74870181084, 'accumulated_eval_time': 3826.5279400348663, 'accumulated_logging_time': 3.8750860691070557, 'global_step': 92871, 'preemption_count': 0}), (93790, {'train/accuracy': 0.7091601490974426, 'train/loss': 1.1998614072799683, 'validation/accuracy': 0.6394400000572205, 'validation/loss': 1.507826566696167, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.150918960571289, 'test/num_examples': 10000, 'score': 42890.90657186508, 'total_duration': 46765.64679956436, 'accumulated_submission_time': 42890.90657186508, 'accumulated_eval_time': 3865.9597566127777, 'accumulated_logging_time': 3.9189999103546143, 'global_step': 93790, 'preemption_count': 0}), (94711, {'train/accuracy': 0.6896093487739563, 'train/loss': 1.2857972383499146, 'validation/accuracy': 0.6369999647140503, 'validation/loss': 1.5179483890533447, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.1582961082458496, 'test/num_examples': 10000, 'score': 43310.93693423271, 'total_duration': 47221.96811914444, 'accumulated_submission_time': 43310.93693423271, 'accumulated_eval_time': 3902.16099691391, 'accumulated_logging_time': 3.960517644882202, 'global_step': 94711, 'preemption_count': 0}), (95632, {'train/accuracy': 0.6902929544448853, 'train/loss': 1.3088555335998535, 'validation/accuracy': 0.6377800107002258, 'validation/loss': 1.5345081090927124, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.168625831604004, 'test/num_examples': 10000, 'score': 43731.27998661995, 'total_duration': 47682.84707713127, 'accumulated_submission_time': 43731.27998661995, 'accumulated_eval_time': 3942.607335329056, 'accumulated_logging_time': 4.002686023712158, 'global_step': 95632, 'preemption_count': 0}), (96552, {'train/accuracy': 0.7011132836341858, 'train/loss': 1.2075631618499756, 'validation/accuracy': 0.6449399590492249, 'validation/loss': 1.4762593507766724, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.1300876140594482, 'test/num_examples': 10000, 'score': 44151.53822731972, 'total_duration': 48142.17348623276, 'accumulated_submission_time': 44151.53822731972, 'accumulated_eval_time': 3981.5831141471863, 'accumulated_logging_time': 4.047634124755859, 'global_step': 96552, 'preemption_count': 0}), (97473, {'train/accuracy': 0.6884570121765137, 'train/loss': 1.2699761390686035, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.4902182817459106, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1305158138275146, 'test/num_examples': 10000, 'score': 44571.59751033783, 'total_duration': 48599.9216735363, 'accumulated_submission_time': 44571.59751033783, 'accumulated_eval_time': 4019.1789577007294, 'accumulated_logging_time': 4.093117952346802, 'global_step': 97473, 'preemption_count': 0}), (98393, {'train/accuracy': 0.691699206829071, 'train/loss': 1.2806599140167236, 'validation/accuracy': 0.6419199705123901, 'validation/loss': 1.4992659091949463, 'validation/num_examples': 50000, 'test/accuracy': 0.5187000036239624, 'test/loss': 2.1577887535095215, 'test/num_examples': 10000, 'score': 44991.82571578026, 'total_duration': 49061.43175268173, 'accumulated_submission_time': 44991.82571578026, 'accumulated_eval_time': 4060.374292612076, 'accumulated_logging_time': 4.132027626037598, 'global_step': 98393, 'preemption_count': 0}), (99312, {'train/accuracy': 0.7009570002555847, 'train/loss': 1.2121543884277344, 'validation/accuracy': 0.6444999575614929, 'validation/loss': 1.4744832515716553, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.101590633392334, 'test/num_examples': 10000, 'score': 45411.43276429176, 'total_duration': 49521.42532157898, 'accumulated_submission_time': 45411.43276429176, 'accumulated_eval_time': 4100.273753166199, 'accumulated_logging_time': 4.570847034454346, 'global_step': 99312, 'preemption_count': 0}), (100232, {'train/accuracy': 0.6933984160423279, 'train/loss': 1.2418538331985474, 'validation/accuracy': 0.6492399573326111, 'validation/loss': 1.4507449865341187, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.115975856781006, 'test/num_examples': 10000, 'score': 45831.65687251091, 'total_duration': 49977.31903076172, 'accumulated_submission_time': 45831.65687251091, 'accumulated_eval_time': 4135.854706764221, 'accumulated_logging_time': 4.6116437911987305, 'global_step': 100232, 'preemption_count': 0}), (101152, {'train/accuracy': 0.6918163895606995, 'train/loss': 1.2562538385391235, 'validation/accuracy': 0.6451799869537354, 'validation/loss': 1.4713010787963867, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1192281246185303, 'test/num_examples': 10000, 'score': 46251.626306295395, 'total_duration': 50434.658855199814, 'accumulated_submission_time': 46251.626306295395, 'accumulated_eval_time': 4173.125863075256, 'accumulated_logging_time': 4.663263559341431, 'global_step': 101152, 'preemption_count': 0}), (102069, {'train/accuracy': 0.7005273103713989, 'train/loss': 1.2236254215240479, 'validation/accuracy': 0.64656001329422, 'validation/loss': 1.4615575075149536, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.0956971645355225, 'test/num_examples': 10000, 'score': 46672.015506744385, 'total_duration': 50894.275889635086, 'accumulated_submission_time': 46672.015506744385, 'accumulated_eval_time': 4212.254663944244, 'accumulated_logging_time': 4.7148377895355225, 'global_step': 102069, 'preemption_count': 0}), (102986, {'train/accuracy': 0.7196484208106995, 'train/loss': 1.1426352262496948, 'validation/accuracy': 0.6477000117301941, 'validation/loss': 1.4675030708312988, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.1090574264526367, 'test/num_examples': 10000, 'score': 47092.12380337715, 'total_duration': 51354.57412791252, 'accumulated_submission_time': 47092.12380337715, 'accumulated_eval_time': 4252.351578950882, 'accumulated_logging_time': 4.760955810546875, 'global_step': 102986, 'preemption_count': 0}), (103907, {'train/accuracy': 0.7007421851158142, 'train/loss': 1.230646014213562, 'validation/accuracy': 0.6519399881362915, 'validation/loss': 1.450840711593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.102874994277954, 'test/num_examples': 10000, 'score': 47512.29468727112, 'total_duration': 51815.11089801788, 'accumulated_submission_time': 47512.29468727112, 'accumulated_eval_time': 4292.62885594368, 'accumulated_logging_time': 4.802370309829712, 'global_step': 103907, 'preemption_count': 0}), (104826, {'train/accuracy': 0.7058984041213989, 'train/loss': 1.1845290660858154, 'validation/accuracy': 0.6528800129890442, 'validation/loss': 1.4384924173355103, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.06282377243042, 'test/num_examples': 10000, 'score': 47932.29152727127, 'total_duration': 52271.9458630085, 'accumulated_submission_time': 47932.29152727127, 'accumulated_eval_time': 4329.373802185059, 'accumulated_logging_time': 4.848757028579712, 'global_step': 104826, 'preemption_count': 0}), (105745, {'train/accuracy': 0.71728515625, 'train/loss': 1.1596193313598633, 'validation/accuracy': 0.6541799902915955, 'validation/loss': 1.4440714120864868, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.0887348651885986, 'test/num_examples': 10000, 'score': 48352.20537209511, 'total_duration': 52731.31432533264, 'accumulated_submission_time': 48352.20537209511, 'accumulated_eval_time': 4368.735318899155, 'accumulated_logging_time': 4.893977880477905, 'global_step': 105745, 'preemption_count': 0}), (106667, {'train/accuracy': 0.7004296779632568, 'train/loss': 1.221923828125, 'validation/accuracy': 0.6500799655914307, 'validation/loss': 1.4510246515274048, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.0904181003570557, 'test/num_examples': 10000, 'score': 48772.653000593185, 'total_duration': 53188.10340118408, 'accumulated_submission_time': 48772.653000593185, 'accumulated_eval_time': 4404.980627298355, 'accumulated_logging_time': 4.941775798797607, 'global_step': 106667, 'preemption_count': 0}), (107589, {'train/accuracy': 0.7095702886581421, 'train/loss': 1.1786792278289795, 'validation/accuracy': 0.655239999294281, 'validation/loss': 1.4211721420288086, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.077857255935669, 'test/num_examples': 10000, 'score': 49192.90219426155, 'total_duration': 53643.56610870361, 'accumulated_submission_time': 49192.90219426155, 'accumulated_eval_time': 4440.104451656342, 'accumulated_logging_time': 4.982898235321045, 'global_step': 107589, 'preemption_count': 0}), (108510, {'train/accuracy': 0.7232226133346558, 'train/loss': 1.1331206560134888, 'validation/accuracy': 0.6595199704170227, 'validation/loss': 1.4153560400009155, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.0454037189483643, 'test/num_examples': 10000, 'score': 49612.81418466568, 'total_duration': 54101.761887550354, 'accumulated_submission_time': 49612.81418466568, 'accumulated_eval_time': 4478.299695491791, 'accumulated_logging_time': 5.023855924606323, 'global_step': 108510, 'preemption_count': 0}), (109430, {'train/accuracy': 0.7046093344688416, 'train/loss': 1.2074859142303467, 'validation/accuracy': 0.6543200016021729, 'validation/loss': 1.4327102899551392, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.0763421058654785, 'test/num_examples': 10000, 'score': 50032.897715091705, 'total_duration': 54561.05555176735, 'accumulated_submission_time': 50032.897715091705, 'accumulated_eval_time': 4517.4173810482025, 'accumulated_logging_time': 5.06796669960022, 'global_step': 109430, 'preemption_count': 0}), (110350, {'train/accuracy': 0.711621105670929, 'train/loss': 1.1574349403381348, 'validation/accuracy': 0.6602199673652649, 'validation/loss': 1.392660140991211, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.0360000133514404, 'test/num_examples': 10000, 'score': 50452.97357225418, 'total_duration': 55019.047716379166, 'accumulated_submission_time': 50452.97357225418, 'accumulated_eval_time': 4555.2443215847015, 'accumulated_logging_time': 5.109800100326538, 'global_step': 110350, 'preemption_count': 0}), (111270, {'train/accuracy': 0.7218554615974426, 'train/loss': 1.1395964622497559, 'validation/accuracy': 0.6591399908065796, 'validation/loss': 1.4095954895019531, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.0380427837371826, 'test/num_examples': 10000, 'score': 50873.10785269737, 'total_duration': 55478.7564125061, 'accumulated_submission_time': 50873.10785269737, 'accumulated_eval_time': 4594.720537662506, 'accumulated_logging_time': 5.160179615020752, 'global_step': 111270, 'preemption_count': 0}), (112189, {'train/accuracy': 0.72132807970047, 'train/loss': 1.1169127225875854, 'validation/accuracy': 0.6618199944496155, 'validation/loss': 1.3724278211593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.0063302516937256, 'test/num_examples': 10000, 'score': 51293.37358379364, 'total_duration': 55939.25805449486, 'accumulated_submission_time': 51293.37358379364, 'accumulated_eval_time': 4634.864460945129, 'accumulated_logging_time': 5.204523324966431, 'global_step': 112189, 'preemption_count': 0}), (113109, {'train/accuracy': 0.7181445360183716, 'train/loss': 1.1483697891235352, 'validation/accuracy': 0.6665199995040894, 'validation/loss': 1.3766143321990967, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.011446714401245, 'test/num_examples': 10000, 'score': 51713.63455915451, 'total_duration': 56399.57521724701, 'accumulated_submission_time': 51713.63455915451, 'accumulated_eval_time': 4674.825638771057, 'accumulated_logging_time': 5.251617193222046, 'global_step': 113109, 'preemption_count': 0}), (114029, {'train/accuracy': 0.7268164157867432, 'train/loss': 1.0959932804107666, 'validation/accuracy': 0.6672999858856201, 'validation/loss': 1.353961706161499, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.0045273303985596, 'test/num_examples': 10000, 'score': 52133.7013399601, 'total_duration': 56860.199070215225, 'accumulated_submission_time': 52133.7013399601, 'accumulated_eval_time': 4715.29369020462, 'accumulated_logging_time': 5.293383836746216, 'global_step': 114029, 'preemption_count': 0}), (114950, {'train/accuracy': 0.7395898103713989, 'train/loss': 1.0591590404510498, 'validation/accuracy': 0.6657999753952026, 'validation/loss': 1.3845274448394775, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.0233569145202637, 'test/num_examples': 10000, 'score': 52553.64809894562, 'total_duration': 57317.40486860275, 'accumulated_submission_time': 52553.64809894562, 'accumulated_eval_time': 4752.462374687195, 'accumulated_logging_time': 5.3358001708984375, 'global_step': 114950, 'preemption_count': 0}), (115872, {'train/accuracy': 0.72279292345047, 'train/loss': 1.1121561527252197, 'validation/accuracy': 0.670799970626831, 'validation/loss': 1.3483924865722656, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 1.9749292135238647, 'test/num_examples': 10000, 'score': 52973.94215083122, 'total_duration': 57775.563047885895, 'accumulated_submission_time': 52973.94215083122, 'accumulated_eval_time': 4790.233314990997, 'accumulated_logging_time': 5.381194114685059, 'global_step': 115872, 'preemption_count': 0}), (116791, {'train/accuracy': 0.7229296565055847, 'train/loss': 1.1437608003616333, 'validation/accuracy': 0.6652799844741821, 'validation/loss': 1.4038317203521729, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.02569580078125, 'test/num_examples': 10000, 'score': 53393.94225502014, 'total_duration': 58233.54987645149, 'accumulated_submission_time': 53393.94225502014, 'accumulated_eval_time': 4828.128947257996, 'accumulated_logging_time': 5.423955678939819, 'global_step': 116791, 'preemption_count': 0}), (117710, {'train/accuracy': 0.741406261920929, 'train/loss': 1.021639108657837, 'validation/accuracy': 0.6748600006103516, 'validation/loss': 1.3188023567199707, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 1.951604962348938, 'test/num_examples': 10000, 'score': 53813.971108198166, 'total_duration': 58690.757637262344, 'accumulated_submission_time': 53813.971108198166, 'accumulated_eval_time': 4865.206468343735, 'accumulated_logging_time': 5.476790428161621, 'global_step': 117710, 'preemption_count': 0}), (118631, {'train/accuracy': 0.7268164157867432, 'train/loss': 1.1247150897979736, 'validation/accuracy': 0.6675800085067749, 'validation/loss': 1.379623532295227, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 1.9980874061584473, 'test/num_examples': 10000, 'score': 54233.92829823494, 'total_duration': 59147.98989415169, 'accumulated_submission_time': 54233.92829823494, 'accumulated_eval_time': 4902.3870396614075, 'accumulated_logging_time': 5.5232415199279785, 'global_step': 118631, 'preemption_count': 0}), (119551, {'train/accuracy': 0.7312304377555847, 'train/loss': 1.0820475816726685, 'validation/accuracy': 0.6734399795532227, 'validation/loss': 1.340196967124939, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 1.9669947624206543, 'test/num_examples': 10000, 'score': 54653.83897304535, 'total_duration': 59607.46091222763, 'accumulated_submission_time': 54653.83897304535, 'accumulated_eval_time': 4941.857255935669, 'accumulated_logging_time': 5.566040754318237, 'global_step': 119551, 'preemption_count': 0}), (120472, {'train/accuracy': 0.7416015267372131, 'train/loss': 1.0313720703125, 'validation/accuracy': 0.6787199974060059, 'validation/loss': 1.3119388818740845, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 1.9518202543258667, 'test/num_examples': 10000, 'score': 55074.146409749985, 'total_duration': 60065.947801828384, 'accumulated_submission_time': 55074.146409749985, 'accumulated_eval_time': 4979.9430141448975, 'accumulated_logging_time': 5.610635757446289, 'global_step': 120472, 'preemption_count': 0}), (121393, {'train/accuracy': 0.7378906011581421, 'train/loss': 1.0619568824768066, 'validation/accuracy': 0.6817599534988403, 'validation/loss': 1.3020442724227905, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 1.9345166683197021, 'test/num_examples': 10000, 'score': 55494.27367591858, 'total_duration': 60526.065910339355, 'accumulated_submission_time': 55494.27367591858, 'accumulated_eval_time': 5019.839179754257, 'accumulated_logging_time': 5.657719373703003, 'global_step': 121393, 'preemption_count': 0}), (122313, {'train/accuracy': 0.73011714220047, 'train/loss': 1.0760235786437988, 'validation/accuracy': 0.6768199801445007, 'validation/loss': 1.3135195970535278, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 1.960036039352417, 'test/num_examples': 10000, 'score': 55914.204786777496, 'total_duration': 60984.79734659195, 'accumulated_submission_time': 55914.204786777496, 'accumulated_eval_time': 5058.548875808716, 'accumulated_logging_time': 5.700714349746704, 'global_step': 122313, 'preemption_count': 0}), (123234, {'train/accuracy': 0.7426952719688416, 'train/loss': 1.0379911661148071, 'validation/accuracy': 0.6814799904823303, 'validation/loss': 1.3065279722213745, 'validation/num_examples': 50000, 'test/accuracy': 0.5596000552177429, 'test/loss': 1.9329463243484497, 'test/num_examples': 10000, 'score': 56334.31851649284, 'total_duration': 61441.13616466522, 'accumulated_submission_time': 56334.31851649284, 'accumulated_eval_time': 5094.679251432419, 'accumulated_logging_time': 5.7470362186431885, 'global_step': 123234, 'preemption_count': 0}), (124154, {'train/accuracy': 0.7546288967132568, 'train/loss': 0.995815634727478, 'validation/accuracy': 0.6815999746322632, 'validation/loss': 1.3149043321609497, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 1.938615322113037, 'test/num_examples': 10000, 'score': 56754.348090171814, 'total_duration': 61900.59717607498, 'accumulated_submission_time': 56754.348090171814, 'accumulated_eval_time': 5134.01913523674, 'accumulated_logging_time': 5.790019989013672, 'global_step': 124154, 'preemption_count': 0}), (125075, {'train/accuracy': 0.7390820384025574, 'train/loss': 1.068745493888855, 'validation/accuracy': 0.6852999925613403, 'validation/loss': 1.3048429489135742, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 1.9468294382095337, 'test/num_examples': 10000, 'score': 57174.4171538353, 'total_duration': 62361.01248407364, 'accumulated_submission_time': 57174.4171538353, 'accumulated_eval_time': 5174.269753456116, 'accumulated_logging_time': 5.836784839630127, 'global_step': 125075, 'preemption_count': 0}), (125998, {'train/accuracy': 0.7458398342132568, 'train/loss': 1.0124694108963013, 'validation/accuracy': 0.6858400106430054, 'validation/loss': 1.2887572050094604, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 1.9115575551986694, 'test/num_examples': 10000, 'score': 57594.61695051193, 'total_duration': 62816.308730363846, 'accumulated_submission_time': 57594.61695051193, 'accumulated_eval_time': 5209.269439458847, 'accumulated_logging_time': 5.885339975357056, 'global_step': 125998, 'preemption_count': 0}), (126918, {'train/accuracy': 0.7591796517372131, 'train/loss': 0.9492820501327515, 'validation/accuracy': 0.6929000020027161, 'validation/loss': 1.2491334676742554, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.88283109664917, 'test/num_examples': 10000, 'score': 58014.5237903595, 'total_duration': 63274.89465546608, 'accumulated_submission_time': 58014.5237903595, 'accumulated_eval_time': 5247.848149061203, 'accumulated_logging_time': 5.937408208847046, 'global_step': 126918, 'preemption_count': 0}), (127838, {'train/accuracy': 0.7441796660423279, 'train/loss': 1.0422630310058594, 'validation/accuracy': 0.6882599592208862, 'validation/loss': 1.2880897521972656, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9071290493011475, 'test/num_examples': 10000, 'score': 58434.72530388832, 'total_duration': 63734.543369054794, 'accumulated_submission_time': 58434.72530388832, 'accumulated_eval_time': 5287.201727390289, 'accumulated_logging_time': 5.98337721824646, 'global_step': 127838, 'preemption_count': 0}), (128759, {'train/accuracy': 0.74853515625, 'train/loss': 1.0025569200515747, 'validation/accuracy': 0.6908800005912781, 'validation/loss': 1.2619134187698364, 'validation/num_examples': 50000, 'test/accuracy': 0.5669000148773193, 'test/loss': 1.8810703754425049, 'test/num_examples': 10000, 'score': 58854.81752896309, 'total_duration': 64194.79561638832, 'accumulated_submission_time': 58854.81752896309, 'accumulated_eval_time': 5327.26294875145, 'accumulated_logging_time': 6.034414768218994, 'global_step': 128759, 'preemption_count': 0}), (129680, {'train/accuracy': 0.7611327767372131, 'train/loss': 0.9462226629257202, 'validation/accuracy': 0.6943999528884888, 'validation/loss': 1.2438348531723022, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 1.8652161359786987, 'test/num_examples': 10000, 'score': 59275.053935050964, 'total_duration': 64651.74168539047, 'accumulated_submission_time': 59275.053935050964, 'accumulated_eval_time': 5363.8732233047485, 'accumulated_logging_time': 6.085147142410278, 'global_step': 129680, 'preemption_count': 0}), (130601, {'train/accuracy': 0.7528710961341858, 'train/loss': 0.9790438413619995, 'validation/accuracy': 0.6981199979782104, 'validation/loss': 1.2228922843933105, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.8345760107040405, 'test/num_examples': 10000, 'score': 59695.36253666878, 'total_duration': 65109.05203604698, 'accumulated_submission_time': 59695.36253666878, 'accumulated_eval_time': 5400.780389070511, 'accumulated_logging_time': 6.132253170013428, 'global_step': 130601, 'preemption_count': 0}), (131524, {'train/accuracy': 0.7593945264816284, 'train/loss': 0.9528237581253052, 'validation/accuracy': 0.695639967918396, 'validation/loss': 1.2331786155700684, 'validation/num_examples': 50000, 'test/accuracy': 0.5743000507354736, 'test/loss': 1.8481074571609497, 'test/num_examples': 10000, 'score': 60115.69743990898, 'total_duration': 65567.70094275475, 'accumulated_submission_time': 60115.69743990898, 'accumulated_eval_time': 5438.998530864716, 'accumulated_logging_time': 6.179803848266602, 'global_step': 131524, 'preemption_count': 0}), (132444, {'train/accuracy': 0.7639452815055847, 'train/loss': 0.9388325214385986, 'validation/accuracy': 0.6972399950027466, 'validation/loss': 1.2304061651229858, 'validation/num_examples': 50000, 'test/accuracy': 0.5740000009536743, 'test/loss': 1.8517664670944214, 'test/num_examples': 10000, 'score': 60535.69753456116, 'total_duration': 66027.71335840225, 'accumulated_submission_time': 60535.69753456116, 'accumulated_eval_time': 5478.9129366874695, 'accumulated_logging_time': 6.230070352554321, 'global_step': 132444, 'preemption_count': 0}), (133364, {'train/accuracy': 0.7642187476158142, 'train/loss': 0.9470322132110596, 'validation/accuracy': 0.7019000053405762, 'validation/loss': 1.2145510911941528, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 1.8425655364990234, 'test/num_examples': 10000, 'score': 60955.82214999199, 'total_duration': 66485.37777233124, 'accumulated_submission_time': 60955.82214999199, 'accumulated_eval_time': 5516.357885599136, 'accumulated_logging_time': 6.277270317077637, 'global_step': 133364, 'preemption_count': 0}), (134284, {'train/accuracy': 0.7623632550239563, 'train/loss': 0.9518302083015442, 'validation/accuracy': 0.7013999819755554, 'validation/loss': 1.2288814783096313, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8447034358978271, 'test/num_examples': 10000, 'score': 61376.02754378319, 'total_duration': 66944.4675412178, 'accumulated_submission_time': 61376.02754378319, 'accumulated_eval_time': 5555.144359827042, 'accumulated_logging_time': 6.326719284057617, 'global_step': 134284, 'preemption_count': 0}), (135201, {'train/accuracy': 0.7685546875, 'train/loss': 0.9147396087646484, 'validation/accuracy': 0.7045199871063232, 'validation/loss': 1.204154372215271, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.818007469177246, 'test/num_examples': 10000, 'score': 61796.07824969292, 'total_duration': 67401.66700196266, 'accumulated_submission_time': 61796.07824969292, 'accumulated_eval_time': 5592.194238185883, 'accumulated_logging_time': 6.378962516784668, 'global_step': 135201, 'preemption_count': 0}), (136122, {'train/accuracy': 0.7792773246765137, 'train/loss': 0.8840520977973938, 'validation/accuracy': 0.7030199766159058, 'validation/loss': 1.2064038515090942, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.8282631635665894, 'test/num_examples': 10000, 'score': 62216.07617998123, 'total_duration': 67860.29677629471, 'accumulated_submission_time': 62216.07617998123, 'accumulated_eval_time': 5630.725626945496, 'accumulated_logging_time': 6.431999683380127, 'global_step': 136122, 'preemption_count': 0}), (137042, {'train/accuracy': 0.765625, 'train/loss': 0.9307951331138611, 'validation/accuracy': 0.7046799659729004, 'validation/loss': 1.205407738685608, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 1.8337697982788086, 'test/num_examples': 10000, 'score': 62636.41758394241, 'total_duration': 68321.68812680244, 'accumulated_submission_time': 62636.41758394241, 'accumulated_eval_time': 5671.676826477051, 'accumulated_logging_time': 6.483324766159058, 'global_step': 137042, 'preemption_count': 0}), (137962, {'train/accuracy': 0.7728710770606995, 'train/loss': 0.9098615050315857, 'validation/accuracy': 0.7051399946212769, 'validation/loss': 1.195683240890503, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.822838544845581, 'test/num_examples': 10000, 'score': 63056.63118767738, 'total_duration': 68779.26647734642, 'accumulated_submission_time': 63056.63118767738, 'accumulated_eval_time': 5708.94930100441, 'accumulated_logging_time': 6.528689861297607, 'global_step': 137962, 'preemption_count': 0}), (138884, {'train/accuracy': 0.7806445360183716, 'train/loss': 0.8815276026725769, 'validation/accuracy': 0.7076799869537354, 'validation/loss': 1.1955801248550415, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 1.8043678998947144, 'test/num_examples': 10000, 'score': 63476.663846731186, 'total_duration': 69239.99764561653, 'accumulated_submission_time': 63476.663846731186, 'accumulated_eval_time': 5749.549212932587, 'accumulated_logging_time': 6.578404903411865, 'global_step': 138884, 'preemption_count': 0}), (139805, {'train/accuracy': 0.771289050579071, 'train/loss': 0.9243224859237671, 'validation/accuracy': 0.7076199650764465, 'validation/loss': 1.2005985975265503, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.8228113651275635, 'test/num_examples': 10000, 'score': 63896.58831167221, 'total_duration': 69700.79360175133, 'accumulated_submission_time': 63896.58831167221, 'accumulated_eval_time': 5790.326649427414, 'accumulated_logging_time': 6.624654054641724, 'global_step': 139805, 'preemption_count': 0}), (140726, {'train/accuracy': 0.777148425579071, 'train/loss': 0.8691675662994385, 'validation/accuracy': 0.7116000056266785, 'validation/loss': 1.1573071479797363, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.7759684324264526, 'test/num_examples': 10000, 'score': 64316.83931660652, 'total_duration': 70159.80298304558, 'accumulated_submission_time': 64316.83931660652, 'accumulated_eval_time': 5828.982895612717, 'accumulated_logging_time': 6.679133176803589, 'global_step': 140726, 'preemption_count': 0}), (141648, {'train/accuracy': 0.7853710651397705, 'train/loss': 0.8723379969596863, 'validation/accuracy': 0.7133600115776062, 'validation/loss': 1.1855190992355347, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.806455373764038, 'test/num_examples': 10000, 'score': 64737.20595860481, 'total_duration': 70620.941873312, 'accumulated_submission_time': 64737.20595860481, 'accumulated_eval_time': 5869.657928228378, 'accumulated_logging_time': 6.726979732513428, 'global_step': 141648, 'preemption_count': 0}), (142567, {'train/accuracy': 0.7803124785423279, 'train/loss': 0.8587034344673157, 'validation/accuracy': 0.7161999940872192, 'validation/loss': 1.1331450939178467, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 1.7515387535095215, 'test/num_examples': 10000, 'score': 65157.241515636444, 'total_duration': 71079.86623930931, 'accumulated_submission_time': 65157.241515636444, 'accumulated_eval_time': 5908.061641693115, 'accumulated_logging_time': 7.164468288421631, 'global_step': 142567, 'preemption_count': 0}), (143486, {'train/accuracy': 0.7825976610183716, 'train/loss': 0.8492751121520996, 'validation/accuracy': 0.7155599594116211, 'validation/loss': 1.143829345703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5928000211715698, 'test/loss': 1.7593907117843628, 'test/num_examples': 10000, 'score': 65577.55893421173, 'total_duration': 71537.87617897987, 'accumulated_submission_time': 65577.55893421173, 'accumulated_eval_time': 5945.656690597534, 'accumulated_logging_time': 7.214048862457275, 'global_step': 143486, 'preemption_count': 0}), (144404, {'train/accuracy': 0.7892382740974426, 'train/loss': 0.8360196352005005, 'validation/accuracy': 0.7186799645423889, 'validation/loss': 1.148780107498169, 'validation/num_examples': 50000, 'test/accuracy': 0.5980000495910645, 'test/loss': 1.7610589265823364, 'test/num_examples': 10000, 'score': 65997.90977883339, 'total_duration': 71999.52923107147, 'accumulated_submission_time': 65997.90977883339, 'accumulated_eval_time': 5986.8613493442535, 'accumulated_logging_time': 7.263937711715698, 'global_step': 144404, 'preemption_count': 0}), (145324, {'train/accuracy': 0.796191394329071, 'train/loss': 0.796824038028717, 'validation/accuracy': 0.7212199568748474, 'validation/loss': 1.1199946403503418, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.755782127380371, 'test/num_examples': 10000, 'score': 66417.887434721, 'total_duration': 72457.2994647026, 'accumulated_submission_time': 66417.887434721, 'accumulated_eval_time': 6024.559996366501, 'accumulated_logging_time': 7.30958104133606, 'global_step': 145324, 'preemption_count': 0}), (146245, {'train/accuracy': 0.7898046970367432, 'train/loss': 0.825894832611084, 'validation/accuracy': 0.7240999937057495, 'validation/loss': 1.1160138845443726, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7424756288528442, 'test/num_examples': 10000, 'score': 66837.9214565754, 'total_duration': 72916.71459913254, 'accumulated_submission_time': 66837.9214565754, 'accumulated_eval_time': 6063.822338104248, 'accumulated_logging_time': 7.380070686340332, 'global_step': 146245, 'preemption_count': 0}), (147163, {'train/accuracy': 0.7924023270606995, 'train/loss': 0.8204847574234009, 'validation/accuracy': 0.7229200005531311, 'validation/loss': 1.114357590675354, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.7341212034225464, 'test/num_examples': 10000, 'score': 67257.93815946579, 'total_duration': 73377.8737001419, 'accumulated_submission_time': 67257.93815946579, 'accumulated_eval_time': 6104.866254329681, 'accumulated_logging_time': 7.430980443954468, 'global_step': 147163, 'preemption_count': 0}), (148082, {'train/accuracy': 0.7983788847923279, 'train/loss': 0.792547345161438, 'validation/accuracy': 0.7223399877548218, 'validation/loss': 1.120306372642517, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.7311818599700928, 'test/num_examples': 10000, 'score': 67677.93710446358, 'total_duration': 73839.43036198616, 'accumulated_submission_time': 67677.93710446358, 'accumulated_eval_time': 6146.32323050499, 'accumulated_logging_time': 7.481757164001465, 'global_step': 148082, 'preemption_count': 0}), (149000, {'train/accuracy': 0.7928906083106995, 'train/loss': 0.8123624920845032, 'validation/accuracy': 0.7275399565696716, 'validation/loss': 1.1038259267807007, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.7259600162506104, 'test/num_examples': 10000, 'score': 68097.8760201931, 'total_duration': 74300.64919734001, 'accumulated_submission_time': 68097.8760201931, 'accumulated_eval_time': 6187.508858203888, 'accumulated_logging_time': 7.528716564178467, 'global_step': 149000, 'preemption_count': 0}), (149923, {'train/accuracy': 0.7988671660423279, 'train/loss': 0.78203284740448, 'validation/accuracy': 0.7282399535179138, 'validation/loss': 1.0843335390090942, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.7013198137283325, 'test/num_examples': 10000, 'score': 68518.12698411942, 'total_duration': 74755.73614430428, 'accumulated_submission_time': 68518.12698411942, 'accumulated_eval_time': 6222.238587379456, 'accumulated_logging_time': 7.587629318237305, 'global_step': 149923, 'preemption_count': 0}), (150845, {'train/accuracy': 0.8016406297683716, 'train/loss': 0.7594988346099854, 'validation/accuracy': 0.7286399602890015, 'validation/loss': 1.0782972574234009, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.6799514293670654, 'test/num_examples': 10000, 'score': 68938.50674057007, 'total_duration': 75219.40028524399, 'accumulated_submission_time': 68938.50674057007, 'accumulated_eval_time': 6265.426444530487, 'accumulated_logging_time': 7.636116981506348, 'global_step': 150845, 'preemption_count': 0}), (151767, {'train/accuracy': 0.7989062070846558, 'train/loss': 0.7856873273849487, 'validation/accuracy': 0.7301799654960632, 'validation/loss': 1.0794763565063477, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.6965135335922241, 'test/num_examples': 10000, 'score': 69358.68147015572, 'total_duration': 75683.92640280724, 'accumulated_submission_time': 69358.68147015572, 'accumulated_eval_time': 6309.68102478981, 'accumulated_logging_time': 7.684762239456177, 'global_step': 151767, 'preemption_count': 0}), (152688, {'train/accuracy': 0.8006835579872131, 'train/loss': 0.7903160452842712, 'validation/accuracy': 0.7302599549293518, 'validation/loss': 1.0970721244812012, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.7170206308364868, 'test/num_examples': 10000, 'score': 69778.69618415833, 'total_duration': 76142.76267409325, 'accumulated_submission_time': 69778.69618415833, 'accumulated_eval_time': 6348.403820991516, 'accumulated_logging_time': 7.73581075668335, 'global_step': 152688, 'preemption_count': 0}), (153610, {'train/accuracy': 0.8105077743530273, 'train/loss': 0.7433952689170837, 'validation/accuracy': 0.7332199811935425, 'validation/loss': 1.0680303573608398, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.6835938692092896, 'test/num_examples': 10000, 'score': 70198.95512890816, 'total_duration': 76602.52445936203, 'accumulated_submission_time': 70198.95512890816, 'accumulated_eval_time': 6387.8042669296265, 'accumulated_logging_time': 7.7897889614105225, 'global_step': 153610, 'preemption_count': 0}), (154531, {'train/accuracy': 0.8120312094688416, 'train/loss': 0.7386379241943359, 'validation/accuracy': 0.7362799644470215, 'validation/loss': 1.0651764869689941, 'validation/num_examples': 50000, 'test/accuracy': 0.6134000420570374, 'test/loss': 1.6804908514022827, 'test/num_examples': 10000, 'score': 70619.00755596161, 'total_duration': 77063.54085183144, 'accumulated_submission_time': 70619.00755596161, 'accumulated_eval_time': 6428.671512365341, 'accumulated_logging_time': 7.8374738693237305, 'global_step': 154531, 'preemption_count': 0}), (155453, {'train/accuracy': 0.8109374642372131, 'train/loss': 0.7333440780639648, 'validation/accuracy': 0.7377199530601501, 'validation/loss': 1.0530829429626465, 'validation/num_examples': 50000, 'test/accuracy': 0.6100000143051147, 'test/loss': 1.6643873453140259, 'test/num_examples': 10000, 'score': 71039.42457556725, 'total_duration': 77521.84715628624, 'accumulated_submission_time': 71039.42457556725, 'accumulated_eval_time': 6466.462705373764, 'accumulated_logging_time': 7.8881707191467285, 'global_step': 155453, 'preemption_count': 0}), (156371, {'train/accuracy': 0.81103515625, 'train/loss': 0.74477618932724, 'validation/accuracy': 0.737060010433197, 'validation/loss': 1.0577281713485718, 'validation/num_examples': 50000, 'test/accuracy': 0.6124000549316406, 'test/loss': 1.6711875200271606, 'test/num_examples': 10000, 'score': 71459.36435127258, 'total_duration': 77982.90768504143, 'accumulated_submission_time': 71459.36435127258, 'accumulated_eval_time': 6507.477152347565, 'accumulated_logging_time': 7.9470250606536865, 'global_step': 156371, 'preemption_count': 0}), (157290, {'train/accuracy': 0.82044917345047, 'train/loss': 0.6939221620559692, 'validation/accuracy': 0.7407599687576294, 'validation/loss': 1.043515920639038, 'validation/num_examples': 50000, 'test/accuracy': 0.6168000102043152, 'test/loss': 1.6577025651931763, 'test/num_examples': 10000, 'score': 71879.43958759308, 'total_duration': 78441.13249969482, 'accumulated_submission_time': 71879.43958759308, 'accumulated_eval_time': 6545.526882886887, 'accumulated_logging_time': 7.999570369720459, 'global_step': 157290, 'preemption_count': 0}), (158209, {'train/accuracy': 0.8133788704872131, 'train/loss': 0.7271475791931152, 'validation/accuracy': 0.7434799671173096, 'validation/loss': 1.0289294719696045, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.6337950229644775, 'test/num_examples': 10000, 'score': 72299.37812900543, 'total_duration': 78900.9204583168, 'accumulated_submission_time': 72299.37812900543, 'accumulated_eval_time': 6585.272683382034, 'accumulated_logging_time': 8.054925918579102, 'global_step': 158209, 'preemption_count': 0}), (159129, {'train/accuracy': 0.8163671493530273, 'train/loss': 0.6988460421562195, 'validation/accuracy': 0.745419979095459, 'validation/loss': 1.0122644901275635, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.6151468753814697, 'test/num_examples': 10000, 'score': 72719.4393901825, 'total_duration': 79362.80851197243, 'accumulated_submission_time': 72719.4393901825, 'accumulated_eval_time': 6626.9921362400055, 'accumulated_logging_time': 8.114330053329468, 'global_step': 159129, 'preemption_count': 0}), (160052, {'train/accuracy': 0.8247265219688416, 'train/loss': 0.6779054403305054, 'validation/accuracy': 0.7456199526786804, 'validation/loss': 1.016263723373413, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.628363013267517, 'test/num_examples': 10000, 'score': 73139.74932837486, 'total_duration': 79824.29507088661, 'accumulated_submission_time': 73139.74932837486, 'accumulated_eval_time': 6668.070779085159, 'accumulated_logging_time': 8.16342830657959, 'global_step': 160052, 'preemption_count': 0}), (160972, {'train/accuracy': 0.8173242211341858, 'train/loss': 0.7082533240318298, 'validation/accuracy': 0.745199978351593, 'validation/loss': 1.0230603218078613, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.6267614364624023, 'test/num_examples': 10000, 'score': 73559.80233550072, 'total_duration': 80281.88353705406, 'accumulated_submission_time': 73559.80233550072, 'accumulated_eval_time': 6705.505133152008, 'accumulated_logging_time': 8.21670150756836, 'global_step': 160972, 'preemption_count': 0}), (161891, {'train/accuracy': 0.8249804377555847, 'train/loss': 0.6753319501876831, 'validation/accuracy': 0.7469199895858765, 'validation/loss': 1.015852451324463, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.627989649772644, 'test/num_examples': 10000, 'score': 73979.69502210617, 'total_duration': 80744.27344155312, 'accumulated_submission_time': 73979.69502210617, 'accumulated_eval_time': 6747.892434120178, 'accumulated_logging_time': 8.278788328170776, 'global_step': 161891, 'preemption_count': 0}), (162812, {'train/accuracy': 0.8253905773162842, 'train/loss': 0.6590047478675842, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.0020933151245117, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.611265778541565, 'test/num_examples': 10000, 'score': 74399.87388181686, 'total_duration': 81203.53269195557, 'accumulated_submission_time': 74399.87388181686, 'accumulated_eval_time': 6786.871521949768, 'accumulated_logging_time': 8.332364797592163, 'global_step': 162812, 'preemption_count': 0}), (163733, {'train/accuracy': 0.8265624642372131, 'train/loss': 0.6701897382736206, 'validation/accuracy': 0.7502399682998657, 'validation/loss': 0.9970006942749023, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.6085433959960938, 'test/num_examples': 10000, 'score': 74820.23160123825, 'total_duration': 81662.96415829659, 'accumulated_submission_time': 74820.23160123825, 'accumulated_eval_time': 6825.848484277725, 'accumulated_logging_time': 8.381687641143799, 'global_step': 163733, 'preemption_count': 0}), (164650, {'train/accuracy': 0.8306249976158142, 'train/loss': 0.6445387601852417, 'validation/accuracy': 0.7535799741744995, 'validation/loss': 0.9883765578269958, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.5907145738601685, 'test/num_examples': 10000, 'score': 75240.45762014389, 'total_duration': 82124.06794404984, 'accumulated_submission_time': 75240.45762014389, 'accumulated_eval_time': 6866.622145652771, 'accumulated_logging_time': 8.43875503540039, 'global_step': 164650, 'preemption_count': 0}), (165572, {'train/accuracy': 0.8291601538658142, 'train/loss': 0.6529331207275391, 'validation/accuracy': 0.7529000043869019, 'validation/loss': 0.9910786747932434, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.5999563932418823, 'test/num_examples': 10000, 'score': 75660.44420909882, 'total_duration': 82583.20623064041, 'accumulated_submission_time': 75660.44420909882, 'accumulated_eval_time': 6905.6733481884, 'accumulated_logging_time': 8.490631818771362, 'global_step': 165572, 'preemption_count': 0}), (166493, {'train/accuracy': 0.8376562595367432, 'train/loss': 0.6226816177368164, 'validation/accuracy': 0.7539199590682983, 'validation/loss': 0.9782753586769104, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.5768295526504517, 'test/num_examples': 10000, 'score': 76080.50569367409, 'total_duration': 83042.480260849, 'accumulated_submission_time': 76080.50569367409, 'accumulated_eval_time': 6944.787847995758, 'accumulated_logging_time': 8.540288209915161, 'global_step': 166493, 'preemption_count': 0}), (167415, {'train/accuracy': 0.8343359231948853, 'train/loss': 0.6367462277412415, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 0.9818673729896545, 'validation/num_examples': 50000, 'test/accuracy': 0.6365000009536743, 'test/loss': 1.5672297477722168, 'test/num_examples': 10000, 'score': 76500.47437024117, 'total_duration': 83502.85957407951, 'accumulated_submission_time': 76500.47437024117, 'accumulated_eval_time': 6985.094571828842, 'accumulated_logging_time': 8.595833539962769, 'global_step': 167415, 'preemption_count': 0}), (168333, {'train/accuracy': 0.8360351324081421, 'train/loss': 0.6291006207466125, 'validation/accuracy': 0.756659984588623, 'validation/loss': 0.974940836429596, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.5703439712524414, 'test/num_examples': 10000, 'score': 76920.42654371262, 'total_duration': 83964.43345236778, 'accumulated_submission_time': 76920.42654371262, 'accumulated_eval_time': 7026.618679523468, 'accumulated_logging_time': 8.646256685256958, 'global_step': 168333, 'preemption_count': 0}), (169253, {'train/accuracy': 0.8400976657867432, 'train/loss': 0.60579913854599, 'validation/accuracy': 0.7610599994659424, 'validation/loss': 0.9543364644050598, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.5526543855667114, 'test/num_examples': 10000, 'score': 77340.37170767784, 'total_duration': 84421.27886939049, 'accumulated_submission_time': 77340.37170767784, 'accumulated_eval_time': 7063.42170381546, 'accumulated_logging_time': 8.696255445480347, 'global_step': 169253, 'preemption_count': 0})], 'global_step': 169654}
I0202 11:46:28.092169 140184451094336 submission_runner.py:586] Timing: 77520.5035700798
I0202 11:46:28.092254 140184451094336 submission_runner.py:588] Total number of evals: 185
I0202 11:46:28.092300 140184451094336 submission_runner.py:589] ====================
I0202 11:46:28.092347 140184451094336 submission_runner.py:542] Using RNG seed 3390244169
I0202 11:46:28.093787 140184451094336 submission_runner.py:551] --- Tuning run 4/5 ---
I0202 11:46:28.093888 140184451094336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4.
I0202 11:46:28.095453 140184451094336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4/hparams.json.
I0202 11:46:28.096322 140184451094336 submission_runner.py:206] Initializing dataset.
I0202 11:46:28.105311 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0202 11:46:28.114981 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0202 11:46:28.319832 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0202 11:46:32.606593 140184451094336 submission_runner.py:213] Initializing model.
I0202 11:46:39.085571 140184451094336 submission_runner.py:255] Initializing optimizer.
I0202 11:46:39.586937 140184451094336 submission_runner.py:262] Initializing metrics bundle.
I0202 11:46:39.587098 140184451094336 submission_runner.py:280] Initializing checkpoint and logger.
I0202 11:46:39.602166 140184451094336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4 with prefix checkpoint_
I0202 11:46:39.602310 140184451094336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0202 11:46:56.107743 140184451094336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0202 11:47:12.289837 140184451094336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4/flags_0.json.
I0202 11:47:12.294702 140184451094336 submission_runner.py:314] Starting training loop.
I0202 11:47:49.074695 140022493714176 logging_writer.py:48] [0] global_step=0, grad_norm=0.3647269308567047, loss=6.9077558517456055
I0202 11:47:49.089878 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:47:57.435452 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:48:15.239488 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:48:16.832740 140184451094336 submission_runner.py:408] Time since start: 64.54s, 	Step: 1, 	{'train/accuracy': 0.001054687425494194, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.79507851600647, 'total_duration': 64.5379867553711, 'accumulated_submission_time': 36.79507851600647, 'accumulated_eval_time': 27.742810249328613, 'accumulated_logging_time': 0}
I0202 11:48:16.842251 140022502106880 logging_writer.py:48] [1] accumulated_eval_time=27.742810, accumulated_logging_time=0, accumulated_submission_time=36.795079, global_step=1, preemption_count=0, score=36.795079, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=64.537987, train/accuracy=0.001055, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0202 11:49:26.537815 140023005427456 logging_writer.py:48] [100] global_step=100, grad_norm=0.5627554059028625, loss=6.814419746398926
I0202 11:50:12.471689 140022518892288 logging_writer.py:48] [200] global_step=200, grad_norm=0.7054970264434814, loss=6.811551570892334
I0202 11:50:58.871126 140023005427456 logging_writer.py:48] [300] global_step=300, grad_norm=1.311789631843567, loss=6.54128360748291
I0202 11:51:45.913998 140022518892288 logging_writer.py:48] [400] global_step=400, grad_norm=1.5205872058868408, loss=6.491572380065918
I0202 11:52:32.217045 140023005427456 logging_writer.py:48] [500] global_step=500, grad_norm=0.7366597056388855, loss=6.606179237365723
I0202 11:53:18.656982 140022518892288 logging_writer.py:48] [600] global_step=600, grad_norm=1.0433236360549927, loss=6.368124961853027
I0202 11:54:04.827356 140023005427456 logging_writer.py:48] [700] global_step=700, grad_norm=0.8857277035713196, loss=6.343846321105957
I0202 11:54:51.211321 140022518892288 logging_writer.py:48] [800] global_step=800, grad_norm=1.0854686498641968, loss=6.248164653778076
I0202 11:55:16.929905 140184451094336 spec.py:321] Evaluating on the training split.
I0202 11:55:27.787243 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 11:55:56.506561 140184451094336 spec.py:349] Evaluating on the test split.
I0202 11:55:58.105650 140184451094336 submission_runner.py:408] Time since start: 525.81s, 	Step: 857, 	{'train/accuracy': 0.03404296934604645, 'train/loss': 5.877542018890381, 'validation/accuracy': 0.032919999212026596, 'validation/loss': 5.916387557983398, 'validation/num_examples': 50000, 'test/accuracy': 0.027300002053380013, 'test/loss': 6.052794933319092, 'test/num_examples': 10000, 'score': 456.8279526233673, 'total_duration': 525.8108751773834, 'accumulated_submission_time': 456.8279526233673, 'accumulated_eval_time': 68.91855096817017, 'accumulated_logging_time': 0.019236087799072266}
I0202 11:55:58.130212 140023005427456 logging_writer.py:48] [857] accumulated_eval_time=68.918551, accumulated_logging_time=0.019236, accumulated_submission_time=456.827953, global_step=857, preemption_count=0, score=456.827953, test/accuracy=0.027300, test/loss=6.052795, test/num_examples=10000, total_duration=525.810875, train/accuracy=0.034043, train/loss=5.877542, validation/accuracy=0.032920, validation/loss=5.916388, validation/num_examples=50000
I0202 11:56:15.711452 140022518892288 logging_writer.py:48] [900] global_step=900, grad_norm=0.7699995636940002, loss=6.178919792175293
I0202 11:56:59.844483 140023005427456 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7294279336929321, loss=6.234711170196533
I0202 11:57:46.067621 140022518892288 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2184175252914429, loss=6.5537028312683105
I0202 11:58:32.531457 140023005427456 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6402589678764343, loss=6.024444580078125
I0202 11:59:19.084487 140022518892288 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.569367527961731, loss=6.5441060066223145
I0202 12:00:05.553590 140023005427456 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5923700928688049, loss=6.064842700958252
I0202 12:00:51.742126 140022518892288 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5805814266204834, loss=6.579643249511719
I0202 12:01:38.258127 140023005427456 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5619780421257019, loss=5.778794765472412
I0202 12:02:24.731348 140022518892288 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7377870678901672, loss=5.955010890960693
I0202 12:02:58.198916 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:03:08.781187 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:03:35.321713 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:03:36.968073 140184451094336 submission_runner.py:408] Time since start: 984.67s, 	Step: 1774, 	{'train/accuracy': 0.06892578303813934, 'train/loss': 5.4872636795043945, 'validation/accuracy': 0.06289999932050705, 'validation/loss': 5.5517730712890625, 'validation/num_examples': 50000, 'test/accuracy': 0.04830000177025795, 'test/loss': 5.741621971130371, 'test/num_examples': 10000, 'score': 876.8367249965668, 'total_duration': 984.6733191013336, 'accumulated_submission_time': 876.8367249965668, 'accumulated_eval_time': 107.68769526481628, 'accumulated_logging_time': 0.05630087852478027}
I0202 12:03:36.982387 140023005427456 logging_writer.py:48] [1774] accumulated_eval_time=107.687695, accumulated_logging_time=0.056301, accumulated_submission_time=876.836725, global_step=1774, preemption_count=0, score=876.836725, test/accuracy=0.048300, test/loss=5.741622, test/num_examples=10000, total_duration=984.673319, train/accuracy=0.068926, train/loss=5.487264, validation/accuracy=0.062900, validation/loss=5.551773, validation/num_examples=50000
I0202 12:03:47.788524 140022518892288 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6627150177955627, loss=5.902739524841309
I0202 12:04:30.371425 140023005427456 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6427426338195801, loss=5.831435203552246
I0202 12:05:16.591563 140022518892288 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6428680419921875, loss=6.327641010284424
I0202 12:06:03.192499 140023005427456 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.48227131366729736, loss=5.688785076141357
I0202 12:06:49.123076 140022518892288 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6654106378555298, loss=5.736089706420898
I0202 12:07:35.319017 140023005427456 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5230230689048767, loss=5.686927795410156
I0202 12:08:21.393614 140022518892288 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.573445200920105, loss=5.6023664474487305
I0202 12:09:08.421086 140023005427456 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.48403942584991455, loss=6.4255595207214355
I0202 12:09:54.404304 140022518892288 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5749818682670593, loss=5.56010627746582
I0202 12:10:37.006529 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:10:47.735160 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:11:13.437620 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:11:15.031930 140184451094336 submission_runner.py:408] Time since start: 1442.74s, 	Step: 2694, 	{'train/accuracy': 0.10423827916383743, 'train/loss': 4.991183280944824, 'validation/accuracy': 0.09743999689817429, 'validation/loss': 5.053380012512207, 'validation/num_examples': 50000, 'test/accuracy': 0.07460000365972519, 'test/loss': 5.3333587646484375, 'test/num_examples': 10000, 'score': 1296.802814245224, 'total_duration': 1442.7371714115143, 'accumulated_submission_time': 1296.802814245224, 'accumulated_eval_time': 145.71309757232666, 'accumulated_logging_time': 0.08081912994384766}
I0202 12:11:15.046921 140023005427456 logging_writer.py:48] [2694] accumulated_eval_time=145.713098, accumulated_logging_time=0.080819, accumulated_submission_time=1296.802814, global_step=2694, preemption_count=0, score=1296.802814, test/accuracy=0.074600, test/loss=5.333359, test/num_examples=10000, total_duration=1442.737171, train/accuracy=0.104238, train/loss=4.991183, validation/accuracy=0.097440, validation/loss=5.053380, validation/num_examples=50000
I0202 12:11:17.849839 140022518892288 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3694935739040375, loss=5.97886848449707
I0202 12:11:59.940698 140023005427456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5815197229385376, loss=5.601109504699707
I0202 12:12:46.163230 140022518892288 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5703931450843811, loss=5.4930219650268555
I0202 12:13:32.636637 140023005427456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.45399031043052673, loss=6.15207052230835
I0202 12:14:18.913640 140022518892288 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7026512026786804, loss=5.527669429779053
I0202 12:15:05.231152 140023005427456 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5565748810768127, loss=6.490830421447754
I0202 12:15:51.400852 140022518892288 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5400921106338501, loss=5.386529922485352
I0202 12:16:37.779551 140023005427456 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8294179439544678, loss=5.542296886444092
I0202 12:17:23.933919 140022518892288 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.49142470955848694, loss=5.344983100891113
I0202 12:18:10.385528 140023005427456 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.47830381989479065, loss=5.404106140136719
I0202 12:18:15.165008 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:18:26.121392 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:18:57.660674 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:18:59.262588 140184451094336 submission_runner.py:408] Time since start: 1906.97s, 	Step: 3612, 	{'train/accuracy': 0.13685546815395355, 'train/loss': 4.678333759307861, 'validation/accuracy': 0.1284399926662445, 'validation/loss': 4.747917175292969, 'validation/num_examples': 50000, 'test/accuracy': 0.09560000151395798, 'test/loss': 5.0893425941467285, 'test/num_examples': 10000, 'score': 1716.8630304336548, 'total_duration': 1906.967833995819, 'accumulated_submission_time': 1716.8630304336548, 'accumulated_eval_time': 189.81067943572998, 'accumulated_logging_time': 0.10532426834106445}
I0202 12:18:59.277777 140022518892288 logging_writer.py:48] [3612] accumulated_eval_time=189.810679, accumulated_logging_time=0.105324, accumulated_submission_time=1716.863030, global_step=3612, preemption_count=0, score=1716.863030, test/accuracy=0.095600, test/loss=5.089343, test/num_examples=10000, total_duration=1906.967834, train/accuracy=0.136855, train/loss=4.678334, validation/accuracy=0.128440, validation/loss=4.747917, validation/num_examples=50000
I0202 12:19:35.884259 140023005427456 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4273364543914795, loss=5.831993103027344
I0202 12:20:22.006747 140022518892288 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5467979907989502, loss=5.568248748779297
I0202 12:21:08.337630 140023005427456 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7770569324493408, loss=5.196690559387207
I0202 12:21:54.653138 140022518892288 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5873162746429443, loss=6.146512031555176
I0202 12:22:40.914291 140023005427456 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7450739145278931, loss=5.517931938171387
I0202 12:23:26.989531 140022518892288 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6335650086402893, loss=5.124578475952148
I0202 12:24:13.322151 140023005427456 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5558951497077942, loss=5.104926586151123
I0202 12:24:59.611217 140022518892288 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5585598945617676, loss=5.91545295715332
I0202 12:25:45.847666 140023005427456 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.716880202293396, loss=5.05278205871582
I0202 12:25:59.387695 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:26:09.987408 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:26:44.595716 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:26:46.200591 140184451094336 submission_runner.py:408] Time since start: 2373.91s, 	Step: 4531, 	{'train/accuracy': 0.18623046576976776, 'train/loss': 4.207225322723389, 'validation/accuracy': 0.17031998932361603, 'validation/loss': 4.318536758422852, 'validation/num_examples': 50000, 'test/accuracy': 0.1284000128507614, 'test/loss': 4.720849514007568, 'test/num_examples': 10000, 'score': 2136.9159696102142, 'total_duration': 2373.9058408737183, 'accumulated_submission_time': 2136.9159696102142, 'accumulated_eval_time': 236.62357091903687, 'accumulated_logging_time': 0.1309516429901123}
I0202 12:26:46.215705 140022518892288 logging_writer.py:48] [4531] accumulated_eval_time=236.623571, accumulated_logging_time=0.130952, accumulated_submission_time=2136.915970, global_step=4531, preemption_count=0, score=2136.915970, test/accuracy=0.128400, test/loss=4.720850, test/num_examples=10000, total_duration=2373.905841, train/accuracy=0.186230, train/loss=4.207225, validation/accuracy=0.170320, validation/loss=4.318537, validation/num_examples=50000
I0202 12:27:14.212566 140023005427456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4963330626487732, loss=6.415805816650391
I0202 12:28:00.179720 140022518892288 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7044650316238403, loss=5.1383233070373535
I0202 12:28:46.873849 140023005427456 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8774619102478027, loss=5.424479961395264
I0202 12:29:33.337174 140022518892288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6492806077003479, loss=6.348214149475098
I0202 12:30:19.767660 140023005427456 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5955714583396912, loss=6.059020042419434
I0202 12:31:06.058862 140022518892288 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.744831383228302, loss=4.869712829589844
I0202 12:31:52.785799 140023005427456 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5489805936813354, loss=5.2218756675720215
I0202 12:32:39.331692 140022518892288 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7165824174880981, loss=4.911005973815918
I0202 12:33:25.622005 140023005427456 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7039783596992493, loss=5.635292053222656
I0202 12:33:46.489339 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:33:57.720707 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:34:22.669149 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:34:24.272044 140184451094336 submission_runner.py:408] Time since start: 2831.98s, 	Step: 5447, 	{'train/accuracy': 0.20121093094348907, 'train/loss': 4.060230731964111, 'validation/accuracy': 0.18803998827934265, 'validation/loss': 4.152525424957275, 'validation/num_examples': 50000, 'test/accuracy': 0.14480000734329224, 'test/loss': 4.589404106140137, 'test/num_examples': 10000, 'score': 2557.1325867176056, 'total_duration': 2831.9772713184357, 'accumulated_submission_time': 2557.1325867176056, 'accumulated_eval_time': 274.40623688697815, 'accumulated_logging_time': 0.15637731552124023}
I0202 12:34:24.289656 140022518892288 logging_writer.py:48] [5447] accumulated_eval_time=274.406237, accumulated_logging_time=0.156377, accumulated_submission_time=2557.132587, global_step=5447, preemption_count=0, score=2557.132587, test/accuracy=0.144800, test/loss=4.589404, test/num_examples=10000, total_duration=2831.977271, train/accuracy=0.201211, train/loss=4.060231, validation/accuracy=0.188040, validation/loss=4.152525, validation/num_examples=50000
I0202 12:34:45.892716 140023005427456 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8548815846443176, loss=5.1190948486328125
I0202 12:35:30.530428 140022518892288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6436126232147217, loss=5.599205017089844
I0202 12:36:16.863491 140023005427456 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6516717672348022, loss=5.163481712341309
I0202 12:37:03.383879 140022518892288 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7134519219398499, loss=4.8135294914245605
I0202 12:37:49.339473 140023005427456 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.590255081653595, loss=5.386788845062256
I0202 12:38:35.642053 140022518892288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8084419369697571, loss=4.68703556060791
I0202 12:39:22.074375 140023005427456 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9097263216972351, loss=6.364633560180664
I0202 12:40:08.184277 140022518892288 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.598702073097229, loss=5.715803623199463
I0202 12:40:56.199526 140023005427456 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7302500605583191, loss=6.107448577880859
I0202 12:41:24.529140 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:41:35.529935 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:42:07.321381 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:42:08.923755 140184451094336 submission_runner.py:408] Time since start: 3296.63s, 	Step: 6363, 	{'train/accuracy': 0.21666015684604645, 'train/loss': 3.984548330307007, 'validation/accuracy': 0.20041999220848083, 'validation/loss': 4.092609405517578, 'validation/num_examples': 50000, 'test/accuracy': 0.15310001373291016, 'test/loss': 4.517242431640625, 'test/num_examples': 10000, 'score': 2977.314273118973, 'total_duration': 3296.628982782364, 'accumulated_submission_time': 2977.314273118973, 'accumulated_eval_time': 318.80083322525024, 'accumulated_logging_time': 0.18465852737426758}
I0202 12:42:08.939556 140022518892288 logging_writer.py:48] [6363] accumulated_eval_time=318.800833, accumulated_logging_time=0.184659, accumulated_submission_time=2977.314273, global_step=6363, preemption_count=0, score=2977.314273, test/accuracy=0.153100, test/loss=4.517242, test/num_examples=10000, total_duration=3296.628983, train/accuracy=0.216660, train/loss=3.984548, validation/accuracy=0.200420, validation/loss=4.092609, validation/num_examples=50000
I0202 12:42:24.145058 140023005427456 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7876780033111572, loss=4.74212646484375
I0202 12:43:07.965861 140022518892288 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8103517293930054, loss=6.2327775955200195
I0202 12:43:54.087363 140023005427456 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7225499153137207, loss=4.715051651000977
I0202 12:44:40.367433 140022518892288 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7334375381469727, loss=4.678022384643555
I0202 12:45:26.777965 140023005427456 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6481285095214844, loss=6.223687648773193
I0202 12:46:12.946365 140022518892288 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8183423280715942, loss=4.8982625007629395
I0202 12:46:59.138144 140023005427456 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7234556674957275, loss=4.772881031036377
I0202 12:47:45.309827 140022518892288 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9767348766326904, loss=4.696301460266113
I0202 12:48:31.363849 140023005427456 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8325657844543457, loss=5.928083419799805
I0202 12:49:09.053177 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:49:19.249367 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:49:53.639549 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:49:55.265919 140184451094336 submission_runner.py:408] Time since start: 3762.97s, 	Step: 7283, 	{'train/accuracy': 0.2351367175579071, 'train/loss': 3.851510524749756, 'validation/accuracy': 0.2165599912405014, 'validation/loss': 3.9796018600463867, 'validation/num_examples': 50000, 'test/accuracy': 0.16660000383853912, 'test/loss': 4.4249958992004395, 'test/num_examples': 10000, 'score': 3397.362140417099, 'total_duration': 3762.9711685180664, 'accumulated_submission_time': 3397.362140417099, 'accumulated_eval_time': 365.0135953426361, 'accumulated_logging_time': 0.21924662590026855}
I0202 12:49:55.282153 140022518892288 logging_writer.py:48] [7283] accumulated_eval_time=365.013595, accumulated_logging_time=0.219247, accumulated_submission_time=3397.362140, global_step=7283, preemption_count=0, score=3397.362140, test/accuracy=0.166600, test/loss=4.424996, test/num_examples=10000, total_duration=3762.971169, train/accuracy=0.235137, train/loss=3.851511, validation/accuracy=0.216560, validation/loss=3.979602, validation/num_examples=50000
I0202 12:50:02.488317 140023005427456 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6790440082550049, loss=4.542566299438477
I0202 12:50:44.988952 140022518892288 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7759641408920288, loss=6.2284650802612305
I0202 12:51:31.286893 140023005427456 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8407164812088013, loss=4.621969699859619
I0202 12:52:17.715819 140022518892288 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8601062893867493, loss=4.5829386711120605
I0202 12:53:03.961400 140023005427456 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8569207787513733, loss=4.789125919342041
I0202 12:53:50.002738 140022518892288 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7948567867279053, loss=4.64623498916626
I0202 12:54:36.336175 140023005427456 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7865219116210938, loss=5.704270362854004
I0202 12:55:22.687338 140022518892288 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.038207769393921, loss=4.714122772216797
I0202 12:56:09.028493 140023005427456 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8199321031570435, loss=4.748013496398926
I0202 12:56:55.128264 140022518892288 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7318798303604126, loss=4.878059387207031
I0202 12:56:55.660984 140184451094336 spec.py:321] Evaluating on the training split.
I0202 12:57:06.965964 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 12:57:39.913919 140184451094336 spec.py:349] Evaluating on the test split.
I0202 12:57:41.516383 140184451094336 submission_runner.py:408] Time since start: 4229.22s, 	Step: 8203, 	{'train/accuracy': 0.26273438334465027, 'train/loss': 3.6297197341918945, 'validation/accuracy': 0.23229999840259552, 'validation/loss': 3.8290367126464844, 'validation/num_examples': 50000, 'test/accuracy': 0.17440000176429749, 'test/loss': 4.296341419219971, 'test/num_examples': 10000, 'score': 3817.682046175003, 'total_duration': 4229.221621990204, 'accumulated_submission_time': 3817.682046175003, 'accumulated_eval_time': 410.8689727783203, 'accumulated_logging_time': 0.24734210968017578}
I0202 12:57:41.531630 140023005427456 logging_writer.py:48] [8203] accumulated_eval_time=410.868973, accumulated_logging_time=0.247342, accumulated_submission_time=3817.682046, global_step=8203, preemption_count=0, score=3817.682046, test/accuracy=0.174400, test/loss=4.296341, test/num_examples=10000, total_duration=4229.221622, train/accuracy=0.262734, train/loss=3.629720, validation/accuracy=0.232300, validation/loss=3.829037, validation/num_examples=50000
I0202 12:58:22.455844 140022518892288 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8821876645088196, loss=4.644952774047852
I0202 12:59:08.464506 140023005427456 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7787541151046753, loss=5.9004364013671875
I0202 12:59:54.773486 140022518892288 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8031683564186096, loss=4.901844501495361
I0202 13:00:40.819847 140023005427456 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0304722785949707, loss=4.576035022735596
I0202 13:01:27.364639 140022518892288 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7663794755935669, loss=5.770967483520508
I0202 13:02:14.024535 140023005427456 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9270430207252502, loss=4.470667362213135
I0202 13:03:00.032130 140022518892288 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6445471048355103, loss=5.740563869476318
I0202 13:03:46.325625 140023005427456 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6758702993392944, loss=6.194179058074951
I0202 13:04:32.522490 140022518892288 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.795312225818634, loss=6.299217224121094
I0202 13:04:41.571223 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:04:52.233309 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:05:25.128359 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:05:26.725336 140184451094336 submission_runner.py:408] Time since start: 4694.43s, 	Step: 9121, 	{'train/accuracy': 0.2595312297344208, 'train/loss': 3.638303518295288, 'validation/accuracy': 0.2396799921989441, 'validation/loss': 3.753018617630005, 'validation/num_examples': 50000, 'test/accuracy': 0.18690000474452972, 'test/loss': 4.2364301681518555, 'test/num_examples': 10000, 'score': 4237.6647737026215, 'total_duration': 4694.430584430695, 'accumulated_submission_time': 4237.6647737026215, 'accumulated_eval_time': 456.02307748794556, 'accumulated_logging_time': 0.2729377746582031}
I0202 13:05:26.741831 140023005427456 logging_writer.py:48] [9121] accumulated_eval_time=456.023077, accumulated_logging_time=0.272938, accumulated_submission_time=4237.664774, global_step=9121, preemption_count=0, score=4237.664774, test/accuracy=0.186900, test/loss=4.236430, test/num_examples=10000, total_duration=4694.430584, train/accuracy=0.259531, train/loss=3.638304, validation/accuracy=0.239680, validation/loss=3.753019, validation/num_examples=50000
I0202 13:05:59.289565 140022518892288 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8486839532852173, loss=4.380267143249512
I0202 13:06:45.377848 140023005427456 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7090351581573486, loss=5.419280052185059
I0202 13:07:31.820093 140022518892288 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8567463755607605, loss=4.783568382263184
I0202 13:08:18.164684 140023005427456 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8549483418464661, loss=4.970811367034912
I0202 13:09:04.215018 140022518892288 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8767702579498291, loss=4.48243522644043
I0202 13:09:50.271517 140023005427456 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6569598317146301, loss=5.356574535369873
I0202 13:10:36.537904 140022518892288 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8493450880050659, loss=4.397398471832275
I0202 13:11:22.852244 140023005427456 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7465824484825134, loss=4.61990213394165
I0202 13:12:09.449566 140022518892288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7756650447845459, loss=5.692750453948975
I0202 13:12:26.789885 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:12:37.349935 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:13:08.693856 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:13:10.289174 140184451094336 submission_runner.py:408] Time since start: 5157.99s, 	Step: 10039, 	{'train/accuracy': 0.27964842319488525, 'train/loss': 3.492593288421631, 'validation/accuracy': 0.2573399841785431, 'validation/loss': 3.628937005996704, 'validation/num_examples': 50000, 'test/accuracy': 0.20080001652240753, 'test/loss': 4.119417667388916, 'test/num_examples': 10000, 'score': 4657.653786182404, 'total_duration': 5157.994423866272, 'accumulated_submission_time': 4657.653786182404, 'accumulated_eval_time': 499.5223741531372, 'accumulated_logging_time': 0.30005645751953125}
I0202 13:13:10.304979 140023005427456 logging_writer.py:48] [10039] accumulated_eval_time=499.522374, accumulated_logging_time=0.300056, accumulated_submission_time=4657.653786, global_step=10039, preemption_count=0, score=4657.653786, test/accuracy=0.200800, test/loss=4.119418, test/num_examples=10000, total_duration=5157.994424, train/accuracy=0.279648, train/loss=3.492593, validation/accuracy=0.257340, validation/loss=3.628937, validation/num_examples=50000
I0202 13:13:35.078753 140022518892288 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.779644787311554, loss=4.378861427307129
I0202 13:14:20.226632 140023005427456 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9235683679580688, loss=4.68268346786499
I0202 13:15:06.613873 140022518892288 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8594942688941956, loss=4.43104362487793
I0202 13:15:53.069920 140023005427456 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.0098915100097656, loss=4.25070858001709
I0202 13:16:39.449156 140022518892288 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7909660935401917, loss=4.32930326461792
I0202 13:17:25.746149 140023005427456 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7702766060829163, loss=5.744535446166992
I0202 13:18:12.042213 140022518892288 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8207916021347046, loss=5.918514251708984
I0202 13:18:58.445928 140023005427456 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.9778714776039124, loss=4.262320041656494
I0202 13:19:44.911283 140022518892288 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8424527645111084, loss=4.745235443115234
I0202 13:20:10.520523 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:20:21.210850 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:20:49.081627 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:20:50.686633 140184451094336 submission_runner.py:408] Time since start: 5618.39s, 	Step: 10957, 	{'train/accuracy': 0.29624998569488525, 'train/loss': 3.400749683380127, 'validation/accuracy': 0.2607399821281433, 'validation/loss': 3.646686553955078, 'validation/num_examples': 50000, 'test/accuracy': 0.1965000033378601, 'test/loss': 4.154642581939697, 'test/num_examples': 10000, 'score': 5077.807441949844, 'total_duration': 5618.391860961914, 'accumulated_submission_time': 5077.807441949844, 'accumulated_eval_time': 539.6884536743164, 'accumulated_logging_time': 0.32504844665527344}
I0202 13:20:50.705895 140023005427456 logging_writer.py:48] [10957] accumulated_eval_time=539.688454, accumulated_logging_time=0.325048, accumulated_submission_time=5077.807442, global_step=10957, preemption_count=0, score=5077.807442, test/accuracy=0.196500, test/loss=4.154643, test/num_examples=10000, total_duration=5618.391861, train/accuracy=0.296250, train/loss=3.400750, validation/accuracy=0.260740, validation/loss=3.646687, validation/num_examples=50000
I0202 13:21:08.329348 140022518892288 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8958429098129272, loss=4.582363128662109
I0202 13:21:52.836841 140023005427456 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7982344627380371, loss=4.5375165939331055
I0202 13:22:39.053272 140022518892288 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.683641791343689, loss=5.503259658813477
I0202 13:23:25.576782 140023005427456 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7569069862365723, loss=4.267767906188965
I0202 13:24:11.713979 140022518892288 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.893497109413147, loss=5.231906890869141
I0202 13:24:58.345451 140023005427456 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8620104193687439, loss=4.606866836547852
I0202 13:25:44.723752 140022518892288 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9400580525398254, loss=4.501862525939941
I0202 13:26:30.898934 140023005427456 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7546855211257935, loss=4.405518531799316
I0202 13:27:17.112784 140022518892288 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8619534373283386, loss=4.313468933105469
I0202 13:27:50.748727 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:28:01.161680 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:28:27.748846 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:28:29.364725 140184451094336 submission_runner.py:408] Time since start: 6077.07s, 	Step: 11874, 	{'train/accuracy': 0.2930664122104645, 'train/loss': 3.4094316959381104, 'validation/accuracy': 0.2763800024986267, 'validation/loss': 3.517740488052368, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.0429534912109375, 'test/num_examples': 10000, 'score': 5497.79346203804, 'total_duration': 6077.069966077805, 'accumulated_submission_time': 5497.79346203804, 'accumulated_eval_time': 578.3044393062592, 'accumulated_logging_time': 0.3541853427886963}
I0202 13:28:29.382024 140023005427456 logging_writer.py:48] [11874] accumulated_eval_time=578.304439, accumulated_logging_time=0.354185, accumulated_submission_time=5497.793462, global_step=11874, preemption_count=0, score=5497.793462, test/accuracy=0.208200, test/loss=4.042953, test/num_examples=10000, total_duration=6077.069966, train/accuracy=0.293066, train/loss=3.409432, validation/accuracy=0.276380, validation/loss=3.517740, validation/num_examples=50000
I0202 13:28:40.193035 140022518892288 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9088510870933533, loss=4.295473098754883
I0202 13:29:23.221557 140023005427456 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9144399762153625, loss=4.795230388641357
I0202 13:30:09.383046 140022518892288 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.250754952430725, loss=4.432422637939453
I0202 13:30:55.546113 140023005427456 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8510709404945374, loss=4.475761413574219
I0202 13:31:41.700135 140022518892288 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7511809468269348, loss=6.180606842041016
I0202 13:32:27.991606 140023005427456 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8135358095169067, loss=5.374018669128418
I0202 13:33:14.370852 140022518892288 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6727339029312134, loss=5.85862398147583
I0202 13:34:00.370648 140023005427456 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.8656859993934631, loss=4.212356090545654
I0202 13:34:46.551030 140022518892288 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.8630878925323486, loss=5.424188613891602
I0202 13:35:29.802895 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:35:40.480297 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:36:08.997225 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:36:10.598914 140184451094336 submission_runner.py:408] Time since start: 6538.30s, 	Step: 12795, 	{'train/accuracy': 0.2978906035423279, 'train/loss': 3.414585590362549, 'validation/accuracy': 0.2738800048828125, 'validation/loss': 3.54900860786438, 'validation/num_examples': 50000, 'test/accuracy': 0.21230001747608185, 'test/loss': 4.034548282623291, 'test/num_examples': 10000, 'score': 5918.157521486282, 'total_duration': 6538.3041615486145, 'accumulated_submission_time': 5918.157521486282, 'accumulated_eval_time': 619.1004593372345, 'accumulated_logging_time': 0.38120341300964355}
I0202 13:36:10.615925 140023005427456 logging_writer.py:48] [12795] accumulated_eval_time=619.100459, accumulated_logging_time=0.381203, accumulated_submission_time=5918.157521, global_step=12795, preemption_count=0, score=5918.157521, test/accuracy=0.212300, test/loss=4.034548, test/num_examples=10000, total_duration=6538.304162, train/accuracy=0.297891, train/loss=3.414586, validation/accuracy=0.273880, validation/loss=3.549009, validation/num_examples=50000
I0202 13:36:13.016348 140022518892288 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7862712144851685, loss=4.644680976867676
I0202 13:36:54.731211 140023005427456 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7316379547119141, loss=5.9185099601745605
I0202 13:37:40.946289 140022518892288 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6741304397583008, loss=5.403319358825684
I0202 13:38:27.250982 140023005427456 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.963908851146698, loss=4.318570137023926
I0202 13:39:13.597131 140022518892288 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6123795509338379, loss=6.170125484466553
I0202 13:39:59.597911 140023005427456 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9692266583442688, loss=4.377283096313477
I0202 13:40:45.757293 140022518892288 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9499334692955017, loss=4.296005725860596
I0202 13:41:32.145317 140023005427456 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9612964987754822, loss=4.241165637969971
I0202 13:42:18.745572 140022518892288 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8677677512168884, loss=4.240525722503662
I0202 13:43:04.711544 140023005427456 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9525246620178223, loss=4.213068962097168
I0202 13:43:10.834882 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:43:21.441377 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:43:53.955987 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:43:55.555540 140184451094336 submission_runner.py:408] Time since start: 7003.26s, 	Step: 13715, 	{'train/accuracy': 0.3108203113079071, 'train/loss': 3.344714641571045, 'validation/accuracy': 0.2833400070667267, 'validation/loss': 3.511706829071045, 'validation/num_examples': 50000, 'test/accuracy': 0.2126000076532364, 'test/loss': 4.019969940185547, 'test/num_examples': 10000, 'score': 6338.3181848526, 'total_duration': 7003.260776519775, 'accumulated_submission_time': 6338.3181848526, 'accumulated_eval_time': 663.8211107254028, 'accumulated_logging_time': 0.40915966033935547}
I0202 13:43:55.572350 140022518892288 logging_writer.py:48] [13715] accumulated_eval_time=663.821111, accumulated_logging_time=0.409160, accumulated_submission_time=6338.318185, global_step=13715, preemption_count=0, score=6338.318185, test/accuracy=0.212600, test/loss=4.019970, test/num_examples=10000, total_duration=7003.260777, train/accuracy=0.310820, train/loss=3.344715, validation/accuracy=0.283340, validation/loss=3.511707, validation/num_examples=50000
I0202 13:44:30.825909 140023005427456 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8621918559074402, loss=5.827727317810059
I0202 13:45:16.993682 140022518892288 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.1395387649536133, loss=4.236080646514893
I0202 13:46:03.296021 140023005427456 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6194532513618469, loss=6.117718696594238
I0202 13:46:49.332465 140022518892288 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.8369024991989136, loss=4.248910903930664
I0202 13:47:35.472509 140023005427456 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8183150291442871, loss=4.202701568603516
I0202 13:48:21.705239 140022518892288 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8494390249252319, loss=4.250916004180908
I0202 13:49:07.891151 140023005427456 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7699524760246277, loss=4.295473575592041
I0202 13:49:54.131201 140022518892288 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8099179267883301, loss=4.121296405792236
I0202 13:50:40.567454 140023005427456 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9214022159576416, loss=4.495444297790527
I0202 13:50:55.961369 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:51:06.617766 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:51:35.059659 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:51:36.663097 140184451094336 submission_runner.py:408] Time since start: 7464.37s, 	Step: 14635, 	{'train/accuracy': 0.31138670444488525, 'train/loss': 3.3009660243988037, 'validation/accuracy': 0.2888199985027313, 'validation/loss': 3.432950496673584, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 3.9814248085021973, 'test/num_examples': 10000, 'score': 6758.649676322937, 'total_duration': 7464.368344545364, 'accumulated_submission_time': 6758.649676322937, 'accumulated_eval_time': 704.5228517055511, 'accumulated_logging_time': 0.43570423126220703}
I0202 13:51:36.680121 140022518892288 logging_writer.py:48] [14635] accumulated_eval_time=704.522852, accumulated_logging_time=0.435704, accumulated_submission_time=6758.649676, global_step=14635, preemption_count=0, score=6758.649676, test/accuracy=0.220500, test/loss=3.981425, test/num_examples=10000, total_duration=7464.368345, train/accuracy=0.311387, train/loss=3.300966, validation/accuracy=0.288820, validation/loss=3.432950, validation/num_examples=50000
I0202 13:52:03.089786 140023005427456 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7811176180839539, loss=5.801952838897705
I0202 13:52:48.488797 140022518892288 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8757323622703552, loss=5.714574813842773
I0202 13:53:34.884369 140023005427456 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.8997581601142883, loss=5.05468225479126
I0202 13:54:21.411995 140022518892288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6824934482574463, loss=5.218642234802246
I0202 13:55:07.743976 140023005427456 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8895592093467712, loss=4.172224044799805
I0202 13:55:54.066296 140022518892288 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7304496765136719, loss=6.083446502685547
I0202 13:56:40.649882 140023005427456 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9760821461677551, loss=4.225032806396484
I0202 13:57:26.961702 140022518892288 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.019934892654419, loss=4.5088210105896
I0202 13:58:13.258441 140023005427456 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8274332284927368, loss=4.286849021911621
I0202 13:58:36.907443 140184451094336 spec.py:321] Evaluating on the training split.
I0202 13:58:47.531370 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 13:59:19.856604 140184451094336 spec.py:349] Evaluating on the test split.
I0202 13:59:21.467240 140184451094336 submission_runner.py:408] Time since start: 7929.17s, 	Step: 15553, 	{'train/accuracy': 0.32255858182907104, 'train/loss': 3.2005932331085205, 'validation/accuracy': 0.30140000581741333, 'validation/loss': 3.3318538665771484, 'validation/num_examples': 50000, 'test/accuracy': 0.23260001838207245, 'test/loss': 3.8751766681671143, 'test/num_examples': 10000, 'score': 7178.820302963257, 'total_duration': 7929.172488689423, 'accumulated_submission_time': 7178.820302963257, 'accumulated_eval_time': 749.0826478004456, 'accumulated_logging_time': 0.462766170501709}
I0202 13:59:21.484610 140022518892288 logging_writer.py:48] [15553] accumulated_eval_time=749.082648, accumulated_logging_time=0.462766, accumulated_submission_time=7178.820303, global_step=15553, preemption_count=0, score=7178.820303, test/accuracy=0.232600, test/loss=3.875177, test/num_examples=10000, total_duration=7929.172489, train/accuracy=0.322559, train/loss=3.200593, validation/accuracy=0.301400, validation/loss=3.331854, validation/num_examples=50000
I0202 13:59:40.696055 140023005427456 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7877606153488159, loss=5.271439075469971
I0202 14:00:25.191853 140022518892288 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.89070063829422, loss=4.137834072113037
I0202 14:01:11.557985 140023005427456 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8950742483139038, loss=4.1176018714904785
I0202 14:01:58.266823 140022518892288 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.9790036082267761, loss=4.217751502990723
I0202 14:02:44.465847 140023005427456 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9309948682785034, loss=4.132190704345703
I0202 14:03:30.880086 140022518892288 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7698395252227783, loss=5.796771049499512
I0202 14:04:16.864576 140023005427456 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.708669900894165, loss=5.994071960449219
I0202 14:05:03.516507 140022518892288 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8234423398971558, loss=4.605895042419434
I0202 14:05:49.356027 140023005427456 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9555296301841736, loss=5.519059181213379
I0202 14:06:21.772174 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:06:32.039278 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:07:00.573097 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:07:02.185958 140184451094336 submission_runner.py:408] Time since start: 8389.89s, 	Step: 16472, 	{'train/accuracy': 0.3410351574420929, 'train/loss': 3.1523284912109375, 'validation/accuracy': 0.31025999784469604, 'validation/loss': 3.32299542427063, 'validation/num_examples': 50000, 'test/accuracy': 0.23600001633167267, 'test/loss': 3.8591465950012207, 'test/num_examples': 10000, 'score': 7599.0502026081085, 'total_duration': 8389.891204357147, 'accumulated_submission_time': 7599.0502026081085, 'accumulated_eval_time': 789.4964265823364, 'accumulated_logging_time': 0.48940229415893555}
I0202 14:07:02.205676 140022518892288 logging_writer.py:48] [16472] accumulated_eval_time=789.496427, accumulated_logging_time=0.489402, accumulated_submission_time=7599.050203, global_step=16472, preemption_count=0, score=7599.050203, test/accuracy=0.236000, test/loss=3.859147, test/num_examples=10000, total_duration=8389.891204, train/accuracy=0.341035, train/loss=3.152328, validation/accuracy=0.310260, validation/loss=3.322995, validation/num_examples=50000
I0202 14:07:14.028820 140023005427456 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.1460671424865723, loss=4.063278675079346
I0202 14:07:57.049789 140022518892288 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.9739133715629578, loss=4.111530780792236
I0202 14:08:43.497609 140023005427456 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9482397437095642, loss=4.329031467437744
I0202 14:09:30.003451 140022518892288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.661523163318634, loss=5.720788955688477
I0202 14:10:16.224702 140023005427456 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9154350757598877, loss=4.065495014190674
I0202 14:11:02.369517 140022518892288 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9757151007652283, loss=4.18324613571167
I0202 14:11:48.494094 140023005427456 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8337856531143188, loss=4.260673999786377
I0202 14:12:34.617077 140022518892288 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9879333972930908, loss=4.317229270935059
I0202 14:13:20.708777 140023005427456 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7827865481376648, loss=4.673954963684082
I0202 14:14:02.477317 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:14:13.890453 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:14:39.898242 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:14:41.498734 140184451094336 submission_runner.py:408] Time since start: 8849.20s, 	Step: 17392, 	{'train/accuracy': 0.3284960985183716, 'train/loss': 3.2336535453796387, 'validation/accuracy': 0.29933997988700867, 'validation/loss': 3.389450788497925, 'validation/num_examples': 50000, 'test/accuracy': 0.22700001299381256, 'test/loss': 3.9175381660461426, 'test/num_examples': 10000, 'score': 8019.264442682266, 'total_duration': 8849.203982591629, 'accumulated_submission_time': 8019.264442682266, 'accumulated_eval_time': 828.5178320407867, 'accumulated_logging_time': 0.5192873477935791}
I0202 14:14:41.517482 140022518892288 logging_writer.py:48] [17392] accumulated_eval_time=828.517832, accumulated_logging_time=0.519287, accumulated_submission_time=8019.264443, global_step=17392, preemption_count=0, score=8019.264443, test/accuracy=0.227000, test/loss=3.917538, test/num_examples=10000, total_duration=8849.203983, train/accuracy=0.328496, train/loss=3.233654, validation/accuracy=0.299340, validation/loss=3.389451, validation/num_examples=50000
I0202 14:14:45.123558 140023005427456 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7851051092147827, loss=5.26802396774292
I0202 14:15:27.128490 140022518892288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.724643886089325, loss=5.9653167724609375
I0202 14:16:13.355707 140023005427456 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0551588535308838, loss=4.256782054901123
I0202 14:16:59.980827 140022518892288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8569457530975342, loss=3.9918508529663086
I0202 14:17:46.204438 140023005427456 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.9083950519561768, loss=4.355256080627441
I0202 14:18:32.735703 140022518892288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8123048543930054, loss=5.065568923950195
I0202 14:19:19.062321 140023005427456 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7306941747665405, loss=4.767064094543457
I0202 14:20:05.421840 140022518892288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.8513537049293518, loss=5.311861991882324
I0202 14:20:51.701048 140023005427456 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.91773921251297, loss=4.722719192504883
I0202 14:21:38.093270 140022518892288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9214829802513123, loss=4.2816572189331055
I0202 14:21:41.967611 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:21:52.345391 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:22:23.652092 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:22:25.262567 140184451094336 submission_runner.py:408] Time since start: 9312.97s, 	Step: 18310, 	{'train/accuracy': 0.3080468773841858, 'train/loss': 3.3751299381256104, 'validation/accuracy': 0.2909199893474579, 'validation/loss': 3.5047309398651123, 'validation/num_examples': 50000, 'test/accuracy': 0.21870000660419464, 'test/loss': 4.029071807861328, 'test/num_examples': 10000, 'score': 8439.656270503998, 'total_duration': 9312.967787742615, 'accumulated_submission_time': 8439.656270503998, 'accumulated_eval_time': 871.812756061554, 'accumulated_logging_time': 0.5486938953399658}
I0202 14:22:25.281695 140023005427456 logging_writer.py:48] [18310] accumulated_eval_time=871.812756, accumulated_logging_time=0.548694, accumulated_submission_time=8439.656271, global_step=18310, preemption_count=0, score=8439.656271, test/accuracy=0.218700, test/loss=4.029072, test/num_examples=10000, total_duration=9312.967788, train/accuracy=0.308047, train/loss=3.375130, validation/accuracy=0.290920, validation/loss=3.504731, validation/num_examples=50000
I0202 14:23:02.897192 140022518892288 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8118628263473511, loss=6.175997734069824
I0202 14:23:48.884865 140023005427456 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9805112481117249, loss=4.334296226501465
I0202 14:24:35.176809 140022518892288 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7629209160804749, loss=4.267281532287598
I0202 14:25:21.450317 140023005427456 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0438411235809326, loss=4.082855224609375
I0202 14:26:07.887395 140022518892288 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.1935805082321167, loss=4.239529609680176
I0202 14:26:54.080392 140023005427456 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8959276676177979, loss=4.248345375061035
I0202 14:27:40.282254 140022518892288 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9781894087791443, loss=4.043281078338623
I0202 14:28:26.231927 140023005427456 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.7953795194625854, loss=4.225079536437988
I0202 14:29:12.455288 140022518892288 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.920016348361969, loss=5.908731460571289
I0202 14:29:25.485278 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:29:36.125903 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:30:09.823132 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:30:11.439277 140184451094336 submission_runner.py:408] Time since start: 9779.14s, 	Step: 19230, 	{'train/accuracy': 0.3353515565395355, 'train/loss': 3.115917921066284, 'validation/accuracy': 0.3131999969482422, 'validation/loss': 3.2783405780792236, 'validation/num_examples': 50000, 'test/accuracy': 0.24080000817775726, 'test/loss': 3.8472251892089844, 'test/num_examples': 10000, 'score': 8859.802419900894, 'total_duration': 9779.14450263977, 'accumulated_submission_time': 8859.802419900894, 'accumulated_eval_time': 917.766725063324, 'accumulated_logging_time': 0.5775036811828613}
I0202 14:30:11.459107 140023005427456 logging_writer.py:48] [19230] accumulated_eval_time=917.766725, accumulated_logging_time=0.577504, accumulated_submission_time=8859.802420, global_step=19230, preemption_count=0, score=8859.802420, test/accuracy=0.240800, test/loss=3.847225, test/num_examples=10000, total_duration=9779.144503, train/accuracy=0.335352, train/loss=3.115918, validation/accuracy=0.313200, validation/loss=3.278341, validation/num_examples=50000
I0202 14:30:39.915514 140022518892288 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0610132217407227, loss=4.191863536834717
I0202 14:31:25.771445 140023005427456 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0087651014328003, loss=3.9180543422698975
I0202 14:32:12.162248 140022518892288 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8537180423736572, loss=4.034160614013672
I0202 14:32:58.268140 140023005427456 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7543544769287109, loss=5.685721397399902
I0202 14:33:44.566604 140022518892288 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9049474000930786, loss=4.04371976852417
I0202 14:34:30.971247 140023005427456 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8402588963508606, loss=4.163123607635498
I0202 14:35:17.064407 140022518892288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7185415029525757, loss=5.676025867462158
I0202 14:36:03.507895 140023005427456 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8887097835540771, loss=6.053211212158203
I0202 14:36:49.885400 140022518892288 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0030467510223389, loss=4.18094539642334
I0202 14:37:11.806314 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:37:22.179337 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:37:52.157408 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:37:53.765089 140184451094336 submission_runner.py:408] Time since start: 10241.47s, 	Step: 20149, 	{'train/accuracy': 0.38164061307907104, 'train/loss': 2.8914709091186523, 'validation/accuracy': 0.32145997881889343, 'validation/loss': 3.214576482772827, 'validation/num_examples': 50000, 'test/accuracy': 0.2436000108718872, 'test/loss': 3.7968075275421143, 'test/num_examples': 10000, 'score': 9280.090950250626, 'total_duration': 10241.470337152481, 'accumulated_submission_time': 9280.090950250626, 'accumulated_eval_time': 959.7255573272705, 'accumulated_logging_time': 0.6077971458435059}
I0202 14:37:53.783200 140023005427456 logging_writer.py:48] [20149] accumulated_eval_time=959.725557, accumulated_logging_time=0.607797, accumulated_submission_time=9280.090950, global_step=20149, preemption_count=0, score=9280.090950, test/accuracy=0.243600, test/loss=3.796808, test/num_examples=10000, total_duration=10241.470337, train/accuracy=0.381641, train/loss=2.891471, validation/accuracy=0.321460, validation/loss=3.214576, validation/num_examples=50000
I0202 14:38:14.603476 140022518892288 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8311536312103271, loss=5.083150863647461
I0202 14:38:59.047796 140023005427456 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8010356426239014, loss=4.560794830322266
I0202 14:39:45.389544 140022518892288 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8538795113563538, loss=4.141664028167725
I0202 14:40:31.740916 140023005427456 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9069688320159912, loss=4.581877708435059
I0202 14:41:17.904317 140022518892288 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7013818621635437, loss=5.5065598487854
I0202 14:42:04.299849 140023005427456 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.2072525024414062, loss=4.109558582305908
I0202 14:42:50.286362 140022518892288 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0978657007217407, loss=4.133142471313477
I0202 14:43:36.479077 140023005427456 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7640658617019653, loss=5.156747341156006
I0202 14:44:22.598887 140022518892288 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9575952291488647, loss=4.015190124511719
I0202 14:44:54.071208 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:45:05.347455 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:45:37.029127 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:45:38.635372 140184451094336 submission_runner.py:408] Time since start: 10706.34s, 	Step: 21070, 	{'train/accuracy': 0.35353514552116394, 'train/loss': 2.995561361312866, 'validation/accuracy': 0.3300800025463104, 'validation/loss': 3.140366554260254, 'validation/num_examples': 50000, 'test/accuracy': 0.2556000053882599, 'test/loss': 3.7248618602752686, 'test/num_examples': 10000, 'score': 9700.320873737335, 'total_duration': 10706.340615034103, 'accumulated_submission_time': 9700.320873737335, 'accumulated_eval_time': 1004.2897145748138, 'accumulated_logging_time': 0.6361017227172852}
I0202 14:45:38.652929 140023005427456 logging_writer.py:48] [21070] accumulated_eval_time=1004.289715, accumulated_logging_time=0.636102, accumulated_submission_time=9700.320874, global_step=21070, preemption_count=0, score=9700.320874, test/accuracy=0.255600, test/loss=3.724862, test/num_examples=10000, total_duration=10706.340615, train/accuracy=0.353535, train/loss=2.995561, validation/accuracy=0.330080, validation/loss=3.140367, validation/num_examples=50000
I0202 14:45:51.071817 140022518892288 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.5796968936920166, loss=3.982886791229248
I0202 14:46:34.606543 140023005427456 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9732951521873474, loss=4.029385089874268
I0202 14:47:20.992407 140022518892288 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0053967237472534, loss=4.0314507484436035
I0202 14:48:07.279927 140023005427456 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7150847315788269, loss=5.63381290435791
I0202 14:48:53.306684 140022518892288 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8033204078674316, loss=4.016541481018066
I0202 14:49:39.431997 140023005427456 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7168801426887512, loss=5.57895040512085
I0202 14:50:25.495720 140022518892288 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.083492398262024, loss=4.103032112121582
I0202 14:51:11.846215 140023005427456 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9616671204566956, loss=4.235451698303223
I0202 14:51:58.255728 140022518892288 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.011450171470642, loss=4.3168487548828125
I0202 14:52:38.906143 140184451094336 spec.py:321] Evaluating on the training split.
I0202 14:52:49.579409 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 14:53:25.919008 140184451094336 spec.py:349] Evaluating on the test split.
I0202 14:53:27.514984 140184451094336 submission_runner.py:408] Time since start: 11175.22s, 	Step: 21990, 	{'train/accuracy': 0.3414843678474426, 'train/loss': 3.077301025390625, 'validation/accuracy': 0.31797999143600464, 'validation/loss': 3.2187304496765137, 'validation/num_examples': 50000, 'test/accuracy': 0.24730001389980316, 'test/loss': 3.81091570854187, 'test/num_examples': 10000, 'score': 10120.517220973969, 'total_duration': 11175.220227003098, 'accumulated_submission_time': 10120.517220973969, 'accumulated_eval_time': 1052.8985612392426, 'accumulated_logging_time': 0.662848949432373}
I0202 14:53:27.532909 140023005427456 logging_writer.py:48] [21990] accumulated_eval_time=1052.898561, accumulated_logging_time=0.662849, accumulated_submission_time=10120.517221, global_step=21990, preemption_count=0, score=10120.517221, test/accuracy=0.247300, test/loss=3.810916, test/num_examples=10000, total_duration=11175.220227, train/accuracy=0.341484, train/loss=3.077301, validation/accuracy=0.317980, validation/loss=3.218730, validation/num_examples=50000
I0202 14:53:31.937696 140022518892288 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.043926477432251, loss=4.331083297729492
I0202 14:54:14.381479 140023005427456 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9351376891136169, loss=4.125846862792969
I0202 14:55:00.292432 140022518892288 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7985778450965881, loss=4.126152038574219
I0202 14:55:47.106293 140023005427456 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.8609897494316101, loss=4.336035251617432
I0202 14:56:33.290225 140022518892288 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.908417284488678, loss=4.111521244049072
I0202 14:57:19.637646 140023005427456 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.0496715307235718, loss=3.9208476543426514
I0202 14:58:06.352340 140022518892288 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7750569581985474, loss=4.988554000854492
I0202 14:58:52.243909 140023005427456 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.8976620435714722, loss=3.991473436355591
I0202 14:59:38.583115 140022518892288 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8682863116264343, loss=6.099746227264404
I0202 15:00:24.625660 140023005427456 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9310627579689026, loss=5.688647747039795
I0202 15:00:27.614969 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:00:38.005830 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:01:07.202822 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:01:08.801224 140184451094336 submission_runner.py:408] Time since start: 11636.51s, 	Step: 22908, 	{'train/accuracy': 0.35158202052116394, 'train/loss': 3.0414631366729736, 'validation/accuracy': 0.3215000033378601, 'validation/loss': 3.234511137008667, 'validation/num_examples': 50000, 'test/accuracy': 0.24690000712871552, 'test/loss': 3.816385269165039, 'test/num_examples': 10000, 'score': 10540.53809094429, 'total_duration': 11636.506449222565, 'accumulated_submission_time': 10540.53809094429, 'accumulated_eval_time': 1094.0847754478455, 'accumulated_logging_time': 0.6952741146087646}
I0202 15:01:08.821267 140022518892288 logging_writer.py:48] [22908] accumulated_eval_time=1094.084775, accumulated_logging_time=0.695274, accumulated_submission_time=10540.538091, global_step=22908, preemption_count=0, score=10540.538091, test/accuracy=0.246900, test/loss=3.816385, test/num_examples=10000, total_duration=11636.506449, train/accuracy=0.351582, train/loss=3.041463, validation/accuracy=0.321500, validation/loss=3.234511, validation/num_examples=50000
I0202 15:01:47.561290 140023005427456 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8050281405448914, loss=5.187289237976074
I0202 15:02:33.646475 140022518892288 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0556211471557617, loss=4.371386528015137
I0202 15:03:20.281509 140023005427456 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.847737729549408, loss=4.611859321594238
I0202 15:04:06.539927 140022518892288 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9752779006958008, loss=4.032173156738281
I0202 15:04:52.782869 140023005427456 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.639300525188446, loss=5.8125200271606445
I0202 15:05:38.988154 140022518892288 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7751167416572571, loss=4.170340061187744
I0202 15:06:25.207665 140023005427456 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.1363388299942017, loss=3.847182035446167
I0202 15:07:11.789063 140022518892288 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.8833111524581909, loss=4.024164199829102
I0202 15:07:58.045946 140023005427456 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8632328510284424, loss=4.13137149810791
I0202 15:08:08.921377 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:08:19.382422 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:08:55.068124 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:08:56.674243 140184451094336 submission_runner.py:408] Time since start: 12104.38s, 	Step: 23825, 	{'train/accuracy': 0.35593748092651367, 'train/loss': 3.025038719177246, 'validation/accuracy': 0.3379800021648407, 'validation/loss': 3.137194871902466, 'validation/num_examples': 50000, 'test/accuracy': 0.26260000467300415, 'test/loss': 3.7027549743652344, 'test/num_examples': 10000, 'score': 10960.580304384232, 'total_duration': 12104.379473924637, 'accumulated_submission_time': 10960.580304384232, 'accumulated_eval_time': 1141.8376290798187, 'accumulated_logging_time': 0.7260003089904785}
I0202 15:08:56.692279 140022518892288 logging_writer.py:48] [23825] accumulated_eval_time=1141.837629, accumulated_logging_time=0.726000, accumulated_submission_time=10960.580304, global_step=23825, preemption_count=0, score=10960.580304, test/accuracy=0.262600, test/loss=3.702755, test/num_examples=10000, total_duration=12104.379474, train/accuracy=0.355937, train/loss=3.025039, validation/accuracy=0.337980, validation/loss=3.137195, validation/num_examples=50000
I0202 15:09:27.265811 140023005427456 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9334688782691956, loss=4.796571731567383
I0202 15:10:13.103614 140022518892288 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.3132959604263306, loss=4.254181861877441
I0202 15:10:59.216327 140023005427456 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0792427062988281, loss=4.170093059539795
I0202 15:11:45.533779 140022518892288 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.935998797416687, loss=3.979865789413452
I0202 15:12:31.968719 140023005427456 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8215898871421814, loss=4.598053932189941
I0202 15:13:18.147998 140022518892288 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8997998833656311, loss=5.008248329162598
I0202 15:14:04.510989 140023005427456 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.9013757705688477, loss=4.4096574783325195
I0202 15:14:50.501338 140022518892288 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.9324969053268433, loss=4.466037750244141
I0202 15:15:36.748517 140023005427456 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0710077285766602, loss=4.113500595092773
I0202 15:15:57.122364 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:16:07.846775 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:16:37.385137 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:16:38.985034 140184451094336 submission_runner.py:408] Time since start: 12566.69s, 	Step: 24746, 	{'train/accuracy': 0.35679686069488525, 'train/loss': 3.0097639560699463, 'validation/accuracy': 0.3316799998283386, 'validation/loss': 3.17022705078125, 'validation/num_examples': 50000, 'test/accuracy': 0.25609999895095825, 'test/loss': 3.7653188705444336, 'test/num_examples': 10000, 'score': 11380.953262805939, 'total_duration': 12566.690264701843, 'accumulated_submission_time': 11380.953262805939, 'accumulated_eval_time': 1183.7002770900726, 'accumulated_logging_time': 0.754091739654541}
I0202 15:16:39.009018 140022518892288 logging_writer.py:48] [24746] accumulated_eval_time=1183.700277, accumulated_logging_time=0.754092, accumulated_submission_time=11380.953263, global_step=24746, preemption_count=0, score=11380.953263, test/accuracy=0.256100, test/loss=3.765319, test/num_examples=10000, total_duration=12566.690265, train/accuracy=0.356797, train/loss=3.009764, validation/accuracy=0.331680, validation/loss=3.170227, validation/num_examples=50000
I0202 15:17:01.180429 140023005427456 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.156960129737854, loss=3.9049150943756104
I0202 15:17:45.920239 140022518892288 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7334498167037964, loss=5.336983680725098
I0202 15:18:32.278422 140023005427456 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.013848900794983, loss=3.9318041801452637
I0202 15:19:18.728951 140022518892288 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.625544011592865, loss=5.585371017456055
I0202 15:20:04.979815 140023005427456 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.9138680696487427, loss=4.1740264892578125
I0202 15:20:51.182031 140022518892288 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7962498664855957, loss=4.399789333343506
I0202 15:21:37.645850 140023005427456 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.1084256172180176, loss=3.9776740074157715
I0202 15:22:23.885313 140022518892288 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9024623036384583, loss=4.117312908172607
I0202 15:23:10.278958 140023005427456 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8724908828735352, loss=4.021941184997559
I0202 15:23:39.356835 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:23:49.657287 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:24:22.487552 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:24:24.093772 140184451094336 submission_runner.py:408] Time since start: 13031.80s, 	Step: 25665, 	{'train/accuracy': 0.3615429699420929, 'train/loss': 3.016035795211792, 'validation/accuracy': 0.3340199887752533, 'validation/loss': 3.1701815128326416, 'validation/num_examples': 50000, 'test/accuracy': 0.25550001859664917, 'test/loss': 3.7496232986450195, 'test/num_examples': 10000, 'score': 11801.243542194366, 'total_duration': 13031.799020767212, 'accumulated_submission_time': 11801.243542194366, 'accumulated_eval_time': 1228.4372079372406, 'accumulated_logging_time': 0.7881994247436523}
I0202 15:24:24.112759 140022518892288 logging_writer.py:48] [25665] accumulated_eval_time=1228.437208, accumulated_logging_time=0.788199, accumulated_submission_time=11801.243542, global_step=25665, preemption_count=0, score=11801.243542, test/accuracy=0.255500, test/loss=3.749623, test/num_examples=10000, total_duration=13031.799021, train/accuracy=0.361543, train/loss=3.016036, validation/accuracy=0.334020, validation/loss=3.170182, validation/num_examples=50000
I0202 15:24:38.525503 140023005427456 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8278096318244934, loss=4.921285152435303
I0202 15:25:22.392359 140022518892288 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9467499256134033, loss=6.1914801597595215
I0202 15:26:08.468738 140023005427456 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8620820641517639, loss=4.107219219207764
I0202 15:26:54.760969 140022518892288 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8792496919631958, loss=4.440596580505371
I0202 15:27:41.071827 140023005427456 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9377230405807495, loss=4.302942276000977
I0202 15:28:27.272310 140022518892288 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9915310144424438, loss=4.054027557373047
I0202 15:29:13.631805 140023005427456 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.9034400582313538, loss=3.9204132556915283
I0202 15:29:59.858586 140022518892288 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9568015336990356, loss=5.70947790145874
I0202 15:30:46.182343 140023005427456 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7831379771232605, loss=6.0003533363342285
I0202 15:31:24.582209 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:31:35.115538 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:32:09.874539 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:32:11.479024 140184451094336 submission_runner.py:408] Time since start: 13499.18s, 	Step: 26585, 	{'train/accuracy': 0.346992164850235, 'train/loss': 3.0902798175811768, 'validation/accuracy': 0.3211599886417389, 'validation/loss': 3.2521159648895264, 'validation/num_examples': 50000, 'test/accuracy': 0.24740001559257507, 'test/loss': 3.8121447563171387, 'test/num_examples': 10000, 'score': 12221.656418085098, 'total_duration': 13499.184258460999, 'accumulated_submission_time': 12221.656418085098, 'accumulated_eval_time': 1275.3340392112732, 'accumulated_logging_time': 0.8167154788970947}
I0202 15:32:11.502086 140022518892288 logging_writer.py:48] [26585] accumulated_eval_time=1275.334039, accumulated_logging_time=0.816715, accumulated_submission_time=12221.656418, global_step=26585, preemption_count=0, score=12221.656418, test/accuracy=0.247400, test/loss=3.812145, test/num_examples=10000, total_duration=13499.184258, train/accuracy=0.346992, train/loss=3.090280, validation/accuracy=0.321160, validation/loss=3.252116, validation/num_examples=50000
I0202 15:32:17.913992 140023005427456 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0415732860565186, loss=4.147299289703369
I0202 15:33:00.892408 140022518892288 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.118047833442688, loss=4.011740684509277
I0202 15:33:47.179739 140023005427456 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.793411374092102, loss=6.104811668395996
I0202 15:34:33.720726 140022518892288 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.8910431861877441, loss=3.940840721130371
I0202 15:35:20.146365 140023005427456 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.3483660221099854, loss=4.052022457122803
I0202 15:36:06.381934 140022518892288 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.0065176486968994, loss=3.8312325477600098
I0202 15:36:52.683914 140023005427456 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9042950868606567, loss=4.394715309143066
I0202 15:37:39.080221 140022518892288 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6877937316894531, loss=4.764461040496826
I0202 15:38:25.484840 140023005427456 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9071451425552368, loss=3.8799424171447754
I0202 15:39:11.882272 140022518892288 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.888146162033081, loss=4.126760482788086
I0202 15:39:11.898364 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:39:22.457480 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:39:49.612514 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:39:51.225268 140184451094336 submission_runner.py:408] Time since start: 13958.93s, 	Step: 27501, 	{'train/accuracy': 0.36302733421325684, 'train/loss': 2.980881929397583, 'validation/accuracy': 0.3391999900341034, 'validation/loss': 3.1102354526519775, 'validation/num_examples': 50000, 'test/accuracy': 0.25530001521110535, 'test/loss': 3.7012126445770264, 'test/num_examples': 10000, 'score': 12641.995535612106, 'total_duration': 13958.930496454239, 'accumulated_submission_time': 12641.995535612106, 'accumulated_eval_time': 1314.6609327793121, 'accumulated_logging_time': 0.8498325347900391}
I0202 15:39:51.246769 140023005427456 logging_writer.py:48] [27501] accumulated_eval_time=1314.660933, accumulated_logging_time=0.849833, accumulated_submission_time=12641.995536, global_step=27501, preemption_count=0, score=12641.995536, test/accuracy=0.255300, test/loss=3.701213, test/num_examples=10000, total_duration=13958.930496, train/accuracy=0.363027, train/loss=2.980882, validation/accuracy=0.339200, validation/loss=3.110235, validation/num_examples=50000
I0202 15:40:32.997752 140022518892288 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6764577031135559, loss=5.770453453063965
I0202 15:41:19.369086 140023005427456 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.0996509790420532, loss=3.9646804332733154
I0202 15:42:06.363838 140022518892288 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.82927405834198, loss=3.927609920501709
I0202 15:42:52.405806 140023005427456 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9897461533546448, loss=4.141818046569824
I0202 15:43:38.770974 140022518892288 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.0411522388458252, loss=4.1638007164001465
I0202 15:44:25.156938 140023005427456 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.9612515568733215, loss=4.071881294250488
I0202 15:45:11.281515 140022518892288 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.8442918658256531, loss=4.110370635986328
I0202 15:45:57.578241 140023005427456 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.9060492515563965, loss=4.047963619232178
I0202 15:46:43.721027 140022518892288 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6451227068901062, loss=5.413881301879883
I0202 15:46:51.380303 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:47:01.961176 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:47:27.813774 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:47:29.424615 140184451094336 submission_runner.py:408] Time since start: 14417.13s, 	Step: 28418, 	{'train/accuracy': 0.3716992139816284, 'train/loss': 2.924288034439087, 'validation/accuracy': 0.3419399857521057, 'validation/loss': 3.0865392684936523, 'validation/num_examples': 50000, 'test/accuracy': 0.2589000165462494, 'test/loss': 3.6658432483673096, 'test/num_examples': 10000, 'score': 13062.072076559067, 'total_duration': 14417.129843950272, 'accumulated_submission_time': 13062.072076559067, 'accumulated_eval_time': 1352.705206155777, 'accumulated_logging_time': 0.8814163208007812}
I0202 15:47:29.451437 140023005427456 logging_writer.py:48] [28418] accumulated_eval_time=1352.705206, accumulated_logging_time=0.881416, accumulated_submission_time=13062.072077, global_step=28418, preemption_count=0, score=13062.072077, test/accuracy=0.258900, test/loss=3.665843, test/num_examples=10000, total_duration=14417.129844, train/accuracy=0.371699, train/loss=2.924288, validation/accuracy=0.341940, validation/loss=3.086539, validation/num_examples=50000
I0202 15:48:03.583023 140022518892288 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8966849446296692, loss=3.925994873046875
I0202 15:48:49.674547 140023005427456 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.9849509596824646, loss=4.7382988929748535
I0202 15:49:36.357915 140022518892288 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9453108906745911, loss=3.9772825241088867
I0202 15:50:22.497004 140023005427456 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7104707360267639, loss=5.317533016204834
I0202 15:51:08.766050 140022518892288 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.910195529460907, loss=3.9381418228149414
I0202 15:51:55.340053 140023005427456 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8767237663269043, loss=5.397586822509766
I0202 15:52:41.594983 140022518892288 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9465889930725098, loss=4.083540439605713
I0202 15:53:27.711119 140023005427456 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9083303809165955, loss=6.006672382354736
I0202 15:54:14.159129 140022518892288 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.9589055776596069, loss=3.8910741806030273
I0202 15:54:29.580337 140184451094336 spec.py:321] Evaluating on the training split.
I0202 15:54:39.947670 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 15:55:11.451754 140184451094336 spec.py:349] Evaluating on the test split.
I0202 15:55:13.045583 140184451094336 submission_runner.py:408] Time since start: 14880.75s, 	Step: 29335, 	{'train/accuracy': 0.3422265648841858, 'train/loss': 3.173394203186035, 'validation/accuracy': 0.31610000133514404, 'validation/loss': 3.3170182704925537, 'validation/num_examples': 50000, 'test/accuracy': 0.24340000748634338, 'test/loss': 3.895664930343628, 'test/num_examples': 10000, 'score': 13482.143792390823, 'total_duration': 14880.750834703445, 'accumulated_submission_time': 13482.143792390823, 'accumulated_eval_time': 1396.1704378128052, 'accumulated_logging_time': 0.9180092811584473}
I0202 15:55:13.064356 140023005427456 logging_writer.py:48] [29335] accumulated_eval_time=1396.170438, accumulated_logging_time=0.918009, accumulated_submission_time=13482.143792, global_step=29335, preemption_count=0, score=13482.143792, test/accuracy=0.243400, test/loss=3.895665, test/num_examples=10000, total_duration=14880.750835, train/accuracy=0.342227, train/loss=3.173394, validation/accuracy=0.316100, validation/loss=3.317018, validation/num_examples=50000
I0202 15:55:39.487673 140022518892288 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.8080854415893555, loss=6.138969898223877
I0202 15:56:24.990644 140023005427456 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1181203126907349, loss=3.9009275436401367
I0202 15:57:11.194311 140022518892288 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.016875982284546, loss=3.974222421646118
I0202 15:57:57.163125 140023005427456 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.005520224571228, loss=4.012880802154541
I0202 15:58:43.476017 140022518892288 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.9252058267593384, loss=3.81711745262146
I0202 15:59:29.706888 140023005427456 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.7439295053482056, loss=5.820873260498047
I0202 16:00:15.871944 140022518892288 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9114922285079956, loss=3.855950117111206
I0202 16:01:02.027060 140023005427456 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9116072654724121, loss=3.8249895572662354
I0202 16:01:48.398559 140022518892288 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.3062435388565063, loss=3.938143253326416
I0202 16:02:13.139170 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:02:23.578423 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:02:58.377583 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:02:59.976554 140184451094336 submission_runner.py:408] Time since start: 15347.68s, 	Step: 30255, 	{'train/accuracy': 0.3800390660762787, 'train/loss': 2.861891746520996, 'validation/accuracy': 0.35273998975753784, 'validation/loss': 3.0179009437561035, 'validation/num_examples': 50000, 'test/accuracy': 0.26980000734329224, 'test/loss': 3.616143226623535, 'test/num_examples': 10000, 'score': 13902.161823272705, 'total_duration': 15347.681805610657, 'accumulated_submission_time': 13902.161823272705, 'accumulated_eval_time': 1443.007826089859, 'accumulated_logging_time': 0.9461681842803955}
I0202 16:02:59.995643 140023005427456 logging_writer.py:48] [30255] accumulated_eval_time=1443.007826, accumulated_logging_time=0.946168, accumulated_submission_time=13902.161823, global_step=30255, preemption_count=0, score=13902.161823, test/accuracy=0.269800, test/loss=3.616143, test/num_examples=10000, total_duration=15347.681806, train/accuracy=0.380039, train/loss=2.861892, validation/accuracy=0.352740, validation/loss=3.017901, validation/num_examples=50000
I0202 16:03:18.419347 140022518892288 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0361660718917847, loss=4.030797481536865
I0202 16:04:02.658660 140023005427456 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.9284324049949646, loss=3.7687630653381348
I0202 16:04:48.963020 140022518892288 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.882951021194458, loss=4.47991943359375
I0202 16:05:35.402431 140023005427456 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0635197162628174, loss=3.8532843589782715
I0202 16:06:21.626445 140022518892288 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0214041471481323, loss=4.050052165985107
I0202 16:07:07.700490 140023005427456 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.0401451587677002, loss=3.9893832206726074
I0202 16:07:53.802031 140022518892288 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0273609161376953, loss=4.022286415100098
I0202 16:08:39.939175 140023005427456 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.8370605707168579, loss=5.80402946472168
I0202 16:09:26.100035 140022518892288 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.7388686537742615, loss=5.832780838012695
I0202 16:10:00.314002 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:10:10.918284 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:10:37.976401 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:10:39.580874 140184451094336 submission_runner.py:408] Time since start: 15807.29s, 	Step: 31176, 	{'train/accuracy': 0.3661523461341858, 'train/loss': 2.9577431678771973, 'validation/accuracy': 0.3359200060367584, 'validation/loss': 3.120572328567505, 'validation/num_examples': 50000, 'test/accuracy': 0.25609999895095825, 'test/loss': 3.68998384475708, 'test/num_examples': 10000, 'score': 14322.417489528656, 'total_duration': 15807.286099910736, 'accumulated_submission_time': 14322.417489528656, 'accumulated_eval_time': 1482.2746975421906, 'accumulated_logging_time': 0.9801223278045654}
I0202 16:10:39.606822 140023005427456 logging_writer.py:48] [31176] accumulated_eval_time=1482.274698, accumulated_logging_time=0.980122, accumulated_submission_time=14322.417490, global_step=31176, preemption_count=0, score=14322.417490, test/accuracy=0.256100, test/loss=3.689984, test/num_examples=10000, total_duration=15807.286100, train/accuracy=0.366152, train/loss=2.957743, validation/accuracy=0.335920, validation/loss=3.120572, validation/num_examples=50000
I0202 16:10:49.614573 140022518892288 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.8223532438278198, loss=5.413018226623535
I0202 16:11:33.001685 140023005427456 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.9762371778488159, loss=3.961756944656372
I0202 16:12:19.269712 140022518892288 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9553001523017883, loss=4.967514991760254
I0202 16:13:05.615383 140023005427456 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9213918447494507, loss=3.8859169483184814
I0202 16:13:51.772173 140022518892288 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.026796817779541, loss=3.9506025314331055
I0202 16:14:38.368572 140023005427456 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6781234741210938, loss=6.00429630279541
I0202 16:15:24.682659 140022518892288 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.9537950754165649, loss=3.8731472492218018
I0202 16:16:11.088216 140023005427456 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.6237303614616394, loss=5.828975200653076
I0202 16:16:57.287314 140022518892288 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.069102168083191, loss=4.556231498718262
I0202 16:17:39.985181 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:17:50.408087 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:18:18.447221 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:18:20.047539 140184451094336 submission_runner.py:408] Time since start: 16267.75s, 	Step: 32094, 	{'train/accuracy': 0.40453124046325684, 'train/loss': 2.735917806625366, 'validation/accuracy': 0.3504199981689453, 'validation/loss': 3.0416879653930664, 'validation/num_examples': 50000, 'test/accuracy': 0.26990002393722534, 'test/loss': 3.6354501247406006, 'test/num_examples': 10000, 'score': 14742.738109588623, 'total_duration': 16267.752788543701, 'accumulated_submission_time': 14742.738109588623, 'accumulated_eval_time': 1522.337045431137, 'accumulated_logging_time': 1.0161523818969727}
I0202 16:18:20.068097 140023005427456 logging_writer.py:48] [32094] accumulated_eval_time=1522.337045, accumulated_logging_time=1.016152, accumulated_submission_time=14742.738110, global_step=32094, preemption_count=0, score=14742.738110, test/accuracy=0.269900, test/loss=3.635450, test/num_examples=10000, total_duration=16267.752789, train/accuracy=0.404531, train/loss=2.735918, validation/accuracy=0.350420, validation/loss=3.041688, validation/num_examples=50000
I0202 16:18:22.876393 140022518892288 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8993692398071289, loss=4.056129455566406
I0202 16:19:05.324880 140023005427456 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.2640310525894165, loss=4.065512657165527
I0202 16:19:51.321862 140022518892288 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.7735973000526428, loss=5.107915878295898
I0202 16:20:37.490170 140023005427456 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.9058318734169006, loss=4.117984771728516
I0202 16:21:23.645836 140022518892288 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0081489086151123, loss=4.516928672790527
I0202 16:22:10.169211 140023005427456 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.2412055730819702, loss=4.148975849151611
I0202 16:22:56.260067 140022518892288 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7839048504829407, loss=4.469538688659668
I0202 16:23:42.350870 140023005427456 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.747948169708252, loss=5.071908950805664
I0202 16:24:28.725570 140022518892288 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7501766085624695, loss=4.957223892211914
I0202 16:25:14.905655 140023005427456 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.971983790397644, loss=4.003484725952148
I0202 16:25:20.103002 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:25:30.825725 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:26:04.326274 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:26:05.925482 140184451094336 submission_runner.py:408] Time since start: 16733.63s, 	Step: 33013, 	{'train/accuracy': 0.371406227350235, 'train/loss': 2.9292590618133545, 'validation/accuracy': 0.34553998708724976, 'validation/loss': 3.0680296421051025, 'validation/num_examples': 50000, 'test/accuracy': 0.27250000834465027, 'test/loss': 3.6371521949768066, 'test/num_examples': 10000, 'score': 15162.715245962143, 'total_duration': 16733.630709409714, 'accumulated_submission_time': 15162.715245962143, 'accumulated_eval_time': 1568.1595079898834, 'accumulated_logging_time': 1.0463981628417969}
I0202 16:26:05.948713 140022518892288 logging_writer.py:48] [33013] accumulated_eval_time=1568.159508, accumulated_logging_time=1.046398, accumulated_submission_time=15162.715246, global_step=33013, preemption_count=0, score=15162.715246, test/accuracy=0.272500, test/loss=3.637152, test/num_examples=10000, total_duration=16733.630709, train/accuracy=0.371406, train/loss=2.929259, validation/accuracy=0.345540, validation/loss=3.068030, validation/num_examples=50000
I0202 16:26:42.095034 140023005427456 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.8195416331291199, loss=4.426539897918701
I0202 16:27:28.060558 140022518892288 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.1771314144134521, loss=3.8682444095611572
I0202 16:28:14.391413 140023005427456 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.8542426824569702, loss=5.902512073516846
I0202 16:29:00.555287 140022518892288 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8708188533782959, loss=4.471983909606934
I0202 16:29:46.801367 140023005427456 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.9837961196899414, loss=3.992924928665161
I0202 16:30:32.850479 140022518892288 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.097110390663147, loss=4.005343437194824
I0202 16:31:19.021634 140023005427456 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.9251642227172852, loss=3.8378052711486816
I0202 16:32:05.662635 140022518892288 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.8242746591567993, loss=3.885019302368164
I0202 16:32:51.781999 140023005427456 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9888201355934143, loss=4.347524166107178
I0202 16:33:06.332603 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:33:16.573711 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:33:47.217914 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:33:48.813464 140184451094336 submission_runner.py:408] Time since start: 17196.52s, 	Step: 33933, 	{'train/accuracy': 0.3720703125, 'train/loss': 2.9328975677490234, 'validation/accuracy': 0.3447999954223633, 'validation/loss': 3.0781383514404297, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.6681697368621826, 'test/num_examples': 10000, 'score': 15583.04086136818, 'total_duration': 17196.518713235855, 'accumulated_submission_time': 15583.04086136818, 'accumulated_eval_time': 1610.6403777599335, 'accumulated_logging_time': 1.0798285007476807}
I0202 16:33:48.837867 140022518892288 logging_writer.py:48] [33933] accumulated_eval_time=1610.640378, accumulated_logging_time=1.079829, accumulated_submission_time=15583.040861, global_step=33933, preemption_count=0, score=15583.040861, test/accuracy=0.267500, test/loss=3.668170, test/num_examples=10000, total_duration=17196.518713, train/accuracy=0.372070, train/loss=2.932898, validation/accuracy=0.344800, validation/loss=3.078138, validation/num_examples=50000
I0202 16:34:16.046109 140023005427456 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0040477514266968, loss=3.758373498916626
I0202 16:35:01.782547 140022518892288 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7600476145744324, loss=4.619525909423828
I0202 16:35:48.185756 140023005427456 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8252649903297424, loss=5.785304069519043
I0202 16:36:34.391098 140022518892288 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.8673673868179321, loss=4.112271308898926
I0202 16:37:20.626759 140023005427456 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.864051878452301, loss=4.408515453338623
I0202 16:38:06.901105 140022518892288 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9310391545295715, loss=3.9608683586120605
I0202 16:38:53.100243 140023005427456 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.7864768505096436, loss=4.939683437347412
I0202 16:39:39.545943 140022518892288 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.2790976762771606, loss=3.984696865081787
I0202 16:40:25.854927 140023005427456 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8574214577674866, loss=5.577008247375488
I0202 16:40:49.118452 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:40:59.363005 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:41:29.990615 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:41:31.607682 140184451094336 submission_runner.py:408] Time since start: 17659.31s, 	Step: 34852, 	{'train/accuracy': 0.39851561188697815, 'train/loss': 2.755237579345703, 'validation/accuracy': 0.3614400029182434, 'validation/loss': 2.9586539268493652, 'validation/num_examples': 50000, 'test/accuracy': 0.27090001106262207, 'test/loss': 3.5934412479400635, 'test/num_examples': 10000, 'score': 16003.265714883804, 'total_duration': 17659.312923192978, 'accumulated_submission_time': 16003.265714883804, 'accumulated_eval_time': 1653.1295936107635, 'accumulated_logging_time': 1.1132240295410156}
I0202 16:41:31.627971 140022518892288 logging_writer.py:48] [34852] accumulated_eval_time=1653.129594, accumulated_logging_time=1.113224, accumulated_submission_time=16003.265715, global_step=34852, preemption_count=0, score=16003.265715, test/accuracy=0.270900, test/loss=3.593441, test/num_examples=10000, total_duration=17659.312923, train/accuracy=0.398516, train/loss=2.755238, validation/accuracy=0.361440, validation/loss=2.958654, validation/num_examples=50000
I0202 16:41:51.254237 140023005427456 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8909288048744202, loss=3.9381675720214844
I0202 16:42:35.889686 140022518892288 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.9881849884986877, loss=4.271842002868652
I0202 16:43:22.324318 140023005427456 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9022390842437744, loss=3.8416707515716553
I0202 16:44:08.642789 140022518892288 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.8523595333099365, loss=3.8956258296966553
I0202 16:44:54.820793 140023005427456 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.018442153930664, loss=3.843839645385742
I0202 16:45:41.233447 140022518892288 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7104371190071106, loss=5.6210527420043945
I0202 16:46:27.342975 140023005427456 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.2199690341949463, loss=3.9924895763397217
I0202 16:47:13.421098 140022518892288 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0239369869232178, loss=4.765498638153076
I0202 16:47:59.661071 140023005427456 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.9687979817390442, loss=3.964836359024048
I0202 16:48:32.018346 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:48:42.297464 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:49:17.687867 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:49:19.284014 140184451094336 submission_runner.py:408] Time since start: 18126.99s, 	Step: 35772, 	{'train/accuracy': 0.3848632872104645, 'train/loss': 2.830144166946411, 'validation/accuracy': 0.3606799840927124, 'validation/loss': 2.9782936573028564, 'validation/num_examples': 50000, 'test/accuracy': 0.27400001883506775, 'test/loss': 3.591972827911377, 'test/num_examples': 10000, 'score': 16423.598749876022, 'total_duration': 18126.9892642498, 'accumulated_submission_time': 16423.598749876022, 'accumulated_eval_time': 1700.3952662944794, 'accumulated_logging_time': 1.143989086151123}
I0202 16:49:19.304001 140022518892288 logging_writer.py:48] [35772] accumulated_eval_time=1700.395266, accumulated_logging_time=1.143989, accumulated_submission_time=16423.598750, global_step=35772, preemption_count=0, score=16423.598750, test/accuracy=0.274000, test/loss=3.591973, test/num_examples=10000, total_duration=18126.989264, train/accuracy=0.384863, train/loss=2.830144, validation/accuracy=0.360680, validation/loss=2.978294, validation/num_examples=50000
I0202 16:49:30.902694 140023005427456 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.9604571461677551, loss=3.8347365856170654
I0202 16:50:14.133466 140022518892288 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.09842050075531, loss=3.887730598449707
I0202 16:51:00.093114 140023005427456 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9743777513504028, loss=3.795647144317627
I0202 16:51:46.590958 140022518892288 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.8737830519676208, loss=4.033333778381348
I0202 16:52:32.668645 140023005427456 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6464543342590332, loss=5.665886878967285
I0202 16:53:18.948308 140022518892288 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.019958734512329, loss=3.9320361614227295
I0202 16:54:04.909078 140023005427456 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7723045945167542, loss=5.7427167892456055
I0202 16:54:50.960784 140022518892288 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1817435026168823, loss=3.8753116130828857
I0202 16:55:37.039635 140023005427456 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0314116477966309, loss=3.889352798461914
I0202 16:56:19.455997 140184451094336 spec.py:321] Evaluating on the training split.
I0202 16:56:29.914824 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 16:57:01.814943 140184451094336 spec.py:349] Evaluating on the test split.
I0202 16:57:03.420150 140184451094336 submission_runner.py:408] Time since start: 18591.13s, 	Step: 36693, 	{'train/accuracy': 0.3790820240974426, 'train/loss': 2.873992919921875, 'validation/accuracy': 0.3569999933242798, 'validation/loss': 3.0040409564971924, 'validation/num_examples': 50000, 'test/accuracy': 0.26920002698898315, 'test/loss': 3.625471591949463, 'test/num_examples': 10000, 'score': 16843.694502830505, 'total_duration': 18591.12539958954, 'accumulated_submission_time': 16843.694502830505, 'accumulated_eval_time': 1744.3594100475311, 'accumulated_logging_time': 1.1727678775787354}
I0202 16:57:03.444183 140022518892288 logging_writer.py:48] [36693] accumulated_eval_time=1744.359410, accumulated_logging_time=1.172768, accumulated_submission_time=16843.694503, global_step=36693, preemption_count=0, score=16843.694503, test/accuracy=0.269200, test/loss=3.625472, test/num_examples=10000, total_duration=18591.125400, train/accuracy=0.379082, train/loss=2.873993, validation/accuracy=0.357000, validation/loss=3.004041, validation/num_examples=50000
I0202 16:57:06.652465 140023005427456 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.985893189907074, loss=3.6429603099823
I0202 16:57:48.728385 140022518892288 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.9744069576263428, loss=3.761557102203369
I0202 16:58:34.780493 140023005427456 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.9635996222496033, loss=4.131099224090576
I0202 16:59:21.028186 140022518892288 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.9242192506790161, loss=3.709920644760132
I0202 17:00:07.192720 140023005427456 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9698836803436279, loss=3.9943277835845947
I0202 17:00:53.398705 140022518892288 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.9251245260238647, loss=3.9402925968170166
I0202 17:01:39.773919 140023005427456 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.9631721377372742, loss=3.884213924407959
I0202 17:02:26.006112 140022518892288 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.1766399145126343, loss=3.923190116882324
I0202 17:03:12.267681 140023005427456 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9533265829086304, loss=4.236629009246826
I0202 17:03:58.487034 140022518892288 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7261055111885071, loss=6.063155651092529
I0202 17:04:03.682427 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:04:14.120404 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:04:49.588263 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:04:51.194832 140184451094336 submission_runner.py:408] Time since start: 19058.90s, 	Step: 37613, 	{'train/accuracy': 0.3933398425579071, 'train/loss': 2.7992677688598633, 'validation/accuracy': 0.35933998227119446, 'validation/loss': 2.9893405437469482, 'validation/num_examples': 50000, 'test/accuracy': 0.2735000252723694, 'test/loss': 3.5844669342041016, 'test/num_examples': 10000, 'score': 17263.87330675125, 'total_duration': 19058.900079011917, 'accumulated_submission_time': 17263.87330675125, 'accumulated_eval_time': 1791.871794462204, 'accumulated_logging_time': 1.2091057300567627}
I0202 17:04:51.214627 140023005427456 logging_writer.py:48] [37613] accumulated_eval_time=1791.871794, accumulated_logging_time=1.209106, accumulated_submission_time=17263.873307, global_step=37613, preemption_count=0, score=17263.873307, test/accuracy=0.273500, test/loss=3.584467, test/num_examples=10000, total_duration=19058.900079, train/accuracy=0.393340, train/loss=2.799268, validation/accuracy=0.359340, validation/loss=2.989341, validation/num_examples=50000
I0202 17:05:27.544310 140022518892288 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.9190772771835327, loss=4.246237754821777
I0202 17:06:13.823027 140023005427456 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1545993089675903, loss=3.8106796741485596
I0202 17:07:00.191530 140022518892288 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.9666649103164673, loss=3.758466958999634
I0202 17:07:46.416095 140023005427456 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1116811037063599, loss=4.251518726348877
I0202 17:08:32.632228 140022518892288 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0384052991867065, loss=3.8380024433135986
I0202 17:09:18.924760 140023005427456 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.8879079818725586, loss=4.3928399085998535
I0202 17:10:05.367464 140022518892288 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.071107268333435, loss=3.7889134883880615
I0202 17:10:51.661998 140023005427456 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7799798250198364, loss=4.361749649047852
I0202 17:11:38.257916 140022518892288 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7610510587692261, loss=4.96811056137085
I0202 17:11:51.363117 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:12:01.916970 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:12:32.383331 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:12:33.989533 140184451094336 submission_runner.py:408] Time since start: 19521.69s, 	Step: 38530, 	{'train/accuracy': 0.3886132836341858, 'train/loss': 2.7967445850372314, 'validation/accuracy': 0.3657599985599518, 'validation/loss': 2.9347126483917236, 'validation/num_examples': 50000, 'test/accuracy': 0.2800000011920929, 'test/loss': 3.5463294982910156, 'test/num_examples': 10000, 'score': 17683.964739084244, 'total_duration': 19521.694784641266, 'accumulated_submission_time': 17683.964739084244, 'accumulated_eval_time': 1834.498204946518, 'accumulated_logging_time': 1.2384934425354004}
I0202 17:12:34.014215 140023005427456 logging_writer.py:48] [38530] accumulated_eval_time=1834.498205, accumulated_logging_time=1.238493, accumulated_submission_time=17683.964739, global_step=38530, preemption_count=0, score=17683.964739, test/accuracy=0.280000, test/loss=3.546329, test/num_examples=10000, total_duration=19521.694785, train/accuracy=0.388613, train/loss=2.796745, validation/accuracy=0.365760, validation/loss=2.934713, validation/num_examples=50000
I0202 17:13:02.833066 140022518892288 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.2484945058822632, loss=3.9031269550323486
I0202 17:13:48.458646 140023005427456 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.0489325523376465, loss=4.701700687408447
I0202 17:14:34.795293 140022518892288 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.248867392539978, loss=3.848907947540283
I0202 17:15:20.998345 140023005427456 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.0352755784988403, loss=3.758063316345215
I0202 17:16:06.992300 140022518892288 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.9505124688148499, loss=4.794129848480225
I0202 17:16:53.190452 140023005427456 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.2448902130126953, loss=3.7612075805664062
I0202 17:17:39.385863 140022518892288 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.9705768823623657, loss=3.820197343826294
I0202 17:18:25.615209 140023005427456 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.9472111463546753, loss=3.704078197479248
I0202 17:19:11.814704 140022518892288 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.7844934463500977, loss=5.062534809112549
I0202 17:19:34.463598 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:19:45.361298 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:20:17.373436 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:20:18.978141 140184451094336 submission_runner.py:408] Time since start: 19986.68s, 	Step: 39451, 	{'train/accuracy': 0.38685545325279236, 'train/loss': 2.8592474460601807, 'validation/accuracy': 0.3622399866580963, 'validation/loss': 2.997967004776001, 'validation/num_examples': 50000, 'test/accuracy': 0.2826000154018402, 'test/loss': 3.571634531021118, 'test/num_examples': 10000, 'score': 18104.35695052147, 'total_duration': 19986.683392047882, 'accumulated_submission_time': 18104.35695052147, 'accumulated_eval_time': 1879.0127630233765, 'accumulated_logging_time': 1.272782802581787}
I0202 17:20:19.002039 140023005427456 logging_writer.py:48] [39451] accumulated_eval_time=1879.012763, accumulated_logging_time=1.272783, accumulated_submission_time=18104.356951, global_step=39451, preemption_count=0, score=18104.356951, test/accuracy=0.282600, test/loss=3.571635, test/num_examples=10000, total_duration=19986.683392, train/accuracy=0.386855, train/loss=2.859247, validation/accuracy=0.362240, validation/loss=2.997967, validation/num_examples=50000
I0202 17:20:39.017219 140022518892288 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8192077875137329, loss=5.7545905113220215
I0202 17:21:23.668495 140023005427456 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.9853301048278809, loss=3.99066424369812
I0202 17:22:10.294624 140022518892288 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.9524293541908264, loss=3.9566216468811035
I0202 17:22:56.596922 140023005427456 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.9586251378059387, loss=3.753777503967285
I0202 17:23:42.836193 140022518892288 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.8898779153823853, loss=5.765567779541016
I0202 17:24:28.929038 140023005427456 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.2028820514678955, loss=4.412466049194336
I0202 17:25:15.354274 140022518892288 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0024585723876953, loss=3.836806058883667
I0202 17:26:01.452647 140023005427456 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.8921336531639099, loss=4.206554889678955
I0202 17:26:47.983978 140022518892288 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1140910387039185, loss=3.785599708557129
I0202 17:27:19.232077 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:27:30.578064 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:28:05.622040 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:28:07.221518 140184451094336 submission_runner.py:408] Time since start: 20454.93s, 	Step: 40369, 	{'train/accuracy': 0.4025976359844208, 'train/loss': 2.7299349308013916, 'validation/accuracy': 0.37046000361442566, 'validation/loss': 2.898860216140747, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.507483720779419, 'test/num_examples': 10000, 'score': 18524.528084754944, 'total_duration': 20454.92676472664, 'accumulated_submission_time': 18524.528084754944, 'accumulated_eval_time': 1927.0022106170654, 'accumulated_logging_time': 1.3080272674560547}
I0202 17:28:07.244950 140023005427456 logging_writer.py:48] [40369] accumulated_eval_time=1927.002211, accumulated_logging_time=1.308027, accumulated_submission_time=18524.528085, global_step=40369, preemption_count=0, score=18524.528085, test/accuracy=0.286500, test/loss=3.507484, test/num_examples=10000, total_duration=20454.926765, train/accuracy=0.402598, train/loss=2.729935, validation/accuracy=0.370460, validation/loss=2.898860, validation/num_examples=50000
I0202 17:28:20.042864 140022518892288 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0916965007781982, loss=3.944610118865967
I0202 17:29:03.580405 140023005427456 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7492920756340027, loss=5.724756240844727
I0202 17:29:49.974781 140022518892288 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8820850849151611, loss=3.7829625606536865
I0202 17:30:36.440262 140023005427456 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0023858547210693, loss=3.9323978424072266
I0202 17:31:22.472032 140022518892288 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.0209397077560425, loss=4.019639492034912
I0202 17:32:09.144309 140023005427456 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8961637020111084, loss=5.08834981918335
I0202 17:32:55.165020 140022518892288 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.9852258563041687, loss=3.6496474742889404
I0202 17:33:41.229015 140023005427456 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.966093122959137, loss=3.618637800216675
I0202 17:34:27.540118 140022518892288 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7217013835906982, loss=4.719075679779053
I0202 17:35:07.419893 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:35:17.723722 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:35:48.035359 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:35:49.642770 140184451094336 submission_runner.py:408] Time since start: 20917.35s, 	Step: 41288, 	{'train/accuracy': 0.41371092200279236, 'train/loss': 2.72369384765625, 'validation/accuracy': 0.36061999201774597, 'validation/loss': 2.9944801330566406, 'validation/num_examples': 50000, 'test/accuracy': 0.28120002150535583, 'test/loss': 3.5632994174957275, 'test/num_examples': 10000, 'score': 18944.646690130234, 'total_duration': 20917.348011016846, 'accumulated_submission_time': 18944.646690130234, 'accumulated_eval_time': 1969.225081205368, 'accumulated_logging_time': 1.3406035900115967}
I0202 17:35:49.667230 140023005427456 logging_writer.py:48] [41288] accumulated_eval_time=1969.225081, accumulated_logging_time=1.340604, accumulated_submission_time=18944.646690, global_step=41288, preemption_count=0, score=18944.646690, test/accuracy=0.281200, test/loss=3.563299, test/num_examples=10000, total_duration=20917.348011, train/accuracy=0.413711, train/loss=2.723694, validation/accuracy=0.360620, validation/loss=2.994480, validation/num_examples=50000
I0202 17:35:54.877894 140022518892288 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.151593565940857, loss=3.774343967437744
I0202 17:36:37.604949 140023005427456 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.1580288410186768, loss=3.8994626998901367
I0202 17:37:24.054130 140022518892288 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.058567762374878, loss=3.7257001399993896
I0202 17:38:10.429280 140023005427456 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.0356591939926147, loss=3.8176002502441406
I0202 17:38:56.661806 140022518892288 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8066666126251221, loss=5.673545837402344
I0202 17:39:42.800078 140023005427456 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.9435803294181824, loss=3.748643398284912
I0202 17:40:29.036576 140022518892288 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.8890597820281982, loss=5.354222774505615
I0202 17:41:15.257880 140023005427456 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.931366503238678, loss=4.8524651527404785
I0202 17:42:01.664424 140022518892288 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0841138362884521, loss=4.2022528648376465
I0202 17:42:47.905889 140023005427456 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.9792197942733765, loss=4.0016188621521
I0202 17:42:49.967553 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:43:00.407790 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:43:26.907495 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:43:28.512122 140184451094336 submission_runner.py:408] Time since start: 21376.22s, 	Step: 42206, 	{'train/accuracy': 0.38896483182907104, 'train/loss': 2.8523201942443848, 'validation/accuracy': 0.367279976606369, 'validation/loss': 2.9817123413085938, 'validation/num_examples': 50000, 'test/accuracy': 0.28290000557899475, 'test/loss': 3.547908067703247, 'test/num_examples': 10000, 'score': 19364.88948059082, 'total_duration': 21376.21737074852, 'accumulated_submission_time': 19364.88948059082, 'accumulated_eval_time': 2007.7696409225464, 'accumulated_logging_time': 1.3750572204589844}
I0202 17:43:28.536134 140022518892288 logging_writer.py:48] [42206] accumulated_eval_time=2007.769641, accumulated_logging_time=1.375057, accumulated_submission_time=19364.889481, global_step=42206, preemption_count=0, score=19364.889481, test/accuracy=0.282900, test/loss=3.547908, test/num_examples=10000, total_duration=21376.217371, train/accuracy=0.388965, train/loss=2.852320, validation/accuracy=0.367280, validation/loss=2.981712, validation/num_examples=50000
I0202 17:44:08.351509 140023005427456 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.0393801927566528, loss=4.085100173950195
I0202 17:44:54.133626 140022518892288 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.0116808414459229, loss=4.139707088470459
I0202 17:45:40.523376 140023005427456 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.8975458145141602, loss=4.559025764465332
I0202 17:46:26.775518 140022518892288 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0182689428329468, loss=3.6719493865966797
I0202 17:47:13.231951 140023005427456 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2917879819869995, loss=4.0994110107421875
I0202 17:47:59.528763 140022518892288 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0364747047424316, loss=3.900080680847168
I0202 17:48:45.567793 140023005427456 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.1042765378952026, loss=3.7432546615600586
I0202 17:49:31.854952 140022518892288 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.1771702766418457, loss=3.8772854804992676
I0202 17:50:18.231277 140023005427456 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9848612546920776, loss=3.7781691551208496
I0202 17:50:28.873519 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:50:39.613153 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:51:13.175793 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:51:14.776870 140184451094336 submission_runner.py:408] Time since start: 21842.48s, 	Step: 43125, 	{'train/accuracy': 0.40107420086860657, 'train/loss': 2.7512407302856445, 'validation/accuracy': 0.3735399842262268, 'validation/loss': 2.8998680114746094, 'validation/num_examples': 50000, 'test/accuracy': 0.28860002756118774, 'test/loss': 3.5031073093414307, 'test/num_examples': 10000, 'score': 19785.169471025467, 'total_duration': 21842.482117176056, 'accumulated_submission_time': 19785.169471025467, 'accumulated_eval_time': 2053.6729724407196, 'accumulated_logging_time': 1.4091379642486572}
I0202 17:51:14.798291 140022518892288 logging_writer.py:48] [43125] accumulated_eval_time=2053.672972, accumulated_logging_time=1.409138, accumulated_submission_time=19785.169471, global_step=43125, preemption_count=0, score=19785.169471, test/accuracy=0.288600, test/loss=3.503107, test/num_examples=10000, total_duration=21842.482117, train/accuracy=0.401074, train/loss=2.751241, validation/accuracy=0.373540, validation/loss=2.899868, validation/num_examples=50000
I0202 17:51:45.381392 140023005427456 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.9976728558540344, loss=3.7324440479278564
I0202 17:52:31.501193 140022518892288 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.0599561929702759, loss=3.7994885444641113
I0202 17:53:17.641775 140023005427456 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.857448160648346, loss=4.486917972564697
I0202 17:54:03.710835 140022518892288 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9565126299858093, loss=3.666778326034546
I0202 17:54:49.994756 140023005427456 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7850527167320251, loss=5.7869672775268555
I0202 17:55:36.192739 140022518892288 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.0301222801208496, loss=3.913978338241577
I0202 17:56:22.654098 140023005427456 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.0479259490966797, loss=3.8823773860931396
I0202 17:57:08.824656 140022518892288 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.9145678281784058, loss=5.944749355316162
I0202 17:57:54.910122 140023005427456 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1170586347579956, loss=3.794830799102783
I0202 17:58:14.919875 140184451094336 spec.py:321] Evaluating on the training split.
I0202 17:58:25.267447 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 17:58:58.682735 140184451094336 spec.py:349] Evaluating on the test split.
I0202 17:59:00.281915 140184451094336 submission_runner.py:408] Time since start: 22307.99s, 	Step: 44045, 	{'train/accuracy': 0.42900389432907104, 'train/loss': 2.601717233657837, 'validation/accuracy': 0.3832799792289734, 'validation/loss': 2.852113723754883, 'validation/num_examples': 50000, 'test/accuracy': 0.29280000925064087, 'test/loss': 3.4496848583221436, 'test/num_examples': 10000, 'score': 20205.232219696045, 'total_duration': 22307.98716187477, 'accumulated_submission_time': 20205.232219696045, 'accumulated_eval_time': 2099.034994125366, 'accumulated_logging_time': 1.4414191246032715}
I0202 17:59:00.308612 140022518892288 logging_writer.py:48] [44045] accumulated_eval_time=2099.034994, accumulated_logging_time=1.441419, accumulated_submission_time=20205.232220, global_step=44045, preemption_count=0, score=20205.232220, test/accuracy=0.292800, test/loss=3.449685, test/num_examples=10000, total_duration=22307.987162, train/accuracy=0.429004, train/loss=2.601717, validation/accuracy=0.383280, validation/loss=2.852114, validation/num_examples=50000
I0202 17:59:22.734303 140023005427456 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.3569176197052002, loss=3.806448221206665
I0202 18:00:07.782532 140022518892288 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.0695548057556152, loss=3.6944754123687744
I0202 18:00:54.414050 140023005427456 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7762617468833923, loss=4.940067768096924
I0202 18:01:40.970462 140022518892288 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.8367156982421875, loss=4.784516334533691
I0202 18:02:27.179114 140023005427456 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.0601954460144043, loss=3.676111936569214
I0202 18:03:13.490821 140022518892288 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1208131313323975, loss=3.8202037811279297
I0202 18:03:59.633058 140023005427456 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.8120890259742737, loss=4.824517726898193
I0202 18:04:46.319706 140022518892288 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.9594854116439819, loss=3.562062978744507
I0202 18:05:32.642484 140023005427456 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.9967093467712402, loss=3.9259533882141113
I0202 18:06:00.449498 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:06:10.808103 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:06:44.045202 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:06:45.643112 140184451094336 submission_runner.py:408] Time since start: 22773.35s, 	Step: 44962, 	{'train/accuracy': 0.4001562297344208, 'train/loss': 2.8011372089385986, 'validation/accuracy': 0.37342000007629395, 'validation/loss': 2.9300620555877686, 'validation/num_examples': 50000, 'test/accuracy': 0.28630000352859497, 'test/loss': 3.5115420818328857, 'test/num_examples': 10000, 'score': 20625.31547307968, 'total_duration': 22773.348356485367, 'accumulated_submission_time': 20625.31547307968, 'accumulated_eval_time': 2144.228601694107, 'accumulated_logging_time': 1.4792120456695557}
I0202 18:06:45.668265 140022518892288 logging_writer.py:48] [44962] accumulated_eval_time=2144.228602, accumulated_logging_time=1.479212, accumulated_submission_time=20625.315473, global_step=44962, preemption_count=0, score=20625.315473, test/accuracy=0.286300, test/loss=3.511542, test/num_examples=10000, total_duration=22773.348356, train/accuracy=0.400156, train/loss=2.801137, validation/accuracy=0.373420, validation/loss=2.930062, validation/num_examples=50000
I0202 18:07:01.561735 140023005427456 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8580586314201355, loss=5.899348258972168
I0202 18:07:45.748176 140022518892288 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.9903877377510071, loss=3.6667582988739014
I0202 18:08:31.833097 140023005427456 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.684179961681366, loss=5.905039310455322
I0202 18:09:18.012883 140022518892288 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.0495930910110474, loss=3.989321708679199
I0202 18:10:04.212239 140023005427456 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.2782849073410034, loss=3.8712100982666016
I0202 18:10:50.478145 140022518892288 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9749847054481506, loss=4.573917388916016
I0202 18:11:36.755171 140023005427456 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7834469676017761, loss=4.405574798583984
I0202 18:12:23.252238 140022518892288 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0198560953140259, loss=3.9527950286865234
I0202 18:13:09.234637 140023005427456 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1251559257507324, loss=3.8557956218719482
I0202 18:13:46.130852 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:13:56.461778 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:14:31.600363 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:14:33.202389 140184451094336 submission_runner.py:408] Time since start: 23240.91s, 	Step: 45882, 	{'train/accuracy': 0.28843748569488525, 'train/loss': 3.4999566078186035, 'validation/accuracy': 0.26857998967170715, 'validation/loss': 3.6102328300476074, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.131436347961426, 'test/num_examples': 10000, 'score': 21045.717796325684, 'total_duration': 23240.907628774643, 'accumulated_submission_time': 21045.717796325684, 'accumulated_eval_time': 2191.300128698349, 'accumulated_logging_time': 1.5172581672668457}
I0202 18:14:33.232303 140022518892288 logging_writer.py:48] [45882] accumulated_eval_time=2191.300129, accumulated_logging_time=1.517258, accumulated_submission_time=21045.717796, global_step=45882, preemption_count=0, score=21045.717796, test/accuracy=0.210700, test/loss=4.131436, test/num_examples=10000, total_duration=23240.907629, train/accuracy=0.288437, train/loss=3.499957, validation/accuracy=0.268580, validation/loss=3.610233, validation/num_examples=50000
I0202 18:14:40.840244 140023005427456 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.0122895240783691, loss=4.250680923461914
I0202 18:15:23.916036 140022518892288 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.0175384283065796, loss=3.937671184539795
I0202 18:16:10.134356 140023005427456 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.9021626710891724, loss=3.753481864929199
I0202 18:16:56.301548 140022518892288 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8894350528717041, loss=4.149630546569824
I0202 18:17:42.437197 140023005427456 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.9791886806488037, loss=3.90159273147583
I0202 18:18:28.725850 140022518892288 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.820713222026825, loss=5.853151798248291
I0202 18:19:14.668283 140023005427456 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8691825270652771, loss=3.7336230278015137
I0202 18:20:00.667801 140022518892288 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2825663089752197, loss=3.810807704925537
I0202 18:20:46.968162 140023005427456 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.0654815435409546, loss=3.7632131576538086
I0202 18:21:33.384168 140022518892288 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8329670429229736, loss=5.487558841705322
I0202 18:21:33.398767 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:21:44.008445 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:22:13.084891 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:22:14.678678 140184451094336 submission_runner.py:408] Time since start: 23702.38s, 	Step: 46801, 	{'train/accuracy': 0.3951171934604645, 'train/loss': 2.819166421890259, 'validation/accuracy': 0.36381998658180237, 'validation/loss': 3.004495143890381, 'validation/num_examples': 50000, 'test/accuracy': 0.2831000089645386, 'test/loss': 3.567754030227661, 'test/num_examples': 10000, 'score': 21465.826851129532, 'total_duration': 23702.38392972946, 'accumulated_submission_time': 21465.826851129532, 'accumulated_eval_time': 2232.58002948761, 'accumulated_logging_time': 1.55698823928833}
I0202 18:22:14.703921 140023005427456 logging_writer.py:48] [46801] accumulated_eval_time=2232.580029, accumulated_logging_time=1.556988, accumulated_submission_time=21465.826851, global_step=46801, preemption_count=0, score=21465.826851, test/accuracy=0.283100, test/loss=3.567754, test/num_examples=10000, total_duration=23702.383930, train/accuracy=0.395117, train/loss=2.819166, validation/accuracy=0.363820, validation/loss=3.004495, validation/num_examples=50000
I0202 18:22:56.393086 140022518892288 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8914118409156799, loss=4.469559669494629
I0202 18:23:42.416535 140023005427456 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8043015003204346, loss=5.446259498596191
I0202 18:24:28.674107 140022518892288 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.9163471460342407, loss=3.858766555786133
I0202 18:25:14.532012 140023005427456 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.9522250294685364, loss=5.788923263549805
I0202 18:26:00.512512 140022518892288 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7799118161201477, loss=5.3051862716674805
I0202 18:26:46.522837 140023005427456 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.0128517150878906, loss=3.88358998298645
I0202 18:27:32.536347 140022518892288 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.597237765789032, loss=5.9054059982299805
I0202 18:28:18.906207 140023005427456 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1254079341888428, loss=4.109892845153809
I0202 18:29:05.196140 140022518892288 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9564125537872314, loss=5.847298622131348
I0202 18:29:15.066904 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:29:25.382784 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:29:56.274678 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:29:57.884835 140184451094336 submission_runner.py:408] Time since start: 24165.59s, 	Step: 47723, 	{'train/accuracy': 0.4143163859844208, 'train/loss': 2.6614110469818115, 'validation/accuracy': 0.3860200047492981, 'validation/loss': 2.80824875831604, 'validation/num_examples': 50000, 'test/accuracy': 0.2997000217437744, 'test/loss': 3.443194627761841, 'test/num_examples': 10000, 'score': 21886.131243228912, 'total_duration': 24165.59008526802, 'accumulated_submission_time': 21886.131243228912, 'accumulated_eval_time': 2275.397953271866, 'accumulated_logging_time': 1.593160629272461}
I0202 18:29:57.910527 140023005427456 logging_writer.py:48] [47723] accumulated_eval_time=2275.397953, accumulated_logging_time=1.593161, accumulated_submission_time=21886.131243, global_step=47723, preemption_count=0, score=21886.131243, test/accuracy=0.299700, test/loss=3.443195, test/num_examples=10000, total_duration=24165.590085, train/accuracy=0.414316, train/loss=2.661411, validation/accuracy=0.386020, validation/loss=2.808249, validation/num_examples=50000
I0202 18:30:29.796547 140022518892288 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.016146183013916, loss=3.8725123405456543
I0202 18:31:15.905895 140023005427456 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.8683292269706726, loss=3.757568836212158
I0202 18:32:02.593723 140022518892288 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6657230257987976, loss=5.787858963012695
I0202 18:32:48.951369 140023005427456 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8720217347145081, loss=4.401159286499023
I0202 18:33:34.833083 140022518892288 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0199341773986816, loss=4.040841102600098
I0202 18:34:21.119883 140023005427456 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.8468163013458252, loss=5.310166835784912
I0202 18:35:07.202795 140022518892288 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1622298955917358, loss=3.8649420738220215
I0202 18:35:53.320695 140023005427456 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0946820974349976, loss=3.7393367290496826
I0202 18:36:39.477600 140022518892288 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0287141799926758, loss=3.6567723751068115
I0202 18:36:58.065207 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:37:08.382610 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:37:43.205564 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:37:44.805592 140184451094336 submission_runner.py:408] Time since start: 24632.51s, 	Step: 48642, 	{'train/accuracy': 0.40107420086860657, 'train/loss': 2.7443270683288574, 'validation/accuracy': 0.3771199882030487, 'validation/loss': 2.8815295696258545, 'validation/num_examples': 50000, 'test/accuracy': 0.29040002822875977, 'test/loss': 3.4814419746398926, 'test/num_examples': 10000, 'score': 22306.227305173874, 'total_duration': 24632.510838747025, 'accumulated_submission_time': 22306.227305173874, 'accumulated_eval_time': 2322.1383290290833, 'accumulated_logging_time': 1.6305792331695557}
I0202 18:37:44.827911 140023005427456 logging_writer.py:48] [48642] accumulated_eval_time=2322.138329, accumulated_logging_time=1.630579, accumulated_submission_time=22306.227305, global_step=48642, preemption_count=0, score=22306.227305, test/accuracy=0.290400, test/loss=3.481442, test/num_examples=10000, total_duration=24632.510839, train/accuracy=0.401074, train/loss=2.744327, validation/accuracy=0.377120, validation/loss=2.881530, validation/num_examples=50000
I0202 18:38:08.440477 140022518892288 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.0358809232711792, loss=3.643040418624878
I0202 18:38:53.514661 140023005427456 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.8342534899711609, loss=5.5601911544799805
I0202 18:39:39.800139 140022518892288 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.9591721296310425, loss=3.8013479709625244
I0202 18:40:25.998037 140023005427456 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9695488810539246, loss=3.668365240097046
I0202 18:41:12.336527 140022518892288 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.9789555668830872, loss=3.736112117767334
I0202 18:41:58.728129 140023005427456 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9491680860519409, loss=4.152754306793213
I0202 18:42:44.987825 140022518892288 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.8457032442092896, loss=5.511391639709473
I0202 18:43:31.076727 140023005427456 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.0209293365478516, loss=3.7010104656219482
I0202 18:44:18.686295 140022518892288 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0527844429016113, loss=3.758448600769043
I0202 18:44:44.916215 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:44:55.343625 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:45:28.184385 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:45:29.776889 140184451094336 submission_runner.py:408] Time since start: 25097.48s, 	Step: 49559, 	{'train/accuracy': 0.41294920444488525, 'train/loss': 2.7169084548950195, 'validation/accuracy': 0.3819800019264221, 'validation/loss': 2.8852477073669434, 'validation/num_examples': 50000, 'test/accuracy': 0.28700000047683716, 'test/loss': 3.4870080947875977, 'test/num_examples': 10000, 'score': 22726.256851911545, 'total_duration': 25097.48213648796, 'accumulated_submission_time': 22726.256851911545, 'accumulated_eval_time': 2366.9990010261536, 'accumulated_logging_time': 1.662933588027954}
I0202 18:45:29.799938 140023005427456 logging_writer.py:48] [49559] accumulated_eval_time=2366.999001, accumulated_logging_time=1.662934, accumulated_submission_time=22726.256852, global_step=49559, preemption_count=0, score=22726.256852, test/accuracy=0.287000, test/loss=3.487008, test/num_examples=10000, total_duration=25097.482136, train/accuracy=0.412949, train/loss=2.716908, validation/accuracy=0.381980, validation/loss=2.885248, validation/num_examples=50000
I0202 18:45:46.592547 140022518892288 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9585410356521606, loss=3.8923163414001465
I0202 18:46:30.796824 140023005427456 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.064454197883606, loss=3.7752227783203125
I0202 18:47:17.143177 140022518892288 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.9407045841217041, loss=3.556016206741333
I0202 18:48:03.323789 140023005427456 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.0496286153793335, loss=3.5720953941345215
I0202 18:48:49.384356 140022518892288 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9168300032615662, loss=3.6042251586914062
I0202 18:49:36.063329 140023005427456 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8261873126029968, loss=5.223290920257568
I0202 18:50:22.248501 140022518892288 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0282726287841797, loss=3.598797082901001
I0202 18:51:08.456619 140023005427456 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.105220913887024, loss=3.6678452491760254
I0202 18:51:54.860095 140022518892288 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.0003645420074463, loss=3.732632637023926
I0202 18:52:30.099992 140184451094336 spec.py:321] Evaluating on the training split.
I0202 18:52:40.542874 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 18:53:14.232700 140184451094336 spec.py:349] Evaluating on the test split.
I0202 18:53:15.833248 140184451094336 submission_runner.py:408] Time since start: 25563.54s, 	Step: 50478, 	{'train/accuracy': 0.4268554449081421, 'train/loss': 2.6102540493011475, 'validation/accuracy': 0.39751997590065, 'validation/loss': 2.7804811000823975, 'validation/num_examples': 50000, 'test/accuracy': 0.3005000054836273, 'test/loss': 3.3917243480682373, 'test/num_examples': 10000, 'score': 23146.497085094452, 'total_duration': 25563.538495779037, 'accumulated_submission_time': 23146.497085094452, 'accumulated_eval_time': 2412.7322528362274, 'accumulated_logging_time': 1.6987645626068115}
I0202 18:53:15.856332 140023005427456 logging_writer.py:48] [50478] accumulated_eval_time=2412.732253, accumulated_logging_time=1.698765, accumulated_submission_time=23146.497085, global_step=50478, preemption_count=0, score=23146.497085, test/accuracy=0.300500, test/loss=3.391724, test/num_examples=10000, total_duration=25563.538496, train/accuracy=0.426855, train/loss=2.610254, validation/accuracy=0.397520, validation/loss=2.780481, validation/num_examples=50000
I0202 18:53:25.062909 140022518892288 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.868502676486969, loss=3.8386292457580566
I0202 18:54:08.127054 140023005427456 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.9945337772369385, loss=3.8833627700805664
I0202 18:54:54.492060 140022518892288 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.9922624826431274, loss=3.819913387298584
I0202 18:55:40.937348 140023005427456 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.0151056051254272, loss=3.692448616027832
I0202 18:56:26.970649 140022518892288 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.109195351600647, loss=3.713231325149536
I0202 18:57:13.376742 140023005427456 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.9364031553268433, loss=5.914506912231445
I0202 18:57:59.266442 140022518892288 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.1718573570251465, loss=3.720093250274658
I0202 18:58:45.389165 140023005427456 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8598804473876953, loss=4.060708522796631
I0202 18:59:31.876635 140022518892288 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.0811365842819214, loss=3.665585994720459
I0202 19:00:15.919584 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:00:26.527774 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:01:02.755865 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:01:04.359596 140184451094336 submission_runner.py:408] Time since start: 26032.06s, 	Step: 51397, 	{'train/accuracy': 0.42949217557907104, 'train/loss': 2.56564998626709, 'validation/accuracy': 0.4009999930858612, 'validation/loss': 2.729902505874634, 'validation/num_examples': 50000, 'test/accuracy': 0.3053000271320343, 'test/loss': 3.3814618587493896, 'test/num_examples': 10000, 'score': 23566.498861551285, 'total_duration': 26032.064838647842, 'accumulated_submission_time': 23566.498861551285, 'accumulated_eval_time': 2461.1722581386566, 'accumulated_logging_time': 1.734628677368164}
I0202 19:01:04.389917 140023005427456 logging_writer.py:48] [51397] accumulated_eval_time=2461.172258, accumulated_logging_time=1.734629, accumulated_submission_time=23566.498862, global_step=51397, preemption_count=0, score=23566.498862, test/accuracy=0.305300, test/loss=3.381462, test/num_examples=10000, total_duration=26032.064839, train/accuracy=0.429492, train/loss=2.565650, validation/accuracy=0.401000, validation/loss=2.729903, validation/num_examples=50000
I0202 19:01:05.991393 140022518892288 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.3914592266082764, loss=3.7359139919281006
I0202 19:01:47.881458 140023005427456 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.9355928897857666, loss=3.6923718452453613
I0202 19:02:34.074858 140022518892288 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8623011112213135, loss=4.422391414642334
I0202 19:03:20.719967 140023005427456 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.4075857400894165, loss=3.8272528648376465
I0202 19:04:07.017797 140022518892288 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.1107696294784546, loss=3.7506959438323975
I0202 19:04:53.188267 140023005427456 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.0976390838623047, loss=3.589815139770508
I0202 19:05:39.534514 140022518892288 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.3492202758789062, loss=3.7340288162231445
I0202 19:06:25.697942 140023005427456 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8148178458213806, loss=5.472726821899414
I0202 19:07:12.149728 140022518892288 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.2731454372406006, loss=3.701772451400757
I0202 19:07:58.504706 140023005427456 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9073122143745422, loss=4.287662506103516
I0202 19:08:04.736837 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:08:15.112201 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:08:47.905464 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:08:49.513314 140184451094336 submission_runner.py:408] Time since start: 26497.22s, 	Step: 52315, 	{'train/accuracy': 0.4299023449420929, 'train/loss': 2.5807902812957764, 'validation/accuracy': 0.39805999398231506, 'validation/loss': 2.752584934234619, 'validation/num_examples': 50000, 'test/accuracy': 0.31070002913475037, 'test/loss': 3.378159523010254, 'test/num_examples': 10000, 'score': 23986.7883477211, 'total_duration': 26497.21853017807, 'accumulated_submission_time': 23986.7883477211, 'accumulated_eval_time': 2505.948692560196, 'accumulated_logging_time': 1.7748017311096191}
I0202 19:08:49.539751 140022518892288 logging_writer.py:48] [52315] accumulated_eval_time=2505.948693, accumulated_logging_time=1.774802, accumulated_submission_time=23986.788348, global_step=52315, preemption_count=0, score=23986.788348, test/accuracy=0.310700, test/loss=3.378160, test/num_examples=10000, total_duration=26497.218530, train/accuracy=0.429902, train/loss=2.580790, validation/accuracy=0.398060, validation/loss=2.752585, validation/num_examples=50000
I0202 19:09:24.938788 140023005427456 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.133683681488037, loss=3.6793203353881836
I0202 19:10:10.942528 140022518892288 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.9581642150878906, loss=3.4889583587646484
I0202 19:10:57.459670 140023005427456 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.0098557472229004, loss=3.637010097503662
I0202 19:11:43.834096 140022518892288 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.047549843788147, loss=3.7050161361694336
I0202 19:12:30.179087 140023005427456 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.1594704389572144, loss=4.386579990386963
I0202 19:13:16.293903 140022518892288 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.0045537948608398, loss=3.5684874057769775
I0202 19:14:02.693965 140023005427456 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0805989503860474, loss=3.620335578918457
I0202 19:14:48.857472 140022518892288 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9352136254310608, loss=3.552391767501831
I0202 19:15:34.935504 140023005427456 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.0399267673492432, loss=3.657255172729492
I0202 19:15:49.782918 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:16:00.472516 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:16:35.050685 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:16:36.652454 140184451094336 submission_runner.py:408] Time since start: 26964.36s, 	Step: 53234, 	{'train/accuracy': 0.47236326336860657, 'train/loss': 2.374985694885254, 'validation/accuracy': 0.4041999876499176, 'validation/loss': 2.7307236194610596, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.353266954421997, 'test/num_examples': 10000, 'score': 24406.975200414658, 'total_duration': 26964.357704639435, 'accumulated_submission_time': 24406.975200414658, 'accumulated_eval_time': 2552.8182249069214, 'accumulated_logging_time': 1.8105263710021973}
I0202 19:16:36.679168 140022518892288 logging_writer.py:48] [53234] accumulated_eval_time=2552.818225, accumulated_logging_time=1.810526, accumulated_submission_time=24406.975200, global_step=53234, preemption_count=0, score=24406.975200, test/accuracy=0.311500, test/loss=3.353267, test/num_examples=10000, total_duration=26964.357705, train/accuracy=0.472363, train/loss=2.374986, validation/accuracy=0.404200, validation/loss=2.730724, validation/num_examples=50000
I0202 19:17:03.633460 140023005427456 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9222642779350281, loss=5.9449238777160645
I0202 19:17:49.398912 140022518892288 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.0297983884811401, loss=3.688419818878174
I0202 19:18:35.351565 140023005427456 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0673129558563232, loss=4.1754913330078125
I0202 19:19:21.711662 140022518892288 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.907370924949646, loss=5.243396759033203
I0202 19:20:07.635699 140023005427456 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.9925072193145752, loss=3.5590415000915527
I0202 19:20:54.080085 140022518892288 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0324739217758179, loss=3.59700083732605
I0202 19:21:40.481292 140023005427456 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.0479133129119873, loss=3.7491207122802734
I0202 19:22:27.142459 140022518892288 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9005966186523438, loss=5.491331100463867
I0202 19:23:13.030939 140023005427456 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.6644847393035889, loss=5.5369744300842285
I0202 19:23:36.877817 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:23:47.147475 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:24:25.710604 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:24:27.314944 140184451094336 submission_runner.py:408] Time since start: 27435.02s, 	Step: 54153, 	{'train/accuracy': 0.4276367127895355, 'train/loss': 2.59726619720459, 'validation/accuracy': 0.3992999792098999, 'validation/loss': 2.7636921405792236, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.398660659790039, 'test/num_examples': 10000, 'score': 24827.1175699234, 'total_duration': 27435.02015376091, 'accumulated_submission_time': 24827.1175699234, 'accumulated_eval_time': 2603.2553062438965, 'accumulated_logging_time': 1.8466713428497314}
I0202 19:24:27.342068 140022518892288 logging_writer.py:48] [54153] accumulated_eval_time=2603.255306, accumulated_logging_time=1.846671, accumulated_submission_time=24827.117570, global_step=54153, preemption_count=0, score=24827.117570, test/accuracy=0.302300, test/loss=3.398661, test/num_examples=10000, total_duration=27435.020154, train/accuracy=0.427637, train/loss=2.597266, validation/accuracy=0.399300, validation/loss=2.763692, validation/num_examples=50000
I0202 19:24:46.561164 140023005427456 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.9974653124809265, loss=3.7380118370056152
I0202 19:25:31.179716 140022518892288 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.8162255883216858, loss=5.858654975891113
I0202 19:26:17.318248 140023005427456 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.9094277024269104, loss=3.636019229888916
I0202 19:27:03.597072 140022518892288 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.0349169969558716, loss=3.6478207111358643
I0202 19:27:49.561859 140023005427456 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.137861728668213, loss=3.5757694244384766
I0202 19:28:35.617108 140022518892288 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.942873477935791, loss=4.600723743438721
I0202 19:29:21.676465 140023005427456 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.7080769538879395, loss=5.911813735961914
I0202 19:30:08.012577 140022518892288 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.1365145444869995, loss=3.7260189056396484
I0202 19:30:53.998076 140023005427456 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2863115072250366, loss=3.5716116428375244
I0202 19:31:27.606060 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:31:38.269912 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:32:09.560454 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:32:11.158044 140184451094336 submission_runner.py:408] Time since start: 27898.86s, 	Step: 55074, 	{'train/accuracy': 0.4376757740974426, 'train/loss': 2.500518798828125, 'validation/accuracy': 0.4105999767780304, 'validation/loss': 2.6731464862823486, 'validation/num_examples': 50000, 'test/accuracy': 0.31850001215934753, 'test/loss': 3.2860732078552246, 'test/num_examples': 10000, 'score': 25247.319982767105, 'total_duration': 27898.86329269409, 'accumulated_submission_time': 25247.319982767105, 'accumulated_eval_time': 2646.8072805404663, 'accumulated_logging_time': 1.887636423110962}
I0202 19:32:11.182359 140022518892288 logging_writer.py:48] [55074] accumulated_eval_time=2646.807281, accumulated_logging_time=1.887636, accumulated_submission_time=25247.319983, global_step=55074, preemption_count=0, score=25247.319983, test/accuracy=0.318500, test/loss=3.286073, test/num_examples=10000, total_duration=27898.863293, train/accuracy=0.437676, train/loss=2.500519, validation/accuracy=0.410600, validation/loss=2.673146, validation/num_examples=50000
I0202 19:32:21.988763 140023005427456 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8613170385360718, loss=4.489890098571777
I0202 19:33:05.508721 140022518892288 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8531044721603394, loss=4.159111022949219
I0202 19:33:51.513991 140023005427456 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1278918981552124, loss=5.924650192260742
I0202 19:34:37.796665 140022518892288 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.3134214878082275, loss=3.7618460655212402
I0202 19:35:23.884832 140023005427456 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1433805227279663, loss=4.19824743270874
I0202 19:36:10.021273 140022518892288 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0919580459594727, loss=3.6755268573760986
I0202 19:36:56.092932 140023005427456 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.9158320426940918, loss=3.921945333480835
I0202 19:37:42.188776 140022518892288 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.7577537298202515, loss=5.664088726043701
I0202 19:38:28.214122 140023005427456 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.044792890548706, loss=3.6157641410827637
I0202 19:39:11.383467 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:39:22.086564 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:39:53.143026 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:39:54.738137 140184451094336 submission_runner.py:408] Time since start: 28362.44s, 	Step: 55995, 	{'train/accuracy': 0.44322264194488525, 'train/loss': 2.532923936843872, 'validation/accuracy': 0.402319997549057, 'validation/loss': 2.7546207904815674, 'validation/num_examples': 50000, 'test/accuracy': 0.302700012922287, 'test/loss': 3.414133310317993, 'test/num_examples': 10000, 'score': 25667.46424293518, 'total_duration': 28362.443382501602, 'accumulated_submission_time': 25667.46424293518, 'accumulated_eval_time': 2690.1619765758514, 'accumulated_logging_time': 1.9212877750396729}
I0202 19:39:54.765599 140022518892288 logging_writer.py:48] [55995] accumulated_eval_time=2690.161977, accumulated_logging_time=1.921288, accumulated_submission_time=25667.464243, global_step=55995, preemption_count=0, score=25667.464243, test/accuracy=0.302700, test/loss=3.414133, test/num_examples=10000, total_duration=28362.443383, train/accuracy=0.443223, train/loss=2.532924, validation/accuracy=0.402320, validation/loss=2.754621, validation/num_examples=50000
I0202 19:39:57.169321 140023005427456 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.8716737627983093, loss=5.289715766906738
I0202 19:40:39.269713 140022518892288 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0285974740982056, loss=3.748904228210449
I0202 19:41:25.465237 140023005427456 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0693820714950562, loss=3.546772003173828
I0202 19:42:12.125410 140022518892288 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.2924721240997314, loss=3.835292339324951
I0202 19:42:58.095687 140023005427456 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.1726984977722168, loss=3.61319637298584
I0202 19:43:44.025240 140022518892288 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.7546564340591431, loss=5.662459850311279
I0202 19:44:30.188040 140023005427456 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.010920763015747, loss=3.9641904830932617
I0202 19:45:16.318523 140022518892288 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0173518657684326, loss=3.6681272983551025
I0202 19:46:02.229879 140023005427456 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.839000940322876, loss=3.6788175106048584
I0202 19:46:48.416047 140022518892288 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7746769785881042, loss=4.86953067779541
I0202 19:46:55.144244 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:47:05.517289 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:47:37.115484 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:47:38.720103 140184451094336 submission_runner.py:408] Time since start: 28826.43s, 	Step: 56916, 	{'train/accuracy': 0.44078123569488525, 'train/loss': 2.5251173973083496, 'validation/accuracy': 0.41113999485969543, 'validation/loss': 2.6807515621185303, 'validation/num_examples': 50000, 'test/accuracy': 0.3193000257015228, 'test/loss': 3.296461582183838, 'test/num_examples': 10000, 'score': 26087.783744573593, 'total_duration': 28826.425322294235, 'accumulated_submission_time': 26087.783744573593, 'accumulated_eval_time': 2733.7377874851227, 'accumulated_logging_time': 1.959881067276001}
I0202 19:47:38.749113 140023005427456 logging_writer.py:48] [56916] accumulated_eval_time=2733.737787, accumulated_logging_time=1.959881, accumulated_submission_time=26087.783745, global_step=56916, preemption_count=0, score=26087.783745, test/accuracy=0.319300, test/loss=3.296462, test/num_examples=10000, total_duration=28826.425322, train/accuracy=0.440781, train/loss=2.525117, validation/accuracy=0.411140, validation/loss=2.680752, validation/num_examples=50000
I0202 19:48:13.905908 140022518892288 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9152143001556396, loss=4.908682823181152
I0202 19:48:59.413887 140023005427456 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.8651392459869385, loss=4.011173725128174
I0202 19:49:45.559685 140022518892288 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.7966763973236084, loss=5.567905902862549
I0202 19:50:31.765446 140023005427456 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.910063624382019, loss=4.091424942016602
I0202 19:51:17.898430 140022518892288 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.0994642972946167, loss=3.8128128051757812
I0202 19:52:04.275160 140023005427456 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1262643337249756, loss=3.8336689472198486
I0202 19:52:50.599818 140022518892288 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.8706334829330444, loss=4.368361473083496
I0202 19:53:36.804322 140023005427456 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.7972920536994934, loss=4.820347309112549
I0202 19:54:23.063974 140022518892288 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.2162137031555176, loss=3.6630427837371826
I0202 19:54:38.939267 140184451094336 spec.py:321] Evaluating on the training split.
I0202 19:54:49.290231 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 19:55:21.929380 140184451094336 spec.py:349] Evaluating on the test split.
I0202 19:55:23.526636 140184451094336 submission_runner.py:408] Time since start: 29291.23s, 	Step: 57836, 	{'train/accuracy': 0.4381054639816284, 'train/loss': 2.5220413208007812, 'validation/accuracy': 0.4074999988079071, 'validation/loss': 2.682382345199585, 'validation/num_examples': 50000, 'test/accuracy': 0.3222000300884247, 'test/loss': 3.286940097808838, 'test/num_examples': 10000, 'score': 26507.91575574875, 'total_duration': 29291.23188686371, 'accumulated_submission_time': 26507.91575574875, 'accumulated_eval_time': 2778.325141429901, 'accumulated_logging_time': 1.9994747638702393}
I0202 19:55:23.551573 140023005427456 logging_writer.py:48] [57836] accumulated_eval_time=2778.325141, accumulated_logging_time=1.999475, accumulated_submission_time=26507.915756, global_step=57836, preemption_count=0, score=26507.915756, test/accuracy=0.322200, test/loss=3.286940, test/num_examples=10000, total_duration=29291.231887, train/accuracy=0.438105, train/loss=2.522041, validation/accuracy=0.407500, validation/loss=2.682382, validation/num_examples=50000
I0202 19:55:49.555639 140022518892288 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.161047101020813, loss=3.7341816425323486
I0202 19:56:35.400388 140023005427456 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.8153933882713318, loss=4.820008754730225
I0202 19:57:21.768176 140022518892288 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2263727188110352, loss=3.699509859085083
I0202 19:58:07.961065 140023005427456 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9336159825325012, loss=5.334804058074951
I0202 19:58:53.992597 140022518892288 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8562521934509277, loss=4.286619186401367
I0202 19:59:40.367560 140023005427456 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.0339826345443726, loss=3.6981465816497803
I0202 20:00:26.605214 140022518892288 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0364247560501099, loss=4.057656288146973
I0202 20:01:12.876184 140023005427456 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.8554081320762634, loss=5.040064811706543
I0202 20:01:59.270626 140022518892288 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2296181917190552, loss=3.63671612739563
I0202 20:02:23.551029 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:02:34.093700 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:03:08.248458 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:03:09.855947 140184451094336 submission_runner.py:408] Time since start: 29757.56s, 	Step: 58754, 	{'train/accuracy': 0.4493750035762787, 'train/loss': 2.5028460025787354, 'validation/accuracy': 0.41589999198913574, 'validation/loss': 2.6864118576049805, 'validation/num_examples': 50000, 'test/accuracy': 0.3222000300884247, 'test/loss': 3.29914927482605, 'test/num_examples': 10000, 'score': 26927.859172344208, 'total_duration': 29757.561191558838, 'accumulated_submission_time': 26927.859172344208, 'accumulated_eval_time': 2824.6300699710846, 'accumulated_logging_time': 2.0336482524871826}
I0202 20:03:09.879595 140023005427456 logging_writer.py:48] [58754] accumulated_eval_time=2824.630070, accumulated_logging_time=2.033648, accumulated_submission_time=26927.859172, global_step=58754, preemption_count=0, score=26927.859172, test/accuracy=0.322200, test/loss=3.299149, test/num_examples=10000, total_duration=29757.561192, train/accuracy=0.449375, train/loss=2.502846, validation/accuracy=0.415900, validation/loss=2.686412, validation/num_examples=50000
I0202 20:03:28.682081 140022518892288 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.8951379060745239, loss=4.835241317749023
I0202 20:04:13.003054 140023005427456 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.0406628847122192, loss=3.5787513256073
I0202 20:04:59.173274 140022518892288 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9640295505523682, loss=3.784043550491333
I0202 20:05:45.670558 140023005427456 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0590635538101196, loss=3.7033498287200928
I0202 20:06:31.812881 140022518892288 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1843229532241821, loss=3.654270887374878
I0202 20:07:17.910661 140023005427456 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1834787130355835, loss=3.7424283027648926
I0202 20:08:03.904975 140022518892288 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9128705859184265, loss=3.6279895305633545
I0202 20:08:49.899771 140023005427456 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0656299591064453, loss=3.9717373847961426
I0202 20:09:36.079434 140022518892288 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9011184573173523, loss=4.660412311553955
I0202 20:10:10.002625 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:10:20.414832 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:10:54.537267 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:10:56.136505 140184451094336 submission_runner.py:408] Time since start: 30223.84s, 	Step: 59675, 	{'train/accuracy': 0.4404882788658142, 'train/loss': 2.5459964275360107, 'validation/accuracy': 0.40943998098373413, 'validation/loss': 2.7102136611938477, 'validation/num_examples': 50000, 'test/accuracy': 0.3230000138282776, 'test/loss': 3.3165783882141113, 'test/num_examples': 10000, 'score': 27347.924685001373, 'total_duration': 30223.841745615005, 'accumulated_submission_time': 27347.924685001373, 'accumulated_eval_time': 2870.763954639435, 'accumulated_logging_time': 2.0666730403900146}
I0202 20:10:56.160812 140023005427456 logging_writer.py:48] [59675] accumulated_eval_time=2870.763955, accumulated_logging_time=2.066673, accumulated_submission_time=27347.924685, global_step=59675, preemption_count=0, score=27347.924685, test/accuracy=0.323000, test/loss=3.316578, test/num_examples=10000, total_duration=30223.841746, train/accuracy=0.440488, train/loss=2.545996, validation/accuracy=0.409440, validation/loss=2.710214, validation/num_examples=50000
I0202 20:11:06.573648 140022518892288 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.074634313583374, loss=3.5445706844329834
I0202 20:11:50.204198 140023005427456 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7623333930969238, loss=5.7908196449279785
I0202 20:12:36.500898 140022518892288 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2144156694412231, loss=3.517413854598999
I0202 20:13:22.821099 140023005427456 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0970054864883423, loss=3.607757091522217
I0202 20:14:09.097041 140022518892288 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1686956882476807, loss=3.672088623046875
I0202 20:14:55.149177 140023005427456 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0410608053207397, loss=3.512803077697754
I0202 20:15:41.320835 140022518892288 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.2919377088546753, loss=3.530322313308716
I0202 20:16:27.720415 140023005427456 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0410555601119995, loss=3.407514810562134
I0202 20:17:13.792345 140022518892288 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.8544098138809204, loss=5.623709201812744
I0202 20:17:56.464531 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:18:07.188414 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:18:39.527059 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:18:41.132675 140184451094336 submission_runner.py:408] Time since start: 30688.84s, 	Step: 60594, 	{'train/accuracy': 0.453437477350235, 'train/loss': 2.441464424133301, 'validation/accuracy': 0.42587998509407043, 'validation/loss': 2.5996832847595215, 'validation/num_examples': 50000, 'test/accuracy': 0.33310002088546753, 'test/loss': 3.216318130493164, 'test/num_examples': 10000, 'score': 27768.17107129097, 'total_duration': 30688.837917804718, 'accumulated_submission_time': 27768.17107129097, 'accumulated_eval_time': 2915.4320845603943, 'accumulated_logging_time': 2.101196527481079}
I0202 20:18:41.164124 140023005427456 logging_writer.py:48] [60594] accumulated_eval_time=2915.432085, accumulated_logging_time=2.101197, accumulated_submission_time=27768.171071, global_step=60594, preemption_count=0, score=27768.171071, test/accuracy=0.333100, test/loss=3.216318, test/num_examples=10000, total_duration=30688.837918, train/accuracy=0.453437, train/loss=2.441464, validation/accuracy=0.425880, validation/loss=2.599683, validation/num_examples=50000
I0202 20:18:43.966936 140022518892288 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9795463681221008, loss=3.9342992305755615
I0202 20:19:26.514621 140023005427456 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2182503938674927, loss=3.6821720600128174
I0202 20:20:12.654370 140022518892288 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9892558455467224, loss=3.5829086303710938
I0202 20:20:58.862714 140023005427456 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0971705913543701, loss=3.833442211151123
I0202 20:21:45.049113 140022518892288 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2666574716567993, loss=3.6107914447784424
I0202 20:22:31.393824 140023005427456 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.8558445572853088, loss=5.7486419677734375
I0202 20:23:17.605687 140022518892288 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9498704075813293, loss=3.5601131916046143
I0202 20:24:03.837106 140023005427456 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1362885236740112, loss=3.9064435958862305
I0202 20:24:49.896682 140022518892288 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.1203681230545044, loss=3.859177827835083
I0202 20:25:36.346525 140023005427456 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.787659764289856, loss=5.749439239501953
I0202 20:25:41.584023 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:25:51.959637 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:26:27.898024 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:26:29.498927 140184451094336 submission_runner.py:408] Time since start: 31157.20s, 	Step: 61513, 	{'train/accuracy': 0.454902321100235, 'train/loss': 2.44576096534729, 'validation/accuracy': 0.4205799996852875, 'validation/loss': 2.644345283508301, 'validation/num_examples': 50000, 'test/accuracy': 0.32280001044273376, 'test/loss': 3.2577946186065674, 'test/num_examples': 10000, 'score': 28188.53337931633, 'total_duration': 31157.204171419144, 'accumulated_submission_time': 28188.53337931633, 'accumulated_eval_time': 2963.3469684123993, 'accumulated_logging_time': 2.1426825523376465}
I0202 20:26:29.526808 140022518892288 logging_writer.py:48] [61513] accumulated_eval_time=2963.346968, accumulated_logging_time=2.142683, accumulated_submission_time=28188.533379, global_step=61513, preemption_count=0, score=28188.533379, test/accuracy=0.322800, test/loss=3.257795, test/num_examples=10000, total_duration=31157.204171, train/accuracy=0.454902, train/loss=2.445761, validation/accuracy=0.420580, validation/loss=2.644345, validation/num_examples=50000
I0202 20:27:05.920613 140023005427456 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.2488898038864136, loss=3.5799529552459717
I0202 20:27:51.804853 140022518892288 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9108796119689941, loss=3.8817949295043945
I0202 20:28:37.980933 140023005427456 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.140528678894043, loss=3.619678258895874
I0202 20:29:24.355127 140022518892288 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0427988767623901, loss=3.939911365509033
I0202 20:30:10.377887 140023005427456 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.097203016281128, loss=3.5143613815307617
I0202 20:30:56.386200 140022518892288 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.2860881090164185, loss=5.266926288604736
I0202 20:31:42.817802 140023005427456 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.8375526666641235, loss=4.550127983093262
I0202 20:32:29.237648 140022518892288 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9320157170295715, loss=3.855196237564087
I0202 20:33:15.532471 140023005427456 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.8321036696434021, loss=5.794581890106201
I0202 20:33:29.900234 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:33:39.990115 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:34:12.963927 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:34:14.562395 140184451094336 submission_runner.py:408] Time since start: 31622.27s, 	Step: 62433, 	{'train/accuracy': 0.45238280296325684, 'train/loss': 2.508619546890259, 'validation/accuracy': 0.4140399992465973, 'validation/loss': 2.6991686820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.32260000705718994, 'test/loss': 3.3033504486083984, 'test/num_examples': 10000, 'score': 28608.848690748215, 'total_duration': 31622.267642736435, 'accumulated_submission_time': 28608.848690748215, 'accumulated_eval_time': 3008.009134531021, 'accumulated_logging_time': 2.1812844276428223}
I0202 20:34:14.587357 140022518892288 logging_writer.py:48] [62433] accumulated_eval_time=3008.009135, accumulated_logging_time=2.181284, accumulated_submission_time=28608.848691, global_step=62433, preemption_count=0, score=28608.848691, test/accuracy=0.322600, test/loss=3.303350, test/num_examples=10000, total_duration=31622.267643, train/accuracy=0.452383, train/loss=2.508620, validation/accuracy=0.414040, validation/loss=2.699169, validation/num_examples=50000
I0202 20:34:41.801929 140023005427456 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0224521160125732, loss=3.4792518615722656
I0202 20:35:27.616207 140022518892288 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.1446747779846191, loss=3.8258330821990967
I0202 20:36:13.809503 140023005427456 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.8656991124153137, loss=4.567785263061523
I0202 20:37:00.199317 140022518892288 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.085930347442627, loss=3.7899489402770996
I0202 20:37:46.191209 140023005427456 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.0075997114181519, loss=3.3787684440612793
I0202 20:38:32.507353 140022518892288 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.8325852751731873, loss=5.474205017089844
I0202 20:39:18.721790 140023005427456 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.0265264511108398, loss=3.5518341064453125
I0202 20:40:04.610913 140022518892288 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.7579419612884521, loss=4.988345146179199
I0202 20:40:50.773814 140023005427456 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0291378498077393, loss=3.596818685531616
I0202 20:41:14.574461 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:41:27.204355 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:42:01.737595 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:42:03.337409 140184451094336 submission_runner.py:408] Time since start: 32091.04s, 	Step: 63353, 	{'train/accuracy': 0.4460742175579071, 'train/loss': 2.4987034797668457, 'validation/accuracy': 0.4134199917316437, 'validation/loss': 2.6845717430114746, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.3364946842193604, 'test/num_examples': 10000, 'score': 29028.77939391136, 'total_duration': 32091.04265642166, 'accumulated_submission_time': 29028.77939391136, 'accumulated_eval_time': 3056.772082090378, 'accumulated_logging_time': 2.215562343597412}
I0202 20:42:03.362924 140022518892288 logging_writer.py:48] [63353] accumulated_eval_time=3056.772082, accumulated_logging_time=2.215562, accumulated_submission_time=29028.779394, global_step=63353, preemption_count=0, score=29028.779394, test/accuracy=0.311300, test/loss=3.336495, test/num_examples=10000, total_duration=32091.042656, train/accuracy=0.446074, train/loss=2.498703, validation/accuracy=0.413420, validation/loss=2.684572, validation/num_examples=50000
I0202 20:42:22.573079 140023005427456 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0546690225601196, loss=3.4936156272888184
I0202 20:43:07.284071 140022518892288 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8880631327629089, loss=3.5089991092681885
I0202 20:43:53.661323 140023005427456 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.1745187044143677, loss=3.444598913192749
I0202 20:44:40.091457 140022518892288 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1215922832489014, loss=3.6041860580444336
I0202 20:45:26.532976 140023005427456 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.067734956741333, loss=3.7526941299438477
I0202 20:46:12.742038 140022518892288 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1768418550491333, loss=3.573455810546875
I0202 20:46:58.934707 140023005427456 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1414324045181274, loss=3.491053819656372
I0202 20:47:45.211676 140022518892288 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0557445287704468, loss=4.397243976593018
I0202 20:48:31.575473 140023005427456 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0277955532073975, loss=4.778598308563232
I0202 20:49:03.576163 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:49:13.890854 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:49:49.893836 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:49:51.493985 140184451094336 submission_runner.py:408] Time since start: 32559.20s, 	Step: 64271, 	{'train/accuracy': 0.45146483182907104, 'train/loss': 2.4649150371551514, 'validation/accuracy': 0.4184199869632721, 'validation/loss': 2.648669719696045, 'validation/num_examples': 50000, 'test/accuracy': 0.32420000433921814, 'test/loss': 3.2666985988616943, 'test/num_examples': 10000, 'score': 29448.936309099197, 'total_duration': 32559.199233531952, 'accumulated_submission_time': 29448.936309099197, 'accumulated_eval_time': 3104.6899168491364, 'accumulated_logging_time': 2.250218152999878}
I0202 20:49:51.518976 140022518892288 logging_writer.py:48] [64271] accumulated_eval_time=3104.689917, accumulated_logging_time=2.250218, accumulated_submission_time=29448.936309, global_step=64271, preemption_count=0, score=29448.936309, test/accuracy=0.324200, test/loss=3.266699, test/num_examples=10000, total_duration=32559.199234, train/accuracy=0.451465, train/loss=2.464915, validation/accuracy=0.418420, validation/loss=2.648670, validation/num_examples=50000
I0202 20:50:03.525784 140023005427456 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.9083696603775024, loss=4.006021976470947
I0202 20:50:47.050825 140022518892288 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1655910015106201, loss=3.505136489868164
I0202 20:51:33.480708 140023005427456 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1280713081359863, loss=3.546879529953003
I0202 20:52:19.939354 140022518892288 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9916806817054749, loss=5.254058837890625
I0202 20:53:05.961070 140023005427456 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1292858123779297, loss=4.548497676849365
I0202 20:53:52.161573 140022518892288 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.0051416158676147, loss=3.736025094985962
I0202 20:54:38.330219 140023005427456 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1486382484436035, loss=3.5840165615081787
I0202 20:55:24.797436 140022518892288 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1232178211212158, loss=3.5260300636291504
I0202 20:56:11.354329 140023005427456 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8441701531410217, loss=4.109751224517822
I0202 20:56:51.790028 140184451094336 spec.py:321] Evaluating on the training split.
I0202 20:57:02.249841 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 20:57:33.870144 140184451094336 spec.py:349] Evaluating on the test split.
I0202 20:57:35.481160 140184451094336 submission_runner.py:408] Time since start: 33023.19s, 	Step: 65189, 	{'train/accuracy': 0.4797070324420929, 'train/loss': 2.2820422649383545, 'validation/accuracy': 0.42282000184059143, 'validation/loss': 2.5929930210113525, 'validation/num_examples': 50000, 'test/accuracy': 0.33160001039505005, 'test/loss': 3.247788906097412, 'test/num_examples': 10000, 'score': 29869.149247169495, 'total_duration': 33023.18640756607, 'accumulated_submission_time': 29869.149247169495, 'accumulated_eval_time': 3148.381046772003, 'accumulated_logging_time': 2.2855420112609863}
I0202 20:57:35.508616 140022518892288 logging_writer.py:48] [65189] accumulated_eval_time=3148.381047, accumulated_logging_time=2.285542, accumulated_submission_time=29869.149247, global_step=65189, preemption_count=0, score=29869.149247, test/accuracy=0.331600, test/loss=3.247789, test/num_examples=10000, total_duration=33023.186408, train/accuracy=0.479707, train/loss=2.282042, validation/accuracy=0.422820, validation/loss=2.592993, validation/num_examples=50000
I0202 20:57:40.305095 140023005427456 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1880582571029663, loss=3.309475898742676
I0202 20:58:22.906607 140022518892288 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.867051362991333, loss=4.305137634277344
I0202 20:59:09.233501 140023005427456 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0186967849731445, loss=3.5181221961975098
I0202 20:59:55.621618 140022518892288 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.6535980701446533, loss=4.931487083435059
I0202 21:00:41.826054 140023005427456 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.0473135709762573, loss=3.5485212802886963
I0202 21:01:28.022327 140022518892288 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.0919294357299805, loss=3.6666789054870605
I0202 21:02:14.350603 140023005427456 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1905410289764404, loss=3.5535428524017334
I0202 21:03:00.417426 140022518892288 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0584229230880737, loss=3.758824586868286
I0202 21:03:46.521217 140023005427456 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.8907791972160339, loss=3.9802651405334473
I0202 21:04:32.965689 140022518892288 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.8267092704772949, loss=5.8143744468688965
I0202 21:04:35.700959 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:04:46.409045 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:05:13.330541 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:05:14.934366 140184451094336 submission_runner.py:408] Time since start: 33482.64s, 	Step: 66108, 	{'train/accuracy': 0.46281248331069946, 'train/loss': 2.4145824909210205, 'validation/accuracy': 0.4338599741458893, 'validation/loss': 2.566502094268799, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.1849935054779053, 'test/num_examples': 10000, 'score': 30289.283529758453, 'total_duration': 33482.63961672783, 'accumulated_submission_time': 30289.283529758453, 'accumulated_eval_time': 3187.6144468784332, 'accumulated_logging_time': 2.323702335357666}
I0202 21:05:14.963902 140023005427456 logging_writer.py:48] [66108] accumulated_eval_time=3187.614447, accumulated_logging_time=2.323702, accumulated_submission_time=30289.283530, global_step=66108, preemption_count=0, score=30289.283530, test/accuracy=0.342700, test/loss=3.184994, test/num_examples=10000, total_duration=33482.639617, train/accuracy=0.462812, train/loss=2.414582, validation/accuracy=0.433860, validation/loss=2.566502, validation/num_examples=50000
I0202 21:05:53.696121 140022518892288 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.8179612755775452, loss=5.297494888305664
I0202 21:06:39.852464 140023005427456 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.9502761363983154, loss=3.7063496112823486
I0202 21:07:26.047197 140022518892288 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.9479752779006958, loss=4.850172519683838
I0202 21:08:12.307817 140023005427456 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8033113479614258, loss=5.102240562438965
I0202 21:08:58.375167 140022518892288 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.0574191808700562, loss=3.414699077606201
I0202 21:09:44.635871 140023005427456 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.9185940027236938, loss=3.6725523471832275
I0202 21:10:30.909750 140022518892288 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0784701108932495, loss=3.8496108055114746
I0202 21:11:16.889431 140023005427456 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.7333323955535889, loss=5.278467655181885
I0202 21:12:03.062365 140022518892288 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2078540325164795, loss=3.656188726425171
I0202 21:12:15.240419 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:12:25.643569 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:12:58.721441 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:13:00.331135 140184451094336 submission_runner.py:408] Time since start: 33948.04s, 	Step: 67028, 	{'train/accuracy': 0.4659765660762787, 'train/loss': 2.3797836303710938, 'validation/accuracy': 0.4309599995613098, 'validation/loss': 2.57188081741333, 'validation/num_examples': 50000, 'test/accuracy': 0.32930001616477966, 'test/loss': 3.2058255672454834, 'test/num_examples': 10000, 'score': 30709.503110408783, 'total_duration': 33948.036386966705, 'accumulated_submission_time': 30709.503110408783, 'accumulated_eval_time': 3232.7051644325256, 'accumulated_logging_time': 2.3622336387634277}
I0202 21:13:00.356657 140023005427456 logging_writer.py:48] [67028] accumulated_eval_time=3232.705164, accumulated_logging_time=2.362234, accumulated_submission_time=30709.503110, global_step=67028, preemption_count=0, score=30709.503110, test/accuracy=0.329300, test/loss=3.205826, test/num_examples=10000, total_duration=33948.036387, train/accuracy=0.465977, train/loss=2.379784, validation/accuracy=0.430960, validation/loss=2.571881, validation/num_examples=50000
I0202 21:13:29.611045 140022518892288 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.91800457239151, loss=4.453868865966797
I0202 21:14:15.503554 140023005427456 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.499115228652954, loss=4.095688819885254
I0202 21:15:01.724910 140022518892288 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.8660933375358582, loss=5.421006679534912
I0202 21:15:48.203346 140023005427456 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0235272645950317, loss=5.093392372131348
I0202 21:16:34.558913 140022518892288 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1491124629974365, loss=3.464550733566284
I0202 21:17:20.971743 140023005427456 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9107134938240051, loss=4.029287338256836
I0202 21:18:07.028868 140022518892288 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.247502326965332, loss=3.3753950595855713
I0202 21:18:53.140904 140023005427456 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.001358985900879, loss=3.444796562194824
I0202 21:19:39.313472 140022518892288 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9766581058502197, loss=5.530672073364258
I0202 21:20:00.777785 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:20:11.247837 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:20:45.653048 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:20:47.254634 140184451094336 submission_runner.py:408] Time since start: 34414.96s, 	Step: 67948, 	{'train/accuracy': 0.4738866984844208, 'train/loss': 2.352745532989502, 'validation/accuracy': 0.4340199828147888, 'validation/loss': 2.5681827068328857, 'validation/num_examples': 50000, 'test/accuracy': 0.3377000093460083, 'test/loss': 3.203951835632324, 'test/num_examples': 10000, 'score': 31129.867817878723, 'total_duration': 34414.95987582207, 'accumulated_submission_time': 31129.867817878723, 'accumulated_eval_time': 3279.18199968338, 'accumulated_logging_time': 2.3966870307922363}
I0202 21:20:47.281832 140023005427456 logging_writer.py:48] [67948] accumulated_eval_time=3279.182000, accumulated_logging_time=2.396687, accumulated_submission_time=31129.867818, global_step=67948, preemption_count=0, score=31129.867818, test/accuracy=0.337700, test/loss=3.203952, test/num_examples=10000, total_duration=34414.959876, train/accuracy=0.473887, train/loss=2.352746, validation/accuracy=0.434020, validation/loss=2.568183, validation/num_examples=50000
I0202 21:21:08.495371 140022518892288 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1367775201797485, loss=3.4286811351776123
I0202 21:21:53.536420 140023005427456 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.044687032699585, loss=3.567415475845337
I0202 21:22:39.593028 140022518892288 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9609032273292542, loss=3.5687813758850098
I0202 21:23:25.824306 140023005427456 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2856361865997314, loss=3.543708086013794
I0202 21:24:11.948332 140022518892288 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.176972508430481, loss=3.58087158203125
I0202 21:24:58.299144 140023005427456 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.9986940026283264, loss=3.5434796810150146
I0202 21:25:44.591907 140022518892288 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0752241611480713, loss=4.057934761047363
I0202 21:26:30.677663 140023005427456 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.0596723556518555, loss=3.3022518157958984
I0202 21:27:16.975137 140022518892288 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.1041216850280762, loss=3.497040271759033
I0202 21:27:47.535232 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:27:57.736547 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:28:30.798182 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:28:32.396250 140184451094336 submission_runner.py:408] Time since start: 34880.10s, 	Step: 68868, 	{'train/accuracy': 0.4649609327316284, 'train/loss': 2.447537422180176, 'validation/accuracy': 0.43293997645378113, 'validation/loss': 2.5877325534820557, 'validation/num_examples': 50000, 'test/accuracy': 0.3359000086784363, 'test/loss': 3.2074527740478516, 'test/num_examples': 10000, 'score': 31550.06188440323, 'total_duration': 34880.10146713257, 'accumulated_submission_time': 31550.06188440323, 'accumulated_eval_time': 3324.0429894924164, 'accumulated_logging_time': 2.434640884399414}
I0202 21:28:32.425236 140023005427456 logging_writer.py:48] [68868] accumulated_eval_time=3324.042989, accumulated_logging_time=2.434641, accumulated_submission_time=31550.061884, global_step=68868, preemption_count=0, score=31550.061884, test/accuracy=0.335900, test/loss=3.207453, test/num_examples=10000, total_duration=34880.101467, train/accuracy=0.464961, train/loss=2.447537, validation/accuracy=0.432940, validation/loss=2.587733, validation/num_examples=50000
I0202 21:28:45.620836 140022518892288 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.0190532207489014, loss=3.3111448287963867
I0202 21:29:29.041195 140023005427456 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.886389434337616, loss=4.922048568725586
I0202 21:30:15.454480 140022518892288 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.9674299955368042, loss=3.4576172828674316
I0202 21:31:01.481391 140023005427456 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0341166257858276, loss=3.6058120727539062
I0202 21:31:47.765661 140022518892288 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.9405927062034607, loss=4.7069902420043945
I0202 21:32:34.260732 140023005427456 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.062232255935669, loss=3.367943286895752
I0202 21:33:20.406431 140022518892288 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.9868074655532837, loss=3.4156179428100586
I0202 21:34:06.400494 140023005427456 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.80665123462677, loss=4.8439178466796875
I0202 21:34:52.288759 140022518892288 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2764170169830322, loss=3.499011993408203
I0202 21:35:32.788906 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:35:43.439600 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:36:17.801966 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:36:19.396236 140184451094336 submission_runner.py:408] Time since start: 35347.10s, 	Step: 69789, 	{'train/accuracy': 0.4654882848262787, 'train/loss': 2.4043633937835693, 'validation/accuracy': 0.4331599771976471, 'validation/loss': 2.563265800476074, 'validation/num_examples': 50000, 'test/accuracy': 0.3391000032424927, 'test/loss': 3.190361976623535, 'test/num_examples': 10000, 'score': 31970.368980884552, 'total_duration': 35347.10148000717, 'accumulated_submission_time': 31970.368980884552, 'accumulated_eval_time': 3370.650318622589, 'accumulated_logging_time': 2.472820997238159}
I0202 21:36:19.423648 140023005427456 logging_writer.py:48] [69789] accumulated_eval_time=3370.650319, accumulated_logging_time=2.472821, accumulated_submission_time=31970.368981, global_step=69789, preemption_count=0, score=31970.368981, test/accuracy=0.339100, test/loss=3.190362, test/num_examples=10000, total_duration=35347.101480, train/accuracy=0.465488, train/loss=2.404363, validation/accuracy=0.433160, validation/loss=2.563266, validation/num_examples=50000
I0202 21:36:24.230792 140022518892288 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.0431849956512451, loss=3.5100607872009277
I0202 21:37:07.012930 140023005427456 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.1572345495224, loss=3.6038053035736084
I0202 21:37:52.938885 140022518892288 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.4615037441253662, loss=3.845480442047119
I0202 21:38:39.869585 140023005427456 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.0259426832199097, loss=3.4481379985809326
I0202 21:39:26.014522 140022518892288 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0939477682113647, loss=3.668321371078491
I0202 21:40:12.303404 140023005427456 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.0091253519058228, loss=3.7170958518981934
I0202 21:40:58.381352 140022518892288 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.8601654171943665, loss=4.564343452453613
I0202 21:41:44.864174 140023005427456 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.0696502923965454, loss=3.4439284801483154
I0202 21:42:31.217806 140022518892288 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.959574818611145, loss=4.113658905029297
I0202 21:43:17.638874 140023005427456 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.035363793373108, loss=3.355121374130249
I0202 21:43:19.664996 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:43:30.297001 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:44:00.360995 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:44:01.972545 140184451094336 submission_runner.py:408] Time since start: 35809.68s, 	Step: 70706, 	{'train/accuracy': 0.47871091961860657, 'train/loss': 2.317901611328125, 'validation/accuracy': 0.439520001411438, 'validation/loss': 2.5207505226135254, 'validation/num_examples': 50000, 'test/accuracy': 0.34360000491142273, 'test/loss': 3.145872116088867, 'test/num_examples': 10000, 'score': 32390.55333662033, 'total_duration': 35809.67774987221, 'accumulated_submission_time': 32390.55333662033, 'accumulated_eval_time': 3412.957806110382, 'accumulated_logging_time': 2.5098109245300293}
I0202 21:44:02.004245 140022518892288 logging_writer.py:48] [70706] accumulated_eval_time=3412.957806, accumulated_logging_time=2.509811, accumulated_submission_time=32390.553337, global_step=70706, preemption_count=0, score=32390.553337, test/accuracy=0.343600, test/loss=3.145872, test/num_examples=10000, total_duration=35809.677750, train/accuracy=0.478711, train/loss=2.317902, validation/accuracy=0.439520, validation/loss=2.520751, validation/num_examples=50000
I0202 21:44:41.590624 140023005427456 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.063454508781433, loss=3.4766130447387695
I0202 21:45:27.901201 140022518892288 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.0365036725997925, loss=3.3051187992095947
I0202 21:46:14.263865 140023005427456 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2313578128814697, loss=3.5235862731933594
I0202 21:47:00.411222 140022518892288 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.543012022972107, loss=3.4704549312591553
I0202 21:47:46.471069 140023005427456 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.9625685811042786, loss=4.17236328125
I0202 21:48:32.943608 140022518892288 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.0183210372924805, loss=3.440321922302246
I0202 21:49:19.117373 140023005427456 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0196021795272827, loss=3.3147976398468018
I0202 21:50:05.126888 140022518892288 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.9820210933685303, loss=3.9857141971588135
I0202 21:50:51.435505 140023005427456 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.0805819034576416, loss=3.4848504066467285
I0202 21:51:02.290458 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:51:12.674981 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:51:47.757339 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:51:49.357125 140184451094336 submission_runner.py:408] Time since start: 36277.06s, 	Step: 71625, 	{'train/accuracy': 0.46162107586860657, 'train/loss': 2.4654088020324707, 'validation/accuracy': 0.4322199821472168, 'validation/loss': 2.6213440895080566, 'validation/num_examples': 50000, 'test/accuracy': 0.3367000222206116, 'test/loss': 3.232973098754883, 'test/num_examples': 10000, 'score': 32810.782964229584, 'total_duration': 36277.062376499176, 'accumulated_submission_time': 32810.782964229584, 'accumulated_eval_time': 3460.0244784355164, 'accumulated_logging_time': 2.551076889038086}
I0202 21:51:49.383560 140022518892288 logging_writer.py:48] [71625] accumulated_eval_time=3460.024478, accumulated_logging_time=2.551077, accumulated_submission_time=32810.782964, global_step=71625, preemption_count=0, score=32810.782964, test/accuracy=0.336700, test/loss=3.232973, test/num_examples=10000, total_duration=36277.062376, train/accuracy=0.461621, train/loss=2.465409, validation/accuracy=0.432220, validation/loss=2.621344, validation/num_examples=50000
I0202 21:52:20.022814 140023005427456 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.0528647899627686, loss=3.389788866043091
I0202 21:53:05.954001 140022518892288 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0460171699523926, loss=3.9524693489074707
I0202 21:53:52.152527 140023005427456 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.2135077714920044, loss=3.6241836547851562
I0202 21:54:38.278450 140022518892288 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.8057917356491089, loss=5.253083229064941
I0202 21:55:24.267375 140023005427456 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.9684945344924927, loss=3.365039825439453
I0202 21:56:10.467776 140022518892288 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.1155991554260254, loss=3.3805184364318848
I0202 21:56:56.580341 140023005427456 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.100520133972168, loss=4.09094762802124
I0202 21:57:42.751354 140022518892288 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.0266810655593872, loss=4.101286888122559
I0202 21:58:28.833449 140023005427456 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.9961385130882263, loss=4.903146743774414
I0202 21:58:49.546419 140184451094336 spec.py:321] Evaluating on the training split.
I0202 21:58:59.583246 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 21:59:33.834492 140184451094336 spec.py:349] Evaluating on the test split.
I0202 21:59:35.451063 140184451094336 submission_runner.py:408] Time since start: 36743.16s, 	Step: 72547, 	{'train/accuracy': 0.4809960722923279, 'train/loss': 2.3176419734954834, 'validation/accuracy': 0.4526999890804291, 'validation/loss': 2.466217041015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3489000201225281, 'test/loss': 3.1029651165008545, 'test/num_examples': 10000, 'score': 33230.88841557503, 'total_duration': 36743.15631175041, 'accumulated_submission_time': 33230.88841557503, 'accumulated_eval_time': 3505.9291064739227, 'accumulated_logging_time': 2.587003231048584}
I0202 21:59:35.476792 140022518892288 logging_writer.py:48] [72547] accumulated_eval_time=3505.929106, accumulated_logging_time=2.587003, accumulated_submission_time=33230.888416, global_step=72547, preemption_count=0, score=33230.888416, test/accuracy=0.348900, test/loss=3.102965, test/num_examples=10000, total_duration=36743.156312, train/accuracy=0.480996, train/loss=2.317642, validation/accuracy=0.452700, validation/loss=2.466217, validation/num_examples=50000
I0202 21:59:57.070737 140023005427456 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.1516441106796265, loss=3.652383804321289
I0202 22:00:42.054760 140022518892288 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.9059114456176758, loss=3.827861785888672
I0202 22:01:28.136047 140023005427456 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.1855056285858154, loss=3.295754909515381
I0202 22:02:14.624994 140022518892288 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.0685440301895142, loss=3.320624351501465
I0202 22:03:00.795092 140023005427456 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3059014081954956, loss=3.4616005420684814
I0202 22:03:47.307354 140022518892288 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.1073386669158936, loss=3.7577052116394043
I0202 22:04:33.613916 140023005427456 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.9453431963920593, loss=5.480979919433594
I0202 22:05:19.680539 140022518892288 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.9289147853851318, loss=4.548895359039307
I0202 22:06:05.880985 140023005427456 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.8140293955802917, loss=4.663388252258301
I0202 22:06:35.606746 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:06:46.391013 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:07:19.434845 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:07:21.058767 140184451094336 submission_runner.py:408] Time since start: 37208.76s, 	Step: 73466, 	{'train/accuracy': 0.48060545325279236, 'train/loss': 2.2896952629089355, 'validation/accuracy': 0.44373998045921326, 'validation/loss': 2.492424249649048, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.121434211730957, 'test/num_examples': 10000, 'score': 33650.96217060089, 'total_duration': 37208.76399350166, 'accumulated_submission_time': 33650.96217060089, 'accumulated_eval_time': 3551.381100177765, 'accumulated_logging_time': 2.621609687805176}
I0202 22:07:21.090014 140022518892288 logging_writer.py:48] [73466] accumulated_eval_time=3551.381100, accumulated_logging_time=2.621610, accumulated_submission_time=33650.962171, global_step=73466, preemption_count=0, score=33650.962171, test/accuracy=0.347300, test/loss=3.121434, test/num_examples=10000, total_duration=37208.763994, train/accuracy=0.480605, train/loss=2.289695, validation/accuracy=0.443740, validation/loss=2.492424, validation/num_examples=50000
I0202 22:07:35.232701 140023005427456 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.9329032301902771, loss=3.8583898544311523
I0202 22:08:19.011481 140022518892288 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.0310447216033936, loss=5.086022853851318
I0202 22:09:05.261097 140023005427456 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1705087423324585, loss=3.491995096206665
I0202 22:09:51.703719 140022518892288 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.1010946035385132, loss=3.5171542167663574
I0202 22:10:37.657401 140023005427456 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.9291675686836243, loss=5.057404518127441
I0202 22:11:23.695816 140022518892288 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.1368565559387207, loss=3.4383773803710938
I0202 22:12:10.295064 140023005427456 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.8732348680496216, loss=4.659369468688965
I0202 22:12:56.262169 140022518892288 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.0782262086868286, loss=3.6008472442626953
I0202 22:13:42.544611 140023005427456 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.9355288147926331, loss=5.240602493286133
I0202 22:14:21.535651 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:14:32.187395 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:15:07.061848 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:15:08.665441 140184451094336 submission_runner.py:408] Time since start: 37676.37s, 	Step: 74386, 	{'train/accuracy': 0.5169921517372131, 'train/loss': 2.1353821754455566, 'validation/accuracy': 0.4467199742794037, 'validation/loss': 2.486428737640381, 'validation/num_examples': 50000, 'test/accuracy': 0.3522000312805176, 'test/loss': 3.1258039474487305, 'test/num_examples': 10000, 'score': 34071.35045528412, 'total_duration': 37676.37067079544, 'accumulated_submission_time': 34071.35045528412, 'accumulated_eval_time': 3598.5108897686005, 'accumulated_logging_time': 2.663499355316162}
I0202 22:15:08.700842 140022518892288 logging_writer.py:48] [74386] accumulated_eval_time=3598.510890, accumulated_logging_time=2.663499, accumulated_submission_time=34071.350455, global_step=74386, preemption_count=0, score=34071.350455, test/accuracy=0.352200, test/loss=3.125804, test/num_examples=10000, total_duration=37676.370671, train/accuracy=0.516992, train/loss=2.135382, validation/accuracy=0.446720, validation/loss=2.486429, validation/num_examples=50000
I0202 22:15:14.696406 140023005427456 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.8236270546913147, loss=4.985737323760986
I0202 22:15:57.354661 140022518892288 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2829891443252563, loss=3.507382869720459
I0202 22:16:43.412530 140023005427456 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.021787166595459, loss=3.492466688156128
I0202 22:17:29.711917 140022518892288 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.074203372001648, loss=3.4506704807281494
I0202 22:18:16.011828 140023005427456 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.8294113874435425, loss=5.298318386077881
I0202 22:19:02.267080 140022518892288 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.0128449201583862, loss=3.632106304168701
I0202 22:19:48.428059 140023005427456 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.8290067315101624, loss=5.772307395935059
I0202 22:20:34.896213 140022518892288 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.491078495979309, loss=3.3447017669677734
I0202 22:21:21.371349 140023005427456 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.0200544595718384, loss=3.3049933910369873
I0202 22:22:07.932603 140022518892288 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2266656160354614, loss=3.3888309001922607
I0202 22:22:09.040303 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:22:19.575404 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:22:47.549349 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:22:49.154691 140184451094336 submission_runner.py:408] Time since start: 38136.86s, 	Step: 75304, 	{'train/accuracy': 0.48771482706069946, 'train/loss': 2.2401325702667236, 'validation/accuracy': 0.45593997836112976, 'validation/loss': 2.4114251136779785, 'validation/num_examples': 50000, 'test/accuracy': 0.35130003094673157, 'test/loss': 3.071357011795044, 'test/num_examples': 10000, 'score': 34491.62985539436, 'total_duration': 38136.859938144684, 'accumulated_submission_time': 34491.62985539436, 'accumulated_eval_time': 3638.625265598297, 'accumulated_logging_time': 2.7113420963287354}
I0202 22:22:49.186899 140023005427456 logging_writer.py:48] [75304] accumulated_eval_time=3638.625266, accumulated_logging_time=2.711342, accumulated_submission_time=34491.629855, global_step=75304, preemption_count=0, score=34491.629855, test/accuracy=0.351300, test/loss=3.071357, test/num_examples=10000, total_duration=38136.859938, train/accuracy=0.487715, train/loss=2.240133, validation/accuracy=0.455940, validation/loss=2.411425, validation/num_examples=50000
I0202 22:23:29.688231 140022518892288 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.1308867931365967, loss=3.7190537452697754
I0202 22:24:15.918011 140023005427456 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.3621811866760254, loss=3.610438346862793
I0202 22:25:02.301836 140022518892288 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.1794804334640503, loss=3.40592622756958
I0202 22:25:48.445666 140023005427456 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.0557749271392822, loss=4.07515811920166
I0202 22:26:34.949020 140022518892288 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.0721813440322876, loss=3.3831324577331543
I0202 22:27:21.022767 140023005427456 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.8495486974716187, loss=4.47503137588501
I0202 22:28:07.451631 140022518892288 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.9203602075576782, loss=5.2453508377075195
I0202 22:28:53.499101 140023005427456 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.9983795285224915, loss=4.401152610778809
I0202 22:29:39.658532 140022518892288 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.147024393081665, loss=3.2551140785217285
I0202 22:29:49.475589 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:30:00.107890 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:30:30.855748 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:30:32.467347 140184451094336 submission_runner.py:408] Time since start: 38600.17s, 	Step: 76223, 	{'train/accuracy': 0.4906249940395355, 'train/loss': 2.2219138145446777, 'validation/accuracy': 0.4575199782848358, 'validation/loss': 2.4190688133239746, 'validation/num_examples': 50000, 'test/accuracy': 0.3555000126361847, 'test/loss': 3.057042121887207, 'test/num_examples': 10000, 'score': 34911.86199808121, 'total_duration': 38600.17257928848, 'accumulated_submission_time': 34911.86199808121, 'accumulated_eval_time': 3681.6170043945312, 'accumulated_logging_time': 2.7527599334716797}
I0202 22:30:32.496139 140023005427456 logging_writer.py:48] [76223] accumulated_eval_time=3681.617004, accumulated_logging_time=2.752760, accumulated_submission_time=34911.861998, global_step=76223, preemption_count=0, score=34911.861998, test/accuracy=0.355500, test/loss=3.057042, test/num_examples=10000, total_duration=38600.172579, train/accuracy=0.490625, train/loss=2.221914, validation/accuracy=0.457520, validation/loss=2.419069, validation/num_examples=50000
I0202 22:31:04.316623 140022518892288 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.914365828037262, loss=5.3998918533325195
I0202 22:31:50.710216 140023005427456 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.8721705079078674, loss=5.783502578735352
I0202 22:32:36.955795 140022518892288 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.9736967086791992, loss=3.634094476699829
I0202 22:33:23.255619 140023005427456 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.9407541155815125, loss=5.677124500274658
I0202 22:34:09.425896 140022518892288 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.8182796239852905, loss=4.3875322341918945
I0202 22:34:55.582971 140023005427456 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2868300676345825, loss=3.3964426517486572
I0202 22:35:41.700518 140022518892288 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.0805139541625977, loss=3.693638801574707
I0202 22:36:27.978102 140023005427456 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.171425223350525, loss=3.3186848163604736
I0202 22:37:14.218214 140022518892288 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1217713356018066, loss=3.2115490436553955
I0202 22:37:32.697292 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:37:42.811255 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:38:16.870896 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:38:18.468565 140184451094336 submission_runner.py:408] Time since start: 39066.17s, 	Step: 77142, 	{'train/accuracy': 0.5056250095367432, 'train/loss': 2.1703872680664062, 'validation/accuracy': 0.4499799907207489, 'validation/loss': 2.44785213470459, 'validation/num_examples': 50000, 'test/accuracy': 0.3538000285625458, 'test/loss': 3.080111503601074, 'test/num_examples': 10000, 'score': 35332.00664615631, 'total_duration': 39066.17379283905, 'accumulated_submission_time': 35332.00664615631, 'accumulated_eval_time': 3727.3882570266724, 'accumulated_logging_time': 2.79075288772583}
I0202 22:38:18.503427 140023005427456 logging_writer.py:48] [77142] accumulated_eval_time=3727.388257, accumulated_logging_time=2.790753, accumulated_submission_time=35332.006646, global_step=77142, preemption_count=0, score=35332.006646, test/accuracy=0.353800, test/loss=3.080112, test/num_examples=10000, total_duration=39066.173793, train/accuracy=0.505625, train/loss=2.170387, validation/accuracy=0.449980, validation/loss=2.447852, validation/num_examples=50000
I0202 22:38:42.110425 140022518892288 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.6163270473480225, loss=3.4269211292266846
I0202 22:39:27.240600 140023005427456 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.1810661554336548, loss=3.43464994430542
I0202 22:40:13.812361 140022518892288 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.8339516520500183, loss=5.235152244567871
I0202 22:41:00.020622 140023005427456 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.202505111694336, loss=3.3512542247772217
I0202 22:41:46.465866 140022518892288 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.0838701725006104, loss=3.3519649505615234
I0202 22:42:32.716501 140023005427456 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.8021150827407837, loss=5.526727676391602
I0202 22:43:18.695933 140022518892288 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.0728651285171509, loss=5.608796119689941
I0202 22:44:05.166554 140023005427456 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.9811156392097473, loss=5.190229415893555
I0202 22:44:50.949295 140022518892288 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.8407089114189148, loss=5.793910026550293
I0202 22:45:18.766670 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:45:28.957644 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:46:01.230116 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:46:02.828089 140184451094336 submission_runner.py:408] Time since start: 39530.53s, 	Step: 78062, 	{'train/accuracy': 0.48580077290534973, 'train/loss': 2.2651078701019287, 'validation/accuracy': 0.45277997851371765, 'validation/loss': 2.438197374343872, 'validation/num_examples': 50000, 'test/accuracy': 0.3588000237941742, 'test/loss': 3.0841751098632812, 'test/num_examples': 10000, 'score': 35752.21084976196, 'total_duration': 39530.53333187103, 'accumulated_submission_time': 35752.21084976196, 'accumulated_eval_time': 3771.4496726989746, 'accumulated_logging_time': 2.83611798286438}
I0202 22:46:02.856730 140023005427456 logging_writer.py:48] [78062] accumulated_eval_time=3771.449673, accumulated_logging_time=2.836118, accumulated_submission_time=35752.210850, global_step=78062, preemption_count=0, score=35752.210850, test/accuracy=0.358800, test/loss=3.084175, test/num_examples=10000, total_duration=39530.533332, train/accuracy=0.485801, train/loss=2.265108, validation/accuracy=0.452780, validation/loss=2.438197, validation/num_examples=50000
I0202 22:46:18.465491 140022518892288 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.1835254430770874, loss=3.2154688835144043
I0202 22:47:02.333173 140023005427456 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.032690405845642, loss=3.2477447986602783
I0202 22:47:48.454293 140022518892288 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.9558953642845154, loss=4.330399036407471
I0202 22:48:34.774996 140023005427456 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.9192782640457153, loss=5.6378068923950195
I0202 22:49:20.935134 140022518892288 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.199816107749939, loss=3.465053081512451
I0202 22:50:07.123126 140023005427456 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.0155290365219116, loss=3.585348129272461
I0202 22:50:53.196063 140022518892288 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1281732320785522, loss=3.748448371887207
I0202 22:51:39.693134 140023005427456 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.1935720443725586, loss=3.3465561866760254
I0202 22:52:26.116505 140022518892288 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.2035409212112427, loss=3.541628122329712
I0202 22:53:03.185446 140184451094336 spec.py:321] Evaluating on the training split.
I0202 22:53:13.477720 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 22:53:46.744688 140184451094336 spec.py:349] Evaluating on the test split.
I0202 22:53:48.350206 140184451094336 submission_runner.py:408] Time since start: 39996.06s, 	Step: 78982, 	{'train/accuracy': 0.48689451813697815, 'train/loss': 2.2959415912628174, 'validation/accuracy': 0.4545799791812897, 'validation/loss': 2.4721052646636963, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.1361148357391357, 'test/num_examples': 10000, 'score': 36172.48115777969, 'total_duration': 39996.05541801453, 'accumulated_submission_time': 36172.48115777969, 'accumulated_eval_time': 3816.6144077777863, 'accumulated_logging_time': 2.875884532928467}
I0202 22:53:48.384924 140023005427456 logging_writer.py:48] [78982] accumulated_eval_time=3816.614408, accumulated_logging_time=2.875885, accumulated_submission_time=36172.481158, global_step=78982, preemption_count=0, score=36172.481158, test/accuracy=0.346200, test/loss=3.136115, test/num_examples=10000, total_duration=39996.055418, train/accuracy=0.486895, train/loss=2.295942, validation/accuracy=0.454580, validation/loss=2.472105, validation/num_examples=50000
I0202 22:53:55.978581 140022518892288 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.1299666166305542, loss=3.24055814743042
I0202 22:54:38.860273 140023005427456 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.0343648195266724, loss=3.411731243133545
I0202 22:55:25.013198 140022518892288 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.0455501079559326, loss=3.442081928253174
I0202 22:56:11.326588 140023005427456 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.092575192451477, loss=3.2709224224090576
I0202 22:56:57.558964 140022518892288 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.0435148477554321, loss=4.491765975952148
I0202 22:57:43.746963 140023005427456 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.048525333404541, loss=4.455591678619385
I0202 22:58:29.650374 140022518892288 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.0722949504852295, loss=3.2543067932128906
I0202 22:59:15.710487 140023005427456 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.0818616151809692, loss=3.3992671966552734
I0202 23:00:01.899471 140022518892288 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.7873097658157349, loss=5.610642910003662
I0202 23:00:48.176377 140023005427456 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.0645174980163574, loss=3.4592323303222656
I0202 23:00:48.776883 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:00:59.365437 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:01:35.849987 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:01:37.452357 140184451094336 submission_runner.py:408] Time since start: 40465.16s, 	Step: 79903, 	{'train/accuracy': 0.4998827874660492, 'train/loss': 2.2071332931518555, 'validation/accuracy': 0.4567599892616272, 'validation/loss': 2.4251766204833984, 'validation/num_examples': 50000, 'test/accuracy': 0.3619000315666199, 'test/loss': 3.0350892543792725, 'test/num_examples': 10000, 'score': 36592.814427137375, 'total_duration': 40465.15758180618, 'accumulated_submission_time': 36592.814427137375, 'accumulated_eval_time': 3865.289860725403, 'accumulated_logging_time': 2.9212820529937744}
I0202 23:01:37.484333 140022518892288 logging_writer.py:48] [79903] accumulated_eval_time=3865.289861, accumulated_logging_time=2.921282, accumulated_submission_time=36592.814427, global_step=79903, preemption_count=0, score=36592.814427, test/accuracy=0.361900, test/loss=3.035089, test/num_examples=10000, total_duration=40465.157582, train/accuracy=0.499883, train/loss=2.207133, validation/accuracy=0.456760, validation/loss=2.425177, validation/num_examples=50000
I0202 23:02:18.346240 140023005427456 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.9526941776275635, loss=5.6178998947143555
I0202 23:03:04.811020 140022518892288 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.0507934093475342, loss=3.14106822013855
I0202 23:03:50.932018 140023005427456 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.8717989921569824, loss=5.680934906005859
I0202 23:04:36.881224 140022518892288 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.2043111324310303, loss=3.4207990169525146
I0202 23:05:23.271851 140023005427456 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.1814137697219849, loss=3.2817039489746094
I0202 23:06:09.581631 140022518892288 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.362534999847412, loss=3.250499963760376
I0202 23:06:55.492374 140023005427456 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.8907042145729065, loss=4.675834655761719
I0202 23:07:41.531979 140022518892288 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.0685111284255981, loss=3.495547294616699
I0202 23:08:28.035424 140023005427456 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.3762123584747314, loss=3.384232521057129
I0202 23:08:37.914050 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:08:48.512115 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:09:17.303514 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:09:18.923588 140184451094336 submission_runner.py:408] Time since start: 40926.63s, 	Step: 80823, 	{'train/accuracy': 0.4993554651737213, 'train/loss': 2.1888513565063477, 'validation/accuracy': 0.471560001373291, 'validation/loss': 2.3428831100463867, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 2.9896857738494873, 'test/num_examples': 10000, 'score': 37013.1857984066, 'total_duration': 40926.628831624985, 'accumulated_submission_time': 37013.1857984066, 'accumulated_eval_time': 3906.299390554428, 'accumulated_logging_time': 2.9639267921447754}
I0202 23:09:18.951933 140022518892288 logging_writer.py:48] [80823] accumulated_eval_time=3906.299391, accumulated_logging_time=2.963927, accumulated_submission_time=37013.185798, global_step=80823, preemption_count=0, score=37013.185798, test/accuracy=0.367900, test/loss=2.989686, test/num_examples=10000, total_duration=40926.628832, train/accuracy=0.499355, train/loss=2.188851, validation/accuracy=0.471560, validation/loss=2.342883, validation/num_examples=50000
I0202 23:09:50.610985 140023005427456 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.1321974992752075, loss=3.315659523010254
I0202 23:10:36.664523 140022518892288 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.0501612424850464, loss=3.463137149810791
I0202 23:11:23.282568 140023005427456 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.2180819511413574, loss=3.3126566410064697
I0202 23:12:09.992751 140022518892288 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.2318578958511353, loss=3.2479248046875
I0202 23:12:56.384692 140023005427456 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.484031081199646, loss=3.285571575164795
I0202 23:13:42.914448 140022518892288 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.9792696833610535, loss=4.290679931640625
I0202 23:14:29.322449 140023005427456 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.0985726118087769, loss=3.4685933589935303
I0202 23:15:15.842883 140022518892288 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.0915831327438354, loss=3.1921002864837646
I0202 23:16:02.080887 140023005427456 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.0995630025863647, loss=3.5032742023468018
I0202 23:16:19.316628 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:16:29.648649 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:17:02.323588 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:17:03.921348 140184451094336 submission_runner.py:408] Time since start: 41391.63s, 	Step: 81739, 	{'train/accuracy': 0.5030664205551147, 'train/loss': 2.173933744430542, 'validation/accuracy': 0.47307997941970825, 'validation/loss': 2.3356215953826904, 'validation/num_examples': 50000, 'test/accuracy': 0.36880001425743103, 'test/loss': 2.9926578998565674, 'test/num_examples': 10000, 'score': 37433.493015527725, 'total_duration': 41391.62658786774, 'accumulated_submission_time': 37433.493015527725, 'accumulated_eval_time': 3950.904098033905, 'accumulated_logging_time': 3.0028018951416016}
I0202 23:17:03.948820 140022518892288 logging_writer.py:48] [81739] accumulated_eval_time=3950.904098, accumulated_logging_time=3.002802, accumulated_submission_time=37433.493016, global_step=81739, preemption_count=0, score=37433.493016, test/accuracy=0.368800, test/loss=2.992658, test/num_examples=10000, total_duration=41391.626588, train/accuracy=0.503066, train/loss=2.173934, validation/accuracy=0.473080, validation/loss=2.335622, validation/num_examples=50000
I0202 23:17:28.752502 140023005427456 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.8932514786720276, loss=5.226981163024902
I0202 23:18:14.033400 140022518892288 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.1075208187103271, loss=3.195779800415039
I0202 23:19:00.529205 140023005427456 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.0654652118682861, loss=3.27608060836792
I0202 23:19:46.786333 140022518892288 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.3205052614212036, loss=3.3401296138763428
I0202 23:20:32.885128 140023005427456 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.1704468727111816, loss=3.6618528366088867
I0202 23:21:19.120518 140022518892288 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3505593538284302, loss=3.2312211990356445
I0202 23:22:05.601488 140023005427456 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.1832877397537231, loss=3.3078529834747314
I0202 23:22:51.829015 140022518892288 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.0129047632217407, loss=3.253873586654663
I0202 23:23:38.419066 140023005427456 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.1891342401504517, loss=3.190722703933716
I0202 23:24:04.071583 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:24:14.269721 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:24:49.197276 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:24:50.802094 140184451094336 submission_runner.py:408] Time since start: 41858.51s, 	Step: 82657, 	{'train/accuracy': 0.5142968893051147, 'train/loss': 2.146437168121338, 'validation/accuracy': 0.47237998247146606, 'validation/loss': 2.358328104019165, 'validation/num_examples': 50000, 'test/accuracy': 0.3644000291824341, 'test/loss': 3.0233078002929688, 'test/num_examples': 10000, 'score': 37853.55633306503, 'total_duration': 41858.507304906845, 'accumulated_submission_time': 37853.55633306503, 'accumulated_eval_time': 3997.634565114975, 'accumulated_logging_time': 3.0432190895080566}
I0202 23:24:50.836745 140022518892288 logging_writer.py:48] [82657] accumulated_eval_time=3997.634565, accumulated_logging_time=3.043219, accumulated_submission_time=37853.556333, global_step=82657, preemption_count=0, score=37853.556333, test/accuracy=0.364400, test/loss=3.023308, test/num_examples=10000, total_duration=41858.507305, train/accuracy=0.514297, train/loss=2.146437, validation/accuracy=0.472380, validation/loss=2.358328, validation/num_examples=50000
I0202 23:25:08.448679 140023005427456 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.1267616748809814, loss=3.167536735534668
I0202 23:25:52.506533 140022518892288 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.003294587135315, loss=4.614762783050537
I0202 23:26:38.772848 140023005427456 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.041800618171692, loss=3.305246591567993
I0202 23:27:25.308101 140022518892288 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.08247971534729, loss=3.586024761199951
I0202 23:28:11.409026 140023005427456 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.140766978263855, loss=3.2524867057800293
I0202 23:28:57.528723 140022518892288 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.096002459526062, loss=3.193013906478882
I0202 23:29:43.608966 140023005427456 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.9898691177368164, loss=3.1541082859039307
I0202 23:30:29.787418 140022518892288 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.9629014730453491, loss=4.189830303192139
I0202 23:31:15.803933 140023005427456 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.0829976797103882, loss=3.1242475509643555
I0202 23:31:51.046765 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:32:01.647840 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:32:26.682462 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:32:28.287995 140184451094336 submission_runner.py:408] Time since start: 42315.99s, 	Step: 83577, 	{'train/accuracy': 0.5132030844688416, 'train/loss': 2.140690803527832, 'validation/accuracy': 0.47481998801231384, 'validation/loss': 2.3312788009643555, 'validation/num_examples': 50000, 'test/accuracy': 0.3646000027656555, 'test/loss': 2.994065284729004, 'test/num_examples': 10000, 'score': 38273.708990097046, 'total_duration': 42315.99324274063, 'accumulated_submission_time': 38273.708990097046, 'accumulated_eval_time': 4034.875780582428, 'accumulated_logging_time': 3.0878307819366455}
I0202 23:32:28.318366 140022518892288 logging_writer.py:48] [83577] accumulated_eval_time=4034.875781, accumulated_logging_time=3.087831, accumulated_submission_time=38273.708990, global_step=83577, preemption_count=0, score=38273.708990, test/accuracy=0.364600, test/loss=2.994065, test/num_examples=10000, total_duration=42315.993243, train/accuracy=0.513203, train/loss=2.140691, validation/accuracy=0.474820, validation/loss=2.331279, validation/num_examples=50000
I0202 23:32:37.937057 140023005427456 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.2491799592971802, loss=3.5940287113189697
I0202 23:33:21.065620 140022518892288 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.1516718864440918, loss=3.0485329627990723
I0202 23:34:07.430986 140023005427456 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.2566465139389038, loss=3.2864670753479004
I0202 23:34:53.855439 140022518892288 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.0506728887557983, loss=3.7092483043670654
I0202 23:35:39.808642 140023005427456 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.4581371545791626, loss=3.3276283740997314
I0202 23:36:25.989258 140022518892288 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.9877415895462036, loss=5.434919834136963
I0202 23:37:12.259028 140023005427456 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.0127885341644287, loss=3.3318824768066406
I0202 23:37:58.337947 140022518892288 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.8338623046875, loss=5.630987167358398
I0202 23:38:44.556272 140023005427456 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.1197718381881714, loss=3.7897987365722656
I0202 23:39:28.568776 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:39:39.093438 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:40:12.224100 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:40:13.821259 140184451094336 submission_runner.py:408] Time since start: 42781.53s, 	Step: 84497, 	{'train/accuracy': 0.5048632621765137, 'train/loss': 2.2373056411743164, 'validation/accuracy': 0.46403998136520386, 'validation/loss': 2.4101569652557373, 'validation/num_examples': 50000, 'test/accuracy': 0.3582000136375427, 'test/loss': 3.0756309032440186, 'test/num_examples': 10000, 'score': 38693.901599407196, 'total_duration': 42781.52650523186, 'accumulated_submission_time': 38693.901599407196, 'accumulated_eval_time': 4080.1282589435577, 'accumulated_logging_time': 3.1285057067871094}
I0202 23:40:13.853116 140022518892288 logging_writer.py:48] [84497] accumulated_eval_time=4080.128259, accumulated_logging_time=3.128506, accumulated_submission_time=38693.901599, global_step=84497, preemption_count=0, score=38693.901599, test/accuracy=0.358200, test/loss=3.075631, test/num_examples=10000, total_duration=42781.526505, train/accuracy=0.504863, train/loss=2.237306, validation/accuracy=0.464040, validation/loss=2.410157, validation/num_examples=50000
I0202 23:40:15.458540 140023005427456 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.0985217094421387, loss=5.409036159515381
I0202 23:40:57.342960 140022518892288 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.9400115609169006, loss=5.618638515472412
I0202 23:41:43.522970 140023005427456 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.9603624939918518, loss=4.172325134277344
I0202 23:42:29.650363 140022518892288 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.1419984102249146, loss=3.735093355178833
I0202 23:43:15.832485 140023005427456 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.9054243564605713, loss=4.108854293823242
I0202 23:44:02.128473 140022518892288 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.2766169309616089, loss=3.1726698875427246
I0202 23:44:48.499958 140023005427456 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.0380815267562866, loss=5.745753288269043
I0202 23:45:34.575644 140022518892288 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.021125316619873, loss=3.2922415733337402
I0202 23:46:20.901181 140023005427456 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.102407455444336, loss=3.7282874584198
I0202 23:47:07.055393 140022518892288 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.1024504899978638, loss=3.3066465854644775
I0202 23:47:14.086295 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:47:24.329192 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:47:55.022871 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:47:56.626659 140184451094336 submission_runner.py:408] Time since start: 43244.33s, 	Step: 85417, 	{'train/accuracy': 0.5148242115974426, 'train/loss': 2.1396749019622803, 'validation/accuracy': 0.4755999743938446, 'validation/loss': 2.3394620418548584, 'validation/num_examples': 50000, 'test/accuracy': 0.3726000189781189, 'test/loss': 2.975393772125244, 'test/num_examples': 10000, 'score': 39114.07578897476, 'total_duration': 43244.331899404526, 'accumulated_submission_time': 39114.07578897476, 'accumulated_eval_time': 4122.668626070023, 'accumulated_logging_time': 3.1721973419189453}
I0202 23:47:56.659741 140023005427456 logging_writer.py:48] [85417] accumulated_eval_time=4122.668626, accumulated_logging_time=3.172197, accumulated_submission_time=39114.075789, global_step=85417, preemption_count=0, score=39114.075789, test/accuracy=0.372600, test/loss=2.975394, test/num_examples=10000, total_duration=43244.331899, train/accuracy=0.514824, train/loss=2.139675, validation/accuracy=0.475600, validation/loss=2.339462, validation/num_examples=50000
I0202 23:48:30.894162 140022518892288 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.3260040283203125, loss=3.233429431915283
I0202 23:49:16.927289 140023005427456 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.9453760385513306, loss=4.353706359863281
I0202 23:50:03.322644 140022518892288 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.102562665939331, loss=3.1676342487335205
I0202 23:50:49.524655 140023005427456 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.1098778247833252, loss=3.7116196155548096
I0202 23:51:35.827824 140022518892288 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2147423028945923, loss=3.2186264991760254
I0202 23:52:22.071645 140023005427456 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.256367564201355, loss=3.1970760822296143
I0202 23:53:08.335077 140022518892288 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.9652388691902161, loss=3.3822813034057617
I0202 23:53:54.277071 140023005427456 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.072313666343689, loss=3.704324245452881
I0202 23:54:40.522486 140022518892288 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.0406193733215332, loss=3.30712628364563
I0202 23:54:57.079977 140184451094336 spec.py:321] Evaluating on the training split.
I0202 23:55:08.512249 140184451094336 spec.py:333] Evaluating on the validation split.
I0202 23:55:42.686141 140184451094336 spec.py:349] Evaluating on the test split.
I0202 23:55:44.293331 140184451094336 submission_runner.py:408] Time since start: 43712.00s, 	Step: 86337, 	{'train/accuracy': 0.5555468797683716, 'train/loss': 1.9123483896255493, 'validation/accuracy': 0.4817200005054474, 'validation/loss': 2.272268295288086, 'validation/num_examples': 50000, 'test/accuracy': 0.37790000438690186, 'test/loss': 2.9314382076263428, 'test/num_examples': 10000, 'score': 39534.43977665901, 'total_duration': 43711.998577833176, 'accumulated_submission_time': 39534.43977665901, 'accumulated_eval_time': 4169.881982803345, 'accumulated_logging_time': 3.214378833770752}
I0202 23:55:44.323181 140023005427456 logging_writer.py:48] [86337] accumulated_eval_time=4169.881983, accumulated_logging_time=3.214379, accumulated_submission_time=39534.439777, global_step=86337, preemption_count=0, score=39534.439777, test/accuracy=0.377900, test/loss=2.931438, test/num_examples=10000, total_duration=43711.998578, train/accuracy=0.555547, train/loss=1.912348, validation/accuracy=0.481720, validation/loss=2.272268, validation/num_examples=50000
I0202 23:56:09.950484 140022518892288 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.8732949495315552, loss=5.266801834106445
I0202 23:56:55.155523 140023005427456 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.9026181697845459, loss=4.5013017654418945
I0202 23:57:41.430206 140022518892288 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2975608110427856, loss=3.132581949234009
I0202 23:58:27.747174 140023005427456 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.0670206546783447, loss=3.1751606464385986
I0202 23:59:13.968693 140022518892288 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.0050851106643677, loss=3.326630115509033
I0203 00:00:00.251632 140023005427456 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.007569670677185, loss=3.5902020931243896
I0203 00:00:46.520923 140022518892288 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.9531928300857544, loss=5.306613445281982
I0203 00:01:32.747445 140023005427456 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.2589032649993896, loss=3.416445732116699
I0203 00:02:19.092193 140022518892288 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.9892473816871643, loss=3.4199635982513428
I0203 00:02:44.592229 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:02:55.014218 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:03:26.272590 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:03:27.869179 140184451094336 submission_runner.py:408] Time since start: 44175.57s, 	Step: 87257, 	{'train/accuracy': 0.52099609375, 'train/loss': 2.0965120792388916, 'validation/accuracy': 0.48593997955322266, 'validation/loss': 2.274020195007324, 'validation/num_examples': 50000, 'test/accuracy': 0.3801000118255615, 'test/loss': 2.9268369674682617, 'test/num_examples': 10000, 'score': 39954.651678562164, 'total_duration': 44175.574429512024, 'accumulated_submission_time': 39954.651678562164, 'accumulated_eval_time': 4213.158932924271, 'accumulated_logging_time': 3.2534549236297607}
I0203 00:03:27.900964 140023005427456 logging_writer.py:48] [87257] accumulated_eval_time=4213.158933, accumulated_logging_time=3.253455, accumulated_submission_time=39954.651679, global_step=87257, preemption_count=0, score=39954.651679, test/accuracy=0.380100, test/loss=2.926837, test/num_examples=10000, total_duration=44175.574430, train/accuracy=0.520996, train/loss=2.096512, validation/accuracy=0.485940, validation/loss=2.274020, validation/num_examples=50000
I0203 00:03:45.506272 140022518892288 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.8608003258705139, loss=5.37797212600708
I0203 00:04:30.041895 140023005427456 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.0858880281448364, loss=3.935065507888794
I0203 00:05:16.240978 140022518892288 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.8827306032180786, loss=4.5551605224609375
I0203 00:06:02.896925 140023005427456 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.2758545875549316, loss=3.1615872383117676
I0203 00:06:49.047595 140022518892288 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2023266553878784, loss=3.1997475624084473
I0203 00:07:35.334602 140023005427456 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.1592589616775513, loss=3.187368392944336
I0203 00:08:21.742467 140022518892288 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.0937764644622803, loss=3.160466432571411
I0203 00:09:08.070227 140023005427456 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.210424542427063, loss=3.227489948272705
I0203 00:09:54.253728 140022518892288 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.0017313957214355, loss=3.2988815307617188
I0203 00:10:28.255214 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:10:38.733155 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:11:14.325010 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:11:15.934704 140184451094336 submission_runner.py:408] Time since start: 44643.64s, 	Step: 88175, 	{'train/accuracy': 0.5269726514816284, 'train/loss': 2.0405514240264893, 'validation/accuracy': 0.48993998765945435, 'validation/loss': 2.239298105239868, 'validation/num_examples': 50000, 'test/accuracy': 0.37540000677108765, 'test/loss': 2.916119337081909, 'test/num_examples': 10000, 'score': 40374.94868397713, 'total_duration': 44643.63994884491, 'accumulated_submission_time': 40374.94868397713, 'accumulated_eval_time': 4260.838416099548, 'accumulated_logging_time': 3.2945399284362793}
I0203 00:11:15.964192 140023005427456 logging_writer.py:48] [88175] accumulated_eval_time=4260.838416, accumulated_logging_time=3.294540, accumulated_submission_time=40374.948684, global_step=88175, preemption_count=0, score=40374.948684, test/accuracy=0.375400, test/loss=2.916119, test/num_examples=10000, total_duration=44643.639949, train/accuracy=0.526973, train/loss=2.040551, validation/accuracy=0.489940, validation/loss=2.239298, validation/num_examples=50000
I0203 00:11:26.365584 140022518892288 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.259590983390808, loss=3.1912028789520264
I0203 00:12:09.788480 140023005427456 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.8565276861190796, loss=5.1212310791015625
I0203 00:12:56.055496 140022518892288 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.0501408576965332, loss=5.6594414710998535
I0203 00:13:42.380108 140023005427456 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.2708640098571777, loss=3.253829002380371
I0203 00:14:28.525752 140022518892288 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.2051897048950195, loss=3.1755752563476562
I0203 00:15:14.589626 140023005427456 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.9707009792327881, loss=4.2233686447143555
I0203 00:16:00.646260 140022518892288 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.2698686122894287, loss=3.086756944656372
I0203 00:16:46.994404 140023005427456 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.080642580986023, loss=3.092489004135132
I0203 00:17:33.127855 140022518892288 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.055970549583435, loss=3.559174060821533
I0203 00:18:16.156445 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:18:26.459354 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:18:59.635981 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:19:01.252644 140184451094336 submission_runner.py:408] Time since start: 45108.96s, 	Step: 89095, 	{'train/accuracy': 0.5325976610183716, 'train/loss': 2.0417697429656982, 'validation/accuracy': 0.4813999831676483, 'validation/loss': 2.3064215183258057, 'validation/num_examples': 50000, 'test/accuracy': 0.38030001521110535, 'test/loss': 2.940920829772949, 'test/num_examples': 10000, 'score': 40795.083706617355, 'total_duration': 45108.95787191391, 'accumulated_submission_time': 40795.083706617355, 'accumulated_eval_time': 4305.934587717056, 'accumulated_logging_time': 3.3338735103607178}
I0203 00:19:01.284334 140023005427456 logging_writer.py:48] [89095] accumulated_eval_time=4305.934588, accumulated_logging_time=3.333874, accumulated_submission_time=40795.083707, global_step=89095, preemption_count=0, score=40795.083707, test/accuracy=0.380300, test/loss=2.940921, test/num_examples=10000, total_duration=45108.957872, train/accuracy=0.532598, train/loss=2.041770, validation/accuracy=0.481400, validation/loss=2.306422, validation/num_examples=50000
I0203 00:19:03.682716 140022518892288 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.260257601737976, loss=3.1387600898742676
I0203 00:19:45.731131 140023005427456 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.2498573064804077, loss=3.2899346351623535
I0203 00:20:31.947937 140022518892288 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.8977741599082947, loss=4.4798078536987305
I0203 00:21:18.477994 140023005427456 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.038835883140564, loss=5.408536434173584
I0203 00:22:04.703331 140022518892288 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.8424819111824036, loss=4.7372331619262695
I0203 00:22:50.843216 140023005427456 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.9554996490478516, loss=3.768054962158203
I0203 00:23:37.155297 140022518892288 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.9108872413635254, loss=5.043489456176758
I0203 00:24:23.298307 140023005427456 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.3276770114898682, loss=3.300448417663574
I0203 00:25:09.550632 140022518892288 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.9993278384208679, loss=5.551568508148193
I0203 00:25:55.931433 140023005427456 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.0247305631637573, loss=3.9017653465270996
I0203 00:26:01.639698 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:26:12.095455 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:26:45.649076 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:26:47.243784 140184451094336 submission_runner.py:408] Time since start: 45574.95s, 	Step: 90014, 	{'train/accuracy': 0.5163866877555847, 'train/loss': 2.1249258518218994, 'validation/accuracy': 0.4807799756526947, 'validation/loss': 2.301429033279419, 'validation/num_examples': 50000, 'test/accuracy': 0.3824000060558319, 'test/loss': 2.9366328716278076, 'test/num_examples': 10000, 'score': 41215.381110191345, 'total_duration': 45574.949031591415, 'accumulated_submission_time': 41215.381110191345, 'accumulated_eval_time': 4351.538655757904, 'accumulated_logging_time': 3.3755288124084473}
I0203 00:26:47.276242 140022518892288 logging_writer.py:48] [90014] accumulated_eval_time=4351.538656, accumulated_logging_time=3.375529, accumulated_submission_time=41215.381110, global_step=90014, preemption_count=0, score=41215.381110, test/accuracy=0.382400, test/loss=2.936633, test/num_examples=10000, total_duration=45574.949032, train/accuracy=0.516387, train/loss=2.124926, validation/accuracy=0.480780, validation/loss=2.301429, validation/num_examples=50000
I0203 00:27:23.331279 140023005427456 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.1163151264190674, loss=3.782830238342285
I0203 00:28:09.391816 140022518892288 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.0951827764511108, loss=4.025664806365967
I0203 00:28:55.494288 140023005427456 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.9329357743263245, loss=4.297501087188721
I0203 00:29:41.770168 140022518892288 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.9043898582458496, loss=4.602558135986328
I0203 00:30:27.977780 140023005427456 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.3721727132797241, loss=3.1380434036254883
I0203 00:31:14.506988 140022518892288 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.8337118029594421, loss=5.085699558258057
I0203 00:32:00.919255 140023005427456 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.1125857830047607, loss=3.0584661960601807
I0203 00:32:46.979382 140022518892288 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.0231397151947021, loss=4.920920372009277
I0203 00:33:33.235476 140023005427456 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.9288336038589478, loss=5.112806797027588
I0203 00:33:47.697544 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:33:58.031470 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:34:30.922482 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:34:32.527391 140184451094336 submission_runner.py:408] Time since start: 46040.23s, 	Step: 90933, 	{'train/accuracy': 0.5344530940055847, 'train/loss': 2.001258134841919, 'validation/accuracy': 0.49903997778892517, 'validation/loss': 2.1946213245391846, 'validation/num_examples': 50000, 'test/accuracy': 0.3911000192165375, 'test/loss': 2.831852436065674, 'test/num_examples': 10000, 'score': 41635.74610328674, 'total_duration': 46040.23263311386, 'accumulated_submission_time': 41635.74610328674, 'accumulated_eval_time': 4396.368488788605, 'accumulated_logging_time': 3.417140483856201}
I0203 00:34:32.558232 140022518892288 logging_writer.py:48] [90933] accumulated_eval_time=4396.368489, accumulated_logging_time=3.417140, accumulated_submission_time=41635.746103, global_step=90933, preemption_count=0, score=41635.746103, test/accuracy=0.391100, test/loss=2.831852, test/num_examples=10000, total_duration=46040.232633, train/accuracy=0.534453, train/loss=2.001258, validation/accuracy=0.499040, validation/loss=2.194621, validation/num_examples=50000
I0203 00:34:59.774653 140023005427456 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.0726662874221802, loss=3.1407625675201416
I0203 00:35:45.779481 140022518892288 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.1164875030517578, loss=3.765733003616333
I0203 00:36:32.153892 140023005427456 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.201668620109558, loss=3.0629446506500244
I0203 00:37:18.462550 140022518892288 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.0008454322814941, loss=3.6421761512756348
I0203 00:38:04.884971 140023005427456 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.2426494359970093, loss=3.077366352081299
I0203 00:38:50.859303 140022518892288 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.9778459072113037, loss=5.517280101776123
I0203 00:39:36.997009 140023005427456 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.2641938924789429, loss=3.0968105792999268
I0203 00:40:23.342693 140022518892288 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.0561214685440063, loss=3.144291400909424
I0203 00:41:09.580168 140023005427456 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.2247885465621948, loss=3.276902675628662
I0203 00:41:32.872629 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:41:43.489586 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:42:19.234170 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:42:20.844943 140184451094336 submission_runner.py:408] Time since start: 46508.55s, 	Step: 91852, 	{'train/accuracy': 0.5383593440055847, 'train/loss': 2.0077288150787354, 'validation/accuracy': 0.49285998940467834, 'validation/loss': 2.2256059646606445, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 2.87404203414917, 'test/num_examples': 10000, 'score': 42056.00476717949, 'total_duration': 46508.55019235611, 'accumulated_submission_time': 42056.00476717949, 'accumulated_eval_time': 4444.340796709061, 'accumulated_logging_time': 3.457115888595581}
I0203 00:42:20.874420 140022518892288 logging_writer.py:48] [91852] accumulated_eval_time=4444.340797, accumulated_logging_time=3.457116, accumulated_submission_time=42056.004767, global_step=91852, preemption_count=0, score=42056.004767, test/accuracy=0.388700, test/loss=2.874042, test/num_examples=10000, total_duration=46508.550192, train/accuracy=0.538359, train/loss=2.007729, validation/accuracy=0.492860, validation/loss=2.225606, validation/num_examples=50000
I0203 00:42:40.481303 140023005427456 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.1179579496383667, loss=2.9986343383789062
I0203 00:43:25.272154 140022518892288 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.3612744808197021, loss=3.2340946197509766
I0203 00:44:11.153324 140023005427456 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.139405608177185, loss=5.386369705200195
I0203 00:44:57.139956 140022518892288 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.10883629322052, loss=3.4297091960906982
I0203 00:45:43.393371 140023005427456 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.8795360326766968, loss=5.439456462860107
I0203 00:46:29.721719 140022518892288 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.8753765225410461, loss=4.2606520652771
I0203 00:47:15.935799 140023005427456 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.3784788846969604, loss=3.145223617553711
I0203 00:48:02.286367 140022518892288 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.0341308116912842, loss=3.5357208251953125
I0203 00:48:49.786593 140023005427456 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.2691218852996826, loss=3.1533000469207764
I0203 00:49:21.042810 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:49:31.572316 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:50:08.135152 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:50:09.736216 140184451094336 submission_runner.py:408] Time since start: 46977.44s, 	Step: 92769, 	{'train/accuracy': 0.5292577743530273, 'train/loss': 2.0604593753814697, 'validation/accuracy': 0.4968799948692322, 'validation/loss': 2.218428134918213, 'validation/num_examples': 50000, 'test/accuracy': 0.39410001039505005, 'test/loss': 2.8445026874542236, 'test/num_examples': 10000, 'score': 42476.11767077446, 'total_duration': 46977.44144821167, 'accumulated_submission_time': 42476.11767077446, 'accumulated_eval_time': 4493.034183979034, 'accumulated_logging_time': 3.495344877243042}
I0203 00:50:09.767982 140022518892288 logging_writer.py:48] [92769] accumulated_eval_time=4493.034184, accumulated_logging_time=3.495345, accumulated_submission_time=42476.117671, global_step=92769, preemption_count=0, score=42476.117671, test/accuracy=0.394100, test/loss=2.844503, test/num_examples=10000, total_duration=46977.441448, train/accuracy=0.529258, train/loss=2.060459, validation/accuracy=0.496880, validation/loss=2.218428, validation/num_examples=50000
I0203 00:50:22.573964 140023005427456 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.2210066318511963, loss=3.1355159282684326
I0203 00:51:05.977946 140022518892288 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.0023692846298218, loss=5.642366886138916
I0203 00:51:52.172168 140023005427456 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.1179980039596558, loss=3.2922651767730713
I0203 00:52:38.372492 140022518892288 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.0611064434051514, loss=4.542064189910889
I0203 00:53:24.280660 140023005427456 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.1554419994354248, loss=3.100285053253174
I0203 00:54:10.372532 140022518892288 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.9879735708236694, loss=4.943434715270996
I0203 00:54:56.345825 140023005427456 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.1462429761886597, loss=3.2019717693328857
I0203 00:55:42.632099 140022518892288 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.1162505149841309, loss=3.192728281021118
I0203 00:56:28.828037 140023005427456 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.9978696703910828, loss=4.094130039215088
I0203 00:57:09.825570 140184451094336 spec.py:321] Evaluating on the training split.
I0203 00:57:20.367784 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 00:57:54.626902 140184451094336 spec.py:349] Evaluating on the test split.
I0203 00:57:56.222632 140184451094336 submission_runner.py:408] Time since start: 47443.93s, 	Step: 93690, 	{'train/accuracy': 0.5306054353713989, 'train/loss': 2.0200607776641846, 'validation/accuracy': 0.497979998588562, 'validation/loss': 2.209547281265259, 'validation/num_examples': 50000, 'test/accuracy': 0.39660000801086426, 'test/loss': 2.847208023071289, 'test/num_examples': 10000, 'score': 42896.114788770676, 'total_duration': 47443.92788076401, 'accumulated_submission_time': 42896.114788770676, 'accumulated_eval_time': 4539.431235074997, 'accumulated_logging_time': 3.540785312652588}
I0203 00:57:56.257076 140022518892288 logging_writer.py:48] [93690] accumulated_eval_time=4539.431235, accumulated_logging_time=3.540785, accumulated_submission_time=42896.114789, global_step=93690, preemption_count=0, score=42896.114789, test/accuracy=0.396600, test/loss=2.847208, test/num_examples=10000, total_duration=47443.927881, train/accuracy=0.530605, train/loss=2.020061, validation/accuracy=0.497980, validation/loss=2.209547, validation/num_examples=50000
I0203 00:58:00.661497 140023005427456 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.1622554063796997, loss=3.188182830810547
I0203 00:58:43.216203 140022518892288 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.930109441280365, loss=5.617238521575928
I0203 00:59:29.340375 140023005427456 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.0750999450683594, loss=4.329182147979736
I0203 01:00:15.619071 140022518892288 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.929201066493988, loss=3.4148435592651367
I0203 01:01:01.877089 140023005427456 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.1773866415023804, loss=3.0253078937530518
I0203 01:01:48.356959 140022518892288 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.2476226091384888, loss=3.276553153991699
I0203 01:02:34.740114 140023005427456 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.9698559641838074, loss=5.0912370681762695
I0203 01:03:20.859793 140022518892288 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.269783854484558, loss=2.9752707481384277
I0203 01:04:07.111100 140023005427456 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.1397111415863037, loss=3.332294464111328
I0203 01:04:53.191295 140022518892288 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.2580065727233887, loss=3.0644712448120117
I0203 01:04:56.562495 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:05:07.154226 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:05:42.893894 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:05:44.496245 140184451094336 submission_runner.py:408] Time since start: 47912.20s, 	Step: 94609, 	{'train/accuracy': 0.5318945050239563, 'train/loss': 2.0958914756774902, 'validation/accuracy': 0.49309998750686646, 'validation/loss': 2.292987108230591, 'validation/num_examples': 50000, 'test/accuracy': 0.38920003175735474, 'test/loss': 2.920865297317505, 'test/num_examples': 10000, 'score': 43316.36288237572, 'total_duration': 47912.20147418976, 'accumulated_submission_time': 43316.36288237572, 'accumulated_eval_time': 4587.36496925354, 'accumulated_logging_time': 3.5851848125457764}
I0203 01:05:44.526164 140023005427456 logging_writer.py:48] [94609] accumulated_eval_time=4587.364969, accumulated_logging_time=3.585185, accumulated_submission_time=43316.362882, global_step=94609, preemption_count=0, score=43316.362882, test/accuracy=0.389200, test/loss=2.920865, test/num_examples=10000, total_duration=47912.201474, train/accuracy=0.531895, train/loss=2.095891, validation/accuracy=0.493100, validation/loss=2.292987, validation/num_examples=50000
I0203 01:06:22.913682 140022518892288 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.1255505084991455, loss=3.4714603424072266
I0203 01:07:09.023042 140023005427456 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.1447679996490479, loss=5.216811656951904
I0203 01:07:55.259492 140022518892288 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.1783231496810913, loss=3.0971102714538574
I0203 01:08:41.535753 140023005427456 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.2615121603012085, loss=2.9978439807891846
I0203 01:09:27.924579 140022518892288 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.2006943225860596, loss=2.9693148136138916
I0203 01:10:14.259504 140023005427456 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.0448757410049438, loss=2.9527015686035156
I0203 01:11:00.496180 140022518892288 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.0090364217758179, loss=3.916530132293701
I0203 01:11:47.079172 140023005427456 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.098237156867981, loss=5.475791931152344
I0203 01:12:33.639373 140022518892288 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.8963977098464966, loss=4.614845275878906
I0203 01:12:44.866381 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:12:55.407502 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:13:31.900308 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:13:33.509244 140184451094336 submission_runner.py:408] Time since start: 48381.21s, 	Step: 95526, 	{'train/accuracy': 0.5541210770606995, 'train/loss': 1.9405015707015991, 'validation/accuracy': 0.504859983921051, 'validation/loss': 2.179647922515869, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.8288564682006836, 'test/num_examples': 10000, 'score': 43736.64577579498, 'total_duration': 48381.214473724365, 'accumulated_submission_time': 43736.64577579498, 'accumulated_eval_time': 4636.007810115814, 'accumulated_logging_time': 3.6254639625549316}
I0203 01:13:33.546636 140023005427456 logging_writer.py:48] [95526] accumulated_eval_time=4636.007810, accumulated_logging_time=3.625464, accumulated_submission_time=43736.645776, global_step=95526, preemption_count=0, score=43736.645776, test/accuracy=0.399200, test/loss=2.828856, test/num_examples=10000, total_duration=48381.214474, train/accuracy=0.554121, train/loss=1.940502, validation/accuracy=0.504860, validation/loss=2.179648, validation/num_examples=50000
I0203 01:14:04.068442 140022518892288 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.063879370689392, loss=3.3099613189697266
I0203 01:14:49.761865 140023005427456 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.2055171728134155, loss=3.0405640602111816
I0203 01:15:35.861989 140022518892288 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.2570408582687378, loss=3.5659239292144775
I0203 01:16:22.093950 140023005427456 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.1659963130950928, loss=2.995616912841797
I0203 01:17:08.274827 140022518892288 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.9925626516342163, loss=3.973684310913086
I0203 01:17:54.509202 140023005427456 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.166786789894104, loss=5.591066837310791
I0203 01:18:40.704138 140022518892288 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.1880372762680054, loss=3.0111727714538574
I0203 01:19:27.071067 140023005427456 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.2233741283416748, loss=2.9119811058044434
I0203 01:20:13.446068 140022518892288 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.318637490272522, loss=3.1077561378479004
I0203 01:20:33.924803 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:20:44.486685 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:21:13.858638 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:21:15.458830 140184451094336 submission_runner.py:408] Time since start: 48843.16s, 	Step: 96446, 	{'train/accuracy': 0.5423046946525574, 'train/loss': 2.020184278488159, 'validation/accuracy': 0.5077199935913086, 'validation/loss': 2.1947858333587646, 'validation/num_examples': 50000, 'test/accuracy': 0.39070001244544983, 'test/loss': 2.8464391231536865, 'test/num_examples': 10000, 'score': 44156.965695381165, 'total_duration': 48843.16408109665, 'accumulated_submission_time': 44156.965695381165, 'accumulated_eval_time': 4677.541831970215, 'accumulated_logging_time': 3.672870397567749}
I0203 01:21:15.488936 140023005427456 logging_writer.py:48] [96446] accumulated_eval_time=4677.541832, accumulated_logging_time=3.672870, accumulated_submission_time=44156.965695, global_step=96446, preemption_count=0, score=44156.965695, test/accuracy=0.390700, test/loss=2.846439, test/num_examples=10000, total_duration=48843.164081, train/accuracy=0.542305, train/loss=2.020184, validation/accuracy=0.507720, validation/loss=2.194786, validation/num_examples=50000
I0203 01:21:37.519982 140022518892288 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.1439192295074463, loss=3.8997726440429688
I0203 01:22:22.978153 140023005427456 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.4472143650054932, loss=3.4312868118286133
I0203 01:23:09.134091 140022518892288 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.117497205734253, loss=3.02976655960083
I0203 01:23:55.396739 140023005427456 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.2672686576843262, loss=3.0981621742248535
I0203 01:24:41.545034 140022518892288 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.975692093372345, loss=5.502377510070801
I0203 01:25:27.539812 140023005427456 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.1424205303192139, loss=2.878803014755249
I0203 01:26:13.708024 140022518892288 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.8680927753448486, loss=5.44185733795166
I0203 01:26:59.712063 140023005427456 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.8563355803489685, loss=5.339343070983887
I0203 01:27:45.993857 140022518892288 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.2353904247283936, loss=3.044161319732666
I0203 01:28:15.733921 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:28:26.179163 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:29:01.735629 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:29:03.338819 140184451094336 submission_runner.py:408] Time since start: 49311.04s, 	Step: 97366, 	{'train/accuracy': 0.5404687523841858, 'train/loss': 2.0097057819366455, 'validation/accuracy': 0.5008000135421753, 'validation/loss': 2.208786725997925, 'validation/num_examples': 50000, 'test/accuracy': 0.395300030708313, 'test/loss': 2.858224391937256, 'test/num_examples': 10000, 'score': 44577.152952194214, 'total_duration': 49311.044062137604, 'accumulated_submission_time': 44577.152952194214, 'accumulated_eval_time': 4725.14670586586, 'accumulated_logging_time': 3.7132389545440674}
I0203 01:29:03.371597 140023005427456 logging_writer.py:48] [97366] accumulated_eval_time=4725.146706, accumulated_logging_time=3.713239, accumulated_submission_time=44577.152952, global_step=97366, preemption_count=0, score=44577.152952, test/accuracy=0.395300, test/loss=2.858224, test/num_examples=10000, total_duration=49311.044062, train/accuracy=0.540469, train/loss=2.009706, validation/accuracy=0.500800, validation/loss=2.208787, validation/num_examples=50000
I0203 01:29:17.381240 140022518892288 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.9646598696708679, loss=4.926090240478516
I0203 01:30:01.124687 140023005427456 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.2619737386703491, loss=2.9723663330078125
I0203 01:30:47.417257 140022518892288 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.1944500207901, loss=3.073063373565674
I0203 01:31:33.805971 140023005427456 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.9632222056388855, loss=4.374818325042725
I0203 01:32:19.841981 140022518892288 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.1531480550765991, loss=3.1238925457000732
I0203 01:33:06.345185 140023005427456 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.297411322593689, loss=3.3052642345428467
I0203 01:33:52.501400 140022518892288 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.8528550863265991, loss=4.5901055335998535
I0203 01:34:38.429811 140023005427456 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.0190309286117554, loss=3.710599422454834
I0203 01:35:24.481808 140022518892288 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.252101182937622, loss=3.080308437347412
I0203 01:36:03.601062 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:36:14.379585 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:36:50.333965 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:36:51.932653 140184451094336 submission_runner.py:408] Time since start: 49779.64s, 	Step: 98286, 	{'train/accuracy': 0.5715429782867432, 'train/loss': 1.8492099046707153, 'validation/accuracy': 0.5137400031089783, 'validation/loss': 2.139185667037964, 'validation/num_examples': 50000, 'test/accuracy': 0.3963000178337097, 'test/loss': 2.810023069381714, 'test/num_examples': 10000, 'score': 44997.3252222538, 'total_duration': 49779.63790535927, 'accumulated_submission_time': 44997.3252222538, 'accumulated_eval_time': 4773.47830247879, 'accumulated_logging_time': 3.7554454803466797}
I0203 01:36:51.969732 140023005427456 logging_writer.py:48] [98286] accumulated_eval_time=4773.478302, accumulated_logging_time=3.755445, accumulated_submission_time=44997.325222, global_step=98286, preemption_count=0, score=44997.325222, test/accuracy=0.396300, test/loss=2.810023, test/num_examples=10000, total_duration=49779.637905, train/accuracy=0.571543, train/loss=1.849210, validation/accuracy=0.513740, validation/loss=2.139186, validation/num_examples=50000
I0203 01:36:57.984193 140022518892288 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.9894070625305176, loss=4.669312000274658
I0203 01:37:40.486557 140023005427456 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.0242185592651367, loss=4.949717044830322
I0203 01:38:26.741299 140022518892288 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.3016806840896606, loss=3.1722817420959473
I0203 01:39:13.029996 140023005427456 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.9626402854919434, loss=5.2344512939453125
I0203 01:39:59.077265 140022518892288 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.0803285837173462, loss=2.9252545833587646
I0203 01:40:45.092464 140023005427456 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.1207959651947021, loss=3.015421152114868
I0203 01:41:31.464475 140022518892288 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.2191059589385986, loss=2.9846255779266357
I0203 01:42:17.735383 140023005427456 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.1679637432098389, loss=2.9237847328186035
I0203 01:43:04.040379 140022518892288 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.8940603137016296, loss=4.334657192230225
I0203 01:43:50.228303 140023005427456 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.2364287376403809, loss=3.0106544494628906
I0203 01:43:52.144262 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:44:02.612631 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:44:32.163640 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:44:33.761662 140184451094336 submission_runner.py:408] Time since start: 50241.47s, 	Step: 99206, 	{'train/accuracy': 0.5505663752555847, 'train/loss': 1.9494009017944336, 'validation/accuracy': 0.5107200145721436, 'validation/loss': 2.1389076709747314, 'validation/num_examples': 50000, 'test/accuracy': 0.40160003304481506, 'test/loss': 2.7917513847351074, 'test/num_examples': 10000, 'score': 45417.43864130974, 'total_duration': 50241.46690893173, 'accumulated_submission_time': 45417.43864130974, 'accumulated_eval_time': 4815.0956864356995, 'accumulated_logging_time': 3.802260398864746}
I0203 01:44:33.792801 140022518892288 logging_writer.py:48] [99206] accumulated_eval_time=4815.095686, accumulated_logging_time=3.802260, accumulated_submission_time=45417.438641, global_step=99206, preemption_count=0, score=45417.438641, test/accuracy=0.401600, test/loss=2.791751, test/num_examples=10000, total_duration=50241.466909, train/accuracy=0.550566, train/loss=1.949401, validation/accuracy=0.510720, validation/loss=2.138908, validation/num_examples=50000
I0203 01:45:13.721281 140023005427456 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.4037903547286987, loss=3.0761256217956543
I0203 01:45:59.449548 140022518892288 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.4640741348266602, loss=3.0820960998535156
I0203 01:46:45.625793 140023005427456 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.3312889337539673, loss=2.9820399284362793
I0203 01:47:31.559701 140022518892288 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.2068849802017212, loss=2.959001064300537
I0203 01:48:17.569366 140023005427456 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.1255602836608887, loss=3.0320796966552734
I0203 01:49:03.775162 140022518892288 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.231511116027832, loss=3.1070361137390137
I0203 01:49:49.903875 140023005427456 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.4499378204345703, loss=2.982429265975952
I0203 01:50:36.459528 140022518892288 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.9737942814826965, loss=4.337820529937744
I0203 01:51:22.611685 140023005427456 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.2975343465805054, loss=3.159587860107422
I0203 01:51:33.803581 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:51:44.435288 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 01:52:20.990273 140184451094336 spec.py:349] Evaluating on the test split.
I0203 01:52:22.594089 140184451094336 submission_runner.py:408] Time since start: 50710.30s, 	Step: 100126, 	{'train/accuracy': 0.5661718845367432, 'train/loss': 1.8715652227401733, 'validation/accuracy': 0.5231800079345703, 'validation/loss': 2.0831315517425537, 'validation/num_examples': 50000, 'test/accuracy': 0.41370001435279846, 'test/loss': 2.7351529598236084, 'test/num_examples': 10000, 'score': 45837.39217829704, 'total_duration': 50710.299309015274, 'accumulated_submission_time': 45837.39217829704, 'accumulated_eval_time': 4863.8861446380615, 'accumulated_logging_time': 3.843090057373047}
I0203 01:52:22.628798 140022518892288 logging_writer.py:48] [100126] accumulated_eval_time=4863.886145, accumulated_logging_time=3.843090, accumulated_submission_time=45837.392178, global_step=100126, preemption_count=0, score=45837.392178, test/accuracy=0.413700, test/loss=2.735153, test/num_examples=10000, total_duration=50710.299309, train/accuracy=0.566172, train/loss=1.871565, validation/accuracy=0.523180, validation/loss=2.083132, validation/num_examples=50000
I0203 01:52:52.953944 140023005427456 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.1756843328475952, loss=3.0035994052886963
I0203 01:53:38.720912 140022518892288 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.4339888095855713, loss=3.017212152481079
I0203 01:54:25.247394 140023005427456 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.9290052056312561, loss=5.357083320617676
I0203 01:55:11.453276 140022518892288 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.1283245086669922, loss=3.470444440841675
I0203 01:55:57.477088 140023005427456 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.156097650527954, loss=3.175468683242798
I0203 01:56:43.784411 140022518892288 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.326339840888977, loss=2.933417558670044
I0203 01:57:29.734829 140023005427456 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.312503695487976, loss=2.9257700443267822
I0203 01:58:15.829856 140022518892288 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.0310105085372925, loss=4.144001483917236
I0203 01:59:01.690716 140023005427456 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.2094640731811523, loss=2.914379119873047
I0203 01:59:22.946921 140184451094336 spec.py:321] Evaluating on the training split.
I0203 01:59:33.380550 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:00:08.282997 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:00:09.882118 140184451094336 submission_runner.py:408] Time since start: 51177.59s, 	Step: 101047, 	{'train/accuracy': 0.5694140791893005, 'train/loss': 1.840009093284607, 'validation/accuracy': 0.5202800035476685, 'validation/loss': 2.0846471786499023, 'validation/num_examples': 50000, 'test/accuracy': 0.4107000231742859, 'test/loss': 2.7359278202056885, 'test/num_examples': 10000, 'score': 46257.652686834335, 'total_duration': 51177.58735990524, 'accumulated_submission_time': 46257.652686834335, 'accumulated_eval_time': 4910.821338653564, 'accumulated_logging_time': 3.887923240661621}
I0203 02:00:09.915603 140022518892288 logging_writer.py:48] [101047] accumulated_eval_time=4910.821339, accumulated_logging_time=3.887923, accumulated_submission_time=46257.652687, global_step=101047, preemption_count=0, score=46257.652687, test/accuracy=0.410700, test/loss=2.735928, test/num_examples=10000, total_duration=51177.587360, train/accuracy=0.569414, train/loss=1.840009, validation/accuracy=0.520280, validation/loss=2.084647, validation/num_examples=50000
I0203 02:00:31.533788 140023005427456 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.2432996034622192, loss=3.1108250617980957
I0203 02:01:16.550500 140022518892288 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.1675587892532349, loss=3.0412423610687256
I0203 02:02:02.960638 140023005427456 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.493129849433899, loss=3.0819196701049805
I0203 02:02:49.267159 140022518892288 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.1409674882888794, loss=3.1064257621765137
I0203 02:03:35.495858 140023005427456 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.29831862449646, loss=2.996185779571533
I0203 02:04:21.808243 140022518892288 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.2876566648483276, loss=3.0762991905212402
I0203 02:05:07.954041 140023005427456 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.192919373512268, loss=3.1228973865509033
I0203 02:05:53.971901 140022518892288 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.0739046335220337, loss=3.879303455352783
I0203 02:06:40.404191 140023005427456 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.185805082321167, loss=3.3991384506225586
I0203 02:07:10.061953 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:07:20.555902 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:07:54.575495 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:07:56.179869 140184451094336 submission_runner.py:408] Time since start: 51643.89s, 	Step: 101966, 	{'train/accuracy': 0.5561913847923279, 'train/loss': 1.9271278381347656, 'validation/accuracy': 0.5159400105476379, 'validation/loss': 2.1237990856170654, 'validation/num_examples': 50000, 'test/accuracy': 0.4108000099658966, 'test/loss': 2.763245105743408, 'test/num_examples': 10000, 'score': 46677.740731954575, 'total_duration': 51643.885112285614, 'accumulated_submission_time': 46677.740731954575, 'accumulated_eval_time': 4956.939247131348, 'accumulated_logging_time': 3.932757616043091}
I0203 02:07:56.210890 140022518892288 logging_writer.py:48] [101966] accumulated_eval_time=4956.939247, accumulated_logging_time=3.932758, accumulated_submission_time=46677.740732, global_step=101966, preemption_count=0, score=46677.740732, test/accuracy=0.410800, test/loss=2.763245, test/num_examples=10000, total_duration=51643.885112, train/accuracy=0.556191, train/loss=1.927128, validation/accuracy=0.515940, validation/loss=2.123799, validation/num_examples=50000
I0203 02:08:10.217739 140023005427456 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.112561821937561, loss=3.9971132278442383
I0203 02:08:54.181947 140022518892288 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.145900845527649, loss=3.0456297397613525
I0203 02:09:40.394851 140023005427456 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4057345390319824, loss=3.0381016731262207
I0203 02:10:26.948312 140022518892288 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.167723298072815, loss=3.2088522911071777
I0203 02:11:13.054696 140023005427456 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.173774003982544, loss=2.9105777740478516
I0203 02:11:59.225555 140022518892288 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.2212427854537964, loss=2.8344566822052
I0203 02:12:45.443654 140023005427456 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.9845554232597351, loss=4.935469627380371
I0203 02:13:31.405148 140022518892288 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.2835884094238281, loss=2.902362585067749
I0203 02:14:17.799885 140023005427456 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.2145785093307495, loss=2.9220213890075684
I0203 02:14:56.292994 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:15:07.035746 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:15:42.534243 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:15:44.137910 140184451094336 submission_runner.py:408] Time since start: 52111.84s, 	Step: 102885, 	{'train/accuracy': 0.5678125023841858, 'train/loss': 1.8487435579299927, 'validation/accuracy': 0.5289799571037292, 'validation/loss': 2.0497944355010986, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.709082841873169, 'test/num_examples': 10000, 'score': 47097.766208171844, 'total_duration': 52111.843133449554, 'accumulated_submission_time': 47097.766208171844, 'accumulated_eval_time': 5004.784141540527, 'accumulated_logging_time': 3.972791910171509}
I0203 02:15:44.177477 140022518892288 logging_writer.py:48] [102885] accumulated_eval_time=5004.784142, accumulated_logging_time=3.972792, accumulated_submission_time=47097.766208, global_step=102885, preemption_count=0, score=47097.766208, test/accuracy=0.423400, test/loss=2.709083, test/num_examples=10000, total_duration=52111.843133, train/accuracy=0.567813, train/loss=1.848744, validation/accuracy=0.528980, validation/loss=2.049794, validation/num_examples=50000
I0203 02:15:50.585490 140023005427456 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.3558584451675415, loss=3.046048879623413
I0203 02:16:33.499682 140022518892288 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.0626758337020874, loss=4.719902992248535
I0203 02:17:19.636599 140023005427456 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.2149330377578735, loss=2.8751611709594727
I0203 02:18:05.819249 140022518892288 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.0534799098968506, loss=4.63864803314209
I0203 02:18:51.740708 140023005427456 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.2032618522644043, loss=2.841953992843628
I0203 02:19:37.649352 140022518892288 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.0545628070831299, loss=5.434311866760254
I0203 02:20:23.739193 140023005427456 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.1530665159225464, loss=3.527536392211914
I0203 02:21:10.063970 140022518892288 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.294218897819519, loss=2.924283742904663
I0203 02:21:56.291843 140023005427456 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.362730860710144, loss=2.876523971557617
I0203 02:22:42.410600 140022518892288 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.948482871055603, loss=4.784119129180908
I0203 02:22:44.407096 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:22:54.845098 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:23:32.171496 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:23:33.783305 140184451094336 submission_runner.py:408] Time since start: 52581.49s, 	Step: 103806, 	{'train/accuracy': 0.5733789205551147, 'train/loss': 1.8240511417388916, 'validation/accuracy': 0.5295799970626831, 'validation/loss': 2.0383520126342773, 'validation/num_examples': 50000, 'test/accuracy': 0.41930001974105835, 'test/loss': 2.696749448776245, 'test/num_examples': 10000, 'score': 47517.937237262726, 'total_duration': 52581.48854184151, 'accumulated_submission_time': 47517.937237262726, 'accumulated_eval_time': 5054.160320997238, 'accumulated_logging_time': 4.023122072219849}
I0203 02:23:33.815844 140023005427456 logging_writer.py:48] [103806] accumulated_eval_time=5054.160321, accumulated_logging_time=4.023122, accumulated_submission_time=47517.937237, global_step=103806, preemption_count=0, score=47517.937237, test/accuracy=0.419300, test/loss=2.696749, test/num_examples=10000, total_duration=52581.488542, train/accuracy=0.573379, train/loss=1.824051, validation/accuracy=0.529580, validation/loss=2.038352, validation/num_examples=50000
I0203 02:24:13.623106 140022518892288 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.2761919498443604, loss=2.987595319747925
I0203 02:24:59.718386 140023005427456 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.1194881200790405, loss=2.907578468322754
I0203 02:25:45.875504 140022518892288 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.2672359943389893, loss=3.055340051651001
I0203 02:26:32.104671 140023005427456 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.3095017671585083, loss=2.950296401977539
I0203 02:27:18.481297 140022518892288 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.216829776763916, loss=2.8581995964050293
I0203 02:28:04.529589 140023005427456 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.251518964767456, loss=2.9214444160461426
I0203 02:28:50.742346 140022518892288 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.3393678665161133, loss=2.910130500793457
I0203 02:29:37.039884 140023005427456 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.378546118736267, loss=2.854572296142578
I0203 02:30:23.197960 140022518892288 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.2151198387145996, loss=2.930758237838745
I0203 02:30:33.859044 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:30:44.485627 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:31:19.961697 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:31:21.569671 140184451094336 submission_runner.py:408] Time since start: 53049.27s, 	Step: 104725, 	{'train/accuracy': 0.567187488079071, 'train/loss': 1.8645880222320557, 'validation/accuracy': 0.5290799736976624, 'validation/loss': 2.053553581237793, 'validation/num_examples': 50000, 'test/accuracy': 0.4147000312805176, 'test/loss': 2.7142860889434814, 'test/num_examples': 10000, 'score': 47937.92220687866, 'total_duration': 53049.274918079376, 'accumulated_submission_time': 47937.92220687866, 'accumulated_eval_time': 5101.870931148529, 'accumulated_logging_time': 4.066912651062012}
I0203 02:31:21.603811 140023005427456 logging_writer.py:48] [104725] accumulated_eval_time=5101.870931, accumulated_logging_time=4.066913, accumulated_submission_time=47937.922207, global_step=104725, preemption_count=0, score=47937.922207, test/accuracy=0.414700, test/loss=2.714286, test/num_examples=10000, total_duration=53049.274918, train/accuracy=0.567187, train/loss=1.864588, validation/accuracy=0.529080, validation/loss=2.053554, validation/num_examples=50000
I0203 02:31:52.521583 140022518892288 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.9408285617828369, loss=5.271934509277344
I0203 02:32:38.549144 140023005427456 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.2186620235443115, loss=2.938025712966919
I0203 02:33:24.912210 140022518892288 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.9937334060668945, loss=5.37213134765625
I0203 02:34:11.317847 140023005427456 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.1465301513671875, loss=2.831723213195801
I0203 02:34:57.251700 140022518892288 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.1978012323379517, loss=2.912745714187622
I0203 02:35:43.385845 140023005427456 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.3721824884414673, loss=3.246206760406494
I0203 02:36:29.709213 140022518892288 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.0441017150878906, loss=3.9020469188690186
I0203 02:37:16.063069 140023005427456 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.3336888551712036, loss=3.0371289253234863
I0203 02:38:02.267751 140022518892288 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.148206114768982, loss=3.4041085243225098
I0203 02:38:21.928661 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:38:32.358027 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:39:10.480122 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:39:12.084243 140184451094336 submission_runner.py:408] Time since start: 53519.79s, 	Step: 105644, 	{'train/accuracy': 0.5702148079872131, 'train/loss': 1.839844822883606, 'validation/accuracy': 0.5308600068092346, 'validation/loss': 2.028477191925049, 'validation/num_examples': 50000, 'test/accuracy': 0.4223000109195709, 'test/loss': 2.6632344722747803, 'test/num_examples': 10000, 'score': 48358.190257549286, 'total_duration': 53519.78946995735, 'accumulated_submission_time': 48358.190257549286, 'accumulated_eval_time': 5152.026482105255, 'accumulated_logging_time': 4.110436916351318}
I0203 02:39:12.115554 140023005427456 logging_writer.py:48] [105644] accumulated_eval_time=5152.026482, accumulated_logging_time=4.110437, accumulated_submission_time=48358.190258, global_step=105644, preemption_count=0, score=48358.190258, test/accuracy=0.422300, test/loss=2.663234, test/num_examples=10000, total_duration=53519.789470, train/accuracy=0.570215, train/loss=1.839845, validation/accuracy=0.530860, validation/loss=2.028477, validation/num_examples=50000
I0203 02:39:34.909703 140022518892288 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.1058528423309326, loss=2.840773105621338
I0203 02:40:20.416101 140023005427456 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.1129099130630493, loss=4.329136848449707
I0203 02:41:06.845750 140022518892288 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.0731019973754883, loss=3.3154990673065186
I0203 02:41:53.205152 140023005427456 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.2661634683609009, loss=2.960212469100952
I0203 02:42:39.299973 140022518892288 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.2745364904403687, loss=2.8912529945373535
I0203 02:43:25.190954 140023005427456 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.8879429697990417, loss=5.308616638183594
I0203 02:44:11.243297 140022518892288 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.3065673112869263, loss=2.9060006141662598
I0203 02:44:57.889121 140023005427456 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.1436771154403687, loss=3.013760805130005
I0203 02:45:43.933485 140022518892288 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.2939971685409546, loss=3.008687973022461
I0203 02:46:12.221655 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:46:22.769400 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:46:54.688429 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:46:56.284547 140184451094336 submission_runner.py:408] Time since start: 53983.99s, 	Step: 106563, 	{'train/accuracy': 0.5782421827316284, 'train/loss': 1.8550362586975098, 'validation/accuracy': 0.5310800075531006, 'validation/loss': 2.070237874984741, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.710397720336914, 'test/num_examples': 10000, 'score': 48778.23556470871, 'total_duration': 53983.98978614807, 'accumulated_submission_time': 48778.23556470871, 'accumulated_eval_time': 5196.08935046196, 'accumulated_logging_time': 4.1553053855896}
I0203 02:46:56.316589 140023005427456 logging_writer.py:48] [106563] accumulated_eval_time=5196.089350, accumulated_logging_time=4.155305, accumulated_submission_time=48778.235565, global_step=106563, preemption_count=0, score=48778.235565, test/accuracy=0.417000, test/loss=2.710398, test/num_examples=10000, total_duration=53983.989786, train/accuracy=0.578242, train/loss=1.855036, validation/accuracy=0.531080, validation/loss=2.070238, validation/num_examples=50000
I0203 02:47:11.536759 140022518892288 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.9277867078781128, loss=4.779317378997803
I0203 02:47:55.500576 140023005427456 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.0848593711853027, loss=5.278863906860352
I0203 02:48:41.745768 140022518892288 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.0807170867919922, loss=3.0769944190979004
I0203 02:49:28.282489 140023005427456 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.9758182168006897, loss=4.205058574676514
I0203 02:50:14.413191 140022518892288 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.3078415393829346, loss=2.78147029876709
I0203 02:51:00.705090 140023005427456 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.2219467163085938, loss=2.9711289405822754
I0203 02:51:47.037901 140022518892288 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.1749680042266846, loss=2.847317695617676
I0203 02:52:33.215004 140023005427456 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.9921308755874634, loss=4.625977039337158
I0203 02:53:19.666947 140022518892288 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.33110511302948, loss=2.9580373764038086
I0203 02:53:56.625024 140184451094336 spec.py:321] Evaluating on the training split.
I0203 02:54:07.207776 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 02:54:42.889556 140184451094336 spec.py:349] Evaluating on the test split.
I0203 02:54:44.488500 140184451094336 submission_runner.py:408] Time since start: 54452.19s, 	Step: 107482, 	{'train/accuracy': 0.6131640672683716, 'train/loss': 1.644121527671814, 'validation/accuracy': 0.536899983882904, 'validation/loss': 2.003920078277588, 'validation/num_examples': 50000, 'test/accuracy': 0.42920002341270447, 'test/loss': 2.6709156036376953, 'test/num_examples': 10000, 'score': 49198.48667836189, 'total_duration': 54452.193747758865, 'accumulated_submission_time': 49198.48667836189, 'accumulated_eval_time': 5243.95282626152, 'accumulated_logging_time': 4.197072982788086}
I0203 02:54:44.526272 140023005427456 logging_writer.py:48] [107482] accumulated_eval_time=5243.952826, accumulated_logging_time=4.197073, accumulated_submission_time=49198.486678, global_step=107482, preemption_count=0, score=49198.486678, test/accuracy=0.429200, test/loss=2.670916, test/num_examples=10000, total_duration=54452.193748, train/accuracy=0.613164, train/loss=1.644122, validation/accuracy=0.536900, validation/loss=2.003920, validation/num_examples=50000
I0203 02:54:52.121175 140022518892288 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.0747953653335571, loss=4.489850044250488
I0203 02:55:35.250603 140023005427456 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.0841953754425049, loss=5.319930553436279
I0203 02:56:21.275348 140022518892288 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.0560191869735718, loss=3.1392481327056885
I0203 02:57:07.713752 140023005427456 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.184177279472351, loss=2.9107720851898193
I0203 02:57:53.760101 140022518892288 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.240355372428894, loss=2.8652329444885254
I0203 02:58:40.045516 140023005427456 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.3539937734603882, loss=2.8800880908966064
I0203 02:59:26.298492 140022518892288 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.0883854627609253, loss=3.9434382915496826
I0203 03:00:12.495421 140023005427456 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.2283339500427246, loss=5.516833305358887
I0203 03:00:58.626478 140022518892288 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.2785569429397583, loss=3.0506742000579834
I0203 03:01:45.194290 140023005427456 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.0049304962158203, loss=5.023299217224121
I0203 03:01:45.214038 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:01:55.534027 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:02:26.966840 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:02:28.585928 140184451094336 submission_runner.py:408] Time since start: 54916.29s, 	Step: 108401, 	{'train/accuracy': 0.5776953101158142, 'train/loss': 1.8307543992996216, 'validation/accuracy': 0.5416600108146667, 'validation/loss': 2.010225296020508, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.666642427444458, 'test/num_examples': 10000, 'score': 49619.11729288101, 'total_duration': 54916.2911529541, 'accumulated_submission_time': 49619.11729288101, 'accumulated_eval_time': 5287.324712753296, 'accumulated_logging_time': 4.244433879852295}
I0203 03:02:28.622354 140022518892288 logging_writer.py:48] [108401] accumulated_eval_time=5287.324713, accumulated_logging_time=4.244434, accumulated_submission_time=49619.117293, global_step=108401, preemption_count=0, score=49619.117293, test/accuracy=0.422800, test/loss=2.666642, test/num_examples=10000, total_duration=54916.291153, train/accuracy=0.577695, train/loss=1.830754, validation/accuracy=0.541660, validation/loss=2.010225, validation/num_examples=50000
I0203 03:03:10.343066 140023005427456 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.0919359922409058, loss=4.025547027587891
I0203 03:03:56.366508 140022518892288 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.3224809169769287, loss=2.8183155059814453
I0203 03:04:42.834672 140023005427456 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.9805905818939209, loss=5.35500955581665
I0203 03:05:28.824274 140022518892288 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.4332515001296997, loss=2.919525623321533
I0203 03:06:15.378954 140023005427456 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.2266833782196045, loss=3.108175754547119
I0203 03:07:01.405192 140022518892288 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.2137616872787476, loss=3.4110426902770996
I0203 03:07:47.669181 140023005427456 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.184898018836975, loss=2.715101718902588
I0203 03:08:33.695939 140022518892288 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.17411208152771, loss=2.996675968170166
I0203 03:09:19.821879 140023005427456 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.2970991134643555, loss=2.865203857421875
I0203 03:09:28.776238 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:09:40.383725 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:10:14.586754 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:10:16.191636 140184451094336 submission_runner.py:408] Time since start: 55383.90s, 	Step: 109321, 	{'train/accuracy': 0.5839257836341858, 'train/loss': 1.781617283821106, 'validation/accuracy': 0.5431399941444397, 'validation/loss': 1.9895758628845215, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.6396257877349854, 'test/num_examples': 10000, 'score': 50039.2133743763, 'total_duration': 55383.89688038826, 'accumulated_submission_time': 50039.2133743763, 'accumulated_eval_time': 5334.740091085434, 'accumulated_logging_time': 4.290728807449341}
I0203 03:10:16.226169 140022518892288 logging_writer.py:48] [109321] accumulated_eval_time=5334.740091, accumulated_logging_time=4.290729, accumulated_submission_time=50039.213374, global_step=109321, preemption_count=0, score=50039.213374, test/accuracy=0.429500, test/loss=2.639626, test/num_examples=10000, total_duration=55383.896880, train/accuracy=0.583926, train/loss=1.781617, validation/accuracy=0.543140, validation/loss=1.989576, validation/num_examples=50000
I0203 03:10:48.840980 140023005427456 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.2396706342697144, loss=3.064551591873169
I0203 03:11:34.894398 140022518892288 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.3123191595077515, loss=3.1641805171966553
I0203 03:12:21.260881 140023005427456 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.1019353866577148, loss=3.2387592792510986
I0203 03:13:07.224208 140022518892288 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.0614181756973267, loss=4.290701389312744
I0203 03:13:53.182933 140023005427456 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.0891748666763306, loss=5.397331237792969
I0203 03:14:39.376847 140022518892288 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.1936804056167603, loss=2.769864320755005
I0203 03:15:25.546225 140023005427456 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.2754769325256348, loss=2.8258488178253174
I0203 03:16:11.587279 140022518892288 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.2494972944259644, loss=2.919856548309326
I0203 03:16:57.647019 140023005427456 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.0435713529586792, loss=5.325304985046387
I0203 03:17:16.316297 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:17:26.750117 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:18:00.372469 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:18:01.979992 140184451094336 submission_runner.py:408] Time since start: 55849.69s, 	Step: 110242, 	{'train/accuracy': 0.6037304401397705, 'train/loss': 1.6702401638031006, 'validation/accuracy': 0.5479999780654907, 'validation/loss': 1.9538955688476562, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.595966100692749, 'test/num_examples': 10000, 'score': 50459.243683576584, 'total_duration': 55849.685177087784, 'accumulated_submission_time': 50459.243683576584, 'accumulated_eval_time': 5380.403715848923, 'accumulated_logging_time': 4.336857795715332}
I0203 03:18:02.041487 140022518892288 logging_writer.py:48] [110242] accumulated_eval_time=5380.403716, accumulated_logging_time=4.336858, accumulated_submission_time=50459.243684, global_step=110242, preemption_count=0, score=50459.243684, test/accuracy=0.437400, test/loss=2.595966, test/num_examples=10000, total_duration=55849.685177, train/accuracy=0.603730, train/loss=1.670240, validation/accuracy=0.548000, validation/loss=1.953896, validation/num_examples=50000
I0203 03:18:25.651447 140023005427456 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.2401739358901978, loss=2.915327787399292
I0203 03:19:10.658691 140022518892288 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.2313302755355835, loss=3.1086018085479736
I0203 03:19:57.067022 140023005427456 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.219657063484192, loss=3.642875909805298
I0203 03:20:43.308946 140022518892288 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.9601507782936096, loss=4.812897682189941
I0203 03:21:29.655220 140023005427456 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.2283684015274048, loss=2.865802049636841
I0203 03:22:16.201759 140022518892288 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.3135395050048828, loss=2.94429874420166
I0203 03:23:02.207706 140023005427456 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.200736403465271, loss=3.3729631900787354
I0203 03:23:48.544898 140022518892288 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.3251081705093384, loss=3.059013843536377
I0203 03:24:34.713324 140023005427456 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.1718685626983643, loss=2.707273244857788
I0203 03:25:02.042492 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:25:12.670902 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:25:42.737131 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:25:44.341516 140184451094336 submission_runner.py:408] Time since start: 56312.05s, 	Step: 111161, 	{'train/accuracy': 0.5822851657867432, 'train/loss': 1.7880244255065918, 'validation/accuracy': 0.5461999773979187, 'validation/loss': 1.9647815227508545, 'validation/num_examples': 50000, 'test/accuracy': 0.4337000250816345, 'test/loss': 2.6289896965026855, 'test/num_examples': 10000, 'score': 50879.184905052185, 'total_duration': 56312.04675197601, 'accumulated_submission_time': 50879.184905052185, 'accumulated_eval_time': 5422.70272564888, 'accumulated_logging_time': 4.410296440124512}
I0203 03:25:44.374073 140022518892288 logging_writer.py:48] [111161] accumulated_eval_time=5422.702726, accumulated_logging_time=4.410296, accumulated_submission_time=50879.184905, global_step=111161, preemption_count=0, score=50879.184905, test/accuracy=0.433700, test/loss=2.628990, test/num_examples=10000, total_duration=56312.046752, train/accuracy=0.582285, train/loss=1.788024, validation/accuracy=0.546200, validation/loss=1.964782, validation/num_examples=50000
I0203 03:26:00.378706 140023005427456 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.3725277185440063, loss=3.0323338508605957
I0203 03:26:44.471703 140022518892288 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.1798052787780762, loss=3.1179370880126953
I0203 03:27:30.948925 140023005427456 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.191978096961975, loss=3.279332399368286
I0203 03:28:17.431512 140022518892288 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.5807280540466309, loss=2.705550193786621
I0203 03:29:03.676280 140023005427456 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.3868939876556396, loss=2.8654890060424805
I0203 03:29:49.853867 140022518892288 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.1819820404052734, loss=2.7793526649475098
I0203 03:30:36.000671 140023005427456 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.1272354125976562, loss=5.402441501617432
I0203 03:31:22.265200 140022518892288 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.034479022026062, loss=3.46474552154541
I0203 03:32:08.993686 140023005427456 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.1254727840423584, loss=5.11407995223999
I0203 03:32:44.662321 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:32:55.018532 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:33:31.501364 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:33:33.096065 140184451094336 submission_runner.py:408] Time since start: 56780.80s, 	Step: 112079, 	{'train/accuracy': 0.5941405892372131, 'train/loss': 1.7171932458877563, 'validation/accuracy': 0.5562199950218201, 'validation/loss': 1.896606683731079, 'validation/num_examples': 50000, 'test/accuracy': 0.44110003113746643, 'test/loss': 2.5732004642486572, 'test/num_examples': 10000, 'score': 51299.41596865654, 'total_duration': 56780.80129766464, 'accumulated_submission_time': 51299.41596865654, 'accumulated_eval_time': 5471.136475563049, 'accumulated_logging_time': 4.4519734382629395}
I0203 03:33:33.132409 140022518892288 logging_writer.py:48] [112079] accumulated_eval_time=5471.136476, accumulated_logging_time=4.451973, accumulated_submission_time=51299.415969, global_step=112079, preemption_count=0, score=51299.415969, test/accuracy=0.441100, test/loss=2.573200, test/num_examples=10000, total_duration=56780.801298, train/accuracy=0.594141, train/loss=1.717193, validation/accuracy=0.556220, validation/loss=1.896607, validation/num_examples=50000
I0203 03:33:41.926617 140023005427456 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.2712383270263672, loss=2.8226194381713867
I0203 03:34:24.979367 140022518892288 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.273930549621582, loss=2.7725319862365723
I0203 03:35:11.124363 140023005427456 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.4663270711898804, loss=2.7656755447387695
I0203 03:35:57.681411 140022518892288 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.2493414878845215, loss=2.8388142585754395
I0203 03:36:43.727818 140023005427456 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.1681450605392456, loss=2.76338267326355
I0203 03:37:30.263629 140022518892288 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.2725919485092163, loss=3.092721939086914
I0203 03:38:16.494477 140023005427456 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.2486025094985962, loss=2.7724714279174805
I0203 03:39:02.450009 140022518892288 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.3048921823501587, loss=2.6779870986938477
I0203 03:39:48.594950 140023005427456 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.367220163345337, loss=2.849921464920044
I0203 03:40:33.405226 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:40:43.708257 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:41:19.612847 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:41:21.207931 140184451094336 submission_runner.py:408] Time since start: 57248.91s, 	Step: 112999, 	{'train/accuracy': 0.6062109470367432, 'train/loss': 1.6639678478240967, 'validation/accuracy': 0.555899977684021, 'validation/loss': 1.902474045753479, 'validation/num_examples': 50000, 'test/accuracy': 0.4442000091075897, 'test/loss': 2.5536797046661377, 'test/num_examples': 10000, 'score': 51719.630373477936, 'total_duration': 57248.913175821304, 'accumulated_submission_time': 51719.630373477936, 'accumulated_eval_time': 5518.939175367355, 'accumulated_logging_time': 4.49985933303833}
I0203 03:41:21.246127 140022518892288 logging_writer.py:48] [112999] accumulated_eval_time=5518.939175, accumulated_logging_time=4.499859, accumulated_submission_time=51719.630373, global_step=112999, preemption_count=0, score=51719.630373, test/accuracy=0.444200, test/loss=2.553680, test/num_examples=10000, total_duration=57248.913176, train/accuracy=0.606211, train/loss=1.663968, validation/accuracy=0.555900, validation/loss=1.902474, validation/num_examples=50000
I0203 03:41:22.044725 140023005427456 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.4196866750717163, loss=2.741623640060425
I0203 03:42:04.094674 140022518892288 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.1890672445297241, loss=3.4459068775177
I0203 03:42:50.189439 140023005427456 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.5408949851989746, loss=2.8932836055755615
I0203 03:43:36.540513 140022518892288 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.0484539270401, loss=4.530248165130615
I0203 03:44:22.480456 140023005427456 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.028975009918213, loss=4.066516399383545
I0203 03:45:08.527080 140022518892288 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.2199203968048096, loss=3.422024726867676
I0203 03:45:54.278872 140023005427456 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.2168501615524292, loss=2.7055811882019043
I0203 03:46:40.658041 140022518892288 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.3218034505844116, loss=2.7346067428588867
I0203 03:47:26.957648 140023005427456 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.0940749645233154, loss=3.2542905807495117
I0203 03:48:13.407973 140022518892288 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.0431020259857178, loss=4.228126525878906
I0203 03:48:21.341714 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:48:31.698648 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:49:09.222280 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:49:10.825536 140184451094336 submission_runner.py:408] Time since start: 57718.53s, 	Step: 113919, 	{'train/accuracy': 0.594042956829071, 'train/loss': 1.7198957204818726, 'validation/accuracy': 0.5557599663734436, 'validation/loss': 1.9055322408676147, 'validation/num_examples': 50000, 'test/accuracy': 0.4449000358581543, 'test/loss': 2.562439203262329, 'test/num_examples': 10000, 'score': 52139.66808462143, 'total_duration': 57718.53077292442, 'accumulated_submission_time': 52139.66808462143, 'accumulated_eval_time': 5568.422976732254, 'accumulated_logging_time': 4.548093795776367}
I0203 03:49:10.859176 140023005427456 logging_writer.py:48] [113919] accumulated_eval_time=5568.422977, accumulated_logging_time=4.548094, accumulated_submission_time=52139.668085, global_step=113919, preemption_count=0, score=52139.668085, test/accuracy=0.444900, test/loss=2.562439, test/num_examples=10000, total_duration=57718.530773, train/accuracy=0.594043, train/loss=1.719896, validation/accuracy=0.555760, validation/loss=1.905532, validation/num_examples=50000
I0203 03:49:44.347649 140022518892288 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.2572263479232788, loss=2.9208123683929443
I0203 03:50:30.287828 140023005427456 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.270308256149292, loss=2.779147148132324
I0203 03:51:16.663934 140022518892288 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.2389956712722778, loss=2.671403169631958
I0203 03:52:02.907651 140023005427456 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.2632700204849243, loss=2.6281559467315674
I0203 03:52:49.086358 140022518892288 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.0921128988265991, loss=3.9054150581359863
I0203 03:53:35.333679 140023005427456 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.1138688325881958, loss=3.599705696105957
I0203 03:54:21.573490 140022518892288 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.4440338611602783, loss=2.7679691314697266
I0203 03:55:07.634754 140023005427456 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.6224061250686646, loss=2.771291494369507
I0203 03:55:53.871877 140022518892288 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.3390005826950073, loss=2.7454183101654053
I0203 03:56:11.137426 140184451094336 spec.py:321] Evaluating on the training split.
I0203 03:56:21.826304 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 03:56:55.228359 140184451094336 spec.py:349] Evaluating on the test split.
I0203 03:56:56.839308 140184451094336 submission_runner.py:408] Time since start: 58184.54s, 	Step: 114839, 	{'train/accuracy': 0.6001757383346558, 'train/loss': 1.7225245237350464, 'validation/accuracy': 0.5551599860191345, 'validation/loss': 1.9314780235290527, 'validation/num_examples': 50000, 'test/accuracy': 0.4402000308036804, 'test/loss': 2.587394952774048, 'test/num_examples': 10000, 'score': 52559.89040374756, 'total_duration': 58184.54452776909, 'accumulated_submission_time': 52559.89040374756, 'accumulated_eval_time': 5614.12481713295, 'accumulated_logging_time': 4.5912158489227295}
I0203 03:56:56.875522 140023005427456 logging_writer.py:48] [114839] accumulated_eval_time=5614.124817, accumulated_logging_time=4.591216, accumulated_submission_time=52559.890404, global_step=114839, preemption_count=0, score=52559.890404, test/accuracy=0.440200, test/loss=2.587395, test/num_examples=10000, total_duration=58184.544528, train/accuracy=0.600176, train/loss=1.722525, validation/accuracy=0.555160, validation/loss=1.931478, validation/num_examples=50000
I0203 03:57:21.679740 140022518892288 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.3190165758132935, loss=2.7573509216308594
I0203 03:58:07.470079 140023005427456 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.423148274421692, loss=2.8836374282836914
I0203 03:58:53.732421 140022518892288 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.3697513341903687, loss=2.79274582862854
I0203 03:59:39.934714 140023005427456 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.337630033493042, loss=2.7835421562194824
I0203 04:00:26.273033 140022518892288 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.3103190660476685, loss=2.819807291030884
I0203 04:01:12.670754 140023005427456 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.3381695747375488, loss=2.751584053039551
I0203 04:01:59.187837 140022518892288 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.0871529579162598, loss=4.609951496124268
I0203 04:02:45.413504 140023005427456 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.2159249782562256, loss=2.66503643989563
I0203 04:03:31.713147 140022518892288 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.1928622722625732, loss=3.1522879600524902
I0203 04:03:57.309880 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:04:07.774881 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:04:42.215415 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:04:43.818701 140184451094336 submission_runner.py:408] Time since start: 58651.52s, 	Step: 115757, 	{'train/accuracy': 0.6089257597923279, 'train/loss': 1.6470164060592651, 'validation/accuracy': 0.5604599714279175, 'validation/loss': 1.8817566633224487, 'validation/num_examples': 50000, 'test/accuracy': 0.4474000334739685, 'test/loss': 2.5390818119049072, 'test/num_examples': 10000, 'score': 52980.26725935936, 'total_duration': 58651.523950338364, 'accumulated_submission_time': 52980.26725935936, 'accumulated_eval_time': 5660.633631229401, 'accumulated_logging_time': 4.6373395919799805}
I0203 04:04:43.851124 140023005427456 logging_writer.py:48] [115757] accumulated_eval_time=5660.633631, accumulated_logging_time=4.637340, accumulated_submission_time=52980.267259, global_step=115757, preemption_count=0, score=52980.267259, test/accuracy=0.447400, test/loss=2.539082, test/num_examples=10000, total_duration=58651.523950, train/accuracy=0.608926, train/loss=1.647016, validation/accuracy=0.560460, validation/loss=1.881757, validation/num_examples=50000
I0203 04:05:01.472276 140022518892288 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.4694167375564575, loss=2.733416795730591
I0203 04:05:45.818745 140023005427456 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.3736934661865234, loss=3.0204920768737793
I0203 04:06:32.069266 140022518892288 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.0475987195968628, loss=4.613198757171631
I0203 04:07:18.557648 140023005427456 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.3163129091262817, loss=2.9696602821350098
I0203 04:08:04.700681 140022518892288 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.1844979524612427, loss=4.0109429359436035
I0203 04:08:50.810092 140023005427456 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.3330960273742676, loss=3.2840662002563477
I0203 04:09:37.033545 140022518892288 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.0935026407241821, loss=4.848682403564453
I0203 04:10:23.322533 140023005427456 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.3074849843978882, loss=2.7704904079437256
I0203 04:11:09.581999 140022518892288 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.2796028852462769, loss=2.8517775535583496
I0203 04:11:44.185337 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:11:54.771476 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:12:29.578302 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:12:31.213410 140184451094336 submission_runner.py:408] Time since start: 59118.92s, 	Step: 116677, 	{'train/accuracy': 0.613476574420929, 'train/loss': 1.6504943370819092, 'validation/accuracy': 0.5635200142860413, 'validation/loss': 1.8760992288589478, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.5303001403808594, 'test/num_examples': 10000, 'score': 53400.54282641411, 'total_duration': 59118.91864323616, 'accumulated_submission_time': 53400.54282641411, 'accumulated_eval_time': 5707.661694765091, 'accumulated_logging_time': 4.6801183223724365}
I0203 04:12:31.249421 140023005427456 logging_writer.py:48] [116677] accumulated_eval_time=5707.661695, accumulated_logging_time=4.680118, accumulated_submission_time=53400.542826, global_step=116677, preemption_count=0, score=53400.542826, test/accuracy=0.447800, test/loss=2.530300, test/num_examples=10000, total_duration=59118.918643, train/accuracy=0.613477, train/loss=1.650494, validation/accuracy=0.563520, validation/loss=1.876099, validation/num_examples=50000
I0203 04:12:40.851993 140022518892288 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.1002472639083862, loss=3.8180689811706543
I0203 04:13:24.242678 140023005427456 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.3244187831878662, loss=2.7234561443328857
I0203 04:14:10.469928 140022518892288 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.2255370616912842, loss=4.744314193725586
I0203 04:14:56.949063 140023005427456 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.371955156326294, loss=2.8163743019104004
I0203 04:15:43.141134 140022518892288 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.176423192024231, loss=4.177647590637207
I0203 04:16:29.334226 140023005427456 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.3300871849060059, loss=2.903193235397339
I0203 04:17:15.455839 140022518892288 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.382664442062378, loss=2.7077250480651855
I0203 04:18:01.466485 140023005427456 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.4876482486724854, loss=2.7219767570495605
I0203 04:18:47.583187 140022518892288 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.5176105499267578, loss=2.5655438899993896
I0203 04:19:31.358984 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:19:41.917686 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:20:15.525299 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:20:17.137851 140184451094336 submission_runner.py:408] Time since start: 59584.84s, 	Step: 117596, 	{'train/accuracy': 0.6048437356948853, 'train/loss': 1.6839405298233032, 'validation/accuracy': 0.5666399598121643, 'validation/loss': 1.874297022819519, 'validation/num_examples': 50000, 'test/accuracy': 0.4523000121116638, 'test/loss': 2.5318267345428467, 'test/num_examples': 10000, 'score': 53820.59474277496, 'total_duration': 59584.84307575226, 'accumulated_submission_time': 53820.59474277496, 'accumulated_eval_time': 5753.440539121628, 'accumulated_logging_time': 4.725946426391602}
I0203 04:20:17.177027 140023005427456 logging_writer.py:48] [117596] accumulated_eval_time=5753.440539, accumulated_logging_time=4.725946, accumulated_submission_time=53820.594743, global_step=117596, preemption_count=0, score=53820.594743, test/accuracy=0.452300, test/loss=2.531827, test/num_examples=10000, total_duration=59584.843076, train/accuracy=0.604844, train/loss=1.683941, validation/accuracy=0.566640, validation/loss=1.874297, validation/num_examples=50000
I0203 04:20:19.185194 140022518892288 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.439258337020874, loss=2.6425371170043945
I0203 04:21:01.515753 140023005427456 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.2175500392913818, loss=3.5829622745513916
I0203 04:21:47.645176 140022518892288 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.1815935373306274, loss=4.944307327270508
I0203 04:22:33.820437 140023005427456 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.1078495979309082, loss=4.143023490905762
I0203 04:23:19.940836 140022518892288 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.2196986675262451, loss=4.002098083496094
I0203 04:24:06.378613 140023005427456 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.0561169385910034, loss=5.158363342285156
I0203 04:24:52.311324 140022518892288 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.204128384590149, loss=5.155487060546875
I0203 04:25:38.410117 140023005427456 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.301628589630127, loss=2.7370760440826416
I0203 04:26:24.604118 140022518892288 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.145673394203186, loss=4.668666362762451
I0203 04:27:10.841696 140023005427456 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.078278660774231, loss=4.349886417388916
I0203 04:27:17.448282 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:27:27.746445 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:28:04.400763 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:28:06.004431 140184451094336 submission_runner.py:408] Time since start: 60053.71s, 	Step: 118516, 	{'train/accuracy': 0.617871105670929, 'train/loss': 1.593552827835083, 'validation/accuracy': 0.5720199942588806, 'validation/loss': 1.8123306035995483, 'validation/num_examples': 50000, 'test/accuracy': 0.4612000286579132, 'test/loss': 2.4730336666107178, 'test/num_examples': 10000, 'score': 54240.80654287338, 'total_duration': 60053.70967531204, 'accumulated_submission_time': 54240.80654287338, 'accumulated_eval_time': 5801.9966831207275, 'accumulated_logging_time': 4.77754282951355}
I0203 04:28:06.038285 140022518892288 logging_writer.py:48] [118516] accumulated_eval_time=5801.996683, accumulated_logging_time=4.777543, accumulated_submission_time=54240.806543, global_step=118516, preemption_count=0, score=54240.806543, test/accuracy=0.461200, test/loss=2.473034, test/num_examples=10000, total_duration=60053.709675, train/accuracy=0.617871, train/loss=1.593553, validation/accuracy=0.572020, validation/loss=1.812331, validation/num_examples=50000
I0203 04:28:41.024823 140023005427456 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.2258418798446655, loss=3.565340995788574
I0203 04:29:27.023797 140022518892288 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.2378450632095337, loss=3.754473924636841
I0203 04:30:13.538144 140023005427456 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.2748593091964722, loss=2.688295364379883
I0203 04:30:59.848669 140022518892288 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.2212365865707397, loss=4.121267318725586
I0203 04:31:46.370097 140023005427456 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.155449390411377, loss=4.921511173248291
I0203 04:32:32.480729 140022518892288 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.6303998231887817, loss=2.7614247798919678
I0203 04:33:18.769816 140023005427456 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.648061752319336, loss=2.6861000061035156
I0203 04:34:05.099063 140022518892288 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.1412224769592285, loss=4.747239589691162
I0203 04:34:51.117347 140023005427456 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.354276180267334, loss=2.702033042907715
I0203 04:35:06.148221 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:35:16.993140 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:35:48.406700 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:35:50.011838 140184451094336 submission_runner.py:408] Time since start: 60517.72s, 	Step: 119434, 	{'train/accuracy': 0.6364452838897705, 'train/loss': 1.5332109928131104, 'validation/accuracy': 0.5678399801254272, 'validation/loss': 1.8640707731246948, 'validation/num_examples': 50000, 'test/accuracy': 0.4488000273704529, 'test/loss': 2.510444164276123, 'test/num_examples': 10000, 'score': 54660.86072707176, 'total_duration': 60517.71708583832, 'accumulated_submission_time': 54660.86072707176, 'accumulated_eval_time': 5845.860307693481, 'accumulated_logging_time': 4.820557355880737}
I0203 04:35:50.046207 140022518892288 logging_writer.py:48] [119434] accumulated_eval_time=5845.860308, accumulated_logging_time=4.820557, accumulated_submission_time=54660.860727, global_step=119434, preemption_count=0, score=54660.860727, test/accuracy=0.448800, test/loss=2.510444, test/num_examples=10000, total_duration=60517.717086, train/accuracy=0.636445, train/loss=1.533211, validation/accuracy=0.567840, validation/loss=1.864071, validation/num_examples=50000
I0203 04:36:16.925470 140023005427456 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.5207816362380981, loss=2.712691068649292
I0203 04:37:02.888625 140022518892288 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.326940655708313, loss=2.996962070465088
I0203 04:37:48.956381 140023005427456 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.2779327630996704, loss=3.0070302486419678
I0203 04:38:35.177344 140022518892288 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.123733639717102, loss=4.892373085021973
I0203 04:39:21.555309 140023005427456 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.0827558040618896, loss=5.137490272521973
I0203 04:40:07.967206 140022518892288 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.3326506614685059, loss=2.843815326690674
I0203 04:40:54.208260 140023005427456 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.0405712127685547, loss=5.0393829345703125
I0203 04:41:40.676904 140022518892288 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.290709137916565, loss=2.767101764678955
I0203 04:42:27.173155 140023005427456 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.139916181564331, loss=4.409767150878906
I0203 04:42:50.507030 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:43:01.144983 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:43:35.470395 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:43:37.071388 140184451094336 submission_runner.py:408] Time since start: 60984.78s, 	Step: 120352, 	{'train/accuracy': 0.6158202886581421, 'train/loss': 1.6390470266342163, 'validation/accuracy': 0.5762199759483337, 'validation/loss': 1.8413300514221191, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.518050193786621, 'test/num_examples': 10000, 'score': 55081.265146017075, 'total_duration': 60984.77663850784, 'accumulated_submission_time': 55081.265146017075, 'accumulated_eval_time': 5892.424654722214, 'accumulated_logging_time': 4.865030288696289}
I0203 04:43:37.108236 140022518892288 logging_writer.py:48] [120352] accumulated_eval_time=5892.424655, accumulated_logging_time=4.865030, accumulated_submission_time=55081.265146, global_step=120352, preemption_count=0, score=55081.265146, test/accuracy=0.461100, test/loss=2.518050, test/num_examples=10000, total_duration=60984.776639, train/accuracy=0.615820, train/loss=1.639047, validation/accuracy=0.576220, validation/loss=1.841330, validation/num_examples=50000
I0203 04:43:56.706729 140023005427456 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.3339341878890991, loss=2.71877121925354
I0203 04:44:41.326587 140022518892288 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.7329883575439453, loss=2.6556336879730225
I0203 04:45:27.601322 140023005427456 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.0809743404388428, loss=4.5437140464782715
I0203 04:46:13.913198 140022518892288 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.1467339992523193, loss=3.293365478515625
I0203 04:46:59.810655 140023005427456 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.1587315797805786, loss=4.820592880249023
I0203 04:47:45.978450 140022518892288 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.3053113222122192, loss=2.783925771713257
I0203 04:48:31.989922 140023005427456 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.4444935321807861, loss=2.6371078491210938
I0203 04:49:18.263756 140022518892288 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.4023451805114746, loss=3.169978618621826
I0203 04:50:04.633799 140023005427456 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.1149195432662964, loss=3.6569032669067383
I0203 04:50:37.468280 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:50:47.964115 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:51:21.390304 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:51:22.995918 140184451094336 submission_runner.py:408] Time since start: 61450.70s, 	Step: 121273, 	{'train/accuracy': 0.6201562285423279, 'train/loss': 1.615761399269104, 'validation/accuracy': 0.5776799917221069, 'validation/loss': 1.8188271522521973, 'validation/num_examples': 50000, 'test/accuracy': 0.45920002460479736, 'test/loss': 2.49786114692688, 'test/num_examples': 10000, 'score': 55501.56821870804, 'total_duration': 61450.701137304306, 'accumulated_submission_time': 55501.56821870804, 'accumulated_eval_time': 5937.952259302139, 'accumulated_logging_time': 4.911031007766724}
I0203 04:51:23.033885 140022518892288 logging_writer.py:48] [121273] accumulated_eval_time=5937.952259, accumulated_logging_time=4.911031, accumulated_submission_time=55501.568219, global_step=121273, preemption_count=0, score=55501.568219, test/accuracy=0.459200, test/loss=2.497861, test/num_examples=10000, total_duration=61450.701137, train/accuracy=0.620156, train/loss=1.615761, validation/accuracy=0.577680, validation/loss=1.818827, validation/num_examples=50000
I0203 04:51:34.252421 140023005427456 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.3110543489456177, loss=2.8126888275146484
I0203 04:52:18.018428 140022518892288 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.378044605255127, loss=2.972552537918091
I0203 04:53:04.133802 140023005427456 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.233810544013977, loss=2.959787368774414
I0203 04:53:50.098052 140022518892288 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.1780116558074951, loss=4.457621097564697
I0203 04:54:36.005725 140023005427456 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.2131528854370117, loss=3.914201259613037
I0203 04:55:22.292782 140022518892288 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.6631686687469482, loss=2.7473952770233154
I0203 04:56:08.371324 140023005427456 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.322618007659912, loss=2.8954672813415527
I0203 04:56:54.436346 140022518892288 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.3454035520553589, loss=2.6759860515594482
I0203 04:57:40.767216 140023005427456 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.2159355878829956, loss=4.622575283050537
I0203 04:58:23.309468 140184451094336 spec.py:321] Evaluating on the training split.
I0203 04:58:33.586998 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 04:59:12.463502 140184451094336 spec.py:349] Evaluating on the test split.
I0203 04:59:14.060714 140184451094336 submission_runner.py:408] Time since start: 61921.77s, 	Step: 122194, 	{'train/accuracy': 0.6444921493530273, 'train/loss': 1.5002819299697876, 'validation/accuracy': 0.583139955997467, 'validation/loss': 1.7808510065078735, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.44468355178833, 'test/num_examples': 10000, 'score': 55921.785950899124, 'total_duration': 61921.7659611702, 'accumulated_submission_time': 55921.785950899124, 'accumulated_eval_time': 5988.703502893448, 'accumulated_logging_time': 4.95950722694397}
I0203 04:59:14.098725 140022518892288 logging_writer.py:48] [122194] accumulated_eval_time=5988.703503, accumulated_logging_time=4.959507, accumulated_submission_time=55921.785951, global_step=122194, preemption_count=0, score=55921.785951, test/accuracy=0.468800, test/loss=2.444684, test/num_examples=10000, total_duration=61921.765961, train/accuracy=0.644492, train/loss=1.500282, validation/accuracy=0.583140, validation/loss=1.780851, validation/num_examples=50000
I0203 04:59:16.909713 140023005427456 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.5553431510925293, loss=2.645984172821045
I0203 04:59:59.258782 140022518892288 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.3992410898208618, loss=2.7600972652435303
I0203 05:00:45.638044 140023005427456 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.446341872215271, loss=2.5579710006713867
I0203 05:01:31.958242 140022518892288 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.191267490386963, loss=5.000452995300293
I0203 05:02:18.193099 140023005427456 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.3441137075424194, loss=3.458360195159912
I0203 05:03:04.276583 140022518892288 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.3834542036056519, loss=3.532052993774414
I0203 05:03:50.225433 140023005427456 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.2590370178222656, loss=5.10860538482666
I0203 05:04:36.235470 140022518892288 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.1107057332992554, loss=4.464381694793701
I0203 05:05:22.334221 140023005427456 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.1397204399108887, loss=5.056363582611084
I0203 05:06:08.565432 140022518892288 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.3682252168655396, loss=2.7131261825561523
I0203 05:06:14.197481 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:06:24.724831 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:06:57.721668 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:06:59.332879 140184451094336 submission_runner.py:408] Time since start: 62387.04s, 	Step: 123114, 	{'train/accuracy': 0.6229882836341858, 'train/loss': 1.5936886072158813, 'validation/accuracy': 0.5823000073432922, 'validation/loss': 1.7864725589752197, 'validation/num_examples': 50000, 'test/accuracy': 0.46320003271102905, 'test/loss': 2.4657235145568848, 'test/num_examples': 10000, 'score': 56341.82621026039, 'total_duration': 62387.038128614426, 'accumulated_submission_time': 56341.82621026039, 'accumulated_eval_time': 6033.838894367218, 'accumulated_logging_time': 5.008333683013916}
I0203 05:06:59.369295 140023005427456 logging_writer.py:48] [123114] accumulated_eval_time=6033.838894, accumulated_logging_time=5.008334, accumulated_submission_time=56341.826210, global_step=123114, preemption_count=0, score=56341.826210, test/accuracy=0.463200, test/loss=2.465724, test/num_examples=10000, total_duration=62387.038129, train/accuracy=0.622988, train/loss=1.593689, validation/accuracy=0.582300, validation/loss=1.786473, validation/num_examples=50000
I0203 05:07:35.273432 140022518892288 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.3240714073181152, loss=2.8603692054748535
I0203 05:08:21.412642 140023005427456 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.3244980573654175, loss=2.657243490219116
I0203 05:09:07.796055 140022518892288 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.2585824728012085, loss=4.0370001792907715
I0203 05:09:53.816176 140023005427456 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.1705914735794067, loss=3.504263401031494
I0203 05:10:40.145008 140022518892288 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.5711853504180908, loss=2.574671745300293
I0203 05:11:26.377326 140023005427456 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.154552698135376, loss=4.974186897277832
I0203 05:12:13.056180 140022518892288 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.262575626373291, loss=2.704220771789551
I0203 05:12:59.500227 140023005427456 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.236096978187561, loss=3.1713204383850098
I0203 05:13:45.847494 140022518892288 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.6594412326812744, loss=2.6429357528686523
I0203 05:13:59.477712 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:14:10.251961 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:14:46.353422 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:14:47.945540 140184451094336 submission_runner.py:408] Time since start: 62855.65s, 	Step: 124031, 	{'train/accuracy': 0.6308007836341858, 'train/loss': 1.54508376121521, 'validation/accuracy': 0.5869199633598328, 'validation/loss': 1.7575558423995972, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.4124157428741455, 'test/num_examples': 10000, 'score': 56761.87673950195, 'total_duration': 62855.65078186989, 'accumulated_submission_time': 56761.87673950195, 'accumulated_eval_time': 6082.30672454834, 'accumulated_logging_time': 5.055681467056274}
I0203 05:14:47.981614 140023005427456 logging_writer.py:48] [124031] accumulated_eval_time=6082.306725, accumulated_logging_time=5.055681, accumulated_submission_time=56761.876740, global_step=124031, preemption_count=0, score=56761.876740, test/accuracy=0.472200, test/loss=2.412416, test/num_examples=10000, total_duration=62855.650782, train/accuracy=0.630801, train/loss=1.545084, validation/accuracy=0.586920, validation/loss=1.757556, validation/num_examples=50000
I0203 05:15:16.184142 140022518892288 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.221103310585022, loss=5.163552284240723
I0203 05:16:01.967031 140023005427456 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.3493304252624512, loss=2.4904751777648926
I0203 05:16:48.101324 140022518892288 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.1060779094696045, loss=5.126818656921387
I0203 05:17:34.471303 140023005427456 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.3259130716323853, loss=2.8535561561584473
I0203 05:18:20.786233 140022518892288 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.248413324356079, loss=3.460867404937744
I0203 05:19:06.891899 140023005427456 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.5586177110671997, loss=2.642972230911255
I0203 05:19:52.823001 140022518892288 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.256041169166565, loss=3.7658019065856934
I0203 05:20:39.346335 140023005427456 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.1718323230743408, loss=4.122621536254883
I0203 05:21:25.540061 140022518892288 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.3872913122177124, loss=5.219282627105713
I0203 05:21:48.115803 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:21:58.654844 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:22:27.945716 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:22:29.548453 140184451094336 submission_runner.py:408] Time since start: 63317.25s, 	Step: 124950, 	{'train/accuracy': 0.6486718654632568, 'train/loss': 1.460990309715271, 'validation/accuracy': 0.5928199887275696, 'validation/loss': 1.7329621315002441, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.372661590576172, 'test/num_examples': 10000, 'score': 57181.95409989357, 'total_duration': 63317.253702163696, 'accumulated_submission_time': 57181.95409989357, 'accumulated_eval_time': 6123.739371538162, 'accumulated_logging_time': 5.101463317871094}
I0203 05:22:29.582092 140023005427456 logging_writer.py:48] [124950] accumulated_eval_time=6123.739372, accumulated_logging_time=5.101463, accumulated_submission_time=57181.954100, global_step=124950, preemption_count=0, score=57181.954100, test/accuracy=0.479400, test/loss=2.372662, test/num_examples=10000, total_duration=63317.253702, train/accuracy=0.648672, train/loss=1.460990, validation/accuracy=0.592820, validation/loss=1.732962, validation/num_examples=50000
I0203 05:22:49.976227 140022518892288 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.201364517211914, loss=3.829808473587036
I0203 05:23:35.097404 140023005427456 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.1845940351486206, loss=4.593964099884033
I0203 05:24:21.448140 140022518892288 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.318324327468872, loss=3.3717899322509766
I0203 05:25:07.749047 140023005427456 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.3930437564849854, loss=2.539942979812622
I0203 05:25:53.754447 140022518892288 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.2997804880142212, loss=2.9503700733184814
I0203 05:26:39.839451 140023005427456 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.2273207902908325, loss=3.5435304641723633
I0203 05:27:26.120006 140022518892288 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.4162349700927734, loss=2.5365025997161865
I0203 05:28:12.178613 140023005427456 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.4640395641326904, loss=2.362661600112915
I0203 05:28:58.392573 140022518892288 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.125341534614563, loss=3.6103034019470215
I0203 05:29:29.681331 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:29:39.930632 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:30:14.431194 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:30:16.042557 140184451094336 submission_runner.py:408] Time since start: 63783.75s, 	Step: 125869, 	{'train/accuracy': 0.636523425579071, 'train/loss': 1.5250691175460815, 'validation/accuracy': 0.5936599969863892, 'validation/loss': 1.7224498987197876, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.383357286453247, 'test/num_examples': 10000, 'score': 57601.99688029289, 'total_duration': 63783.74777674675, 'accumulated_submission_time': 57601.99688029289, 'accumulated_eval_time': 6170.100564241409, 'accumulated_logging_time': 5.144175052642822}
I0203 05:30:16.082703 140023005427456 logging_writer.py:48] [125869] accumulated_eval_time=6170.100564, accumulated_logging_time=5.144175, accumulated_submission_time=57601.996880, global_step=125869, preemption_count=0, score=57601.996880, test/accuracy=0.481100, test/loss=2.383357, test/num_examples=10000, total_duration=63783.747777, train/accuracy=0.636523, train/loss=1.525069, validation/accuracy=0.593660, validation/loss=1.722450, validation/num_examples=50000
I0203 05:30:28.885425 140022518892288 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.5091913938522339, loss=2.6824963092803955
I0203 05:31:12.562391 140023005427456 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.383379578590393, loss=2.5312955379486084
I0203 05:31:58.533621 140022518892288 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.4143632650375366, loss=2.4471912384033203
I0203 05:32:44.875981 140023005427456 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.4101229906082153, loss=3.0273218154907227
I0203 05:33:30.854105 140022518892288 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.4037004709243774, loss=2.5527048110961914
I0203 05:34:17.126477 140023005427456 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.452763319015503, loss=2.5876216888427734
I0203 05:35:03.342942 140022518892288 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.5458015203475952, loss=2.5276200771331787
I0203 05:35:49.460805 140023005427456 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.3876534700393677, loss=2.53841495513916
I0203 05:36:35.702291 140022518892288 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.2286365032196045, loss=3.0850260257720947
I0203 05:37:16.398459 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:37:26.787626 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:37:58.831654 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:38:00.434667 140184451094336 submission_runner.py:408] Time since start: 64248.14s, 	Step: 126790, 	{'train/accuracy': 0.6359765529632568, 'train/loss': 1.5459275245666504, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.755147933959961, 'validation/num_examples': 50000, 'test/accuracy': 0.4742000102996826, 'test/loss': 2.4011406898498535, 'test/num_examples': 10000, 'score': 58022.25514602661, 'total_duration': 64248.139909505844, 'accumulated_submission_time': 58022.25514602661, 'accumulated_eval_time': 6214.136778354645, 'accumulated_logging_time': 5.194725275039673}
I0203 05:38:00.477080 140023005427456 logging_writer.py:48] [126790] accumulated_eval_time=6214.136778, accumulated_logging_time=5.194725, accumulated_submission_time=58022.255146, global_step=126790, preemption_count=0, score=58022.255146, test/accuracy=0.474200, test/loss=2.401141, test/num_examples=10000, total_duration=64248.139910, train/accuracy=0.635977, train/loss=1.545928, validation/accuracy=0.588980, validation/loss=1.755148, validation/num_examples=50000
I0203 05:38:04.880440 140022518892288 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.517323613166809, loss=2.501760482788086
I0203 05:38:47.126897 140023005427456 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.4258956909179688, loss=2.447618007659912
I0203 05:39:32.925245 140022518892288 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.3130578994750977, loss=2.7659108638763428
I0203 05:40:19.221087 140023005427456 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.3554531335830688, loss=2.6752407550811768
I0203 05:41:05.327986 140022518892288 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.2188092470169067, loss=4.112317085266113
I0203 05:41:51.626596 140023005427456 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.239840030670166, loss=3.165667772293091
I0203 05:42:38.201558 140022518892288 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.6203160285949707, loss=2.539429187774658
I0203 05:43:24.434476 140023005427456 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.5648330450057983, loss=2.63959002494812
I0203 05:44:10.869846 140022518892288 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.7713744640350342, loss=2.627261161804199
I0203 05:44:56.868178 140023005427456 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.4842833280563354, loss=2.4305367469787598
I0203 05:45:00.647085 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:45:11.147577 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:45:46.457010 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:45:48.066128 140184451094336 submission_runner.py:408] Time since start: 64715.77s, 	Step: 127710, 	{'train/accuracy': 0.6518163681030273, 'train/loss': 1.4520666599273682, 'validation/accuracy': 0.600059986114502, 'validation/loss': 1.690841794013977, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.344245195388794, 'test/num_examples': 10000, 'score': 58442.36844062805, 'total_duration': 64715.771369457245, 'accumulated_submission_time': 58442.36844062805, 'accumulated_eval_time': 6261.555802345276, 'accumulated_logging_time': 5.246778249740601}
I0203 05:45:48.101285 140022518892288 logging_writer.py:48] [127710] accumulated_eval_time=6261.555802, accumulated_logging_time=5.246778, accumulated_submission_time=58442.368441, global_step=127710, preemption_count=0, score=58442.368441, test/accuracy=0.484300, test/loss=2.344245, test/num_examples=10000, total_duration=64715.771369, train/accuracy=0.651816, train/loss=1.452067, validation/accuracy=0.600060, validation/loss=1.690842, validation/num_examples=50000
I0203 05:46:25.581106 140023005427456 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.5741068124771118, loss=2.7850193977355957
I0203 05:47:11.551307 140022518892288 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.2260819673538208, loss=4.692438125610352
I0203 05:47:57.925020 140023005427456 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.5391960144042969, loss=2.525390625
I0203 05:48:44.171943 140022518892288 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.486088752746582, loss=2.542616128921509
I0203 05:49:30.451620 140023005427456 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.4184075593948364, loss=2.9103384017944336
I0203 05:50:16.642001 140022518892288 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.2489957809448242, loss=4.0709075927734375
I0203 05:51:02.943704 140023005427456 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.382063388824463, loss=5.017092227935791
I0203 05:51:49.073534 140022518892288 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.2986030578613281, loss=3.554898262023926
I0203 05:52:35.431637 140023005427456 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.4358524084091187, loss=4.921856880187988
I0203 05:52:48.553392 140184451094336 spec.py:321] Evaluating on the training split.
I0203 05:52:59.250710 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 05:53:31.941037 140184451094336 spec.py:349] Evaluating on the test split.
I0203 05:53:33.542618 140184451094336 submission_runner.py:408] Time since start: 65181.25s, 	Step: 128630, 	{'train/accuracy': 0.6659374833106995, 'train/loss': 1.415810465812683, 'validation/accuracy': 0.5946199893951416, 'validation/loss': 1.7279531955718994, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.3771519660949707, 'test/num_examples': 10000, 'score': 58862.76211476326, 'total_duration': 65181.24786877632, 'accumulated_submission_time': 58862.76211476326, 'accumulated_eval_time': 6306.545022726059, 'accumulated_logging_time': 5.292807579040527}
I0203 05:53:33.578051 140022518892288 logging_writer.py:48] [128630] accumulated_eval_time=6306.545023, accumulated_logging_time=5.292808, accumulated_submission_time=58862.762115, global_step=128630, preemption_count=0, score=58862.762115, test/accuracy=0.479200, test/loss=2.377152, test/num_examples=10000, total_duration=65181.247869, train/accuracy=0.665937, train/loss=1.415810, validation/accuracy=0.594620, validation/loss=1.727953, validation/num_examples=50000
I0203 05:54:01.986398 140023005427456 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.5893231630325317, loss=2.6615982055664062
I0203 05:54:48.011252 140022518892288 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.5947694778442383, loss=2.6221842765808105
I0203 05:55:34.591636 140023005427456 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.5383377075195312, loss=2.575566291809082
I0203 05:56:20.820351 140022518892288 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.3416935205459595, loss=2.7726378440856934
I0203 05:57:07.085671 140023005427456 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.4859048128128052, loss=2.913010597229004
I0203 05:57:53.203256 140022518892288 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.4473990201950073, loss=2.4312195777893066
I0203 05:58:39.282828 140023005427456 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.6697474718093872, loss=2.4747228622436523
I0203 05:59:25.505997 140022518892288 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.374136209487915, loss=5.067826271057129
I0203 06:00:11.685324 140023005427456 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.2593547105789185, loss=4.708836555480957
I0203 06:00:33.954019 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:00:45.105147 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:01:15.878331 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:01:17.479253 140184451094336 submission_runner.py:408] Time since start: 65645.18s, 	Step: 129550, 	{'train/accuracy': 0.6496288776397705, 'train/loss': 1.4651296138763428, 'validation/accuracy': 0.6078599691390991, 'validation/loss': 1.6644974946975708, 'validation/num_examples': 50000, 'test/accuracy': 0.4895000159740448, 'test/loss': 2.340092182159424, 'test/num_examples': 10000, 'score': 59283.08157444, 'total_duration': 65645.18449640274, 'accumulated_submission_time': 59283.08157444, 'accumulated_eval_time': 6350.070232391357, 'accumulated_logging_time': 5.337663650512695}
I0203 06:01:17.517947 140022518892288 logging_writer.py:48] [129550] accumulated_eval_time=6350.070232, accumulated_logging_time=5.337664, accumulated_submission_time=59283.081574, global_step=129550, preemption_count=0, score=59283.081574, test/accuracy=0.489500, test/loss=2.340092, test/num_examples=10000, total_duration=65645.184496, train/accuracy=0.649629, train/loss=1.465130, validation/accuracy=0.607860, validation/loss=1.664497, validation/num_examples=50000
I0203 06:01:37.926379 140023005427456 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.6056406497955322, loss=2.5689809322357178
I0203 06:02:23.158754 140022518892288 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.5374490022659302, loss=2.502574920654297
I0203 06:03:09.593642 140023005427456 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.3521894216537476, loss=3.496725082397461
I0203 06:03:55.928915 140022518892288 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.6991729736328125, loss=2.4788057804107666
I0203 06:04:42.079123 140023005427456 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.4290690422058105, loss=2.5796027183532715
I0203 06:05:28.322239 140022518892288 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.3940578699111938, loss=2.4223427772521973
I0203 06:06:14.534732 140023005427456 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.4984056949615479, loss=2.5407278537750244
I0203 06:07:00.596112 140022518892288 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.3126287460327148, loss=3.551161766052246
I0203 06:07:47.141701 140023005427456 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.4255179166793823, loss=3.1708343029022217
I0203 06:08:17.714384 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:08:28.277518 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:09:01.731355 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:09:03.331045 140184451094336 submission_runner.py:408] Time since start: 66111.04s, 	Step: 130468, 	{'train/accuracy': 0.6561523079872131, 'train/loss': 1.4382413625717163, 'validation/accuracy': 0.6042400002479553, 'validation/loss': 1.6798323392868042, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.341444730758667, 'test/num_examples': 10000, 'score': 59703.20712137222, 'total_duration': 66111.03629493713, 'accumulated_submission_time': 59703.20712137222, 'accumulated_eval_time': 6395.686897754669, 'accumulated_logging_time': 5.386809349060059}
I0203 06:09:03.365530 140022518892288 logging_writer.py:48] [130468] accumulated_eval_time=6395.686898, accumulated_logging_time=5.386809, accumulated_submission_time=59703.207121, global_step=130468, preemption_count=0, score=59703.207121, test/accuracy=0.487900, test/loss=2.341445, test/num_examples=10000, total_duration=66111.036295, train/accuracy=0.656152, train/loss=1.438241, validation/accuracy=0.604240, validation/loss=1.679832, validation/num_examples=50000
I0203 06:09:16.582999 140023005427456 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.5798063278198242, loss=2.887385606765747
I0203 06:10:00.180756 140022518892288 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.376036524772644, loss=2.590390682220459
I0203 06:10:46.328224 140023005427456 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.3084113597869873, loss=4.172232627868652
I0203 06:11:32.572783 140022518892288 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.3075275421142578, loss=3.0644359588623047
I0203 06:12:18.662133 140023005427456 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.5847989320755005, loss=2.6087183952331543
I0203 06:13:04.917279 140022518892288 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.8603085279464722, loss=2.563950538635254
I0203 06:13:51.160518 140023005427456 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.5718036890029907, loss=2.5957629680633545
I0203 06:14:37.262421 140022518892288 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.3886594772338867, loss=5.069003105163574
I0203 06:15:23.321505 140023005427456 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.5345852375030518, loss=3.244307279586792
I0203 06:16:03.712924 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:16:14.174431 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:16:48.212404 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:16:49.826013 140184451094336 submission_runner.py:408] Time since start: 66577.53s, 	Step: 131389, 	{'train/accuracy': 0.6699999570846558, 'train/loss': 1.370593547821045, 'validation/accuracy': 0.6080399751663208, 'validation/loss': 1.6591289043426514, 'validation/num_examples': 50000, 'test/accuracy': 0.48350003361701965, 'test/loss': 2.3345508575439453, 'test/num_examples': 10000, 'score': 60123.49499297142, 'total_duration': 66577.5312511921, 'accumulated_submission_time': 60123.49499297142, 'accumulated_eval_time': 6441.799973964691, 'accumulated_logging_time': 5.43379020690918}
I0203 06:16:49.862650 140022518892288 logging_writer.py:48] [131389] accumulated_eval_time=6441.799974, accumulated_logging_time=5.433790, accumulated_submission_time=60123.494993, global_step=131389, preemption_count=0, score=60123.494993, test/accuracy=0.483500, test/loss=2.334551, test/num_examples=10000, total_duration=66577.531251, train/accuracy=0.670000, train/loss=1.370594, validation/accuracy=0.608040, validation/loss=1.659129, validation/num_examples=50000
I0203 06:16:54.681376 140023005427456 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.3400219678878784, loss=4.258767604827881
I0203 06:17:36.968176 140022518892288 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.3326566219329834, loss=4.745294570922852
I0203 06:18:23.246358 140023005427456 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.695831060409546, loss=2.493682622909546
I0203 06:19:10.046532 140022518892288 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.5649691820144653, loss=2.4864678382873535
I0203 06:19:56.213595 140023005427456 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.3414095640182495, loss=4.9454545974731445
I0203 06:20:42.591773 140022518892288 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.3360750675201416, loss=4.925507545471191
I0203 06:21:28.904635 140023005427456 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.4503347873687744, loss=3.6063625812530518
I0203 06:22:15.554502 140022518892288 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.5532679557800293, loss=4.068293571472168
I0203 06:23:01.572418 140023005427456 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.5649179220199585, loss=2.602248430252075
I0203 06:23:47.751345 140022518892288 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.569669246673584, loss=2.644737720489502
I0203 06:23:50.222500 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:24:01.623444 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:24:33.793981 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:24:35.403086 140184451094336 submission_runner.py:408] Time since start: 67043.11s, 	Step: 132307, 	{'train/accuracy': 0.6594530940055847, 'train/loss': 1.4391076564788818, 'validation/accuracy': 0.6137199997901917, 'validation/loss': 1.645622968673706, 'validation/num_examples': 50000, 'test/accuracy': 0.49220001697540283, 'test/loss': 2.302058219909668, 'test/num_examples': 10000, 'score': 60543.795784950256, 'total_duration': 67043.1083316803, 'accumulated_submission_time': 60543.795784950256, 'accumulated_eval_time': 6486.980547428131, 'accumulated_logging_time': 5.481791257858276}
I0203 06:24:35.440391 140023005427456 logging_writer.py:48] [132307] accumulated_eval_time=6486.980547, accumulated_logging_time=5.481791, accumulated_submission_time=60543.795785, global_step=132307, preemption_count=0, score=60543.795785, test/accuracy=0.492200, test/loss=2.302058, test/num_examples=10000, total_duration=67043.108332, train/accuracy=0.659453, train/loss=1.439108, validation/accuracy=0.613720, validation/loss=1.645623, validation/num_examples=50000
I0203 06:25:14.600912 140022518892288 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.2543507814407349, loss=3.264655590057373
I0203 06:26:00.652442 140023005427456 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.4490984678268433, loss=2.4801549911499023
I0203 06:26:47.163909 140022518892288 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.6032720804214478, loss=2.6720633506774902
I0203 06:27:33.477334 140023005427456 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.4157499074935913, loss=3.214712142944336
I0203 06:28:19.856696 140022518892288 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.5181241035461426, loss=2.70600962638855
I0203 06:29:06.479494 140023005427456 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.664061427116394, loss=2.370588779449463
I0203 06:29:52.512460 140022518892288 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.7301822900772095, loss=2.4984772205352783
I0203 06:30:38.885910 140023005427456 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.3526828289031982, loss=3.511720657348633
I0203 06:31:25.098204 140022518892288 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.4454803466796875, loss=3.0552902221679688
I0203 06:31:35.620440 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:31:46.200649 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:32:21.765238 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:32:23.376087 140184451094336 submission_runner.py:408] Time since start: 67511.08s, 	Step: 133224, 	{'train/accuracy': 0.6636718511581421, 'train/loss': 1.4158997535705566, 'validation/accuracy': 0.6115800142288208, 'validation/loss': 1.6641314029693604, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.308748245239258, 'test/num_examples': 10000, 'score': 60963.91841840744, 'total_duration': 67511.08133149147, 'accumulated_submission_time': 60963.91841840744, 'accumulated_eval_time': 6534.7361924648285, 'accumulated_logging_time': 5.529500961303711}
I0203 06:32:23.411411 140023005427456 logging_writer.py:48] [133224] accumulated_eval_time=6534.736192, accumulated_logging_time=5.529501, accumulated_submission_time=60963.918418, global_step=133224, preemption_count=0, score=60963.918418, test/accuracy=0.494600, test/loss=2.308748, test/num_examples=10000, total_duration=67511.081331, train/accuracy=0.663672, train/loss=1.415900, validation/accuracy=0.611580, validation/loss=1.664131, validation/num_examples=50000
I0203 06:32:54.780267 140022518892288 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.4590047597885132, loss=3.18673038482666
I0203 06:33:40.912017 140023005427456 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.632229208946228, loss=2.501920461654663
I0203 06:34:27.464316 140022518892288 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.6805495023727417, loss=2.446525812149048
I0203 06:35:13.806980 140023005427456 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.7519161701202393, loss=2.720372438430786
I0203 06:35:59.811978 140022518892288 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.6788899898529053, loss=2.540860652923584
I0203 06:36:46.063133 140023005427456 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.6185201406478882, loss=2.405355930328369
I0203 06:37:32.499854 140022518892288 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.6734447479248047, loss=2.4107205867767334
I0203 06:38:18.649451 140023005427456 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.2584969997406006, loss=3.3235511779785156
I0203 06:39:04.817951 140022518892288 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.4825434684753418, loss=3.044649362564087
I0203 06:39:23.576153 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:39:34.106589 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:40:11.087320 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:40:12.687799 140184451094336 submission_runner.py:408] Time since start: 67980.39s, 	Step: 134142, 	{'train/accuracy': 0.6744531393051147, 'train/loss': 1.3474888801574707, 'validation/accuracy': 0.6168599724769592, 'validation/loss': 1.62572181224823, 'validation/num_examples': 50000, 'test/accuracy': 0.4912000298500061, 'test/loss': 2.289562702178955, 'test/num_examples': 10000, 'score': 61384.026109695435, 'total_duration': 67980.39304852486, 'accumulated_submission_time': 61384.026109695435, 'accumulated_eval_time': 6583.84783744812, 'accumulated_logging_time': 5.574039936065674}
I0203 06:40:12.726172 140023005427456 logging_writer.py:48] [134142] accumulated_eval_time=6583.847837, accumulated_logging_time=5.574040, accumulated_submission_time=61384.026110, global_step=134142, preemption_count=0, score=61384.026110, test/accuracy=0.491200, test/loss=2.289563, test/num_examples=10000, total_duration=67980.393049, train/accuracy=0.674453, train/loss=1.347489, validation/accuracy=0.616860, validation/loss=1.625722, validation/num_examples=50000
I0203 06:40:36.332238 140022518892288 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.656584620475769, loss=2.3579609394073486
I0203 06:41:21.572851 140023005427456 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.3853039741516113, loss=3.830495595932007
I0203 06:42:08.273815 140022518892288 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.4224343299865723, loss=4.371245861053467
I0203 06:42:54.601007 140023005427456 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.2874125242233276, loss=4.274526596069336
I0203 06:43:40.775382 140022518892288 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.2567617893218994, loss=4.649980068206787
I0203 06:44:27.105592 140023005427456 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.3661378622055054, loss=4.832693099975586
I0203 06:45:13.325358 140022518892288 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.8011995553970337, loss=2.3596508502960205
I0203 06:45:59.147508 140023005427456 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.4599815607070923, loss=2.737549066543579
I0203 06:46:45.470478 140022518892288 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.5121076107025146, loss=2.2074034214019775
I0203 06:47:12.994553 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:47:23.425747 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:47:59.429759 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:48:01.030608 140184451094336 submission_runner.py:408] Time since start: 68448.74s, 	Step: 135061, 	{'train/accuracy': 0.6664648056030273, 'train/loss': 1.4257535934448242, 'validation/accuracy': 0.6180999875068665, 'validation/loss': 1.6305259466171265, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.297254800796509, 'test/num_examples': 10000, 'score': 61804.2365424633, 'total_duration': 68448.73585152626, 'accumulated_submission_time': 61804.2365424633, 'accumulated_eval_time': 6631.8838946819305, 'accumulated_logging_time': 5.623882532119751}
I0203 06:48:01.069312 140023005427456 logging_writer.py:48] [135061] accumulated_eval_time=6631.883895, accumulated_logging_time=5.623883, accumulated_submission_time=61804.236542, global_step=135061, preemption_count=0, score=61804.236542, test/accuracy=0.493800, test/loss=2.297255, test/num_examples=10000, total_duration=68448.735852, train/accuracy=0.666465, train/loss=1.425754, validation/accuracy=0.618100, validation/loss=1.630526, validation/num_examples=50000
I0203 06:48:17.067190 140022518892288 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.6241304874420166, loss=2.445786237716675
I0203 06:49:01.502621 140023005427456 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.5398905277252197, loss=2.771988868713379
I0203 06:49:47.623176 140022518892288 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.6553428173065186, loss=2.3114778995513916
I0203 06:50:34.232334 140023005427456 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.5172702074050903, loss=3.9305601119995117
I0203 06:51:20.410888 140022518892288 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.4105631113052368, loss=2.696824550628662
I0203 06:52:06.928691 140023005427456 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.6861653327941895, loss=2.4773342609405518
I0203 06:52:53.232330 140022518892288 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.4274625778198242, loss=3.2085201740264893
I0203 06:53:40.812585 140023005427456 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.577893614768982, loss=2.440577268600464
I0203 06:54:27.268777 140022518892288 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.5666192770004272, loss=2.29719614982605
I0203 06:55:01.097135 140184451094336 spec.py:321] Evaluating on the training split.
I0203 06:55:11.841596 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 06:55:45.101822 140184451094336 spec.py:349] Evaluating on the test split.
I0203 06:55:46.711464 140184451094336 submission_runner.py:408] Time since start: 68914.42s, 	Step: 135975, 	{'train/accuracy': 0.6676562428474426, 'train/loss': 1.3916078805923462, 'validation/accuracy': 0.6185399889945984, 'validation/loss': 1.6243489980697632, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.278455972671509, 'test/num_examples': 10000, 'score': 62224.20766711235, 'total_duration': 68914.41670441628, 'accumulated_submission_time': 62224.20766711235, 'accumulated_eval_time': 6677.498216867447, 'accumulated_logging_time': 5.672069072723389}
I0203 06:55:46.752216 140023005427456 logging_writer.py:48] [135975] accumulated_eval_time=6677.498217, accumulated_logging_time=5.672069, accumulated_submission_time=62224.207667, global_step=135975, preemption_count=0, score=62224.207667, test/accuracy=0.500000, test/loss=2.278456, test/num_examples=10000, total_duration=68914.416704, train/accuracy=0.667656, train/loss=1.391608, validation/accuracy=0.618540, validation/loss=1.624349, validation/num_examples=50000
I0203 06:55:57.156239 140022518892288 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.5124523639678955, loss=2.7350516319274902
I0203 06:56:40.823298 140023005427456 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.4709655046463013, loss=2.9210872650146484
I0203 06:57:27.383226 140022518892288 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.318171501159668, loss=4.172743320465088
I0203 06:58:13.935872 140023005427456 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.7588791847229004, loss=2.3180930614471436
I0203 06:59:00.300782 140022518892288 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.5872164964675903, loss=4.497219085693359
I0203 06:59:46.866502 140023005427456 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.5351372957229614, loss=2.6539509296417236
I0203 07:00:33.372726 140022518892288 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.5463473796844482, loss=2.8948915004730225
I0203 07:01:19.667672 140023005427456 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.6342573165893555, loss=2.3556509017944336
I0203 07:02:06.291963 140022518892288 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.8010989427566528, loss=2.395181894302368
I0203 07:02:46.792539 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:02:57.389097 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:03:32.727258 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:03:34.337535 140184451094336 submission_runner.py:408] Time since start: 69382.04s, 	Step: 136889, 	{'train/accuracy': 0.6796679496765137, 'train/loss': 1.3299959897994995, 'validation/accuracy': 0.6280199885368347, 'validation/loss': 1.5712664127349854, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.23734450340271, 'test/num_examples': 10000, 'score': 62644.19136214256, 'total_duration': 69382.04277920723, 'accumulated_submission_time': 62644.19136214256, 'accumulated_eval_time': 6725.043194055557, 'accumulated_logging_time': 5.722181797027588}
I0203 07:03:34.376240 140023005427456 logging_writer.py:48] [136889] accumulated_eval_time=6725.043194, accumulated_logging_time=5.722182, accumulated_submission_time=62644.191362, global_step=136889, preemption_count=0, score=62644.191362, test/accuracy=0.502100, test/loss=2.237345, test/num_examples=10000, total_duration=69382.042779, train/accuracy=0.679668, train/loss=1.329996, validation/accuracy=0.628020, validation/loss=1.571266, validation/num_examples=50000
I0203 07:03:39.176150 140022518892288 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.383180856704712, loss=4.302092552185059
I0203 07:04:21.938812 140023005427456 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.696706771850586, loss=2.384061336517334
I0203 07:05:08.046809 140022518892288 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.7734692096710205, loss=2.2696568965911865
I0203 07:05:54.522958 140023005427456 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.5967882871627808, loss=2.9694437980651855
I0203 07:06:40.688536 140022518892288 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.4768024682998657, loss=3.873322010040283
I0203 07:07:26.798109 140023005427456 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.6451081037521362, loss=2.6127705574035645
I0203 07:08:12.987973 140022518892288 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.6616559028625488, loss=2.3657026290893555
I0203 07:08:59.160878 140023005427456 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.6443169116973877, loss=2.909083604812622
I0203 07:09:45.683167 140022518892288 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.3786267042160034, loss=4.731078624725342
I0203 07:10:31.763268 140023005427456 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.684122920036316, loss=2.479771375656128
I0203 07:10:34.729416 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:10:45.348514 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:11:20.752135 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:11:22.364699 140184451094336 submission_runner.py:408] Time since start: 69850.07s, 	Step: 137808, 	{'train/accuracy': 0.6735937595367432, 'train/loss': 1.3464230298995972, 'validation/accuracy': 0.6296399831771851, 'validation/loss': 1.558292031288147, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2221529483795166, 'test/num_examples': 10000, 'score': 63064.48580121994, 'total_duration': 69850.06992220879, 'accumulated_submission_time': 63064.48580121994, 'accumulated_eval_time': 6772.678442955017, 'accumulated_logging_time': 5.771515846252441}
I0203 07:11:22.413971 140022518892288 logging_writer.py:48] [137808] accumulated_eval_time=6772.678443, accumulated_logging_time=5.771516, accumulated_submission_time=63064.485801, global_step=137808, preemption_count=0, score=63064.485801, test/accuracy=0.506000, test/loss=2.222153, test/num_examples=10000, total_duration=69850.069922, train/accuracy=0.673594, train/loss=1.346423, validation/accuracy=0.629640, validation/loss=1.558292, validation/num_examples=50000
I0203 07:12:01.127097 140023005427456 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.4429839849472046, loss=4.919626712799072
I0203 07:12:47.116596 140022518892288 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.4664812088012695, loss=3.4144842624664307
I0203 07:13:33.349444 140023005427456 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.5844392776489258, loss=2.6123194694519043
I0203 07:14:19.696885 140022518892288 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.699286937713623, loss=4.734616756439209
I0203 07:15:06.149644 140023005427456 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.404658317565918, loss=3.461885929107666
I0203 07:15:52.272256 140022518892288 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.531960129737854, loss=3.1134071350097656
I0203 07:16:38.516243 140023005427456 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.662807822227478, loss=2.5864038467407227
I0203 07:17:24.807551 140022518892288 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.549210548400879, loss=2.706819772720337
I0203 07:18:10.984286 140023005427456 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.4778333902359009, loss=3.622282028198242
I0203 07:18:22.574088 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:18:33.392958 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:19:09.038759 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:19:10.645791 140184451094336 submission_runner.py:408] Time since start: 70318.35s, 	Step: 138727, 	{'train/accuracy': 0.6751171946525574, 'train/loss': 1.3565669059753418, 'validation/accuracy': 0.628600001335144, 'validation/loss': 1.5680512189865112, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.221627950668335, 'test/num_examples': 10000, 'score': 63484.58924078941, 'total_duration': 70318.35100626945, 'accumulated_submission_time': 63484.58924078941, 'accumulated_eval_time': 6820.7501039505005, 'accumulated_logging_time': 5.830377578735352}
I0203 07:19:10.692107 140022518892288 logging_writer.py:48] [138727] accumulated_eval_time=6820.750104, accumulated_logging_time=5.830378, accumulated_submission_time=63484.589241, global_step=138727, preemption_count=0, score=63484.589241, test/accuracy=0.509600, test/loss=2.221628, test/num_examples=10000, total_duration=70318.351006, train/accuracy=0.675117, train/loss=1.356567, validation/accuracy=0.628600, validation/loss=1.568051, validation/num_examples=50000
I0203 07:19:40.823142 140023005427456 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.6280412673950195, loss=2.5245449542999268
I0203 07:20:27.082729 140022518892288 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.416357159614563, loss=4.115372657775879
I0203 07:21:13.491010 140023005427456 logging_writer.py:48] [139000] global_step=139000, grad_norm=1.5870943069458008, loss=2.2910871505737305
I0203 07:21:59.820550 140022518892288 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.5751328468322754, loss=2.94384503364563
I0203 07:22:46.261441 140023005427456 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.6883856058120728, loss=2.4054312705993652
I0203 07:23:32.585002 140022518892288 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.5489717721939087, loss=2.457207679748535
I0203 07:24:18.733932 140023005427456 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.6661747694015503, loss=2.3995347023010254
I0203 07:25:05.226975 140022518892288 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.6387618780136108, loss=4.482386589050293
I0203 07:25:51.440081 140023005427456 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.686570644378662, loss=2.4117019176483154
I0203 07:26:11.118584 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:26:21.855508 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:26:55.958866 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:26:57.556241 140184451094336 submission_runner.py:408] Time since start: 70785.26s, 	Step: 139644, 	{'train/accuracy': 0.6910156011581421, 'train/loss': 1.2635481357574463, 'validation/accuracy': 0.6337400078773499, 'validation/loss': 1.5286023616790771, 'validation/num_examples': 50000, 'test/accuracy': 0.5097000002861023, 'test/loss': 2.1982059478759766, 'test/num_examples': 10000, 'score': 63904.95752739906, 'total_duration': 70785.26147270203, 'accumulated_submission_time': 63904.95752739906, 'accumulated_eval_time': 6867.18776845932, 'accumulated_logging_time': 5.888091802597046}
I0203 07:26:57.591937 140022518892288 logging_writer.py:48] [139644] accumulated_eval_time=6867.187768, accumulated_logging_time=5.888092, accumulated_submission_time=63904.957527, global_step=139644, preemption_count=0, score=63904.957527, test/accuracy=0.509700, test/loss=2.198206, test/num_examples=10000, total_duration=70785.261473, train/accuracy=0.691016, train/loss=1.263548, validation/accuracy=0.633740, validation/loss=1.528602, validation/num_examples=50000
I0203 07:27:20.398263 140023005427456 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.806325912475586, loss=2.2460362911224365
I0203 07:28:06.094298 140022518892288 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.5543336868286133, loss=3.045938730239868
I0203 07:28:52.371869 140023005427456 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.7493746280670166, loss=2.391914129257202
I0203 07:29:38.808830 140022518892288 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.6783784627914429, loss=2.26564884185791
I0203 07:30:25.321112 140023005427456 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.7029229402542114, loss=2.3083367347717285
I0203 07:31:11.812124 140022518892288 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.7783275842666626, loss=2.443023681640625
I0203 07:31:58.228834 140023005427456 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.6554561853408813, loss=2.2589573860168457
I0203 07:32:44.590305 140022518892288 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.6801375150680542, loss=2.3504393100738525
I0203 07:33:30.795258 140023005427456 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.8507304191589355, loss=2.3507182598114014
I0203 07:33:57.779086 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:34:08.515516 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:34:42.676169 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:34:44.278018 140184451094336 submission_runner.py:408] Time since start: 71251.98s, 	Step: 140560, 	{'train/accuracy': 0.7136914134025574, 'train/loss': 1.185178518295288, 'validation/accuracy': 0.6374599933624268, 'validation/loss': 1.5168884992599487, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.171229839324951, 'test/num_examples': 10000, 'score': 64325.088150024414, 'total_duration': 71251.98325324059, 'accumulated_submission_time': 64325.088150024414, 'accumulated_eval_time': 6913.686674833298, 'accumulated_logging_time': 5.933101177215576}
I0203 07:34:44.314101 140022518892288 logging_writer.py:48] [140560] accumulated_eval_time=6913.686675, accumulated_logging_time=5.933101, accumulated_submission_time=64325.088150, global_step=140560, preemption_count=0, score=64325.088150, test/accuracy=0.517000, test/loss=2.171230, test/num_examples=10000, total_duration=71251.983253, train/accuracy=0.713691, train/loss=1.185179, validation/accuracy=0.637460, validation/loss=1.516888, validation/num_examples=50000
I0203 07:35:00.712959 140023005427456 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.626235008239746, loss=3.004952907562256
I0203 07:35:45.132696 140022518892288 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.909719467163086, loss=2.3064160346984863
I0203 07:36:31.115061 140023005427456 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.6688936948776245, loss=2.446429967880249
I0203 07:37:17.331497 140022518892288 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.6273682117462158, loss=3.3424503803253174
I0203 07:38:03.699327 140023005427456 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.9277675151824951, loss=2.2967255115509033
I0203 07:38:49.820760 140022518892288 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.720682978630066, loss=2.1544525623321533
I0203 07:39:35.911239 140023005427456 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.6842732429504395, loss=2.7415878772735596
I0203 07:40:22.220574 140022518892288 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.7470752000808716, loss=2.245375156402588
I0203 07:41:08.575873 140023005427456 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.5210787057876587, loss=3.564650535583496
I0203 07:41:44.566803 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:41:55.338363 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:42:29.304384 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:42:30.915280 140184451094336 submission_runner.py:408] Time since start: 71718.62s, 	Step: 141479, 	{'train/accuracy': 0.6903125047683716, 'train/loss': 1.27427077293396, 'validation/accuracy': 0.6414600014686584, 'validation/loss': 1.5009756088256836, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.166109800338745, 'test/num_examples': 10000, 'score': 64745.282984018326, 'total_duration': 71718.62048172951, 'accumulated_submission_time': 64745.282984018326, 'accumulated_eval_time': 6960.035108566284, 'accumulated_logging_time': 5.978979587554932}
I0203 07:42:30.963483 140022518892288 logging_writer.py:48] [141479] accumulated_eval_time=6960.035109, accumulated_logging_time=5.978980, accumulated_submission_time=64745.282984, global_step=141479, preemption_count=0, score=64745.282984, test/accuracy=0.517400, test/loss=2.166110, test/num_examples=10000, total_duration=71718.620482, train/accuracy=0.690313, train/loss=1.274271, validation/accuracy=0.641460, validation/loss=1.500976, validation/num_examples=50000
I0203 07:42:39.768953 140023005427456 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.501688838005066, loss=4.095826625823975
I0203 07:43:23.147052 140022518892288 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.9147862195968628, loss=2.2859954833984375
I0203 07:44:09.333819 140023005427456 logging_writer.py:48] [141700] global_step=141700, grad_norm=1.741292953491211, loss=2.3056888580322266
I0203 07:44:55.847455 140022518892288 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.7111952304840088, loss=3.01701283454895
I0203 07:45:42.183056 140023005427456 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.901157021522522, loss=2.346273183822632
I0203 07:46:28.648378 140022518892288 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.5426949262619019, loss=2.8353419303894043
I0203 07:47:15.068487 140023005427456 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.748877763748169, loss=2.3437318801879883
I0203 07:48:01.377681 140022518892288 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.6810083389282227, loss=2.5700626373291016
I0203 07:48:47.991295 140023005427456 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.8216384649276733, loss=2.44549560546875
I0203 07:49:31.272883 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:49:41.850173 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:50:13.756166 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:50:15.352904 140184451094336 submission_runner.py:408] Time since start: 72183.06s, 	Step: 142395, 	{'train/accuracy': 0.6958788633346558, 'train/loss': 1.2445529699325562, 'validation/accuracy': 0.6466599702835083, 'validation/loss': 1.4790695905685425, 'validation/num_examples': 50000, 'test/accuracy': 0.5193000435829163, 'test/loss': 2.146132469177246, 'test/num_examples': 10000, 'score': 65165.53315329552, 'total_duration': 72183.05815005302, 'accumulated_submission_time': 65165.53315329552, 'accumulated_eval_time': 7004.11513710022, 'accumulated_logging_time': 6.038940191268921}
I0203 07:50:15.392833 140022518892288 logging_writer.py:48] [142395] accumulated_eval_time=7004.115137, accumulated_logging_time=6.038940, accumulated_submission_time=65165.533153, global_step=142395, preemption_count=0, score=65165.533153, test/accuracy=0.519300, test/loss=2.146132, test/num_examples=10000, total_duration=72183.058150, train/accuracy=0.695879, train/loss=1.244553, validation/accuracy=0.646660, validation/loss=1.479070, validation/num_examples=50000
I0203 07:50:17.798727 140023005427456 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.7676798105239868, loss=4.817152976989746
I0203 07:51:00.473774 140022518892288 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.5165629386901855, loss=3.153566598892212
I0203 07:51:46.772949 140023005427456 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.5666754245758057, loss=3.2755110263824463
I0203 07:52:33.453711 140022518892288 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.7523348331451416, loss=2.2314670085906982
I0203 07:53:19.379158 140023005427456 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.073230028152466, loss=2.294524669647217
I0203 07:54:05.736623 140022518892288 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.8422214984893799, loss=2.199256658554077
I0203 07:54:51.821386 140023005427456 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.6121622323989868, loss=3.503124713897705
I0203 07:55:38.260792 140022518892288 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.77138352394104, loss=4.839400768280029
I0203 07:56:24.575503 140023005427456 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.498684048652649, loss=3.113569498062134
I0203 07:57:10.840049 140022518892288 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.526820421218872, loss=4.218379974365234
I0203 07:57:15.555514 140184451094336 spec.py:321] Evaluating on the training split.
I0203 07:57:26.194339 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 07:58:01.309195 140184451094336 spec.py:349] Evaluating on the test split.
I0203 07:58:02.926671 140184451094336 submission_runner.py:408] Time since start: 72650.63s, 	Step: 143312, 	{'train/accuracy': 0.7118749618530273, 'train/loss': 1.1865516901016235, 'validation/accuracy': 0.6448000073432922, 'validation/loss': 1.483980417251587, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1349503993988037, 'test/num_examples': 10000, 'score': 65585.63843154907, 'total_duration': 72650.63191390038, 'accumulated_submission_time': 65585.63843154907, 'accumulated_eval_time': 7051.486275434494, 'accumulated_logging_time': 6.0886406898498535}
I0203 07:58:02.967681 140023005427456 logging_writer.py:48] [143312] accumulated_eval_time=7051.486275, accumulated_logging_time=6.088641, accumulated_submission_time=65585.638432, global_step=143312, preemption_count=0, score=65585.638432, test/accuracy=0.523800, test/loss=2.134950, test/num_examples=10000, total_duration=72650.631914, train/accuracy=0.711875, train/loss=1.186552, validation/accuracy=0.644800, validation/loss=1.483980, validation/num_examples=50000
I0203 07:58:39.615202 140022518892288 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.9999923706054688, loss=2.3125076293945312
I0203 07:59:25.903562 140023005427456 logging_writer.py:48] [143500] global_step=143500, grad_norm=1.6087169647216797, loss=2.786712408065796
I0203 08:00:12.538029 140022518892288 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.6815860271453857, loss=2.7745697498321533
I0203 08:00:58.922313 140023005427456 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.5820831060409546, loss=2.666008472442627
I0203 08:01:45.513747 140022518892288 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.1124136447906494, loss=4.851924419403076
I0203 08:02:32.046910 140023005427456 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.5850275754928589, loss=4.019384384155273
I0203 08:03:18.433716 140022518892288 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.0094451904296875, loss=2.2351393699645996
I0203 08:04:04.829086 140023005427456 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.6105847358703613, loss=4.690445899963379
I0203 08:04:51.095965 140022518892288 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.6017417907714844, loss=3.226315498352051
I0203 08:05:03.384944 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:05:13.948889 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:05:47.182430 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:05:48.780970 140184451094336 submission_runner.py:408] Time since start: 73116.49s, 	Step: 144228, 	{'train/accuracy': 0.6990429759025574, 'train/loss': 1.2320014238357544, 'validation/accuracy': 0.6526199579238892, 'validation/loss': 1.4438382387161255, 'validation/num_examples': 50000, 'test/accuracy': 0.5264000296592712, 'test/loss': 2.108189821243286, 'test/num_examples': 10000, 'score': 66005.99985575676, 'total_duration': 73116.48621463776, 'accumulated_submission_time': 66005.99985575676, 'accumulated_eval_time': 7096.882295846939, 'accumulated_logging_time': 6.138426780700684}
I0203 08:05:48.820262 140023005427456 logging_writer.py:48] [144228] accumulated_eval_time=7096.882296, accumulated_logging_time=6.138427, accumulated_submission_time=66005.999856, global_step=144228, preemption_count=0, score=66005.999856, test/accuracy=0.526400, test/loss=2.108190, test/num_examples=10000, total_duration=73116.486215, train/accuracy=0.699043, train/loss=1.232001, validation/accuracy=0.652620, validation/loss=1.443838, validation/num_examples=50000
I0203 08:06:18.372233 140022518892288 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.8783817291259766, loss=2.212674617767334
I0203 08:07:04.521909 140023005427456 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.793403148651123, loss=2.5208606719970703
I0203 08:07:50.744312 140022518892288 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.8328444957733154, loss=2.292382001876831
I0203 08:08:36.917692 140023005427456 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.8002771139144897, loss=2.101994752883911
I0203 08:09:23.038669 140022518892288 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.6608213186264038, loss=2.244345188140869
I0203 08:10:09.355718 140023005427456 logging_writer.py:48] [144800] global_step=144800, grad_norm=1.7058289051055908, loss=4.329807281494141
I0203 08:10:55.176601 140022518892288 logging_writer.py:48] [144900] global_step=144900, grad_norm=1.880678415298462, loss=2.182189464569092
I0203 08:11:41.720368 140023005427456 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.828401803970337, loss=4.281785011291504
I0203 08:12:27.769577 140022518892288 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.6620171070098877, loss=3.999638795852661
I0203 08:12:48.802009 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:12:59.132672 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:13:34.551349 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:13:36.146215 140184451094336 submission_runner.py:408] Time since start: 73583.85s, 	Step: 145147, 	{'train/accuracy': 0.7080078125, 'train/loss': 1.2053229808807373, 'validation/accuracy': 0.6554399728775024, 'validation/loss': 1.4488352537155151, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.1001083850860596, 'test/num_examples': 10000, 'score': 66425.925065279, 'total_duration': 73583.85144233704, 'accumulated_submission_time': 66425.925065279, 'accumulated_eval_time': 7144.226463794708, 'accumulated_logging_time': 6.186929225921631}
I0203 08:13:36.187121 140023005427456 logging_writer.py:48] [145147] accumulated_eval_time=7144.226464, accumulated_logging_time=6.186929, accumulated_submission_time=66425.925065, global_step=145147, preemption_count=0, score=66425.925065, test/accuracy=0.532100, test/loss=2.100108, test/num_examples=10000, total_duration=73583.851442, train/accuracy=0.708008, train/loss=1.205323, validation/accuracy=0.655440, validation/loss=1.448835, validation/num_examples=50000
I0203 08:13:57.803840 140022518892288 logging_writer.py:48] [145200] global_step=145200, grad_norm=1.7644301652908325, loss=3.2683024406433105
I0203 08:14:42.956815 140023005427456 logging_writer.py:48] [145300] global_step=145300, grad_norm=1.6706981658935547, loss=4.576995849609375
I0203 08:15:29.218150 140022518892288 logging_writer.py:48] [145400] global_step=145400, grad_norm=1.7751548290252686, loss=2.505671501159668
I0203 08:16:15.487544 140023005427456 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.5429359674453735, loss=3.343317747116089
I0203 08:17:01.747320 140022518892288 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.0574920177459717, loss=2.049333095550537
I0203 08:17:48.238705 140023005427456 logging_writer.py:48] [145700] global_step=145700, grad_norm=1.7980949878692627, loss=3.4418046474456787
I0203 08:18:34.354841 140022518892288 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.9040955305099487, loss=2.7260963916778564
I0203 08:19:20.453344 140023005427456 logging_writer.py:48] [145900] global_step=145900, grad_norm=1.9696656465530396, loss=2.177903652191162
I0203 08:20:06.703547 140022518892288 logging_writer.py:48] [146000] global_step=146000, grad_norm=1.8706032037734985, loss=4.175295352935791
I0203 08:20:36.418420 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:20:47.342783 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:21:23.230567 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:21:24.830801 140184451094336 submission_runner.py:408] Time since start: 74052.54s, 	Step: 146066, 	{'train/accuracy': 0.7203124761581421, 'train/loss': 1.154877781867981, 'validation/accuracy': 0.6532799601554871, 'validation/loss': 1.4429513216018677, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.0999159812927246, 'test/num_examples': 10000, 'score': 66846.09749627113, 'total_duration': 74052.53604912758, 'accumulated_submission_time': 66846.09749627113, 'accumulated_eval_time': 7192.638860940933, 'accumulated_logging_time': 6.239060163497925}
I0203 08:21:24.867809 140023005427456 logging_writer.py:48] [146066] accumulated_eval_time=7192.638861, accumulated_logging_time=6.239060, accumulated_submission_time=66846.097496, global_step=146066, preemption_count=0, score=66846.097496, test/accuracy=0.530000, test/loss=2.099916, test/num_examples=10000, total_duration=74052.536049, train/accuracy=0.720312, train/loss=1.154878, validation/accuracy=0.653280, validation/loss=1.442951, validation/num_examples=50000
I0203 08:21:38.858129 140022518892288 logging_writer.py:48] [146100] global_step=146100, grad_norm=1.9273217916488647, loss=2.5020952224731445
I0203 08:22:22.890808 140023005427456 logging_writer.py:48] [146200] global_step=146200, grad_norm=1.9810831546783447, loss=2.2144546508789062
I0203 08:23:09.346855 140022518892288 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.6563163995742798, loss=3.540200710296631
I0203 08:23:55.771568 140023005427456 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.674161672592163, loss=2.5487682819366455
I0203 08:24:41.875880 140022518892288 logging_writer.py:48] [146500] global_step=146500, grad_norm=1.755233645439148, loss=4.227907180786133
I0203 08:25:28.424559 140023005427456 logging_writer.py:48] [146600] global_step=146600, grad_norm=1.8898768424987793, loss=3.6972086429595947
I0203 08:26:14.790413 140022518892288 logging_writer.py:48] [146700] global_step=146700, grad_norm=1.8678312301635742, loss=2.415565013885498
I0203 08:27:00.870655 140023005427456 logging_writer.py:48] [146800] global_step=146800, grad_norm=1.6571812629699707, loss=4.512309551239014
I0203 08:27:47.175624 140022518892288 logging_writer.py:48] [146900] global_step=146900, grad_norm=1.643485188484192, loss=3.598937749862671
I0203 08:28:25.218411 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:28:35.722644 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:29:10.914614 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:29:12.511470 140184451094336 submission_runner.py:408] Time since start: 74520.22s, 	Step: 146984, 	{'train/accuracy': 0.708789050579071, 'train/loss': 1.1932083368301392, 'validation/accuracy': 0.6591599583625793, 'validation/loss': 1.4202784299850464, 'validation/num_examples': 50000, 'test/accuracy': 0.5374000072479248, 'test/loss': 2.074611186981201, 'test/num_examples': 10000, 'score': 67266.39178800583, 'total_duration': 74520.21671199799, 'accumulated_submission_time': 67266.39178800583, 'accumulated_eval_time': 7239.931909561157, 'accumulated_logging_time': 6.285334825515747}
I0203 08:29:12.557553 140023005427456 logging_writer.py:48] [146984] accumulated_eval_time=7239.931910, accumulated_logging_time=6.285335, accumulated_submission_time=67266.391788, global_step=146984, preemption_count=0, score=67266.391788, test/accuracy=0.537400, test/loss=2.074611, test/num_examples=10000, total_duration=74520.216712, train/accuracy=0.708789, train/loss=1.193208, validation/accuracy=0.659160, validation/loss=1.420278, validation/num_examples=50000
I0203 08:29:19.373187 140022518892288 logging_writer.py:48] [147000] global_step=147000, grad_norm=1.941804051399231, loss=2.1901209354400635
I0203 08:30:02.465291 140023005427456 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.0150535106658936, loss=2.1592278480529785
I0203 08:30:48.658150 140022518892288 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.175597906112671, loss=2.366475820541382
I0203 08:31:35.218584 140023005427456 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.9053717851638794, loss=2.283924102783203
I0203 08:32:21.951673 140022518892288 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.919514775276184, loss=2.465456485748291
I0203 08:33:07.708679 140023005427456 logging_writer.py:48] [147500] global_step=147500, grad_norm=1.8864479064941406, loss=2.192714214324951
I0203 08:33:53.811600 140022518892288 logging_writer.py:48] [147600] global_step=147600, grad_norm=1.785772442817688, loss=2.361833333969116
I0203 08:34:40.220550 140023005427456 logging_writer.py:48] [147700] global_step=147700, grad_norm=1.897047519683838, loss=2.1825826168060303
I0203 08:35:26.400508 140022518892288 logging_writer.py:48] [147800] global_step=147800, grad_norm=1.785704493522644, loss=2.910128355026245
I0203 08:36:12.941129 140023005427456 logging_writer.py:48] [147900] global_step=147900, grad_norm=1.7698215246200562, loss=2.1214356422424316
I0203 08:36:12.956521 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:36:23.740692 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:36:58.271031 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:36:59.877356 140184451094336 submission_runner.py:408] Time since start: 74987.58s, 	Step: 147901, 	{'train/accuracy': 0.7182226181030273, 'train/loss': 1.1583360433578491, 'validation/accuracy': 0.6647399663925171, 'validation/loss': 1.4021943807601929, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.045015811920166, 'test/num_examples': 10000, 'score': 67686.7302069664, 'total_duration': 74987.58259272575, 'accumulated_submission_time': 67686.7302069664, 'accumulated_eval_time': 7286.852725744247, 'accumulated_logging_time': 6.344372987747192}
I0203 08:36:59.916646 140022518892288 logging_writer.py:48] [147901] accumulated_eval_time=7286.852726, accumulated_logging_time=6.344373, accumulated_submission_time=67686.730207, global_step=147901, preemption_count=0, score=67686.730207, test/accuracy=0.541800, test/loss=2.045016, test/num_examples=10000, total_duration=74987.582593, train/accuracy=0.718223, train/loss=1.158336, validation/accuracy=0.664740, validation/loss=1.402194, validation/num_examples=50000
I0203 08:37:41.963819 140023005427456 logging_writer.py:48] [148000] global_step=148000, grad_norm=1.770506501197815, loss=3.3066930770874023
I0203 08:38:28.001515 140022518892288 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.0378334522247314, loss=2.2996902465820312
I0203 08:39:14.673247 140023005427456 logging_writer.py:48] [148200] global_step=148200, grad_norm=1.7809339761734009, loss=2.0619754791259766
I0203 08:40:00.952102 140022518892288 logging_writer.py:48] [148300] global_step=148300, grad_norm=1.8944538831710815, loss=2.1637020111083984
I0203 08:40:47.192753 140023005427456 logging_writer.py:48] [148400] global_step=148400, grad_norm=1.9386601448059082, loss=2.2319564819335938
I0203 08:41:33.770660 140022518892288 logging_writer.py:48] [148500] global_step=148500, grad_norm=1.9088268280029297, loss=2.1739022731781006
I0203 08:42:20.081867 140023005427456 logging_writer.py:48] [148600] global_step=148600, grad_norm=1.796180009841919, loss=3.7943716049194336
I0203 08:43:06.555668 140022518892288 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.2939565181732178, loss=2.0901217460632324
I0203 08:43:52.758530 140023005427456 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.228801727294922, loss=2.203028678894043
I0203 08:44:00.318125 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:44:10.905585 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:44:46.754782 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:44:48.355887 140184451094336 submission_runner.py:408] Time since start: 75456.06s, 	Step: 148818, 	{'train/accuracy': 0.7275585532188416, 'train/loss': 1.1043344736099243, 'validation/accuracy': 0.6648199558258057, 'validation/loss': 1.3845757246017456, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.0354931354522705, 'test/num_examples': 10000, 'score': 68107.07111549377, 'total_duration': 75456.06113243103, 'accumulated_submission_time': 68107.07111549377, 'accumulated_eval_time': 7334.890476465225, 'accumulated_logging_time': 6.396288871765137}
I0203 08:44:48.395587 140022518892288 logging_writer.py:48] [148818] accumulated_eval_time=7334.890476, accumulated_logging_time=6.396289, accumulated_submission_time=68107.071115, global_step=148818, preemption_count=0, score=68107.071115, test/accuracy=0.541200, test/loss=2.035493, test/num_examples=10000, total_duration=75456.061132, train/accuracy=0.727559, train/loss=1.104334, validation/accuracy=0.664820, validation/loss=1.384576, validation/num_examples=50000
I0203 08:45:22.896880 140023005427456 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.0282108783721924, loss=2.6309561729431152
I0203 08:46:08.945352 140022518892288 logging_writer.py:48] [149000] global_step=149000, grad_norm=1.914412260055542, loss=4.562434196472168
I0203 08:46:55.371006 140023005427456 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.0412893295288086, loss=2.1102569103240967
I0203 08:47:41.768208 140022518892288 logging_writer.py:48] [149200] global_step=149200, grad_norm=1.9774574041366577, loss=2.137662172317505
I0203 08:48:28.228207 140023005427456 logging_writer.py:48] [149300] global_step=149300, grad_norm=1.9597162008285522, loss=2.092853307723999
I0203 08:49:14.619209 140022518892288 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.084606170654297, loss=1.9654970169067383
I0203 08:50:00.893891 140023005427456 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.199660539627075, loss=2.207705497741699
I0203 08:50:47.314145 140022518892288 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.0471811294555664, loss=2.1692705154418945
I0203 08:51:33.814265 140023005427456 logging_writer.py:48] [149700] global_step=149700, grad_norm=1.9529718160629272, loss=2.3306353092193604
I0203 08:51:48.450253 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:51:58.813105 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 08:52:30.855453 140184451094336 spec.py:349] Evaluating on the test split.
I0203 08:52:32.449550 140184451094336 submission_runner.py:408] Time since start: 75920.15s, 	Step: 149733, 	{'train/accuracy': 0.7219140529632568, 'train/loss': 1.1348050832748413, 'validation/accuracy': 0.6680399775505066, 'validation/loss': 1.3798613548278809, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 2.020998001098633, 'test/num_examples': 10000, 'score': 68527.06919646263, 'total_duration': 75920.15477275848, 'accumulated_submission_time': 68527.06919646263, 'accumulated_eval_time': 7378.8897659778595, 'accumulated_logging_time': 6.444957733154297}
I0203 08:52:32.489772 140022518892288 logging_writer.py:48] [149733] accumulated_eval_time=7378.889766, accumulated_logging_time=6.444958, accumulated_submission_time=68527.069196, global_step=149733, preemption_count=0, score=68527.069196, test/accuracy=0.551100, test/loss=2.020998, test/num_examples=10000, total_duration=75920.154773, train/accuracy=0.721914, train/loss=1.134805, validation/accuracy=0.668040, validation/loss=1.379861, validation/num_examples=50000
I0203 08:52:59.891975 140023005427456 logging_writer.py:48] [149800] global_step=149800, grad_norm=1.868193507194519, loss=4.632335662841797
I0203 08:53:45.688749 140022518892288 logging_writer.py:48] [149900] global_step=149900, grad_norm=1.8698546886444092, loss=3.998859405517578
I0203 08:54:31.849447 140023005427456 logging_writer.py:48] [150000] global_step=150000, grad_norm=1.756872534751892, loss=3.1244001388549805
I0203 08:55:18.163711 140022518892288 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.188565969467163, loss=2.2302908897399902
I0203 08:56:04.706837 140023005427456 logging_writer.py:48] [150200] global_step=150200, grad_norm=1.8360567092895508, loss=3.861994504928589
I0203 08:56:50.976055 140022518892288 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.014299154281616, loss=2.9240498542785645
I0203 08:57:37.298465 140023005427456 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.189297914505005, loss=2.3451457023620605
I0203 08:58:23.730792 140022518892288 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.1263561248779297, loss=2.21873140335083
I0203 08:59:10.248499 140023005427456 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.099679708480835, loss=2.48838472366333
I0203 08:59:32.892406 140184451094336 spec.py:321] Evaluating on the training split.
I0203 08:59:43.303003 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:00:21.097840 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:00:22.697797 140184451094336 submission_runner.py:408] Time since start: 76390.40s, 	Step: 150651, 	{'train/accuracy': 0.7286132574081421, 'train/loss': 1.118777871131897, 'validation/accuracy': 0.6726199984550476, 'validation/loss': 1.3689404726028442, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.0204763412475586, 'test/num_examples': 10000, 'score': 68947.41572165489, 'total_duration': 76390.40303850174, 'accumulated_submission_time': 68947.41572165489, 'accumulated_eval_time': 7428.695145845413, 'accumulated_logging_time': 6.494152307510376}
I0203 09:00:22.739825 140022518892288 logging_writer.py:48] [150651] accumulated_eval_time=7428.695146, accumulated_logging_time=6.494152, accumulated_submission_time=68947.415722, global_step=150651, preemption_count=0, score=68947.415722, test/accuracy=0.547100, test/loss=2.020476, test/num_examples=10000, total_duration=76390.403039, train/accuracy=0.728613, train/loss=1.118778, validation/accuracy=0.672620, validation/loss=1.368940, validation/num_examples=50000
I0203 09:00:42.742664 140023005427456 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.108393669128418, loss=2.1087865829467773
I0203 09:01:27.730547 140022518892288 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.1997737884521484, loss=2.199023962020874
I0203 09:02:14.116700 140023005427456 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.0133707523345947, loss=2.258803129196167
I0203 09:03:00.238492 140022518892288 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.1596415042877197, loss=2.4836647510528564
I0203 09:03:46.355885 140023005427456 logging_writer.py:48] [151100] global_step=151100, grad_norm=1.8828672170639038, loss=2.3366687297821045
I0203 09:04:32.866508 140022518892288 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.1898140907287598, loss=4.51201057434082
I0203 09:05:19.157011 140023005427456 logging_writer.py:48] [151300] global_step=151300, grad_norm=1.8837038278579712, loss=3.6079814434051514
I0203 09:06:05.492488 140022518892288 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.0564193725585938, loss=4.447518348693848
I0203 09:06:51.577081 140023005427456 logging_writer.py:48] [151500] global_step=151500, grad_norm=1.800275206565857, loss=4.529128074645996
I0203 09:07:22.762355 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:07:33.431916 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:08:08.081456 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:08:09.678866 140184451094336 submission_runner.py:408] Time since start: 76857.38s, 	Step: 151569, 	{'train/accuracy': 0.7353515625, 'train/loss': 1.0825318098068237, 'validation/accuracy': 0.6772199869155884, 'validation/loss': 1.343247890472412, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 1.9848430156707764, 'test/num_examples': 10000, 'score': 69367.38053894043, 'total_duration': 76857.38411259651, 'accumulated_submission_time': 69367.38053894043, 'accumulated_eval_time': 7475.611652612686, 'accumulated_logging_time': 6.545987844467163}
I0203 09:08:09.730130 140022518892288 logging_writer.py:48] [151569] accumulated_eval_time=7475.611653, accumulated_logging_time=6.545988, accumulated_submission_time=69367.380539, global_step=151569, preemption_count=0, score=69367.380539, test/accuracy=0.552100, test/loss=1.984843, test/num_examples=10000, total_duration=76857.384113, train/accuracy=0.735352, train/loss=1.082532, validation/accuracy=0.677220, validation/loss=1.343248, validation/num_examples=50000
I0203 09:08:22.540300 140023005427456 logging_writer.py:48] [151600] global_step=151600, grad_norm=1.9691529273986816, loss=2.6949565410614014
I0203 09:09:06.446800 140022518892288 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.363018751144409, loss=2.177851438522339
I0203 09:09:52.413719 140023005427456 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.1567511558532715, loss=2.0907182693481445
I0203 09:10:39.126827 140022518892288 logging_writer.py:48] [151900] global_step=151900, grad_norm=1.9006884098052979, loss=2.244368076324463
I0203 09:11:25.450424 140023005427456 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.3813114166259766, loss=2.133634090423584
I0203 09:12:11.817322 140022518892288 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.076101779937744, loss=2.0088672637939453
I0203 09:12:57.969066 140023005427456 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.0352866649627686, loss=2.3691883087158203
I0203 09:13:44.160483 140022518892288 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.3574330806732178, loss=1.9758899211883545
I0203 09:14:30.616782 140023005427456 logging_writer.py:48] [152400] global_step=152400, grad_norm=1.9270892143249512, loss=3.629143238067627
I0203 09:15:09.764455 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:15:20.261769 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:15:54.916629 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:15:56.517165 140184451094336 submission_runner.py:408] Time since start: 77324.22s, 	Step: 152486, 	{'train/accuracy': 0.7512304782867432, 'train/loss': 1.0053696632385254, 'validation/accuracy': 0.6732400059700012, 'validation/loss': 1.3476532697677612, 'validation/num_examples': 50000, 'test/accuracy': 0.5507000088691711, 'test/loss': 1.9955780506134033, 'test/num_examples': 10000, 'score': 69787.35775566101, 'total_duration': 77324.2224123478, 'accumulated_submission_time': 69787.35775566101, 'accumulated_eval_time': 7522.364356279373, 'accumulated_logging_time': 6.606665372848511}
I0203 09:15:56.560336 140022518892288 logging_writer.py:48] [152486] accumulated_eval_time=7522.364356, accumulated_logging_time=6.606665, accumulated_submission_time=69787.357756, global_step=152486, preemption_count=0, score=69787.357756, test/accuracy=0.550700, test/loss=1.995578, test/num_examples=10000, total_duration=77324.222412, train/accuracy=0.751230, train/loss=1.005370, validation/accuracy=0.673240, validation/loss=1.347653, validation/num_examples=50000
I0203 09:16:02.569765 140023005427456 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.2759382724761963, loss=2.255582094192505
I0203 09:16:45.629372 140022518892288 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.172429323196411, loss=2.099654197692871
I0203 09:17:31.761941 140023005427456 logging_writer.py:48] [152700] global_step=152700, grad_norm=1.8783775568008423, loss=4.226180076599121
I0203 09:18:18.049949 140022518892288 logging_writer.py:48] [152800] global_step=152800, grad_norm=1.9570683240890503, loss=4.0407795906066895
I0203 09:19:04.228377 140023005427456 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.1616013050079346, loss=2.066030740737915
I0203 09:19:50.423085 140022518892288 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.2209267616271973, loss=1.9552268981933594
I0203 09:20:36.601861 140023005427456 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.123004913330078, loss=1.9367570877075195
I0203 09:21:22.777560 140022518892288 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.18265700340271, loss=2.027587652206421
I0203 09:22:09.225798 140023005427456 logging_writer.py:48] [153300] global_step=153300, grad_norm=1.9139057397842407, loss=4.062768459320068
I0203 09:22:55.384435 140022518892288 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.1956374645233154, loss=2.195690393447876
I0203 09:22:56.948151 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:23:07.455031 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:23:42.845741 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:23:44.443068 140184451094336 submission_runner.py:408] Time since start: 77792.15s, 	Step: 153405, 	{'train/accuracy': 0.7347655892372131, 'train/loss': 1.0813597440719604, 'validation/accuracy': 0.6788199543952942, 'validation/loss': 1.333372950553894, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 1.9810631275177002, 'test/num_examples': 10000, 'score': 70207.6869328022, 'total_duration': 77792.1483130455, 'accumulated_submission_time': 70207.6869328022, 'accumulated_eval_time': 7569.859260797501, 'accumulated_logging_time': 6.660884857177734}
I0203 09:23:44.486022 140023005427456 logging_writer.py:48] [153405] accumulated_eval_time=7569.859261, accumulated_logging_time=6.660885, accumulated_submission_time=70207.686933, global_step=153405, preemption_count=0, score=70207.686933, test/accuracy=0.548300, test/loss=1.981063, test/num_examples=10000, total_duration=77792.148313, train/accuracy=0.734766, train/loss=1.081360, validation/accuracy=0.678820, validation/loss=1.333373, validation/num_examples=50000
I0203 09:24:24.445402 140022518892288 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.1434218883514404, loss=2.6692302227020264
I0203 09:25:10.499709 140023005427456 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.1085562705993652, loss=1.9906792640686035
I0203 09:25:56.840601 140022518892288 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.196582794189453, loss=2.08186411857605
I0203 09:26:43.013617 140023005427456 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.109894037246704, loss=2.498067855834961
I0203 09:27:29.423100 140022518892288 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.2495486736297607, loss=2.0526719093322754
I0203 09:28:15.700932 140023005427456 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.2696735858917236, loss=2.053940773010254
I0203 09:29:02.067150 140022518892288 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.0125458240509033, loss=3.138363838195801
I0203 09:29:48.013309 140023005427456 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.1380889415740967, loss=1.9298045635223389
I0203 09:30:34.270376 140022518892288 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.212185859680176, loss=2.0460641384124756
I0203 09:30:44.525261 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:30:55.128838 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:31:30.195696 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:31:31.797059 140184451094336 submission_runner.py:408] Time since start: 78259.50s, 	Step: 154324, 	{'train/accuracy': 0.7437695264816284, 'train/loss': 1.0343624353408813, 'validation/accuracy': 0.6834799647331238, 'validation/loss': 1.3101791143417358, 'validation/num_examples': 50000, 'test/accuracy': 0.5578000545501709, 'test/loss': 1.9628030061721802, 'test/num_examples': 10000, 'score': 70627.66973996162, 'total_duration': 78259.50230240822, 'accumulated_submission_time': 70627.66973996162, 'accumulated_eval_time': 7617.13103890419, 'accumulated_logging_time': 6.713019609451294}
I0203 09:31:31.840418 140023005427456 logging_writer.py:48] [154324] accumulated_eval_time=7617.131039, accumulated_logging_time=6.713020, accumulated_submission_time=70627.669740, global_step=154324, preemption_count=0, score=70627.669740, test/accuracy=0.557800, test/loss=1.962803, test/num_examples=10000, total_duration=78259.502302, train/accuracy=0.743770, train/loss=1.034362, validation/accuracy=0.683480, validation/loss=1.310179, validation/num_examples=50000
I0203 09:32:03.304381 140022518892288 logging_writer.py:48] [154400] global_step=154400, grad_norm=1.963979959487915, loss=3.9109127521514893
I0203 09:32:49.253883 140023005427456 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.088660717010498, loss=3.793501377105713
I0203 09:33:35.344840 140022518892288 logging_writer.py:48] [154600] global_step=154600, grad_norm=1.9905060529708862, loss=2.6210246086120605
I0203 09:34:21.537056 140023005427456 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.3314738273620605, loss=3.6267166137695312
I0203 09:35:07.770396 140022518892288 logging_writer.py:48] [154800] global_step=154800, grad_norm=1.9740432500839233, loss=2.550757884979248
I0203 09:35:53.860078 140023005427456 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.1389193534851074, loss=4.579461574554443
I0203 09:36:40.303631 140022518892288 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.2998263835906982, loss=1.9165712594985962
I0203 09:37:26.592564 140023005427456 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.207261085510254, loss=2.999807357788086
I0203 09:38:12.901681 140022518892288 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.3000800609588623, loss=2.0759572982788086
I0203 09:38:32.001505 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:38:43.793034 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:39:18.207771 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:39:19.831630 140184451094336 submission_runner.py:408] Time since start: 78727.54s, 	Step: 155243, 	{'train/accuracy': 0.7542773485183716, 'train/loss': 1.0230635404586792, 'validation/accuracy': 0.6821399927139282, 'validation/loss': 1.3291398286819458, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 1.9734231233596802, 'test/num_examples': 10000, 'score': 71047.77196288109, 'total_duration': 78727.53687787056, 'accumulated_submission_time': 71047.77196288109, 'accumulated_eval_time': 7664.961159944534, 'accumulated_logging_time': 6.767343282699585}
I0203 09:39:19.870871 140023005427456 logging_writer.py:48] [155243] accumulated_eval_time=7664.961160, accumulated_logging_time=6.767343, accumulated_submission_time=71047.771963, global_step=155243, preemption_count=0, score=71047.771963, test/accuracy=0.554900, test/loss=1.973423, test/num_examples=10000, total_duration=78727.536878, train/accuracy=0.754277, train/loss=1.023064, validation/accuracy=0.682140, validation/loss=1.329140, validation/num_examples=50000
I0203 09:39:43.074334 140022518892288 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.170017719268799, loss=4.5132036209106445
I0203 09:40:28.700160 140023005427456 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.2445454597473145, loss=3.6271133422851562
I0203 09:41:14.960017 140022518892288 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.198768138885498, loss=1.9490861892700195
I0203 09:42:01.249820 140023005427456 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.272458076477051, loss=2.0920286178588867
I0203 09:42:47.337375 140022518892288 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.337689161300659, loss=2.1437714099884033
I0203 09:43:33.432424 140023005427456 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.062239170074463, loss=3.7038075923919678
I0203 09:44:19.507705 140022518892288 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.3066353797912598, loss=2.467412233352661
I0203 09:45:05.733711 140023005427456 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.112962007522583, loss=4.011956214904785
I0203 09:45:51.844158 140022518892288 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.094862222671509, loss=2.212153196334839
I0203 09:46:20.023378 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:46:30.526262 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:47:02.886168 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:47:04.509949 140184451094336 submission_runner.py:408] Time since start: 79192.22s, 	Step: 156162, 	{'train/accuracy': 0.7413867115974426, 'train/loss': 1.0416297912597656, 'validation/accuracy': 0.6865999698638916, 'validation/loss': 1.2964109182357788, 'validation/num_examples': 50000, 'test/accuracy': 0.5647000074386597, 'test/loss': 1.9309802055358887, 'test/num_examples': 10000, 'score': 71467.86416912079, 'total_duration': 79192.2151761055, 'accumulated_submission_time': 71467.86416912079, 'accumulated_eval_time': 7709.447716712952, 'accumulated_logging_time': 6.819774866104126}
I0203 09:47:04.559004 140023005427456 logging_writer.py:48] [156162] accumulated_eval_time=7709.447717, accumulated_logging_time=6.819775, accumulated_submission_time=71467.864169, global_step=156162, preemption_count=0, score=71467.864169, test/accuracy=0.564700, test/loss=1.930980, test/num_examples=10000, total_duration=79192.215176, train/accuracy=0.741387, train/loss=1.041630, validation/accuracy=0.686600, validation/loss=1.296411, validation/num_examples=50000
I0203 09:47:20.172501 140022518892288 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.32210373878479, loss=2.0212759971618652
I0203 09:48:04.664757 140023005427456 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.19156813621521, loss=2.66434907913208
I0203 09:48:51.055466 140022518892288 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.16144061088562, loss=1.825968623161316
I0203 09:49:37.141197 140023005427456 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.0693299770355225, loss=3.816718578338623
I0203 09:50:23.414006 140022518892288 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.0339856147766113, loss=3.5002593994140625
I0203 09:51:09.622916 140023005427456 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.6321933269500732, loss=4.456592082977295
I0203 09:51:55.872581 140022518892288 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.2638328075408936, loss=1.9315221309661865
I0203 09:52:42.183343 140023005427456 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.3557968139648438, loss=3.1525254249572754
I0203 09:53:28.472194 140022518892288 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.5319504737854004, loss=4.439879894256592
I0203 09:54:04.726992 140184451094336 spec.py:321] Evaluating on the training split.
I0203 09:54:15.077580 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 09:54:50.665766 140184451094336 spec.py:349] Evaluating on the test split.
I0203 09:54:52.259460 140184451094336 submission_runner.py:408] Time since start: 79659.96s, 	Step: 157080, 	{'train/accuracy': 0.7487695217132568, 'train/loss': 1.0085744857788086, 'validation/accuracy': 0.6894800066947937, 'validation/loss': 1.2726929187774658, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.9182018041610718, 'test/num_examples': 10000, 'score': 71887.97418117523, 'total_duration': 79659.96469688416, 'accumulated_submission_time': 71887.97418117523, 'accumulated_eval_time': 7756.980162143707, 'accumulated_logging_time': 6.879199504852295}
I0203 09:54:52.303218 140023005427456 logging_writer.py:48] [157080] accumulated_eval_time=7756.980162, accumulated_logging_time=6.879200, accumulated_submission_time=71887.974181, global_step=157080, preemption_count=0, score=71887.974181, test/accuracy=0.567500, test/loss=1.918202, test/num_examples=10000, total_duration=79659.964697, train/accuracy=0.748770, train/loss=1.008574, validation/accuracy=0.689480, validation/loss=1.272693, validation/num_examples=50000
I0203 09:55:00.717264 140022518892288 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.433353900909424, loss=1.8931150436401367
I0203 09:55:43.914799 140023005427456 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.334404468536377, loss=1.94524085521698
I0203 09:56:30.296485 140022518892288 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.0654242038726807, loss=3.5083980560302734
I0203 09:57:17.300176 140023005427456 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.3404483795166016, loss=2.8176348209381104
I0203 09:58:03.414746 140022518892288 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.2163004875183105, loss=2.944413661956787
I0203 09:58:49.733003 140023005427456 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.2665462493896484, loss=2.375967264175415
I0203 09:59:36.387857 140022518892288 logging_writer.py:48] [157700] global_step=157700, grad_norm=1.9970782995224, loss=3.3859622478485107
I0203 10:00:22.734487 140023005427456 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.3954412937164307, loss=1.8816702365875244
I0203 10:01:08.978848 140022518892288 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.450881004333496, loss=4.406538009643555
I0203 10:01:52.321960 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:02:02.807392 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:02:35.045639 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:02:36.668468 140184451094336 submission_runner.py:408] Time since start: 80124.37s, 	Step: 157995, 	{'train/accuracy': 0.7559570074081421, 'train/loss': 0.9801447987556458, 'validation/accuracy': 0.6914199590682983, 'validation/loss': 1.2745487689971924, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 1.9162464141845703, 'test/num_examples': 10000, 'score': 72307.93619275093, 'total_duration': 80124.37371206284, 'accumulated_submission_time': 72307.93619275093, 'accumulated_eval_time': 7801.3266661167145, 'accumulated_logging_time': 6.932545900344849}
I0203 10:02:36.719055 140023005427456 logging_writer.py:48] [157995] accumulated_eval_time=7801.326666, accumulated_logging_time=6.932546, accumulated_submission_time=72307.936193, global_step=157995, preemption_count=0, score=72307.936193, test/accuracy=0.567100, test/loss=1.916246, test/num_examples=10000, total_duration=80124.373712, train/accuracy=0.755957, train/loss=0.980145, validation/accuracy=0.691420, validation/loss=1.274549, validation/num_examples=50000
I0203 10:02:39.109215 140022518892288 logging_writer.py:48] [158000] global_step=158000, grad_norm=1.9783254861831665, loss=3.077651262283325
I0203 10:03:21.422561 140023005427456 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.5208237171173096, loss=2.064791679382324
I0203 10:04:07.269890 140022518892288 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.4122796058654785, loss=4.394179821014404
I0203 10:04:53.564327 140023005427456 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.286250114440918, loss=1.9466444253921509
I0203 10:05:39.796819 140022518892288 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.002260208129883, loss=3.444323778152466
I0203 10:06:25.950645 140023005427456 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.3372459411621094, loss=4.338556289672852
I0203 10:07:12.033460 140022518892288 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.5921478271484375, loss=2.278991937637329
I0203 10:07:57.998054 140023005427456 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.5660183429718018, loss=2.045930862426758
I0203 10:08:44.146447 140022518892288 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.334017038345337, loss=1.885373592376709
I0203 10:09:30.751955 140023005427456 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.394516944885254, loss=1.9118633270263672
I0203 10:09:37.010842 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:09:47.354344 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:10:21.705069 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:10:23.307746 140184451094336 submission_runner.py:408] Time since start: 80591.01s, 	Step: 158915, 	{'train/accuracy': 0.7525194883346558, 'train/loss': 1.00764000415802, 'validation/accuracy': 0.6936399936676025, 'validation/loss': 1.2615045309066772, 'validation/num_examples': 50000, 'test/accuracy': 0.569100022315979, 'test/loss': 1.889735460281372, 'test/num_examples': 10000, 'score': 72728.1712462902, 'total_duration': 80591.012966156, 'accumulated_submission_time': 72728.1712462902, 'accumulated_eval_time': 7847.6235818862915, 'accumulated_logging_time': 6.992994785308838}
I0203 10:10:23.356208 140022518892288 logging_writer.py:48] [158915] accumulated_eval_time=7847.623582, accumulated_logging_time=6.992995, accumulated_submission_time=72728.171246, global_step=158915, preemption_count=0, score=72728.171246, test/accuracy=0.569100, test/loss=1.889735, test/num_examples=10000, total_duration=80591.012966, train/accuracy=0.752519, train/loss=1.007640, validation/accuracy=0.693640, validation/loss=1.261505, validation/num_examples=50000
I0203 10:10:58.867796 140023005427456 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.5748398303985596, loss=1.8221626281738281
I0203 10:11:45.066437 140022518892288 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.3683342933654785, loss=3.688969135284424
I0203 10:12:31.529621 140023005427456 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.150439739227295, loss=4.041788101196289
I0203 10:13:17.825733 140022518892288 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.439560890197754, loss=1.8983798027038574
I0203 10:14:04.241486 140023005427456 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.3808720111846924, loss=2.0188894271850586
I0203 10:14:50.398324 140022518892288 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.5495121479034424, loss=2.063610792160034
I0203 10:15:36.576851 140023005427456 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.594058036804199, loss=1.9318362474441528
I0203 10:16:22.855372 140022518892288 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.282752275466919, loss=3.0662498474121094
I0203 10:17:09.327516 140023005427456 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.174248695373535, loss=3.0701723098754883
I0203 10:17:23.492065 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:17:34.041906 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:18:10.468753 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:18:12.070117 140184451094336 submission_runner.py:408] Time since start: 81059.78s, 	Step: 159832, 	{'train/accuracy': 0.7587695121765137, 'train/loss': 0.973080575466156, 'validation/accuracy': 0.6987999677658081, 'validation/loss': 1.2378002405166626, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 1.876285195350647, 'test/num_examples': 10000, 'score': 73148.25042939186, 'total_duration': 81059.77536559105, 'accumulated_submission_time': 73148.25042939186, 'accumulated_eval_time': 7896.201631784439, 'accumulated_logging_time': 7.052062034606934}
I0203 10:18:12.110358 140022518892288 logging_writer.py:48] [159832] accumulated_eval_time=7896.201632, accumulated_logging_time=7.052062, accumulated_submission_time=73148.250429, global_step=159832, preemption_count=0, score=73148.250429, test/accuracy=0.573100, test/loss=1.876285, test/num_examples=10000, total_duration=81059.775366, train/accuracy=0.758770, train/loss=0.973081, validation/accuracy=0.698800, validation/loss=1.237800, validation/num_examples=50000
I0203 10:18:39.709736 140023005427456 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.5411276817321777, loss=1.9006233215332031
I0203 10:19:25.612543 140022518892288 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.6755197048187256, loss=4.059168338775635
I0203 10:20:12.154930 140023005427456 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.561610698699951, loss=1.8671596050262451
I0203 10:20:58.544212 140022518892288 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.5229134559631348, loss=1.9332960844039917
I0203 10:21:44.845941 140023005427456 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.4843332767486572, loss=1.9288825988769531
I0203 10:22:31.123306 140022518892288 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.453847885131836, loss=1.9393231868743896
I0203 10:23:17.486646 140023005427456 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.5307819843292236, loss=1.8903406858444214
I0203 10:24:03.672549 140022518892288 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.5214040279388428, loss=1.8917229175567627
I0203 10:24:49.789781 140023005427456 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.617598056793213, loss=1.8365765810012817
I0203 10:25:12.268639 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:25:22.659091 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:25:56.348314 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:25:57.948095 140184451094336 submission_runner.py:408] Time since start: 81525.65s, 	Step: 160750, 	{'train/accuracy': 0.7638476490974426, 'train/loss': 0.9550341367721558, 'validation/accuracy': 0.7001399993896484, 'validation/loss': 1.2401357889175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.8870264291763306, 'test/num_examples': 10000, 'score': 73568.35084533691, 'total_duration': 81525.65334272385, 'accumulated_submission_time': 73568.35084533691, 'accumulated_eval_time': 7941.881090164185, 'accumulated_logging_time': 7.1026670932769775}
I0203 10:25:57.991554 140022518892288 logging_writer.py:48] [160750] accumulated_eval_time=7941.881090, accumulated_logging_time=7.102667, accumulated_submission_time=73568.350845, global_step=160750, preemption_count=0, score=73568.350845, test/accuracy=0.575100, test/loss=1.887026, test/num_examples=10000, total_duration=81525.653343, train/accuracy=0.763848, train/loss=0.955034, validation/accuracy=0.700140, validation/loss=1.240136, validation/num_examples=50000
I0203 10:26:18.372820 140023005427456 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.4434807300567627, loss=3.2644424438476562
I0203 10:27:03.289638 140022518892288 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.2451012134552, loss=2.511709451675415
I0203 10:27:49.518928 140023005427456 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.2242319583892822, loss=3.2871906757354736
I0203 10:28:35.932427 140022518892288 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.3181474208831787, loss=2.112905263900757
I0203 10:29:22.290734 140023005427456 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.372654438018799, loss=2.4020073413848877
I0203 10:30:08.605662 140022518892288 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.5418810844421387, loss=1.929309368133545
I0203 10:30:54.852283 140023005427456 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.535432815551758, loss=3.0793418884277344
I0203 10:31:41.342763 140022518892288 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.523698568344116, loss=3.811629295349121
I0203 10:32:27.392362 140023005427456 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.5434131622314453, loss=1.8684401512145996
I0203 10:32:58.434873 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:33:09.199734 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:33:41.506271 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:33:43.111889 140184451094336 submission_runner.py:408] Time since start: 81990.82s, 	Step: 161669, 	{'train/accuracy': 0.7666015625, 'train/loss': 0.9483379125595093, 'validation/accuracy': 0.7057600021362305, 'validation/loss': 1.2242356538772583, 'validation/num_examples': 50000, 'test/accuracy': 0.5799000263214111, 'test/loss': 1.8605889081954956, 'test/num_examples': 10000, 'score': 73988.73741221428, 'total_duration': 81990.81713628769, 'accumulated_submission_time': 73988.73741221428, 'accumulated_eval_time': 7986.558121442795, 'accumulated_logging_time': 7.155567407608032}
I0203 10:33:43.154349 140022518892288 logging_writer.py:48] [161669] accumulated_eval_time=7986.558121, accumulated_logging_time=7.155567, accumulated_submission_time=73988.737412, global_step=161669, preemption_count=0, score=73988.737412, test/accuracy=0.579900, test/loss=1.860589, test/num_examples=10000, total_duration=81990.817136, train/accuracy=0.766602, train/loss=0.948338, validation/accuracy=0.705760, validation/loss=1.224236, validation/num_examples=50000
I0203 10:33:55.962391 140023005427456 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.2872748374938965, loss=2.711578845977783
I0203 10:34:39.737259 140022518892288 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.553550958633423, loss=4.334172248840332
I0203 10:35:26.074367 140023005427456 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.309485912322998, loss=2.419574499130249
I0203 10:36:12.873261 140022518892288 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.457021474838257, loss=3.6319122314453125
I0203 10:36:59.005837 140023005427456 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.432849884033203, loss=2.357771158218384
I0203 10:37:45.399640 140022518892288 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.4132814407348633, loss=1.885183334350586
I0203 10:38:31.765266 140023005427456 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.32443904876709, loss=2.2855007648468018
I0203 10:39:17.904025 140022518892288 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.5400969982147217, loss=1.7601382732391357
I0203 10:40:04.271195 140023005427456 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.632803440093994, loss=4.269589424133301
I0203 10:40:43.360840 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:40:54.370279 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:41:31.252226 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:41:32.860708 140184451094336 submission_runner.py:408] Time since start: 82460.57s, 	Step: 162587, 	{'train/accuracy': 0.7708789110183716, 'train/loss': 0.9288029074668884, 'validation/accuracy': 0.7047399878501892, 'validation/loss': 1.2123870849609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.8519184589385986, 'test/num_examples': 10000, 'score': 74408.8857395649, 'total_duration': 82460.56594634056, 'accumulated_submission_time': 74408.8857395649, 'accumulated_eval_time': 8036.057965755463, 'accumulated_logging_time': 7.209298849105835}
I0203 10:41:32.903657 140022518892288 logging_writer.py:48] [162587] accumulated_eval_time=8036.057966, accumulated_logging_time=7.209299, accumulated_submission_time=74408.885740, global_step=162587, preemption_count=0, score=74408.885740, test/accuracy=0.580900, test/loss=1.851918, test/num_examples=10000, total_duration=82460.565946, train/accuracy=0.770879, train/loss=0.928803, validation/accuracy=0.704740, validation/loss=1.212387, validation/num_examples=50000
I0203 10:41:38.505852 140023005427456 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.495408296585083, loss=2.532055377960205
I0203 10:42:21.441466 140022518892288 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.5316123962402344, loss=3.7993505001068115
I0203 10:43:07.500470 140023005427456 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.299992084503174, loss=2.54561448097229
I0203 10:43:53.709436 140022518892288 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.9631452560424805, loss=1.8309240341186523
I0203 10:44:39.673545 140023005427456 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.2352657318115234, loss=3.360802412033081
I0203 10:45:25.746606 140022518892288 logging_writer.py:48] [163100] global_step=163100, grad_norm=3.158747911453247, loss=4.39520263671875
I0203 10:46:12.135911 140023005427456 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.866936445236206, loss=1.8873412609100342
I0203 10:46:57.976366 140022518892288 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.5521435737609863, loss=2.8804242610931396
I0203 10:47:44.135616 140023005427456 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.8151090145111084, loss=2.2174768447875977
I0203 10:48:30.398473 140022518892288 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.713108777999878, loss=1.828955888748169
I0203 10:48:33.292135 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:48:43.940511 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:49:16.752523 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:49:18.360654 140184451094336 submission_runner.py:408] Time since start: 82926.07s, 	Step: 163508, 	{'train/accuracy': 0.7753124833106995, 'train/loss': 0.9027910232543945, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1837961673736572, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.826006293296814, 'test/num_examples': 10000, 'score': 74829.21450185776, 'total_duration': 82926.06588101387, 'accumulated_submission_time': 74829.21450185776, 'accumulated_eval_time': 8081.126464605331, 'accumulated_logging_time': 7.264871120452881}
I0203 10:49:18.409523 140023005427456 logging_writer.py:48] [163508] accumulated_eval_time=8081.126465, accumulated_logging_time=7.264871, accumulated_submission_time=74829.214502, global_step=163508, preemption_count=0, score=74829.214502, test/accuracy=0.587300, test/loss=1.826006, test/num_examples=10000, total_duration=82926.065881, train/accuracy=0.775312, train/loss=0.902791, validation/accuracy=0.711880, validation/loss=1.183796, validation/num_examples=50000
I0203 10:49:57.181365 140022518892288 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.715726137161255, loss=1.7639434337615967
I0203 10:50:43.251892 140023005427456 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.4897589683532715, loss=2.062917947769165
I0203 10:51:29.667203 140022518892288 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.790430784225464, loss=4.226624965667725
I0203 10:52:16.382958 140023005427456 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.681056499481201, loss=4.08968448638916
I0203 10:53:02.890227 140022518892288 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.7040622234344482, loss=1.7437450885772705
I0203 10:53:49.208281 140023005427456 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.7460241317749023, loss=1.7508691549301147
I0203 10:54:35.572822 140022518892288 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.680267572402954, loss=2.072497844696045
I0203 10:55:22.027923 140023005427456 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.7907750606536865, loss=3.469438076019287
I0203 10:56:08.263673 140022518892288 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.3922882080078125, loss=2.2226970195770264
I0203 10:56:18.504325 140184451094336 spec.py:321] Evaluating on the training split.
I0203 10:56:29.135076 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 10:57:03.810621 140184451094336 spec.py:349] Evaluating on the test split.
I0203 10:57:05.408438 140184451094336 submission_runner.py:408] Time since start: 83393.11s, 	Step: 164424, 	{'train/accuracy': 0.7840234041213989, 'train/loss': 0.861545979976654, 'validation/accuracy': 0.7120199799537659, 'validation/loss': 1.1774080991744995, 'validation/num_examples': 50000, 'test/accuracy': 0.5870000123977661, 'test/loss': 1.8180652856826782, 'test/num_examples': 10000, 'score': 75249.25192141533, 'total_duration': 83393.11365270615, 'accumulated_submission_time': 75249.25192141533, 'accumulated_eval_time': 8128.030569314957, 'accumulated_logging_time': 7.324113607406616}
I0203 10:57:05.452559 140023005427456 logging_writer.py:48] [164424] accumulated_eval_time=8128.030569, accumulated_logging_time=7.324114, accumulated_submission_time=75249.251921, global_step=164424, preemption_count=0, score=75249.251921, test/accuracy=0.587000, test/loss=1.818065, test/num_examples=10000, total_duration=83393.113653, train/accuracy=0.784023, train/loss=0.861546, validation/accuracy=0.712020, validation/loss=1.177408, validation/num_examples=50000
I0203 10:57:36.728421 140022518892288 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.51141357421875, loss=3.3246352672576904
I0203 10:58:22.676281 140023005427456 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.657602071762085, loss=1.642438530921936
I0203 10:59:08.827969 140022518892288 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.5713374614715576, loss=2.747591495513916
I0203 10:59:54.947304 140023005427456 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.625469446182251, loss=1.7624818086624146
I0203 11:00:41.040729 140022518892288 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.6374964714050293, loss=2.456045627593994
I0203 11:01:27.182610 140023005427456 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.6917884349823, loss=1.9614448547363281
I0203 11:02:13.548122 140022518892288 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.427964210510254, loss=1.8402081727981567
I0203 11:02:59.839015 140023005427456 logging_writer.py:48] [165200] global_step=165200, grad_norm=3.0373849868774414, loss=1.8846608400344849
I0203 11:03:45.846837 140022518892288 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.6072263717651367, loss=1.8504713773727417
I0203 11:04:05.456165 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:04:15.616843 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:04:51.325820 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:04:52.928891 140184451094336 submission_runner.py:408] Time since start: 83860.63s, 	Step: 165344, 	{'train/accuracy': 0.78076171875, 'train/loss': 0.8917228579521179, 'validation/accuracy': 0.7150999903678894, 'validation/loss': 1.1734659671783447, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.806179404258728, 'test/num_examples': 10000, 'score': 75669.19865012169, 'total_duration': 83860.63411188126, 'accumulated_submission_time': 75669.19865012169, 'accumulated_eval_time': 8175.50325012207, 'accumulated_logging_time': 7.37749457359314}
I0203 11:04:52.980561 140023005427456 logging_writer.py:48] [165344] accumulated_eval_time=8175.503250, accumulated_logging_time=7.377495, accumulated_submission_time=75669.198650, global_step=165344, preemption_count=0, score=75669.198650, test/accuracy=0.591800, test/loss=1.806179, test/num_examples=10000, total_duration=83860.634112, train/accuracy=0.780762, train/loss=0.891723, validation/accuracy=0.715100, validation/loss=1.173466, validation/num_examples=50000
I0203 11:05:15.778975 140022518892288 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.5919840335845947, loss=3.2920327186584473
I0203 11:06:00.886138 140023005427456 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.8067383766174316, loss=1.848312497138977
I0203 11:06:46.890744 140022518892288 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.660209894180298, loss=3.1300644874572754
I0203 11:07:33.115334 140023005427456 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.9719297885894775, loss=4.046852111816406
I0203 11:08:19.316375 140022518892288 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.790022373199463, loss=2.2663726806640625
I0203 11:09:05.593712 140023005427456 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.6321756839752197, loss=1.7158074378967285
I0203 11:09:51.504832 140022518892288 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.788635730743408, loss=1.7688677310943604
I0203 11:10:37.492193 140023005427456 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.994894027709961, loss=1.847881555557251
I0203 11:11:23.797291 140022518892288 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.5065722465515137, loss=2.725224256515503
I0203 11:11:53.098921 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:12:03.868704 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:12:36.071573 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:12:37.683723 140184451094336 submission_runner.py:408] Time since start: 84325.39s, 	Step: 166265, 	{'train/accuracy': 0.7815039157867432, 'train/loss': 0.8719916939735413, 'validation/accuracy': 0.7160800099372864, 'validation/loss': 1.1593176126480103, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.798190951347351, 'test/num_examples': 10000, 'score': 76089.25742650032, 'total_duration': 84325.38896870613, 'accumulated_submission_time': 76089.25742650032, 'accumulated_eval_time': 8220.088045358658, 'accumulated_logging_time': 7.440832138061523}
I0203 11:12:37.727613 140023005427456 logging_writer.py:48] [166265] accumulated_eval_time=8220.088045, accumulated_logging_time=7.440832, accumulated_submission_time=76089.257427, global_step=166265, preemption_count=0, score=76089.257427, test/accuracy=0.591300, test/loss=1.798191, test/num_examples=10000, total_duration=84325.388969, train/accuracy=0.781504, train/loss=0.871992, validation/accuracy=0.716080, validation/loss=1.159318, validation/num_examples=50000
I0203 11:12:52.137235 140022518892288 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.664539098739624, loss=2.861046552658081
I0203 11:13:36.314348 140023005427456 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.6357085704803467, loss=2.1692593097686768
I0203 11:14:22.724544 140022518892288 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.754079580307007, loss=1.6760274171829224
I0203 11:15:09.446455 140023005427456 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.8173940181732178, loss=4.013119697570801
I0203 11:15:55.478453 140022518892288 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.826420307159424, loss=1.7793397903442383
I0203 11:16:41.895262 140023005427456 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.996649742126465, loss=1.8886972665786743
I0203 11:17:28.399611 140022518892288 logging_writer.py:48] [166900] global_step=166900, grad_norm=3.745443344116211, loss=1.7817049026489258
I0203 11:18:14.845526 140023005427456 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.833721399307251, loss=1.7396910190582275
I0203 11:19:01.106783 140022518892288 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.9293532371520996, loss=1.9979751110076904
I0203 11:19:37.873729 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:19:48.215904 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:20:23.033927 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:20:24.642257 140184451094336 submission_runner.py:408] Time since start: 84792.35s, 	Step: 167181, 	{'train/accuracy': 0.789843738079071, 'train/loss': 0.8536946177482605, 'validation/accuracy': 0.7184199690818787, 'validation/loss': 1.1556397676467896, 'validation/num_examples': 50000, 'test/accuracy': 0.5931000113487244, 'test/loss': 1.7854821681976318, 'test/num_examples': 10000, 'score': 76509.34638428688, 'total_duration': 84792.34748697281, 'accumulated_submission_time': 76509.34638428688, 'accumulated_eval_time': 8266.85658288002, 'accumulated_logging_time': 7.495449066162109}
I0203 11:20:24.689457 140023005427456 logging_writer.py:48] [167181] accumulated_eval_time=8266.856583, accumulated_logging_time=7.495449, accumulated_submission_time=76509.346384, global_step=167181, preemption_count=0, score=76509.346384, test/accuracy=0.593100, test/loss=1.785482, test/num_examples=10000, total_duration=84792.347487, train/accuracy=0.789844, train/loss=0.853695, validation/accuracy=0.718420, validation/loss=1.155640, validation/num_examples=50000
I0203 11:20:32.686940 140022518892288 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.600654363632202, loss=2.4815568923950195
I0203 11:21:15.936995 140023005427456 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.64801025390625, loss=3.3086905479431152
I0203 11:22:02.066580 140022518892288 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.818253517150879, loss=1.7716025114059448
I0203 11:22:48.468281 140023005427456 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.777456760406494, loss=3.2393248081207275
I0203 11:23:34.677032 140022518892288 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.7867929935455322, loss=1.7716035842895508
I0203 11:24:21.011685 140023005427456 logging_writer.py:48] [167700] global_step=167700, grad_norm=2.649319648742676, loss=2.5095884799957275
I0203 11:25:07.190266 140022518892288 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.1846282482147217, loss=1.8010255098342896
I0203 11:25:53.556473 140023005427456 logging_writer.py:48] [167900] global_step=167900, grad_norm=3.09971022605896, loss=1.6870514154434204
I0203 11:26:39.827632 140022518892288 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.9244508743286133, loss=1.5845431089401245
I0203 11:27:24.979510 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:27:35.360743 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:28:10.996649 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:28:12.598076 140184451094336 submission_runner.py:408] Time since start: 85260.30s, 	Step: 168099, 	{'train/accuracy': 0.7865429520606995, 'train/loss': 0.8519309759140015, 'validation/accuracy': 0.7182199954986572, 'validation/loss': 1.1389554738998413, 'validation/num_examples': 50000, 'test/accuracy': 0.5954000353813171, 'test/loss': 1.7669183015823364, 'test/num_examples': 10000, 'score': 76929.57842612267, 'total_duration': 85260.30331659317, 'accumulated_submission_time': 76929.57842612267, 'accumulated_eval_time': 8314.475160121918, 'accumulated_logging_time': 7.552535772323608}
I0203 11:28:12.647979 140023005427456 logging_writer.py:48] [168099] accumulated_eval_time=8314.475160, accumulated_logging_time=7.552536, accumulated_submission_time=76929.578426, global_step=168099, preemption_count=0, score=76929.578426, test/accuracy=0.595400, test/loss=1.766918, test/num_examples=10000, total_duration=85260.303317, train/accuracy=0.786543, train/loss=0.851931, validation/accuracy=0.718220, validation/loss=1.138955, validation/num_examples=50000
I0203 11:28:13.453223 140022518892288 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.7978053092956543, loss=1.8062783479690552
I0203 11:28:55.557991 140023005427456 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.1851158142089844, loss=1.7297570705413818
I0203 11:29:41.417394 140022518892288 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.7133543491363525, loss=2.407905340194702
I0203 11:30:27.673029 140023005427456 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.143949270248413, loss=2.5848615169525146
I0203 11:31:13.833232 140022518892288 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.801708221435547, loss=2.6065049171447754
I0203 11:31:59.957946 140023005427456 logging_writer.py:48] [168600] global_step=168600, grad_norm=3.077989101409912, loss=1.7217507362365723
I0203 11:32:46.229880 140022518892288 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.623676061630249, loss=1.7528609037399292
I0203 11:33:32.542046 140023005427456 logging_writer.py:48] [168800] global_step=168800, grad_norm=3.1992709636688232, loss=1.7466908693313599
I0203 11:34:18.622054 140022518892288 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.0651748180389404, loss=1.697871446609497
I0203 11:35:04.686693 140023005427456 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.11275315284729, loss=1.7374411821365356
I0203 11:35:12.700600 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:35:23.107025 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:35:55.816599 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:35:57.413722 140184451094336 submission_runner.py:408] Time since start: 85725.12s, 	Step: 169019, 	{'train/accuracy': 0.7899804711341858, 'train/loss': 0.8419513702392578, 'validation/accuracy': 0.7230599522590637, 'validation/loss': 1.1312774419784546, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.7649180889129639, 'test/num_examples': 10000, 'score': 77349.57205319405, 'total_duration': 85725.11896824837, 'accumulated_submission_time': 77349.57205319405, 'accumulated_eval_time': 8359.18827176094, 'accumulated_logging_time': 7.6130571365356445}
I0203 11:35:57.454275 140022518892288 logging_writer.py:48] [169019] accumulated_eval_time=8359.188272, accumulated_logging_time=7.613057, accumulated_submission_time=77349.572053, global_step=169019, preemption_count=0, score=77349.572053, test/accuracy=0.596900, test/loss=1.764918, test/num_examples=10000, total_duration=85725.118968, train/accuracy=0.789980, train/loss=0.841951, validation/accuracy=0.723060, validation/loss=1.131277, validation/num_examples=50000
I0203 11:36:31.106951 140023005427456 logging_writer.py:48] [169100] global_step=169100, grad_norm=3.093163251876831, loss=3.7802698612213135
I0203 11:37:17.073927 140022518892288 logging_writer.py:48] [169200] global_step=169200, grad_norm=3.1068127155303955, loss=2.0162160396575928
I0203 11:38:03.251694 140023005427456 logging_writer.py:48] [169300] global_step=169300, grad_norm=2.9312260150909424, loss=2.7352657318115234
I0203 11:38:48.238244 140022518892288 logging_writer.py:48] [169399] global_step=169399, preemption_count=0, score=77520.270592
I0203 11:38:48.927530 140184451094336 checkpoints.py:490] Saving checkpoint at step: 169399
I0203 11:38:50.212584 140184451094336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4/checkpoint_169399
I0203 11:38:50.235787 140184451094336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_4/checkpoint_169399.
I0203 11:38:51.357115 140184451094336 submission_runner.py:583] Tuning trial 4/5
I0203 11:38:51.357332 140184451094336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0203 11:38:51.379212 140184451094336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.001054687425494194, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.79507851600647, 'total_duration': 64.5379867553711, 'accumulated_submission_time': 36.79507851600647, 'accumulated_eval_time': 27.742810249328613, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (857, {'train/accuracy': 0.03404296934604645, 'train/loss': 5.877542018890381, 'validation/accuracy': 0.032919999212026596, 'validation/loss': 5.916387557983398, 'validation/num_examples': 50000, 'test/accuracy': 0.027300002053380013, 'test/loss': 6.052794933319092, 'test/num_examples': 10000, 'score': 456.8279526233673, 'total_duration': 525.8108751773834, 'accumulated_submission_time': 456.8279526233673, 'accumulated_eval_time': 68.91855096817017, 'accumulated_logging_time': 0.019236087799072266, 'global_step': 857, 'preemption_count': 0}), (1774, {'train/accuracy': 0.06892578303813934, 'train/loss': 5.4872636795043945, 'validation/accuracy': 0.06289999932050705, 'validation/loss': 5.5517730712890625, 'validation/num_examples': 50000, 'test/accuracy': 0.04830000177025795, 'test/loss': 5.741621971130371, 'test/num_examples': 10000, 'score': 876.8367249965668, 'total_duration': 984.6733191013336, 'accumulated_submission_time': 876.8367249965668, 'accumulated_eval_time': 107.68769526481628, 'accumulated_logging_time': 0.05630087852478027, 'global_step': 1774, 'preemption_count': 0}), (2694, {'train/accuracy': 0.10423827916383743, 'train/loss': 4.991183280944824, 'validation/accuracy': 0.09743999689817429, 'validation/loss': 5.053380012512207, 'validation/num_examples': 50000, 'test/accuracy': 0.07460000365972519, 'test/loss': 5.3333587646484375, 'test/num_examples': 10000, 'score': 1296.802814245224, 'total_duration': 1442.7371714115143, 'accumulated_submission_time': 1296.802814245224, 'accumulated_eval_time': 145.71309757232666, 'accumulated_logging_time': 0.08081912994384766, 'global_step': 2694, 'preemption_count': 0}), (3612, {'train/accuracy': 0.13685546815395355, 'train/loss': 4.678333759307861, 'validation/accuracy': 0.1284399926662445, 'validation/loss': 4.747917175292969, 'validation/num_examples': 50000, 'test/accuracy': 0.09560000151395798, 'test/loss': 5.0893425941467285, 'test/num_examples': 10000, 'score': 1716.8630304336548, 'total_duration': 1906.967833995819, 'accumulated_submission_time': 1716.8630304336548, 'accumulated_eval_time': 189.81067943572998, 'accumulated_logging_time': 0.10532426834106445, 'global_step': 3612, 'preemption_count': 0}), (4531, {'train/accuracy': 0.18623046576976776, 'train/loss': 4.207225322723389, 'validation/accuracy': 0.17031998932361603, 'validation/loss': 4.318536758422852, 'validation/num_examples': 50000, 'test/accuracy': 0.1284000128507614, 'test/loss': 4.720849514007568, 'test/num_examples': 10000, 'score': 2136.9159696102142, 'total_duration': 2373.9058408737183, 'accumulated_submission_time': 2136.9159696102142, 'accumulated_eval_time': 236.62357091903687, 'accumulated_logging_time': 0.1309516429901123, 'global_step': 4531, 'preemption_count': 0}), (5447, {'train/accuracy': 0.20121093094348907, 'train/loss': 4.060230731964111, 'validation/accuracy': 0.18803998827934265, 'validation/loss': 4.152525424957275, 'validation/num_examples': 50000, 'test/accuracy': 0.14480000734329224, 'test/loss': 4.589404106140137, 'test/num_examples': 10000, 'score': 2557.1325867176056, 'total_duration': 2831.9772713184357, 'accumulated_submission_time': 2557.1325867176056, 'accumulated_eval_time': 274.40623688697815, 'accumulated_logging_time': 0.15637731552124023, 'global_step': 5447, 'preemption_count': 0}), (6363, {'train/accuracy': 0.21666015684604645, 'train/loss': 3.984548330307007, 'validation/accuracy': 0.20041999220848083, 'validation/loss': 4.092609405517578, 'validation/num_examples': 50000, 'test/accuracy': 0.15310001373291016, 'test/loss': 4.517242431640625, 'test/num_examples': 10000, 'score': 2977.314273118973, 'total_duration': 3296.628982782364, 'accumulated_submission_time': 2977.314273118973, 'accumulated_eval_time': 318.80083322525024, 'accumulated_logging_time': 0.18465852737426758, 'global_step': 6363, 'preemption_count': 0}), (7283, {'train/accuracy': 0.2351367175579071, 'train/loss': 3.851510524749756, 'validation/accuracy': 0.2165599912405014, 'validation/loss': 3.9796018600463867, 'validation/num_examples': 50000, 'test/accuracy': 0.16660000383853912, 'test/loss': 4.4249958992004395, 'test/num_examples': 10000, 'score': 3397.362140417099, 'total_duration': 3762.9711685180664, 'accumulated_submission_time': 3397.362140417099, 'accumulated_eval_time': 365.0135953426361, 'accumulated_logging_time': 0.21924662590026855, 'global_step': 7283, 'preemption_count': 0}), (8203, {'train/accuracy': 0.26273438334465027, 'train/loss': 3.6297197341918945, 'validation/accuracy': 0.23229999840259552, 'validation/loss': 3.8290367126464844, 'validation/num_examples': 50000, 'test/accuracy': 0.17440000176429749, 'test/loss': 4.296341419219971, 'test/num_examples': 10000, 'score': 3817.682046175003, 'total_duration': 4229.221621990204, 'accumulated_submission_time': 3817.682046175003, 'accumulated_eval_time': 410.8689727783203, 'accumulated_logging_time': 0.24734210968017578, 'global_step': 8203, 'preemption_count': 0}), (9121, {'train/accuracy': 0.2595312297344208, 'train/loss': 3.638303518295288, 'validation/accuracy': 0.2396799921989441, 'validation/loss': 3.753018617630005, 'validation/num_examples': 50000, 'test/accuracy': 0.18690000474452972, 'test/loss': 4.2364301681518555, 'test/num_examples': 10000, 'score': 4237.6647737026215, 'total_duration': 4694.430584430695, 'accumulated_submission_time': 4237.6647737026215, 'accumulated_eval_time': 456.02307748794556, 'accumulated_logging_time': 0.2729377746582031, 'global_step': 9121, 'preemption_count': 0}), (10039, {'train/accuracy': 0.27964842319488525, 'train/loss': 3.492593288421631, 'validation/accuracy': 0.2573399841785431, 'validation/loss': 3.628937005996704, 'validation/num_examples': 50000, 'test/accuracy': 0.20080001652240753, 'test/loss': 4.119417667388916, 'test/num_examples': 10000, 'score': 4657.653786182404, 'total_duration': 5157.994423866272, 'accumulated_submission_time': 4657.653786182404, 'accumulated_eval_time': 499.5223741531372, 'accumulated_logging_time': 0.30005645751953125, 'global_step': 10039, 'preemption_count': 0}), (10957, {'train/accuracy': 0.29624998569488525, 'train/loss': 3.400749683380127, 'validation/accuracy': 0.2607399821281433, 'validation/loss': 3.646686553955078, 'validation/num_examples': 50000, 'test/accuracy': 0.1965000033378601, 'test/loss': 4.154642581939697, 'test/num_examples': 10000, 'score': 5077.807441949844, 'total_duration': 5618.391860961914, 'accumulated_submission_time': 5077.807441949844, 'accumulated_eval_time': 539.6884536743164, 'accumulated_logging_time': 0.32504844665527344, 'global_step': 10957, 'preemption_count': 0}), (11874, {'train/accuracy': 0.2930664122104645, 'train/loss': 3.4094316959381104, 'validation/accuracy': 0.2763800024986267, 'validation/loss': 3.517740488052368, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.0429534912109375, 'test/num_examples': 10000, 'score': 5497.79346203804, 'total_duration': 6077.069966077805, 'accumulated_submission_time': 5497.79346203804, 'accumulated_eval_time': 578.3044393062592, 'accumulated_logging_time': 0.3541853427886963, 'global_step': 11874, 'preemption_count': 0}), (12795, {'train/accuracy': 0.2978906035423279, 'train/loss': 3.414585590362549, 'validation/accuracy': 0.2738800048828125, 'validation/loss': 3.54900860786438, 'validation/num_examples': 50000, 'test/accuracy': 0.21230001747608185, 'test/loss': 4.034548282623291, 'test/num_examples': 10000, 'score': 5918.157521486282, 'total_duration': 6538.3041615486145, 'accumulated_submission_time': 5918.157521486282, 'accumulated_eval_time': 619.1004593372345, 'accumulated_logging_time': 0.38120341300964355, 'global_step': 12795, 'preemption_count': 0}), (13715, {'train/accuracy': 0.3108203113079071, 'train/loss': 3.344714641571045, 'validation/accuracy': 0.2833400070667267, 'validation/loss': 3.511706829071045, 'validation/num_examples': 50000, 'test/accuracy': 0.2126000076532364, 'test/loss': 4.019969940185547, 'test/num_examples': 10000, 'score': 6338.3181848526, 'total_duration': 7003.260776519775, 'accumulated_submission_time': 6338.3181848526, 'accumulated_eval_time': 663.8211107254028, 'accumulated_logging_time': 0.40915966033935547, 'global_step': 13715, 'preemption_count': 0}), (14635, {'train/accuracy': 0.31138670444488525, 'train/loss': 3.3009660243988037, 'validation/accuracy': 0.2888199985027313, 'validation/loss': 3.432950496673584, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 3.9814248085021973, 'test/num_examples': 10000, 'score': 6758.649676322937, 'total_duration': 7464.368344545364, 'accumulated_submission_time': 6758.649676322937, 'accumulated_eval_time': 704.5228517055511, 'accumulated_logging_time': 0.43570423126220703, 'global_step': 14635, 'preemption_count': 0}), (15553, {'train/accuracy': 0.32255858182907104, 'train/loss': 3.2005932331085205, 'validation/accuracy': 0.30140000581741333, 'validation/loss': 3.3318538665771484, 'validation/num_examples': 50000, 'test/accuracy': 0.23260001838207245, 'test/loss': 3.8751766681671143, 'test/num_examples': 10000, 'score': 7178.820302963257, 'total_duration': 7929.172488689423, 'accumulated_submission_time': 7178.820302963257, 'accumulated_eval_time': 749.0826478004456, 'accumulated_logging_time': 0.462766170501709, 'global_step': 15553, 'preemption_count': 0}), (16472, {'train/accuracy': 0.3410351574420929, 'train/loss': 3.1523284912109375, 'validation/accuracy': 0.31025999784469604, 'validation/loss': 3.32299542427063, 'validation/num_examples': 50000, 'test/accuracy': 0.23600001633167267, 'test/loss': 3.8591465950012207, 'test/num_examples': 10000, 'score': 7599.0502026081085, 'total_duration': 8389.891204357147, 'accumulated_submission_time': 7599.0502026081085, 'accumulated_eval_time': 789.4964265823364, 'accumulated_logging_time': 0.48940229415893555, 'global_step': 16472, 'preemption_count': 0}), (17392, {'train/accuracy': 0.3284960985183716, 'train/loss': 3.2336535453796387, 'validation/accuracy': 0.29933997988700867, 'validation/loss': 3.389450788497925, 'validation/num_examples': 50000, 'test/accuracy': 0.22700001299381256, 'test/loss': 3.9175381660461426, 'test/num_examples': 10000, 'score': 8019.264442682266, 'total_duration': 8849.203982591629, 'accumulated_submission_time': 8019.264442682266, 'accumulated_eval_time': 828.5178320407867, 'accumulated_logging_time': 0.5192873477935791, 'global_step': 17392, 'preemption_count': 0}), (18310, {'train/accuracy': 0.3080468773841858, 'train/loss': 3.3751299381256104, 'validation/accuracy': 0.2909199893474579, 'validation/loss': 3.5047309398651123, 'validation/num_examples': 50000, 'test/accuracy': 0.21870000660419464, 'test/loss': 4.029071807861328, 'test/num_examples': 10000, 'score': 8439.656270503998, 'total_duration': 9312.967787742615, 'accumulated_submission_time': 8439.656270503998, 'accumulated_eval_time': 871.812756061554, 'accumulated_logging_time': 0.5486938953399658, 'global_step': 18310, 'preemption_count': 0}), (19230, {'train/accuracy': 0.3353515565395355, 'train/loss': 3.115917921066284, 'validation/accuracy': 0.3131999969482422, 'validation/loss': 3.2783405780792236, 'validation/num_examples': 50000, 'test/accuracy': 0.24080000817775726, 'test/loss': 3.8472251892089844, 'test/num_examples': 10000, 'score': 8859.802419900894, 'total_duration': 9779.14450263977, 'accumulated_submission_time': 8859.802419900894, 'accumulated_eval_time': 917.766725063324, 'accumulated_logging_time': 0.5775036811828613, 'global_step': 19230, 'preemption_count': 0}), (20149, {'train/accuracy': 0.38164061307907104, 'train/loss': 2.8914709091186523, 'validation/accuracy': 0.32145997881889343, 'validation/loss': 3.214576482772827, 'validation/num_examples': 50000, 'test/accuracy': 0.2436000108718872, 'test/loss': 3.7968075275421143, 'test/num_examples': 10000, 'score': 9280.090950250626, 'total_duration': 10241.470337152481, 'accumulated_submission_time': 9280.090950250626, 'accumulated_eval_time': 959.7255573272705, 'accumulated_logging_time': 0.6077971458435059, 'global_step': 20149, 'preemption_count': 0}), (21070, {'train/accuracy': 0.35353514552116394, 'train/loss': 2.995561361312866, 'validation/accuracy': 0.3300800025463104, 'validation/loss': 3.140366554260254, 'validation/num_examples': 50000, 'test/accuracy': 0.2556000053882599, 'test/loss': 3.7248618602752686, 'test/num_examples': 10000, 'score': 9700.320873737335, 'total_duration': 10706.340615034103, 'accumulated_submission_time': 9700.320873737335, 'accumulated_eval_time': 1004.2897145748138, 'accumulated_logging_time': 0.6361017227172852, 'global_step': 21070, 'preemption_count': 0}), (21990, {'train/accuracy': 0.3414843678474426, 'train/loss': 3.077301025390625, 'validation/accuracy': 0.31797999143600464, 'validation/loss': 3.2187304496765137, 'validation/num_examples': 50000, 'test/accuracy': 0.24730001389980316, 'test/loss': 3.81091570854187, 'test/num_examples': 10000, 'score': 10120.517220973969, 'total_duration': 11175.220227003098, 'accumulated_submission_time': 10120.517220973969, 'accumulated_eval_time': 1052.8985612392426, 'accumulated_logging_time': 0.662848949432373, 'global_step': 21990, 'preemption_count': 0}), (22908, {'train/accuracy': 0.35158202052116394, 'train/loss': 3.0414631366729736, 'validation/accuracy': 0.3215000033378601, 'validation/loss': 3.234511137008667, 'validation/num_examples': 50000, 'test/accuracy': 0.24690000712871552, 'test/loss': 3.816385269165039, 'test/num_examples': 10000, 'score': 10540.53809094429, 'total_duration': 11636.506449222565, 'accumulated_submission_time': 10540.53809094429, 'accumulated_eval_time': 1094.0847754478455, 'accumulated_logging_time': 0.6952741146087646, 'global_step': 22908, 'preemption_count': 0}), (23825, {'train/accuracy': 0.35593748092651367, 'train/loss': 3.025038719177246, 'validation/accuracy': 0.3379800021648407, 'validation/loss': 3.137194871902466, 'validation/num_examples': 50000, 'test/accuracy': 0.26260000467300415, 'test/loss': 3.7027549743652344, 'test/num_examples': 10000, 'score': 10960.580304384232, 'total_duration': 12104.379473924637, 'accumulated_submission_time': 10960.580304384232, 'accumulated_eval_time': 1141.8376290798187, 'accumulated_logging_time': 0.7260003089904785, 'global_step': 23825, 'preemption_count': 0}), (24746, {'train/accuracy': 0.35679686069488525, 'train/loss': 3.0097639560699463, 'validation/accuracy': 0.3316799998283386, 'validation/loss': 3.17022705078125, 'validation/num_examples': 50000, 'test/accuracy': 0.25609999895095825, 'test/loss': 3.7653188705444336, 'test/num_examples': 10000, 'score': 11380.953262805939, 'total_duration': 12566.690264701843, 'accumulated_submission_time': 11380.953262805939, 'accumulated_eval_time': 1183.7002770900726, 'accumulated_logging_time': 0.754091739654541, 'global_step': 24746, 'preemption_count': 0}), (25665, {'train/accuracy': 0.3615429699420929, 'train/loss': 3.016035795211792, 'validation/accuracy': 0.3340199887752533, 'validation/loss': 3.1701815128326416, 'validation/num_examples': 50000, 'test/accuracy': 0.25550001859664917, 'test/loss': 3.7496232986450195, 'test/num_examples': 10000, 'score': 11801.243542194366, 'total_duration': 13031.799020767212, 'accumulated_submission_time': 11801.243542194366, 'accumulated_eval_time': 1228.4372079372406, 'accumulated_logging_time': 0.7881994247436523, 'global_step': 25665, 'preemption_count': 0}), (26585, {'train/accuracy': 0.346992164850235, 'train/loss': 3.0902798175811768, 'validation/accuracy': 0.3211599886417389, 'validation/loss': 3.2521159648895264, 'validation/num_examples': 50000, 'test/accuracy': 0.24740001559257507, 'test/loss': 3.8121447563171387, 'test/num_examples': 10000, 'score': 12221.656418085098, 'total_duration': 13499.184258460999, 'accumulated_submission_time': 12221.656418085098, 'accumulated_eval_time': 1275.3340392112732, 'accumulated_logging_time': 0.8167154788970947, 'global_step': 26585, 'preemption_count': 0}), (27501, {'train/accuracy': 0.36302733421325684, 'train/loss': 2.980881929397583, 'validation/accuracy': 0.3391999900341034, 'validation/loss': 3.1102354526519775, 'validation/num_examples': 50000, 'test/accuracy': 0.25530001521110535, 'test/loss': 3.7012126445770264, 'test/num_examples': 10000, 'score': 12641.995535612106, 'total_duration': 13958.930496454239, 'accumulated_submission_time': 12641.995535612106, 'accumulated_eval_time': 1314.6609327793121, 'accumulated_logging_time': 0.8498325347900391, 'global_step': 27501, 'preemption_count': 0}), (28418, {'train/accuracy': 0.3716992139816284, 'train/loss': 2.924288034439087, 'validation/accuracy': 0.3419399857521057, 'validation/loss': 3.0865392684936523, 'validation/num_examples': 50000, 'test/accuracy': 0.2589000165462494, 'test/loss': 3.6658432483673096, 'test/num_examples': 10000, 'score': 13062.072076559067, 'total_duration': 14417.129843950272, 'accumulated_submission_time': 13062.072076559067, 'accumulated_eval_time': 1352.705206155777, 'accumulated_logging_time': 0.8814163208007812, 'global_step': 28418, 'preemption_count': 0}), (29335, {'train/accuracy': 0.3422265648841858, 'train/loss': 3.173394203186035, 'validation/accuracy': 0.31610000133514404, 'validation/loss': 3.3170182704925537, 'validation/num_examples': 50000, 'test/accuracy': 0.24340000748634338, 'test/loss': 3.895664930343628, 'test/num_examples': 10000, 'score': 13482.143792390823, 'total_duration': 14880.750834703445, 'accumulated_submission_time': 13482.143792390823, 'accumulated_eval_time': 1396.1704378128052, 'accumulated_logging_time': 0.9180092811584473, 'global_step': 29335, 'preemption_count': 0}), (30255, {'train/accuracy': 0.3800390660762787, 'train/loss': 2.861891746520996, 'validation/accuracy': 0.35273998975753784, 'validation/loss': 3.0179009437561035, 'validation/num_examples': 50000, 'test/accuracy': 0.26980000734329224, 'test/loss': 3.616143226623535, 'test/num_examples': 10000, 'score': 13902.161823272705, 'total_duration': 15347.681805610657, 'accumulated_submission_time': 13902.161823272705, 'accumulated_eval_time': 1443.007826089859, 'accumulated_logging_time': 0.9461681842803955, 'global_step': 30255, 'preemption_count': 0}), (31176, {'train/accuracy': 0.3661523461341858, 'train/loss': 2.9577431678771973, 'validation/accuracy': 0.3359200060367584, 'validation/loss': 3.120572328567505, 'validation/num_examples': 50000, 'test/accuracy': 0.25609999895095825, 'test/loss': 3.68998384475708, 'test/num_examples': 10000, 'score': 14322.417489528656, 'total_duration': 15807.286099910736, 'accumulated_submission_time': 14322.417489528656, 'accumulated_eval_time': 1482.2746975421906, 'accumulated_logging_time': 0.9801223278045654, 'global_step': 31176, 'preemption_count': 0}), (32094, {'train/accuracy': 0.40453124046325684, 'train/loss': 2.735917806625366, 'validation/accuracy': 0.3504199981689453, 'validation/loss': 3.0416879653930664, 'validation/num_examples': 50000, 'test/accuracy': 0.26990002393722534, 'test/loss': 3.6354501247406006, 'test/num_examples': 10000, 'score': 14742.738109588623, 'total_duration': 16267.752788543701, 'accumulated_submission_time': 14742.738109588623, 'accumulated_eval_time': 1522.337045431137, 'accumulated_logging_time': 1.0161523818969727, 'global_step': 32094, 'preemption_count': 0}), (33013, {'train/accuracy': 0.371406227350235, 'train/loss': 2.9292590618133545, 'validation/accuracy': 0.34553998708724976, 'validation/loss': 3.0680296421051025, 'validation/num_examples': 50000, 'test/accuracy': 0.27250000834465027, 'test/loss': 3.6371521949768066, 'test/num_examples': 10000, 'score': 15162.715245962143, 'total_duration': 16733.630709409714, 'accumulated_submission_time': 15162.715245962143, 'accumulated_eval_time': 1568.1595079898834, 'accumulated_logging_time': 1.0463981628417969, 'global_step': 33013, 'preemption_count': 0}), (33933, {'train/accuracy': 0.3720703125, 'train/loss': 2.9328975677490234, 'validation/accuracy': 0.3447999954223633, 'validation/loss': 3.0781383514404297, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.6681697368621826, 'test/num_examples': 10000, 'score': 15583.04086136818, 'total_duration': 17196.518713235855, 'accumulated_submission_time': 15583.04086136818, 'accumulated_eval_time': 1610.6403777599335, 'accumulated_logging_time': 1.0798285007476807, 'global_step': 33933, 'preemption_count': 0}), (34852, {'train/accuracy': 0.39851561188697815, 'train/loss': 2.755237579345703, 'validation/accuracy': 0.3614400029182434, 'validation/loss': 2.9586539268493652, 'validation/num_examples': 50000, 'test/accuracy': 0.27090001106262207, 'test/loss': 3.5934412479400635, 'test/num_examples': 10000, 'score': 16003.265714883804, 'total_duration': 17659.312923192978, 'accumulated_submission_time': 16003.265714883804, 'accumulated_eval_time': 1653.1295936107635, 'accumulated_logging_time': 1.1132240295410156, 'global_step': 34852, 'preemption_count': 0}), (35772, {'train/accuracy': 0.3848632872104645, 'train/loss': 2.830144166946411, 'validation/accuracy': 0.3606799840927124, 'validation/loss': 2.9782936573028564, 'validation/num_examples': 50000, 'test/accuracy': 0.27400001883506775, 'test/loss': 3.591972827911377, 'test/num_examples': 10000, 'score': 16423.598749876022, 'total_duration': 18126.9892642498, 'accumulated_submission_time': 16423.598749876022, 'accumulated_eval_time': 1700.3952662944794, 'accumulated_logging_time': 1.143989086151123, 'global_step': 35772, 'preemption_count': 0}), (36693, {'train/accuracy': 0.3790820240974426, 'train/loss': 2.873992919921875, 'validation/accuracy': 0.3569999933242798, 'validation/loss': 3.0040409564971924, 'validation/num_examples': 50000, 'test/accuracy': 0.26920002698898315, 'test/loss': 3.625471591949463, 'test/num_examples': 10000, 'score': 16843.694502830505, 'total_duration': 18591.12539958954, 'accumulated_submission_time': 16843.694502830505, 'accumulated_eval_time': 1744.3594100475311, 'accumulated_logging_time': 1.1727678775787354, 'global_step': 36693, 'preemption_count': 0}), (37613, {'train/accuracy': 0.3933398425579071, 'train/loss': 2.7992677688598633, 'validation/accuracy': 0.35933998227119446, 'validation/loss': 2.9893405437469482, 'validation/num_examples': 50000, 'test/accuracy': 0.2735000252723694, 'test/loss': 3.5844669342041016, 'test/num_examples': 10000, 'score': 17263.87330675125, 'total_duration': 19058.900079011917, 'accumulated_submission_time': 17263.87330675125, 'accumulated_eval_time': 1791.871794462204, 'accumulated_logging_time': 1.2091057300567627, 'global_step': 37613, 'preemption_count': 0}), (38530, {'train/accuracy': 0.3886132836341858, 'train/loss': 2.7967445850372314, 'validation/accuracy': 0.3657599985599518, 'validation/loss': 2.9347126483917236, 'validation/num_examples': 50000, 'test/accuracy': 0.2800000011920929, 'test/loss': 3.5463294982910156, 'test/num_examples': 10000, 'score': 17683.964739084244, 'total_duration': 19521.694784641266, 'accumulated_submission_time': 17683.964739084244, 'accumulated_eval_time': 1834.498204946518, 'accumulated_logging_time': 1.2384934425354004, 'global_step': 38530, 'preemption_count': 0}), (39451, {'train/accuracy': 0.38685545325279236, 'train/loss': 2.8592474460601807, 'validation/accuracy': 0.3622399866580963, 'validation/loss': 2.997967004776001, 'validation/num_examples': 50000, 'test/accuracy': 0.2826000154018402, 'test/loss': 3.571634531021118, 'test/num_examples': 10000, 'score': 18104.35695052147, 'total_duration': 19986.683392047882, 'accumulated_submission_time': 18104.35695052147, 'accumulated_eval_time': 1879.0127630233765, 'accumulated_logging_time': 1.272782802581787, 'global_step': 39451, 'preemption_count': 0}), (40369, {'train/accuracy': 0.4025976359844208, 'train/loss': 2.7299349308013916, 'validation/accuracy': 0.37046000361442566, 'validation/loss': 2.898860216140747, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.507483720779419, 'test/num_examples': 10000, 'score': 18524.528084754944, 'total_duration': 20454.92676472664, 'accumulated_submission_time': 18524.528084754944, 'accumulated_eval_time': 1927.0022106170654, 'accumulated_logging_time': 1.3080272674560547, 'global_step': 40369, 'preemption_count': 0}), (41288, {'train/accuracy': 0.41371092200279236, 'train/loss': 2.72369384765625, 'validation/accuracy': 0.36061999201774597, 'validation/loss': 2.9944801330566406, 'validation/num_examples': 50000, 'test/accuracy': 0.28120002150535583, 'test/loss': 3.5632994174957275, 'test/num_examples': 10000, 'score': 18944.646690130234, 'total_duration': 20917.348011016846, 'accumulated_submission_time': 18944.646690130234, 'accumulated_eval_time': 1969.225081205368, 'accumulated_logging_time': 1.3406035900115967, 'global_step': 41288, 'preemption_count': 0}), (42206, {'train/accuracy': 0.38896483182907104, 'train/loss': 2.8523201942443848, 'validation/accuracy': 0.367279976606369, 'validation/loss': 2.9817123413085938, 'validation/num_examples': 50000, 'test/accuracy': 0.28290000557899475, 'test/loss': 3.547908067703247, 'test/num_examples': 10000, 'score': 19364.88948059082, 'total_duration': 21376.21737074852, 'accumulated_submission_time': 19364.88948059082, 'accumulated_eval_time': 2007.7696409225464, 'accumulated_logging_time': 1.3750572204589844, 'global_step': 42206, 'preemption_count': 0}), (43125, {'train/accuracy': 0.40107420086860657, 'train/loss': 2.7512407302856445, 'validation/accuracy': 0.3735399842262268, 'validation/loss': 2.8998680114746094, 'validation/num_examples': 50000, 'test/accuracy': 0.28860002756118774, 'test/loss': 3.5031073093414307, 'test/num_examples': 10000, 'score': 19785.169471025467, 'total_duration': 21842.482117176056, 'accumulated_submission_time': 19785.169471025467, 'accumulated_eval_time': 2053.6729724407196, 'accumulated_logging_time': 1.4091379642486572, 'global_step': 43125, 'preemption_count': 0}), (44045, {'train/accuracy': 0.42900389432907104, 'train/loss': 2.601717233657837, 'validation/accuracy': 0.3832799792289734, 'validation/loss': 2.852113723754883, 'validation/num_examples': 50000, 'test/accuracy': 0.29280000925064087, 'test/loss': 3.4496848583221436, 'test/num_examples': 10000, 'score': 20205.232219696045, 'total_duration': 22307.98716187477, 'accumulated_submission_time': 20205.232219696045, 'accumulated_eval_time': 2099.034994125366, 'accumulated_logging_time': 1.4414191246032715, 'global_step': 44045, 'preemption_count': 0}), (44962, {'train/accuracy': 0.4001562297344208, 'train/loss': 2.8011372089385986, 'validation/accuracy': 0.37342000007629395, 'validation/loss': 2.9300620555877686, 'validation/num_examples': 50000, 'test/accuracy': 0.28630000352859497, 'test/loss': 3.5115420818328857, 'test/num_examples': 10000, 'score': 20625.31547307968, 'total_duration': 22773.348356485367, 'accumulated_submission_time': 20625.31547307968, 'accumulated_eval_time': 2144.228601694107, 'accumulated_logging_time': 1.4792120456695557, 'global_step': 44962, 'preemption_count': 0}), (45882, {'train/accuracy': 0.28843748569488525, 'train/loss': 3.4999566078186035, 'validation/accuracy': 0.26857998967170715, 'validation/loss': 3.6102328300476074, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.131436347961426, 'test/num_examples': 10000, 'score': 21045.717796325684, 'total_duration': 23240.907628774643, 'accumulated_submission_time': 21045.717796325684, 'accumulated_eval_time': 2191.300128698349, 'accumulated_logging_time': 1.5172581672668457, 'global_step': 45882, 'preemption_count': 0}), (46801, {'train/accuracy': 0.3951171934604645, 'train/loss': 2.819166421890259, 'validation/accuracy': 0.36381998658180237, 'validation/loss': 3.004495143890381, 'validation/num_examples': 50000, 'test/accuracy': 0.2831000089645386, 'test/loss': 3.567754030227661, 'test/num_examples': 10000, 'score': 21465.826851129532, 'total_duration': 23702.38392972946, 'accumulated_submission_time': 21465.826851129532, 'accumulated_eval_time': 2232.58002948761, 'accumulated_logging_time': 1.55698823928833, 'global_step': 46801, 'preemption_count': 0}), (47723, {'train/accuracy': 0.4143163859844208, 'train/loss': 2.6614110469818115, 'validation/accuracy': 0.3860200047492981, 'validation/loss': 2.80824875831604, 'validation/num_examples': 50000, 'test/accuracy': 0.2997000217437744, 'test/loss': 3.443194627761841, 'test/num_examples': 10000, 'score': 21886.131243228912, 'total_duration': 24165.59008526802, 'accumulated_submission_time': 21886.131243228912, 'accumulated_eval_time': 2275.397953271866, 'accumulated_logging_time': 1.593160629272461, 'global_step': 47723, 'preemption_count': 0}), (48642, {'train/accuracy': 0.40107420086860657, 'train/loss': 2.7443270683288574, 'validation/accuracy': 0.3771199882030487, 'validation/loss': 2.8815295696258545, 'validation/num_examples': 50000, 'test/accuracy': 0.29040002822875977, 'test/loss': 3.4814419746398926, 'test/num_examples': 10000, 'score': 22306.227305173874, 'total_duration': 24632.510838747025, 'accumulated_submission_time': 22306.227305173874, 'accumulated_eval_time': 2322.1383290290833, 'accumulated_logging_time': 1.6305792331695557, 'global_step': 48642, 'preemption_count': 0}), (49559, {'train/accuracy': 0.41294920444488525, 'train/loss': 2.7169084548950195, 'validation/accuracy': 0.3819800019264221, 'validation/loss': 2.8852477073669434, 'validation/num_examples': 50000, 'test/accuracy': 0.28700000047683716, 'test/loss': 3.4870080947875977, 'test/num_examples': 10000, 'score': 22726.256851911545, 'total_duration': 25097.48213648796, 'accumulated_submission_time': 22726.256851911545, 'accumulated_eval_time': 2366.9990010261536, 'accumulated_logging_time': 1.662933588027954, 'global_step': 49559, 'preemption_count': 0}), (50478, {'train/accuracy': 0.4268554449081421, 'train/loss': 2.6102540493011475, 'validation/accuracy': 0.39751997590065, 'validation/loss': 2.7804811000823975, 'validation/num_examples': 50000, 'test/accuracy': 0.3005000054836273, 'test/loss': 3.3917243480682373, 'test/num_examples': 10000, 'score': 23146.497085094452, 'total_duration': 25563.538495779037, 'accumulated_submission_time': 23146.497085094452, 'accumulated_eval_time': 2412.7322528362274, 'accumulated_logging_time': 1.6987645626068115, 'global_step': 50478, 'preemption_count': 0}), (51397, {'train/accuracy': 0.42949217557907104, 'train/loss': 2.56564998626709, 'validation/accuracy': 0.4009999930858612, 'validation/loss': 2.729902505874634, 'validation/num_examples': 50000, 'test/accuracy': 0.3053000271320343, 'test/loss': 3.3814618587493896, 'test/num_examples': 10000, 'score': 23566.498861551285, 'total_duration': 26032.064838647842, 'accumulated_submission_time': 23566.498861551285, 'accumulated_eval_time': 2461.1722581386566, 'accumulated_logging_time': 1.734628677368164, 'global_step': 51397, 'preemption_count': 0}), (52315, {'train/accuracy': 0.4299023449420929, 'train/loss': 2.5807902812957764, 'validation/accuracy': 0.39805999398231506, 'validation/loss': 2.752584934234619, 'validation/num_examples': 50000, 'test/accuracy': 0.31070002913475037, 'test/loss': 3.378159523010254, 'test/num_examples': 10000, 'score': 23986.7883477211, 'total_duration': 26497.21853017807, 'accumulated_submission_time': 23986.7883477211, 'accumulated_eval_time': 2505.948692560196, 'accumulated_logging_time': 1.7748017311096191, 'global_step': 52315, 'preemption_count': 0}), (53234, {'train/accuracy': 0.47236326336860657, 'train/loss': 2.374985694885254, 'validation/accuracy': 0.4041999876499176, 'validation/loss': 2.7307236194610596, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.353266954421997, 'test/num_examples': 10000, 'score': 24406.975200414658, 'total_duration': 26964.357704639435, 'accumulated_submission_time': 24406.975200414658, 'accumulated_eval_time': 2552.8182249069214, 'accumulated_logging_time': 1.8105263710021973, 'global_step': 53234, 'preemption_count': 0}), (54153, {'train/accuracy': 0.4276367127895355, 'train/loss': 2.59726619720459, 'validation/accuracy': 0.3992999792098999, 'validation/loss': 2.7636921405792236, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.398660659790039, 'test/num_examples': 10000, 'score': 24827.1175699234, 'total_duration': 27435.02015376091, 'accumulated_submission_time': 24827.1175699234, 'accumulated_eval_time': 2603.2553062438965, 'accumulated_logging_time': 1.8466713428497314, 'global_step': 54153, 'preemption_count': 0}), (55074, {'train/accuracy': 0.4376757740974426, 'train/loss': 2.500518798828125, 'validation/accuracy': 0.4105999767780304, 'validation/loss': 2.6731464862823486, 'validation/num_examples': 50000, 'test/accuracy': 0.31850001215934753, 'test/loss': 3.2860732078552246, 'test/num_examples': 10000, 'score': 25247.319982767105, 'total_duration': 27898.86329269409, 'accumulated_submission_time': 25247.319982767105, 'accumulated_eval_time': 2646.8072805404663, 'accumulated_logging_time': 1.887636423110962, 'global_step': 55074, 'preemption_count': 0}), (55995, {'train/accuracy': 0.44322264194488525, 'train/loss': 2.532923936843872, 'validation/accuracy': 0.402319997549057, 'validation/loss': 2.7546207904815674, 'validation/num_examples': 50000, 'test/accuracy': 0.302700012922287, 'test/loss': 3.414133310317993, 'test/num_examples': 10000, 'score': 25667.46424293518, 'total_duration': 28362.443382501602, 'accumulated_submission_time': 25667.46424293518, 'accumulated_eval_time': 2690.1619765758514, 'accumulated_logging_time': 1.9212877750396729, 'global_step': 55995, 'preemption_count': 0}), (56916, {'train/accuracy': 0.44078123569488525, 'train/loss': 2.5251173973083496, 'validation/accuracy': 0.41113999485969543, 'validation/loss': 2.6807515621185303, 'validation/num_examples': 50000, 'test/accuracy': 0.3193000257015228, 'test/loss': 3.296461582183838, 'test/num_examples': 10000, 'score': 26087.783744573593, 'total_duration': 28826.425322294235, 'accumulated_submission_time': 26087.783744573593, 'accumulated_eval_time': 2733.7377874851227, 'accumulated_logging_time': 1.959881067276001, 'global_step': 56916, 'preemption_count': 0}), (57836, {'train/accuracy': 0.4381054639816284, 'train/loss': 2.5220413208007812, 'validation/accuracy': 0.4074999988079071, 'validation/loss': 2.682382345199585, 'validation/num_examples': 50000, 'test/accuracy': 0.3222000300884247, 'test/loss': 3.286940097808838, 'test/num_examples': 10000, 'score': 26507.91575574875, 'total_duration': 29291.23188686371, 'accumulated_submission_time': 26507.91575574875, 'accumulated_eval_time': 2778.325141429901, 'accumulated_logging_time': 1.9994747638702393, 'global_step': 57836, 'preemption_count': 0}), (58754, {'train/accuracy': 0.4493750035762787, 'train/loss': 2.5028460025787354, 'validation/accuracy': 0.41589999198913574, 'validation/loss': 2.6864118576049805, 'validation/num_examples': 50000, 'test/accuracy': 0.3222000300884247, 'test/loss': 3.29914927482605, 'test/num_examples': 10000, 'score': 26927.859172344208, 'total_duration': 29757.561191558838, 'accumulated_submission_time': 26927.859172344208, 'accumulated_eval_time': 2824.6300699710846, 'accumulated_logging_time': 2.0336482524871826, 'global_step': 58754, 'preemption_count': 0}), (59675, {'train/accuracy': 0.4404882788658142, 'train/loss': 2.5459964275360107, 'validation/accuracy': 0.40943998098373413, 'validation/loss': 2.7102136611938477, 'validation/num_examples': 50000, 'test/accuracy': 0.3230000138282776, 'test/loss': 3.3165783882141113, 'test/num_examples': 10000, 'score': 27347.924685001373, 'total_duration': 30223.841745615005, 'accumulated_submission_time': 27347.924685001373, 'accumulated_eval_time': 2870.763954639435, 'accumulated_logging_time': 2.0666730403900146, 'global_step': 59675, 'preemption_count': 0}), (60594, {'train/accuracy': 0.453437477350235, 'train/loss': 2.441464424133301, 'validation/accuracy': 0.42587998509407043, 'validation/loss': 2.5996832847595215, 'validation/num_examples': 50000, 'test/accuracy': 0.33310002088546753, 'test/loss': 3.216318130493164, 'test/num_examples': 10000, 'score': 27768.17107129097, 'total_duration': 30688.837917804718, 'accumulated_submission_time': 27768.17107129097, 'accumulated_eval_time': 2915.4320845603943, 'accumulated_logging_time': 2.101196527481079, 'global_step': 60594, 'preemption_count': 0}), (61513, {'train/accuracy': 0.454902321100235, 'train/loss': 2.44576096534729, 'validation/accuracy': 0.4205799996852875, 'validation/loss': 2.644345283508301, 'validation/num_examples': 50000, 'test/accuracy': 0.32280001044273376, 'test/loss': 3.2577946186065674, 'test/num_examples': 10000, 'score': 28188.53337931633, 'total_duration': 31157.204171419144, 'accumulated_submission_time': 28188.53337931633, 'accumulated_eval_time': 2963.3469684123993, 'accumulated_logging_time': 2.1426825523376465, 'global_step': 61513, 'preemption_count': 0}), (62433, {'train/accuracy': 0.45238280296325684, 'train/loss': 2.508619546890259, 'validation/accuracy': 0.4140399992465973, 'validation/loss': 2.6991686820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.32260000705718994, 'test/loss': 3.3033504486083984, 'test/num_examples': 10000, 'score': 28608.848690748215, 'total_duration': 31622.267642736435, 'accumulated_submission_time': 28608.848690748215, 'accumulated_eval_time': 3008.009134531021, 'accumulated_logging_time': 2.1812844276428223, 'global_step': 62433, 'preemption_count': 0}), (63353, {'train/accuracy': 0.4460742175579071, 'train/loss': 2.4987034797668457, 'validation/accuracy': 0.4134199917316437, 'validation/loss': 2.6845717430114746, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.3364946842193604, 'test/num_examples': 10000, 'score': 29028.77939391136, 'total_duration': 32091.04265642166, 'accumulated_submission_time': 29028.77939391136, 'accumulated_eval_time': 3056.772082090378, 'accumulated_logging_time': 2.215562343597412, 'global_step': 63353, 'preemption_count': 0}), (64271, {'train/accuracy': 0.45146483182907104, 'train/loss': 2.4649150371551514, 'validation/accuracy': 0.4184199869632721, 'validation/loss': 2.648669719696045, 'validation/num_examples': 50000, 'test/accuracy': 0.32420000433921814, 'test/loss': 3.2666985988616943, 'test/num_examples': 10000, 'score': 29448.936309099197, 'total_duration': 32559.199233531952, 'accumulated_submission_time': 29448.936309099197, 'accumulated_eval_time': 3104.6899168491364, 'accumulated_logging_time': 2.250218152999878, 'global_step': 64271, 'preemption_count': 0}), (65189, {'train/accuracy': 0.4797070324420929, 'train/loss': 2.2820422649383545, 'validation/accuracy': 0.42282000184059143, 'validation/loss': 2.5929930210113525, 'validation/num_examples': 50000, 'test/accuracy': 0.33160001039505005, 'test/loss': 3.247788906097412, 'test/num_examples': 10000, 'score': 29869.149247169495, 'total_duration': 33023.18640756607, 'accumulated_submission_time': 29869.149247169495, 'accumulated_eval_time': 3148.381046772003, 'accumulated_logging_time': 2.2855420112609863, 'global_step': 65189, 'preemption_count': 0}), (66108, {'train/accuracy': 0.46281248331069946, 'train/loss': 2.4145824909210205, 'validation/accuracy': 0.4338599741458893, 'validation/loss': 2.566502094268799, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.1849935054779053, 'test/num_examples': 10000, 'score': 30289.283529758453, 'total_duration': 33482.63961672783, 'accumulated_submission_time': 30289.283529758453, 'accumulated_eval_time': 3187.6144468784332, 'accumulated_logging_time': 2.323702335357666, 'global_step': 66108, 'preemption_count': 0}), (67028, {'train/accuracy': 0.4659765660762787, 'train/loss': 2.3797836303710938, 'validation/accuracy': 0.4309599995613098, 'validation/loss': 2.57188081741333, 'validation/num_examples': 50000, 'test/accuracy': 0.32930001616477966, 'test/loss': 3.2058255672454834, 'test/num_examples': 10000, 'score': 30709.503110408783, 'total_duration': 33948.036386966705, 'accumulated_submission_time': 30709.503110408783, 'accumulated_eval_time': 3232.7051644325256, 'accumulated_logging_time': 2.3622336387634277, 'global_step': 67028, 'preemption_count': 0}), (67948, {'train/accuracy': 0.4738866984844208, 'train/loss': 2.352745532989502, 'validation/accuracy': 0.4340199828147888, 'validation/loss': 2.5681827068328857, 'validation/num_examples': 50000, 'test/accuracy': 0.3377000093460083, 'test/loss': 3.203951835632324, 'test/num_examples': 10000, 'score': 31129.867817878723, 'total_duration': 34414.95987582207, 'accumulated_submission_time': 31129.867817878723, 'accumulated_eval_time': 3279.18199968338, 'accumulated_logging_time': 2.3966870307922363, 'global_step': 67948, 'preemption_count': 0}), (68868, {'train/accuracy': 0.4649609327316284, 'train/loss': 2.447537422180176, 'validation/accuracy': 0.43293997645378113, 'validation/loss': 2.5877325534820557, 'validation/num_examples': 50000, 'test/accuracy': 0.3359000086784363, 'test/loss': 3.2074527740478516, 'test/num_examples': 10000, 'score': 31550.06188440323, 'total_duration': 34880.10146713257, 'accumulated_submission_time': 31550.06188440323, 'accumulated_eval_time': 3324.0429894924164, 'accumulated_logging_time': 2.434640884399414, 'global_step': 68868, 'preemption_count': 0}), (69789, {'train/accuracy': 0.4654882848262787, 'train/loss': 2.4043633937835693, 'validation/accuracy': 0.4331599771976471, 'validation/loss': 2.563265800476074, 'validation/num_examples': 50000, 'test/accuracy': 0.3391000032424927, 'test/loss': 3.190361976623535, 'test/num_examples': 10000, 'score': 31970.368980884552, 'total_duration': 35347.10148000717, 'accumulated_submission_time': 31970.368980884552, 'accumulated_eval_time': 3370.650318622589, 'accumulated_logging_time': 2.472820997238159, 'global_step': 69789, 'preemption_count': 0}), (70706, {'train/accuracy': 0.47871091961860657, 'train/loss': 2.317901611328125, 'validation/accuracy': 0.439520001411438, 'validation/loss': 2.5207505226135254, 'validation/num_examples': 50000, 'test/accuracy': 0.34360000491142273, 'test/loss': 3.145872116088867, 'test/num_examples': 10000, 'score': 32390.55333662033, 'total_duration': 35809.67774987221, 'accumulated_submission_time': 32390.55333662033, 'accumulated_eval_time': 3412.957806110382, 'accumulated_logging_time': 2.5098109245300293, 'global_step': 70706, 'preemption_count': 0}), (71625, {'train/accuracy': 0.46162107586860657, 'train/loss': 2.4654088020324707, 'validation/accuracy': 0.4322199821472168, 'validation/loss': 2.6213440895080566, 'validation/num_examples': 50000, 'test/accuracy': 0.3367000222206116, 'test/loss': 3.232973098754883, 'test/num_examples': 10000, 'score': 32810.782964229584, 'total_duration': 36277.062376499176, 'accumulated_submission_time': 32810.782964229584, 'accumulated_eval_time': 3460.0244784355164, 'accumulated_logging_time': 2.551076889038086, 'global_step': 71625, 'preemption_count': 0}), (72547, {'train/accuracy': 0.4809960722923279, 'train/loss': 2.3176419734954834, 'validation/accuracy': 0.4526999890804291, 'validation/loss': 2.466217041015625, 'validation/num_examples': 50000, 'test/accuracy': 0.3489000201225281, 'test/loss': 3.1029651165008545, 'test/num_examples': 10000, 'score': 33230.88841557503, 'total_duration': 36743.15631175041, 'accumulated_submission_time': 33230.88841557503, 'accumulated_eval_time': 3505.9291064739227, 'accumulated_logging_time': 2.587003231048584, 'global_step': 72547, 'preemption_count': 0}), (73466, {'train/accuracy': 0.48060545325279236, 'train/loss': 2.2896952629089355, 'validation/accuracy': 0.44373998045921326, 'validation/loss': 2.492424249649048, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.121434211730957, 'test/num_examples': 10000, 'score': 33650.96217060089, 'total_duration': 37208.76399350166, 'accumulated_submission_time': 33650.96217060089, 'accumulated_eval_time': 3551.381100177765, 'accumulated_logging_time': 2.621609687805176, 'global_step': 73466, 'preemption_count': 0}), (74386, {'train/accuracy': 0.5169921517372131, 'train/loss': 2.1353821754455566, 'validation/accuracy': 0.4467199742794037, 'validation/loss': 2.486428737640381, 'validation/num_examples': 50000, 'test/accuracy': 0.3522000312805176, 'test/loss': 3.1258039474487305, 'test/num_examples': 10000, 'score': 34071.35045528412, 'total_duration': 37676.37067079544, 'accumulated_submission_time': 34071.35045528412, 'accumulated_eval_time': 3598.5108897686005, 'accumulated_logging_time': 2.663499355316162, 'global_step': 74386, 'preemption_count': 0}), (75304, {'train/accuracy': 0.48771482706069946, 'train/loss': 2.2401325702667236, 'validation/accuracy': 0.45593997836112976, 'validation/loss': 2.4114251136779785, 'validation/num_examples': 50000, 'test/accuracy': 0.35130003094673157, 'test/loss': 3.071357011795044, 'test/num_examples': 10000, 'score': 34491.62985539436, 'total_duration': 38136.859938144684, 'accumulated_submission_time': 34491.62985539436, 'accumulated_eval_time': 3638.625265598297, 'accumulated_logging_time': 2.7113420963287354, 'global_step': 75304, 'preemption_count': 0}), (76223, {'train/accuracy': 0.4906249940395355, 'train/loss': 2.2219138145446777, 'validation/accuracy': 0.4575199782848358, 'validation/loss': 2.4190688133239746, 'validation/num_examples': 50000, 'test/accuracy': 0.3555000126361847, 'test/loss': 3.057042121887207, 'test/num_examples': 10000, 'score': 34911.86199808121, 'total_duration': 38600.17257928848, 'accumulated_submission_time': 34911.86199808121, 'accumulated_eval_time': 3681.6170043945312, 'accumulated_logging_time': 2.7527599334716797, 'global_step': 76223, 'preemption_count': 0}), (77142, {'train/accuracy': 0.5056250095367432, 'train/loss': 2.1703872680664062, 'validation/accuracy': 0.4499799907207489, 'validation/loss': 2.44785213470459, 'validation/num_examples': 50000, 'test/accuracy': 0.3538000285625458, 'test/loss': 3.080111503601074, 'test/num_examples': 10000, 'score': 35332.00664615631, 'total_duration': 39066.17379283905, 'accumulated_submission_time': 35332.00664615631, 'accumulated_eval_time': 3727.3882570266724, 'accumulated_logging_time': 2.79075288772583, 'global_step': 77142, 'preemption_count': 0}), (78062, {'train/accuracy': 0.48580077290534973, 'train/loss': 2.2651078701019287, 'validation/accuracy': 0.45277997851371765, 'validation/loss': 2.438197374343872, 'validation/num_examples': 50000, 'test/accuracy': 0.3588000237941742, 'test/loss': 3.0841751098632812, 'test/num_examples': 10000, 'score': 35752.21084976196, 'total_duration': 39530.53333187103, 'accumulated_submission_time': 35752.21084976196, 'accumulated_eval_time': 3771.4496726989746, 'accumulated_logging_time': 2.83611798286438, 'global_step': 78062, 'preemption_count': 0}), (78982, {'train/accuracy': 0.48689451813697815, 'train/loss': 2.2959415912628174, 'validation/accuracy': 0.4545799791812897, 'validation/loss': 2.4721052646636963, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.1361148357391357, 'test/num_examples': 10000, 'score': 36172.48115777969, 'total_duration': 39996.05541801453, 'accumulated_submission_time': 36172.48115777969, 'accumulated_eval_time': 3816.6144077777863, 'accumulated_logging_time': 2.875884532928467, 'global_step': 78982, 'preemption_count': 0}), (79903, {'train/accuracy': 0.4998827874660492, 'train/loss': 2.2071332931518555, 'validation/accuracy': 0.4567599892616272, 'validation/loss': 2.4251766204833984, 'validation/num_examples': 50000, 'test/accuracy': 0.3619000315666199, 'test/loss': 3.0350892543792725, 'test/num_examples': 10000, 'score': 36592.814427137375, 'total_duration': 40465.15758180618, 'accumulated_submission_time': 36592.814427137375, 'accumulated_eval_time': 3865.289860725403, 'accumulated_logging_time': 2.9212820529937744, 'global_step': 79903, 'preemption_count': 0}), (80823, {'train/accuracy': 0.4993554651737213, 'train/loss': 2.1888513565063477, 'validation/accuracy': 0.471560001373291, 'validation/loss': 2.3428831100463867, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 2.9896857738494873, 'test/num_examples': 10000, 'score': 37013.1857984066, 'total_duration': 40926.628831624985, 'accumulated_submission_time': 37013.1857984066, 'accumulated_eval_time': 3906.299390554428, 'accumulated_logging_time': 2.9639267921447754, 'global_step': 80823, 'preemption_count': 0}), (81739, {'train/accuracy': 0.5030664205551147, 'train/loss': 2.173933744430542, 'validation/accuracy': 0.47307997941970825, 'validation/loss': 2.3356215953826904, 'validation/num_examples': 50000, 'test/accuracy': 0.36880001425743103, 'test/loss': 2.9926578998565674, 'test/num_examples': 10000, 'score': 37433.493015527725, 'total_duration': 41391.62658786774, 'accumulated_submission_time': 37433.493015527725, 'accumulated_eval_time': 3950.904098033905, 'accumulated_logging_time': 3.0028018951416016, 'global_step': 81739, 'preemption_count': 0}), (82657, {'train/accuracy': 0.5142968893051147, 'train/loss': 2.146437168121338, 'validation/accuracy': 0.47237998247146606, 'validation/loss': 2.358328104019165, 'validation/num_examples': 50000, 'test/accuracy': 0.3644000291824341, 'test/loss': 3.0233078002929688, 'test/num_examples': 10000, 'score': 37853.55633306503, 'total_duration': 41858.507304906845, 'accumulated_submission_time': 37853.55633306503, 'accumulated_eval_time': 3997.634565114975, 'accumulated_logging_time': 3.0432190895080566, 'global_step': 82657, 'preemption_count': 0}), (83577, {'train/accuracy': 0.5132030844688416, 'train/loss': 2.140690803527832, 'validation/accuracy': 0.47481998801231384, 'validation/loss': 2.3312788009643555, 'validation/num_examples': 50000, 'test/accuracy': 0.3646000027656555, 'test/loss': 2.994065284729004, 'test/num_examples': 10000, 'score': 38273.708990097046, 'total_duration': 42315.99324274063, 'accumulated_submission_time': 38273.708990097046, 'accumulated_eval_time': 4034.875780582428, 'accumulated_logging_time': 3.0878307819366455, 'global_step': 83577, 'preemption_count': 0}), (84497, {'train/accuracy': 0.5048632621765137, 'train/loss': 2.2373056411743164, 'validation/accuracy': 0.46403998136520386, 'validation/loss': 2.4101569652557373, 'validation/num_examples': 50000, 'test/accuracy': 0.3582000136375427, 'test/loss': 3.0756309032440186, 'test/num_examples': 10000, 'score': 38693.901599407196, 'total_duration': 42781.52650523186, 'accumulated_submission_time': 38693.901599407196, 'accumulated_eval_time': 4080.1282589435577, 'accumulated_logging_time': 3.1285057067871094, 'global_step': 84497, 'preemption_count': 0}), (85417, {'train/accuracy': 0.5148242115974426, 'train/loss': 2.1396749019622803, 'validation/accuracy': 0.4755999743938446, 'validation/loss': 2.3394620418548584, 'validation/num_examples': 50000, 'test/accuracy': 0.3726000189781189, 'test/loss': 2.975393772125244, 'test/num_examples': 10000, 'score': 39114.07578897476, 'total_duration': 43244.331899404526, 'accumulated_submission_time': 39114.07578897476, 'accumulated_eval_time': 4122.668626070023, 'accumulated_logging_time': 3.1721973419189453, 'global_step': 85417, 'preemption_count': 0}), (86337, {'train/accuracy': 0.5555468797683716, 'train/loss': 1.9123483896255493, 'validation/accuracy': 0.4817200005054474, 'validation/loss': 2.272268295288086, 'validation/num_examples': 50000, 'test/accuracy': 0.37790000438690186, 'test/loss': 2.9314382076263428, 'test/num_examples': 10000, 'score': 39534.43977665901, 'total_duration': 43711.998577833176, 'accumulated_submission_time': 39534.43977665901, 'accumulated_eval_time': 4169.881982803345, 'accumulated_logging_time': 3.214378833770752, 'global_step': 86337, 'preemption_count': 0}), (87257, {'train/accuracy': 0.52099609375, 'train/loss': 2.0965120792388916, 'validation/accuracy': 0.48593997955322266, 'validation/loss': 2.274020195007324, 'validation/num_examples': 50000, 'test/accuracy': 0.3801000118255615, 'test/loss': 2.9268369674682617, 'test/num_examples': 10000, 'score': 39954.651678562164, 'total_duration': 44175.574429512024, 'accumulated_submission_time': 39954.651678562164, 'accumulated_eval_time': 4213.158932924271, 'accumulated_logging_time': 3.2534549236297607, 'global_step': 87257, 'preemption_count': 0}), (88175, {'train/accuracy': 0.5269726514816284, 'train/loss': 2.0405514240264893, 'validation/accuracy': 0.48993998765945435, 'validation/loss': 2.239298105239868, 'validation/num_examples': 50000, 'test/accuracy': 0.37540000677108765, 'test/loss': 2.916119337081909, 'test/num_examples': 10000, 'score': 40374.94868397713, 'total_duration': 44643.63994884491, 'accumulated_submission_time': 40374.94868397713, 'accumulated_eval_time': 4260.838416099548, 'accumulated_logging_time': 3.2945399284362793, 'global_step': 88175, 'preemption_count': 0}), (89095, {'train/accuracy': 0.5325976610183716, 'train/loss': 2.0417697429656982, 'validation/accuracy': 0.4813999831676483, 'validation/loss': 2.3064215183258057, 'validation/num_examples': 50000, 'test/accuracy': 0.38030001521110535, 'test/loss': 2.940920829772949, 'test/num_examples': 10000, 'score': 40795.083706617355, 'total_duration': 45108.95787191391, 'accumulated_submission_time': 40795.083706617355, 'accumulated_eval_time': 4305.934587717056, 'accumulated_logging_time': 3.3338735103607178, 'global_step': 89095, 'preemption_count': 0}), (90014, {'train/accuracy': 0.5163866877555847, 'train/loss': 2.1249258518218994, 'validation/accuracy': 0.4807799756526947, 'validation/loss': 2.301429033279419, 'validation/num_examples': 50000, 'test/accuracy': 0.3824000060558319, 'test/loss': 2.9366328716278076, 'test/num_examples': 10000, 'score': 41215.381110191345, 'total_duration': 45574.949031591415, 'accumulated_submission_time': 41215.381110191345, 'accumulated_eval_time': 4351.538655757904, 'accumulated_logging_time': 3.3755288124084473, 'global_step': 90014, 'preemption_count': 0}), (90933, {'train/accuracy': 0.5344530940055847, 'train/loss': 2.001258134841919, 'validation/accuracy': 0.49903997778892517, 'validation/loss': 2.1946213245391846, 'validation/num_examples': 50000, 'test/accuracy': 0.3911000192165375, 'test/loss': 2.831852436065674, 'test/num_examples': 10000, 'score': 41635.74610328674, 'total_duration': 46040.23263311386, 'accumulated_submission_time': 41635.74610328674, 'accumulated_eval_time': 4396.368488788605, 'accumulated_logging_time': 3.417140483856201, 'global_step': 90933, 'preemption_count': 0}), (91852, {'train/accuracy': 0.5383593440055847, 'train/loss': 2.0077288150787354, 'validation/accuracy': 0.49285998940467834, 'validation/loss': 2.2256059646606445, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 2.87404203414917, 'test/num_examples': 10000, 'score': 42056.00476717949, 'total_duration': 46508.55019235611, 'accumulated_submission_time': 42056.00476717949, 'accumulated_eval_time': 4444.340796709061, 'accumulated_logging_time': 3.457115888595581, 'global_step': 91852, 'preemption_count': 0}), (92769, {'train/accuracy': 0.5292577743530273, 'train/loss': 2.0604593753814697, 'validation/accuracy': 0.4968799948692322, 'validation/loss': 2.218428134918213, 'validation/num_examples': 50000, 'test/accuracy': 0.39410001039505005, 'test/loss': 2.8445026874542236, 'test/num_examples': 10000, 'score': 42476.11767077446, 'total_duration': 46977.44144821167, 'accumulated_submission_time': 42476.11767077446, 'accumulated_eval_time': 4493.034183979034, 'accumulated_logging_time': 3.495344877243042, 'global_step': 92769, 'preemption_count': 0}), (93690, {'train/accuracy': 0.5306054353713989, 'train/loss': 2.0200607776641846, 'validation/accuracy': 0.497979998588562, 'validation/loss': 2.209547281265259, 'validation/num_examples': 50000, 'test/accuracy': 0.39660000801086426, 'test/loss': 2.847208023071289, 'test/num_examples': 10000, 'score': 42896.114788770676, 'total_duration': 47443.92788076401, 'accumulated_submission_time': 42896.114788770676, 'accumulated_eval_time': 4539.431235074997, 'accumulated_logging_time': 3.540785312652588, 'global_step': 93690, 'preemption_count': 0}), (94609, {'train/accuracy': 0.5318945050239563, 'train/loss': 2.0958914756774902, 'validation/accuracy': 0.49309998750686646, 'validation/loss': 2.292987108230591, 'validation/num_examples': 50000, 'test/accuracy': 0.38920003175735474, 'test/loss': 2.920865297317505, 'test/num_examples': 10000, 'score': 43316.36288237572, 'total_duration': 47912.20147418976, 'accumulated_submission_time': 43316.36288237572, 'accumulated_eval_time': 4587.36496925354, 'accumulated_logging_time': 3.5851848125457764, 'global_step': 94609, 'preemption_count': 0}), (95526, {'train/accuracy': 0.5541210770606995, 'train/loss': 1.9405015707015991, 'validation/accuracy': 0.504859983921051, 'validation/loss': 2.179647922515869, 'validation/num_examples': 50000, 'test/accuracy': 0.3992000222206116, 'test/loss': 2.8288564682006836, 'test/num_examples': 10000, 'score': 43736.64577579498, 'total_duration': 48381.214473724365, 'accumulated_submission_time': 43736.64577579498, 'accumulated_eval_time': 4636.007810115814, 'accumulated_logging_time': 3.6254639625549316, 'global_step': 95526, 'preemption_count': 0}), (96446, {'train/accuracy': 0.5423046946525574, 'train/loss': 2.020184278488159, 'validation/accuracy': 0.5077199935913086, 'validation/loss': 2.1947858333587646, 'validation/num_examples': 50000, 'test/accuracy': 0.39070001244544983, 'test/loss': 2.8464391231536865, 'test/num_examples': 10000, 'score': 44156.965695381165, 'total_duration': 48843.16408109665, 'accumulated_submission_time': 44156.965695381165, 'accumulated_eval_time': 4677.541831970215, 'accumulated_logging_time': 3.672870397567749, 'global_step': 96446, 'preemption_count': 0}), (97366, {'train/accuracy': 0.5404687523841858, 'train/loss': 2.0097057819366455, 'validation/accuracy': 0.5008000135421753, 'validation/loss': 2.208786725997925, 'validation/num_examples': 50000, 'test/accuracy': 0.395300030708313, 'test/loss': 2.858224391937256, 'test/num_examples': 10000, 'score': 44577.152952194214, 'total_duration': 49311.044062137604, 'accumulated_submission_time': 44577.152952194214, 'accumulated_eval_time': 4725.14670586586, 'accumulated_logging_time': 3.7132389545440674, 'global_step': 97366, 'preemption_count': 0}), (98286, {'train/accuracy': 0.5715429782867432, 'train/loss': 1.8492099046707153, 'validation/accuracy': 0.5137400031089783, 'validation/loss': 2.139185667037964, 'validation/num_examples': 50000, 'test/accuracy': 0.3963000178337097, 'test/loss': 2.810023069381714, 'test/num_examples': 10000, 'score': 44997.3252222538, 'total_duration': 49779.63790535927, 'accumulated_submission_time': 44997.3252222538, 'accumulated_eval_time': 4773.47830247879, 'accumulated_logging_time': 3.7554454803466797, 'global_step': 98286, 'preemption_count': 0}), (99206, {'train/accuracy': 0.5505663752555847, 'train/loss': 1.9494009017944336, 'validation/accuracy': 0.5107200145721436, 'validation/loss': 2.1389076709747314, 'validation/num_examples': 50000, 'test/accuracy': 0.40160003304481506, 'test/loss': 2.7917513847351074, 'test/num_examples': 10000, 'score': 45417.43864130974, 'total_duration': 50241.46690893173, 'accumulated_submission_time': 45417.43864130974, 'accumulated_eval_time': 4815.0956864356995, 'accumulated_logging_time': 3.802260398864746, 'global_step': 99206, 'preemption_count': 0}), (100126, {'train/accuracy': 0.5661718845367432, 'train/loss': 1.8715652227401733, 'validation/accuracy': 0.5231800079345703, 'validation/loss': 2.0831315517425537, 'validation/num_examples': 50000, 'test/accuracy': 0.41370001435279846, 'test/loss': 2.7351529598236084, 'test/num_examples': 10000, 'score': 45837.39217829704, 'total_duration': 50710.299309015274, 'accumulated_submission_time': 45837.39217829704, 'accumulated_eval_time': 4863.8861446380615, 'accumulated_logging_time': 3.843090057373047, 'global_step': 100126, 'preemption_count': 0}), (101047, {'train/accuracy': 0.5694140791893005, 'train/loss': 1.840009093284607, 'validation/accuracy': 0.5202800035476685, 'validation/loss': 2.0846471786499023, 'validation/num_examples': 50000, 'test/accuracy': 0.4107000231742859, 'test/loss': 2.7359278202056885, 'test/num_examples': 10000, 'score': 46257.652686834335, 'total_duration': 51177.58735990524, 'accumulated_submission_time': 46257.652686834335, 'accumulated_eval_time': 4910.821338653564, 'accumulated_logging_time': 3.887923240661621, 'global_step': 101047, 'preemption_count': 0}), (101966, {'train/accuracy': 0.5561913847923279, 'train/loss': 1.9271278381347656, 'validation/accuracy': 0.5159400105476379, 'validation/loss': 2.1237990856170654, 'validation/num_examples': 50000, 'test/accuracy': 0.4108000099658966, 'test/loss': 2.763245105743408, 'test/num_examples': 10000, 'score': 46677.740731954575, 'total_duration': 51643.885112285614, 'accumulated_submission_time': 46677.740731954575, 'accumulated_eval_time': 4956.939247131348, 'accumulated_logging_time': 3.932757616043091, 'global_step': 101966, 'preemption_count': 0}), (102885, {'train/accuracy': 0.5678125023841858, 'train/loss': 1.8487435579299927, 'validation/accuracy': 0.5289799571037292, 'validation/loss': 2.0497944355010986, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.709082841873169, 'test/num_examples': 10000, 'score': 47097.766208171844, 'total_duration': 52111.843133449554, 'accumulated_submission_time': 47097.766208171844, 'accumulated_eval_time': 5004.784141540527, 'accumulated_logging_time': 3.972791910171509, 'global_step': 102885, 'preemption_count': 0}), (103806, {'train/accuracy': 0.5733789205551147, 'train/loss': 1.8240511417388916, 'validation/accuracy': 0.5295799970626831, 'validation/loss': 2.0383520126342773, 'validation/num_examples': 50000, 'test/accuracy': 0.41930001974105835, 'test/loss': 2.696749448776245, 'test/num_examples': 10000, 'score': 47517.937237262726, 'total_duration': 52581.48854184151, 'accumulated_submission_time': 47517.937237262726, 'accumulated_eval_time': 5054.160320997238, 'accumulated_logging_time': 4.023122072219849, 'global_step': 103806, 'preemption_count': 0}), (104725, {'train/accuracy': 0.567187488079071, 'train/loss': 1.8645880222320557, 'validation/accuracy': 0.5290799736976624, 'validation/loss': 2.053553581237793, 'validation/num_examples': 50000, 'test/accuracy': 0.4147000312805176, 'test/loss': 2.7142860889434814, 'test/num_examples': 10000, 'score': 47937.92220687866, 'total_duration': 53049.274918079376, 'accumulated_submission_time': 47937.92220687866, 'accumulated_eval_time': 5101.870931148529, 'accumulated_logging_time': 4.066912651062012, 'global_step': 104725, 'preemption_count': 0}), (105644, {'train/accuracy': 0.5702148079872131, 'train/loss': 1.839844822883606, 'validation/accuracy': 0.5308600068092346, 'validation/loss': 2.028477191925049, 'validation/num_examples': 50000, 'test/accuracy': 0.4223000109195709, 'test/loss': 2.6632344722747803, 'test/num_examples': 10000, 'score': 48358.190257549286, 'total_duration': 53519.78946995735, 'accumulated_submission_time': 48358.190257549286, 'accumulated_eval_time': 5152.026482105255, 'accumulated_logging_time': 4.110436916351318, 'global_step': 105644, 'preemption_count': 0}), (106563, {'train/accuracy': 0.5782421827316284, 'train/loss': 1.8550362586975098, 'validation/accuracy': 0.5310800075531006, 'validation/loss': 2.070237874984741, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.710397720336914, 'test/num_examples': 10000, 'score': 48778.23556470871, 'total_duration': 53983.98978614807, 'accumulated_submission_time': 48778.23556470871, 'accumulated_eval_time': 5196.08935046196, 'accumulated_logging_time': 4.1553053855896, 'global_step': 106563, 'preemption_count': 0}), (107482, {'train/accuracy': 0.6131640672683716, 'train/loss': 1.644121527671814, 'validation/accuracy': 0.536899983882904, 'validation/loss': 2.003920078277588, 'validation/num_examples': 50000, 'test/accuracy': 0.42920002341270447, 'test/loss': 2.6709156036376953, 'test/num_examples': 10000, 'score': 49198.48667836189, 'total_duration': 54452.193747758865, 'accumulated_submission_time': 49198.48667836189, 'accumulated_eval_time': 5243.95282626152, 'accumulated_logging_time': 4.197072982788086, 'global_step': 107482, 'preemption_count': 0}), (108401, {'train/accuracy': 0.5776953101158142, 'train/loss': 1.8307543992996216, 'validation/accuracy': 0.5416600108146667, 'validation/loss': 2.010225296020508, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.666642427444458, 'test/num_examples': 10000, 'score': 49619.11729288101, 'total_duration': 54916.2911529541, 'accumulated_submission_time': 49619.11729288101, 'accumulated_eval_time': 5287.324712753296, 'accumulated_logging_time': 4.244433879852295, 'global_step': 108401, 'preemption_count': 0}), (109321, {'train/accuracy': 0.5839257836341858, 'train/loss': 1.781617283821106, 'validation/accuracy': 0.5431399941444397, 'validation/loss': 1.9895758628845215, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.6396257877349854, 'test/num_examples': 10000, 'score': 50039.2133743763, 'total_duration': 55383.89688038826, 'accumulated_submission_time': 50039.2133743763, 'accumulated_eval_time': 5334.740091085434, 'accumulated_logging_time': 4.290728807449341, 'global_step': 109321, 'preemption_count': 0}), (110242, {'train/accuracy': 0.6037304401397705, 'train/loss': 1.6702401638031006, 'validation/accuracy': 0.5479999780654907, 'validation/loss': 1.9538955688476562, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.595966100692749, 'test/num_examples': 10000, 'score': 50459.243683576584, 'total_duration': 55849.685177087784, 'accumulated_submission_time': 50459.243683576584, 'accumulated_eval_time': 5380.403715848923, 'accumulated_logging_time': 4.336857795715332, 'global_step': 110242, 'preemption_count': 0}), (111161, {'train/accuracy': 0.5822851657867432, 'train/loss': 1.7880244255065918, 'validation/accuracy': 0.5461999773979187, 'validation/loss': 1.9647815227508545, 'validation/num_examples': 50000, 'test/accuracy': 0.4337000250816345, 'test/loss': 2.6289896965026855, 'test/num_examples': 10000, 'score': 50879.184905052185, 'total_duration': 56312.04675197601, 'accumulated_submission_time': 50879.184905052185, 'accumulated_eval_time': 5422.70272564888, 'accumulated_logging_time': 4.410296440124512, 'global_step': 111161, 'preemption_count': 0}), (112079, {'train/accuracy': 0.5941405892372131, 'train/loss': 1.7171932458877563, 'validation/accuracy': 0.5562199950218201, 'validation/loss': 1.896606683731079, 'validation/num_examples': 50000, 'test/accuracy': 0.44110003113746643, 'test/loss': 2.5732004642486572, 'test/num_examples': 10000, 'score': 51299.41596865654, 'total_duration': 56780.80129766464, 'accumulated_submission_time': 51299.41596865654, 'accumulated_eval_time': 5471.136475563049, 'accumulated_logging_time': 4.4519734382629395, 'global_step': 112079, 'preemption_count': 0}), (112999, {'train/accuracy': 0.6062109470367432, 'train/loss': 1.6639678478240967, 'validation/accuracy': 0.555899977684021, 'validation/loss': 1.902474045753479, 'validation/num_examples': 50000, 'test/accuracy': 0.4442000091075897, 'test/loss': 2.5536797046661377, 'test/num_examples': 10000, 'score': 51719.630373477936, 'total_duration': 57248.913175821304, 'accumulated_submission_time': 51719.630373477936, 'accumulated_eval_time': 5518.939175367355, 'accumulated_logging_time': 4.49985933303833, 'global_step': 112999, 'preemption_count': 0}), (113919, {'train/accuracy': 0.594042956829071, 'train/loss': 1.7198957204818726, 'validation/accuracy': 0.5557599663734436, 'validation/loss': 1.9055322408676147, 'validation/num_examples': 50000, 'test/accuracy': 0.4449000358581543, 'test/loss': 2.562439203262329, 'test/num_examples': 10000, 'score': 52139.66808462143, 'total_duration': 57718.53077292442, 'accumulated_submission_time': 52139.66808462143, 'accumulated_eval_time': 5568.422976732254, 'accumulated_logging_time': 4.548093795776367, 'global_step': 113919, 'preemption_count': 0}), (114839, {'train/accuracy': 0.6001757383346558, 'train/loss': 1.7225245237350464, 'validation/accuracy': 0.5551599860191345, 'validation/loss': 1.9314780235290527, 'validation/num_examples': 50000, 'test/accuracy': 0.4402000308036804, 'test/loss': 2.587394952774048, 'test/num_examples': 10000, 'score': 52559.89040374756, 'total_duration': 58184.54452776909, 'accumulated_submission_time': 52559.89040374756, 'accumulated_eval_time': 5614.12481713295, 'accumulated_logging_time': 4.5912158489227295, 'global_step': 114839, 'preemption_count': 0}), (115757, {'train/accuracy': 0.6089257597923279, 'train/loss': 1.6470164060592651, 'validation/accuracy': 0.5604599714279175, 'validation/loss': 1.8817566633224487, 'validation/num_examples': 50000, 'test/accuracy': 0.4474000334739685, 'test/loss': 2.5390818119049072, 'test/num_examples': 10000, 'score': 52980.26725935936, 'total_duration': 58651.523950338364, 'accumulated_submission_time': 52980.26725935936, 'accumulated_eval_time': 5660.633631229401, 'accumulated_logging_time': 4.6373395919799805, 'global_step': 115757, 'preemption_count': 0}), (116677, {'train/accuracy': 0.613476574420929, 'train/loss': 1.6504943370819092, 'validation/accuracy': 0.5635200142860413, 'validation/loss': 1.8760992288589478, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.5303001403808594, 'test/num_examples': 10000, 'score': 53400.54282641411, 'total_duration': 59118.91864323616, 'accumulated_submission_time': 53400.54282641411, 'accumulated_eval_time': 5707.661694765091, 'accumulated_logging_time': 4.6801183223724365, 'global_step': 116677, 'preemption_count': 0}), (117596, {'train/accuracy': 0.6048437356948853, 'train/loss': 1.6839405298233032, 'validation/accuracy': 0.5666399598121643, 'validation/loss': 1.874297022819519, 'validation/num_examples': 50000, 'test/accuracy': 0.4523000121116638, 'test/loss': 2.5318267345428467, 'test/num_examples': 10000, 'score': 53820.59474277496, 'total_duration': 59584.84307575226, 'accumulated_submission_time': 53820.59474277496, 'accumulated_eval_time': 5753.440539121628, 'accumulated_logging_time': 4.725946426391602, 'global_step': 117596, 'preemption_count': 0}), (118516, {'train/accuracy': 0.617871105670929, 'train/loss': 1.593552827835083, 'validation/accuracy': 0.5720199942588806, 'validation/loss': 1.8123306035995483, 'validation/num_examples': 50000, 'test/accuracy': 0.4612000286579132, 'test/loss': 2.4730336666107178, 'test/num_examples': 10000, 'score': 54240.80654287338, 'total_duration': 60053.70967531204, 'accumulated_submission_time': 54240.80654287338, 'accumulated_eval_time': 5801.9966831207275, 'accumulated_logging_time': 4.77754282951355, 'global_step': 118516, 'preemption_count': 0}), (119434, {'train/accuracy': 0.6364452838897705, 'train/loss': 1.5332109928131104, 'validation/accuracy': 0.5678399801254272, 'validation/loss': 1.8640707731246948, 'validation/num_examples': 50000, 'test/accuracy': 0.4488000273704529, 'test/loss': 2.510444164276123, 'test/num_examples': 10000, 'score': 54660.86072707176, 'total_duration': 60517.71708583832, 'accumulated_submission_time': 54660.86072707176, 'accumulated_eval_time': 5845.860307693481, 'accumulated_logging_time': 4.820557355880737, 'global_step': 119434, 'preemption_count': 0}), (120352, {'train/accuracy': 0.6158202886581421, 'train/loss': 1.6390470266342163, 'validation/accuracy': 0.5762199759483337, 'validation/loss': 1.8413300514221191, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.518050193786621, 'test/num_examples': 10000, 'score': 55081.265146017075, 'total_duration': 60984.77663850784, 'accumulated_submission_time': 55081.265146017075, 'accumulated_eval_time': 5892.424654722214, 'accumulated_logging_time': 4.865030288696289, 'global_step': 120352, 'preemption_count': 0}), (121273, {'train/accuracy': 0.6201562285423279, 'train/loss': 1.615761399269104, 'validation/accuracy': 0.5776799917221069, 'validation/loss': 1.8188271522521973, 'validation/num_examples': 50000, 'test/accuracy': 0.45920002460479736, 'test/loss': 2.49786114692688, 'test/num_examples': 10000, 'score': 55501.56821870804, 'total_duration': 61450.701137304306, 'accumulated_submission_time': 55501.56821870804, 'accumulated_eval_time': 5937.952259302139, 'accumulated_logging_time': 4.911031007766724, 'global_step': 121273, 'preemption_count': 0}), (122194, {'train/accuracy': 0.6444921493530273, 'train/loss': 1.5002819299697876, 'validation/accuracy': 0.583139955997467, 'validation/loss': 1.7808510065078735, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.44468355178833, 'test/num_examples': 10000, 'score': 55921.785950899124, 'total_duration': 61921.7659611702, 'accumulated_submission_time': 55921.785950899124, 'accumulated_eval_time': 5988.703502893448, 'accumulated_logging_time': 4.95950722694397, 'global_step': 122194, 'preemption_count': 0}), (123114, {'train/accuracy': 0.6229882836341858, 'train/loss': 1.5936886072158813, 'validation/accuracy': 0.5823000073432922, 'validation/loss': 1.7864725589752197, 'validation/num_examples': 50000, 'test/accuracy': 0.46320003271102905, 'test/loss': 2.4657235145568848, 'test/num_examples': 10000, 'score': 56341.82621026039, 'total_duration': 62387.038128614426, 'accumulated_submission_time': 56341.82621026039, 'accumulated_eval_time': 6033.838894367218, 'accumulated_logging_time': 5.008333683013916, 'global_step': 123114, 'preemption_count': 0}), (124031, {'train/accuracy': 0.6308007836341858, 'train/loss': 1.54508376121521, 'validation/accuracy': 0.5869199633598328, 'validation/loss': 1.7575558423995972, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.4124157428741455, 'test/num_examples': 10000, 'score': 56761.87673950195, 'total_duration': 62855.65078186989, 'accumulated_submission_time': 56761.87673950195, 'accumulated_eval_time': 6082.30672454834, 'accumulated_logging_time': 5.055681467056274, 'global_step': 124031, 'preemption_count': 0}), (124950, {'train/accuracy': 0.6486718654632568, 'train/loss': 1.460990309715271, 'validation/accuracy': 0.5928199887275696, 'validation/loss': 1.7329621315002441, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.372661590576172, 'test/num_examples': 10000, 'score': 57181.95409989357, 'total_duration': 63317.253702163696, 'accumulated_submission_time': 57181.95409989357, 'accumulated_eval_time': 6123.739371538162, 'accumulated_logging_time': 5.101463317871094, 'global_step': 124950, 'preemption_count': 0}), (125869, {'train/accuracy': 0.636523425579071, 'train/loss': 1.5250691175460815, 'validation/accuracy': 0.5936599969863892, 'validation/loss': 1.7224498987197876, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.383357286453247, 'test/num_examples': 10000, 'score': 57601.99688029289, 'total_duration': 63783.74777674675, 'accumulated_submission_time': 57601.99688029289, 'accumulated_eval_time': 6170.100564241409, 'accumulated_logging_time': 5.144175052642822, 'global_step': 125869, 'preemption_count': 0}), (126790, {'train/accuracy': 0.6359765529632568, 'train/loss': 1.5459275245666504, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.755147933959961, 'validation/num_examples': 50000, 'test/accuracy': 0.4742000102996826, 'test/loss': 2.4011406898498535, 'test/num_examples': 10000, 'score': 58022.25514602661, 'total_duration': 64248.139909505844, 'accumulated_submission_time': 58022.25514602661, 'accumulated_eval_time': 6214.136778354645, 'accumulated_logging_time': 5.194725275039673, 'global_step': 126790, 'preemption_count': 0}), (127710, {'train/accuracy': 0.6518163681030273, 'train/loss': 1.4520666599273682, 'validation/accuracy': 0.600059986114502, 'validation/loss': 1.690841794013977, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.344245195388794, 'test/num_examples': 10000, 'score': 58442.36844062805, 'total_duration': 64715.771369457245, 'accumulated_submission_time': 58442.36844062805, 'accumulated_eval_time': 6261.555802345276, 'accumulated_logging_time': 5.246778249740601, 'global_step': 127710, 'preemption_count': 0}), (128630, {'train/accuracy': 0.6659374833106995, 'train/loss': 1.415810465812683, 'validation/accuracy': 0.5946199893951416, 'validation/loss': 1.7279531955718994, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.3771519660949707, 'test/num_examples': 10000, 'score': 58862.76211476326, 'total_duration': 65181.24786877632, 'accumulated_submission_time': 58862.76211476326, 'accumulated_eval_time': 6306.545022726059, 'accumulated_logging_time': 5.292807579040527, 'global_step': 128630, 'preemption_count': 0}), (129550, {'train/accuracy': 0.6496288776397705, 'train/loss': 1.4651296138763428, 'validation/accuracy': 0.6078599691390991, 'validation/loss': 1.6644974946975708, 'validation/num_examples': 50000, 'test/accuracy': 0.4895000159740448, 'test/loss': 2.340092182159424, 'test/num_examples': 10000, 'score': 59283.08157444, 'total_duration': 65645.18449640274, 'accumulated_submission_time': 59283.08157444, 'accumulated_eval_time': 6350.070232391357, 'accumulated_logging_time': 5.337663650512695, 'global_step': 129550, 'preemption_count': 0}), (130468, {'train/accuracy': 0.6561523079872131, 'train/loss': 1.4382413625717163, 'validation/accuracy': 0.6042400002479553, 'validation/loss': 1.6798323392868042, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.341444730758667, 'test/num_examples': 10000, 'score': 59703.20712137222, 'total_duration': 66111.03629493713, 'accumulated_submission_time': 59703.20712137222, 'accumulated_eval_time': 6395.686897754669, 'accumulated_logging_time': 5.386809349060059, 'global_step': 130468, 'preemption_count': 0}), (131389, {'train/accuracy': 0.6699999570846558, 'train/loss': 1.370593547821045, 'validation/accuracy': 0.6080399751663208, 'validation/loss': 1.6591289043426514, 'validation/num_examples': 50000, 'test/accuracy': 0.48350003361701965, 'test/loss': 2.3345508575439453, 'test/num_examples': 10000, 'score': 60123.49499297142, 'total_duration': 66577.5312511921, 'accumulated_submission_time': 60123.49499297142, 'accumulated_eval_time': 6441.799973964691, 'accumulated_logging_time': 5.43379020690918, 'global_step': 131389, 'preemption_count': 0}), (132307, {'train/accuracy': 0.6594530940055847, 'train/loss': 1.4391076564788818, 'validation/accuracy': 0.6137199997901917, 'validation/loss': 1.645622968673706, 'validation/num_examples': 50000, 'test/accuracy': 0.49220001697540283, 'test/loss': 2.302058219909668, 'test/num_examples': 10000, 'score': 60543.795784950256, 'total_duration': 67043.1083316803, 'accumulated_submission_time': 60543.795784950256, 'accumulated_eval_time': 6486.980547428131, 'accumulated_logging_time': 5.481791257858276, 'global_step': 132307, 'preemption_count': 0}), (133224, {'train/accuracy': 0.6636718511581421, 'train/loss': 1.4158997535705566, 'validation/accuracy': 0.6115800142288208, 'validation/loss': 1.6641314029693604, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.308748245239258, 'test/num_examples': 10000, 'score': 60963.91841840744, 'total_duration': 67511.08133149147, 'accumulated_submission_time': 60963.91841840744, 'accumulated_eval_time': 6534.7361924648285, 'accumulated_logging_time': 5.529500961303711, 'global_step': 133224, 'preemption_count': 0}), (134142, {'train/accuracy': 0.6744531393051147, 'train/loss': 1.3474888801574707, 'validation/accuracy': 0.6168599724769592, 'validation/loss': 1.62572181224823, 'validation/num_examples': 50000, 'test/accuracy': 0.4912000298500061, 'test/loss': 2.289562702178955, 'test/num_examples': 10000, 'score': 61384.026109695435, 'total_duration': 67980.39304852486, 'accumulated_submission_time': 61384.026109695435, 'accumulated_eval_time': 6583.84783744812, 'accumulated_logging_time': 5.574039936065674, 'global_step': 134142, 'preemption_count': 0}), (135061, {'train/accuracy': 0.6664648056030273, 'train/loss': 1.4257535934448242, 'validation/accuracy': 0.6180999875068665, 'validation/loss': 1.6305259466171265, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.297254800796509, 'test/num_examples': 10000, 'score': 61804.2365424633, 'total_duration': 68448.73585152626, 'accumulated_submission_time': 61804.2365424633, 'accumulated_eval_time': 6631.8838946819305, 'accumulated_logging_time': 5.623882532119751, 'global_step': 135061, 'preemption_count': 0}), (135975, {'train/accuracy': 0.6676562428474426, 'train/loss': 1.3916078805923462, 'validation/accuracy': 0.6185399889945984, 'validation/loss': 1.6243489980697632, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.278455972671509, 'test/num_examples': 10000, 'score': 62224.20766711235, 'total_duration': 68914.41670441628, 'accumulated_submission_time': 62224.20766711235, 'accumulated_eval_time': 6677.498216867447, 'accumulated_logging_time': 5.672069072723389, 'global_step': 135975, 'preemption_count': 0}), (136889, {'train/accuracy': 0.6796679496765137, 'train/loss': 1.3299959897994995, 'validation/accuracy': 0.6280199885368347, 'validation/loss': 1.5712664127349854, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.23734450340271, 'test/num_examples': 10000, 'score': 62644.19136214256, 'total_duration': 69382.04277920723, 'accumulated_submission_time': 62644.19136214256, 'accumulated_eval_time': 6725.043194055557, 'accumulated_logging_time': 5.722181797027588, 'global_step': 136889, 'preemption_count': 0}), (137808, {'train/accuracy': 0.6735937595367432, 'train/loss': 1.3464230298995972, 'validation/accuracy': 0.6296399831771851, 'validation/loss': 1.558292031288147, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2221529483795166, 'test/num_examples': 10000, 'score': 63064.48580121994, 'total_duration': 69850.06992220879, 'accumulated_submission_time': 63064.48580121994, 'accumulated_eval_time': 6772.678442955017, 'accumulated_logging_time': 5.771515846252441, 'global_step': 137808, 'preemption_count': 0}), (138727, {'train/accuracy': 0.6751171946525574, 'train/loss': 1.3565669059753418, 'validation/accuracy': 0.628600001335144, 'validation/loss': 1.5680512189865112, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.221627950668335, 'test/num_examples': 10000, 'score': 63484.58924078941, 'total_duration': 70318.35100626945, 'accumulated_submission_time': 63484.58924078941, 'accumulated_eval_time': 6820.7501039505005, 'accumulated_logging_time': 5.830377578735352, 'global_step': 138727, 'preemption_count': 0}), (139644, {'train/accuracy': 0.6910156011581421, 'train/loss': 1.2635481357574463, 'validation/accuracy': 0.6337400078773499, 'validation/loss': 1.5286023616790771, 'validation/num_examples': 50000, 'test/accuracy': 0.5097000002861023, 'test/loss': 2.1982059478759766, 'test/num_examples': 10000, 'score': 63904.95752739906, 'total_duration': 70785.26147270203, 'accumulated_submission_time': 63904.95752739906, 'accumulated_eval_time': 6867.18776845932, 'accumulated_logging_time': 5.888091802597046, 'global_step': 139644, 'preemption_count': 0}), (140560, {'train/accuracy': 0.7136914134025574, 'train/loss': 1.185178518295288, 'validation/accuracy': 0.6374599933624268, 'validation/loss': 1.5168884992599487, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.171229839324951, 'test/num_examples': 10000, 'score': 64325.088150024414, 'total_duration': 71251.98325324059, 'accumulated_submission_time': 64325.088150024414, 'accumulated_eval_time': 6913.686674833298, 'accumulated_logging_time': 5.933101177215576, 'global_step': 140560, 'preemption_count': 0}), (141479, {'train/accuracy': 0.6903125047683716, 'train/loss': 1.27427077293396, 'validation/accuracy': 0.6414600014686584, 'validation/loss': 1.5009756088256836, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.166109800338745, 'test/num_examples': 10000, 'score': 64745.282984018326, 'total_duration': 71718.62048172951, 'accumulated_submission_time': 64745.282984018326, 'accumulated_eval_time': 6960.035108566284, 'accumulated_logging_time': 5.978979587554932, 'global_step': 141479, 'preemption_count': 0}), (142395, {'train/accuracy': 0.6958788633346558, 'train/loss': 1.2445529699325562, 'validation/accuracy': 0.6466599702835083, 'validation/loss': 1.4790695905685425, 'validation/num_examples': 50000, 'test/accuracy': 0.5193000435829163, 'test/loss': 2.146132469177246, 'test/num_examples': 10000, 'score': 65165.53315329552, 'total_duration': 72183.05815005302, 'accumulated_submission_time': 65165.53315329552, 'accumulated_eval_time': 7004.11513710022, 'accumulated_logging_time': 6.038940191268921, 'global_step': 142395, 'preemption_count': 0}), (143312, {'train/accuracy': 0.7118749618530273, 'train/loss': 1.1865516901016235, 'validation/accuracy': 0.6448000073432922, 'validation/loss': 1.483980417251587, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1349503993988037, 'test/num_examples': 10000, 'score': 65585.63843154907, 'total_duration': 72650.63191390038, 'accumulated_submission_time': 65585.63843154907, 'accumulated_eval_time': 7051.486275434494, 'accumulated_logging_time': 6.0886406898498535, 'global_step': 143312, 'preemption_count': 0}), (144228, {'train/accuracy': 0.6990429759025574, 'train/loss': 1.2320014238357544, 'validation/accuracy': 0.6526199579238892, 'validation/loss': 1.4438382387161255, 'validation/num_examples': 50000, 'test/accuracy': 0.5264000296592712, 'test/loss': 2.108189821243286, 'test/num_examples': 10000, 'score': 66005.99985575676, 'total_duration': 73116.48621463776, 'accumulated_submission_time': 66005.99985575676, 'accumulated_eval_time': 7096.882295846939, 'accumulated_logging_time': 6.138426780700684, 'global_step': 144228, 'preemption_count': 0}), (145147, {'train/accuracy': 0.7080078125, 'train/loss': 1.2053229808807373, 'validation/accuracy': 0.6554399728775024, 'validation/loss': 1.4488352537155151, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.1001083850860596, 'test/num_examples': 10000, 'score': 66425.925065279, 'total_duration': 73583.85144233704, 'accumulated_submission_time': 66425.925065279, 'accumulated_eval_time': 7144.226463794708, 'accumulated_logging_time': 6.186929225921631, 'global_step': 145147, 'preemption_count': 0}), (146066, {'train/accuracy': 0.7203124761581421, 'train/loss': 1.154877781867981, 'validation/accuracy': 0.6532799601554871, 'validation/loss': 1.4429513216018677, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.0999159812927246, 'test/num_examples': 10000, 'score': 66846.09749627113, 'total_duration': 74052.53604912758, 'accumulated_submission_time': 66846.09749627113, 'accumulated_eval_time': 7192.638860940933, 'accumulated_logging_time': 6.239060163497925, 'global_step': 146066, 'preemption_count': 0}), (146984, {'train/accuracy': 0.708789050579071, 'train/loss': 1.1932083368301392, 'validation/accuracy': 0.6591599583625793, 'validation/loss': 1.4202784299850464, 'validation/num_examples': 50000, 'test/accuracy': 0.5374000072479248, 'test/loss': 2.074611186981201, 'test/num_examples': 10000, 'score': 67266.39178800583, 'total_duration': 74520.21671199799, 'accumulated_submission_time': 67266.39178800583, 'accumulated_eval_time': 7239.931909561157, 'accumulated_logging_time': 6.285334825515747, 'global_step': 146984, 'preemption_count': 0}), (147901, {'train/accuracy': 0.7182226181030273, 'train/loss': 1.1583360433578491, 'validation/accuracy': 0.6647399663925171, 'validation/loss': 1.4021943807601929, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.045015811920166, 'test/num_examples': 10000, 'score': 67686.7302069664, 'total_duration': 74987.58259272575, 'accumulated_submission_time': 67686.7302069664, 'accumulated_eval_time': 7286.852725744247, 'accumulated_logging_time': 6.344372987747192, 'global_step': 147901, 'preemption_count': 0}), (148818, {'train/accuracy': 0.7275585532188416, 'train/loss': 1.1043344736099243, 'validation/accuracy': 0.6648199558258057, 'validation/loss': 1.3845757246017456, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.0354931354522705, 'test/num_examples': 10000, 'score': 68107.07111549377, 'total_duration': 75456.06113243103, 'accumulated_submission_time': 68107.07111549377, 'accumulated_eval_time': 7334.890476465225, 'accumulated_logging_time': 6.396288871765137, 'global_step': 148818, 'preemption_count': 0}), (149733, {'train/accuracy': 0.7219140529632568, 'train/loss': 1.1348050832748413, 'validation/accuracy': 0.6680399775505066, 'validation/loss': 1.3798613548278809, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 2.020998001098633, 'test/num_examples': 10000, 'score': 68527.06919646263, 'total_duration': 75920.15477275848, 'accumulated_submission_time': 68527.06919646263, 'accumulated_eval_time': 7378.8897659778595, 'accumulated_logging_time': 6.444957733154297, 'global_step': 149733, 'preemption_count': 0}), (150651, {'train/accuracy': 0.7286132574081421, 'train/loss': 1.118777871131897, 'validation/accuracy': 0.6726199984550476, 'validation/loss': 1.3689404726028442, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.0204763412475586, 'test/num_examples': 10000, 'score': 68947.41572165489, 'total_duration': 76390.40303850174, 'accumulated_submission_time': 68947.41572165489, 'accumulated_eval_time': 7428.695145845413, 'accumulated_logging_time': 6.494152307510376, 'global_step': 150651, 'preemption_count': 0}), (151569, {'train/accuracy': 0.7353515625, 'train/loss': 1.0825318098068237, 'validation/accuracy': 0.6772199869155884, 'validation/loss': 1.343247890472412, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 1.9848430156707764, 'test/num_examples': 10000, 'score': 69367.38053894043, 'total_duration': 76857.38411259651, 'accumulated_submission_time': 69367.38053894043, 'accumulated_eval_time': 7475.611652612686, 'accumulated_logging_time': 6.545987844467163, 'global_step': 151569, 'preemption_count': 0}), (152486, {'train/accuracy': 0.7512304782867432, 'train/loss': 1.0053696632385254, 'validation/accuracy': 0.6732400059700012, 'validation/loss': 1.3476532697677612, 'validation/num_examples': 50000, 'test/accuracy': 0.5507000088691711, 'test/loss': 1.9955780506134033, 'test/num_examples': 10000, 'score': 69787.35775566101, 'total_duration': 77324.2224123478, 'accumulated_submission_time': 69787.35775566101, 'accumulated_eval_time': 7522.364356279373, 'accumulated_logging_time': 6.606665372848511, 'global_step': 152486, 'preemption_count': 0}), (153405, {'train/accuracy': 0.7347655892372131, 'train/loss': 1.0813597440719604, 'validation/accuracy': 0.6788199543952942, 'validation/loss': 1.333372950553894, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 1.9810631275177002, 'test/num_examples': 10000, 'score': 70207.6869328022, 'total_duration': 77792.1483130455, 'accumulated_submission_time': 70207.6869328022, 'accumulated_eval_time': 7569.859260797501, 'accumulated_logging_time': 6.660884857177734, 'global_step': 153405, 'preemption_count': 0}), (154324, {'train/accuracy': 0.7437695264816284, 'train/loss': 1.0343624353408813, 'validation/accuracy': 0.6834799647331238, 'validation/loss': 1.3101791143417358, 'validation/num_examples': 50000, 'test/accuracy': 0.5578000545501709, 'test/loss': 1.9628030061721802, 'test/num_examples': 10000, 'score': 70627.66973996162, 'total_duration': 78259.50230240822, 'accumulated_submission_time': 70627.66973996162, 'accumulated_eval_time': 7617.13103890419, 'accumulated_logging_time': 6.713019609451294, 'global_step': 154324, 'preemption_count': 0}), (155243, {'train/accuracy': 0.7542773485183716, 'train/loss': 1.0230635404586792, 'validation/accuracy': 0.6821399927139282, 'validation/loss': 1.3291398286819458, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 1.9734231233596802, 'test/num_examples': 10000, 'score': 71047.77196288109, 'total_duration': 78727.53687787056, 'accumulated_submission_time': 71047.77196288109, 'accumulated_eval_time': 7664.961159944534, 'accumulated_logging_time': 6.767343282699585, 'global_step': 155243, 'preemption_count': 0}), (156162, {'train/accuracy': 0.7413867115974426, 'train/loss': 1.0416297912597656, 'validation/accuracy': 0.6865999698638916, 'validation/loss': 1.2964109182357788, 'validation/num_examples': 50000, 'test/accuracy': 0.5647000074386597, 'test/loss': 1.9309802055358887, 'test/num_examples': 10000, 'score': 71467.86416912079, 'total_duration': 79192.2151761055, 'accumulated_submission_time': 71467.86416912079, 'accumulated_eval_time': 7709.447716712952, 'accumulated_logging_time': 6.819774866104126, 'global_step': 156162, 'preemption_count': 0}), (157080, {'train/accuracy': 0.7487695217132568, 'train/loss': 1.0085744857788086, 'validation/accuracy': 0.6894800066947937, 'validation/loss': 1.2726929187774658, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.9182018041610718, 'test/num_examples': 10000, 'score': 71887.97418117523, 'total_duration': 79659.96469688416, 'accumulated_submission_time': 71887.97418117523, 'accumulated_eval_time': 7756.980162143707, 'accumulated_logging_time': 6.879199504852295, 'global_step': 157080, 'preemption_count': 0}), (157995, {'train/accuracy': 0.7559570074081421, 'train/loss': 0.9801447987556458, 'validation/accuracy': 0.6914199590682983, 'validation/loss': 1.2745487689971924, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 1.9162464141845703, 'test/num_examples': 10000, 'score': 72307.93619275093, 'total_duration': 80124.37371206284, 'accumulated_submission_time': 72307.93619275093, 'accumulated_eval_time': 7801.3266661167145, 'accumulated_logging_time': 6.932545900344849, 'global_step': 157995, 'preemption_count': 0}), (158915, {'train/accuracy': 0.7525194883346558, 'train/loss': 1.00764000415802, 'validation/accuracy': 0.6936399936676025, 'validation/loss': 1.2615045309066772, 'validation/num_examples': 50000, 'test/accuracy': 0.569100022315979, 'test/loss': 1.889735460281372, 'test/num_examples': 10000, 'score': 72728.1712462902, 'total_duration': 80591.012966156, 'accumulated_submission_time': 72728.1712462902, 'accumulated_eval_time': 7847.6235818862915, 'accumulated_logging_time': 6.992994785308838, 'global_step': 158915, 'preemption_count': 0}), (159832, {'train/accuracy': 0.7587695121765137, 'train/loss': 0.973080575466156, 'validation/accuracy': 0.6987999677658081, 'validation/loss': 1.2378002405166626, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 1.876285195350647, 'test/num_examples': 10000, 'score': 73148.25042939186, 'total_duration': 81059.77536559105, 'accumulated_submission_time': 73148.25042939186, 'accumulated_eval_time': 7896.201631784439, 'accumulated_logging_time': 7.052062034606934, 'global_step': 159832, 'preemption_count': 0}), (160750, {'train/accuracy': 0.7638476490974426, 'train/loss': 0.9550341367721558, 'validation/accuracy': 0.7001399993896484, 'validation/loss': 1.2401357889175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.8870264291763306, 'test/num_examples': 10000, 'score': 73568.35084533691, 'total_duration': 81525.65334272385, 'accumulated_submission_time': 73568.35084533691, 'accumulated_eval_time': 7941.881090164185, 'accumulated_logging_time': 7.1026670932769775, 'global_step': 160750, 'preemption_count': 0}), (161669, {'train/accuracy': 0.7666015625, 'train/loss': 0.9483379125595093, 'validation/accuracy': 0.7057600021362305, 'validation/loss': 1.2242356538772583, 'validation/num_examples': 50000, 'test/accuracy': 0.5799000263214111, 'test/loss': 1.8605889081954956, 'test/num_examples': 10000, 'score': 73988.73741221428, 'total_duration': 81990.81713628769, 'accumulated_submission_time': 73988.73741221428, 'accumulated_eval_time': 7986.558121442795, 'accumulated_logging_time': 7.155567407608032, 'global_step': 161669, 'preemption_count': 0}), (162587, {'train/accuracy': 0.7708789110183716, 'train/loss': 0.9288029074668884, 'validation/accuracy': 0.7047399878501892, 'validation/loss': 1.2123870849609375, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.8519184589385986, 'test/num_examples': 10000, 'score': 74408.8857395649, 'total_duration': 82460.56594634056, 'accumulated_submission_time': 74408.8857395649, 'accumulated_eval_time': 8036.057965755463, 'accumulated_logging_time': 7.209298849105835, 'global_step': 162587, 'preemption_count': 0}), (163508, {'train/accuracy': 0.7753124833106995, 'train/loss': 0.9027910232543945, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1837961673736572, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.826006293296814, 'test/num_examples': 10000, 'score': 74829.21450185776, 'total_duration': 82926.06588101387, 'accumulated_submission_time': 74829.21450185776, 'accumulated_eval_time': 8081.126464605331, 'accumulated_logging_time': 7.264871120452881, 'global_step': 163508, 'preemption_count': 0}), (164424, {'train/accuracy': 0.7840234041213989, 'train/loss': 0.861545979976654, 'validation/accuracy': 0.7120199799537659, 'validation/loss': 1.1774080991744995, 'validation/num_examples': 50000, 'test/accuracy': 0.5870000123977661, 'test/loss': 1.8180652856826782, 'test/num_examples': 10000, 'score': 75249.25192141533, 'total_duration': 83393.11365270615, 'accumulated_submission_time': 75249.25192141533, 'accumulated_eval_time': 8128.030569314957, 'accumulated_logging_time': 7.324113607406616, 'global_step': 164424, 'preemption_count': 0}), (165344, {'train/accuracy': 0.78076171875, 'train/loss': 0.8917228579521179, 'validation/accuracy': 0.7150999903678894, 'validation/loss': 1.1734659671783447, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.806179404258728, 'test/num_examples': 10000, 'score': 75669.19865012169, 'total_duration': 83860.63411188126, 'accumulated_submission_time': 75669.19865012169, 'accumulated_eval_time': 8175.50325012207, 'accumulated_logging_time': 7.37749457359314, 'global_step': 165344, 'preemption_count': 0}), (166265, {'train/accuracy': 0.7815039157867432, 'train/loss': 0.8719916939735413, 'validation/accuracy': 0.7160800099372864, 'validation/loss': 1.1593176126480103, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.798190951347351, 'test/num_examples': 10000, 'score': 76089.25742650032, 'total_duration': 84325.38896870613, 'accumulated_submission_time': 76089.25742650032, 'accumulated_eval_time': 8220.088045358658, 'accumulated_logging_time': 7.440832138061523, 'global_step': 166265, 'preemption_count': 0}), (167181, {'train/accuracy': 0.789843738079071, 'train/loss': 0.8536946177482605, 'validation/accuracy': 0.7184199690818787, 'validation/loss': 1.1556397676467896, 'validation/num_examples': 50000, 'test/accuracy': 0.5931000113487244, 'test/loss': 1.7854821681976318, 'test/num_examples': 10000, 'score': 76509.34638428688, 'total_duration': 84792.34748697281, 'accumulated_submission_time': 76509.34638428688, 'accumulated_eval_time': 8266.85658288002, 'accumulated_logging_time': 7.495449066162109, 'global_step': 167181, 'preemption_count': 0}), (168099, {'train/accuracy': 0.7865429520606995, 'train/loss': 0.8519309759140015, 'validation/accuracy': 0.7182199954986572, 'validation/loss': 1.1389554738998413, 'validation/num_examples': 50000, 'test/accuracy': 0.5954000353813171, 'test/loss': 1.7669183015823364, 'test/num_examples': 10000, 'score': 76929.57842612267, 'total_duration': 85260.30331659317, 'accumulated_submission_time': 76929.57842612267, 'accumulated_eval_time': 8314.475160121918, 'accumulated_logging_time': 7.552535772323608, 'global_step': 168099, 'preemption_count': 0}), (169019, {'train/accuracy': 0.7899804711341858, 'train/loss': 0.8419513702392578, 'validation/accuracy': 0.7230599522590637, 'validation/loss': 1.1312774419784546, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.7649180889129639, 'test/num_examples': 10000, 'score': 77349.57205319405, 'total_duration': 85725.11896824837, 'accumulated_submission_time': 77349.57205319405, 'accumulated_eval_time': 8359.18827176094, 'accumulated_logging_time': 7.6130571365356445, 'global_step': 169019, 'preemption_count': 0})], 'global_step': 169399}
I0203 11:38:51.380215 140184451094336 submission_runner.py:586] Timing: 77520.27059221268
I0203 11:38:51.380303 140184451094336 submission_runner.py:588] Total number of evals: 185
I0203 11:38:51.380351 140184451094336 submission_runner.py:589] ====================
I0203 11:38:51.380400 140184451094336 submission_runner.py:542] Using RNG seed 3390244169
I0203 11:38:51.381956 140184451094336 submission_runner.py:551] --- Tuning run 5/5 ---
I0203 11:38:51.382063 140184451094336 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5.
I0203 11:38:51.384991 140184451094336 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5/hparams.json.
I0203 11:38:51.385853 140184451094336 submission_runner.py:206] Initializing dataset.
I0203 11:38:51.395519 140184451094336 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 11:38:51.406250 140184451094336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 11:38:51.597289 140184451094336 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 11:38:55.905697 140184451094336 submission_runner.py:213] Initializing model.
I0203 11:39:02.558489 140184451094336 submission_runner.py:255] Initializing optimizer.
I0203 11:39:03.071818 140184451094336 submission_runner.py:262] Initializing metrics bundle.
I0203 11:39:03.071981 140184451094336 submission_runner.py:280] Initializing checkpoint and logger.
I0203 11:39:03.087195 140184451094336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5 with prefix checkpoint_
I0203 11:39:03.087319 140184451094336 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0203 11:39:19.525117 140184451094336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0203 11:39:35.719280 140184451094336 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5/flags_0.json.
I0203 11:39:35.724567 140184451094336 submission_runner.py:314] Starting training loop.
I0203 11:40:16.187690 140022493714176 logging_writer.py:48] [0] global_step=0, grad_norm=0.3486858606338501, loss=6.9077558517456055
I0203 11:40:16.205412 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:40:24.568392 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:40:43.125145 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:40:44.724882 140184451094336 submission_runner.py:408] Time since start: 69.00s, 	Step: 1, 	{'train/accuracy': 0.0007812499534338713, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 40.48073649406433, 'total_duration': 69.00026822090149, 'accumulated_submission_time': 40.48073649406433, 'accumulated_eval_time': 28.51941752433777, 'accumulated_logging_time': 0}
I0203 11:40:44.733858 140022502106880 logging_writer.py:48] [1] accumulated_eval_time=28.519418, accumulated_logging_time=0, accumulated_submission_time=40.480736, global_step=1, preemption_count=0, score=40.480736, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=69.000268, train/accuracy=0.000781, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0203 11:41:55.729750 140023005427456 logging_writer.py:48] [100] global_step=100, grad_norm=0.5142108798027039, loss=6.878161430358887
I0203 11:42:41.255476 140022518892288 logging_writer.py:48] [200] global_step=200, grad_norm=0.5444875359535217, loss=6.850555419921875
I0203 11:43:28.301026 140023005427456 logging_writer.py:48] [300] global_step=300, grad_norm=0.8027284741401672, loss=6.671980381011963
I0203 11:44:14.973353 140022518892288 logging_writer.py:48] [400] global_step=400, grad_norm=1.0152220726013184, loss=6.601391315460205
I0203 11:45:03.661303 140023005427456 logging_writer.py:48] [500] global_step=500, grad_norm=0.7019421458244324, loss=6.638510704040527
I0203 11:45:50.271385 140022518892288 logging_writer.py:48] [600] global_step=600, grad_norm=1.1332300901412964, loss=6.426348686218262
I0203 11:46:36.938343 140023005427456 logging_writer.py:48] [700] global_step=700, grad_norm=1.1584498882293701, loss=6.310060024261475
I0203 11:47:23.279791 140022518892288 logging_writer.py:48] [800] global_step=800, grad_norm=1.5335047245025635, loss=6.253180027008057
I0203 11:47:44.877317 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:47:56.211021 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:48:24.545602 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:48:26.145158 140184451094336 submission_runner.py:408] Time since start: 530.42s, 	Step: 848, 	{'train/accuracy': 0.029550781473517418, 'train/loss': 5.966935157775879, 'validation/accuracy': 0.02531999908387661, 'validation/loss': 6.030439376831055, 'validation/num_examples': 50000, 'test/accuracy': 0.0215000007301569, 'test/loss': 6.148784637451172, 'test/num_examples': 10000, 'score': 460.5695321559906, 'total_duration': 530.42049741745, 'accumulated_submission_time': 460.5695321559906, 'accumulated_eval_time': 69.7873125076294, 'accumulated_logging_time': 0.019226789474487305}
I0203 11:48:26.162271 140023005427456 logging_writer.py:48] [848] accumulated_eval_time=69.787313, accumulated_logging_time=0.019227, accumulated_submission_time=460.569532, global_step=848, preemption_count=0, score=460.569532, test/accuracy=0.021500, test/loss=6.148785, test/num_examples=10000, total_duration=530.420497, train/accuracy=0.029551, train/loss=5.966935, validation/accuracy=0.025320, validation/loss=6.030439, validation/num_examples=50000
I0203 11:48:48.282769 140022518892288 logging_writer.py:48] [900] global_step=900, grad_norm=1.5010019540786743, loss=6.188492774963379
I0203 11:49:32.936128 140023005427456 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.021470069885254, loss=6.2164764404296875
I0203 11:50:19.546643 140022518892288 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.3658008575439453, loss=6.363521575927734
I0203 11:51:06.123987 140023005427456 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1695151329040527, loss=5.981964111328125
I0203 11:51:52.709340 140022518892288 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.087891936302185, loss=6.49670934677124
I0203 11:52:39.067973 140023005427456 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2808326482772827, loss=6.0027546882629395
I0203 11:53:25.417238 140022518892288 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8914127945899963, loss=6.56924295425415
I0203 11:54:11.991749 140023005427456 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.157641887664795, loss=5.666966915130615
I0203 11:54:58.198056 140022518892288 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.1082814931869507, loss=5.721007823944092
I0203 11:55:26.172853 140184451094336 spec.py:321] Evaluating on the training split.
I0203 11:55:37.056932 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 11:56:12.344368 140184451094336 spec.py:349] Evaluating on the test split.
I0203 11:56:13.940118 140184451094336 submission_runner.py:408] Time since start: 998.22s, 	Step: 1761, 	{'train/accuracy': 0.07119140774011612, 'train/loss': 5.351015567779541, 'validation/accuracy': 0.06855999678373337, 'validation/loss': 5.379694938659668, 'validation/num_examples': 50000, 'test/accuracy': 0.054100003093481064, 'test/loss': 5.57318639755249, 'test/num_examples': 10000, 'score': 880.5223081111908, 'total_duration': 998.2155048847198, 'accumulated_submission_time': 880.5223081111908, 'accumulated_eval_time': 117.55456948280334, 'accumulated_logging_time': 0.046671152114868164}
I0203 11:56:13.954550 140023005427456 logging_writer.py:48] [1761] accumulated_eval_time=117.554569, accumulated_logging_time=0.046671, accumulated_submission_time=880.522308, global_step=1761, preemption_count=0, score=880.522308, test/accuracy=0.054100, test/loss=5.573186, test/num_examples=10000, total_duration=998.215505, train/accuracy=0.071191, train/loss=5.351016, validation/accuracy=0.068560, validation/loss=5.379695, validation/num_examples=50000
I0203 11:56:30.654496 140022518892288 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1635187864303589, loss=5.766044616699219
I0203 11:57:14.546301 140023005427456 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.114993691444397, loss=5.637308120727539
I0203 11:58:00.707989 140022518892288 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0045043230056763, loss=6.221920013427734
I0203 11:58:47.128444 140023005427456 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9291443824768066, loss=5.573497772216797
I0203 11:59:33.405182 140022518892288 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.1588282585144043, loss=5.582738876342773
I0203 12:00:19.877377 140023005427456 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9705277681350708, loss=5.52020263671875
I0203 12:01:05.935686 140022518892288 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.305063247680664, loss=5.4489336013793945
I0203 12:01:52.422520 140023005427456 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9549691081047058, loss=6.3708906173706055
I0203 12:02:38.659001 140022518892288 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0871238708496094, loss=5.343404769897461
I0203 12:03:14.015094 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:03:24.718562 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:03:59.189978 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:04:00.796155 140184451094336 submission_runner.py:408] Time since start: 1465.07s, 	Step: 2678, 	{'train/accuracy': 0.12623046338558197, 'train/loss': 4.751564025878906, 'validation/accuracy': 0.11949999630451202, 'validation/loss': 4.812889099121094, 'validation/num_examples': 50000, 'test/accuracy': 0.08980000764131546, 'test/loss': 5.094581604003906, 'test/num_examples': 10000, 'score': 1300.5238733291626, 'total_duration': 1465.0715363025665, 'accumulated_submission_time': 1300.5238733291626, 'accumulated_eval_time': 164.33563232421875, 'accumulated_logging_time': 0.07172465324401855}
I0203 12:04:00.811748 140023005427456 logging_writer.py:48] [2678] accumulated_eval_time=164.335632, accumulated_logging_time=0.071725, accumulated_submission_time=1300.523873, global_step=2678, preemption_count=0, score=1300.523873, test/accuracy=0.089800, test/loss=5.094582, test/num_examples=10000, total_duration=1465.071536, train/accuracy=0.126230, train/loss=4.751564, validation/accuracy=0.119500, validation/loss=4.812889, validation/num_examples=50000
I0203 12:04:10.439566 140022518892288 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8798058032989502, loss=5.912313938140869
I0203 12:04:53.725019 140023005427456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8726126551628113, loss=5.369306564331055
I0203 12:05:39.775994 140022518892288 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.4286264181137085, loss=5.343835353851318
I0203 12:06:26.000547 140023005427456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7779800295829773, loss=6.011005401611328
I0203 12:07:12.266342 140022518892288 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1469961404800415, loss=5.137511253356934
I0203 12:07:58.320375 140023005427456 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2401258945465088, loss=6.454500198364258
I0203 12:08:44.592565 140022518892288 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.2144438028335571, loss=5.185372352600098
I0203 12:09:30.863640 140023005427456 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9785791039466858, loss=5.265130043029785
I0203 12:10:17.317818 140022518892288 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0412472486495972, loss=5.099306583404541
I0203 12:11:01.265345 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:11:12.101243 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:11:48.979130 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:11:50.581765 140184451094336 submission_runner.py:408] Time since start: 1934.86s, 	Step: 3597, 	{'train/accuracy': 0.18238280713558197, 'train/loss': 4.224600315093994, 'validation/accuracy': 0.16603998839855194, 'validation/loss': 4.346518516540527, 'validation/num_examples': 50000, 'test/accuracy': 0.1284000128507614, 'test/loss': 4.73599910736084, 'test/num_examples': 10000, 'score': 1720.9155325889587, 'total_duration': 1934.8571481704712, 'accumulated_submission_time': 1720.9155325889587, 'accumulated_eval_time': 213.65205097198486, 'accumulated_logging_time': 0.10139894485473633}
I0203 12:11:50.597642 140023005427456 logging_writer.py:48] [3597] accumulated_eval_time=213.652051, accumulated_logging_time=0.101399, accumulated_submission_time=1720.915533, global_step=3597, preemption_count=0, score=1720.915533, test/accuracy=0.128400, test/loss=4.735999, test/num_examples=10000, total_duration=1934.857148, train/accuracy=0.182383, train/loss=4.224600, validation/accuracy=0.166040, validation/loss=4.346519, validation/num_examples=50000
I0203 12:11:52.280707 140022518892288 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8642562031745911, loss=5.169642448425293
I0203 12:12:34.856507 140023005427456 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8519134521484375, loss=5.708612442016602
I0203 12:13:21.026458 140022518892288 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8471729159355164, loss=5.437470436096191
I0203 12:14:07.288432 140023005427456 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.2229453325271606, loss=4.95887565612793
I0203 12:14:53.536349 140022518892288 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7435622811317444, loss=6.016948699951172
I0203 12:15:39.539400 140023005427456 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9175936579704285, loss=5.25320291519165
I0203 12:16:25.711077 140022518892288 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9897973537445068, loss=4.773472785949707
I0203 12:17:11.894243 140023005427456 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7460939884185791, loss=4.6976447105407715
I0203 12:17:57.873999 140022518892288 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6644101142883301, loss=5.753461837768555
I0203 12:18:44.192224 140023005427456 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8694307208061218, loss=4.699179172515869
I0203 12:18:50.745008 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:19:01.367230 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:19:35.217885 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:19:36.818747 140184451094336 submission_runner.py:408] Time since start: 2401.09s, 	Step: 4516, 	{'train/accuracy': 0.2391015589237213, 'train/loss': 3.7565841674804688, 'validation/accuracy': 0.22258000075817108, 'validation/loss': 3.8583083152770996, 'validation/num_examples': 50000, 'test/accuracy': 0.1705000102519989, 'test/loss': 4.336967945098877, 'test/num_examples': 10000, 'score': 2141.0058159828186, 'total_duration': 2401.094133615494, 'accumulated_submission_time': 2141.0058159828186, 'accumulated_eval_time': 259.7257878780365, 'accumulated_logging_time': 0.1269831657409668}
I0203 12:19:36.834176 140022518892288 logging_writer.py:48] [4516] accumulated_eval_time=259.725788, accumulated_logging_time=0.126983, accumulated_submission_time=2141.005816, global_step=4516, preemption_count=0, score=2141.005816, test/accuracy=0.170500, test/loss=4.336968, test/num_examples=10000, total_duration=2401.094134, train/accuracy=0.239102, train/loss=3.756584, validation/accuracy=0.222580, validation/loss=3.858308, validation/num_examples=50000
I0203 12:20:12.532224 140023005427456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5663595795631409, loss=6.309725761413574
I0203 12:20:58.572486 140022518892288 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9047514200210571, loss=4.840024948120117
I0203 12:21:45.258040 140023005427456 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0465235710144043, loss=5.153026580810547
I0203 12:22:31.269372 140022518892288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7137991189956665, loss=6.272828102111816
I0203 12:23:17.905680 140023005427456 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.665092945098877, loss=5.864606857299805
I0203 12:24:04.097667 140022518892288 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9026344418525696, loss=4.480412006378174
I0203 12:24:50.707618 140023005427456 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7031750679016113, loss=4.906788349151611
I0203 12:25:37.090815 140022518892288 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9540070295333862, loss=4.491708278656006
I0203 12:26:23.584987 140023005427456 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7588194012641907, loss=5.355624198913574
I0203 12:26:37.065580 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:26:47.861417 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:27:22.342231 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:27:23.979828 140184451094336 submission_runner.py:408] Time since start: 2868.26s, 	Step: 5431, 	{'train/accuracy': 0.27052733302116394, 'train/loss': 3.628831386566162, 'validation/accuracy': 0.24855999648571014, 'validation/loss': 3.7476320266723633, 'validation/num_examples': 50000, 'test/accuracy': 0.1940000057220459, 'test/loss': 4.203524112701416, 'test/num_examples': 10000, 'score': 2561.1740078926086, 'total_duration': 2868.2551929950714, 'accumulated_submission_time': 2561.1740078926086, 'accumulated_eval_time': 306.6399974822998, 'accumulated_logging_time': 0.15290021896362305}
I0203 12:27:23.998524 140022518892288 logging_writer.py:48] [5431] accumulated_eval_time=306.639997, accumulated_logging_time=0.152900, accumulated_submission_time=2561.174008, global_step=5431, preemption_count=0, score=2561.174008, test/accuracy=0.194000, test/loss=4.203524, test/num_examples=10000, total_duration=2868.255193, train/accuracy=0.270527, train/loss=3.628831, validation/accuracy=0.248560, validation/loss=3.747632, validation/num_examples=50000
I0203 12:27:53.251728 140023005427456 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8807530999183655, loss=4.725475788116455
I0203 12:28:38.786248 140022518892288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6669090390205383, loss=5.336331844329834
I0203 12:29:24.924982 140023005427456 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7707909941673279, loss=4.810790061950684
I0203 12:30:11.134781 140022518892288 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7656800746917725, loss=4.335514545440674
I0203 12:30:57.302668 140023005427456 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6883767247200012, loss=5.050717830657959
I0203 12:31:43.678042 140022518892288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8626163005828857, loss=4.2004289627075195
I0203 12:32:30.038313 140023005427456 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6808557510375977, loss=6.136274814605713
I0203 12:33:16.070667 140022518892288 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6686094999313354, loss=5.4633355140686035
I0203 12:34:02.733310 140023005427456 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5819511413574219, loss=5.854628562927246
I0203 12:34:24.162075 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:34:35.003468 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:35:08.986578 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:35:10.585357 140184451094336 submission_runner.py:408] Time since start: 3334.86s, 	Step: 6348, 	{'train/accuracy': 0.3121093809604645, 'train/loss': 3.2937893867492676, 'validation/accuracy': 0.2845599949359894, 'validation/loss': 3.4468889236450195, 'validation/num_examples': 50000, 'test/accuracy': 0.2184000164270401, 'test/loss': 3.9825544357299805, 'test/num_examples': 10000, 'score': 2981.2767839431763, 'total_duration': 3334.8607263565063, 'accumulated_submission_time': 2981.2767839431763, 'accumulated_eval_time': 353.06326150894165, 'accumulated_logging_time': 0.18476366996765137}
I0203 12:35:10.601223 140022518892288 logging_writer.py:48] [6348] accumulated_eval_time=353.063262, accumulated_logging_time=0.184764, accumulated_submission_time=2981.276784, global_step=6348, preemption_count=0, score=2981.276784, test/accuracy=0.218400, test/loss=3.982554, test/num_examples=10000, total_duration=3334.860726, train/accuracy=0.312109, train/loss=3.293789, validation/accuracy=0.284560, validation/loss=3.446889, validation/num_examples=50000
I0203 12:35:32.759662 140023005427456 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7239190936088562, loss=4.1762895584106445
I0203 12:36:17.684118 140022518892288 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7570593357086182, loss=6.027089595794678
I0203 12:37:04.250037 140023005427456 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8793684840202332, loss=4.212399005889893
I0203 12:37:50.640117 140022518892288 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7570570111274719, loss=4.084972858428955
I0203 12:38:37.186821 140023005427456 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6194343566894531, loss=6.068855285644531
I0203 12:39:23.636351 140022518892288 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8977214097976685, loss=4.351362228393555
I0203 12:40:09.970505 140023005427456 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.898183286190033, loss=4.314730167388916
I0203 12:40:56.335052 140022518892288 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9236326217651367, loss=4.110878944396973
I0203 12:41:43.072805 140023005427456 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8836186528205872, loss=5.660463809967041
I0203 12:42:10.668457 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:42:21.429465 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:42:57.472716 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:42:59.066672 140184451094336 submission_runner.py:408] Time since start: 3803.34s, 	Step: 7261, 	{'train/accuracy': 0.3370117247104645, 'train/loss': 3.1292214393615723, 'validation/accuracy': 0.3139199912548065, 'validation/loss': 3.2608413696289062, 'validation/num_examples': 50000, 'test/accuracy': 0.24220001697540283, 'test/loss': 3.815594434738159, 'test/num_examples': 10000, 'score': 3401.2823746204376, 'total_duration': 3803.3420593738556, 'accumulated_submission_time': 3401.2823746204376, 'accumulated_eval_time': 401.46146631240845, 'accumulated_logging_time': 0.21497297286987305}
I0203 12:42:59.082334 140022518892288 logging_writer.py:48] [7261] accumulated_eval_time=401.461466, accumulated_logging_time=0.214973, accumulated_submission_time=3401.282375, global_step=7261, preemption_count=0, score=3401.282375, test/accuracy=0.242200, test/loss=3.815594, test/num_examples=10000, total_duration=3803.342059, train/accuracy=0.337012, train/loss=3.129221, validation/accuracy=0.313920, validation/loss=3.260841, validation/num_examples=50000
I0203 12:43:15.805994 140023005427456 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9101249575614929, loss=4.074015140533447
I0203 12:44:00.085246 140022518892288 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.708443284034729, loss=6.0424628257751465
I0203 12:44:46.609418 140023005427456 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7189760208129883, loss=4.033257961273193
I0203 12:45:33.156687 140022518892288 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8328487277030945, loss=3.988800525665283
I0203 12:46:19.633247 140023005427456 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9095364212989807, loss=4.159860610961914
I0203 12:47:06.240875 140022518892288 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7844652533531189, loss=4.032930850982666
I0203 12:47:52.708319 140023005427456 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.558199405670166, loss=5.385499000549316
I0203 12:48:39.244769 140022518892288 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.4842158555984497, loss=4.133234977722168
I0203 12:49:25.672506 140023005427456 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.842492938041687, loss=4.286568641662598
I0203 12:49:59.132333 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:50:10.111932 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:50:43.074606 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:50:44.675668 140184451094336 submission_runner.py:408] Time since start: 4268.95s, 	Step: 8174, 	{'train/accuracy': 0.35986328125, 'train/loss': 3.0289080142974854, 'validation/accuracy': 0.3312399983406067, 'validation/loss': 3.1762967109680176, 'validation/num_examples': 50000, 'test/accuracy': 0.25690001249313354, 'test/loss': 3.737917423248291, 'test/num_examples': 10000, 'score': 3821.274812936783, 'total_duration': 4268.951048851013, 'accumulated_submission_time': 3821.274812936783, 'accumulated_eval_time': 447.0047791004181, 'accumulated_logging_time': 0.23990464210510254}
I0203 12:50:44.691607 140022518892288 logging_writer.py:48] [8174] accumulated_eval_time=447.004779, accumulated_logging_time=0.239905, accumulated_submission_time=3821.274813, global_step=8174, preemption_count=0, score=3821.274813, test/accuracy=0.256900, test/loss=3.737917, test/num_examples=10000, total_duration=4268.951049, train/accuracy=0.359863, train/loss=3.028908, validation/accuracy=0.331240, validation/loss=3.176297, validation/num_examples=50000
I0203 12:50:55.988421 140023005427456 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7950267791748047, loss=4.335018634796143
I0203 12:51:39.799457 140022518892288 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7859442234039307, loss=3.90565824508667
I0203 12:52:26.129841 140023005427456 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7887584567070007, loss=5.668187141418457
I0203 12:53:12.590584 140022518892288 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7591422200202942, loss=4.361467361450195
I0203 12:53:58.690345 140023005427456 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8206923604011536, loss=3.869323492050171
I0203 12:54:45.152596 140022518892288 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6426704525947571, loss=5.43206787109375
I0203 12:55:31.513105 140023005427456 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8347452282905579, loss=3.788367986679077
I0203 12:56:18.058989 140022518892288 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.616698145866394, loss=5.40407657623291
I0203 12:57:04.322860 140023005427456 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7082701921463013, loss=6.018120765686035
I0203 12:57:44.848447 140184451094336 spec.py:321] Evaluating on the training split.
I0203 12:57:55.722968 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 12:58:31.359058 140184451094336 spec.py:349] Evaluating on the test split.
I0203 12:58:32.980622 140184451094336 submission_runner.py:408] Time since start: 4737.26s, 	Step: 9089, 	{'train/accuracy': 0.3876953125, 'train/loss': 2.8449742794036865, 'validation/accuracy': 0.3534199893474579, 'validation/loss': 3.0242254734039307, 'validation/num_examples': 50000, 'test/accuracy': 0.2750000059604645, 'test/loss': 3.5979087352752686, 'test/num_examples': 10000, 'score': 4241.370996952057, 'total_duration': 4737.255994081497, 'accumulated_submission_time': 4241.370996952057, 'accumulated_eval_time': 495.13693618774414, 'accumulated_logging_time': 0.269237756729126}
I0203 12:58:32.999584 140022518892288 logging_writer.py:48] [9089] accumulated_eval_time=495.136936, accumulated_logging_time=0.269238, accumulated_submission_time=4241.370997, global_step=9089, preemption_count=0, score=4241.370997, test/accuracy=0.275000, test/loss=3.597909, test/num_examples=10000, total_duration=4737.255994, train/accuracy=0.387695, train/loss=2.844974, validation/accuracy=0.353420, validation/loss=3.024225, validation/num_examples=50000
I0203 12:58:38.017420 140023005427456 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.565862238407135, loss=6.007802486419678
I0203 12:59:21.377374 140022518892288 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8591018915176392, loss=3.7757151126861572
I0203 13:00:07.593875 140023005427456 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6208008527755737, loss=5.011926174163818
I0203 13:00:53.949799 140022518892288 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8132384419441223, loss=4.19210958480835
I0203 13:01:40.311079 140023005427456 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7811437845230103, loss=4.519105434417725
I0203 13:02:26.893351 140022518892288 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.868999719619751, loss=3.82309627532959
I0203 13:03:13.128983 140023005427456 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6444828510284424, loss=4.9124674797058105
I0203 13:03:59.249992 140022518892288 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7924631834030151, loss=3.742307186126709
I0203 13:04:45.891530 140023005427456 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7797815799713135, loss=3.9956367015838623
I0203 13:05:32.524988 140022518892288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7591843008995056, loss=5.318031311035156
I0203 13:05:33.135319 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:05:43.832868 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:06:20.559527 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:06:22.184314 140184451094336 submission_runner.py:408] Time since start: 5206.46s, 	Step: 10003, 	{'train/accuracy': 0.40519529581069946, 'train/loss': 2.718560218811035, 'validation/accuracy': 0.3734799921512604, 'validation/loss': 2.8857717514038086, 'validation/num_examples': 50000, 'test/accuracy': 0.28700000047683716, 'test/loss': 3.4828357696533203, 'test/num_examples': 10000, 'score': 4661.446325063705, 'total_duration': 5206.459691762924, 'accumulated_submission_time': 4661.446325063705, 'accumulated_eval_time': 544.185914516449, 'accumulated_logging_time': 0.30090880393981934}
I0203 13:06:22.201357 140023005427456 logging_writer.py:48] [10003] accumulated_eval_time=544.185915, accumulated_logging_time=0.300909, accumulated_submission_time=4661.446325, global_step=10003, preemption_count=0, score=4661.446325, test/accuracy=0.287000, test/loss=3.482836, test/num_examples=10000, total_duration=5206.459692, train/accuracy=0.405195, train/loss=2.718560, validation/accuracy=0.373480, validation/loss=2.885772, validation/num_examples=50000
I0203 13:07:03.937872 140022518892288 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9276741743087769, loss=3.7244036197662354
I0203 13:07:49.859168 140023005427456 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8314372897148132, loss=4.002188682556152
I0203 13:08:36.495608 140022518892288 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9835067391395569, loss=3.712566375732422
I0203 13:09:22.813772 140023005427456 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.1427092552185059, loss=3.576374053955078
I0203 13:10:09.190641 140022518892288 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0313336849212646, loss=3.6230344772338867
I0203 13:10:55.492363 140023005427456 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5816145539283752, loss=5.381346702575684
I0203 13:11:41.746695 140022518892288 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.793664276599884, loss=5.552172660827637
I0203 13:12:28.199836 140023005427456 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8824090361595154, loss=3.6377453804016113
I0203 13:13:14.657027 140022518892288 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6973950862884521, loss=4.108604907989502
I0203 13:13:22.559223 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:13:33.407502 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:14:08.641016 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:14:10.252775 140184451094336 submission_runner.py:408] Time since start: 5674.53s, 	Step: 10919, 	{'train/accuracy': 0.42525389790534973, 'train/loss': 2.589329719543457, 'validation/accuracy': 0.39239999651908875, 'validation/loss': 2.7610809803009033, 'validation/num_examples': 50000, 'test/accuracy': 0.3006000220775604, 'test/loss': 3.390204429626465, 'test/num_examples': 10000, 'score': 5081.747814178467, 'total_duration': 5674.528142929077, 'accumulated_submission_time': 5081.747814178467, 'accumulated_eval_time': 591.8794357776642, 'accumulated_logging_time': 0.32680320739746094}
I0203 13:14:10.273746 140023005427456 logging_writer.py:48] [10919] accumulated_eval_time=591.879436, accumulated_logging_time=0.326803, accumulated_submission_time=5081.747814, global_step=10919, preemption_count=0, score=5081.747814, test/accuracy=0.300600, test/loss=3.390204, test/num_examples=10000, total_duration=5674.528143, train/accuracy=0.425254, train/loss=2.589330, validation/accuracy=0.392400, validation/loss=2.761081, validation/num_examples=50000
I0203 13:14:45.016289 140022518892288 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9177301526069641, loss=3.891065835952759
I0203 13:15:30.938411 140023005427456 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8483420014381409, loss=3.7974331378936768
I0203 13:16:17.594066 140022518892288 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7937599420547485, loss=5.168766498565674
I0203 13:17:04.225245 140023005427456 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.8302853107452393, loss=3.5231847763061523
I0203 13:17:50.473897 140022518892288 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7724977135658264, loss=4.658187389373779
I0203 13:18:36.882030 140023005427456 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8166686296463013, loss=3.8188161849975586
I0203 13:19:23.405819 140022518892288 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8587902784347534, loss=3.8125205039978027
I0203 13:20:09.880455 140023005427456 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8473049998283386, loss=3.5978856086730957
I0203 13:20:56.391831 140022518892288 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8217619061470032, loss=3.6085524559020996
I0203 13:21:10.442023 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:21:21.314721 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:21:57.349576 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:21:58.949437 140184451094336 submission_runner.py:408] Time since start: 6143.22s, 	Step: 11832, 	{'train/accuracy': 0.4370703101158142, 'train/loss': 2.549715757369995, 'validation/accuracy': 0.398499995470047, 'validation/loss': 2.7432808876037598, 'validation/num_examples': 50000, 'test/accuracy': 0.30980002880096436, 'test/loss': 3.3450920581817627, 'test/num_examples': 10000, 'score': 5501.858088970184, 'total_duration': 6143.224822998047, 'accumulated_submission_time': 5501.858088970184, 'accumulated_eval_time': 640.3868417739868, 'accumulated_logging_time': 0.3585836887359619}
I0203 13:21:58.966388 140023005427456 logging_writer.py:48] [11832] accumulated_eval_time=640.386842, accumulated_logging_time=0.358584, accumulated_submission_time=5501.858089, global_step=11832, preemption_count=0, score=5501.858089, test/accuracy=0.309800, test/loss=3.345092, test/num_examples=10000, total_duration=6143.224823, train/accuracy=0.437070, train/loss=2.549716, validation/accuracy=0.398500, validation/loss=2.743281, validation/num_examples=50000
I0203 13:22:27.803249 140022518892288 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9641459584236145, loss=3.5388712882995605
I0203 13:23:13.685008 140023005427456 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9366421699523926, loss=4.194087028503418
I0203 13:23:59.970554 140022518892288 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0068678855895996, loss=3.5514163970947266
I0203 13:24:46.227193 140023005427456 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9855959415435791, loss=3.8230605125427246
I0203 13:25:32.234120 140022518892288 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7630220055580139, loss=5.81490421295166
I0203 13:26:18.481360 140023005427456 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6943848729133606, loss=4.891114711761475
I0203 13:27:05.117686 140022518892288 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6599424481391907, loss=5.506092548370361
I0203 13:27:52.051553 140023005427456 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9460755586624146, loss=3.4575071334838867
I0203 13:28:38.669770 140022518892288 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7964988946914673, loss=4.950387954711914
I0203 13:28:59.147198 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:29:10.181972 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:29:46.114823 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:29:47.724166 140184451094336 submission_runner.py:408] Time since start: 6612.00s, 	Step: 12746, 	{'train/accuracy': 0.48222655057907104, 'train/loss': 2.3162496089935303, 'validation/accuracy': 0.4213799834251404, 'validation/loss': 2.6229944229125977, 'validation/num_examples': 50000, 'test/accuracy': 0.3286000192165375, 'test/loss': 3.2327558994293213, 'test/num_examples': 10000, 'score': 5921.982423782349, 'total_duration': 6611.999529123306, 'accumulated_submission_time': 5921.982423782349, 'accumulated_eval_time': 688.9637801647186, 'accumulated_logging_time': 0.3849976062774658}
I0203 13:29:47.743716 140023005427456 logging_writer.py:48] [12746] accumulated_eval_time=688.963780, accumulated_logging_time=0.384998, accumulated_submission_time=5921.982424, global_step=12746, preemption_count=0, score=5921.982424, test/accuracy=0.328600, test/loss=3.232756, test/num_examples=10000, total_duration=6611.999529, train/accuracy=0.482227, train/loss=2.316250, validation/accuracy=0.421380, validation/loss=2.622994, validation/num_examples=50000
I0203 13:30:10.714040 140022518892288 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8485199213027954, loss=4.0327887535095215
I0203 13:30:55.864901 140023005427456 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7739713191986084, loss=5.556835174560547
I0203 13:31:42.510466 140022518892288 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7088441252708435, loss=4.8916120529174805
I0203 13:32:28.858638 140023005427456 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0208196640014648, loss=3.5489025115966797
I0203 13:33:14.896858 140022518892288 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6662991046905518, loss=5.839771270751953
I0203 13:34:01.007257 140023005427456 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0775668621063232, loss=3.556629180908203
I0203 13:34:47.528884 140022518892288 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8878315091133118, loss=3.4826877117156982
I0203 13:35:33.682796 140023005427456 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.1141443252563477, loss=3.416571617126465
I0203 13:36:20.222229 140022518892288 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9938366413116455, loss=3.4561195373535156
I0203 13:36:47.994777 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:36:58.911745 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:37:35.294513 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:37:36.888033 140184451094336 submission_runner.py:408] Time since start: 7081.16s, 	Step: 13662, 	{'train/accuracy': 0.4601757824420929, 'train/loss': 2.428586721420288, 'validation/accuracy': 0.4309999942779541, 'validation/loss': 2.577209711074829, 'validation/num_examples': 50000, 'test/accuracy': 0.33230000734329224, 'test/loss': 3.2250514030456543, 'test/num_examples': 10000, 'score': 6342.175592184067, 'total_duration': 7081.163420915604, 'accumulated_submission_time': 6342.175592184067, 'accumulated_eval_time': 737.8570251464844, 'accumulated_logging_time': 0.41483449935913086}
I0203 13:37:36.908116 140023005427456 logging_writer.py:48] [13662] accumulated_eval_time=737.857025, accumulated_logging_time=0.414834, accumulated_submission_time=6342.175592, global_step=13662, preemption_count=0, score=6342.175592, test/accuracy=0.332300, test/loss=3.225051, test/num_examples=10000, total_duration=7081.163421, train/accuracy=0.460176, train/loss=2.428587, validation/accuracy=0.431000, validation/loss=2.577210, validation/num_examples=50000
I0203 13:37:53.205189 140022518892288 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.08957839012146, loss=3.413713216781616
I0203 13:38:37.745289 140023005427456 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6982699632644653, loss=5.3439531326293945
I0203 13:39:24.013272 140022518892288 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0454093217849731, loss=3.4418792724609375
I0203 13:40:10.382400 140023005427456 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6251384019851685, loss=5.751432418823242
I0203 13:40:56.410993 140022518892288 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0069639682769775, loss=3.431119203567505
I0203 13:41:42.902714 140023005427456 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.084168553352356, loss=3.415245771408081
I0203 13:42:29.444476 140022518892288 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.9977787137031555, loss=3.418191909790039
I0203 13:43:15.628495 140023005427456 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0038939714431763, loss=3.520936965942383
I0203 13:44:01.878459 140022518892288 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9012773036956787, loss=3.384084463119507
I0203 13:44:37.219015 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:44:48.065619 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:45:26.965145 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:45:28.572175 140184451094336 submission_runner.py:408] Time since start: 7552.85s, 	Step: 14578, 	{'train/accuracy': 0.46898436546325684, 'train/loss': 2.3901267051696777, 'validation/accuracy': 0.43695998191833496, 'validation/loss': 2.566824197769165, 'validation/num_examples': 50000, 'test/accuracy': 0.3360000252723694, 'test/loss': 3.2194483280181885, 'test/num_examples': 10000, 'score': 6762.429019451141, 'total_duration': 7552.847557067871, 'accumulated_submission_time': 6762.429019451141, 'accumulated_eval_time': 789.2101700305939, 'accumulated_logging_time': 0.4447028636932373}
I0203 13:45:28.589787 140023005427456 logging_writer.py:48] [14578] accumulated_eval_time=789.210170, accumulated_logging_time=0.444703, accumulated_submission_time=6762.429019, global_step=14578, preemption_count=0, score=6762.429019, test/accuracy=0.336000, test/loss=3.219448, test/num_examples=10000, total_duration=7552.847557, train/accuracy=0.468984, train/loss=2.390127, validation/accuracy=0.436960, validation/loss=2.566824, validation/num_examples=50000
I0203 13:45:38.209853 140022518892288 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9345570802688599, loss=3.7912588119506836
I0203 13:46:22.004363 140023005427456 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7283259034156799, loss=5.3531999588012695
I0203 13:47:08.567603 140022518892288 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8058234453201294, loss=5.272281169891357
I0203 13:47:55.203206 140023005427456 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7995180487632751, loss=4.470217227935791
I0203 13:48:41.478527 140022518892288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.729026734828949, loss=4.686773300170898
I0203 13:49:27.971993 140023005427456 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0950326919555664, loss=3.379594087600708
I0203 13:50:14.320324 140022518892288 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7430221438407898, loss=5.7188191413879395
I0203 13:51:00.623930 140023005427456 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9584475755691528, loss=3.3944921493530273
I0203 13:51:47.130198 140022518892288 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9083345532417297, loss=3.7147183418273926
I0203 13:52:28.853564 140184451094336 spec.py:321] Evaluating on the training split.
I0203 13:52:39.833401 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 13:53:17.081646 140184451094336 spec.py:349] Evaluating on the test split.
I0203 13:53:18.682713 140184451094336 submission_runner.py:408] Time since start: 8022.96s, 	Step: 15492, 	{'train/accuracy': 0.509960949420929, 'train/loss': 2.1471524238586426, 'validation/accuracy': 0.45419999957084656, 'validation/loss': 2.4339747428894043, 'validation/num_examples': 50000, 'test/accuracy': 0.35110002756118774, 'test/loss': 3.085043430328369, 'test/num_examples': 10000, 'score': 7182.634547472, 'total_duration': 8022.958084821701, 'accumulated_submission_time': 7182.634547472, 'accumulated_eval_time': 839.039304971695, 'accumulated_logging_time': 0.47350120544433594}
I0203 13:53:18.709514 140023005427456 logging_writer.py:48] [15492] accumulated_eval_time=839.039305, accumulated_logging_time=0.473501, accumulated_submission_time=7182.634547, global_step=15492, preemption_count=0, score=7182.634547, test/accuracy=0.351100, test/loss=3.085043, test/num_examples=10000, total_duration=8022.958085, train/accuracy=0.509961, train/loss=2.147152, validation/accuracy=0.454200, validation/loss=2.433975, validation/num_examples=50000
I0203 13:53:22.477896 140022518892288 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9502330422401428, loss=3.472424268722534
I0203 13:54:05.799543 140023005427456 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7320500612258911, loss=4.689000606536865
I0203 13:54:51.755230 140022518892288 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.046457290649414, loss=3.2146215438842773
I0203 13:55:38.231941 140023005427456 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9415948390960693, loss=3.2735202312469482
I0203 13:56:24.337552 140022518892288 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0920215845108032, loss=3.4198107719421387
I0203 13:57:10.561304 140023005427456 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.1117669343948364, loss=3.3599863052368164
I0203 13:57:56.828935 140022518892288 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7087807655334473, loss=5.321350574493408
I0203 13:58:43.131261 140023005427456 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.664173424243927, loss=5.580276966094971
I0203 13:59:29.919278 140022518892288 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9142444729804993, loss=3.953570604324341
I0203 14:00:16.031041 140023005427456 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7661323547363281, loss=4.928045272827148
I0203 14:00:18.951622 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:00:30.314088 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:01:06.804190 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:01:08.414471 140184451094336 submission_runner.py:408] Time since start: 8492.69s, 	Step: 16408, 	{'train/accuracy': 0.48423826694488525, 'train/loss': 2.291804552078247, 'validation/accuracy': 0.45715999603271484, 'validation/loss': 2.4441986083984375, 'validation/num_examples': 50000, 'test/accuracy': 0.3532000184059143, 'test/loss': 3.1026744842529297, 'test/num_examples': 10000, 'score': 7602.818835020065, 'total_duration': 8492.689853906631, 'accumulated_submission_time': 7602.818835020065, 'accumulated_eval_time': 888.5021407604218, 'accumulated_logging_time': 0.5111334323883057}
I0203 14:01:08.431222 140022518892288 logging_writer.py:48] [16408] accumulated_eval_time=888.502141, accumulated_logging_time=0.511133, accumulated_submission_time=7602.818835, global_step=16408, preemption_count=0, score=7602.818835, test/accuracy=0.353200, test/loss=3.102674, test/num_examples=10000, total_duration=8492.689854, train/accuracy=0.484238, train/loss=2.291805, validation/accuracy=0.457160, validation/loss=2.444199, validation/num_examples=50000
I0203 14:01:48.136533 140023005427456 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0767252445220947, loss=3.114994764328003
I0203 14:02:34.430883 140022518892288 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1488958597183228, loss=3.2414355278015137
I0203 14:03:20.826345 140023005427456 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.103101372718811, loss=3.363152265548706
I0203 14:04:07.093586 140022518892288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7161156535148621, loss=5.209926605224609
I0203 14:04:53.343426 140023005427456 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.03230619430542, loss=3.163649320602417
I0203 14:05:39.580559 140022518892288 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1059796810150146, loss=3.280276298522949
I0203 14:06:25.896701 140023005427456 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.963980495929718, loss=3.408999443054199
I0203 14:07:12.236321 140022518892288 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.209532618522644, loss=3.4920005798339844
I0203 14:07:58.548212 140023005427456 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.8574002385139465, loss=3.939624309539795
I0203 14:08:08.469008 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:08:19.347743 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:08:55.920205 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:08:57.526602 140184451094336 submission_runner.py:408] Time since start: 8961.80s, 	Step: 17323, 	{'train/accuracy': 0.4905664026737213, 'train/loss': 2.2800023555755615, 'validation/accuracy': 0.4546799957752228, 'validation/loss': 2.4581525325775146, 'validation/num_examples': 50000, 'test/accuracy': 0.3483000099658966, 'test/loss': 3.1094908714294434, 'test/num_examples': 10000, 'score': 8022.798815488815, 'total_duration': 8961.801989793777, 'accumulated_submission_time': 8022.798815488815, 'accumulated_eval_time': 937.5597274303436, 'accumulated_logging_time': 0.5380949974060059}
I0203 14:08:57.543751 140022518892288 logging_writer.py:48] [17323] accumulated_eval_time=937.559727, accumulated_logging_time=0.538095, accumulated_submission_time=8022.798815, global_step=17323, preemption_count=0, score=8022.798815, test/accuracy=0.348300, test/loss=3.109491, test/num_examples=10000, total_duration=8961.801990, train/accuracy=0.490566, train/loss=2.280002, validation/accuracy=0.454680, validation/loss=2.458153, validation/num_examples=50000
I0203 14:09:30.253870 140023005427456 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8017102479934692, loss=4.721028804779053
I0203 14:10:16.513859 140022518892288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8079944849014282, loss=5.5857062339782715
I0203 14:11:02.802089 140023005427456 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0040366649627686, loss=3.383713483810425
I0203 14:11:49.204562 140022518892288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9903205037117004, loss=3.0978479385375977
I0203 14:12:35.404567 140023005427456 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.0189391374588013, loss=3.444808006286621
I0203 14:13:21.585183 140022518892288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.779858410358429, loss=4.471797466278076
I0203 14:14:08.030081 140023005427456 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7949243783950806, loss=4.061333179473877
I0203 14:14:54.238415 140022518892288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.8342490196228027, loss=4.788015842437744
I0203 14:15:40.554055 140023005427456 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9034597873687744, loss=3.9619522094726562
I0203 14:15:57.864416 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:16:08.819522 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:16:44.774336 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:16:46.394444 140184451094336 submission_runner.py:408] Time since start: 9430.67s, 	Step: 18239, 	{'train/accuracy': 0.520800769329071, 'train/loss': 2.135014533996582, 'validation/accuracy': 0.46879997849464417, 'validation/loss': 2.398402452468872, 'validation/num_examples': 50000, 'test/accuracy': 0.3587000072002411, 'test/loss': 3.0502817630767822, 'test/num_examples': 10000, 'score': 8443.062378168106, 'total_duration': 9430.669814825058, 'accumulated_submission_time': 8443.062378168106, 'accumulated_eval_time': 986.0897233486176, 'accumulated_logging_time': 0.5650601387023926}
I0203 14:16:46.414406 140022518892288 logging_writer.py:48] [18239] accumulated_eval_time=986.089723, accumulated_logging_time=0.565060, accumulated_submission_time=8443.062378, global_step=18239, preemption_count=0, score=8443.062378, test/accuracy=0.358700, test/loss=3.050282, test/num_examples=10000, total_duration=9430.669815, train/accuracy=0.520801, train/loss=2.135015, validation/accuracy=0.468800, validation/loss=2.398402, validation/num_examples=50000
I0203 14:17:12.327111 140023005427456 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0172843933105469, loss=3.3837697505950928
I0203 14:17:57.736041 140022518892288 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8703429698944092, loss=5.7530903816223145
I0203 14:18:43.915770 140023005427456 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9470130205154419, loss=3.549886465072632
I0203 14:19:30.277137 140022518892288 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9103862047195435, loss=3.4856083393096924
I0203 14:20:16.580084 140023005427456 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.01050865650177, loss=3.202350378036499
I0203 14:21:02.782733 140022518892288 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0452958345413208, loss=3.3707263469696045
I0203 14:21:49.139104 140023005427456 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9586161375045776, loss=3.4144105911254883
I0203 14:22:35.544006 140022518892288 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.0925254821777344, loss=3.1005609035491943
I0203 14:23:21.860076 140023005427456 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9152598977088928, loss=3.4505858421325684
I0203 14:23:46.600210 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:23:57.475331 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:24:36.750842 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:24:38.374880 140184451094336 submission_runner.py:408] Time since start: 9902.65s, 	Step: 19155, 	{'train/accuracy': 0.5138086080551147, 'train/loss': 2.095219135284424, 'validation/accuracy': 0.4810999929904938, 'validation/loss': 2.2757034301757812, 'validation/num_examples': 50000, 'test/accuracy': 0.3728000223636627, 'test/loss': 2.9553446769714355, 'test/num_examples': 10000, 'score': 8863.191219329834, 'total_duration': 9902.65026807785, 'accumulated_submission_time': 8863.191219329834, 'accumulated_eval_time': 1037.8643803596497, 'accumulated_logging_time': 0.5954127311706543}
I0203 14:24:38.392215 140022518892288 logging_writer.py:48] [19155] accumulated_eval_time=1037.864380, accumulated_logging_time=0.595413, accumulated_submission_time=8863.191219, global_step=19155, preemption_count=0, score=8863.191219, test/accuracy=0.372800, test/loss=2.955345, test/num_examples=10000, total_duration=9902.650268, train/accuracy=0.513809, train/loss=2.095219, validation/accuracy=0.481100, validation/loss=2.275703, validation/num_examples=50000
I0203 14:24:57.623231 140023005427456 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9053913950920105, loss=5.377089977264404
I0203 14:25:42.483164 140022518892288 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0999486446380615, loss=3.2436330318450928
I0203 14:26:28.750545 140023005427456 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.2662843465805054, loss=2.959585189819336
I0203 14:27:14.917844 140022518892288 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.2048611640930176, loss=3.251682996749878
I0203 14:28:00.975269 140023005427456 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7343118190765381, loss=5.209621906280518
I0203 14:28:47.269766 140022518892288 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.1372264623641968, loss=3.0905117988586426
I0203 14:29:33.663801 140023005427456 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1082794666290283, loss=3.3335013389587402
I0203 14:30:19.944476 140022518892288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9562896490097046, loss=5.216878890991211
I0203 14:31:06.467220 140023005427456 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9036642909049988, loss=5.526471138000488
I0203 14:31:38.583918 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:31:49.297091 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:32:24.937705 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:32:26.546982 140184451094336 submission_runner.py:408] Time since start: 10370.82s, 	Step: 20070, 	{'train/accuracy': 0.5203906297683716, 'train/loss': 2.1330671310424805, 'validation/accuracy': 0.4761599898338318, 'validation/loss': 2.3456084728240967, 'validation/num_examples': 50000, 'test/accuracy': 0.3712000250816345, 'test/loss': 2.999906301498413, 'test/num_examples': 10000, 'score': 9283.326484203339, 'total_duration': 10370.82237124443, 'accumulated_submission_time': 9283.326484203339, 'accumulated_eval_time': 1085.8274364471436, 'accumulated_logging_time': 0.6221892833709717}
I0203 14:32:26.566347 140022518892288 logging_writer.py:48] [20070] accumulated_eval_time=1085.827436, accumulated_logging_time=0.622189, accumulated_submission_time=9283.326484, global_step=20070, preemption_count=0, score=9283.326484, test/accuracy=0.371200, test/loss=2.999906, test/num_examples=10000, total_duration=10370.822371, train/accuracy=0.520391, train/loss=2.133067, validation/accuracy=0.476160, validation/loss=2.345608, validation/num_examples=50000
I0203 14:32:39.521576 140023005427456 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.128228783607483, loss=3.273545265197754
I0203 14:33:23.412539 140022518892288 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8721919059753418, loss=4.447612762451172
I0203 14:34:09.675805 140023005427456 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8984450697898865, loss=3.747786045074463
I0203 14:34:56.150551 140022518892288 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0281660556793213, loss=3.18456768989563
I0203 14:35:42.380646 140023005427456 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9358302354812622, loss=3.823030471801758
I0203 14:36:28.727414 140022518892288 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7333653569221497, loss=4.902586460113525
I0203 14:37:15.007409 140023005427456 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.193115234375, loss=3.1042356491088867
I0203 14:38:01.302681 140022518892288 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.1024770736694336, loss=3.259007453918457
I0203 14:38:47.462276 140023005427456 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7982279062271118, loss=4.547725677490234
I0203 14:39:26.825356 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:39:37.635407 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:40:15.044294 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:40:16.640943 140184451094336 submission_runner.py:408] Time since start: 10840.92s, 	Step: 20987, 	{'train/accuracy': 0.5329296588897705, 'train/loss': 2.0133917331695557, 'validation/accuracy': 0.48829999566078186, 'validation/loss': 2.249237060546875, 'validation/num_examples': 50000, 'test/accuracy': 0.3790000081062317, 'test/loss': 2.913668394088745, 'test/num_examples': 10000, 'score': 9703.52631521225, 'total_duration': 10840.916332960129, 'accumulated_submission_time': 9703.52631521225, 'accumulated_eval_time': 1135.6430156230927, 'accumulated_logging_time': 0.6531627178192139}
I0203 14:40:16.659652 140022518892288 logging_writer.py:48] [20987] accumulated_eval_time=1135.643016, accumulated_logging_time=0.653163, accumulated_submission_time=9703.526315, global_step=20987, preemption_count=0, score=9703.526315, test/accuracy=0.379000, test/loss=2.913668, test/num_examples=10000, total_duration=10840.916333, train/accuracy=0.532930, train/loss=2.013392, validation/accuracy=0.488300, validation/loss=2.249237, validation/num_examples=50000
I0203 14:40:22.518465 140023005427456 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9786388874053955, loss=3.112226963043213
I0203 14:41:06.159752 140022518892288 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.1457829475402832, loss=2.928893566131592
I0203 14:41:52.613391 140023005427456 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.151201605796814, loss=2.9853055477142334
I0203 14:42:39.132131 140022518892288 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.9661982655525208, loss=3.0371785163879395
I0203 14:43:25.340881 140023005427456 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8008112907409668, loss=5.124088287353516
I0203 14:44:11.520067 140022518892288 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.9639875888824463, loss=3.088216543197632
I0203 14:44:57.761111 140023005427456 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7575154900550842, loss=5.02409029006958
I0203 14:45:44.167722 140022518892288 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.053765892982483, loss=3.205440044403076
I0203 14:46:30.377909 140023005427456 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9283965826034546, loss=3.35835599899292
I0203 14:47:16.693835 140022518892288 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.196156620979309, loss=3.2659027576446533
I0203 14:47:16.708862 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:47:27.546877 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:48:01.054326 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:48:02.665299 140184451094336 submission_runner.py:408] Time since start: 11306.94s, 	Step: 21901, 	{'train/accuracy': 0.5308398604393005, 'train/loss': 2.069345474243164, 'validation/accuracy': 0.49559998512268066, 'validation/loss': 2.2566778659820557, 'validation/num_examples': 50000, 'test/accuracy': 0.3831000328063965, 'test/loss': 2.9034087657928467, 'test/num_examples': 10000, 'score': 10123.515281438828, 'total_duration': 11306.940642356873, 'accumulated_submission_time': 10123.515281438828, 'accumulated_eval_time': 1181.5994005203247, 'accumulated_logging_time': 0.6843507289886475}
I0203 14:48:02.685693 140023005427456 logging_writer.py:48] [21901] accumulated_eval_time=1181.599401, accumulated_logging_time=0.684351, accumulated_submission_time=10123.515281, global_step=21901, preemption_count=0, score=10123.515281, test/accuracy=0.383100, test/loss=2.903409, test/num_examples=10000, total_duration=11306.940642, train/accuracy=0.530840, train/loss=2.069345, validation/accuracy=0.495600, validation/loss=2.256678, validation/num_examples=50000
I0203 14:48:45.423287 140022518892288 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0458728075027466, loss=3.4022154808044434
I0203 14:49:31.446622 140023005427456 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.15027916431427, loss=3.1526551246643066
I0203 14:50:17.926409 140022518892288 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.0693684816360474, loss=3.1747703552246094
I0203 14:51:04.456172 140023005427456 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9885081648826599, loss=3.533267021179199
I0203 14:51:50.811138 140022518892288 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.010176658630371, loss=3.0312254428863525
I0203 14:52:37.131818 140023005427456 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.0214639902114868, loss=2.934311628341675
I0203 14:53:23.663290 140022518892288 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.8034259080886841, loss=4.3057074546813965
I0203 14:54:09.747931 140023005427456 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.0184427499771118, loss=3.048208475112915
I0203 14:54:56.042153 140022518892288 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8543460965156555, loss=5.585075855255127
I0203 14:55:02.778764 140184451094336 spec.py:321] Evaluating on the training split.
I0203 14:55:13.551695 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 14:55:49.116079 140184451094336 spec.py:349] Evaluating on the test split.
I0203 14:55:50.742097 140184451094336 submission_runner.py:408] Time since start: 11775.02s, 	Step: 22816, 	{'train/accuracy': 0.5394726395606995, 'train/loss': 2.0032269954681396, 'validation/accuracy': 0.5013399720191956, 'validation/loss': 2.2060070037841797, 'validation/num_examples': 50000, 'test/accuracy': 0.3904000222682953, 'test/loss': 2.8718252182006836, 'test/num_examples': 10000, 'score': 10543.550062179565, 'total_duration': 11775.017482995987, 'accumulated_submission_time': 10543.550062179565, 'accumulated_eval_time': 1229.5627224445343, 'accumulated_logging_time': 0.7155659198760986}
I0203 14:55:50.762758 140023005427456 logging_writer.py:48] [22816] accumulated_eval_time=1229.562722, accumulated_logging_time=0.715566, accumulated_submission_time=10543.550062, global_step=22816, preemption_count=0, score=10543.550062, test/accuracy=0.390400, test/loss=2.871825, test/num_examples=10000, total_duration=11775.017483, train/accuracy=0.539473, train/loss=2.003227, validation/accuracy=0.501340, validation/loss=2.206007, validation/num_examples=50000
I0203 14:56:26.684325 140022518892288 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8278544545173645, loss=5.070528030395508
I0203 14:57:12.947898 140023005427456 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7840491533279419, loss=4.49867582321167
I0203 14:57:59.173130 140022518892288 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0565844774246216, loss=3.5401506423950195
I0203 14:58:45.511164 140023005427456 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.9289140701293945, loss=3.8229241371154785
I0203 14:59:31.624574 140022518892288 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.09990656375885, loss=3.0198326110839844
I0203 15:00:18.111424 140023005427456 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.7848013043403625, loss=5.341743469238281
I0203 15:01:04.276027 140022518892288 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.1354727745056152, loss=3.349424123764038
I0203 15:01:50.758691 140023005427456 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.022950291633606, loss=2.8976387977600098
I0203 15:02:36.873576 140022518892288 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.1241079568862915, loss=3.1912426948547363
I0203 15:02:50.860121 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:03:01.348453 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:03:38.134285 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:03:39.748944 140184451094336 submission_runner.py:408] Time since start: 12244.02s, 	Step: 23732, 	{'train/accuracy': 0.551074206829071, 'train/loss': 1.9518203735351562, 'validation/accuracy': 0.5073400139808655, 'validation/loss': 2.178410768508911, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.8280670642852783, 'test/num_examples': 10000, 'score': 10963.586684465408, 'total_duration': 12244.024310827255, 'accumulated_submission_time': 10963.586684465408, 'accumulated_eval_time': 1278.4515182971954, 'accumulated_logging_time': 0.7490296363830566}
I0203 15:03:39.770409 140023005427456 logging_writer.py:48] [23732] accumulated_eval_time=1278.451518, accumulated_logging_time=0.749030, accumulated_submission_time=10963.586684, global_step=23732, preemption_count=0, score=10963.586684, test/accuracy=0.398900, test/loss=2.828067, test/num_examples=10000, total_duration=12244.024311, train/accuracy=0.551074, train/loss=1.951820, validation/accuracy=0.507340, validation/loss=2.178411, validation/num_examples=50000
I0203 15:04:08.591600 140022518892288 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.9450960159301758, loss=3.3221778869628906
I0203 15:04:54.266371 140023005427456 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8689277172088623, loss=4.011855125427246
I0203 15:05:40.751890 140022518892288 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1053694486618042, loss=3.344021797180176
I0203 15:06:27.052743 140023005427456 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0928221940994263, loss=3.0413060188293457
I0203 15:07:13.323528 140022518892288 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.021360158920288, loss=2.9735827445983887
I0203 15:07:59.625455 140023005427456 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9615597724914551, loss=3.8843727111816406
I0203 15:08:45.713088 140022518892288 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8296294212341309, loss=4.275031089782715
I0203 15:09:32.167494 140023005427456 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.9493635892868042, loss=3.5786566734313965
I0203 15:10:18.252417 140022518892288 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0907396078109741, loss=3.7121503353118896
I0203 15:10:39.978975 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:10:50.738599 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:11:27.280270 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:11:28.874300 140184451094336 submission_runner.py:408] Time since start: 12713.15s, 	Step: 24649, 	{'train/accuracy': 0.5817968845367432, 'train/loss': 1.7977644205093384, 'validation/accuracy': 0.5110399723052979, 'validation/loss': 2.1437582969665527, 'validation/num_examples': 50000, 'test/accuracy': 0.39900001883506775, 'test/loss': 2.818315029144287, 'test/num_examples': 10000, 'score': 11383.736985206604, 'total_duration': 12713.149666309357, 'accumulated_submission_time': 11383.736985206604, 'accumulated_eval_time': 1327.3468503952026, 'accumulated_logging_time': 0.781226396560669}
I0203 15:11:28.893327 140023005427456 logging_writer.py:48] [24649] accumulated_eval_time=1327.346850, accumulated_logging_time=0.781226, accumulated_submission_time=11383.736985, global_step=24649, preemption_count=0, score=11383.736985, test/accuracy=0.399000, test/loss=2.818315, test/num_examples=10000, total_duration=12713.149666, train/accuracy=0.581797, train/loss=1.797764, validation/accuracy=0.511040, validation/loss=2.143758, validation/num_examples=50000
I0203 15:11:50.626241 140022518892288 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0296334028244019, loss=3.0438477993011475
I0203 15:12:35.638242 140023005427456 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0639814138412476, loss=2.810138702392578
I0203 15:13:21.906132 140022518892288 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8619269132614136, loss=4.6960601806640625
I0203 15:14:08.145763 140023005427456 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.2181437015533447, loss=3.0847995281219482
I0203 15:14:54.320837 140022518892288 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7994091510772705, loss=5.039063453674316
I0203 15:15:40.460440 140023005427456 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2514514923095703, loss=3.1446263790130615
I0203 15:16:26.764343 140022518892288 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9411294460296631, loss=3.6212637424468994
I0203 15:17:13.196812 140023005427456 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.1579008102416992, loss=2.916998863220215
I0203 15:17:59.566149 140022518892288 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.142027735710144, loss=3.1743245124816895
I0203 15:18:29.053429 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:18:39.961323 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:19:15.915411 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:19:17.520830 140184451094336 submission_runner.py:408] Time since start: 13181.80s, 	Step: 25565, 	{'train/accuracy': 0.5573828220367432, 'train/loss': 1.887807011604309, 'validation/accuracy': 0.5200200080871582, 'validation/loss': 2.082399606704712, 'validation/num_examples': 50000, 'test/accuracy': 0.40450000762939453, 'test/loss': 2.7608931064605713, 'test/num_examples': 10000, 'score': 11803.839218378067, 'total_duration': 13181.796215295792, 'accumulated_submission_time': 11803.839218378067, 'accumulated_eval_time': 1375.8142375946045, 'accumulated_logging_time': 0.8103833198547363}
I0203 15:19:17.543056 140023005427456 logging_writer.py:48] [25565] accumulated_eval_time=1375.814238, accumulated_logging_time=0.810383, accumulated_submission_time=11803.839218, global_step=25565, preemption_count=0, score=11803.839218, test/accuracy=0.404500, test/loss=2.760893, test/num_examples=10000, total_duration=13181.796215, train/accuracy=0.557383, train/loss=1.887807, validation/accuracy=0.520020, validation/loss=2.082400, validation/num_examples=50000
I0203 15:19:32.584305 140022518892288 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0682929754257202, loss=3.0470166206359863
I0203 15:20:17.036323 140023005427456 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.926243007183075, loss=4.214746952056885
I0203 15:21:03.269896 140022518892288 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9383423924446106, loss=5.673295974731445
I0203 15:21:49.708456 140023005427456 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1581289768218994, loss=3.08040714263916
I0203 15:22:35.888454 140022518892288 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9112744927406311, loss=3.60689640045166
I0203 15:23:22.337958 140023005427456 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9448797106742859, loss=3.37241268157959
I0203 15:24:08.239233 140022518892288 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.3105697631835938, loss=3.115415334701538
I0203 15:24:54.604585 140023005427456 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0494506359100342, loss=2.9302539825439453
I0203 15:25:40.844537 140022518892288 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8372123837471008, loss=5.036287784576416
I0203 15:26:17.635021 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:26:28.886215 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:27:05.553604 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:27:07.158461 140184451094336 submission_runner.py:408] Time since start: 13651.43s, 	Step: 26481, 	{'train/accuracy': 0.5679296851158142, 'train/loss': 1.8611342906951904, 'validation/accuracy': 0.5214599967002869, 'validation/loss': 2.0894598960876465, 'validation/num_examples': 50000, 'test/accuracy': 0.40790000557899475, 'test/loss': 2.758775234222412, 'test/num_examples': 10000, 'score': 12223.86883687973, 'total_duration': 13651.433849334717, 'accumulated_submission_time': 12223.86883687973, 'accumulated_eval_time': 1425.337683916092, 'accumulated_logging_time': 0.8424429893493652}
I0203 15:27:07.178367 140023005427456 logging_writer.py:48] [26481] accumulated_eval_time=1425.337684, accumulated_logging_time=0.842443, accumulated_submission_time=12223.868837, global_step=26481, preemption_count=0, score=12223.868837, test/accuracy=0.407900, test/loss=2.758775, test/num_examples=10000, total_duration=13651.433849, train/accuracy=0.567930, train/loss=1.861134, validation/accuracy=0.521460, validation/loss=2.089460, validation/num_examples=50000
I0203 15:27:15.540694 140022518892288 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.8507710695266724, loss=5.445605278015137
I0203 15:27:59.100270 140023005427456 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.2152799367904663, loss=3.1332781314849854
I0203 15:28:45.200265 140022518892288 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.1981372833251953, loss=2.924006700515747
I0203 15:29:31.675551 140023005427456 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.839366614818573, loss=5.575567245483398
I0203 15:30:17.956956 140022518892288 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.102354645729065, loss=2.969393491744995
I0203 15:31:04.313723 140023005427456 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.3086401224136353, loss=2.919621467590332
I0203 15:31:50.705643 140022518892288 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.0779896974563599, loss=2.851421356201172
I0203 15:32:36.852252 140023005427456 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.0574774742126465, loss=3.581796169281006
I0203 15:33:23.487686 140022518892288 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8269640803337097, loss=4.103298187255859
I0203 15:34:07.489375 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:34:18.102284 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:34:56.162075 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:34:57.761753 140184451094336 submission_runner.py:408] Time since start: 14122.04s, 	Step: 27397, 	{'train/accuracy': 0.5859179496765137, 'train/loss': 1.7362204790115356, 'validation/accuracy': 0.5262599587440491, 'validation/loss': 2.0381267070770264, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.729538917541504, 'test/num_examples': 10000, 'score': 12644.12185382843, 'total_duration': 14122.03713798523, 'accumulated_submission_time': 12644.12185382843, 'accumulated_eval_time': 1475.6100606918335, 'accumulated_logging_time': 0.8734757900238037}
I0203 15:34:57.782359 140023005427456 logging_writer.py:48] [27397] accumulated_eval_time=1475.610061, accumulated_logging_time=0.873476, accumulated_submission_time=12644.121854, global_step=27397, preemption_count=0, score=12644.121854, test/accuracy=0.415400, test/loss=2.729539, test/num_examples=10000, total_duration=14122.037138, train/accuracy=0.585918, train/loss=1.736220, validation/accuracy=0.526260, validation/loss=2.038127, validation/num_examples=50000
I0203 15:34:59.455631 140022518892288 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0079305171966553, loss=2.793116331100464
I0203 15:35:42.491574 140023005427456 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0596774816513062, loss=3.162122964859009
I0203 15:36:28.770219 140022518892288 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8377081751823425, loss=5.145019054412842
I0203 15:37:15.422719 140023005427456 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.1140846014022827, loss=2.89766788482666
I0203 15:38:01.539313 140022518892288 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.0680187940597534, loss=2.958460807800293
I0203 15:38:47.616663 140023005427456 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.1513841152191162, loss=3.0514402389526367
I0203 15:39:33.914205 140022518892288 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.0186762809753418, loss=3.2854628562927246
I0203 15:40:20.443808 140023005427456 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.1669418811798096, loss=3.0332438945770264
I0203 15:41:06.503668 140022518892288 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0140585899353027, loss=3.1709494590759277
I0203 15:41:52.807458 140023005427456 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.057101845741272, loss=3.095867872238159
I0203 15:41:57.903259 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:42:08.542620 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:42:44.224825 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:42:45.835585 140184451094336 submission_runner.py:408] Time since start: 14590.11s, 	Step: 28313, 	{'train/accuracy': 0.5691796541213989, 'train/loss': 1.8370025157928467, 'validation/accuracy': 0.5263400077819824, 'validation/loss': 2.0492756366729736, 'validation/num_examples': 50000, 'test/accuracy': 0.4115000069141388, 'test/loss': 2.7372734546661377, 'test/num_examples': 10000, 'score': 13064.183827638626, 'total_duration': 14590.110958814621, 'accumulated_submission_time': 13064.183827638626, 'accumulated_eval_time': 1523.542355298996, 'accumulated_logging_time': 0.9049403667449951}
I0203 15:42:45.860538 140022518892288 logging_writer.py:48] [28313] accumulated_eval_time=1523.542355, accumulated_logging_time=0.904940, accumulated_submission_time=13064.183828, global_step=28313, preemption_count=0, score=13064.183828, test/accuracy=0.411500, test/loss=2.737273, test/num_examples=10000, total_duration=14590.110959, train/accuracy=0.569180, train/loss=1.837003, validation/accuracy=0.526340, validation/loss=2.049276, validation/num_examples=50000
I0203 15:43:23.403021 140023005427456 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.8841740489006042, loss=4.846214771270752
I0203 15:44:09.255981 140022518892288 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1329995393753052, loss=2.9110326766967773
I0203 15:44:55.584852 140023005427456 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.9047492146492004, loss=3.9046149253845215
I0203 15:45:41.881442 140022518892288 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.227664589881897, loss=2.893289804458618
I0203 15:46:28.355252 140023005427456 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8276521563529968, loss=4.662720680236816
I0203 15:47:14.446341 140022518892288 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.1354941129684448, loss=2.9753975868225098
I0203 15:48:00.547749 140023005427456 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.1062718629837036, loss=4.729865074157715
I0203 15:48:46.556415 140022518892288 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.1554492712020874, loss=3.0720396041870117
I0203 15:49:32.764995 140023005427456 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9679559469223022, loss=5.378979206085205
I0203 15:49:45.872093 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:49:56.717392 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:50:33.552397 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:50:35.150506 140184451094336 submission_runner.py:408] Time since start: 15059.43s, 	Step: 29230, 	{'train/accuracy': 0.5762890577316284, 'train/loss': 1.8103874921798706, 'validation/accuracy': 0.5307799577713013, 'validation/loss': 2.0295510292053223, 'validation/num_examples': 50000, 'test/accuracy': 0.4165000319480896, 'test/loss': 2.705432653427124, 'test/num_examples': 10000, 'score': 13484.137912511826, 'total_duration': 15059.425895929337, 'accumulated_submission_time': 13484.137912511826, 'accumulated_eval_time': 1572.8207762241364, 'accumulated_logging_time': 0.9397439956665039}
I0203 15:50:35.178917 140022518892288 logging_writer.py:48] [29230] accumulated_eval_time=1572.820776, accumulated_logging_time=0.939744, accumulated_submission_time=13484.137913, global_step=29230, preemption_count=0, score=13484.137913, test/accuracy=0.416500, test/loss=2.705433, test/num_examples=10000, total_duration=15059.425896, train/accuracy=0.576289, train/loss=1.810387, validation/accuracy=0.530780, validation/loss=2.029551, validation/num_examples=50000
I0203 15:51:04.836081 140023005427456 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0642409324645996, loss=2.921638250350952
I0203 15:51:50.861384 140022518892288 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9334244132041931, loss=5.584783554077148
I0203 15:52:37.244480 140023005427456 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.0422078371047974, loss=2.8513479232788086
I0203 15:53:23.257834 140022518892288 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2848966121673584, loss=2.8992671966552734
I0203 15:54:09.868672 140023005427456 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.1550043821334839, loss=2.8960962295532227
I0203 15:54:55.889795 140022518892288 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.0271135568618774, loss=2.8757247924804688
I0203 15:55:42.056836 140023005427456 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.0621229410171509, loss=5.281367778778076
I0203 15:56:28.236297 140022518892288 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.0338914394378662, loss=2.8988418579101562
I0203 15:57:14.604888 140023005427456 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0589267015457153, loss=2.816493034362793
I0203 15:57:35.473966 140184451094336 spec.py:321] Evaluating on the training split.
I0203 15:57:46.066902 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 15:58:22.452840 140184451094336 spec.py:349] Evaluating on the test split.
I0203 15:58:24.054372 140184451094336 submission_runner.py:408] Time since start: 15528.33s, 	Step: 30147, 	{'train/accuracy': 0.586718738079071, 'train/loss': 1.7852870225906372, 'validation/accuracy': 0.5297799706459045, 'validation/loss': 2.055450677871704, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.7173359394073486, 'test/num_examples': 10000, 'score': 13904.376311302185, 'total_duration': 15528.329751729965, 'accumulated_submission_time': 13904.376311302185, 'accumulated_eval_time': 1621.401178598404, 'accumulated_logging_time': 0.9774858951568604}
I0203 15:58:24.073770 140022518892288 logging_writer.py:48] [30147] accumulated_eval_time=1621.401179, accumulated_logging_time=0.977486, accumulated_submission_time=13904.376311, global_step=30147, preemption_count=0, score=13904.376311, test/accuracy=0.416700, test/loss=2.717336, test/num_examples=10000, total_duration=15528.329752, train/accuracy=0.586719, train/loss=1.785287, validation/accuracy=0.529780, validation/loss=2.055451, validation/num_examples=50000
I0203 15:58:46.638685 140023005427456 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.1980855464935303, loss=2.801647663116455
I0203 15:59:31.840001 140022518892288 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0607736110687256, loss=2.945882797241211
I0203 16:00:18.314231 140023005427456 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.2065083980560303, loss=2.790853977203369
I0203 16:01:04.558539 140022518892288 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0252485275268555, loss=3.6696887016296387
I0203 16:01:50.790222 140023005427456 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0888867378234863, loss=2.8391411304473877
I0203 16:02:37.066285 140022518892288 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0140727758407593, loss=3.1315348148345947
I0203 16:03:23.286710 140023005427456 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.185225248336792, loss=2.9250619411468506
I0203 16:04:09.460280 140022518892288 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.1131747961044312, loss=2.9835124015808105
I0203 16:04:55.638574 140023005427456 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.9142458438873291, loss=5.214085102081299
I0203 16:05:24.405535 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:05:34.893643 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:06:10.458781 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:06:12.062835 140184451094336 submission_runner.py:408] Time since start: 15996.34s, 	Step: 31064, 	{'train/accuracy': 0.5789452791213989, 'train/loss': 1.8326478004455566, 'validation/accuracy': 0.5329399704933167, 'validation/loss': 2.0538718700408936, 'validation/num_examples': 50000, 'test/accuracy': 0.42000001668930054, 'test/loss': 2.728641986846924, 'test/num_examples': 10000, 'score': 14324.65103316307, 'total_duration': 15996.338223218918, 'accumulated_submission_time': 14324.65103316307, 'accumulated_eval_time': 1669.0584816932678, 'accumulated_logging_time': 1.0061118602752686}
I0203 16:06:12.081790 140022518892288 logging_writer.py:48] [31064] accumulated_eval_time=1669.058482, accumulated_logging_time=1.006112, accumulated_submission_time=14324.651033, global_step=31064, preemption_count=0, score=14324.651033, test/accuracy=0.420000, test/loss=2.728642, test/num_examples=10000, total_duration=15996.338223, train/accuracy=0.578945, train/loss=1.832648, validation/accuracy=0.532940, validation/loss=2.053872, validation/num_examples=50000
I0203 16:06:27.542472 140023005427456 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.9762289524078369, loss=5.2775044441223145
I0203 16:07:11.851313 140022518892288 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9775497913360596, loss=4.799607276916504
I0203 16:07:58.292476 140023005427456 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.1445430517196655, loss=2.9301154613494873
I0203 16:08:44.481312 140022518892288 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9960737824440002, loss=4.252123832702637
I0203 16:09:30.542413 140023005427456 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.2178956270217896, loss=2.8772130012512207
I0203 16:10:16.734859 140022518892288 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.2116570472717285, loss=2.8309082984924316
I0203 16:11:02.938030 140023005427456 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.01682448387146, loss=5.483550548553467
I0203 16:11:49.286274 140022518892288 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.1683099269866943, loss=2.8753764629364014
I0203 16:12:35.427194 140023005427456 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7965594530105591, loss=5.221083164215088
I0203 16:13:12.167728 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:13:22.893100 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:13:58.031987 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:13:59.636472 140184451094336 submission_runner.py:408] Time since start: 16463.91s, 	Step: 31981, 	{'train/accuracy': 0.5824804306030273, 'train/loss': 1.7766609191894531, 'validation/accuracy': 0.5413399934768677, 'validation/loss': 1.9840885400772095, 'validation/num_examples': 50000, 'test/accuracy': 0.4280000329017639, 'test/loss': 2.649587631225586, 'test/num_examples': 10000, 'score': 14744.678804397583, 'total_duration': 16463.911828041077, 'accumulated_submission_time': 14744.678804397583, 'accumulated_eval_time': 1716.5272045135498, 'accumulated_logging_time': 1.0351231098175049}
I0203 16:13:59.665415 140022518892288 logging_writer.py:48] [31981] accumulated_eval_time=1716.527205, accumulated_logging_time=1.035123, accumulated_submission_time=14744.678804, global_step=31981, preemption_count=0, score=14744.678804, test/accuracy=0.428000, test/loss=2.649588, test/num_examples=10000, total_duration=16463.911828, train/accuracy=0.582480, train/loss=1.776661, validation/accuracy=0.541340, validation/loss=1.984089, validation/num_examples=50000
I0203 16:14:08.040995 140023005427456 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9085926413536072, loss=3.725788116455078
I0203 16:14:51.674727 140022518892288 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.1209304332733154, loss=2.8643720149993896
I0203 16:15:37.830918 140023005427456 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.2434459924697876, loss=3.023709774017334
I0203 16:16:24.294751 140022518892288 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.8249626755714417, loss=4.384552001953125
I0203 16:17:10.418024 140023005427456 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.0358139276504517, loss=3.1523756980895996
I0203 16:17:56.625631 140022518892288 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.073154330253601, loss=3.6770005226135254
I0203 16:18:43.030866 140023005427456 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.1425626277923584, loss=2.9645581245422363
I0203 16:19:29.394737 140022518892288 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1395230293273926, loss=3.654916763305664
I0203 16:20:15.657860 140023005427456 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0786190032958984, loss=4.354963779449463
I0203 16:20:59.757258 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:21:10.823279 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:21:45.542062 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:21:47.153681 140184451094336 submission_runner.py:408] Time since start: 16931.43s, 	Step: 32897, 	{'train/accuracy': 0.5879296660423279, 'train/loss': 1.7226225137710571, 'validation/accuracy': 0.5427199602127075, 'validation/loss': 1.9646421670913696, 'validation/num_examples': 50000, 'test/accuracy': 0.4245000183582306, 'test/loss': 2.6344099044799805, 'test/num_examples': 10000, 'score': 15164.711621046066, 'total_duration': 16931.429044008255, 'accumulated_submission_time': 15164.711621046066, 'accumulated_eval_time': 1763.92360329628, 'accumulated_logging_time': 1.0761663913726807}
I0203 16:21:47.184149 140022518892288 logging_writer.py:48] [32897] accumulated_eval_time=1763.923603, accumulated_logging_time=1.076166, accumulated_submission_time=15164.711621, global_step=32897, preemption_count=0, score=15164.711621, test/accuracy=0.424500, test/loss=2.634410, test/num_examples=10000, total_duration=16931.429044, train/accuracy=0.587930, train/loss=1.722623, validation/accuracy=0.542720, validation/loss=1.964642, validation/num_examples=50000
I0203 16:21:48.859501 140023005427456 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.0187135934829712, loss=4.302453994750977
I0203 16:22:31.998043 140022518892288 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.129647970199585, loss=3.0672454833984375
I0203 16:23:18.331810 140023005427456 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.0091862678527832, loss=3.6633169651031494
I0203 16:24:04.847840 140022518892288 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.2168500423431396, loss=2.8205108642578125
I0203 16:24:50.929601 140023005427456 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1035118103027344, loss=5.298443794250488
I0203 16:25:37.420318 140022518892288 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.168125867843628, loss=3.7089319229125977
I0203 16:26:23.826738 140023005427456 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.108581781387329, loss=2.8082048892974854
I0203 16:27:10.080484 140022518892288 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.108655333518982, loss=2.9327709674835205
I0203 16:27:56.316002 140023005427456 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.1895655393600464, loss=2.809840679168701
I0203 16:28:42.933447 140022518892288 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1392040252685547, loss=2.908012628555298
I0203 16:28:47.364078 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:28:58.184897 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:29:33.757863 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:29:35.364480 140184451094336 submission_runner.py:408] Time since start: 17399.64s, 	Step: 33811, 	{'train/accuracy': 0.5936523079872131, 'train/loss': 1.7037583589553833, 'validation/accuracy': 0.5494799613952637, 'validation/loss': 1.9261177778244019, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.6165106296539307, 'test/num_examples': 10000, 'score': 15584.832740068436, 'total_duration': 17399.639848709106, 'accumulated_submission_time': 15584.832740068436, 'accumulated_eval_time': 1811.9239838123322, 'accumulated_logging_time': 1.1177661418914795}
I0203 16:29:35.388788 140023005427456 logging_writer.py:48] [33811] accumulated_eval_time=1811.923984, accumulated_logging_time=1.117766, accumulated_submission_time=15584.832740, global_step=33811, preemption_count=0, score=15584.832740, test/accuracy=0.430800, test/loss=2.616511, test/num_examples=10000, total_duration=17399.639849, train/accuracy=0.593652, train/loss=1.703758, validation/accuracy=0.549480, validation/loss=1.926118, validation/num_examples=50000
I0203 16:30:13.542159 140022518892288 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.964695155620575, loss=3.430452585220337
I0203 16:30:59.674324 140023005427456 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0653862953186035, loss=2.69266414642334
I0203 16:31:46.434676 140022518892288 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.9783526062965393, loss=3.819397211074829
I0203 16:32:32.526619 140023005427456 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.0176507234573364, loss=5.132377624511719
I0203 16:33:18.742053 140022518892288 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0748192071914673, loss=3.2313661575317383
I0203 16:34:05.100092 140023005427456 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.9866974353790283, loss=3.54154372215271
I0203 16:34:51.209699 140022518892288 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.2536894083023071, loss=2.8919358253479004
I0203 16:35:37.469657 140023005427456 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.9288539886474609, loss=4.2036848068237305
I0203 16:36:23.780326 140022518892288 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.1469110250473022, loss=2.834275722503662
I0203 16:36:35.399425 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:36:46.303611 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:37:23.684631 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:37:25.283034 140184451094336 submission_runner.py:408] Time since start: 17869.56s, 	Step: 34727, 	{'train/accuracy': 0.5853710770606995, 'train/loss': 1.762331485748291, 'validation/accuracy': 0.5461999773979187, 'validation/loss': 1.964341163635254, 'validation/num_examples': 50000, 'test/accuracy': 0.43240001797676086, 'test/loss': 2.64608097076416, 'test/num_examples': 10000, 'score': 16004.785893440247, 'total_duration': 17869.55842280388, 'accumulated_submission_time': 16004.785893440247, 'accumulated_eval_time': 1861.8075823783875, 'accumulated_logging_time': 1.1519536972045898}
I0203 16:37:25.302980 140023005427456 logging_writer.py:48] [34727] accumulated_eval_time=1861.807582, accumulated_logging_time=1.151954, accumulated_submission_time=16004.785893, global_step=34727, preemption_count=0, score=16004.785893, test/accuracy=0.432400, test/loss=2.646081, test/num_examples=10000, total_duration=17869.558423, train/accuracy=0.585371, train/loss=1.762331, validation/accuracy=0.546200, validation/loss=1.964341, validation/num_examples=50000
I0203 16:37:56.239426 140022518892288 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.971893846988678, loss=4.813776969909668
I0203 16:38:42.443369 140023005427456 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.0977743864059448, loss=2.869968891143799
I0203 16:39:28.855093 140022518892288 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.039193034172058, loss=3.344259262084961
I0203 16:40:15.322687 140023005427456 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1744921207427979, loss=2.830714702606201
I0203 16:41:01.365164 140022518892288 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.1101806163787842, loss=3.021071672439575
I0203 16:41:47.971132 140023005427456 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.178289771080017, loss=2.78755521774292
I0203 16:42:34.066393 140022518892288 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.8825459480285645, loss=4.924054145812988
I0203 16:43:20.334101 140023005427456 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.214029312133789, loss=2.9154624938964844
I0203 16:44:06.974967 140022518892288 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.9296541810035706, loss=3.964279890060425
I0203 16:44:25.672143 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:44:36.263173 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:45:12.997729 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:45:14.600394 140184451094336 submission_runner.py:408] Time since start: 18338.88s, 	Step: 35643, 	{'train/accuracy': 0.6008984446525574, 'train/loss': 1.6964315176010132, 'validation/accuracy': 0.5519599914550781, 'validation/loss': 1.9376044273376465, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.613935708999634, 'test/num_examples': 10000, 'score': 16425.0973944664, 'total_duration': 18338.875772714615, 'accumulated_submission_time': 16425.0973944664, 'accumulated_eval_time': 1910.7358112335205, 'accumulated_logging_time': 1.1812076568603516}
I0203 16:45:14.622739 140023005427456 logging_writer.py:48] [35643] accumulated_eval_time=1910.735811, accumulated_logging_time=1.181208, accumulated_submission_time=16425.097394, global_step=35643, preemption_count=0, score=16425.097394, test/accuracy=0.434000, test/loss=2.613936, test/num_examples=10000, total_duration=18338.875773, train/accuracy=0.600898, train/loss=1.696432, validation/accuracy=0.551960, validation/loss=1.937604, validation/num_examples=50000
I0203 16:45:38.841792 140022518892288 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.2330982685089111, loss=2.7945799827575684
I0203 16:46:24.358561 140023005427456 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.171005368232727, loss=2.851402997970581
I0203 16:47:10.915912 140022518892288 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.2059977054595947, loss=2.7636284828186035
I0203 16:47:57.347157 140023005427456 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.2127889394760132, loss=2.703383684158325
I0203 16:48:43.784004 140022518892288 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.0291911363601685, loss=3.190789222717285
I0203 16:49:30.402435 140023005427456 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.8795667886734009, loss=4.984066486358643
I0203 16:50:17.271281 140022518892288 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1417346000671387, loss=2.948265552520752
I0203 16:51:03.604021 140023005427456 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.8835371136665344, loss=5.014075756072998
I0203 16:51:50.218121 140022518892288 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0839033126831055, loss=2.8088159561157227
I0203 16:52:14.876048 140184451094336 spec.py:321] Evaluating on the training split.
I0203 16:52:25.767880 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 16:53:02.644024 140184451094336 spec.py:349] Evaluating on the test split.
I0203 16:53:04.252615 140184451094336 submission_runner.py:408] Time since start: 18808.53s, 	Step: 36555, 	{'train/accuracy': 0.6290624737739563, 'train/loss': 1.551759958267212, 'validation/accuracy': 0.5550999641418457, 'validation/loss': 1.8873655796051025, 'validation/num_examples': 50000, 'test/accuracy': 0.4384000301361084, 'test/loss': 2.5619187355041504, 'test/num_examples': 10000, 'score': 16845.29321050644, 'total_duration': 18808.527985811234, 'accumulated_submission_time': 16845.29321050644, 'accumulated_eval_time': 1960.112357378006, 'accumulated_logging_time': 1.2128477096557617}
I0203 16:53:04.278875 140023005427456 logging_writer.py:48] [36555] accumulated_eval_time=1960.112357, accumulated_logging_time=1.212848, accumulated_submission_time=16845.293211, global_step=36555, preemption_count=0, score=16845.293211, test/accuracy=0.438400, test/loss=2.561919, test/num_examples=10000, total_duration=18808.527986, train/accuracy=0.629062, train/loss=1.551760, validation/accuracy=0.555100, validation/loss=1.887366, validation/num_examples=50000
I0203 16:53:23.504188 140022518892288 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.190198302268982, loss=2.7690539360046387
I0203 16:54:08.375308 140023005427456 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.2189027070999146, loss=2.7272818088531494
I0203 16:54:54.572126 140022518892288 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0655207633972168, loss=2.6592535972595215
I0203 16:55:41.021744 140023005427456 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.178635835647583, loss=3.230234146118164
I0203 16:56:27.046397 140022518892288 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.101163625717163, loss=2.7416651248931885
I0203 16:57:13.503165 140023005427456 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.0797219276428223, loss=3.068927764892578
I0203 16:57:59.752833 140022518892288 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.11687171459198, loss=2.9862396717071533
I0203 16:58:46.231954 140023005427456 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.1969330310821533, loss=2.8608829975128174
I0203 16:59:32.610287 140022518892288 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.1841857433319092, loss=2.749447822570801
I0203 17:00:04.462741 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:00:15.211908 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:00:53.127886 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:00:54.730688 140184451094336 submission_runner.py:408] Time since start: 19279.01s, 	Step: 37470, 	{'train/accuracy': 0.5981054306030273, 'train/loss': 1.7004003524780273, 'validation/accuracy': 0.5553799867630005, 'validation/loss': 1.9004456996917725, 'validation/num_examples': 50000, 'test/accuracy': 0.44300001859664917, 'test/loss': 2.575869560241699, 'test/num_examples': 10000, 'score': 17265.42009329796, 'total_duration': 19279.006051301956, 'accumulated_submission_time': 17265.42009329796, 'accumulated_eval_time': 2010.38028049469, 'accumulated_logging_time': 1.2492239475250244}
I0203 17:00:54.755140 140023005427456 logging_writer.py:48] [37470] accumulated_eval_time=2010.380280, accumulated_logging_time=1.249224, accumulated_submission_time=17265.420093, global_step=37470, preemption_count=0, score=17265.420093, test/accuracy=0.443000, test/loss=2.575870, test/num_examples=10000, total_duration=19279.006051, train/accuracy=0.598105, train/loss=1.700400, validation/accuracy=0.555380, validation/loss=1.900446, validation/num_examples=50000
I0203 17:01:07.723651 140022518892288 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.0539039373397827, loss=3.3196377754211426
I0203 17:01:52.172816 140023005427456 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.9568471908569336, loss=5.442728042602539
I0203 17:02:38.416913 140022518892288 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.0472878217697144, loss=3.350303888320923
I0203 17:03:24.940230 140023005427456 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.0686051845550537, loss=2.726259231567383
I0203 17:04:11.237663 140022518892288 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1605151891708374, loss=2.8006012439727783
I0203 17:04:57.480727 140023005427456 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0995651483535767, loss=3.1430397033691406
I0203 17:05:43.950773 140022518892288 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1657506227493286, loss=2.752089738845825
I0203 17:06:30.423047 140023005427456 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.9512346982955933, loss=3.5203027725219727
I0203 17:07:16.838056 140022518892288 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.1048648357391357, loss=2.741576671600342
I0203 17:07:54.884060 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:08:05.735167 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:08:42.401269 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:08:44.001419 140184451094336 submission_runner.py:408] Time since start: 19748.28s, 	Step: 38384, 	{'train/accuracy': 0.5977538824081421, 'train/loss': 1.6697273254394531, 'validation/accuracy': 0.5511000156402588, 'validation/loss': 1.906893253326416, 'validation/num_examples': 50000, 'test/accuracy': 0.4426000118255615, 'test/loss': 2.580732822418213, 'test/num_examples': 10000, 'score': 17685.49142575264, 'total_duration': 19748.276788711548, 'accumulated_submission_time': 17685.49142575264, 'accumulated_eval_time': 2059.4976251125336, 'accumulated_logging_time': 1.2838959693908691}
I0203 17:08:44.032927 140023005427456 logging_writer.py:48] [38384] accumulated_eval_time=2059.497625, accumulated_logging_time=1.283896, accumulated_submission_time=17685.491426, global_step=38384, preemption_count=0, score=17685.491426, test/accuracy=0.442600, test/loss=2.580733, test/num_examples=10000, total_duration=19748.276789, train/accuracy=0.597754, train/loss=1.669727, validation/accuracy=0.551100, validation/loss=1.906893, validation/num_examples=50000
I0203 17:08:51.133971 140022518892288 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.0298951864242554, loss=3.5355913639068604
I0203 17:09:35.073288 140023005427456 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8421681523323059, loss=4.210069179534912
I0203 17:10:21.616381 140022518892288 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.14767324924469, loss=2.803079843521118
I0203 17:11:07.953208 140023005427456 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.1104669570922852, loss=3.8290374279022217
I0203 17:11:54.536218 140022518892288 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.171957015991211, loss=2.764740228652954
I0203 17:12:40.743245 140023005427456 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2159056663513184, loss=2.7101798057556152
I0203 17:13:27.086712 140022518892288 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.9939016699790955, loss=3.9571306705474854
I0203 17:14:13.378176 140023005427456 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1983362436294556, loss=2.7467892169952393
I0203 17:14:59.648072 140022518892288 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.0745179653167725, loss=2.6972455978393555
I0203 17:15:44.390970 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:15:55.036905 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:16:31.446848 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:16:33.057387 140184451094336 submission_runner.py:408] Time since start: 20217.33s, 	Step: 39298, 	{'train/accuracy': 0.6231836080551147, 'train/loss': 1.5941240787506104, 'validation/accuracy': 0.5628399848937988, 'validation/loss': 1.888396143913269, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.558645725250244, 'test/num_examples': 10000, 'score': 18105.790986537933, 'total_duration': 20217.332773685455, 'accumulated_submission_time': 18105.790986537933, 'accumulated_eval_time': 2108.164050579071, 'accumulated_logging_time': 1.3266685009002686}
I0203 17:16:33.079571 140023005427456 logging_writer.py:48] [39298] accumulated_eval_time=2108.164051, accumulated_logging_time=1.326669, accumulated_submission_time=18105.790987, global_step=39298, preemption_count=0, score=18105.790987, test/accuracy=0.443800, test/loss=2.558646, test/num_examples=10000, total_duration=20217.332774, train/accuracy=0.623184, train/loss=1.594124, validation/accuracy=0.562840, validation/loss=1.888396, validation/num_examples=50000
I0203 17:16:34.336480 140022518892288 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.2066320180892944, loss=2.7646484375
I0203 17:17:17.420816 140023005427456 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9113163352012634, loss=4.336408615112305
I0203 17:18:03.347696 140022518892288 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.075231671333313, loss=5.19812536239624
I0203 17:18:49.653664 140023005427456 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0747969150543213, loss=2.9280917644500732
I0203 17:19:35.773370 140022518892288 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.030578374862671, loss=3.031860589981079
I0203 17:20:22.194165 140023005427456 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.1204280853271484, loss=2.7151880264282227
I0203 17:21:08.691488 140022518892288 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.9867449402809143, loss=5.098997116088867
I0203 17:21:55.100330 140023005427456 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9243650436401367, loss=3.4416041374206543
I0203 17:22:41.570683 140022518892288 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.1368799209594727, loss=2.763293743133545
I0203 17:23:27.831911 140023005427456 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.067860722541809, loss=3.3032469749450684
I0203 17:23:33.503079 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:23:44.433403 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:24:20.570842 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:24:22.178040 140184451094336 submission_runner.py:408] Time since start: 20686.45s, 	Step: 40214, 	{'train/accuracy': 0.5969336032867432, 'train/loss': 1.700569987297058, 'validation/accuracy': 0.55485999584198, 'validation/loss': 1.9114704132080078, 'validation/num_examples': 50000, 'test/accuracy': 0.44140002131462097, 'test/loss': 2.5693047046661377, 'test/num_examples': 10000, 'score': 18526.15670633316, 'total_duration': 20686.453429460526, 'accumulated_submission_time': 18526.15670633316, 'accumulated_eval_time': 2156.8390045166016, 'accumulated_logging_time': 1.3593740463256836}
I0203 17:24:22.199327 140022518892288 logging_writer.py:48] [40214] accumulated_eval_time=2156.839005, accumulated_logging_time=1.359374, accumulated_submission_time=18526.156706, global_step=40214, preemption_count=0, score=18526.156706, test/accuracy=0.441400, test/loss=2.569305, test/num_examples=10000, total_duration=20686.453429, train/accuracy=0.596934, train/loss=1.700570, validation/accuracy=0.554860, validation/loss=1.911470, validation/num_examples=50000
I0203 17:24:59.096675 140023005427456 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.2229341268539429, loss=2.7207860946655273
I0203 17:25:45.327344 140022518892288 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0892019271850586, loss=2.9220337867736816
I0203 17:26:31.733017 140023005427456 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.9022656679153442, loss=5.080505847930908
I0203 17:27:18.152992 140022518892288 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1093779802322388, loss=2.8079538345336914
I0203 17:28:04.296769 140023005427456 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0903935432434082, loss=2.839395523071289
I0203 17:28:50.329899 140022518892288 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.1808096170425415, loss=2.9215197563171387
I0203 17:29:36.766437 140023005427456 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9437946677207947, loss=4.317713737487793
I0203 17:30:23.148553 140022518892288 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.152808427810669, loss=2.6725151538848877
I0203 17:31:09.209727 140023005427456 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.208693027496338, loss=2.6286168098449707
I0203 17:31:22.236269 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:31:33.388038 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:32:09.705413 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:32:11.312639 140184451094336 submission_runner.py:408] Time since start: 21155.59s, 	Step: 41130, 	{'train/accuracy': 0.6115429401397705, 'train/loss': 1.6528892517089844, 'validation/accuracy': 0.5624399781227112, 'validation/loss': 1.895995855331421, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.548941135406494, 'test/num_examples': 10000, 'score': 18946.13728427887, 'total_duration': 21155.588027715683, 'accumulated_submission_time': 18946.13728427887, 'accumulated_eval_time': 2205.9153735637665, 'accumulated_logging_time': 1.389624834060669}
I0203 17:32:11.333134 140022518892288 logging_writer.py:48] [41130] accumulated_eval_time=2205.915374, accumulated_logging_time=1.389625, accumulated_submission_time=18946.137284, global_step=41130, preemption_count=0, score=18946.137284, test/accuracy=0.445600, test/loss=2.548941, test/num_examples=10000, total_duration=21155.588028, train/accuracy=0.611543, train/loss=1.652889, validation/accuracy=0.562440, validation/loss=1.895996, validation/num_examples=50000
I0203 17:32:40.975868 140023005427456 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.9400755763053894, loss=4.022316932678223
I0203 17:33:27.224406 140022518892288 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.4952651262283325, loss=2.7369115352630615
I0203 17:34:13.724325 140023005427456 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.2938308715820312, loss=2.783115863800049
I0203 17:34:59.811479 140022518892288 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.079407811164856, loss=2.680370807647705
I0203 17:35:46.276741 140023005427456 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.33595871925354, loss=2.775097370147705
I0203 17:36:32.610211 140022518892288 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9526473879814148, loss=5.021924018859863
I0203 17:37:19.103285 140023005427456 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.2891167402267456, loss=2.7460856437683105
I0203 17:38:05.698860 140022518892288 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9417665600776672, loss=4.575263023376465
I0203 17:38:51.903879 140023005427456 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1041719913482666, loss=4.064043045043945
I0203 17:39:11.577766 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:39:22.459213 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:39:59.103226 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:40:00.707684 140184451094336 submission_runner.py:408] Time since start: 21624.98s, 	Step: 42044, 	{'train/accuracy': 0.6252148151397705, 'train/loss': 1.5360467433929443, 'validation/accuracy': 0.5721399784088135, 'validation/loss': 1.8095941543579102, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.498311758041382, 'test/num_examples': 10000, 'score': 19366.324320554733, 'total_duration': 21624.983068466187, 'accumulated_submission_time': 19366.324320554733, 'accumulated_eval_time': 2255.0452842712402, 'accumulated_logging_time': 1.4197359085083008}
I0203 17:40:00.728912 140022518892288 logging_writer.py:48] [42044] accumulated_eval_time=2255.045284, accumulated_logging_time=1.419736, accumulated_submission_time=19366.324321, global_step=42044, preemption_count=0, score=19366.324321, test/accuracy=0.454200, test/loss=2.498312, test/num_examples=10000, total_duration=21624.983068, train/accuracy=0.625215, train/loss=1.536047, validation/accuracy=0.572140, validation/loss=1.809594, validation/num_examples=50000
I0203 17:40:24.554762 140023005427456 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0279254913330078, loss=3.197028398513794
I0203 17:41:10.100557 140022518892288 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1645021438598633, loss=2.9008426666259766
I0203 17:41:56.662582 140023005427456 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.143566608428955, loss=3.176152467727661
I0203 17:42:43.117618 140022518892288 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.4142122268676758, loss=3.2638840675354004
I0203 17:43:29.598997 140023005427456 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0096405744552612, loss=3.7073724269866943
I0203 17:44:16.090305 140022518892288 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.103068470954895, loss=2.629611015319824
I0203 17:45:02.446853 140023005427456 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2379854917526245, loss=3.156036853790283
I0203 17:45:48.937427 140022518892288 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0638117790222168, loss=2.829317092895508
I0203 17:46:35.392724 140023005427456 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.2563015222549438, loss=2.651073932647705
I0203 17:47:00.851028 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:47:11.526724 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:47:49.140154 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:47:50.744444 140184451094336 submission_runner.py:408] Time since start: 22095.02s, 	Step: 42957, 	{'train/accuracy': 0.6100390553474426, 'train/loss': 1.6844910383224487, 'validation/accuracy': 0.5631600022315979, 'validation/loss': 1.9153586626052856, 'validation/num_examples': 50000, 'test/accuracy': 0.44850000739097595, 'test/loss': 2.561943292617798, 'test/num_examples': 10000, 'score': 19786.389555692673, 'total_duration': 22095.019829034805, 'accumulated_submission_time': 19786.389555692673, 'accumulated_eval_time': 2304.938737630844, 'accumulated_logging_time': 1.450535774230957}
I0203 17:47:50.767511 140022518892288 logging_writer.py:48] [42957] accumulated_eval_time=2304.938738, accumulated_logging_time=1.450536, accumulated_submission_time=19786.389556, global_step=42957, preemption_count=0, score=19786.389556, test/accuracy=0.448500, test/loss=2.561943, test/num_examples=10000, total_duration=22095.019829, train/accuracy=0.610039, train/loss=1.684491, validation/accuracy=0.563160, validation/loss=1.915359, validation/num_examples=50000
I0203 17:48:09.158407 140023005427456 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.2047053575515747, loss=2.7932748794555664
I0203 17:48:53.816173 140022518892288 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.3947219848632812, loss=2.716543436050415
I0203 17:49:40.199131 140023005427456 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1428765058517456, loss=2.6666922569274902
I0203 17:50:26.606232 140022518892288 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.1465694904327393, loss=2.8437697887420654
I0203 17:51:12.807216 140023005427456 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.9587029814720154, loss=3.693458318710327
I0203 17:51:59.182810 140022518892288 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.199784278869629, loss=2.566568613052368
I0203 17:52:47.225329 140023005427456 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.9342169165611267, loss=5.148436546325684
I0203 17:53:33.361825 140022518892288 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.3050655126571655, loss=2.862123966217041
I0203 17:54:19.776517 140023005427456 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.3091343641281128, loss=2.7512989044189453
I0203 17:54:50.782662 140184451094336 spec.py:321] Evaluating on the training split.
I0203 17:55:01.640203 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 17:55:37.059808 140184451094336 spec.py:349] Evaluating on the test split.
I0203 17:55:38.667018 140184451094336 submission_runner.py:408] Time since start: 22562.94s, 	Step: 43869, 	{'train/accuracy': 0.6125780940055847, 'train/loss': 1.6683449745178223, 'validation/accuracy': 0.5681599974632263, 'validation/loss': 1.8960041999816895, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.5512001514434814, 'test/num_examples': 10000, 'score': 20206.347382307053, 'total_duration': 22562.94238114357, 'accumulated_submission_time': 20206.347382307053, 'accumulated_eval_time': 2352.8230979442596, 'accumulated_logging_time': 1.4831857681274414}
I0203 17:55:38.693248 140022518892288 logging_writer.py:48] [43869] accumulated_eval_time=2352.823098, accumulated_logging_time=1.483186, accumulated_submission_time=20206.347382, global_step=43869, preemption_count=0, score=20206.347382, test/accuracy=0.450200, test/loss=2.551200, test/num_examples=10000, total_duration=22562.942381, train/accuracy=0.612578, train/loss=1.668345, validation/accuracy=0.568160, validation/loss=1.896004, validation/num_examples=50000
I0203 17:55:52.064548 140023005427456 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.9008479714393616, loss=5.298202991485596
I0203 17:56:36.274238 140022518892288 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.3058626651763916, loss=2.672415018081665
I0203 17:57:22.536617 140023005427456 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.2225393056869507, loss=2.715488910675049
I0203 17:58:09.173334 140022518892288 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.282322883605957, loss=2.6418607234954834
I0203 17:58:55.513712 140023005427456 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.9250032901763916, loss=4.208644390106201
I0203 17:59:42.208332 140022518892288 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.9856204390525818, loss=4.032801628112793
I0203 18:00:28.542023 140023005427456 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.3064641952514648, loss=2.725633144378662
I0203 18:01:14.862730 140022518892288 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1920111179351807, loss=2.6944470405578613
I0203 18:02:01.284278 140023005427456 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.9091869592666626, loss=4.057712554931641
I0203 18:02:38.773680 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:02:49.857778 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:03:26.766154 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:03:28.375238 140184451094336 submission_runner.py:408] Time since start: 23032.65s, 	Step: 44782, 	{'train/accuracy': 0.6306250095367432, 'train/loss': 1.5366846323013306, 'validation/accuracy': 0.5754599571228027, 'validation/loss': 1.7981550693511963, 'validation/num_examples': 50000, 'test/accuracy': 0.4561000168323517, 'test/loss': 2.4726850986480713, 'test/num_examples': 10000, 'score': 20626.368554592133, 'total_duration': 23032.650621652603, 'accumulated_submission_time': 20626.368554592133, 'accumulated_eval_time': 2402.4246587753296, 'accumulated_logging_time': 1.521988868713379}
I0203 18:03:28.400320 140022518892288 logging_writer.py:48] [44782] accumulated_eval_time=2402.424659, accumulated_logging_time=1.521989, accumulated_submission_time=20626.368555, global_step=44782, preemption_count=0, score=20626.368555, test/accuracy=0.456100, test/loss=2.472685, test/num_examples=10000, total_duration=23032.650622, train/accuracy=0.630625, train/loss=1.536685, validation/accuracy=0.575460, validation/loss=1.798155, validation/num_examples=50000
I0203 18:03:36.336790 140023005427456 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.1291428804397583, loss=2.569737434387207
I0203 18:04:19.997312 140022518892288 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.162577748298645, loss=2.951570510864258
I0203 18:05:06.269830 140023005427456 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1873555183410645, loss=5.304109573364258
I0203 18:05:52.902557 140022518892288 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.1075108051300049, loss=2.6447715759277344
I0203 18:06:39.234609 140023005427456 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8398955464363098, loss=5.271960258483887
I0203 18:07:25.573143 140022518892288 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.0349923372268677, loss=3.0970473289489746
I0203 18:08:12.059923 140023005427456 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1743698120117188, loss=2.787539005279541
I0203 18:08:58.775235 140022518892288 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0339986085891724, loss=3.8611583709716797
I0203 18:09:45.074160 140023005427456 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.049251914024353, loss=3.5717880725860596
I0203 18:10:28.471238 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:10:39.074137 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:11:15.692951 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:11:17.295120 140184451094336 submission_runner.py:408] Time since start: 23501.57s, 	Step: 45695, 	{'train/accuracy': 0.6145312190055847, 'train/loss': 1.6023259162902832, 'validation/accuracy': 0.5731599926948547, 'validation/loss': 1.8077213764190674, 'validation/num_examples': 50000, 'test/accuracy': 0.46310001611709595, 'test/loss': 2.4594175815582275, 'test/num_examples': 10000, 'score': 21046.379149913788, 'total_duration': 23501.57047510147, 'accumulated_submission_time': 21046.379149913788, 'accumulated_eval_time': 2451.248507976532, 'accumulated_logging_time': 1.560218334197998}
I0203 18:11:17.322706 140022518892288 logging_writer.py:48] [45695] accumulated_eval_time=2451.248508, accumulated_logging_time=1.560218, accumulated_submission_time=21046.379150, global_step=45695, preemption_count=0, score=21046.379150, test/accuracy=0.463100, test/loss=2.459418, test/num_examples=10000, total_duration=23501.570475, train/accuracy=0.614531, train/loss=1.602326, validation/accuracy=0.573160, validation/loss=1.807721, validation/num_examples=50000
I0203 18:11:19.833534 140023005427456 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1421785354614258, loss=2.999001979827881
I0203 18:12:03.186825 140022518892288 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1664460897445679, loss=2.6313540935516357
I0203 18:12:49.347928 140023005427456 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.159737229347229, loss=2.708371877670288
I0203 18:13:35.816979 140022518892288 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1863633394241333, loss=2.746530532836914
I0203 18:14:22.002141 140023005427456 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.23537015914917, loss=2.629760265350342
I0203 18:15:08.176013 140022518892288 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.0400198698043823, loss=3.0648789405822754
I0203 18:15:54.608405 140023005427456 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.3237497806549072, loss=2.7252659797668457
I0203 18:16:40.705475 140022518892288 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9426966309547424, loss=5.182876110076904
I0203 18:17:26.981400 140023005427456 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.2398077249526978, loss=2.7414982318878174
I0203 18:18:13.175003 140022518892288 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1457915306091309, loss=2.622161626815796
I0203 18:18:17.463471 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:18:28.174663 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:19:04.700786 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:19:06.299419 140184451094336 submission_runner.py:408] Time since start: 23970.57s, 	Step: 46611, 	{'train/accuracy': 0.6163476705551147, 'train/loss': 1.598486304283142, 'validation/accuracy': 0.5700199604034424, 'validation/loss': 1.8413797616958618, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.510585308074951, 'test/num_examples': 10000, 'score': 21466.459342956543, 'total_duration': 23970.574808597565, 'accumulated_submission_time': 21466.459342956543, 'accumulated_eval_time': 2500.084460258484, 'accumulated_logging_time': 1.5997076034545898}
I0203 18:19:06.321718 140023005427456 logging_writer.py:48] [46611] accumulated_eval_time=2500.084460, accumulated_logging_time=1.599708, accumulated_submission_time=21466.459343, global_step=46611, preemption_count=0, score=21466.459343, test/accuracy=0.453100, test/loss=2.510585, test/num_examples=10000, total_duration=23970.574809, train/accuracy=0.616348, train/loss=1.598486, validation/accuracy=0.570020, validation/loss=1.841380, validation/num_examples=50000
I0203 18:19:44.773167 140022518892288 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.216654658317566, loss=2.6634347438812256
I0203 18:20:31.174034 140023005427456 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9748333692550659, loss=4.69539737701416
I0203 18:21:17.788859 140022518892288 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.093973994255066, loss=3.6685292720794678
I0203 18:22:04.365343 140023005427456 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9658128023147583, loss=4.705395221710205
I0203 18:22:50.975239 140022518892288 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.1507818698883057, loss=2.7654638290405273
I0203 18:23:37.341255 140023005427456 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.1787594556808472, loss=5.135567665100098
I0203 18:24:23.982097 140022518892288 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.9523088335990906, loss=4.636449813842773
I0203 18:25:10.840905 140023005427456 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1738563776016235, loss=2.694598436355591
I0203 18:25:57.289985 140022518892288 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8596399426460266, loss=5.293460369110107
I0203 18:26:06.305632 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:26:16.748733 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:26:54.302582 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:26:55.903284 140184451094336 submission_runner.py:408] Time since start: 24440.18s, 	Step: 47521, 	{'train/accuracy': 0.629199206829071, 'train/loss': 1.6050124168395996, 'validation/accuracy': 0.575760006904602, 'validation/loss': 1.858893871307373, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.499596118927002, 'test/num_examples': 10000, 'score': 21886.386165380478, 'total_duration': 24440.17867231369, 'accumulated_submission_time': 21886.386165380478, 'accumulated_eval_time': 2549.6821115016937, 'accumulated_logging_time': 1.631948709487915}
I0203 18:26:55.926026 140023005427456 logging_writer.py:48] [47521] accumulated_eval_time=2549.682112, accumulated_logging_time=1.631949, accumulated_submission_time=21886.386165, global_step=47521, preemption_count=0, score=21886.386165, test/accuracy=0.459500, test/loss=2.499596, test/num_examples=10000, total_duration=24440.178672, train/accuracy=0.629199, train/loss=1.605012, validation/accuracy=0.575760, validation/loss=1.858894, validation/num_examples=50000
I0203 18:27:29.620719 140022518892288 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1071885824203491, loss=3.0611774921417236
I0203 18:28:15.775385 140023005427456 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.0852266550064087, loss=5.14154052734375
I0203 18:29:02.477429 140022518892288 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.161084532737732, loss=2.7192208766937256
I0203 18:29:49.237094 140023005427456 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0378799438476562, loss=2.789407730102539
I0203 18:30:35.706421 140022518892288 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.8352534174919128, loss=5.130422115325928
I0203 18:31:22.310865 140023005427456 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.0960378646850586, loss=3.6448593139648438
I0203 18:32:09.344292 140022518892288 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.110848069190979, loss=3.0079755783081055
I0203 18:32:55.602227 140023005427456 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0243864059448242, loss=4.610630512237549
I0203 18:33:42.180801 140022518892288 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0503853559494019, loss=2.8541746139526367
I0203 18:33:55.930508 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:34:06.795787 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:34:42.364659 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:34:43.967083 140184451094336 submission_runner.py:408] Time since start: 24908.24s, 	Step: 48431, 	{'train/accuracy': 0.6450585722923279, 'train/loss': 1.445672631263733, 'validation/accuracy': 0.5813400149345398, 'validation/loss': 1.7478997707366943, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.401552677154541, 'test/num_examples': 10000, 'score': 22306.334075450897, 'total_duration': 24908.24243426323, 'accumulated_submission_time': 22306.334075450897, 'accumulated_eval_time': 2597.7186567783356, 'accumulated_logging_time': 1.6638882160186768}
I0203 18:34:43.996181 140023005427456 logging_writer.py:48] [48431] accumulated_eval_time=2597.718657, accumulated_logging_time=1.663888, accumulated_submission_time=22306.334075, global_step=48431, preemption_count=0, score=22306.334075, test/accuracy=0.467800, test/loss=2.401553, test/num_examples=10000, total_duration=24908.242434, train/accuracy=0.645059, train/loss=1.445673, validation/accuracy=0.581340, validation/loss=1.747900, validation/num_examples=50000
I0203 18:35:13.255624 140022518892288 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2880016565322876, loss=2.6812784671783447
I0203 18:35:59.104168 140023005427456 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.113558292388916, loss=2.6780033111572266
I0203 18:36:45.695460 140022518892288 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1291331052780151, loss=2.643559217453003
I0203 18:37:32.340868 140023005427456 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.021161437034607, loss=4.93669319152832
I0203 18:38:18.863452 140022518892288 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.1169967651367188, loss=2.709665298461914
I0203 18:39:05.506159 140023005427456 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.2196038961410522, loss=2.6136674880981445
I0203 18:39:52.016205 140022518892288 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.2265313863754272, loss=2.735743522644043
I0203 18:40:38.635861 140023005427456 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.0681813955307007, loss=3.3471169471740723
I0203 18:41:25.126029 140022518892288 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9885063171386719, loss=4.805300235748291
I0203 18:41:44.422643 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:41:55.276298 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:42:31.847918 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:42:33.447670 140184451094336 submission_runner.py:408] Time since start: 25377.72s, 	Step: 49343, 	{'train/accuracy': 0.623046875, 'train/loss': 1.623903512954712, 'validation/accuracy': 0.5805599689483643, 'validation/loss': 1.835086703300476, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.4847159385681152, 'test/num_examples': 10000, 'score': 22726.702834129333, 'total_duration': 25377.72305703163, 'accumulated_submission_time': 22726.702834129333, 'accumulated_eval_time': 2646.743688106537, 'accumulated_logging_time': 1.7032458782196045}
I0203 18:42:33.474016 140023005427456 logging_writer.py:48] [49343] accumulated_eval_time=2646.743688, accumulated_logging_time=1.703246, accumulated_submission_time=22726.702834, global_step=49343, preemption_count=0, score=22726.702834, test/accuracy=0.458900, test/loss=2.484716, test/num_examples=10000, total_duration=25377.723057, train/accuracy=0.623047, train/loss=1.623904, validation/accuracy=0.580560, validation/loss=1.835087, validation/num_examples=50000
I0203 18:42:57.713992 140022518892288 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.2257671356201172, loss=2.719252586364746
I0203 18:43:43.339399 140023005427456 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1668187379837036, loss=2.718975782394409
I0203 18:44:29.603224 140022518892288 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.3489727973937988, loss=2.7850966453552246
I0203 18:45:16.021326 140023005427456 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.1071810722351074, loss=2.736086130142212
I0203 18:46:02.362777 140022518892288 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.3421517610549927, loss=2.512125015258789
I0203 18:46:48.532743 140023005427456 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2627490758895874, loss=2.6680407524108887
I0203 18:47:34.888125 140022518892288 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1647752523422241, loss=2.6201701164245605
I0203 18:48:21.434203 140023005427456 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9012573957443237, loss=4.490702152252197
I0203 18:49:07.695651 140022518892288 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.2362052202224731, loss=2.5849623680114746
I0203 18:49:33.793872 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:49:44.386778 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:50:22.163962 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:50:23.771393 140184451094336 submission_runner.py:408] Time since start: 25848.05s, 	Step: 50258, 	{'train/accuracy': 0.640917956829071, 'train/loss': 1.4934781789779663, 'validation/accuracy': 0.5890600085258484, 'validation/loss': 1.7496922016143799, 'validation/num_examples': 50000, 'test/accuracy': 0.46650001406669617, 'test/loss': 2.422419309616089, 'test/num_examples': 10000, 'score': 23146.965981721878, 'total_duration': 25848.046773910522, 'accumulated_submission_time': 23146.965981721878, 'accumulated_eval_time': 2696.7211923599243, 'accumulated_logging_time': 1.7389581203460693}
I0203 18:50:23.797919 140023005427456 logging_writer.py:48] [50258] accumulated_eval_time=2696.721192, accumulated_logging_time=1.738958, accumulated_submission_time=23146.965982, global_step=50258, preemption_count=0, score=23146.965982, test/accuracy=0.466500, test/loss=2.422419, test/num_examples=10000, total_duration=25848.046774, train/accuracy=0.640918, train/loss=1.493478, validation/accuracy=0.589060, validation/loss=1.749692, validation/num_examples=50000
I0203 18:50:41.763181 140022518892288 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.2322272062301636, loss=2.6582083702087402
I0203 18:51:26.583343 140023005427456 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2499337196350098, loss=2.6153361797332764
I0203 18:52:13.249691 140022518892288 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0983154773712158, loss=2.8388848304748535
I0203 18:52:59.541773 140023005427456 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1733022928237915, loss=2.9372050762176514
I0203 18:53:45.915014 140022518892288 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1064995527267456, loss=2.9827706813812256
I0203 18:54:32.254221 140023005427456 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.2009303569793701, loss=2.709348678588867
I0203 18:55:18.618971 140022518892288 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.3024808168411255, loss=2.6549088954925537
I0203 18:56:05.024211 140023005427456 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0964561700820923, loss=5.277180194854736
I0203 18:56:51.159952 140022518892288 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.2534875869750977, loss=2.585731029510498
I0203 18:57:24.142326 140184451094336 spec.py:321] Evaluating on the training split.
I0203 18:57:34.673371 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 18:58:10.761789 140184451094336 spec.py:349] Evaluating on the test split.
I0203 18:58:12.365661 140184451094336 submission_runner.py:408] Time since start: 26316.64s, 	Step: 51173, 	{'train/accuracy': 0.654101550579071, 'train/loss': 1.4827535152435303, 'validation/accuracy': 0.582260012626648, 'validation/loss': 1.8158226013183594, 'validation/num_examples': 50000, 'test/accuracy': 0.46250003576278687, 'test/loss': 2.461416482925415, 'test/num_examples': 10000, 'score': 23567.253214359283, 'total_duration': 26316.64104104042, 'accumulated_submission_time': 23567.253214359283, 'accumulated_eval_time': 2744.944550037384, 'accumulated_logging_time': 1.7745862007141113}
I0203 18:58:12.393204 140023005427456 logging_writer.py:48] [51173] accumulated_eval_time=2744.944550, accumulated_logging_time=1.774586, accumulated_submission_time=23567.253214, global_step=51173, preemption_count=0, score=23567.253214, test/accuracy=0.462500, test/loss=2.461416, test/num_examples=10000, total_duration=26316.641041, train/accuracy=0.654102, train/loss=1.482754, validation/accuracy=0.582260, validation/loss=1.815823, validation/num_examples=50000
I0203 18:58:24.091858 140022518892288 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0276963710784912, loss=3.31461238861084
I0203 18:59:08.461699 140023005427456 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.1785478591918945, loss=2.616636037826538
I0203 18:59:54.776293 140022518892288 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.5893968343734741, loss=2.605297565460205
I0203 19:00:41.083646 140023005427456 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.1520743370056152, loss=2.610715389251709
I0203 19:01:27.574170 140022518892288 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.9712604284286499, loss=3.5791733264923096
I0203 19:02:14.200819 140023005427456 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.301346778869629, loss=2.7343733310699463
I0203 19:03:00.488801 140022518892288 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.2137367725372314, loss=2.750896453857422
I0203 19:03:46.903742 140023005427456 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2087551355361938, loss=2.495591640472412
I0203 19:04:33.317251 140022518892288 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2412848472595215, loss=2.611342191696167
I0203 19:05:12.403414 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:05:23.177454 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:05:58.519010 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:06:00.137077 140184451094336 submission_runner.py:408] Time since start: 26784.41s, 	Step: 52086, 	{'train/accuracy': 0.6284374594688416, 'train/loss': 1.56432044506073, 'validation/accuracy': 0.5817399621009827, 'validation/loss': 1.7849321365356445, 'validation/num_examples': 50000, 'test/accuracy': 0.4642000198364258, 'test/loss': 2.44661545753479, 'test/num_examples': 10000, 'score': 23987.205542325974, 'total_duration': 26784.412437677383, 'accumulated_submission_time': 23987.205542325974, 'accumulated_eval_time': 2792.6781933307648, 'accumulated_logging_time': 1.8138036727905273}
I0203 19:06:00.169528 140023005427456 logging_writer.py:48] [52086] accumulated_eval_time=2792.678193, accumulated_logging_time=1.813804, accumulated_submission_time=23987.205542, global_step=52086, preemption_count=0, score=23987.205542, test/accuracy=0.464200, test/loss=2.446615, test/num_examples=10000, total_duration=26784.412438, train/accuracy=0.628437, train/loss=1.564320, validation/accuracy=0.581740, validation/loss=1.784932, validation/num_examples=50000
I0203 19:06:06.440834 140022518892288 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.0961207151412964, loss=4.675600051879883
I0203 19:06:49.998244 140023005427456 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.1662321090698242, loss=2.6244897842407227
I0203 19:07:36.405941 140022518892288 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9534484148025513, loss=3.3974685668945312
I0203 19:08:22.717643 140023005427456 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.3969537019729614, loss=2.6231155395507812
I0203 19:09:08.909258 140022518892288 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.109656810760498, loss=2.457285165786743
I0203 19:09:55.315294 140023005427456 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.2312109470367432, loss=2.5557334423065186
I0203 19:10:41.654259 140022518892288 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.2232658863067627, loss=2.62418794631958
I0203 19:11:27.766562 140023005427456 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.0378302335739136, loss=3.4003868103027344
I0203 19:12:13.923451 140022518892288 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1408097743988037, loss=2.61011004447937
I0203 19:13:00.066750 140023005427456 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.2413899898529053, loss=2.622036933898926
I0203 19:13:00.182789 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:13:10.835806 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:13:47.332282 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:13:48.930796 140184451094336 submission_runner.py:408] Time since start: 27253.21s, 	Step: 53002, 	{'train/accuracy': 0.6423242092132568, 'train/loss': 1.4505491256713867, 'validation/accuracy': 0.5980799794197083, 'validation/loss': 1.690264344215393, 'validation/num_examples': 50000, 'test/accuracy': 0.4789000153541565, 'test/loss': 2.357414960861206, 'test/num_examples': 10000, 'score': 24407.16034078598, 'total_duration': 27253.20618200302, 'accumulated_submission_time': 24407.16034078598, 'accumulated_eval_time': 2841.4261870384216, 'accumulated_logging_time': 1.8572685718536377}
I0203 19:13:48.959357 140022518892288 logging_writer.py:48] [53002] accumulated_eval_time=2841.426187, accumulated_logging_time=1.857269, accumulated_submission_time=24407.160341, global_step=53002, preemption_count=0, score=24407.160341, test/accuracy=0.478900, test/loss=2.357415, test/num_examples=10000, total_duration=27253.206182, train/accuracy=0.642324, train/loss=1.450549, validation/accuracy=0.598080, validation/loss=1.690264, validation/num_examples=50000
I0203 19:14:31.397846 140023005427456 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.206052541732788, loss=2.514319658279419
I0203 19:15:17.319108 140022518892288 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2618443965911865, loss=2.7207722663879395
I0203 19:16:03.920248 140023005427456 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9796953797340393, loss=5.303234100341797
I0203 19:16:50.249995 140022518892288 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1327046155929565, loss=2.691582679748535
I0203 19:17:36.708146 140023005427456 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0036394596099854, loss=3.249281883239746
I0203 19:18:23.165980 140022518892288 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9301646947860718, loss=4.533670902252197
I0203 19:19:09.573293 140023005427456 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.2622764110565186, loss=2.478639602661133
I0203 19:19:56.055644 140022518892288 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.159766435623169, loss=2.5602922439575195
I0203 19:20:42.609065 140023005427456 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.1131725311279297, loss=2.776264190673828
I0203 19:20:49.209299 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:21:00.192457 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:21:38.623756 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:21:40.230549 140184451094336 submission_runner.py:408] Time since start: 27724.51s, 	Step: 53916, 	{'train/accuracy': 0.6451367139816284, 'train/loss': 1.5169159173965454, 'validation/accuracy': 0.583579957485199, 'validation/loss': 1.813071370124817, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.465859889984131, 'test/num_examples': 10000, 'score': 24827.353207588196, 'total_duration': 27724.505935430527, 'accumulated_submission_time': 24827.353207588196, 'accumulated_eval_time': 2892.447431087494, 'accumulated_logging_time': 1.8953602313995361}
I0203 19:21:40.258322 140022518892288 logging_writer.py:48] [53916] accumulated_eval_time=2892.447431, accumulated_logging_time=1.895360, accumulated_submission_time=24827.353208, global_step=53916, preemption_count=0, score=24827.353208, test/accuracy=0.464800, test/loss=2.465860, test/num_examples=10000, total_duration=27724.505935, train/accuracy=0.645137, train/loss=1.516916, validation/accuracy=0.583580, validation/loss=1.813071, validation/num_examples=50000
I0203 19:22:16.413005 140023005427456 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0877494812011719, loss=4.771155834197998
I0203 19:23:02.336479 140022518892288 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9428339600563049, loss=4.81313419342041
I0203 19:23:48.630527 140023005427456 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1604527235031128, loss=2.661470890045166
I0203 19:24:34.944798 140022518892288 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9103007316589355, loss=5.198497772216797
I0203 19:25:21.031143 140023005427456 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2912598848342896, loss=2.647892951965332
I0203 19:26:07.555046 140022518892288 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2481448650360107, loss=2.606299638748169
I0203 19:26:53.650465 140023005427456 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.3562618494033813, loss=2.4408226013183594
I0203 19:27:39.970025 140022518892288 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.0672388076782227, loss=3.772864818572998
I0203 19:28:26.213039 140023005427456 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9659181833267212, loss=5.324734687805176
I0203 19:28:40.236563 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:28:51.148190 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:29:27.187579 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:29:28.806129 140184451094336 submission_runner.py:408] Time since start: 28193.08s, 	Step: 54832, 	{'train/accuracy': 0.6351171731948853, 'train/loss': 1.4957752227783203, 'validation/accuracy': 0.5937199592590332, 'validation/loss': 1.7042373418807983, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.388838529586792, 'test/num_examples': 10000, 'score': 25247.272602796555, 'total_duration': 28193.08148908615, 'accumulated_submission_time': 25247.272602796555, 'accumulated_eval_time': 2941.016970396042, 'accumulated_logging_time': 1.9336466789245605}
I0203 19:29:28.832936 140022518892288 logging_writer.py:48] [54832] accumulated_eval_time=2941.016970, accumulated_logging_time=1.933647, accumulated_submission_time=25247.272603, global_step=54832, preemption_count=0, score=25247.272603, test/accuracy=0.472200, test/loss=2.388839, test/num_examples=10000, total_duration=28193.081489, train/accuracy=0.635117, train/loss=1.495775, validation/accuracy=0.593720, validation/loss=1.704237, validation/num_examples=50000
I0203 19:29:57.699279 140023005427456 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.197270154953003, loss=2.676517963409424
I0203 19:30:43.936308 140022518892288 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2111899852752686, loss=2.5266220569610596
I0203 19:31:30.699867 140023005427456 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0417360067367554, loss=3.567720890045166
I0203 19:32:17.502035 140022518892288 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.2325408458709717, loss=3.4108641147613525
I0203 19:33:04.431646 140023005427456 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.04706871509552, loss=5.195743560791016
I0203 19:33:50.910131 140022518892288 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.1952452659606934, loss=2.6339118480682373
I0203 19:34:37.780670 140023005427456 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1747870445251465, loss=3.3645546436309814
I0203 19:35:24.342699 140022518892288 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.26878821849823, loss=2.5530903339385986
I0203 19:36:10.841772 140023005427456 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0821892023086548, loss=2.9218761920928955
I0203 19:36:28.826582 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:36:39.615899 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:37:17.694183 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:37:19.297570 140184451094336 submission_runner.py:408] Time since start: 28663.57s, 	Step: 55740, 	{'train/accuracy': 0.64501953125, 'train/loss': 1.4755982160568237, 'validation/accuracy': 0.5958200097084045, 'validation/loss': 1.7111175060272217, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.404808759689331, 'test/num_examples': 10000, 'score': 25667.20871949196, 'total_duration': 28663.57295012474, 'accumulated_submission_time': 25667.20871949196, 'accumulated_eval_time': 2991.4879484176636, 'accumulated_logging_time': 1.9717350006103516}
I0203 19:37:19.321154 140022518892288 logging_writer.py:48] [55740] accumulated_eval_time=2991.487948, accumulated_logging_time=1.971735, accumulated_submission_time=25667.208719, global_step=55740, preemption_count=0, score=25667.208719, test/accuracy=0.468000, test/loss=2.404809, test/num_examples=10000, total_duration=28663.572950, train/accuracy=0.645020, train/loss=1.475598, validation/accuracy=0.595820, validation/loss=1.711118, validation/num_examples=50000
I0203 19:37:44.794516 140023005427456 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.9684585928916931, loss=4.919435501098633
I0203 19:38:30.398561 140022518892288 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.234299898147583, loss=2.588083505630493
I0203 19:39:17.185858 140023005427456 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9702489972114563, loss=4.548047065734863
I0203 19:40:03.752901 140022518892288 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.2370564937591553, loss=2.674567699432373
I0203 19:40:50.473036 140023005427456 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.2224456071853638, loss=2.4132039546966553
I0203 19:41:37.025585 140022518892288 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.1981432437896729, loss=2.628150463104248
I0203 19:42:23.675845 140023005427456 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2090657949447632, loss=2.5535027980804443
I0203 19:43:10.067500 140022518892288 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9807839393615723, loss=5.00323486328125
I0203 19:43:56.613742 140023005427456 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.1874055862426758, loss=3.0682201385498047
I0203 19:44:19.657987 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:44:30.268595 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:45:06.768922 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:45:08.375603 140184451094336 submission_runner.py:408] Time since start: 29132.65s, 	Step: 56651, 	{'train/accuracy': 0.6513866782188416, 'train/loss': 1.4567426443099976, 'validation/accuracy': 0.590999960899353, 'validation/loss': 1.7368868589401245, 'validation/num_examples': 50000, 'test/accuracy': 0.4706000089645386, 'test/loss': 2.3935937881469727, 'test/num_examples': 10000, 'score': 26087.488626241684, 'total_duration': 29132.650985717773, 'accumulated_submission_time': 26087.488626241684, 'accumulated_eval_time': 3040.205552339554, 'accumulated_logging_time': 2.005126714706421}
I0203 19:45:08.402991 140022518892288 logging_writer.py:48] [56651] accumulated_eval_time=3040.205552, accumulated_logging_time=2.005127, accumulated_submission_time=26087.488626, global_step=56651, preemption_count=0, score=26087.488626, test/accuracy=0.470600, test/loss=2.393594, test/num_examples=10000, total_duration=29132.650986, train/accuracy=0.651387, train/loss=1.456743, validation/accuracy=0.591000, validation/loss=1.736887, validation/num_examples=50000
I0203 19:45:29.289753 140023005427456 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2111183404922485, loss=2.564131736755371
I0203 19:46:15.018171 140022518892288 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.0571200847625732, loss=2.914640188217163
I0203 19:47:01.558767 140023005427456 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.1106256246566772, loss=4.189543724060059
I0203 19:47:48.209841 140022518892288 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1244133710861206, loss=4.167304992675781
I0203 19:48:34.910113 140023005427456 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.1034549474716187, loss=3.1302289962768555
I0203 19:49:21.296284 140022518892288 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9600939750671387, loss=4.828814506530762
I0203 19:50:08.007647 140023005427456 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1548352241516113, loss=3.2066919803619385
I0203 19:50:54.559989 140022518892288 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2061232328414917, loss=2.8359756469726562
I0203 19:51:41.169260 140023005427456 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1799577474594116, loss=2.9196887016296387
I0203 19:52:08.430742 140184451094336 spec.py:321] Evaluating on the training split.
I0203 19:52:18.895336 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 19:52:56.523407 140184451094336 spec.py:349] Evaluating on the test split.
I0203 19:52:58.134360 140184451094336 submission_runner.py:408] Time since start: 29602.41s, 	Step: 57560, 	{'train/accuracy': 0.6385351419448853, 'train/loss': 1.554219126701355, 'validation/accuracy': 0.593239963054657, 'validation/loss': 1.763049840927124, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.4218482971191406, 'test/num_examples': 10000, 'score': 26507.458403348923, 'total_duration': 29602.409747600555, 'accumulated_submission_time': 26507.458403348923, 'accumulated_eval_time': 3089.9091703891754, 'accumulated_logging_time': 2.0419747829437256}
I0203 19:52:58.162096 140022518892288 logging_writer.py:48] [57560] accumulated_eval_time=3089.909170, accumulated_logging_time=2.041975, accumulated_submission_time=26507.458403, global_step=57560, preemption_count=0, score=26507.458403, test/accuracy=0.473000, test/loss=2.421848, test/num_examples=10000, total_duration=29602.409748, train/accuracy=0.638535, train/loss=1.554219, validation/accuracy=0.593240, validation/loss=1.763050, validation/num_examples=50000
I0203 19:53:15.290498 140023005427456 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9986616969108582, loss=3.5973405838012695
I0203 19:54:00.032238 140022518892288 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8967729806900024, loss=4.022360801696777
I0203 19:54:46.575271 140023005427456 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.3264626264572144, loss=2.588270902633667
I0203 19:55:33.144016 140022518892288 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.13762366771698, loss=2.6455698013305664
I0203 19:56:19.511964 140023005427456 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9944414496421814, loss=4.129863262176514
I0203 19:57:05.951211 140022518892288 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.209671974182129, loss=2.581076145172119
I0203 19:57:52.500858 140023005427456 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.0146769285202026, loss=4.62747859954834
I0203 19:58:38.788275 140022518892288 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9838463068008423, loss=3.5199248790740967
I0203 19:59:25.089362 140023005427456 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.436827540397644, loss=2.738696575164795
I0203 19:59:58.370307 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:00:08.988415 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:00:47.262859 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:00:48.877374 140184451094336 submission_runner.py:408] Time since start: 30073.15s, 	Step: 58474, 	{'train/accuracy': 0.6503124833106995, 'train/loss': 1.4144947528839111, 'validation/accuracy': 0.6011399626731873, 'validation/loss': 1.6524994373321533, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.323306083679199, 'test/num_examples': 10000, 'score': 26927.61052799225, 'total_duration': 30073.152743577957, 'accumulated_submission_time': 26927.61052799225, 'accumulated_eval_time': 3140.4162259101868, 'accumulated_logging_time': 2.0792157649993896}
I0203 20:00:48.906957 140022518892288 logging_writer.py:48] [58474] accumulated_eval_time=3140.416226, accumulated_logging_time=2.079216, accumulated_submission_time=26927.610528, global_step=58474, preemption_count=0, score=26927.610528, test/accuracy=0.484300, test/loss=2.323306, test/num_examples=10000, total_duration=30073.152744, train/accuracy=0.650312, train/loss=1.414495, validation/accuracy=0.601140, validation/loss=1.652499, validation/num_examples=50000
I0203 20:01:00.181449 140023005427456 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.2078895568847656, loss=2.9729928970336914
I0203 20:01:44.416897 140022518892288 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.958965003490448, loss=4.318291664123535
I0203 20:02:30.865225 140023005427456 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.213989496231079, loss=2.57146954536438
I0203 20:03:17.357122 140022518892288 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9682229161262512, loss=4.076796054840088
I0203 20:04:03.803897 140023005427456 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.1662077903747559, loss=2.56400990486145
I0203 20:04:50.034957 140022518892288 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2723628282546997, loss=2.789301872253418
I0203 20:05:36.334712 140023005427456 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.1046472787857056, loss=2.7086002826690674
I0203 20:06:22.564574 140022518892288 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1983656883239746, loss=2.489253044128418
I0203 20:07:08.961980 140023005427456 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.322350025177002, loss=2.719491958618164
I0203 20:07:49.245318 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:08:00.293786 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:08:36.312705 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:08:37.915705 140184451094336 submission_runner.py:408] Time since start: 30542.19s, 	Step: 59389, 	{'train/accuracy': 0.6558203101158142, 'train/loss': 1.472233533859253, 'validation/accuracy': 0.5989399552345276, 'validation/loss': 1.731550931930542, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.381978988647461, 'test/num_examples': 10000, 'score': 27347.89120697975, 'total_duration': 30542.191057682037, 'accumulated_submission_time': 27347.89120697975, 'accumulated_eval_time': 3189.0866141319275, 'accumulated_logging_time': 2.1189894676208496}
I0203 20:08:37.941181 140022518892288 logging_writer.py:48] [59389] accumulated_eval_time=3189.086614, accumulated_logging_time=2.118989, accumulated_submission_time=27347.891207, global_step=59389, preemption_count=0, score=27347.891207, test/accuracy=0.476100, test/loss=2.381979, test/num_examples=10000, total_duration=30542.191058, train/accuracy=0.655820, train/loss=1.472234, validation/accuracy=0.598940, validation/loss=1.731551, validation/num_examples=50000
I0203 20:08:42.969892 140023005427456 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.0917143821716309, loss=2.6776697635650635
I0203 20:09:26.314074 140022518892288 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1983181238174438, loss=3.103355884552002
I0203 20:10:12.360758 140023005427456 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0383498668670654, loss=3.8474957942962646
I0203 20:10:58.807296 140022518892288 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2506206035614014, loss=2.5096383094787598
I0203 20:11:45.498014 140023005427456 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9530778527259827, loss=5.153644561767578
I0203 20:12:31.958700 140022518892288 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.363966941833496, loss=2.4849483966827393
I0203 20:13:18.171420 140023005427456 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.2610836029052734, loss=2.572326421737671
I0203 20:14:04.696125 140022518892288 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2371468544006348, loss=2.597839117050171
I0203 20:14:51.236100 140023005427456 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.3010753393173218, loss=2.518495559692383
I0203 20:15:38.075843 140022518892288 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.1327018737792969, loss=2.4356398582458496
I0203 20:15:38.091827 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:15:48.824884 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:16:25.985888 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:16:27.589883 140184451094336 submission_runner.py:408] Time since start: 31011.87s, 	Step: 60301, 	{'train/accuracy': 0.64208984375, 'train/loss': 1.5133836269378662, 'validation/accuracy': 0.5927799940109253, 'validation/loss': 1.7447527647018433, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.4132392406463623, 'test/num_examples': 10000, 'score': 27767.984453201294, 'total_duration': 31011.865272283554, 'accumulated_submission_time': 27767.984453201294, 'accumulated_eval_time': 3238.5846648216248, 'accumulated_logging_time': 2.1544106006622314}
I0203 20:16:27.614027 140023005427456 logging_writer.py:48] [60301] accumulated_eval_time=3238.584665, accumulated_logging_time=2.154411, accumulated_submission_time=27767.984453, global_step=60301, preemption_count=0, score=27767.984453, test/accuracy=0.471600, test/loss=2.413239, test/num_examples=10000, total_duration=31011.865272, train/accuracy=0.642090, train/loss=1.513384, validation/accuracy=0.592780, validation/loss=1.744753, validation/num_examples=50000
I0203 20:17:10.595021 140022518892288 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.2143272161483765, loss=2.404078960418701
I0203 20:17:56.639664 140023005427456 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9935735464096069, loss=4.939925193786621
I0203 20:18:43.418514 140022518892288 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.0908641815185547, loss=3.051736354827881
I0203 20:19:29.748676 140023005427456 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.1599208116531372, loss=2.6011922359466553
I0203 20:20:16.171601 140022518892288 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.1554473638534546, loss=2.5913376808166504
I0203 20:21:02.426707 140023005427456 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2809243202209473, loss=2.878359794616699
I0203 20:21:49.013308 140022518892288 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.217969298362732, loss=2.5603532791137695
I0203 20:22:35.361314 140023005427456 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0629178285598755, loss=5.065753936767578
I0203 20:23:21.501603 140022518892288 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.2186708450317383, loss=2.5887956619262695
I0203 20:23:27.725989 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:23:38.389119 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:24:16.802582 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:24:18.410012 140184451094336 submission_runner.py:408] Time since start: 31482.69s, 	Step: 61215, 	{'train/accuracy': 0.6486132740974426, 'train/loss': 1.5043102502822876, 'validation/accuracy': 0.6002399921417236, 'validation/loss': 1.7300899028778076, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.3871309757232666, 'test/num_examples': 10000, 'score': 28188.03951358795, 'total_duration': 31482.68537425995, 'accumulated_submission_time': 28188.03951358795, 'accumulated_eval_time': 3289.2686598300934, 'accumulated_logging_time': 2.187922954559326}
I0203 20:24:18.441667 140023005427456 logging_writer.py:48] [61215] accumulated_eval_time=3289.268660, accumulated_logging_time=2.187923, accumulated_submission_time=28188.039514, global_step=61215, preemption_count=0, score=28188.039514, test/accuracy=0.477100, test/loss=2.387131, test/num_examples=10000, total_duration=31482.685374, train/accuracy=0.648613, train/loss=1.504310, validation/accuracy=0.600240, validation/loss=1.730090, validation/num_examples=50000
I0203 20:24:55.131057 140022518892288 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.3199331760406494, loss=2.9800710678100586
I0203 20:25:41.201084 140023005427456 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.0529903173446655, loss=2.826957941055298
I0203 20:26:28.118320 140022518892288 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9722105860710144, loss=4.995754718780518
I0203 20:27:14.778126 140023005427456 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.3243885040283203, loss=2.5435659885406494
I0203 20:28:01.048227 140022518892288 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.163460612297058, loss=3.0437824726104736
I0203 20:28:47.670785 140023005427456 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.23727285861969, loss=2.6630449295043945
I0203 20:29:34.093255 140022518892288 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0786106586456299, loss=3.0073442459106445
I0203 20:30:20.535714 140023005427456 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2697011232376099, loss=2.6379261016845703
I0203 20:31:07.192326 140022518892288 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0658926963806152, loss=4.458230018615723
I0203 20:31:18.437712 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:31:29.148947 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:32:07.041470 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:32:08.644400 140184451094336 submission_runner.py:408] Time since start: 31952.92s, 	Step: 62126, 	{'train/accuracy': 0.6597460508346558, 'train/loss': 1.4151033163070679, 'validation/accuracy': 0.6001200079917908, 'validation/loss': 1.6884820461273193, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.353926658630371, 'test/num_examples': 10000, 'score': 28607.97818994522, 'total_duration': 31952.919785261154, 'accumulated_submission_time': 28607.97818994522, 'accumulated_eval_time': 3339.475342273712, 'accumulated_logging_time': 2.2301175594329834}
I0203 20:32:08.669811 140023005427456 logging_writer.py:48] [62126] accumulated_eval_time=3339.475342, accumulated_logging_time=2.230118, accumulated_submission_time=28607.978190, global_step=62126, preemption_count=0, score=28607.978190, test/accuracy=0.486900, test/loss=2.353927, test/num_examples=10000, total_duration=31952.919785, train/accuracy=0.659746, train/loss=1.415103, validation/accuracy=0.600120, validation/loss=1.688482, validation/num_examples=50000
I0203 20:32:40.210547 140022518892288 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.9739974141120911, loss=3.822681427001953
I0203 20:33:26.463570 140023005427456 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.1158885955810547, loss=3.0387167930603027
I0203 20:34:12.939090 140022518892288 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.9949834942817688, loss=5.06868314743042
I0203 20:34:59.432912 140023005427456 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2174216508865356, loss=2.4618191719055176
I0203 20:35:45.892592 140022518892288 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.258785367012024, loss=2.898883581161499
I0203 20:36:32.209877 140023005427456 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9686679244041443, loss=3.7562966346740723
I0203 20:37:18.632842 140022518892288 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1215578317642212, loss=2.7958614826202393
I0203 20:38:04.903889 140023005427456 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.2667021751403809, loss=2.468942165374756
I0203 20:38:51.301697 140022518892288 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0302191972732544, loss=4.751604080200195
I0203 20:39:09.005300 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:39:19.739538 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:39:55.144338 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:39:56.810540 140184451094336 submission_runner.py:408] Time since start: 32421.09s, 	Step: 63040, 	{'train/accuracy': 0.6842187643051147, 'train/loss': 1.2707115411758423, 'validation/accuracy': 0.6073399782180786, 'validation/loss': 1.6327868700027466, 'validation/num_examples': 50000, 'test/accuracy': 0.4918000102043152, 'test/loss': 2.2991316318511963, 'test/num_examples': 10000, 'score': 29028.256913661957, 'total_duration': 32421.085930347443, 'accumulated_submission_time': 29028.256913661957, 'accumulated_eval_time': 3387.2805788517, 'accumulated_logging_time': 2.26474666595459}
I0203 20:39:56.837047 140023005427456 logging_writer.py:48] [63040] accumulated_eval_time=3387.280579, accumulated_logging_time=2.264747, accumulated_submission_time=29028.256914, global_step=63040, preemption_count=0, score=29028.256914, test/accuracy=0.491800, test/loss=2.299132, test/num_examples=10000, total_duration=32421.085930, train/accuracy=0.684219, train/loss=1.270712, validation/accuracy=0.607340, validation/loss=1.632787, validation/num_examples=50000
I0203 20:40:22.324422 140022518892288 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.2990423440933228, loss=2.560504674911499
I0203 20:41:07.991476 140023005427456 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0310002565383911, loss=4.315956115722656
I0203 20:41:54.599377 140022518892288 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1415820121765137, loss=2.6063358783721924
I0203 20:42:40.920011 140023005427456 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3032183647155762, loss=2.5858278274536133
I0203 20:43:27.283224 140022518892288 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.201767086982727, loss=2.461815357208252
I0203 20:44:13.593249 140023005427456 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2248519659042358, loss=2.4046859741210938
I0203 20:44:59.846680 140022518892288 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1752369403839111, loss=2.515244483947754
I0203 20:45:46.664433 140023005427456 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.1312907934188843, loss=2.7965457439422607
I0203 20:46:33.204860 140022518892288 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.2698173522949219, loss=2.5275917053222656
I0203 20:46:57.181125 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:47:07.781374 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:47:45.256255 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:47:46.849510 140184451094336 submission_runner.py:408] Time since start: 32891.12s, 	Step: 63953, 	{'train/accuracy': 0.6486914157867432, 'train/loss': 1.453892469406128, 'validation/accuracy': 0.6010400056838989, 'validation/loss': 1.6871942281723022, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.333249092102051, 'test/num_examples': 10000, 'score': 29448.54456448555, 'total_duration': 32891.12489771843, 'accumulated_submission_time': 29448.54456448555, 'accumulated_eval_time': 3436.948952436447, 'accumulated_logging_time': 2.3011348247528076}
I0203 20:47:46.879610 140023005427456 logging_writer.py:48] [63953] accumulated_eval_time=3436.948952, accumulated_logging_time=2.301135, accumulated_submission_time=29448.544564, global_step=63953, preemption_count=0, score=29448.544564, test/accuracy=0.483000, test/loss=2.333249, test/num_examples=10000, total_duration=32891.124898, train/accuracy=0.648691, train/loss=1.453892, validation/accuracy=0.601040, validation/loss=1.687194, validation/num_examples=50000
I0203 20:48:07.096392 140022518892288 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.2934356927871704, loss=2.4785943031311035
I0203 20:48:51.844998 140023005427456 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0343443155288696, loss=3.5824546813964844
I0203 20:49:38.382355 140022518892288 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0449131727218628, loss=4.046964645385742
I0203 20:50:24.888145 140023005427456 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1271157264709473, loss=3.188404083251953
I0203 20:51:11.224097 140022518892288 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1990681886672974, loss=2.5950071811676025
I0203 20:51:57.731148 140023005427456 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3184809684753418, loss=2.465139389038086
I0203 20:52:44.239192 140022518892288 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0730063915252686, loss=4.4259138107299805
I0203 20:53:30.500929 140023005427456 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1116079092025757, loss=3.5291378498077393
I0203 20:54:16.773057 140022518892288 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.169981598854065, loss=2.745154857635498
I0203 20:54:46.961460 140184451094336 spec.py:321] Evaluating on the training split.
I0203 20:54:57.724145 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 20:55:36.692475 140184451094336 spec.py:349] Evaluating on the test split.
I0203 20:55:38.308173 140184451094336 submission_runner.py:408] Time since start: 33362.58s, 	Step: 64867, 	{'train/accuracy': 0.658203125, 'train/loss': 1.4299410581588745, 'validation/accuracy': 0.6074599623680115, 'validation/loss': 1.6725072860717773, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.3327584266662598, 'test/num_examples': 10000, 'score': 29868.568327188492, 'total_duration': 33362.58354306221, 'accumulated_submission_time': 29868.568327188492, 'accumulated_eval_time': 3488.2956540584564, 'accumulated_logging_time': 2.3419806957244873}
I0203 20:55:38.342981 140023005427456 logging_writer.py:48] [64867] accumulated_eval_time=3488.295654, accumulated_logging_time=2.341981, accumulated_submission_time=29868.568327, global_step=64867, preemption_count=0, score=29868.568327, test/accuracy=0.488900, test/loss=2.332758, test/num_examples=10000, total_duration=33362.583543, train/accuracy=0.658203, train/loss=1.429941, validation/accuracy=0.607460, validation/loss=1.672507, validation/num_examples=50000
I0203 20:55:52.547032 140022518892288 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2993701696395874, loss=2.5185177326202393
I0203 20:56:37.171524 140023005427456 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3468798398971558, loss=2.5116405487060547
I0203 20:57:23.788573 140022518892288 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.1236178874969482, loss=3.311737060546875
I0203 20:58:10.273851 140023005427456 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.5932658910751343, loss=2.3453054428100586
I0203 20:58:56.524201 140022518892288 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.031837821006775, loss=3.5013651847839355
I0203 20:59:42.769477 140023005427456 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.185187816619873, loss=2.6046109199523926
I0203 21:00:29.283358 140022518892288 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.053048849105835, loss=4.258884429931641
I0203 21:01:15.786369 140023005427456 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2288119792938232, loss=2.642146587371826
I0203 21:02:02.252810 140022518892288 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.3741264343261719, loss=2.596787452697754
I0203 21:02:38.486477 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:02:49.178177 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:03:26.388937 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:03:27.989629 140184451094336 submission_runner.py:408] Time since start: 33832.27s, 	Step: 65780, 	{'train/accuracy': 0.6772069931030273, 'train/loss': 1.291306734085083, 'validation/accuracy': 0.6087999939918518, 'validation/loss': 1.6291512250900269, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.2988369464874268, 'test/num_examples': 10000, 'score': 30288.65084552765, 'total_duration': 33832.26501727104, 'accumulated_submission_time': 30288.65084552765, 'accumulated_eval_time': 3537.7987973690033, 'accumulated_logging_time': 2.3904261589050293}
I0203 21:03:28.015932 140023005427456 logging_writer.py:48] [65780] accumulated_eval_time=3537.798797, accumulated_logging_time=2.390426, accumulated_submission_time=30288.650846, global_step=65780, preemption_count=0, score=30288.650846, test/accuracy=0.487400, test/loss=2.298837, test/num_examples=10000, total_duration=33832.265017, train/accuracy=0.677207, train/loss=1.291307, validation/accuracy=0.608800, validation/loss=1.629151, validation/num_examples=50000
I0203 21:03:36.786596 140022518892288 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.3510695695877075, loss=2.560941457748413
I0203 21:04:20.789967 140023005427456 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0989042520523071, loss=2.8773934841156006
I0203 21:05:06.937296 140022518892288 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.101029396057129, loss=3.2144224643707275
I0203 21:05:53.242527 140023005427456 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.154263973236084, loss=5.097346305847168
I0203 21:06:39.609784 140022518892288 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.9688742756843567, loss=4.5711469650268555
I0203 21:07:26.068928 140023005427456 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2842835187911987, loss=2.8125081062316895
I0203 21:08:12.492655 140022518892288 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0935497283935547, loss=4.180183410644531
I0203 21:08:58.758053 140023005427456 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.091189980506897, loss=4.4199934005737305
I0203 21:09:45.187305 140022518892288 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.2725023031234741, loss=2.5233404636383057
I0203 21:10:28.403594 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:10:39.237491 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:11:15.464297 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:11:17.060551 140184451094336 submission_runner.py:408] Time since start: 34301.34s, 	Step: 66695, 	{'train/accuracy': 0.6553320288658142, 'train/loss': 1.4468097686767578, 'validation/accuracy': 0.6100599765777588, 'validation/loss': 1.668556571006775, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.307793378829956, 'test/num_examples': 10000, 'score': 30708.979912281036, 'total_duration': 34301.335938215256, 'accumulated_submission_time': 30708.979912281036, 'accumulated_eval_time': 3586.4557435512543, 'accumulated_logging_time': 2.428715467453003}
I0203 21:11:17.089004 140023005427456 logging_writer.py:48] [66695] accumulated_eval_time=3586.455744, accumulated_logging_time=2.428715, accumulated_submission_time=30708.979912, global_step=66695, preemption_count=0, score=30708.979912, test/accuracy=0.495800, test/loss=2.307793, test/num_examples=10000, total_duration=34301.335938, train/accuracy=0.655332, train/loss=1.446810, validation/accuracy=0.610060, validation/loss=1.668557, validation/num_examples=50000
I0203 21:11:19.604712 140022518892288 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.1494227647781372, loss=2.9106626510620117
I0203 21:12:03.047667 140023005427456 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.2275817394256592, loss=3.026315450668335
I0203 21:12:49.102768 140022518892288 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.9842337965965271, loss=4.557669162750244
I0203 21:13:35.379858 140023005427456 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2150601148605347, loss=2.647667169570923
I0203 21:14:21.586399 140022518892288 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.0234607458114624, loss=3.68229079246521
I0203 21:15:07.797129 140023005427456 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.2453184127807617, loss=3.091264486312866
I0203 21:15:53.775468 140022518892288 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.076243281364441, loss=4.723422050476074
I0203 21:16:40.125912 140023005427456 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.1098090410232544, loss=4.338197231292725
I0203 21:17:26.690229 140022518892288 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3190122842788696, loss=2.455256700515747
I0203 21:18:13.325812 140023005427456 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.084675669670105, loss=3.202317237854004
I0203 21:18:17.158783 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:18:28.014797 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:19:04.392524 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:19:05.992504 140184451094336 submission_runner.py:408] Time since start: 34770.27s, 	Step: 67610, 	{'train/accuracy': 0.6596874594688416, 'train/loss': 1.3802225589752197, 'validation/accuracy': 0.6093400120735168, 'validation/loss': 1.6242049932479858, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3031609058380127, 'test/num_examples': 10000, 'score': 31128.98797106743, 'total_duration': 34770.267876148224, 'accumulated_submission_time': 31128.98797106743, 'accumulated_eval_time': 3635.289438724518, 'accumulated_logging_time': 2.4718663692474365}
I0203 21:19:06.025497 140022518892288 logging_writer.py:48] [67610] accumulated_eval_time=3635.289439, accumulated_logging_time=2.471866, accumulated_submission_time=31128.987971, global_step=67610, preemption_count=0, score=31128.987971, test/accuracy=0.486700, test/loss=2.303161, test/num_examples=10000, total_duration=34770.267876, train/accuracy=0.659687, train/loss=1.380223, validation/accuracy=0.609340, validation/loss=1.624205, validation/num_examples=50000
I0203 21:19:44.770523 140023005427456 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.2437025308609009, loss=2.4015371799468994
I0203 21:20:30.901073 140022518892288 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.300479531288147, loss=2.5242345333099365
I0203 21:21:17.770638 140023005427456 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2554078102111816, loss=4.875873565673828
I0203 21:22:04.194769 140022518892288 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.2572450637817383, loss=2.4577934741973877
I0203 21:22:50.296668 140023005427456 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2386382818222046, loss=2.5895283222198486
I0203 21:23:36.601580 140022518892288 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3714553117752075, loss=2.7948825359344482
I0203 21:24:22.911344 140023005427456 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3084148168563843, loss=2.5957512855529785
I0203 21:25:09.168827 140022518892288 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3513288497924805, loss=2.541780710220337
I0203 21:25:55.673775 140023005427456 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.321192741394043, loss=2.7106785774230957
I0203 21:26:06.074843 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:26:16.765690 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:26:52.677505 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:26:54.274739 140184451094336 submission_runner.py:408] Time since start: 35238.55s, 	Step: 68524, 	{'train/accuracy': 0.6707226634025574, 'train/loss': 1.330780267715454, 'validation/accuracy': 0.6122999787330627, 'validation/loss': 1.6297461986541748, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.2985339164733887, 'test/num_examples': 10000, 'score': 31548.975848913193, 'total_duration': 35238.550125837326, 'accumulated_submission_time': 31548.975848913193, 'accumulated_eval_time': 3683.4893350601196, 'accumulated_logging_time': 2.515212059020996}
I0203 21:26:54.303320 140022518892288 logging_writer.py:48] [68524] accumulated_eval_time=3683.489335, accumulated_logging_time=2.515212, accumulated_submission_time=31548.975849, global_step=68524, preemption_count=0, score=31548.975849, test/accuracy=0.493900, test/loss=2.298534, test/num_examples=10000, total_duration=35238.550126, train/accuracy=0.670723, train/loss=1.330780, validation/accuracy=0.612300, validation/loss=1.629746, validation/num_examples=50000
I0203 21:27:26.900800 140023005427456 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.1605849266052246, loss=3.204810380935669
I0203 21:28:13.050060 140022518892288 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.3029732704162598, loss=2.4597115516662598
I0203 21:28:59.624633 140023005427456 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.3189451694488525, loss=2.5110788345336914
I0203 21:29:45.799005 140022518892288 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.1240015029907227, loss=2.452031373977661
I0203 21:30:32.241314 140023005427456 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.101072907447815, loss=4.172994613647461
I0203 21:31:19.016636 140022518892288 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3550441265106201, loss=2.6099324226379395
I0203 21:32:05.686468 140023005427456 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.2615858316421509, loss=2.8551979064941406
I0203 21:32:52.014469 140022518892288 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.9760683178901672, loss=3.935087203979492
I0203 21:33:38.816125 140023005427456 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.2741906642913818, loss=2.5095138549804688
I0203 21:33:54.280542 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:34:05.225933 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:34:40.930287 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:34:42.525020 140184451094336 submission_runner.py:408] Time since start: 35706.80s, 	Step: 69435, 	{'train/accuracy': 0.6629882454872131, 'train/loss': 1.3963218927383423, 'validation/accuracy': 0.6138799786567688, 'validation/loss': 1.6306712627410889, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.291445732116699, 'test/num_examples': 10000, 'score': 31968.895731449127, 'total_duration': 35706.80040049553, 'accumulated_submission_time': 31968.895731449127, 'accumulated_eval_time': 3731.7338008880615, 'accumulated_logging_time': 2.553818941116333}
I0203 21:34:42.551003 140022518892288 logging_writer.py:48] [69435] accumulated_eval_time=3731.733801, accumulated_logging_time=2.553819, accumulated_submission_time=31968.895731, global_step=69435, preemption_count=0, score=31968.895731, test/accuracy=0.490500, test/loss=2.291446, test/num_examples=10000, total_duration=35706.800400, train/accuracy=0.662988, train/loss=1.396322, validation/accuracy=0.613880, validation/loss=1.630671, validation/num_examples=50000
I0203 21:35:10.127050 140023005427456 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.3787339925765991, loss=2.644131660461426
I0203 21:35:56.025106 140022518892288 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1731454133987427, loss=4.1318511962890625
I0203 21:36:42.285827 140023005427456 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2663134336471558, loss=2.5826351642608643
I0203 21:37:28.543217 140022518892288 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.4341685771942139, loss=2.5647196769714355
I0203 21:38:14.791158 140023005427456 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.078688621520996, loss=2.716303586959839
I0203 21:39:01.302799 140022518892288 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.185439944267273, loss=2.9663331508636475
I0203 21:39:47.614837 140023005427456 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.2760406732559204, loss=2.617788314819336
I0203 21:40:33.999887 140022518892288 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3430721759796143, loss=2.726247787475586
I0203 21:41:20.502425 140023005427456 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1419575214385986, loss=2.9297242164611816
I0203 21:41:42.931101 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:41:53.603401 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:42:31.013763 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:42:32.626400 140184451094336 submission_runner.py:408] Time since start: 36176.90s, 	Step: 70350, 	{'train/accuracy': 0.66259765625, 'train/loss': 1.3925042152404785, 'validation/accuracy': 0.6094799637794495, 'validation/loss': 1.6426547765731812, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.2989656925201416, 'test/num_examples': 10000, 'score': 32389.219320058823, 'total_duration': 36176.90175771713, 'accumulated_submission_time': 32389.219320058823, 'accumulated_eval_time': 3781.4290795326233, 'accumulated_logging_time': 2.589449882507324}
I0203 21:42:32.655546 140022518892288 logging_writer.py:48] [70350] accumulated_eval_time=3781.429080, accumulated_logging_time=2.589450, accumulated_submission_time=32389.219320, global_step=70350, preemption_count=0, score=32389.219320, test/accuracy=0.491000, test/loss=2.298966, test/num_examples=10000, total_duration=36176.901758, train/accuracy=0.662598, train/loss=1.392504, validation/accuracy=0.609480, validation/loss=1.642655, validation/num_examples=50000
I0203 21:42:53.964852 140023005427456 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0550185441970825, loss=3.917436361312866
I0203 21:43:38.977095 140022518892288 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.249646544456482, loss=2.577247142791748
I0203 21:44:25.344133 140023005427456 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.167174220085144, loss=3.382697343826294
I0203 21:45:11.780781 140022518892288 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2906403541564941, loss=2.454042911529541
I0203 21:45:57.999840 140023005427456 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2268328666687012, loss=2.493830919265747
I0203 21:46:44.337294 140022518892288 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.4408514499664307, loss=2.4533584117889404
I0203 21:47:30.695617 140023005427456 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3843638896942139, loss=2.4720869064331055
I0203 21:48:17.201934 140022518892288 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.389923095703125, loss=2.410025119781494
I0203 21:49:03.763065 140023005427456 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.2902419567108154, loss=3.5156946182250977
I0203 21:49:32.862653 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:49:43.433056 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:50:20.807966 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:50:22.431990 140184451094336 submission_runner.py:408] Time since start: 36646.71s, 	Step: 71265, 	{'train/accuracy': 0.6683593392372131, 'train/loss': 1.397711157798767, 'validation/accuracy': 0.6119399666786194, 'validation/loss': 1.6650700569152832, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.3102755546569824, 'test/num_examples': 10000, 'score': 32809.368601322174, 'total_duration': 36646.70735788345, 'accumulated_submission_time': 32809.368601322174, 'accumulated_eval_time': 3830.9983818531036, 'accumulated_logging_time': 2.6295628547668457}
I0203 21:50:22.463564 140022518892288 logging_writer.py:48] [71265] accumulated_eval_time=3830.998382, accumulated_logging_time=2.629563, accumulated_submission_time=32809.368601, global_step=71265, preemption_count=0, score=32809.368601, test/accuracy=0.486400, test/loss=2.310276, test/num_examples=10000, total_duration=36646.707358, train/accuracy=0.668359, train/loss=1.397711, validation/accuracy=0.611940, validation/loss=1.665070, validation/num_examples=50000
I0203 21:50:37.509695 140023005427456 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.430937647819519, loss=2.5384292602539062
I0203 21:51:22.199767 140022518892288 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.4022560119628906, loss=2.5061724185943604
I0203 21:52:08.772035 140023005427456 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.158139944076538, loss=3.1878814697265625
I0203 21:52:55.131082 140022518892288 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3588452339172363, loss=2.5031509399414062
I0203 21:53:41.287867 140023005427456 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3207517862319946, loss=2.4853672981262207
I0203 21:54:27.644974 140022518892288 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0578566789627075, loss=3.1138241291046143
I0203 21:55:13.886299 140023005427456 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3993927240371704, loss=2.7329423427581787
I0203 21:56:00.031494 140022518892288 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.002352237701416, loss=4.579397678375244
I0203 21:56:46.650737 140023005427456 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.4099596738815308, loss=2.5085699558258057
I0203 21:57:22.568036 140184451094336 spec.py:321] Evaluating on the training split.
I0203 21:57:33.659570 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 21:58:10.759100 140184451094336 spec.py:349] Evaluating on the test split.
I0203 21:58:12.357571 140184451094336 submission_runner.py:408] Time since start: 37116.63s, 	Step: 72179, 	{'train/accuracy': 0.6663671731948853, 'train/loss': 1.3786489963531494, 'validation/accuracy': 0.6188600063323975, 'validation/loss': 1.6109440326690674, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.268369674682617, 'test/num_examples': 10000, 'score': 33229.415531635284, 'total_duration': 37116.63294029236, 'accumulated_submission_time': 33229.415531635284, 'accumulated_eval_time': 3880.7879054546356, 'accumulated_logging_time': 2.671271324157715}
I0203 21:58:12.391223 140022518892288 logging_writer.py:48] [72179] accumulated_eval_time=3880.787905, accumulated_logging_time=2.671271, accumulated_submission_time=33229.415532, global_step=72179, preemption_count=0, score=33229.415532, test/accuracy=0.498100, test/loss=2.268370, test/num_examples=10000, total_duration=37116.632940, train/accuracy=0.666367, train/loss=1.378649, validation/accuracy=0.618860, validation/loss=1.610944, validation/num_examples=50000
I0203 21:58:21.583500 140023005427456 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2848246097564697, loss=2.4885659217834473
I0203 21:59:05.737403 140022518892288 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.099066138267517, loss=3.261824131011963
I0203 21:59:51.951709 140023005427456 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.1743026971817017, loss=3.2654800415039062
I0203 22:00:38.499387 140022518892288 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.1788569688796997, loss=4.122838497161865
I0203 22:01:24.762575 140023005427456 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.296858787536621, loss=2.5401763916015625
I0203 22:02:11.301097 140022518892288 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.2341301441192627, loss=3.048081398010254
I0203 22:02:57.338113 140023005427456 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.276578664779663, loss=2.274317741394043
I0203 22:03:43.769252 140022518892288 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.2297732830047607, loss=2.3347551822662354
I0203 22:04:30.131705 140023005427456 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3011319637298584, loss=2.3730530738830566
I0203 22:05:12.663996 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:05:23.851368 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:06:00.015872 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:06:01.629447 140184451094336 submission_runner.py:408] Time since start: 37585.90s, 	Step: 73093, 	{'train/accuracy': 0.6658984422683716, 'train/loss': 1.3752154111862183, 'validation/accuracy': 0.6152799725532532, 'validation/loss': 1.6217827796936035, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.2937076091766357, 'test/num_examples': 10000, 'score': 33649.62982439995, 'total_duration': 37585.90481185913, 'accumulated_submission_time': 33649.62982439995, 'accumulated_eval_time': 3929.7533457279205, 'accumulated_logging_time': 2.7157506942749023}
I0203 22:06:01.664733 140022518892288 logging_writer.py:48] [73093] accumulated_eval_time=3929.753346, accumulated_logging_time=2.715751, accumulated_submission_time=33649.629824, global_step=73093, preemption_count=0, score=33649.629824, test/accuracy=0.490000, test/loss=2.293708, test/num_examples=10000, total_duration=37585.904812, train/accuracy=0.665898, train/loss=1.375215, validation/accuracy=0.615280, validation/loss=1.621783, validation/num_examples=50000
I0203 22:06:05.007710 140023005427456 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.263429045677185, loss=2.957304000854492
I0203 22:06:48.405055 140022518892288 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1628506183624268, loss=4.715209007263184
I0203 22:07:34.554626 140023005427456 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.1009867191314697, loss=3.8055567741394043
I0203 22:08:20.953164 140022518892288 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.0975204706192017, loss=3.961799383163452
I0203 22:09:07.187335 140023005427456 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.1330820322036743, loss=3.0915818214416504
I0203 22:09:53.810845 140022518892288 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.211930751800537, loss=4.317281246185303
I0203 22:10:40.192981 140023005427456 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.2164608240127563, loss=2.511220693588257
I0203 22:11:26.477136 140022518892288 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3362314701080322, loss=2.5819499492645264
I0203 22:12:12.826269 140023005427456 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.9903401136398315, loss=4.288389205932617
I0203 22:12:58.932813 140022518892288 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3834950923919678, loss=2.407665967941284
I0203 22:13:01.970817 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:13:12.508082 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:13:49.724256 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:13:51.323072 140184451094336 submission_runner.py:408] Time since start: 38055.60s, 	Step: 74008, 	{'train/accuracy': 0.6753124594688416, 'train/loss': 1.3523850440979004, 'validation/accuracy': 0.6179999709129333, 'validation/loss': 1.608821153640747, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.2665867805480957, 'test/num_examples': 10000, 'score': 34069.875893354416, 'total_duration': 38055.59846138954, 'accumulated_submission_time': 34069.875893354416, 'accumulated_eval_time': 3979.105614423752, 'accumulated_logging_time': 2.763178586959839}
I0203 22:13:51.349774 140023005427456 logging_writer.py:48] [74008] accumulated_eval_time=3979.105614, accumulated_logging_time=2.763179, accumulated_submission_time=34069.875893, global_step=74008, preemption_count=0, score=34069.875893, test/accuracy=0.497800, test/loss=2.266587, test/num_examples=10000, total_duration=38055.598461, train/accuracy=0.675312, train/loss=1.352385, validation/accuracy=0.618000, validation/loss=1.608821, validation/num_examples=50000
I0203 22:14:31.334532 140022518892288 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.0594710111618042, loss=3.923750877380371
I0203 22:15:17.428417 140023005427456 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3019436597824097, loss=2.6707603931427
I0203 22:16:04.434549 140022518892288 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1966793537139893, loss=4.5545654296875
I0203 22:16:50.719081 140023005427456 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.0583549737930298, loss=4.266961097717285
I0203 22:17:37.230317 140022518892288 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3305535316467285, loss=2.5124480724334717
I0203 22:18:23.744097 140023005427456 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.3551898002624512, loss=2.5601954460144043
I0203 22:19:10.219485 140022518892288 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4170340299606323, loss=2.6095330715179443
I0203 22:19:56.521410 140023005427456 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.085553765296936, loss=4.602773189544678
I0203 22:20:43.003836 140022518892288 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1643731594085693, loss=2.8544392585754395
I0203 22:20:51.323307 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:21:02.300023 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:21:39.518232 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:21:41.122908 140184451094336 submission_runner.py:408] Time since start: 38525.40s, 	Step: 74919, 	{'train/accuracy': 0.695605456829071, 'train/loss': 1.2618730068206787, 'validation/accuracy': 0.6238799691200256, 'validation/loss': 1.5817739963531494, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.2340173721313477, 'test/num_examples': 10000, 'score': 34489.79175186157, 'total_duration': 38525.39829945564, 'accumulated_submission_time': 34489.79175186157, 'accumulated_eval_time': 4028.905200958252, 'accumulated_logging_time': 2.799935817718506}
I0203 22:21:41.150253 140023005427456 logging_writer.py:48] [74919] accumulated_eval_time=4028.905201, accumulated_logging_time=2.799936, accumulated_submission_time=34489.791752, global_step=74919, preemption_count=0, score=34489.791752, test/accuracy=0.505700, test/loss=2.234017, test/num_examples=10000, total_duration=38525.398299, train/accuracy=0.695605, train/loss=1.261873, validation/accuracy=0.623880, validation/loss=1.581774, validation/num_examples=50000
I0203 22:22:15.976503 140022518892288 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.0174261331558228, loss=5.012093544006348
I0203 22:23:02.026684 140023005427456 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.415229320526123, loss=2.3641371726989746
I0203 22:23:48.500901 140022518892288 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.2980581521987915, loss=2.3654074668884277
I0203 22:24:35.075440 140023005427456 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4961044788360596, loss=2.4952406883239746
I0203 22:25:21.260987 140022518892288 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.1998416185379028, loss=2.748195171356201
I0203 22:26:07.767330 140023005427456 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.3010962009429932, loss=2.703019618988037
I0203 22:26:54.300921 140022518892288 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.4308894872665405, loss=2.5388100147247314
I0203 22:27:40.655739 140023005427456 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.0599727630615234, loss=3.2526590824127197
I0203 22:28:27.239719 140022518892288 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.351155161857605, loss=2.39048171043396
I0203 22:28:41.204471 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:28:51.902557 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:29:32.015038 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:29:33.613878 140184451094336 submission_runner.py:408] Time since start: 38997.89s, 	Step: 75832, 	{'train/accuracy': 0.6705663800239563, 'train/loss': 1.3746696710586548, 'validation/accuracy': 0.6190599799156189, 'validation/loss': 1.6183385848999023, 'validation/num_examples': 50000, 'test/accuracy': 0.4936000108718872, 'test/loss': 2.2937138080596924, 'test/num_examples': 10000, 'score': 34909.78695511818, 'total_duration': 38997.88926529884, 'accumulated_submission_time': 34909.78695511818, 'accumulated_eval_time': 4081.3146035671234, 'accumulated_logging_time': 2.8388900756835938}
I0203 22:29:33.644472 140023005427456 logging_writer.py:48] [75832] accumulated_eval_time=4081.314604, accumulated_logging_time=2.838890, accumulated_submission_time=34909.786955, global_step=75832, preemption_count=0, score=34909.786955, test/accuracy=0.493600, test/loss=2.293714, test/num_examples=10000, total_duration=38997.889265, train/accuracy=0.670566, train/loss=1.374670, validation/accuracy=0.619060, validation/loss=1.618339, validation/num_examples=50000
I0203 22:30:02.463084 140022518892288 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.0905684232711792, loss=3.7764720916748047
I0203 22:30:48.372604 140023005427456 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.121869683265686, loss=4.542335033416748
I0203 22:31:35.625662 140022518892288 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1678129434585571, loss=3.7018914222717285
I0203 22:32:21.867194 140023005427456 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.2492880821228027, loss=2.3522589206695557
I0203 22:33:08.332801 140022518892288 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.3400046825408936, loss=4.672372817993164
I0203 22:33:54.692422 140023005427456 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.0801016092300415, loss=5.063529968261719
I0203 22:34:40.855513 140022518892288 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.2739465236663818, loss=2.7902941703796387
I0203 22:35:27.327586 140023005427456 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.3227981328964233, loss=5.048490524291992
I0203 22:36:13.587959 140022518892288 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.1458064317703247, loss=3.6848361492156982
I0203 22:36:33.664105 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:36:44.748116 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:37:19.595422 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:37:21.199715 140184451094336 submission_runner.py:408] Time since start: 39465.48s, 	Step: 76745, 	{'train/accuracy': 0.6803905963897705, 'train/loss': 1.297563910484314, 'validation/accuracy': 0.6237199902534485, 'validation/loss': 1.563851237297058, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.204143762588501, 'test/num_examples': 10000, 'score': 35329.75003695488, 'total_duration': 39465.47510480881, 'accumulated_submission_time': 35329.75003695488, 'accumulated_eval_time': 4128.850201368332, 'accumulated_logging_time': 2.8785297870635986}
I0203 22:37:21.227615 140023005427456 logging_writer.py:48] [76745] accumulated_eval_time=4128.850201, accumulated_logging_time=2.878530, accumulated_submission_time=35329.750037, global_step=76745, preemption_count=0, score=35329.750037, test/accuracy=0.508900, test/loss=2.204144, test/num_examples=10000, total_duration=39465.475105, train/accuracy=0.680391, train/loss=1.297564, validation/accuracy=0.623720, validation/loss=1.563851, validation/num_examples=50000
I0203 22:37:44.630019 140022518892288 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.425779938697815, loss=2.4783737659454346
I0203 22:38:29.952381 140023005427456 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1291835308074951, loss=2.845583915710449
I0203 22:39:15.989652 140022518892288 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.3949975967407227, loss=2.4342546463012695
I0203 22:40:02.447240 140023005427456 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.2601251602172852, loss=2.3419313430786133
I0203 22:40:48.497762 140022518892288 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.5161664485931396, loss=2.3427343368530273
I0203 22:41:35.082480 140023005427456 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.4043177366256714, loss=2.538768768310547
I0203 22:42:21.602832 140022518892288 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.1945714950561523, loss=4.508818626403809
I0203 22:43:07.863392 140023005427456 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.4070712327957153, loss=2.602530002593994
I0203 22:43:54.288359 140022518892288 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.3355000019073486, loss=2.496065855026245
I0203 22:44:21.282809 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:44:32.299938 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:45:10.107848 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:45:11.713766 140184451094336 submission_runner.py:408] Time since start: 39935.99s, 	Step: 77660, 	{'train/accuracy': 0.6931836009025574, 'train/loss': 1.2485311031341553, 'validation/accuracy': 0.6234999895095825, 'validation/loss': 1.5847680568695068, 'validation/num_examples': 50000, 'test/accuracy': 0.5022000074386597, 'test/loss': 2.245917320251465, 'test/num_examples': 10000, 'score': 35749.74824357033, 'total_duration': 39935.989151239395, 'accumulated_submission_time': 35749.74824357033, 'accumulated_eval_time': 4179.281141281128, 'accumulated_logging_time': 2.9159533977508545}
I0203 22:45:11.741006 140023005427456 logging_writer.py:48] [77660] accumulated_eval_time=4179.281141, accumulated_logging_time=2.915953, accumulated_submission_time=35749.748244, global_step=77660, preemption_count=0, score=35749.748244, test/accuracy=0.502200, test/loss=2.245917, test/num_examples=10000, total_duration=39935.989151, train/accuracy=0.693184, train/loss=1.248531, validation/accuracy=0.623500, validation/loss=1.584768, validation/num_examples=50000
I0203 22:45:28.873029 140022518892288 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.1132906675338745, loss=4.822157859802246
I0203 22:46:13.419076 140023005427456 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.180469036102295, loss=4.861629486083984
I0203 22:47:00.109600 140022518892288 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2506383657455444, loss=4.401858806610107
I0203 22:47:46.694475 140023005427456 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.1456302404403687, loss=5.143817901611328
I0203 22:48:33.041038 140022518892288 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3829621076583862, loss=2.3605990409851074
I0203 22:49:19.304883 140023005427456 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3568034172058105, loss=2.382270574569702
I0203 22:50:05.510810 140022518892288 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1127142906188965, loss=3.629584789276123
I0203 22:50:51.914635 140023005427456 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.0927033424377441, loss=4.876157760620117
I0203 22:51:38.586310 140022518892288 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.3209747076034546, loss=2.4846553802490234
I0203 22:52:12.029432 140184451094336 spec.py:321] Evaluating on the training split.
I0203 22:52:22.867288 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 22:52:56.949157 140184451094336 spec.py:349] Evaluating on the test split.
I0203 22:52:58.556012 140184451094336 submission_runner.py:408] Time since start: 40402.83s, 	Step: 78574, 	{'train/accuracy': 0.6760546565055847, 'train/loss': 1.3154542446136475, 'validation/accuracy': 0.6225399971008301, 'validation/loss': 1.5719152688980103, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.222673177719116, 'test/num_examples': 10000, 'score': 36169.981214761734, 'total_duration': 40402.831394672394, 'accumulated_submission_time': 36169.981214761734, 'accumulated_eval_time': 4225.807715415955, 'accumulated_logging_time': 2.9519596099853516}
I0203 22:52:58.586004 140023005427456 logging_writer.py:48] [78574] accumulated_eval_time=4225.807715, accumulated_logging_time=2.951960, accumulated_submission_time=36169.981215, global_step=78574, preemption_count=0, score=36169.981215, test/accuracy=0.508400, test/loss=2.222673, test/num_examples=10000, total_duration=40402.831395, train/accuracy=0.676055, train/loss=1.315454, validation/accuracy=0.622540, validation/loss=1.571915, validation/num_examples=50000
I0203 22:53:09.866541 140022518892288 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.2897320985794067, loss=2.8303470611572266
I0203 22:53:53.912759 140023005427456 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.2149405479431152, loss=2.929530143737793
I0203 22:54:40.314074 140022518892288 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.3275386095046997, loss=2.3336105346679688
I0203 22:55:26.791484 140023005427456 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.7285891771316528, loss=2.527942657470703
I0203 22:56:13.089017 140022518892288 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.313459038734436, loss=2.4007680416107178
I0203 22:56:59.471805 140023005427456 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.2776390314102173, loss=2.515315055847168
I0203 22:57:45.820861 140022518892288 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.2196568250656128, loss=2.5354301929473877
I0203 22:58:32.314534 140023005427456 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.2995463609695435, loss=2.353947639465332
I0203 22:59:18.748415 140022518892288 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.1269056797027588, loss=3.804152250289917
I0203 22:59:59.037492 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:00:09.797477 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:00:44.629149 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:00:46.229994 140184451094336 submission_runner.py:408] Time since start: 40870.51s, 	Step: 79489, 	{'train/accuracy': 0.6782616972923279, 'train/loss': 1.3288729190826416, 'validation/accuracy': 0.6286799907684326, 'validation/loss': 1.572872281074524, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.23213791847229, 'test/num_examples': 10000, 'score': 36590.37432670593, 'total_duration': 40870.50536131859, 'accumulated_submission_time': 36590.37432670593, 'accumulated_eval_time': 4273.000194072723, 'accumulated_logging_time': 2.9928932189941406}
I0203 23:00:46.266889 140023005427456 logging_writer.py:48] [79489] accumulated_eval_time=4273.000194, accumulated_logging_time=2.992893, accumulated_submission_time=36590.374327, global_step=79489, preemption_count=0, score=36590.374327, test/accuracy=0.505600, test/loss=2.232138, test/num_examples=10000, total_duration=40870.505361, train/accuracy=0.678262, train/loss=1.328873, validation/accuracy=0.628680, validation/loss=1.572872, validation/num_examples=50000
I0203 23:00:51.277936 140022518892288 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.098196029663086, loss=3.6660778522491455
I0203 23:01:34.750561 140023005427456 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.3395189046859741, loss=2.406010627746582
I0203 23:02:21.054494 140022518892288 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.3662053346633911, loss=2.413752794265747
I0203 23:03:07.258927 140023005427456 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.0740771293640137, loss=4.998567581176758
I0203 23:03:53.527959 140022518892288 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.3279120922088623, loss=2.5713930130004883
I0203 23:04:39.738874 140023005427456 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.1514840126037598, loss=5.001409530639648
I0203 23:05:26.156195 140022518892288 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.3115708827972412, loss=2.3863422870635986
I0203 23:06:12.452206 140023005427456 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.0634456872940063, loss=5.004085540771484
I0203 23:06:58.488302 140022518892288 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.425315260887146, loss=2.481071949005127
I0203 23:07:44.694275 140023005427456 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.5203192234039307, loss=2.345773458480835
I0203 23:07:46.235256 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:07:56.769013 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:08:33.284281 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:08:34.883541 140184451094336 submission_runner.py:408] Time since start: 41339.16s, 	Step: 80405, 	{'train/accuracy': 0.6900194883346558, 'train/loss': 1.277381420135498, 'validation/accuracy': 0.6231399774551392, 'validation/loss': 1.5857053995132446, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.244521379470825, 'test/num_examples': 10000, 'score': 37010.2838101387, 'total_duration': 41339.1589307785, 'accumulated_submission_time': 37010.2838101387, 'accumulated_eval_time': 4321.6484797000885, 'accumulated_logging_time': 3.0400874614715576}
I0203 23:08:34.912161 140022518892288 logging_writer.py:48] [80405] accumulated_eval_time=4321.648480, accumulated_logging_time=3.040087, accumulated_submission_time=37010.283810, global_step=80405, preemption_count=0, score=37010.283810, test/accuracy=0.499500, test/loss=2.244521, test/num_examples=10000, total_duration=41339.158931, train/accuracy=0.690019, train/loss=1.277381, validation/accuracy=0.623140, validation/loss=1.585705, validation/num_examples=50000
I0203 23:09:15.957573 140023005427456 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.4637398719787598, loss=2.2603585720062256
I0203 23:10:02.051076 140022518892288 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.194197416305542, loss=3.9830703735351562
I0203 23:10:48.414707 140023005427456 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.3139255046844482, loss=2.6653900146484375
I0203 23:11:34.818638 140022518892288 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.4561188220977783, loss=2.5082361698150635
I0203 23:12:21.296162 140023005427456 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.428540587425232, loss=2.561599016189575
I0203 23:13:07.511466 140022518892288 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.322594404220581, loss=2.7126049995422363
I0203 23:13:53.562166 140023005427456 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.53119957447052, loss=2.369649887084961
I0203 23:14:40.032131 140022518892288 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.5638500452041626, loss=2.3356034755706787
I0203 23:15:26.537892 140023005427456 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.3934998512268066, loss=2.3362772464752197
I0203 23:15:35.209749 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:15:46.008208 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:16:23.161841 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:16:24.769622 140184451094336 submission_runner.py:408] Time since start: 41809.04s, 	Step: 81320, 	{'train/accuracy': 0.6727538704872131, 'train/loss': 1.3480576276779175, 'validation/accuracy': 0.6234999895095825, 'validation/loss': 1.5841037034988403, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2456095218658447, 'test/num_examples': 10000, 'score': 37430.524639606476, 'total_duration': 41809.0449860096, 'accumulated_submission_time': 37430.524639606476, 'accumulated_eval_time': 4371.208312034607, 'accumulated_logging_time': 3.0785329341888428}
I0203 23:16:24.808460 140022518892288 logging_writer.py:48] [81320] accumulated_eval_time=4371.208312, accumulated_logging_time=3.078533, accumulated_submission_time=37430.524640, global_step=81320, preemption_count=0, score=37430.524640, test/accuracy=0.504900, test/loss=2.245610, test/num_examples=10000, total_duration=41809.044986, train/accuracy=0.672754, train/loss=1.348058, validation/accuracy=0.623500, validation/loss=1.584104, validation/num_examples=50000
I0203 23:16:58.946025 140023005427456 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.0918582677841187, loss=3.603621482849121
I0203 23:17:45.119668 140022518892288 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.2262636423110962, loss=2.6221890449523926
I0203 23:18:31.782759 140023005427456 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.326912522315979, loss=2.3505167961120605
I0203 23:19:18.055444 140022518892288 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.3110690116882324, loss=2.700974464416504
I0203 23:20:04.508328 140023005427456 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.170799970626831, loss=4.535957336425781
I0203 23:20:50.936637 140022518892288 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.412640929222107, loss=2.409363031387329
I0203 23:21:37.490267 140023005427456 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.3556816577911377, loss=2.450622081756592
I0203 23:22:24.120351 140022518892288 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.4279216527938843, loss=2.387589693069458
I0203 23:23:10.755700 140023005427456 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.3103524446487427, loss=2.8391995429992676
I0203 23:23:24.835508 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:23:35.720058 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:24:11.805772 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:24:13.412662 140184451094336 submission_runner.py:408] Time since start: 42277.69s, 	Step: 82232, 	{'train/accuracy': 0.6878319978713989, 'train/loss': 1.2847343683242798, 'validation/accuracy': 0.6326599717140198, 'validation/loss': 1.5323657989501953, 'validation/num_examples': 50000, 'test/accuracy': 0.5116000175476074, 'test/loss': 2.190577745437622, 'test/num_examples': 10000, 'score': 37850.4936645031, 'total_duration': 42277.68804812431, 'accumulated_submission_time': 37850.4936645031, 'accumulated_eval_time': 4419.785451173782, 'accumulated_logging_time': 3.1283504962921143}
I0203 23:24:13.444483 140022518892288 logging_writer.py:48] [82232] accumulated_eval_time=4419.785451, accumulated_logging_time=3.128350, accumulated_submission_time=37850.493665, global_step=82232, preemption_count=0, score=37850.493665, test/accuracy=0.511600, test/loss=2.190578, test/num_examples=10000, total_duration=42277.688048, train/accuracy=0.687832, train/loss=1.284734, validation/accuracy=0.632660, validation/loss=1.532366, validation/num_examples=50000
I0203 23:24:42.275499 140023005427456 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3057101964950562, loss=2.3562510013580322
I0203 23:25:28.505757 140022518892288 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3477007150650024, loss=2.470383882522583
I0203 23:26:15.002939 140023005427456 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.2764972448349, loss=2.4303998947143555
I0203 23:27:01.312508 140022518892288 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.4138877391815186, loss=2.326491594314575
I0203 23:27:47.842398 140023005427456 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.3914741277694702, loss=2.3702924251556396
I0203 23:28:34.306744 140022518892288 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.1973416805267334, loss=3.8746230602264404
I0203 23:29:20.780101 140023005427456 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2776501178741455, loss=2.527740478515625
I0203 23:30:07.069768 140022518892288 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.3386198282241821, loss=2.7256524562835693
I0203 23:30:53.413848 140023005427456 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.3458247184753418, loss=2.3118560314178467
I0203 23:31:13.463916 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:31:24.291209 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:31:58.233085 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:31:59.839320 140184451094336 submission_runner.py:408] Time since start: 42744.11s, 	Step: 83145, 	{'train/accuracy': 0.6927343606948853, 'train/loss': 1.2806254625320435, 'validation/accuracy': 0.6339799761772156, 'validation/loss': 1.5582224130630493, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.209243059158325, 'test/num_examples': 10000, 'score': 38270.45635151863, 'total_duration': 42744.11470103264, 'accumulated_submission_time': 38270.45635151863, 'accumulated_eval_time': 4466.1608464717865, 'accumulated_logging_time': 3.169515371322632}
I0203 23:31:59.867207 140022518892288 logging_writer.py:48] [83145] accumulated_eval_time=4466.160846, accumulated_logging_time=3.169515, accumulated_submission_time=38270.456352, global_step=83145, preemption_count=0, score=38270.456352, test/accuracy=0.512900, test/loss=2.209243, test/num_examples=10000, total_duration=42744.114701, train/accuracy=0.692734, train/loss=1.280625, validation/accuracy=0.633980, validation/loss=1.558222, validation/num_examples=50000
I0203 23:32:23.255623 140023005427456 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.3015626668930054, loss=2.3687734603881836
I0203 23:33:08.967917 140022518892288 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.3276920318603516, loss=2.3384578227996826
I0203 23:33:55.303451 140023005427456 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.1665012836456299, loss=3.4921603202819824
I0203 23:34:41.829626 140022518892288 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.3572707176208496, loss=2.2703723907470703
I0203 23:35:28.595770 140023005427456 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.2608683109283447, loss=2.6636462211608887
I0203 23:36:14.902092 140022518892288 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.4231373071670532, loss=2.2830824851989746
I0203 23:37:01.385123 140023005427456 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.4450414180755615, loss=2.272979736328125
I0203 23:37:47.902029 140022518892288 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.3491361141204834, loss=2.889167070388794
I0203 23:38:34.132603 140023005427456 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.6100190877914429, loss=2.3558292388916016
I0203 23:39:00.150226 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:39:10.914434 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:39:48.248695 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:39:49.854877 140184451094336 submission_runner.py:408] Time since start: 43214.13s, 	Step: 84058, 	{'train/accuracy': 0.6813867092132568, 'train/loss': 1.28969407081604, 'validation/accuracy': 0.6294599771499634, 'validation/loss': 1.5353686809539795, 'validation/num_examples': 50000, 'test/accuracy': 0.5073000192642212, 'test/loss': 2.186338186264038, 'test/num_examples': 10000, 'score': 38690.68273019791, 'total_duration': 43214.1302447319, 'accumulated_submission_time': 38690.68273019791, 'accumulated_eval_time': 4515.86549949646, 'accumulated_logging_time': 3.206382989883423}
I0203 23:39:49.888246 140022518892288 logging_writer.py:48] [84058] accumulated_eval_time=4515.865499, accumulated_logging_time=3.206383, accumulated_submission_time=38690.682730, global_step=84058, preemption_count=0, score=38690.682730, test/accuracy=0.507300, test/loss=2.186338, test/num_examples=10000, total_duration=43214.130245, train/accuracy=0.681387, train/loss=1.289694, validation/accuracy=0.629460, validation/loss=1.535369, validation/num_examples=50000
I0203 23:40:07.860695 140023005427456 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.274124026298523, loss=4.697776794433594
I0203 23:40:52.512021 140022518892288 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.3649996519088745, loss=2.473595380783081
I0203 23:41:39.045422 140023005427456 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.1687414646148682, loss=4.994839668273926
I0203 23:42:25.267371 140022518892288 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.2356023788452148, loss=2.95889949798584
I0203 23:43:11.768202 140023005427456 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.191464900970459, loss=4.745188236236572
I0203 23:43:58.175402 140022518892288 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.1198662519454956, loss=4.871233940124512
I0203 23:44:44.286614 140023005427456 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.2676712274551392, loss=3.506627082824707
I0203 23:45:30.480217 140022518892288 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.2265375852584839, loss=2.9694995880126953
I0203 23:46:16.568232 140023005427456 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.0918245315551758, loss=3.3905937671661377
I0203 23:46:50.367442 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:47:01.087784 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:47:37.225719 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:47:38.835976 140184451094336 submission_runner.py:408] Time since start: 43683.11s, 	Step: 84974, 	{'train/accuracy': 0.688281238079071, 'train/loss': 1.2811274528503418, 'validation/accuracy': 0.6375600099563599, 'validation/loss': 1.5175962448120117, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.1811633110046387, 'test/num_examples': 10000, 'score': 39111.102942466736, 'total_duration': 43683.11135816574, 'accumulated_submission_time': 39111.102942466736, 'accumulated_eval_time': 4564.3340582847595, 'accumulated_logging_time': 3.2509865760803223}
I0203 23:47:38.867931 140022518892288 logging_writer.py:48] [84974] accumulated_eval_time=4564.334058, accumulated_logging_time=3.250987, accumulated_submission_time=39111.102942, global_step=84974, preemption_count=0, score=39111.102942, test/accuracy=0.512800, test/loss=2.181163, test/num_examples=10000, total_duration=43683.111358, train/accuracy=0.688281, train/loss=1.281127, validation/accuracy=0.637560, validation/loss=1.517596, validation/num_examples=50000
I0203 23:47:50.154779 140023005427456 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.3467155694961548, loss=2.2605819702148438
I0203 23:48:34.431265 140022518892288 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.182881236076355, loss=5.046831130981445
I0203 23:49:20.554462 140023005427456 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.5216543674468994, loss=2.4479269981384277
I0203 23:50:06.985029 140022518892288 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.2054393291473389, loss=2.9977567195892334
I0203 23:50:53.205435 140023005427456 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.493424415588379, loss=2.408747911453247
I0203 23:51:39.889655 140022518892288 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.274432897567749, loss=2.319222927093506
I0203 23:52:26.394440 140023005427456 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.1441696882247925, loss=3.67801833152771
I0203 23:53:12.829799 140022518892288 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.349998116493225, loss=2.20355224609375
I0203 23:53:59.351065 140023005427456 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.3125399351119995, loss=2.9690558910369873
I0203 23:54:38.897117 140184451094336 spec.py:321] Evaluating on the training split.
I0203 23:54:49.952669 140184451094336 spec.py:333] Evaluating on the validation split.
I0203 23:55:25.773219 140184451094336 spec.py:349] Evaluating on the test split.
I0203 23:55:27.373353 140184451094336 submission_runner.py:408] Time since start: 44151.65s, 	Step: 85887, 	{'train/accuracy': 0.6952148079872131, 'train/loss': 1.2292883396148682, 'validation/accuracy': 0.6356799602508545, 'validation/loss': 1.519616723060608, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.1769161224365234, 'test/num_examples': 10000, 'score': 39531.074053287506, 'total_duration': 44151.64874100685, 'accumulated_submission_time': 39531.074053287506, 'accumulated_eval_time': 4612.810303688049, 'accumulated_logging_time': 3.2939980030059814}
I0203 23:55:27.403832 140022518892288 logging_writer.py:48] [85887] accumulated_eval_time=4612.810304, accumulated_logging_time=3.293998, accumulated_submission_time=39531.074053, global_step=85887, preemption_count=0, score=39531.074053, test/accuracy=0.517800, test/loss=2.176916, test/num_examples=10000, total_duration=44151.648741, train/accuracy=0.695215, train/loss=1.229288, validation/accuracy=0.635680, validation/loss=1.519617, validation/num_examples=50000
I0203 23:55:33.251174 140023005427456 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.5043385028839111, loss=2.314039707183838
I0203 23:56:16.770075 140022518892288 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.342629075050354, loss=2.3117003440856934
I0203 23:57:03.046301 140023005427456 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.4117772579193115, loss=2.4923813343048096
I0203 23:57:49.416165 140022518892288 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.320244312286377, loss=2.9537808895111084
I0203 23:58:35.603100 140023005427456 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.3181164264678955, loss=2.518949270248413
I0203 23:59:22.052783 140022518892288 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.151389241218567, loss=4.580992221832275
I0204 00:00:08.255784 140023005427456 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.1003209352493286, loss=3.8495752811431885
I0204 00:00:54.683721 140022518892288 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2795230150222778, loss=2.2135376930236816
I0204 00:01:41.279448 140023005427456 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.4726492166519165, loss=2.249181032180786
I0204 00:02:27.585794 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:02:38.409138 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:03:14.957978 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:03:16.559027 140184451094336 submission_runner.py:408] Time since start: 44620.83s, 	Step: 86798, 	{'train/accuracy': 0.6976562142372131, 'train/loss': 1.220523476600647, 'validation/accuracy': 0.6376199722290039, 'validation/loss': 1.5006834268569946, 'validation/num_examples': 50000, 'test/accuracy': 0.5117000341415405, 'test/loss': 2.1646652221679688, 'test/num_examples': 10000, 'score': 39951.19725751877, 'total_duration': 44620.83441233635, 'accumulated_submission_time': 39951.19725751877, 'accumulated_eval_time': 4661.783539533615, 'accumulated_logging_time': 3.335458278656006}
I0204 00:03:16.592222 140022518892288 logging_writer.py:48] [86798] accumulated_eval_time=4661.783540, accumulated_logging_time=3.335458, accumulated_submission_time=39951.197258, global_step=86798, preemption_count=0, score=39951.197258, test/accuracy=0.511700, test/loss=2.164665, test/num_examples=10000, total_duration=44620.834412, train/accuracy=0.697656, train/loss=1.220523, validation/accuracy=0.637620, validation/loss=1.500683, validation/num_examples=50000
I0204 00:03:17.850149 140023005427456 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.368493676185608, loss=2.5214052200317383
I0204 00:04:00.732479 140022518892288 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5252541303634644, loss=2.8485019207000732
I0204 00:04:47.022855 140023005427456 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.1603577136993408, loss=4.530692100524902
I0204 00:05:33.613044 140022518892288 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.4135169982910156, loss=2.452077865600586
I0204 00:06:19.627102 140023005427456 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.4447431564331055, loss=2.5598299503326416
I0204 00:07:05.985461 140022518892288 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.0874484777450562, loss=4.709712028503418
I0204 00:07:52.420783 140023005427456 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.2344328165054321, loss=3.224062204360962
I0204 00:08:38.636301 140022518892288 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1998828649520874, loss=3.8638548851013184
I0204 00:09:25.076354 140023005427456 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.3754901885986328, loss=2.29262113571167
I0204 00:10:11.498697 140022518892288 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.293671727180481, loss=2.400434732437134
I0204 00:10:16.680026 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:10:27.605146 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:11:06.190553 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:11:07.776468 140184451094336 submission_runner.py:408] Time since start: 45092.05s, 	Step: 87713, 	{'train/accuracy': 0.6788867115974426, 'train/loss': 1.3864643573760986, 'validation/accuracy': 0.6287400126457214, 'validation/loss': 1.6301475763320923, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.2643375396728516, 'test/num_examples': 10000, 'score': 40371.22511386871, 'total_duration': 45092.05185699463, 'accumulated_submission_time': 40371.22511386871, 'accumulated_eval_time': 4712.8799839019775, 'accumulated_logging_time': 3.3804314136505127}
I0204 00:11:07.809099 140023005427456 logging_writer.py:48] [87713] accumulated_eval_time=4712.879984, accumulated_logging_time=3.380431, accumulated_submission_time=40371.225114, global_step=87713, preemption_count=0, score=40371.225114, test/accuracy=0.502800, test/loss=2.264338, test/num_examples=10000, total_duration=45092.051857, train/accuracy=0.678887, train/loss=1.386464, validation/accuracy=0.628740, validation/loss=1.630148, validation/num_examples=50000
I0204 00:11:45.191353 140022518892288 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.3912363052368164, loss=2.3437371253967285
I0204 00:12:31.468317 140023005427456 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.4950274229049683, loss=2.232426643371582
I0204 00:13:18.158990 140022518892288 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.3823812007904053, loss=2.3166184425354004
I0204 00:14:04.565559 140023005427456 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3560508489608765, loss=2.566196918487549
I0204 00:14:50.803333 140022518892288 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.3752272129058838, loss=2.284943103790283
I0204 00:15:37.272516 140023005427456 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.2255820035934448, loss=4.4451751708984375
I0204 00:16:23.637034 140022518892288 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.1313610076904297, loss=4.89525842666626
I0204 00:17:10.167163 140023005427456 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.423349142074585, loss=2.296635866165161
I0204 00:17:56.592521 140022518892288 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.4313052892684937, loss=2.215744972229004
I0204 00:18:07.949676 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:18:18.793352 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:18:53.525329 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:18:55.125283 140184451094336 submission_runner.py:408] Time since start: 45559.40s, 	Step: 88626, 	{'train/accuracy': 0.696582019329071, 'train/loss': 1.239540934562683, 'validation/accuracy': 0.6377399563789368, 'validation/loss': 1.517307162284851, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.1748321056365967, 'test/num_examples': 10000, 'score': 40791.30937457085, 'total_duration': 45559.40066933632, 'accumulated_submission_time': 40791.30937457085, 'accumulated_eval_time': 4760.05557847023, 'accumulated_logging_time': 3.4225516319274902}
I0204 00:18:55.154523 140023005427456 logging_writer.py:48] [88626] accumulated_eval_time=4760.055578, accumulated_logging_time=3.422552, accumulated_submission_time=40791.309375, global_step=88626, preemption_count=0, score=40791.309375, test/accuracy=0.516100, test/loss=2.174832, test/num_examples=10000, total_duration=45559.400669, train/accuracy=0.696582, train/loss=1.239541, validation/accuracy=0.637740, validation/loss=1.517307, validation/num_examples=50000
I0204 00:19:26.695737 140022518892288 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.2068142890930176, loss=3.552488088607788
I0204 00:20:12.984163 140023005427456 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4840940237045288, loss=2.2855618000030518
I0204 00:20:59.334073 140022518892288 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.4859837293624878, loss=2.2763004302978516
I0204 00:21:45.879853 140023005427456 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.5360461473464966, loss=2.8621175289154053
I0204 00:22:32.439848 140022518892288 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.7475136518478394, loss=2.2847816944122314
I0204 00:23:18.799707 140023005427456 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.568419098854065, loss=2.3722262382507324
I0204 00:24:05.519478 140022518892288 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.1238847970962524, loss=3.8018531799316406
I0204 00:24:52.067362 140023005427456 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.2658408880233765, loss=4.667501449584961
I0204 00:25:38.374347 140022518892288 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.1602267026901245, loss=4.033083915710449
I0204 00:25:55.202486 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:26:05.781469 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:26:43.197667 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:26:44.801137 140184451094336 submission_runner.py:408] Time since start: 46029.08s, 	Step: 89538, 	{'train/accuracy': 0.71888667345047, 'train/loss': 1.155856728553772, 'validation/accuracy': 0.6437000036239624, 'validation/loss': 1.506482481956482, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.1656334400177, 'test/num_examples': 10000, 'score': 41211.301359415054, 'total_duration': 46029.07649850845, 'accumulated_submission_time': 41211.301359415054, 'accumulated_eval_time': 4809.654201030731, 'accumulated_logging_time': 3.460923194885254}
I0204 00:26:44.834901 140023005427456 logging_writer.py:48] [89538] accumulated_eval_time=4809.654201, accumulated_logging_time=3.460923, accumulated_submission_time=41211.301359, global_step=89538, preemption_count=0, score=41211.301359, test/accuracy=0.515200, test/loss=2.165633, test/num_examples=10000, total_duration=46029.076499, train/accuracy=0.718887, train/loss=1.155857, validation/accuracy=0.643700, validation/loss=1.506482, validation/num_examples=50000
I0204 00:27:11.157451 140022518892288 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.429124355316162, loss=3.1030216217041016
I0204 00:27:56.913999 140023005427456 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.2039806842803955, loss=4.3078765869140625
I0204 00:28:43.378322 140022518892288 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.4175169467926025, loss=2.40561580657959
I0204 00:29:30.105634 140023005427456 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.2282222509384155, loss=4.849508285522461
I0204 00:30:16.755559 140022518892288 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.2127372026443481, loss=3.2600576877593994
I0204 00:31:03.294773 140023005427456 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.2647095918655396, loss=3.0511722564697266
I0204 00:31:49.971539 140022518892288 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2903445959091187, loss=3.276834487915039
I0204 00:32:36.519707 140023005427456 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.1690268516540527, loss=3.567404270172119
I0204 00:33:23.296646 140022518892288 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.2423895597457886, loss=3.978437662124634
I0204 00:33:45.115405 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:33:55.807275 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:34:32.800726 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:34:34.403213 140184451094336 submission_runner.py:408] Time since start: 46498.68s, 	Step: 90449, 	{'train/accuracy': 0.6948632597923279, 'train/loss': 1.2670438289642334, 'validation/accuracy': 0.6413399577140808, 'validation/loss': 1.5187033414840698, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.1620571613311768, 'test/num_examples': 10000, 'score': 41631.52504038811, 'total_duration': 46498.67860245705, 'accumulated_submission_time': 41631.52504038811, 'accumulated_eval_time': 4858.942010641098, 'accumulated_logging_time': 3.504815101623535}
I0204 00:34:34.432348 140023005427456 logging_writer.py:48] [90449] accumulated_eval_time=4858.942011, accumulated_logging_time=3.504815, accumulated_submission_time=41631.525040, global_step=90449, preemption_count=0, score=41631.525040, test/accuracy=0.518500, test/loss=2.162057, test/num_examples=10000, total_duration=46498.678602, train/accuracy=0.694863, train/loss=1.267044, validation/accuracy=0.641340, validation/loss=1.518703, validation/num_examples=50000
I0204 00:34:56.134679 140022518892288 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.4323328733444214, loss=2.1422998905181885
I0204 00:35:41.230091 140023005427456 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.1914682388305664, loss=4.424806118011475
I0204 00:36:27.378456 140022518892288 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.3597378730773926, loss=2.2241709232330322
I0204 00:37:13.802266 140023005427456 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.1287182569503784, loss=4.1950507164001465
I0204 00:37:59.804023 140022518892288 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.3289185762405396, loss=4.41016960144043
I0204 00:38:46.086287 140023005427456 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.430291771888733, loss=2.363495349884033
I0204 00:39:32.405480 140022518892288 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.2129511833190918, loss=3.0164074897766113
I0204 00:40:18.959562 140023005427456 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.472692847251892, loss=2.187039375305176
I0204 00:41:05.114254 140022518892288 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.31461501121521, loss=2.8629016876220703
I0204 00:41:34.683562 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:41:45.416045 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:42:21.250319 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:42:22.850576 140184451094336 submission_runner.py:408] Time since start: 46967.13s, 	Step: 91365, 	{'train/accuracy': 0.7024804353713989, 'train/loss': 1.197124719619751, 'validation/accuracy': 0.6451199650764465, 'validation/loss': 1.4732191562652588, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.1140828132629395, 'test/num_examples': 10000, 'score': 42051.71841979027, 'total_duration': 46967.12596321106, 'accumulated_submission_time': 42051.71841979027, 'accumulated_eval_time': 4907.109016418457, 'accumulated_logging_time': 3.5443942546844482}
I0204 00:42:22.886477 140023005427456 logging_writer.py:48] [91365] accumulated_eval_time=4907.109016, accumulated_logging_time=3.544394, accumulated_submission_time=42051.718420, global_step=91365, preemption_count=0, score=42051.718420, test/accuracy=0.521600, test/loss=2.114083, test/num_examples=10000, total_duration=46967.125963, train/accuracy=0.702480, train/loss=1.197125, validation/accuracy=0.645120, validation/loss=1.473219, validation/num_examples=50000
I0204 00:42:37.919404 140022518892288 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.4105123281478882, loss=2.2516467571258545
I0204 00:43:22.265833 140023005427456 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.2806216478347778, loss=4.872192859649658
I0204 00:44:08.891519 140022518892288 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.396376371383667, loss=2.192233085632324
I0204 00:44:55.397611 140023005427456 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.3757961988449097, loss=2.3769688606262207
I0204 00:45:41.665030 140022518892288 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.4140071868896484, loss=2.341087579727173
I0204 00:46:28.055435 140023005427456 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.349351406097412, loss=2.1973202228546143
I0204 00:47:14.241048 140022518892288 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.3522769212722778, loss=2.34751558303833
I0204 00:48:00.342514 140023005427456 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.2872134447097778, loss=4.685637950897217
I0204 00:48:46.837059 140022518892288 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.6507197618484497, loss=2.687894582748413
I0204 00:49:23.168064 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:49:33.719781 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:50:09.627135 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:50:11.233722 140184451094336 submission_runner.py:408] Time since start: 47435.51s, 	Step: 92280, 	{'train/accuracy': 0.7185351252555847, 'train/loss': 1.1746147871017456, 'validation/accuracy': 0.6480799913406372, 'validation/loss': 1.501075267791748, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.161663293838501, 'test/num_examples': 10000, 'score': 42471.94208693504, 'total_duration': 47435.50911140442, 'accumulated_submission_time': 42471.94208693504, 'accumulated_eval_time': 4955.174675226212, 'accumulated_logging_time': 3.5913169384002686}
I0204 00:50:11.268398 140023005427456 logging_writer.py:48] [92280] accumulated_eval_time=4955.174675, accumulated_logging_time=3.591317, accumulated_submission_time=42471.942087, global_step=92280, preemption_count=0, score=42471.942087, test/accuracy=0.520500, test/loss=2.161663, test/num_examples=10000, total_duration=47435.509111, train/accuracy=0.718535, train/loss=1.174615, validation/accuracy=0.648080, validation/loss=1.501075, validation/num_examples=50000
I0204 00:50:20.042583 140022518892288 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.235011339187622, loss=4.760970115661621
I0204 00:51:04.001173 140023005427456 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.1504415273666382, loss=3.6089799404144287
I0204 00:51:50.343858 140022518892288 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.6202489137649536, loss=2.283209800720215
I0204 00:52:37.060338 140023005427456 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.3592251539230347, loss=2.9467999935150146
I0204 00:53:23.134315 140022518892288 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.5358929634094238, loss=2.413797378540039
I0204 00:54:09.546483 140023005427456 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.3109604120254517, loss=2.194664478302002
I0204 00:54:55.771557 140022518892288 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.1622916460037231, loss=4.958489418029785
I0204 00:55:42.193244 140023005427456 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.2643249034881592, loss=2.53014874458313
I0204 00:56:28.571938 140022518892288 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.2268211841583252, loss=3.846250057220459
I0204 00:57:11.510935 140184451094336 spec.py:321] Evaluating on the training split.
I0204 00:57:22.050365 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 00:58:00.308223 140184451094336 spec.py:349] Evaluating on the test split.
I0204 00:58:01.919831 140184451094336 submission_runner.py:408] Time since start: 47906.20s, 	Step: 93194, 	{'train/accuracy': 0.7062109112739563, 'train/loss': 1.185657262802124, 'validation/accuracy': 0.6523799896240234, 'validation/loss': 1.4381099939346313, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.1004345417022705, 'test/num_examples': 10000, 'score': 42892.12707614899, 'total_duration': 47906.19521903992, 'accumulated_submission_time': 42892.12707614899, 'accumulated_eval_time': 5005.583575248718, 'accumulated_logging_time': 3.6359968185424805}
I0204 00:58:01.950803 140023005427456 logging_writer.py:48] [93194] accumulated_eval_time=5005.583575, accumulated_logging_time=3.635997, accumulated_submission_time=42892.127076, global_step=93194, preemption_count=0, score=42892.127076, test/accuracy=0.527400, test/loss=2.100435, test/num_examples=10000, total_duration=47906.195219, train/accuracy=0.706211, train/loss=1.185657, validation/accuracy=0.652380, validation/loss=1.438110, validation/num_examples=50000
I0204 00:58:04.874912 140022518892288 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.5668277740478516, loss=2.2043869495391846
I0204 00:58:47.925004 140023005427456 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.162554383277893, loss=4.2326884269714355
I0204 00:59:34.117071 140022518892288 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.485861897468567, loss=2.3242082595825195
I0204 01:00:20.558540 140023005427456 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.3481746912002563, loss=2.420928955078125
I0204 01:01:06.840110 140022518892288 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.2828830480575562, loss=3.4741768836975098
I0204 01:01:53.335757 140023005427456 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.3948924541473389, loss=2.2401161193847656
I0204 01:02:39.560137 140022518892288 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.1687958240509033, loss=4.925414085388184
I0204 01:03:26.080782 140023005427456 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.3072901964187622, loss=3.695525646209717
I0204 01:04:12.175492 140022518892288 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.348258376121521, loss=2.777698278427124
I0204 01:04:58.386111 140023005427456 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.3977161645889282, loss=2.24570631980896
I0204 01:05:02.202448 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:05:13.171245 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:05:50.465015 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:05:52.066550 140184451094336 submission_runner.py:408] Time since start: 48376.34s, 	Step: 94110, 	{'train/accuracy': 0.7049023509025574, 'train/loss': 1.2190285921096802, 'validation/accuracy': 0.6449199914932251, 'validation/loss': 1.500189185142517, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.142191171646118, 'test/num_examples': 10000, 'score': 43312.31781864166, 'total_duration': 48376.341938734055, 'accumulated_submission_time': 43312.31781864166, 'accumulated_eval_time': 5055.447685480118, 'accumulated_logging_time': 3.6795918941497803}
I0204 01:05:52.097787 140022518892288 logging_writer.py:48] [94110] accumulated_eval_time=5055.447685, accumulated_logging_time=3.679592, accumulated_submission_time=43312.317819, global_step=94110, preemption_count=0, score=43312.317819, test/accuracy=0.523100, test/loss=2.142191, test/num_examples=10000, total_duration=48376.341939, train/accuracy=0.704902, train/loss=1.219029, validation/accuracy=0.644920, validation/loss=1.500189, validation/num_examples=50000
I0204 01:06:30.799637 140023005427456 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.4309256076812744, loss=2.3783607482910156
I0204 01:07:16.909098 140022518892288 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.4280577898025513, loss=4.3365583419799805
I0204 01:08:03.128703 140023005427456 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.4911099672317505, loss=2.148447275161743
I0204 01:08:49.218167 140022518892288 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.443808674812317, loss=2.673027276992798
I0204 01:09:35.603769 140023005427456 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.4803334474563599, loss=2.2193493843078613
I0204 01:10:22.046377 140022518892288 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.329162836074829, loss=2.7418606281280518
I0204 01:11:08.532905 140023005427456 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.21044921875, loss=4.517414093017578
I0204 01:11:55.086386 140022518892288 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.588025450706482, loss=2.328903913497925
I0204 01:12:41.416063 140023005427456 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.3885293006896973, loss=2.1714251041412354
I0204 01:12:52.283504 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:13:03.091658 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:13:40.411795 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:13:42.016489 140184451094336 submission_runner.py:408] Time since start: 48846.29s, 	Step: 95025, 	{'train/accuracy': 0.7074413895606995, 'train/loss': 1.209861397743225, 'validation/accuracy': 0.6448799967765808, 'validation/loss': 1.5044829845428467, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.164458990097046, 'test/num_examples': 10000, 'score': 43732.444350004196, 'total_duration': 48846.29187488556, 'accumulated_submission_time': 43732.444350004196, 'accumulated_eval_time': 5105.180654525757, 'accumulated_logging_time': 3.722055435180664}
I0204 01:13:42.047063 140022518892288 logging_writer.py:48] [95025] accumulated_eval_time=5105.180655, accumulated_logging_time=3.722055, accumulated_submission_time=43732.444350, global_step=95025, preemption_count=0, score=43732.444350, test/accuracy=0.519900, test/loss=2.164459, test/num_examples=10000, total_duration=48846.291875, train/accuracy=0.707441, train/loss=1.209861, validation/accuracy=0.644880, validation/loss=1.504483, validation/num_examples=50000
I0204 01:14:14.158424 140023005427456 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.5220342874526978, loss=2.149899482727051
I0204 01:15:00.545920 140022518892288 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.4161616563796997, loss=2.182976245880127
I0204 01:15:47.205601 140023005427456 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.2993404865264893, loss=3.306027412414551
I0204 01:16:33.672401 140022518892288 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.1280109882354736, loss=4.7896342277526855
I0204 01:17:20.356551 140023005427456 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.2634210586547852, loss=3.96343731880188
I0204 01:18:06.938841 140022518892288 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.403691291809082, loss=2.5544421672821045
I0204 01:18:53.147894 140023005427456 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3912991285324097, loss=2.1926932334899902
I0204 01:19:39.528329 140022518892288 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.3852883577346802, loss=2.743575096130371
I0204 01:20:25.806465 140023005427456 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.5484095811843872, loss=2.2783873081207275
I0204 01:20:42.211822 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:20:53.478954 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:21:29.841685 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:21:31.446948 140184451094336 submission_runner.py:408] Time since start: 49315.72s, 	Step: 95937, 	{'train/accuracy': 0.7051367163658142, 'train/loss': 1.1937915086746216, 'validation/accuracy': 0.6563999652862549, 'validation/loss': 1.434855580329895, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.0843238830566406, 'test/num_examples': 10000, 'score': 44152.55052447319, 'total_duration': 49315.72231268883, 'accumulated_submission_time': 44152.55052447319, 'accumulated_eval_time': 5154.415741682053, 'accumulated_logging_time': 3.7638611793518066}
I0204 01:21:31.484566 140022518892288 logging_writer.py:48] [95937] accumulated_eval_time=5154.415742, accumulated_logging_time=3.763861, accumulated_submission_time=44152.550524, global_step=95937, preemption_count=0, score=44152.550524, test/accuracy=0.534800, test/loss=2.084324, test/num_examples=10000, total_duration=49315.722313, train/accuracy=0.705137, train/loss=1.193792, validation/accuracy=0.656400, validation/loss=1.434856, validation/num_examples=50000
I0204 01:21:58.222713 140023005427456 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.3315390348434448, loss=3.2870426177978516
I0204 01:22:44.260909 140022518892288 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.2058085203170776, loss=4.8296732902526855
I0204 01:23:30.416964 140023005427456 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.4883906841278076, loss=2.222703218460083
I0204 01:24:16.660070 140022518892288 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.4824423789978027, loss=2.148122549057007
I0204 01:25:03.180650 140023005427456 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.5671480894088745, loss=2.173703670501709
I0204 01:25:49.206085 140022518892288 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.2538115978240967, loss=3.2365639209747314
I0204 01:26:35.730337 140023005427456 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.5507391691207886, loss=2.6877493858337402
I0204 01:27:21.895967 140022518892288 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.5290225744247437, loss=2.181382179260254
I0204 01:28:08.337374 140023005427456 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.450789213180542, loss=2.251181125640869
I0204 01:28:31.615915 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:28:42.484242 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:29:17.789559 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:29:19.385378 140184451094336 submission_runner.py:408] Time since start: 49783.66s, 	Step: 96852, 	{'train/accuracy': 0.7074413895606995, 'train/loss': 1.221797227859497, 'validation/accuracy': 0.6502400040626526, 'validation/loss': 1.4847956895828247, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.126685857772827, 'test/num_examples': 10000, 'score': 44572.624385118484, 'total_duration': 49783.660767793655, 'accumulated_submission_time': 44572.624385118484, 'accumulated_eval_time': 5202.185204267502, 'accumulated_logging_time': 3.811398506164551}
I0204 01:29:19.415776 140022518892288 logging_writer.py:48] [96852] accumulated_eval_time=5202.185204, accumulated_logging_time=3.811399, accumulated_submission_time=44572.624385, global_step=96852, preemption_count=0, score=44572.624385, test/accuracy=0.532900, test/loss=2.126686, test/num_examples=10000, total_duration=49783.660768, train/accuracy=0.707441, train/loss=1.221797, validation/accuracy=0.650240, validation/loss=1.484796, validation/num_examples=50000
I0204 01:29:39.885294 140023005427456 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.2340902090072632, loss=4.74305534362793
I0204 01:30:25.046983 140022518892288 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.3436435461044312, loss=2.1531829833984375
I0204 01:31:11.628629 140023005427456 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.1635853052139282, loss=4.74249792098999
I0204 01:31:58.193511 140022518892288 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.1405388116836548, loss=4.700864791870117
I0204 01:32:44.689445 140023005427456 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.5381014347076416, loss=2.1821136474609375
I0204 01:33:31.207806 140022518892288 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.2051953077316284, loss=4.233057498931885
I0204 01:34:17.838457 140023005427456 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.4544389247894287, loss=2.168827533721924
I0204 01:35:04.295755 140022518892288 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.5507034063339233, loss=2.2633185386657715
I0204 01:35:50.821720 140023005427456 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.3210569620132446, loss=3.7603087425231934
I0204 01:36:19.802428 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:36:30.819872 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:37:09.497951 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:37:11.099604 140184451094336 submission_runner.py:408] Time since start: 50255.37s, 	Step: 97764, 	{'train/accuracy': 0.7198437452316284, 'train/loss': 1.1401715278625488, 'validation/accuracy': 0.6556599736213684, 'validation/loss': 1.4324363470077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.0989506244659424, 'test/num_examples': 10000, 'score': 44992.955062150955, 'total_duration': 50255.37498688698, 'accumulated_submission_time': 44992.955062150955, 'accumulated_eval_time': 5253.482377767563, 'accumulated_logging_time': 3.8508574962615967}
I0204 01:37:11.137140 140022518892288 logging_writer.py:48] [97764] accumulated_eval_time=5253.482378, accumulated_logging_time=3.850857, accumulated_submission_time=44992.955062, global_step=97764, preemption_count=0, score=44992.955062, test/accuracy=0.528100, test/loss=2.098951, test/num_examples=10000, total_duration=50255.374987, train/accuracy=0.719844, train/loss=1.140172, validation/accuracy=0.655660, validation/loss=1.432436, validation/num_examples=50000
I0204 01:37:26.578663 140023005427456 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.341272234916687, loss=2.265266180038452
I0204 01:38:11.336312 140022518892288 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.5392398834228516, loss=2.534222364425659
I0204 01:38:57.727318 140023005427456 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.2848680019378662, loss=3.9200258255004883
I0204 01:39:44.601211 140022518892288 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.255408525466919, loss=3.1145823001861572
I0204 01:40:31.137112 140023005427456 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.5287525653839111, loss=2.2587733268737793
I0204 01:41:17.679466 140022518892288 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.4080172777175903, loss=4.044506072998047
I0204 01:42:04.120768 140023005427456 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.3523027896881104, loss=4.2958526611328125
I0204 01:42:50.214753 140022518892288 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.6757879257202148, loss=2.282787799835205
I0204 01:43:36.669320 140023005427456 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.206956386566162, loss=4.548498630523682
I0204 01:44:11.395645 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:44:22.163378 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:44:58.854415 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:45:00.457989 140184451094336 submission_runner.py:408] Time since start: 50724.73s, 	Step: 98676, 	{'train/accuracy': 0.7141015529632568, 'train/loss': 1.1600228548049927, 'validation/accuracy': 0.6577399969100952, 'validation/loss': 1.420377254486084, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.072648525238037, 'test/num_examples': 10000, 'score': 45413.15657520294, 'total_duration': 50724.73337721825, 'accumulated_submission_time': 45413.15657520294, 'accumulated_eval_time': 5302.544720649719, 'accumulated_logging_time': 3.8982207775115967}
I0204 01:45:00.492918 140022518892288 logging_writer.py:48] [98676] accumulated_eval_time=5302.544721, accumulated_logging_time=3.898221, accumulated_submission_time=45413.156575, global_step=98676, preemption_count=0, score=45413.156575, test/accuracy=0.530700, test/loss=2.072649, test/num_examples=10000, total_duration=50724.733377, train/accuracy=0.714102, train/loss=1.160023, validation/accuracy=0.657740, validation/loss=1.420377, validation/num_examples=50000
I0204 01:45:10.928506 140023005427456 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.545677661895752, loss=2.1428418159484863
I0204 01:45:54.933009 140022518892288 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.4602255821228027, loss=2.1772398948669434
I0204 01:46:41.362152 140023005427456 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.4732099771499634, loss=2.1157217025756836
I0204 01:47:27.742254 140022518892288 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.571290373802185, loss=2.1698741912841797
I0204 01:48:13.969190 140023005427456 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.2844634056091309, loss=3.720137357711792
I0204 01:49:00.032972 140022518892288 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.5249351263046265, loss=2.1727519035339355
I0204 01:49:46.550712 140023005427456 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.4761346578598022, loss=2.245836019515991
I0204 01:50:32.632205 140022518892288 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.5141593217849731, loss=2.165149211883545
I0204 01:51:18.783841 140023005427456 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.414085030555725, loss=2.1208620071411133
I0204 01:52:00.614778 140184451094336 spec.py:321] Evaluating on the training split.
I0204 01:52:11.520534 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 01:52:47.862661 140184451094336 spec.py:349] Evaluating on the test split.
I0204 01:52:49.464699 140184451094336 submission_runner.py:408] Time since start: 51193.74s, 	Step: 99592, 	{'train/accuracy': 0.7101367115974426, 'train/loss': 1.1785920858383179, 'validation/accuracy': 0.6572999954223633, 'validation/loss': 1.4439600706100464, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.08896541595459, 'test/num_examples': 10000, 'score': 45833.22182202339, 'total_duration': 51193.74008560181, 'accumulated_submission_time': 45833.22182202339, 'accumulated_eval_time': 5351.394645690918, 'accumulated_logging_time': 3.9429714679718018}
I0204 01:52:49.500116 140022518892288 logging_writer.py:48] [99592] accumulated_eval_time=5351.394646, accumulated_logging_time=3.942971, accumulated_submission_time=45833.221822, global_step=99592, preemption_count=0, score=45833.221822, test/accuracy=0.531500, test/loss=2.088965, test/num_examples=10000, total_duration=51193.740086, train/accuracy=0.710137, train/loss=1.178592, validation/accuracy=0.657300, validation/loss=1.443960, validation/num_examples=50000
I0204 01:52:53.264015 140023005427456 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.4069366455078125, loss=2.1658830642700195
I0204 01:53:36.561213 140022518892288 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.5494647026062012, loss=2.3099329471588135
I0204 01:54:22.607775 140023005427456 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.4473860263824463, loss=2.2606711387634277
I0204 01:55:09.303307 140022518892288 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.5100500583648682, loss=2.138587713241577
I0204 01:55:55.577808 140023005427456 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.2742884159088135, loss=3.6737403869628906
I0204 01:56:42.218859 140022518892288 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.4696680307388306, loss=2.3359103202819824
I0204 01:57:28.463749 140023005427456 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.4222733974456787, loss=2.234470844268799
I0204 01:58:15.032674 140022518892288 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.493277907371521, loss=2.1995327472686768
I0204 01:59:01.178312 140023005427456 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.2286479473114014, loss=4.641655921936035
I0204 01:59:47.537976 140022518892288 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.3315017223358154, loss=2.72387433052063
I0204 01:59:49.519458 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:00:00.247490 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:00:36.000808 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:00:37.606797 140184451094336 submission_runner.py:408] Time since start: 51661.88s, 	Step: 100506, 	{'train/accuracy': 0.7159960865974426, 'train/loss': 1.1765002012252808, 'validation/accuracy': 0.6542199850082397, 'validation/loss': 1.4638800621032715, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1265082359313965, 'test/num_examples': 10000, 'score': 46253.18224287033, 'total_duration': 51661.882147789, 'accumulated_submission_time': 46253.18224287033, 'accumulated_eval_time': 5399.48194694519, 'accumulated_logging_time': 3.9898531436920166}
I0204 02:00:37.644048 140023005427456 logging_writer.py:48] [100506] accumulated_eval_time=5399.481947, accumulated_logging_time=3.989853, accumulated_submission_time=46253.182243, global_step=100506, preemption_count=0, score=46253.182243, test/accuracy=0.528800, test/loss=2.126508, test/num_examples=10000, total_duration=51661.882148, train/accuracy=0.715996, train/loss=1.176500, validation/accuracy=0.654220, validation/loss=1.463880, validation/num_examples=50000
I0204 02:01:18.433529 140022518892288 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.5403746366500854, loss=2.412811040878296
I0204 02:02:04.875863 140023005427456 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.462380051612854, loss=2.114454746246338
I0204 02:02:51.340365 140022518892288 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.4424692392349243, loss=2.1963658332824707
I0204 02:03:37.847546 140023005427456 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.257915735244751, loss=3.5323750972747803
I0204 02:04:24.009876 140022518892288 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.54362154006958, loss=2.172441005706787
I0204 02:05:10.765185 140023005427456 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.4943360090255737, loss=2.240226984024048
I0204 02:05:56.650109 140022518892288 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.4506721496582031, loss=2.214195966720581
I0204 02:06:42.825317 140023005427456 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.424882411956787, loss=2.2086398601531982
I0204 02:07:29.368414 140022518892288 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.6075845956802368, loss=2.3041906356811523
I0204 02:07:37.999742 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:07:49.220351 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:08:26.270146 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:08:27.868443 140184451094336 submission_runner.py:408] Time since start: 52132.14s, 	Step: 101420, 	{'train/accuracy': 0.7417382597923279, 'train/loss': 1.0204622745513916, 'validation/accuracy': 0.6606000065803528, 'validation/loss': 1.399212121963501, 'validation/num_examples': 50000, 'test/accuracy': 0.5382000207901001, 'test/loss': 2.0457940101623535, 'test/num_examples': 10000, 'score': 46673.48130655289, 'total_duration': 52132.143812179565, 'accumulated_submission_time': 46673.48130655289, 'accumulated_eval_time': 5449.350612878799, 'accumulated_logging_time': 4.036839246749878}
I0204 02:08:27.902217 140023005427456 logging_writer.py:48] [101420] accumulated_eval_time=5449.350613, accumulated_logging_time=4.036839, accumulated_submission_time=46673.481307, global_step=101420, preemption_count=0, score=46673.481307, test/accuracy=0.538200, test/loss=2.045794, test/num_examples=10000, total_duration=52132.143812, train/accuracy=0.741738, train/loss=1.020462, validation/accuracy=0.660600, validation/loss=1.399212, validation/num_examples=50000
I0204 02:09:02.014691 140022518892288 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.4680514335632324, loss=2.1909239292144775
I0204 02:09:48.255483 140023005427456 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.632189154624939, loss=2.313973903656006
I0204 02:10:34.648909 140022518892288 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.4859775304794312, loss=2.394289493560791
I0204 02:11:20.968645 140023005427456 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.4131263494491577, loss=3.200148582458496
I0204 02:12:07.554249 140022518892288 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.4695006608963013, loss=2.6971216201782227
I0204 02:12:53.827171 140023005427456 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.3312138319015503, loss=3.3709254264831543
I0204 02:13:39.970727 140022518892288 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.4790573120117188, loss=2.2547976970672607
I0204 02:14:26.101092 140023005427456 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4042550325393677, loss=2.242769718170166
I0204 02:15:12.378943 140022518892288 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.459399938583374, loss=2.5173325538635254
I0204 02:15:27.942912 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:15:38.767643 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:16:15.459094 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:16:17.060084 140184451094336 submission_runner.py:408] Time since start: 52601.34s, 	Step: 102335, 	{'train/accuracy': 0.7127734422683716, 'train/loss': 1.167461633682251, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.4336830377578735, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.095097064971924, 'test/num_examples': 10000, 'score': 47093.46519160271, 'total_duration': 52601.335465192795, 'accumulated_submission_time': 47093.46519160271, 'accumulated_eval_time': 5498.46777844429, 'accumulated_logging_time': 4.080301761627197}
I0204 02:16:17.091990 140023005427456 logging_writer.py:48] [102335] accumulated_eval_time=5498.467778, accumulated_logging_time=4.080302, accumulated_submission_time=47093.465192, global_step=102335, preemption_count=0, score=47093.465192, test/accuracy=0.531400, test/loss=2.095097, test/num_examples=10000, total_duration=52601.335465, train/accuracy=0.712773, train/loss=1.167462, validation/accuracy=0.656680, validation/loss=1.433683, validation/num_examples=50000
I0204 02:16:44.635178 140022518892288 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.4645813703536987, loss=2.149425983428955
I0204 02:17:30.357838 140023005427456 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.5533334016799927, loss=2.107595443725586
I0204 02:18:17.041163 140022518892288 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.3632110357284546, loss=4.290387153625488
I0204 02:19:03.137425 140023005427456 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.556054949760437, loss=2.057255506515503
I0204 02:19:49.341609 140022518892288 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.410948395729065, loss=2.2013845443725586
I0204 02:20:35.712203 140023005427456 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.5434774160385132, loss=2.228207588195801
I0204 02:21:22.068976 140022518892288 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.5089319944381714, loss=4.0457563400268555
I0204 02:22:08.495879 140023005427456 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.465648889541626, loss=2.144361734390259
I0204 02:22:54.870565 140022518892288 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.3087106943130493, loss=3.9545810222625732
I0204 02:23:17.365821 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:23:28.055229 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:24:05.215046 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:24:06.816313 140184451094336 submission_runner.py:408] Time since start: 53071.09s, 	Step: 103250, 	{'train/accuracy': 0.7226171493530273, 'train/loss': 1.1213455200195312, 'validation/accuracy': 0.6646199822425842, 'validation/loss': 1.3939058780670166, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.0467312335968018, 'test/num_examples': 10000, 'score': 47513.68147063255, 'total_duration': 53071.09167742729, 'accumulated_submission_time': 47513.68147063255, 'accumulated_eval_time': 5547.918229103088, 'accumulated_logging_time': 4.1213812828063965}
I0204 02:24:06.854687 140023005427456 logging_writer.py:48] [103250] accumulated_eval_time=5547.918229, accumulated_logging_time=4.121381, accumulated_submission_time=47513.681471, global_step=103250, preemption_count=0, score=47513.681471, test/accuracy=0.540000, test/loss=2.046731, test/num_examples=10000, total_duration=53071.091677, train/accuracy=0.722617, train/loss=1.121346, validation/accuracy=0.664620, validation/loss=1.393906, validation/num_examples=50000
I0204 02:24:28.149180 140022518892288 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.6034302711486816, loss=2.105231761932373
I0204 02:25:13.227150 140023005427456 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.4305212497711182, loss=4.849025726318359
I0204 02:25:59.585563 140022518892288 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.4641913175582886, loss=2.8894577026367188
I0204 02:26:45.835284 140023005427456 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.56979238986969, loss=2.1253502368927
I0204 02:27:31.993920 140022518892288 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.5445959568023682, loss=2.13205623626709
I0204 02:28:18.238428 140023005427456 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.4112427234649658, loss=4.034694671630859
I0204 02:29:04.730811 140022518892288 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.4281936883926392, loss=2.2013347148895264
I0204 02:29:50.805787 140023005427456 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.4654333591461182, loss=2.044261932373047
I0204 02:30:37.168772 140022518892288 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.440561294555664, loss=2.216764450073242
I0204 02:31:07.091830 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:31:17.870754 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:31:54.064713 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:31:55.667951 140184451094336 submission_runner.py:408] Time since start: 53539.94s, 	Step: 104166, 	{'train/accuracy': 0.7356249690055847, 'train/loss': 1.0597867965698242, 'validation/accuracy': 0.6630399823188782, 'validation/loss': 1.3899242877960205, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.0377378463745117, 'test/num_examples': 10000, 'score': 47933.8608417511, 'total_duration': 53539.94333600998, 'accumulated_submission_time': 47933.8608417511, 'accumulated_eval_time': 5596.494349956512, 'accumulated_logging_time': 4.169813394546509}
I0204 02:31:55.700042 140023005427456 logging_writer.py:48] [104166] accumulated_eval_time=5596.494350, accumulated_logging_time=4.169813, accumulated_submission_time=47933.860842, global_step=104166, preemption_count=0, score=47933.860842, test/accuracy=0.541600, test/loss=2.037738, test/num_examples=10000, total_duration=53539.943336, train/accuracy=0.735625, train/loss=1.059787, validation/accuracy=0.663040, validation/loss=1.389924, validation/num_examples=50000
I0204 02:32:10.319701 140022518892288 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.5219427347183228, loss=2.1924004554748535
I0204 02:32:54.643594 140023005427456 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.6842035055160522, loss=2.1494853496551514
I0204 02:33:41.086372 140022518892288 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.7034835815429688, loss=2.0596985816955566
I0204 02:34:27.444471 140023005427456 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.5632095336914062, loss=2.089031219482422
I0204 02:35:13.809964 140022518892288 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.652152419090271, loss=2.135573387145996
I0204 02:35:59.920274 140023005427456 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.6201081275939941, loss=2.151003837585449
I0204 02:36:46.189681 140022518892288 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.2602511644363403, loss=4.635780334472656
I0204 02:37:32.856616 140023005427456 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.7513903379440308, loss=2.2195382118225098
I0204 02:38:18.961398 140022518892288 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.4311530590057373, loss=4.722731113433838
I0204 02:38:55.778388 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:39:06.594461 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:39:44.080702 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:39:45.681960 140184451094336 submission_runner.py:408] Time since start: 54009.96s, 	Step: 105081, 	{'train/accuracy': 0.7218359112739563, 'train/loss': 1.1346908807754517, 'validation/accuracy': 0.6655399799346924, 'validation/loss': 1.3913841247558594, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.0553183555603027, 'test/num_examples': 10000, 'score': 48353.88178706169, 'total_duration': 54009.957320690155, 'accumulated_submission_time': 48353.88178706169, 'accumulated_eval_time': 5646.397901058197, 'accumulated_logging_time': 4.211780786514282}
I0204 02:39:45.718054 140023005427456 logging_writer.py:48] [105081] accumulated_eval_time=5646.397901, accumulated_logging_time=4.211781, accumulated_submission_time=48353.881787, global_step=105081, preemption_count=0, score=48353.881787, test/accuracy=0.539800, test/loss=2.055318, test/num_examples=10000, total_duration=54009.957321, train/accuracy=0.721836, train/loss=1.134691, validation/accuracy=0.665540, validation/loss=1.391384, validation/num_examples=50000
I0204 02:39:54.063096 140022518892288 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.514536738395691, loss=2.1627585887908936
I0204 02:40:37.684981 140023005427456 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.573354959487915, loss=2.1422617435455322
I0204 02:41:24.007209 140022518892288 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.6757274866104126, loss=2.481522560119629
I0204 02:42:10.772773 140023005427456 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.51645028591156, loss=3.291457176208496
I0204 02:42:57.135671 140022518892288 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.8877019882202148, loss=2.1864402294158936
I0204 02:43:43.628437 140023005427456 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.4591352939605713, loss=2.618978977203369
I0204 02:44:30.031293 140022518892288 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.4964739084243774, loss=2.0986151695251465
I0204 02:45:16.495594 140023005427456 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.243370771408081, loss=3.699397563934326
I0204 02:46:02.862343 140022518892288 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.5104626417160034, loss=2.7082362174987793
I0204 02:46:46.017767 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:46:56.817466 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:47:36.226742 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:47:37.823398 140184451094336 submission_runner.py:408] Time since start: 54482.10s, 	Step: 105995, 	{'train/accuracy': 0.7276366949081421, 'train/loss': 1.0901107788085938, 'validation/accuracy': 0.6678199768066406, 'validation/loss': 1.3639365434646606, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0250608921051025, 'test/num_examples': 10000, 'score': 48774.124626636505, 'total_duration': 54482.09878492355, 'accumulated_submission_time': 48774.124626636505, 'accumulated_eval_time': 5698.203535318375, 'accumulated_logging_time': 4.257709503173828}
I0204 02:47:37.859754 140023005427456 logging_writer.py:48] [105995] accumulated_eval_time=5698.203535, accumulated_logging_time=4.257710, accumulated_submission_time=48774.124627, global_step=105995, preemption_count=0, score=48774.124627, test/accuracy=0.542100, test/loss=2.025061, test/num_examples=10000, total_duration=54482.098785, train/accuracy=0.727637, train/loss=1.090111, validation/accuracy=0.667820, validation/loss=1.363937, validation/num_examples=50000
I0204 02:47:40.364601 140022518892288 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.5502300262451172, loss=2.1863832473754883
I0204 02:48:23.623521 140023005427456 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.4906984567642212, loss=2.122502088546753
I0204 02:49:09.679699 140022518892288 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.2930554151535034, loss=4.645242691040039
I0204 02:49:56.315860 140023005427456 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.6986181735992432, loss=2.0868494510650635
I0204 02:50:42.757973 140022518892288 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.517061471939087, loss=2.274075508117676
I0204 02:51:28.952521 140023005427456 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.5113039016723633, loss=2.2769148349761963
I0204 02:52:15.500419 140022518892288 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.3899332284927368, loss=4.113940238952637
I0204 02:53:01.697956 140023005427456 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.3827484846115112, loss=4.528956413269043
I0204 02:53:48.272904 140022518892288 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.4198987483978271, loss=2.3360791206359863
I0204 02:54:34.705192 140023005427456 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.5672852993011475, loss=3.5977845191955566
I0204 02:54:38.050993 140184451094336 spec.py:321] Evaluating on the training split.
I0204 02:54:48.826679 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 02:55:24.305473 140184451094336 spec.py:349] Evaluating on the test split.
I0204 02:55:25.905503 140184451094336 submission_runner.py:408] Time since start: 54950.18s, 	Step: 106909, 	{'train/accuracy': 0.7373046875, 'train/loss': 1.0468322038650513, 'validation/accuracy': 0.6692599654197693, 'validation/loss': 1.3726061582565308, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.0267672538757324, 'test/num_examples': 10000, 'score': 49194.25314736366, 'total_duration': 54950.18088555336, 'accumulated_submission_time': 49194.25314736366, 'accumulated_eval_time': 5746.058041095734, 'accumulated_logging_time': 4.303881406784058}
I0204 02:55:25.940084 140022518892288 logging_writer.py:48] [106909] accumulated_eval_time=5746.058041, accumulated_logging_time=4.303881, accumulated_submission_time=49194.253147, global_step=106909, preemption_count=0, score=49194.253147, test/accuracy=0.545800, test/loss=2.026767, test/num_examples=10000, total_duration=54950.180886, train/accuracy=0.737305, train/loss=1.046832, validation/accuracy=0.669260, validation/loss=1.372606, validation/num_examples=50000
I0204 02:56:05.455620 140023005427456 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.5744004249572754, loss=1.9696249961853027
I0204 02:56:51.535468 140022518892288 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.6220344305038452, loss=2.1637418270111084
I0204 02:57:38.155143 140023005427456 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.6361973285675049, loss=2.1767501831054688
I0204 02:58:24.585130 140022518892288 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.3254162073135376, loss=3.9153318405151367
I0204 02:59:11.112152 140023005427456 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.6002963781356812, loss=2.1817095279693604
I0204 02:59:57.124482 140022518892288 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.3943071365356445, loss=3.940213203430176
I0204 03:00:43.985970 140023005427456 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.4378392696380615, loss=4.631784915924072
I0204 03:01:30.297411 140022518892288 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.3296217918395996, loss=2.4889187812805176
I0204 03:02:17.324002 140023005427456 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.5772576332092285, loss=2.1878278255462646
I0204 03:02:26.290428 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:02:36.755571 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:03:13.375165 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:03:14.976067 140184451094336 submission_runner.py:408] Time since start: 55419.25s, 	Step: 107821, 	{'train/accuracy': 0.7249218821525574, 'train/loss': 1.1311174631118774, 'validation/accuracy': 0.6684199571609497, 'validation/loss': 1.3853036165237427, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.035310745239258, 'test/num_examples': 10000, 'score': 49614.54778671265, 'total_duration': 55419.25144505501, 'accumulated_submission_time': 49614.54778671265, 'accumulated_eval_time': 5794.7436735630035, 'accumulated_logging_time': 4.3474390506744385}
I0204 03:03:15.008414 140022518892288 logging_writer.py:48] [107821] accumulated_eval_time=5794.743674, accumulated_logging_time=4.347439, accumulated_submission_time=49614.547787, global_step=107821, preemption_count=0, score=49614.547787, test/accuracy=0.547300, test/loss=2.035311, test/num_examples=10000, total_duration=55419.251445, train/accuracy=0.724922, train/loss=1.131117, validation/accuracy=0.668420, validation/loss=1.385304, validation/num_examples=50000
I0204 03:03:48.808411 140023005427456 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.7798985242843628, loss=2.1694881916046143
I0204 03:04:34.862078 140022518892288 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.7827255725860596, loss=2.162997245788574
I0204 03:05:21.486776 140023005427456 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.3065170049667358, loss=3.345024585723877
I0204 03:06:07.749466 140022518892288 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.4409126043319702, loss=4.837244033813477
I0204 03:06:54.033875 140023005427456 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.4174628257751465, loss=2.3725104331970215
I0204 03:07:40.357508 140022518892288 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.398345708847046, loss=4.359259128570557
I0204 03:08:26.749498 140023005427456 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.4673347473144531, loss=3.399357795715332
I0204 03:09:13.331653 140022518892288 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.6910189390182495, loss=2.037229061126709
I0204 03:09:59.855181 140023005427456 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.4964783191680908, loss=4.751686096191406
I0204 03:10:15.332412 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:10:26.212311 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:11:03.723356 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:11:05.325662 140184451094336 submission_runner.py:408] Time since start: 55889.60s, 	Step: 108735, 	{'train/accuracy': 0.7259179353713989, 'train/loss': 1.0984207391738892, 'validation/accuracy': 0.6665199995040894, 'validation/loss': 1.3816251754760742, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.0237200260162354, 'test/num_examples': 10000, 'score': 50034.81283926964, 'total_duration': 55889.601029634476, 'accumulated_submission_time': 50034.81283926964, 'accumulated_eval_time': 5844.7369022369385, 'accumulated_logging_time': 4.391482353210449}
I0204 03:11:05.358622 140022518892288 logging_writer.py:48] [108735] accumulated_eval_time=5844.736902, accumulated_logging_time=4.391482, accumulated_submission_time=50034.812839, global_step=108735, preemption_count=0, score=50034.812839, test/accuracy=0.549700, test/loss=2.023720, test/num_examples=10000, total_duration=55889.601030, train/accuracy=0.725918, train/loss=1.098421, validation/accuracy=0.666520, validation/loss=1.381625, validation/num_examples=50000
I0204 03:11:32.929840 140023005427456 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.6504676342010498, loss=2.069035530090332
I0204 03:12:19.334215 140022518892288 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.546487808227539, loss=2.393202781677246
I0204 03:13:05.437515 140023005427456 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.405612826347351, loss=2.8017868995666504
I0204 03:13:51.880582 140022518892288 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.553404450416565, loss=2.003031015396118
I0204 03:14:38.316829 140023005427456 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.5098708868026733, loss=2.411771059036255
I0204 03:15:24.726471 140022518892288 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.606571912765503, loss=2.103095531463623
I0204 03:16:10.947861 140023005427456 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.627105951309204, loss=2.3526275157928467
I0204 03:16:57.312239 140022518892288 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.4941405057907104, loss=2.482814311981201
I0204 03:17:43.466353 140023005427456 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.4948700666427612, loss=2.572420597076416
I0204 03:18:05.773720 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:18:16.520371 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:18:52.961516 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:18:54.561106 140184451094336 submission_runner.py:408] Time since start: 56358.84s, 	Step: 109650, 	{'train/accuracy': 0.7411913871765137, 'train/loss': 1.045373558998108, 'validation/accuracy': 0.6743599772453308, 'validation/loss': 1.3515647649765015, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 1.9914335012435913, 'test/num_examples': 10000, 'score': 50455.170258522034, 'total_duration': 56358.836493730545, 'accumulated_submission_time': 50455.170258522034, 'accumulated_eval_time': 5893.524285316467, 'accumulated_logging_time': 4.434794902801514}
I0204 03:18:54.594685 140022518892288 logging_writer.py:48] [109650] accumulated_eval_time=5893.524285, accumulated_logging_time=4.434795, accumulated_submission_time=50455.170259, global_step=109650, preemption_count=0, score=50455.170259, test/accuracy=0.554400, test/loss=1.991434, test/num_examples=10000, total_duration=56358.836494, train/accuracy=0.741191, train/loss=1.045374, validation/accuracy=0.674360, validation/loss=1.351565, validation/num_examples=50000
I0204 03:19:15.901493 140023005427456 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.312150478363037, loss=3.635024309158325
I0204 03:20:00.938152 140022518892288 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.6233031749725342, loss=4.670053482055664
I0204 03:20:47.369627 140023005427456 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.5980477333068848, loss=2.094931125640869
I0204 03:21:34.142868 140022518892288 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.5816090106964111, loss=2.047722339630127
I0204 03:22:20.642138 140023005427456 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.4601366519927979, loss=2.1800498962402344
I0204 03:23:06.863365 140022518892288 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.4233107566833496, loss=4.671576499938965
I0204 03:23:53.023313 140023005427456 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.6599266529083252, loss=2.2320961952209473
I0204 03:24:39.277034 140022518892288 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.5919997692108154, loss=2.4054834842681885
I0204 03:25:25.920624 140023005427456 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.5537034273147583, loss=3.0460214614868164
I0204 03:25:54.921475 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:26:05.768764 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:26:43.234354 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:26:44.833316 140184451094336 submission_runner.py:408] Time since start: 56829.11s, 	Step: 110565, 	{'train/accuracy': 0.7310742139816284, 'train/loss': 1.0999258756637573, 'validation/accuracy': 0.6731199622154236, 'validation/loss': 1.3631587028503418, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.017176866531372, 'test/num_examples': 10000, 'score': 50875.440262556076, 'total_duration': 56829.10870409012, 'accumulated_submission_time': 50875.440262556076, 'accumulated_eval_time': 5943.436125278473, 'accumulated_logging_time': 4.477481842041016}
I0204 03:26:44.869971 140022518892288 logging_writer.py:48] [110565] accumulated_eval_time=5943.436125, accumulated_logging_time=4.477482, accumulated_submission_time=50875.440263, global_step=110565, preemption_count=0, score=50875.440263, test/accuracy=0.552500, test/loss=2.017177, test/num_examples=10000, total_duration=56829.108704, train/accuracy=0.731074, train/loss=1.099926, validation/accuracy=0.673120, validation/loss=1.363159, validation/num_examples=50000
I0204 03:26:59.912518 140023005427456 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.3435242176055908, loss=4.119672775268555
I0204 03:27:44.431880 140022518892288 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.625994324684143, loss=2.103715658187866
I0204 03:28:31.232197 140023005427456 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.5112500190734863, loss=2.2360217571258545
I0204 03:29:17.389204 140022518892288 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.5487619638442993, loss=2.711893081665039
I0204 03:30:03.894675 140023005427456 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.5346391201019287, loss=2.3920793533325195
I0204 03:30:50.325522 140022518892288 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.6242979764938354, loss=1.9638994932174683
I0204 03:31:36.949209 140023005427456 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.593721866607666, loss=2.2360472679138184
I0204 03:32:23.258083 140022518892288 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.515565276145935, loss=2.4789106845855713
I0204 03:33:09.417823 140023005427456 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.554915189743042, loss=2.652440309524536
I0204 03:33:45.199335 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:33:55.843320 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:34:32.944996 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:34:34.542575 140184451094336 submission_runner.py:408] Time since start: 57298.82s, 	Step: 111479, 	{'train/accuracy': 0.7366992235183716, 'train/loss': 1.0674903392791748, 'validation/accuracy': 0.6771799921989441, 'validation/loss': 1.3465999364852905, 'validation/num_examples': 50000, 'test/accuracy': 0.5570000410079956, 'test/loss': 1.9866653680801392, 'test/num_examples': 10000, 'score': 51295.71018028259, 'total_duration': 57298.81796312332, 'accumulated_submission_time': 51295.71018028259, 'accumulated_eval_time': 5992.779366493225, 'accumulated_logging_time': 4.526298999786377}
I0204 03:34:34.577732 140022518892288 logging_writer.py:48] [111479] accumulated_eval_time=5992.779366, accumulated_logging_time=4.526299, accumulated_submission_time=51295.710180, global_step=111479, preemption_count=0, score=51295.710180, test/accuracy=0.557000, test/loss=1.986665, test/num_examples=10000, total_duration=57298.817963, train/accuracy=0.736699, train/loss=1.067490, validation/accuracy=0.677180, validation/loss=1.346600, validation/num_examples=50000
I0204 03:34:43.764358 140023005427456 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.697356104850769, loss=2.0365896224975586
I0204 03:35:27.599882 140022518892288 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.8570743799209595, loss=2.1657519340515137
I0204 03:36:13.975120 140023005427456 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.883675217628479, loss=2.0764060020446777
I0204 03:37:00.639493 140022518892288 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.510801076889038, loss=4.726054668426514
I0204 03:37:46.828444 140023005427456 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.4797377586364746, loss=2.8865714073181152
I0204 03:38:33.105095 140022518892288 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.6299244165420532, loss=4.498554229736328
I0204 03:39:19.343707 140023005427456 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.6129719018936157, loss=2.1077749729156494
I0204 03:40:05.780704 140022518892288 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.5608128309249878, loss=2.064836025238037
I0204 03:40:51.881684 140023005427456 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.6099008321762085, loss=1.9753998517990112
I0204 03:41:34.548659 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:41:45.147411 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:42:20.837950 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:42:22.435617 140184451094336 submission_runner.py:408] Time since start: 57766.71s, 	Step: 112394, 	{'train/accuracy': 0.7471093535423279, 'train/loss': 1.0021445751190186, 'validation/accuracy': 0.6795799732208252, 'validation/loss': 1.3151804208755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 1.9630584716796875, 'test/num_examples': 10000, 'score': 51715.62279224396, 'total_duration': 57766.71099972725, 'accumulated_submission_time': 51715.62279224396, 'accumulated_eval_time': 6040.666308164597, 'accumulated_logging_time': 4.571993350982666}
I0204 03:42:22.468397 140022518892288 logging_writer.py:48] [112394] accumulated_eval_time=6040.666308, accumulated_logging_time=4.571993, accumulated_submission_time=51715.622792, global_step=112394, preemption_count=0, score=51715.622792, test/accuracy=0.552300, test/loss=1.963058, test/num_examples=10000, total_duration=57766.711000, train/accuracy=0.747109, train/loss=1.002145, validation/accuracy=0.679580, validation/loss=1.315180, validation/num_examples=50000
I0204 03:42:25.397833 140023005427456 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.5973328351974487, loss=2.106330394744873
I0204 03:43:08.683392 140022518892288 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.5376064777374268, loss=2.0648703575134277
I0204 03:43:54.829771 140023005427456 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.551999807357788, loss=2.4938716888427734
I0204 03:44:41.099086 140022518892288 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.5886356830596924, loss=2.034061908721924
I0204 03:45:27.323957 140023005427456 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.7991191148757935, loss=1.9754154682159424
I0204 03:46:13.783601 140022518892288 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.4149515628814697, loss=2.192284107208252
I0204 03:46:59.859255 140023005427456 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.653590202331543, loss=2.082685947418213
I0204 03:47:46.458938 140022518892288 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.5603065490722656, loss=2.9222705364227295
I0204 03:48:32.672928 140023005427456 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.976525902748108, loss=2.0993833541870117
I0204 03:49:18.874532 140022518892288 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.3168466091156006, loss=3.860712766647339
I0204 03:49:22.719743 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:49:33.600078 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:50:08.595658 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:50:10.200144 140184451094336 submission_runner.py:408] Time since start: 58234.48s, 	Step: 113310, 	{'train/accuracy': 0.7491015195846558, 'train/loss': 1.0073623657226562, 'validation/accuracy': 0.6755200028419495, 'validation/loss': 1.3467522859573364, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.0030879974365234, 'test/num_examples': 10000, 'score': 52135.815131664276, 'total_duration': 58234.475531339645, 'accumulated_submission_time': 52135.815131664276, 'accumulated_eval_time': 6088.146703958511, 'accumulated_logging_time': 4.616669178009033}
I0204 03:50:10.232523 140023005427456 logging_writer.py:48] [113310] accumulated_eval_time=6088.146704, accumulated_logging_time=4.616669, accumulated_submission_time=52135.815132, global_step=113310, preemption_count=0, score=52135.815132, test/accuracy=0.552300, test/loss=2.003088, test/num_examples=10000, total_duration=58234.475531, train/accuracy=0.749102, train/loss=1.007362, validation/accuracy=0.675520, validation/loss=1.346752, validation/num_examples=50000
I0204 03:50:48.986032 140022518892288 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.4252434968948364, loss=3.4321742057800293
I0204 03:51:35.481370 140023005427456 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.5290133953094482, loss=2.9009203910827637
I0204 03:52:21.879510 140022518892288 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.6563975811004639, loss=1.9893871545791626
I0204 03:53:08.142110 140023005427456 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.621452808380127, loss=2.10736346244812
I0204 03:53:54.174538 140022518892288 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.7330660820007324, loss=2.654142379760742
I0204 03:54:40.484021 140023005427456 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.5325309038162231, loss=3.609762668609619
I0204 03:55:26.993556 140022518892288 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.7921191453933716, loss=2.284463882446289
I0204 03:56:13.259313 140023005427456 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.5498130321502686, loss=2.0662789344787598
I0204 03:56:59.375850 140022518892288 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.5779844522476196, loss=2.0258660316467285
I0204 03:57:10.734742 140184451094336 spec.py:321] Evaluating on the training split.
I0204 03:57:21.415595 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 03:57:59.438905 140184451094336 spec.py:349] Evaluating on the test split.
I0204 03:58:01.036667 140184451094336 submission_runner.py:408] Time since start: 58705.31s, 	Step: 114226, 	{'train/accuracy': 0.7378710508346558, 'train/loss': 1.050584316253662, 'validation/accuracy': 0.6758399605751038, 'validation/loss': 1.3396703004837036, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.981272578239441, 'test/num_examples': 10000, 'score': 52556.26023578644, 'total_duration': 58705.31205654144, 'accumulated_submission_time': 52556.26023578644, 'accumulated_eval_time': 6138.448607206345, 'accumulated_logging_time': 4.658712863922119}
I0204 03:58:01.071928 140023005427456 logging_writer.py:48] [114226] accumulated_eval_time=6138.448607, accumulated_logging_time=4.658713, accumulated_submission_time=52556.260236, global_step=114226, preemption_count=0, score=52556.260236, test/accuracy=0.555000, test/loss=1.981273, test/num_examples=10000, total_duration=58705.312057, train/accuracy=0.737871, train/loss=1.050584, validation/accuracy=0.675840, validation/loss=1.339670, validation/num_examples=50000
I0204 03:58:32.519079 140022518892288 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.6995095014572144, loss=1.927309274673462
I0204 03:59:18.342397 140023005427456 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.5587257146835327, loss=3.322885513305664
I0204 04:00:04.784753 140022518892288 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.5037935972213745, loss=3.047990560531616
I0204 04:00:51.060165 140023005427456 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.8206958770751953, loss=2.0970802307128906
I0204 04:01:37.768415 140022518892288 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.6820422410964966, loss=2.0588064193725586
I0204 04:02:24.378017 140023005427456 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.8072240352630615, loss=2.042886257171631
I0204 04:03:10.825795 140022518892288 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.6745256185531616, loss=1.987175703048706
I0204 04:03:56.836951 140023005427456 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.6893140077590942, loss=2.1840932369232178
I0204 04:04:43.367075 140022518892288 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.4891403913497925, loss=2.137840986251831
I0204 04:05:01.165871 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:05:11.878926 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:05:50.545740 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:05:52.142803 140184451094336 submission_runner.py:408] Time since start: 59176.42s, 	Step: 115140, 	{'train/accuracy': 0.7449804544448853, 'train/loss': 1.0316308736801147, 'validation/accuracy': 0.6810399889945984, 'validation/loss': 1.3292573690414429, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 1.9811760187149048, 'test/num_examples': 10000, 'score': 52976.29587888718, 'total_duration': 59176.41819024086, 'accumulated_submission_time': 52976.29587888718, 'accumulated_eval_time': 6189.425545454025, 'accumulated_logging_time': 4.7043726444244385}
I0204 04:05:52.178698 140023005427456 logging_writer.py:48] [115140] accumulated_eval_time=6189.425545, accumulated_logging_time=4.704373, accumulated_submission_time=52976.295879, global_step=115140, preemption_count=0, score=52976.295879, test/accuracy=0.554900, test/loss=1.981176, test/num_examples=10000, total_duration=59176.418190, train/accuracy=0.744980, train/loss=1.031631, validation/accuracy=0.681040, validation/loss=1.329257, validation/num_examples=50000
I0204 04:06:17.648193 140022518892288 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.6818181276321411, loss=1.973595380783081
I0204 04:07:03.123768 140023005427456 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.6164690256118774, loss=2.1814918518066406
I0204 04:07:49.647994 140022518892288 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.708264946937561, loss=2.1000590324401855
I0204 04:08:35.840216 140023005427456 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.6236671209335327, loss=4.057858467102051
I0204 04:09:22.360709 140022518892288 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.7631244659423828, loss=2.0334131717681885
I0204 04:10:08.701968 140023005427456 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.576431155204773, loss=2.586193561553955
I0204 04:10:54.997089 140022518892288 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.6808115243911743, loss=1.98533034324646
I0204 04:11:41.504770 140023005427456 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.596380352973938, loss=2.387549877166748
I0204 04:12:28.306625 140022518892288 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.3897192478179932, loss=3.976489543914795
I0204 04:12:52.387473 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:13:03.340825 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:13:42.159322 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:13:43.749145 140184451094336 submission_runner.py:408] Time since start: 59648.02s, 	Step: 116054, 	{'train/accuracy': 0.7593359351158142, 'train/loss': 0.968052327632904, 'validation/accuracy': 0.6818599700927734, 'validation/loss': 1.3153448104858398, 'validation/num_examples': 50000, 'test/accuracy': 0.5635000467300415, 'test/loss': 1.9470332860946655, 'test/num_examples': 10000, 'score': 53396.448315382004, 'total_duration': 59648.02452993393, 'accumulated_submission_time': 53396.448315382004, 'accumulated_eval_time': 6240.787206888199, 'accumulated_logging_time': 4.749409198760986}
I0204 04:13:43.784596 140023005427456 logging_writer.py:48] [116054] accumulated_eval_time=6240.787207, accumulated_logging_time=4.749409, accumulated_submission_time=53396.448315, global_step=116054, preemption_count=0, score=53396.448315, test/accuracy=0.563500, test/loss=1.947033, test/num_examples=10000, total_duration=59648.024530, train/accuracy=0.759336, train/loss=0.968052, validation/accuracy=0.681860, validation/loss=1.315345, validation/num_examples=50000
I0204 04:14:03.404494 140022518892288 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.5923774242401123, loss=2.256995439529419
I0204 04:14:48.372211 140023005427456 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.493087649345398, loss=3.40761137008667
I0204 04:15:34.715445 140022518892288 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.806473970413208, loss=2.634805202484131
I0204 04:16:21.080744 140023005427456 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.4897302389144897, loss=4.253101348876953
I0204 04:17:07.258647 140022518892288 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.8414870500564575, loss=2.042386770248413
I0204 04:17:53.681337 140023005427456 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.7799686193466187, loss=2.2332804203033447
I0204 04:18:39.795935 140022518892288 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.493906021118164, loss=3.3335189819335938
I0204 04:19:26.188518 140023005427456 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.725199818611145, loss=2.0287840366363525
I0204 04:20:12.409657 140022518892288 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.497267484664917, loss=4.069502830505371
I0204 04:20:44.113060 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:20:54.735371 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:21:33.381011 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:21:34.981509 140184451094336 submission_runner.py:408] Time since start: 60119.26s, 	Step: 116970, 	{'train/accuracy': 0.7439843416213989, 'train/loss': 1.0527466535568237, 'validation/accuracy': 0.6826399564743042, 'validation/loss': 1.3293102979660034, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 1.9881199598312378, 'test/num_examples': 10000, 'score': 53816.71623015404, 'total_duration': 60119.25689291954, 'accumulated_submission_time': 53816.71623015404, 'accumulated_eval_time': 6291.6556515693665, 'accumulated_logging_time': 4.797953367233276}
I0204 04:21:35.017443 140023005427456 logging_writer.py:48] [116970] accumulated_eval_time=6291.655652, accumulated_logging_time=4.797953, accumulated_submission_time=53816.716230, global_step=116970, preemption_count=0, score=53816.716230, test/accuracy=0.559100, test/loss=1.988120, test/num_examples=10000, total_duration=60119.256893, train/accuracy=0.743984, train/loss=1.052747, validation/accuracy=0.682640, validation/loss=1.329310, validation/num_examples=50000
I0204 04:21:47.971101 140022518892288 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.7251149415969849, loss=2.080479383468628
I0204 04:22:32.442230 140023005427456 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.492702841758728, loss=3.5669212341308594
I0204 04:23:18.687608 140022518892288 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.743348479270935, loss=2.191074848175049
I0204 04:24:04.978246 140023005427456 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.00785756111145, loss=2.06850266456604
I0204 04:24:51.031330 140022518892288 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.763588786125183, loss=1.9918513298034668
I0204 04:25:37.240471 140023005427456 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.620876669883728, loss=1.8730369806289673
I0204 04:26:23.686343 140022518892288 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.7352505922317505, loss=1.9225420951843262
I0204 04:27:10.201273 140023005427456 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.4946950674057007, loss=2.9844839572906494
I0204 04:27:56.582669 140022518892288 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.6380503177642822, loss=4.3125834465026855
I0204 04:28:35.048161 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:28:45.843542 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:29:22.759513 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:29:24.370070 140184451094336 submission_runner.py:408] Time since start: 60588.65s, 	Step: 117884, 	{'train/accuracy': 0.748828113079071, 'train/loss': 1.0334590673446655, 'validation/accuracy': 0.6802399754524231, 'validation/loss': 1.337364912033081, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9683139324188232, 'test/num_examples': 10000, 'score': 54236.68958616257, 'total_duration': 60588.64545035362, 'accumulated_submission_time': 54236.68958616257, 'accumulated_eval_time': 6340.977566003799, 'accumulated_logging_time': 4.843794584274292}
I0204 04:29:24.406516 140023005427456 logging_writer.py:48] [117884] accumulated_eval_time=6340.977566, accumulated_logging_time=4.843795, accumulated_submission_time=54236.689586, global_step=117884, preemption_count=0, score=54236.689586, test/accuracy=0.562100, test/loss=1.968314, test/num_examples=10000, total_duration=60588.645450, train/accuracy=0.748828, train/loss=1.033459, validation/accuracy=0.680240, validation/loss=1.337365, validation/num_examples=50000
I0204 04:29:31.500529 140022518892288 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.4725738763809204, loss=3.6032981872558594
I0204 04:30:15.275006 140023005427456 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.5611522197723389, loss=3.4245684146881104
I0204 04:31:01.504225 140022518892288 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.543034553527832, loss=4.564085483551025
I0204 04:31:48.256234 140023005427456 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.3991811275482178, loss=4.465499401092529
I0204 04:32:34.259973 140022518892288 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.7645491361618042, loss=2.0973124504089355
I0204 04:33:20.565315 140023005427456 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.609449028968811, loss=4.049048900604248
I0204 04:34:06.826664 140022518892288 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.5480762720108032, loss=3.850264310836792
I0204 04:34:53.213015 140023005427456 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.4293699264526367, loss=2.9878695011138916
I0204 04:35:39.604296 140022518892288 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.5917173624038696, loss=3.1766908168792725
I0204 04:36:24.544289 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:36:35.353430 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:37:12.513534 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:37:14.130241 140184451094336 submission_runner.py:408] Time since start: 61058.41s, 	Step: 118799, 	{'train/accuracy': 0.7620312571525574, 'train/loss': 0.9354296922683716, 'validation/accuracy': 0.6877599954605103, 'validation/loss': 1.279233694076538, 'validation/num_examples': 50000, 'test/accuracy': 0.5672000050544739, 'test/loss': 1.9073601961135864, 'test/num_examples': 10000, 'score': 54656.7682967186, 'total_duration': 61058.40561413765, 'accumulated_submission_time': 54656.7682967186, 'accumulated_eval_time': 6390.563494682312, 'accumulated_logging_time': 4.891946077346802}
I0204 04:37:14.168493 140023005427456 logging_writer.py:48] [118799] accumulated_eval_time=6390.563495, accumulated_logging_time=4.891946, accumulated_submission_time=54656.768297, global_step=118799, preemption_count=0, score=54656.768297, test/accuracy=0.567200, test/loss=1.907360, test/num_examples=10000, total_duration=61058.405614, train/accuracy=0.762031, train/loss=0.935430, validation/accuracy=0.687760, validation/loss=1.279234, validation/num_examples=50000
I0204 04:37:15.008019 140022518892288 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.9637658596038818, loss=2.0277938842773438
I0204 04:37:58.085644 140023005427456 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.656341791152954, loss=3.546327590942383
I0204 04:38:44.262087 140022518892288 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.6238524913787842, loss=4.287160396575928
I0204 04:39:30.654912 140023005427456 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.9448044300079346, loss=1.975104570388794
I0204 04:40:17.062445 140022518892288 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.8131306171417236, loss=2.014787435531616
I0204 04:41:03.453880 140023005427456 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.6189483404159546, loss=4.115932464599609
I0204 04:41:49.872471 140022518892288 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.7371273040771484, loss=2.018889904022217
I0204 04:42:36.307971 140023005427456 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.8344672918319702, loss=1.9697725772857666
I0204 04:43:22.494422 140022518892288 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.086172342300415, loss=2.3655290603637695
I0204 04:44:08.586276 140023005427456 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.6743916273117065, loss=2.432990074157715
I0204 04:44:14.213040 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:44:25.206525 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:45:01.111321 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:45:02.709458 140184451094336 submission_runner.py:408] Time since start: 61526.98s, 	Step: 119714, 	{'train/accuracy': 0.7458398342132568, 'train/loss': 1.0181658267974854, 'validation/accuracy': 0.6830599904060364, 'validation/loss': 1.3007621765136719, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9275920391082764, 'test/num_examples': 10000, 'score': 55076.75383043289, 'total_duration': 61526.98484683037, 'accumulated_submission_time': 55076.75383043289, 'accumulated_eval_time': 6439.0599138736725, 'accumulated_logging_time': 4.941753149032593}
I0204 04:45:02.746666 140022518892288 logging_writer.py:48] [119714] accumulated_eval_time=6439.059914, accumulated_logging_time=4.941753, accumulated_submission_time=55076.753830, global_step=119714, preemption_count=0, score=55076.753830, test/accuracy=0.561300, test/loss=1.927592, test/num_examples=10000, total_duration=61526.984847, train/accuracy=0.745840, train/loss=1.018166, validation/accuracy=0.683060, validation/loss=1.300762, validation/num_examples=50000
I0204 04:45:39.777942 140023005427456 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.6130813360214233, loss=4.2810564041137695
I0204 04:46:25.771776 140022518892288 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.4797217845916748, loss=4.512434482574463
I0204 04:47:11.737699 140023005427456 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.6857106685638428, loss=2.153903007507324
I0204 04:47:57.944632 140022518892288 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.6032207012176514, loss=4.39436674118042
I0204 04:48:44.330952 140023005427456 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.5693094730377197, loss=2.200223922729492
I0204 04:49:30.588911 140022518892288 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.5256255865097046, loss=3.814223527908325
I0204 04:50:17.044655 140023005427456 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.7810994386672974, loss=2.070035696029663
I0204 04:51:03.216256 140022518892288 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.924584984779358, loss=2.041757345199585
I0204 04:51:49.972074 140023005427456 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.5791209936141968, loss=3.972292423248291
I0204 04:52:02.742389 140184451094336 spec.py:321] Evaluating on the training split.
I0204 04:52:13.437555 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 04:52:51.053101 140184451094336 spec.py:349] Evaluating on the test split.
I0204 04:52:52.658115 140184451094336 submission_runner.py:408] Time since start: 61996.93s, 	Step: 120629, 	{'train/accuracy': 0.7542773485183716, 'train/loss': 0.9806972146034241, 'validation/accuracy': 0.6851599812507629, 'validation/loss': 1.289842963218689, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 1.920379638671875, 'test/num_examples': 10000, 'score': 55496.691744565964, 'total_duration': 61996.933457136154, 'accumulated_submission_time': 55496.691744565964, 'accumulated_eval_time': 6488.97558426857, 'accumulated_logging_time': 4.9889538288116455}
I0204 04:52:52.696567 140022518892288 logging_writer.py:48] [120629] accumulated_eval_time=6488.975584, accumulated_logging_time=4.988954, accumulated_submission_time=55496.691745, global_step=120629, preemption_count=0, score=55496.691745, test/accuracy=0.561000, test/loss=1.920380, test/num_examples=10000, total_duration=61996.933457, train/accuracy=0.754277, train/loss=0.980697, validation/accuracy=0.685160, validation/loss=1.289843, validation/num_examples=50000
I0204 04:53:22.952293 140023005427456 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.5790373086929321, loss=2.8159499168395996
I0204 04:54:09.141409 140022518892288 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.53086256980896, loss=4.269198417663574
I0204 04:54:55.767195 140023005427456 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.770561933517456, loss=2.0653905868530273
I0204 04:55:42.317497 140022518892288 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.7312750816345215, loss=1.8842653036117554
I0204 04:56:29.073588 140023005427456 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.6060715913772583, loss=2.5831661224365234
I0204 04:57:15.884779 140022518892288 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.5031200647354126, loss=3.1959118843078613
I0204 04:58:02.346242 140023005427456 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.6321015357971191, loss=2.2395384311676025
I0204 04:58:48.871482 140022518892288 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.65027916431427, loss=2.3503384590148926
I0204 04:59:35.459213 140023005427456 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.5727535486221313, loss=2.388084650039673
I0204 04:59:52.817796 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:00:03.672157 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:00:40.581918 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:00:42.181675 140184451094336 submission_runner.py:408] Time since start: 62466.46s, 	Step: 121539, 	{'train/accuracy': 0.7638280987739563, 'train/loss': 0.9494880437850952, 'validation/accuracy': 0.6881799697875977, 'validation/loss': 1.2904366254806519, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.9232209920883179, 'test/num_examples': 10000, 'score': 55916.75475072861, 'total_duration': 62466.457062006, 'accumulated_submission_time': 55916.75475072861, 'accumulated_eval_time': 6538.339520454407, 'accumulated_logging_time': 5.038301229476929}
I0204 05:00:42.217714 140022518892288 logging_writer.py:48] [121539] accumulated_eval_time=6538.339520, accumulated_logging_time=5.038301, accumulated_submission_time=55916.754751, global_step=121539, preemption_count=0, score=55916.754751, test/accuracy=0.570800, test/loss=1.923221, test/num_examples=10000, total_duration=62466.457062, train/accuracy=0.763828, train/loss=0.949488, validation/accuracy=0.688180, validation/loss=1.290437, validation/num_examples=50000
I0204 05:01:08.105320 140023005427456 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.473724365234375, loss=3.843413829803467
I0204 05:01:53.677399 140022518892288 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.5266990661621094, loss=3.3905081748962402
I0204 05:02:39.916680 140023005427456 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.7119370698928833, loss=2.0714478492736816
I0204 05:03:26.218282 140022518892288 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.6609548330307007, loss=2.281532049179077
I0204 05:04:12.695535 140023005427456 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.7039729356765747, loss=2.0535805225372314
I0204 05:04:59.064400 140022518892288 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.6488184928894043, loss=4.070705890655518
I0204 05:05:45.481040 140023005427456 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.795644760131836, loss=1.9034993648529053
I0204 05:06:31.968610 140022518892288 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.9478956460952759, loss=2.0452494621276855
I0204 05:07:18.502961 140023005427456 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.7405345439910889, loss=1.922436237335205
I0204 05:07:42.564471 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:07:53.282066 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:08:30.894776 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:08:32.493758 140184451094336 submission_runner.py:408] Time since start: 62936.77s, 	Step: 122454, 	{'train/accuracy': 0.753613293170929, 'train/loss': 1.002234935760498, 'validation/accuracy': 0.6880599856376648, 'validation/loss': 1.3044852018356323, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 1.932502269744873, 'test/num_examples': 10000, 'score': 56337.04533982277, 'total_duration': 62936.76914644241, 'accumulated_submission_time': 56337.04533982277, 'accumulated_eval_time': 6588.268809556961, 'accumulated_logging_time': 5.083476781845093}
I0204 05:08:32.527515 140022518892288 logging_writer.py:48] [122454] accumulated_eval_time=6588.268810, accumulated_logging_time=5.083477, accumulated_submission_time=56337.045340, global_step=122454, preemption_count=0, score=56337.045340, test/accuracy=0.565900, test/loss=1.932502, test/num_examples=10000, total_duration=62936.769146, train/accuracy=0.753613, train/loss=1.002235, validation/accuracy=0.688060, validation/loss=1.304485, validation/num_examples=50000
I0204 05:08:52.152714 140023005427456 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.645822525024414, loss=4.495570659637451
I0204 05:09:37.218629 140022518892288 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.7146528959274292, loss=2.9047346115112305
I0204 05:10:23.531130 140023005427456 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.8487157821655273, loss=2.9455108642578125
I0204 05:11:09.988031 140022518892288 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.739688515663147, loss=4.4449567794799805
I0204 05:11:56.383556 140023005427456 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.529245376586914, loss=3.8759355545043945
I0204 05:12:42.619831 140022518892288 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.5110973119735718, loss=4.38395357131958
I0204 05:13:29.034427 140023005427456 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.7990264892578125, loss=2.006256103515625
I0204 05:14:15.320965 140022518892288 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.849950909614563, loss=2.3291656970977783
I0204 05:15:01.596252 140023005427456 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.932243824005127, loss=1.9938806295394897
I0204 05:15:32.789172 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:15:43.314342 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:16:19.023292 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:16:20.623424 140184451094336 submission_runner.py:408] Time since start: 63404.90s, 	Step: 123369, 	{'train/accuracy': 0.7582421898841858, 'train/loss': 0.9696675539016724, 'validation/accuracy': 0.692579984664917, 'validation/loss': 1.262717843055725, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.9009531736373901, 'test/num_examples': 10000, 'score': 56757.24954080582, 'total_duration': 63404.89881134033, 'accumulated_submission_time': 56757.24954080582, 'accumulated_eval_time': 6636.103061914444, 'accumulated_logging_time': 5.127235651016235}
I0204 05:16:20.661280 140022518892288 logging_writer.py:48] [123369] accumulated_eval_time=6636.103062, accumulated_logging_time=5.127236, accumulated_submission_time=56757.249541, global_step=123369, preemption_count=0, score=56757.249541, test/accuracy=0.574800, test/loss=1.900953, test/num_examples=10000, total_duration=63404.898811, train/accuracy=0.758242, train/loss=0.969668, validation/accuracy=0.692580, validation/loss=1.262718, validation/num_examples=50000
I0204 05:16:34.033123 140023005427456 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.7764965295791626, loss=3.5260837078094482
I0204 05:17:18.297123 140022518892288 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.9234050512313843, loss=3.012960433959961
I0204 05:18:04.617131 140023005427456 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.0752763748168945, loss=1.9409898519515991
I0204 05:18:50.673076 140022518892288 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.6245875358581543, loss=4.37844181060791
I0204 05:19:37.051279 140023005427456 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.9202090501785278, loss=2.137454032897949
I0204 05:20:23.461603 140022518892288 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.9207550287246704, loss=2.694936990737915
I0204 05:21:09.704434 140023005427456 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.8060530424118042, loss=1.9940544366836548
I0204 05:21:56.021442 140022518892288 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.034778118133545, loss=4.616211891174316
I0204 05:22:42.258031 140023005427456 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.8428359031677246, loss=1.8358241319656372
I0204 05:23:20.809202 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:23:31.308070 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:24:05.647226 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:24:07.246750 140184451094336 submission_runner.py:408] Time since start: 63871.52s, 	Step: 124285, 	{'train/accuracy': 0.7618163824081421, 'train/loss': 0.9538049101829529, 'validation/accuracy': 0.6948999762535095, 'validation/loss': 1.2704392671585083, 'validation/num_examples': 50000, 'test/accuracy': 0.5712000131607056, 'test/loss': 1.8988127708435059, 'test/num_examples': 10000, 'score': 57177.33669304848, 'total_duration': 63871.522136449814, 'accumulated_submission_time': 57177.33669304848, 'accumulated_eval_time': 6682.54062128067, 'accumulated_logging_time': 5.174166440963745}
I0204 05:24:07.281489 140022518892288 logging_writer.py:48] [124285] accumulated_eval_time=6682.540621, accumulated_logging_time=5.174166, accumulated_submission_time=57177.336693, global_step=124285, preemption_count=0, score=57177.336693, test/accuracy=0.571200, test/loss=1.898813, test/num_examples=10000, total_duration=63871.522136, train/accuracy=0.761816, train/loss=0.953805, validation/accuracy=0.694900, validation/loss=1.270439, validation/num_examples=50000
I0204 05:24:13.963836 140023005427456 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.5939579010009766, loss=4.53817081451416
I0204 05:24:57.597509 140022518892288 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.7445393800735474, loss=2.143137216567993
I0204 05:25:43.919471 140023005427456 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.671688437461853, loss=2.981194257736206
I0204 05:26:30.369830 140022518892288 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.889160394668579, loss=2.004716396331787
I0204 05:27:16.533129 140023005427456 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.6834537982940674, loss=3.2635741233825684
I0204 05:28:02.943885 140022518892288 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.7444474697113037, loss=3.5593674182891846
I0204 05:28:49.246634 140023005427456 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.9083216190338135, loss=4.555889129638672
I0204 05:29:35.781290 140022518892288 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.9260330200195312, loss=3.3464515209198
I0204 05:30:22.469091 140023005427456 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.9860540628433228, loss=4.0849761962890625
I0204 05:31:07.293809 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:31:18.236213 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:31:56.556441 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:31:58.160379 140184451094336 submission_runner.py:408] Time since start: 64342.44s, 	Step: 125198, 	{'train/accuracy': 0.7659569978713989, 'train/loss': 0.9690631628036499, 'validation/accuracy': 0.6935399770736694, 'validation/loss': 1.2955999374389648, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.9468671083450317, 'test/num_examples': 10000, 'score': 57597.292941093445, 'total_duration': 64342.43574547768, 'accumulated_submission_time': 57597.292941093445, 'accumulated_eval_time': 6733.407159566879, 'accumulated_logging_time': 5.218145370483398}
I0204 05:31:58.204404 140022518892288 logging_writer.py:48] [125198] accumulated_eval_time=6733.407160, accumulated_logging_time=5.218145, accumulated_submission_time=57597.292941, global_step=125198, preemption_count=0, score=57597.292941, test/accuracy=0.573300, test/loss=1.946867, test/num_examples=10000, total_duration=64342.435745, train/accuracy=0.765957, train/loss=0.969063, validation/accuracy=0.693540, validation/loss=1.295600, validation/num_examples=50000
I0204 05:31:59.467615 140023005427456 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.4723488092422485, loss=2.800050973892212
I0204 05:32:42.321424 140022518892288 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.7486119270324707, loss=1.913390040397644
I0204 05:33:28.568727 140023005427456 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.5613497495651245, loss=2.384927272796631
I0204 05:34:15.121446 140022518892288 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.5410267114639282, loss=3.0744235515594482
I0204 05:35:01.455559 140023005427456 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.178933620452881, loss=1.8790194988250732
I0204 05:35:47.616994 140022518892288 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.9553601741790771, loss=1.8136861324310303
I0204 05:36:34.225056 140023005427456 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.735062599182129, loss=3.150312900543213
I0204 05:37:20.658491 140022518892288 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.9413235187530518, loss=2.0650179386138916
I0204 05:38:07.051599 140023005427456 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.821303129196167, loss=1.8905625343322754
I0204 05:38:53.419102 140022518892288 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.677166223526001, loss=1.7698874473571777
I0204 05:38:58.200169 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:39:08.987111 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:39:47.042284 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:39:48.644150 140184451094336 submission_runner.py:408] Time since start: 64812.92s, 	Step: 126112, 	{'train/accuracy': 0.7572070360183716, 'train/loss': 0.9875859618186951, 'validation/accuracy': 0.6926199793815613, 'validation/loss': 1.279220461845398, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9271174669265747, 'test/num_examples': 10000, 'score': 58017.22869372368, 'total_duration': 64812.9195394516, 'accumulated_submission_time': 58017.22869372368, 'accumulated_eval_time': 6783.851147174835, 'accumulated_logging_time': 5.274590492248535}
I0204 05:39:48.680159 140023005427456 logging_writer.py:48] [126112] accumulated_eval_time=6783.851147, accumulated_logging_time=5.274590, accumulated_submission_time=58017.228694, global_step=126112, preemption_count=0, score=58017.228694, test/accuracy=0.568000, test/loss=1.927117, test/num_examples=10000, total_duration=64812.919539, train/accuracy=0.757207, train/loss=0.987586, validation/accuracy=0.692620, validation/loss=1.279220, validation/num_examples=50000
I0204 05:40:26.643610 140022518892288 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.7114051580429077, loss=2.3236522674560547
I0204 05:41:12.634059 140023005427456 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.9736199378967285, loss=1.9378509521484375
I0204 05:41:59.464967 140022518892288 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.8054968118667603, loss=1.9175608158111572
I0204 05:42:45.908271 140023005427456 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.1074094772338867, loss=1.8805066347122192
I0204 05:43:32.230143 140022518892288 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.926058053970337, loss=1.905706524848938
I0204 05:44:18.638267 140023005427456 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.6109906435012817, loss=2.5633370876312256
I0204 05:45:05.050135 140022518892288 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.2008254528045654, loss=1.9066400527954102
I0204 05:45:51.262279 140023005427456 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.9380489587783813, loss=1.826867699623108
I0204 05:46:37.833847 140022518892288 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.8175592422485352, loss=2.1611220836639404
I0204 05:46:48.656477 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:46:59.237536 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:47:36.515923 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:47:38.122279 140184451094336 submission_runner.py:408] Time since start: 65282.40s, 	Step: 127025, 	{'train/accuracy': 0.7670117020606995, 'train/loss': 0.9423275589942932, 'validation/accuracy': 0.6972000002861023, 'validation/loss': 1.2566183805465698, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 1.893884301185608, 'test/num_examples': 10000, 'score': 58437.148782491684, 'total_duration': 65282.39766430855, 'accumulated_submission_time': 58437.148782491684, 'accumulated_eval_time': 6833.316943883896, 'accumulated_logging_time': 5.319833278656006}
I0204 05:47:38.160504 140023005427456 logging_writer.py:48] [127025] accumulated_eval_time=6833.316944, accumulated_logging_time=5.319833, accumulated_submission_time=58437.148782, global_step=127025, preemption_count=0, score=58437.148782, test/accuracy=0.573400, test/loss=1.893884, test/num_examples=10000, total_duration=65282.397664, train/accuracy=0.767012, train/loss=0.942328, validation/accuracy=0.697200, validation/loss=1.256618, validation/num_examples=50000
I0204 05:48:10.051668 140022518892288 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.8459820747375488, loss=2.1233997344970703
I0204 05:48:56.108007 140023005427456 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.209191083908081, loss=3.522498369216919
I0204 05:49:42.544587 140022518892288 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.939854621887207, loss=2.702549695968628
I0204 05:50:29.056041 140023005427456 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.8403528928756714, loss=1.8346408605575562
I0204 05:51:15.387471 140022518892288 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.0471761226654053, loss=1.8716421127319336
I0204 05:52:02.066641 140023005427456 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.053457498550415, loss=2.037883996963501
I0204 05:52:48.582354 140022518892288 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.9612082242965698, loss=1.855422854423523
I0204 05:53:34.893989 140023005427456 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.4954209327697754, loss=2.1725730895996094
I0204 05:54:21.316771 140022518892288 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.758863925933838, loss=4.09096622467041
I0204 05:54:38.532087 140184451094336 spec.py:321] Evaluating on the training split.
I0204 05:54:49.593502 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 05:55:26.356035 140184451094336 spec.py:349] Evaluating on the test split.
I0204 05:55:27.946933 140184451094336 submission_runner.py:408] Time since start: 65752.22s, 	Step: 127939, 	{'train/accuracy': 0.7809179425239563, 'train/loss': 0.883868932723999, 'validation/accuracy': 0.6997999548912048, 'validation/loss': 1.2388687133789062, 'validation/num_examples': 50000, 'test/accuracy': 0.5772000551223755, 'test/loss': 1.8826701641082764, 'test/num_examples': 10000, 'score': 58857.462636232376, 'total_duration': 65752.2223212719, 'accumulated_submission_time': 58857.462636232376, 'accumulated_eval_time': 6882.7317888736725, 'accumulated_logging_time': 5.368030786514282}
I0204 05:55:27.981997 140023005427456 logging_writer.py:48] [127939] accumulated_eval_time=6882.731789, accumulated_logging_time=5.368031, accumulated_submission_time=58857.462636, global_step=127939, preemption_count=0, score=58857.462636, test/accuracy=0.577200, test/loss=1.882670, test/num_examples=10000, total_duration=65752.222321, train/accuracy=0.780918, train/loss=0.883869, validation/accuracy=0.699800, validation/loss=1.238869, validation/num_examples=50000
I0204 05:55:53.871950 140022518892288 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.8273639678955078, loss=1.8348796367645264
I0204 05:56:39.629736 140023005427456 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.1876718997955322, loss=1.8424944877624512
I0204 05:57:25.954609 140022518892288 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.8019850254058838, loss=2.3322067260742188
I0204 05:58:12.132764 140023005427456 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.6671420335769653, loss=3.5453994274139404
I0204 05:58:58.328536 140022518892288 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.7774063348770142, loss=4.43389892578125
I0204 05:59:44.560098 140023005427456 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.661860466003418, loss=3.0900368690490723
I0204 06:00:30.853665 140022518892288 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.8896368741989136, loss=4.3160929679870605
I0204 06:01:17.261579 140023005427456 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.905648946762085, loss=2.053645133972168
I0204 06:02:03.671010 140022518892288 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.7618868350982666, loss=2.0133793354034424
I0204 06:02:28.042805 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:02:38.720505 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:03:13.083216 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:03:14.689079 140184451094336 submission_runner.py:408] Time since start: 66218.96s, 	Step: 128854, 	{'train/accuracy': 0.7710937261581421, 'train/loss': 0.9130802154541016, 'validation/accuracy': 0.7034199833869934, 'validation/loss': 1.210724949836731, 'validation/num_examples': 50000, 'test/accuracy': 0.582800030708313, 'test/loss': 1.8451893329620361, 'test/num_examples': 10000, 'score': 59277.466651916504, 'total_duration': 66218.96444773674, 'accumulated_submission_time': 59277.466651916504, 'accumulated_eval_time': 6929.378044605255, 'accumulated_logging_time': 5.412663459777832}
I0204 06:03:14.731883 140023005427456 logging_writer.py:48] [128854] accumulated_eval_time=6929.378045, accumulated_logging_time=5.412663, accumulated_submission_time=59277.466652, global_step=128854, preemption_count=0, score=59277.466652, test/accuracy=0.582800, test/loss=1.845189, test/num_examples=10000, total_duration=66218.964448, train/accuracy=0.771094, train/loss=0.913080, validation/accuracy=0.703420, validation/loss=1.210725, validation/num_examples=50000
I0204 06:03:34.379824 140022518892288 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.9609755277633667, loss=1.9790022373199463
I0204 06:04:19.356724 140023005427456 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.7688500881195068, loss=2.287750244140625
I0204 06:05:05.583195 140022518892288 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.9187909364700317, loss=2.3514583110809326
I0204 06:05:51.819805 140023005427456 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.0564117431640625, loss=1.8017351627349854
I0204 06:06:38.120296 140022518892288 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.8210299015045166, loss=1.8410675525665283
I0204 06:07:24.786839 140023005427456 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.812327265739441, loss=4.436377048492432
I0204 06:08:11.245247 140022518892288 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.5795462131500244, loss=4.152633190155029
I0204 06:08:57.798562 140023005427456 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.8821492195129395, loss=1.980966329574585
I0204 06:09:44.475063 140022518892288 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.195080041885376, loss=1.973236083984375
I0204 06:10:14.939886 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:10:25.590426 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:11:02.835616 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:11:04.444343 140184451094336 submission_runner.py:408] Time since start: 66688.72s, 	Step: 129767, 	{'train/accuracy': 0.7718359231948853, 'train/loss': 0.9090878367424011, 'validation/accuracy': 0.6992599964141846, 'validation/loss': 1.23419189453125, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.8769426345825195, 'test/num_examples': 10000, 'score': 59697.61399292946, 'total_duration': 66688.71971225739, 'accumulated_submission_time': 59697.61399292946, 'accumulated_eval_time': 6978.882484436035, 'accumulated_logging_time': 5.467691898345947}
I0204 06:11:04.480892 140023005427456 logging_writer.py:48] [129767] accumulated_eval_time=6978.882484, accumulated_logging_time=5.467692, accumulated_submission_time=59697.613993, global_step=129767, preemption_count=0, score=59697.613993, test/accuracy=0.573600, test/loss=1.876943, test/num_examples=10000, total_duration=66688.719712, train/accuracy=0.771836, train/loss=0.909088, validation/accuracy=0.699260, validation/loss=1.234192, validation/num_examples=50000
I0204 06:11:19.102664 140022518892288 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.8929452896118164, loss=2.9510910511016846
I0204 06:12:03.612630 140023005427456 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.0213873386383057, loss=1.9110147953033447
I0204 06:12:49.721096 140022518892288 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.0579733848571777, loss=2.1087169647216797
I0204 06:13:35.998681 140023005427456 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.2154064178466797, loss=1.8722872734069824
I0204 06:14:22.459266 140022518892288 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.8872877359390259, loss=1.9782556295394897
I0204 06:15:08.684926 140023005427456 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.7784618139266968, loss=3.0488829612731934
I0204 06:15:54.780940 140022518892288 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.7426815032958984, loss=2.696887493133545
I0204 06:16:40.993343 140023005427456 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.701535940170288, loss=2.3204352855682373
I0204 06:17:27.211486 140022518892288 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.0232174396514893, loss=2.0942115783691406
I0204 06:18:04.834314 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:18:16.388766 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:18:55.681747 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:18:57.286684 140184451094336 submission_runner.py:408] Time since start: 67161.56s, 	Step: 130683, 	{'train/accuracy': 0.7808593511581421, 'train/loss': 0.8642115592956543, 'validation/accuracy': 0.7039200067520142, 'validation/loss': 1.2181345224380493, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.8567651510238647, 'test/num_examples': 10000, 'score': 60117.49275612831, 'total_duration': 67161.56207251549, 'accumulated_submission_time': 60117.49275612831, 'accumulated_eval_time': 7031.3348553180695, 'accumulated_logging_time': 5.931267976760864}
I0204 06:18:57.326169 140023005427456 logging_writer.py:48] [130683] accumulated_eval_time=7031.334855, accumulated_logging_time=5.931268, accumulated_submission_time=60117.492756, global_step=130683, preemption_count=0, score=60117.492756, test/accuracy=0.580500, test/loss=1.856765, test/num_examples=10000, total_duration=67161.562073, train/accuracy=0.780859, train/loss=0.864212, validation/accuracy=0.703920, validation/loss=1.218135, validation/num_examples=50000
I0204 06:19:04.846675 140022518892288 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.86212158203125, loss=3.6765291690826416
I0204 06:19:48.554315 140023005427456 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.731516718864441, loss=2.6469826698303223
I0204 06:20:34.841306 140022518892288 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.0107667446136475, loss=1.9389426708221436
I0204 06:21:21.513081 140023005427456 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.913382887840271, loss=1.9290388822555542
I0204 06:22:08.143230 140022518892288 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.4551594257354736, loss=1.9920754432678223
I0204 06:22:54.461209 140023005427456 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.034557342529297, loss=4.550400733947754
I0204 06:23:40.836361 140022518892288 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.665434718132019, loss=2.635077476501465
I0204 06:24:27.309413 140023005427456 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.0481488704681396, loss=3.718956232070923
I0204 06:25:13.878968 140022518892288 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.629903793334961, loss=4.211179256439209
I0204 06:25:57.636678 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:26:08.298996 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:26:46.778582 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:26:48.376245 140184451094336 submission_runner.py:408] Time since start: 67632.65s, 	Step: 131596, 	{'train/accuracy': 0.7708203196525574, 'train/loss': 0.8994258642196655, 'validation/accuracy': 0.7038799524307251, 'validation/loss': 1.2020835876464844, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8415201902389526, 'test/num_examples': 10000, 'score': 60537.74565386772, 'total_duration': 67632.65162563324, 'accumulated_submission_time': 60537.74565386772, 'accumulated_eval_time': 7082.0744042396545, 'accumulated_logging_time': 5.981815576553345}
I0204 06:26:48.416904 140023005427456 logging_writer.py:48] [131596] accumulated_eval_time=7082.074404, accumulated_logging_time=5.981816, accumulated_submission_time=60537.745654, global_step=131596, preemption_count=0, score=60537.745654, test/accuracy=0.581900, test/loss=1.841520, test/num_examples=10000, total_duration=67632.651626, train/accuracy=0.770820, train/loss=0.899426, validation/accuracy=0.703880, validation/loss=1.202084, validation/num_examples=50000
I0204 06:26:50.516412 140022518892288 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.0099592208862305, loss=1.868199110031128
I0204 06:27:33.765656 140023005427456 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.000915765762329, loss=1.9046266078948975
I0204 06:28:19.846174 140022518892288 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.8160146474838257, loss=4.360508441925049
I0204 06:29:06.280162 140023005427456 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.823174238204956, loss=4.33383321762085
I0204 06:29:52.493518 140022518892288 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8953105211257935, loss=3.176663637161255
I0204 06:30:38.637017 140023005427456 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.9179507493972778, loss=3.59906005859375
I0204 06:31:25.017243 140022518892288 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.9166128635406494, loss=2.014178514480591
I0204 06:32:11.688745 140023005427456 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.9757843017578125, loss=2.1090598106384277
I0204 06:32:57.823629 140022518892288 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.8522557020187378, loss=2.817333459854126
I0204 06:33:44.215173 140023005427456 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.29244327545166, loss=1.896872639656067
I0204 06:33:48.519082 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:33:59.308582 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:34:37.805651 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:34:39.405444 140184451094336 submission_runner.py:408] Time since start: 68103.68s, 	Step: 132511, 	{'train/accuracy': 0.7748632431030273, 'train/loss': 0.8906111717224121, 'validation/accuracy': 0.7036799788475037, 'validation/loss': 1.2126256227493286, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 1.8380862474441528, 'test/num_examples': 10000, 'score': 60957.78933095932, 'total_duration': 68103.68083000183, 'accumulated_submission_time': 60957.78933095932, 'accumulated_eval_time': 7132.960758924484, 'accumulated_logging_time': 6.033755540847778}
I0204 06:34:39.442636 140022518892288 logging_writer.py:48] [132511] accumulated_eval_time=7132.960759, accumulated_logging_time=6.033756, accumulated_submission_time=60957.789331, global_step=132511, preemption_count=0, score=60957.789331, test/accuracy=0.583500, test/loss=1.838086, test/num_examples=10000, total_duration=68103.680830, train/accuracy=0.774863, train/loss=0.890611, validation/accuracy=0.703680, validation/loss=1.212626, validation/num_examples=50000
I0204 06:35:18.146859 140023005427456 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.133014440536499, loss=2.1063716411590576
I0204 06:36:04.507221 140022518892288 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.9172006845474243, loss=2.781083106994629
I0204 06:36:50.562020 140023005427456 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.100127935409546, loss=2.2063469886779785
I0204 06:37:36.963306 140022518892288 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.702512264251709, loss=1.7370966672897339
I0204 06:38:23.267891 140023005427456 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.947989583015442, loss=2.018826723098755
I0204 06:39:09.615724 140022518892288 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.7273719310760498, loss=3.0742921829223633
I0204 06:39:55.717582 140023005427456 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.7156790494918823, loss=2.498225450515747
I0204 06:40:42.015173 140022518892288 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.787475347518921, loss=2.7463245391845703
I0204 06:41:28.343725 140023005427456 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.9243730306625366, loss=1.88662588596344
I0204 06:41:39.595097 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:41:50.423405 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:42:24.349638 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:42:25.955449 140184451094336 submission_runner.py:408] Time since start: 68570.23s, 	Step: 133426, 	{'train/accuracy': 0.7884570360183716, 'train/loss': 0.8362293839454651, 'validation/accuracy': 0.7095800042152405, 'validation/loss': 1.1812160015106201, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.816677451133728, 'test/num_examples': 10000, 'score': 61377.8827316761, 'total_duration': 68570.23081469536, 'accumulated_submission_time': 61377.8827316761, 'accumulated_eval_time': 7179.321115255356, 'accumulated_logging_time': 6.082520008087158}
I0204 06:42:26.004207 140022518892288 logging_writer.py:48] [133426] accumulated_eval_time=7179.321115, accumulated_logging_time=6.082520, accumulated_submission_time=61377.882732, global_step=133426, preemption_count=0, score=61377.882732, test/accuracy=0.586600, test/loss=1.816677, test/num_examples=10000, total_duration=68570.230815, train/accuracy=0.788457, train/loss=0.836229, validation/accuracy=0.709580, validation/loss=1.181216, validation/num_examples=50000
I0204 06:42:57.329805 140023005427456 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.8917369842529297, loss=1.8270434141159058
I0204 06:43:43.218797 140022518892288 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.9773688316345215, loss=2.1831438541412354
I0204 06:44:29.396986 140023005427456 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.071326494216919, loss=1.9986079931259155
I0204 06:45:15.648689 140022518892288 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.7275768518447876, loss=1.7707587480545044
I0204 06:46:02.026280 140023005427456 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.8857864141464233, loss=1.8684629201889038
I0204 06:46:48.179916 140022518892288 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.826308012008667, loss=2.829594612121582
I0204 06:47:34.414626 140023005427456 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.8386179208755493, loss=2.597216844558716
I0204 06:48:20.849604 140022518892288 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.036137342453003, loss=1.7830156087875366
I0204 06:49:07.123940 140023005427456 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.6523749828338623, loss=3.3739969730377197
I0204 06:49:26.338336 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:49:37.148095 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:50:15.018853 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:50:16.629819 140184451094336 submission_runner.py:408] Time since start: 69040.91s, 	Step: 134343, 	{'train/accuracy': 0.7745312452316284, 'train/loss': 0.8990936279296875, 'validation/accuracy': 0.7057799696922302, 'validation/loss': 1.2014926671981812, 'validation/num_examples': 50000, 'test/accuracy': 0.5829000473022461, 'test/loss': 1.8295793533325195, 'test/num_examples': 10000, 'score': 61798.15866804123, 'total_duration': 69040.90519189835, 'accumulated_submission_time': 61798.15866804123, 'accumulated_eval_time': 7229.612575292587, 'accumulated_logging_time': 6.1410746574401855}
I0204 06:50:16.674414 140022518892288 logging_writer.py:48] [134343] accumulated_eval_time=7229.612575, accumulated_logging_time=6.141075, accumulated_submission_time=61798.158668, global_step=134343, preemption_count=0, score=61798.158668, test/accuracy=0.582900, test/loss=1.829579, test/num_examples=10000, total_duration=69040.905192, train/accuracy=0.774531, train/loss=0.899094, validation/accuracy=0.705780, validation/loss=1.201493, validation/num_examples=50000
I0204 06:50:40.899720 140023005427456 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.9049502611160278, loss=3.8121674060821533
I0204 06:51:26.350914 140022518892288 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.8115992546081543, loss=3.7551164627075195
I0204 06:52:12.778926 140023005427456 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.8796403408050537, loss=4.131015300750732
I0204 06:52:59.052549 140022518892288 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.1328773498535156, loss=4.306841850280762
I0204 06:53:45.568650 140023005427456 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.0677902698516846, loss=1.7768635749816895
I0204 06:54:31.799620 140022518892288 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.9527835845947266, loss=2.3295488357543945
I0204 06:55:18.369857 140023005427456 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.2287957668304443, loss=1.7389044761657715
I0204 06:56:04.792493 140022518892288 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.057220458984375, loss=1.870415210723877
I0204 06:56:51.378380 140023005427456 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.837420105934143, loss=2.254870891571045
I0204 06:57:16.740854 140184451094336 spec.py:321] Evaluating on the training split.
I0204 06:57:27.352285 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 06:58:04.089748 140184451094336 spec.py:349] Evaluating on the test split.
I0204 06:58:05.693470 140184451094336 submission_runner.py:408] Time since start: 69509.97s, 	Step: 135256, 	{'train/accuracy': 0.7787109017372131, 'train/loss': 0.886769711971283, 'validation/accuracy': 0.7080000042915344, 'validation/loss': 1.2056972980499268, 'validation/num_examples': 50000, 'test/accuracy': 0.5863000154495239, 'test/loss': 1.8409180641174316, 'test/num_examples': 10000, 'score': 62218.16717839241, 'total_duration': 69509.96885418892, 'accumulated_submission_time': 62218.16717839241, 'accumulated_eval_time': 7278.565196990967, 'accumulated_logging_time': 6.1958794593811035}
I0204 06:58:05.730690 140022518892288 logging_writer.py:48] [135256] accumulated_eval_time=7278.565197, accumulated_logging_time=6.195879, accumulated_submission_time=62218.167178, global_step=135256, preemption_count=0, score=62218.167178, test/accuracy=0.586300, test/loss=1.840918, test/num_examples=10000, total_duration=69509.968854, train/accuracy=0.778711, train/loss=0.886770, validation/accuracy=0.708000, validation/loss=1.205697, validation/num_examples=50000
I0204 06:58:24.535148 140023005427456 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.2523138523101807, loss=1.771458387374878
I0204 06:59:09.512075 140022518892288 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.5986173152923584, loss=3.474745273590088
I0204 06:59:55.813992 140023005427456 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.9645112752914429, loss=2.214667320251465
I0204 07:00:42.355227 140022518892288 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.1106369495391846, loss=1.8591973781585693
I0204 07:01:28.565096 140023005427456 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.7938565015792847, loss=2.7473483085632324
I0204 07:02:15.127155 140022518892288 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.147631883621216, loss=1.9326024055480957
I0204 07:03:01.224998 140023005427456 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.8777060508728027, loss=1.7253880500793457
I0204 07:03:47.639878 140022518892288 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.0948543548583984, loss=2.2136447429656982
I0204 07:04:34.034890 140023005427456 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.9870967864990234, loss=2.3635823726654053
I0204 07:05:05.901885 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:05:16.629510 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:05:52.579134 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:05:54.180678 140184451094336 submission_runner.py:408] Time since start: 69978.46s, 	Step: 136170, 	{'train/accuracy': 0.7862108945846558, 'train/loss': 0.8529349565505981, 'validation/accuracy': 0.7099399566650391, 'validation/loss': 1.196141004562378, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8285341262817383, 'test/num_examples': 10000, 'score': 62638.280061244965, 'total_duration': 69978.45606637001, 'accumulated_submission_time': 62638.280061244965, 'accumulated_eval_time': 7326.843999385834, 'accumulated_logging_time': 6.2438578605651855}
I0204 07:05:54.218316 140022518892288 logging_writer.py:48] [136170] accumulated_eval_time=7326.843999, accumulated_logging_time=6.243858, accumulated_submission_time=62638.280061, global_step=136170, preemption_count=0, score=62638.280061, test/accuracy=0.590700, test/loss=1.828534, test/num_examples=10000, total_duration=69978.456066, train/accuracy=0.786211, train/loss=0.852935, validation/accuracy=0.709940, validation/loss=1.196141, validation/num_examples=50000
I0204 07:06:07.376786 140023005427456 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.8002777099609375, loss=3.7614500522613525
I0204 07:06:51.600194 140022518892288 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.966752290725708, loss=1.6525790691375732
I0204 07:07:38.156883 140023005427456 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.8861069679260254, loss=3.88627290725708
I0204 07:08:24.610511 140022518892288 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.799386978149414, loss=2.1770994663238525
I0204 07:09:10.826974 140023005427456 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.899587631225586, loss=2.4034829139709473
I0204 07:09:57.075316 140022518892288 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.126512050628662, loss=1.8329455852508545
I0204 07:10:43.519392 140023005427456 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.1255173683166504, loss=1.8033642768859863
I0204 07:11:30.075906 140022518892288 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.8801785707473755, loss=3.7540359497070312
I0204 07:12:16.509045 140023005427456 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.9161523580551147, loss=1.8069623708724976
I0204 07:12:54.558328 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:13:05.423460 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:13:44.106619 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:13:45.726950 140184451094336 submission_runner.py:408] Time since start: 70450.00s, 	Step: 137084, 	{'train/accuracy': 0.7843554615974426, 'train/loss': 0.8620164394378662, 'validation/accuracy': 0.7106199860572815, 'validation/loss': 1.1773674488067627, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8074219226837158, 'test/num_examples': 10000, 'score': 63058.562294483185, 'total_duration': 70450.00231456757, 'accumulated_submission_time': 63058.562294483185, 'accumulated_eval_time': 7378.012609243393, 'accumulated_logging_time': 6.291548013687134}
I0204 07:13:45.770375 140022518892288 logging_writer.py:48] [137084] accumulated_eval_time=7378.012609, accumulated_logging_time=6.291548, accumulated_submission_time=63058.562294, global_step=137084, preemption_count=0, score=63058.562294, test/accuracy=0.590900, test/loss=1.807422, test/num_examples=10000, total_duration=70450.002315, train/accuracy=0.784355, train/loss=0.862016, validation/accuracy=0.710620, validation/loss=1.177367, validation/num_examples=50000
I0204 07:13:52.875905 140023005427456 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.2201545238494873, loss=1.7073547840118408
I0204 07:14:36.549083 140022518892288 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.9446672201156616, loss=2.531266689300537
I0204 07:15:22.645221 140023005427456 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.1919312477111816, loss=3.395580291748047
I0204 07:16:09.112249 140022518892288 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.0782036781311035, loss=2.057286500930786
I0204 07:16:55.355221 140023005427456 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.9645243883132935, loss=1.7124320268630981
I0204 07:17:41.810680 140022518892288 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.041048049926758, loss=2.3779420852661133
I0204 07:18:28.526100 140023005427456 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.7817068099975586, loss=4.261482238769531
I0204 07:19:14.969012 140022518892288 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.113276243209839, loss=1.992393970489502
I0204 07:20:01.471894 140023005427456 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.2014963626861572, loss=4.3194122314453125
I0204 07:20:46.180524 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:20:56.625220 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:21:33.884856 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:21:35.487527 140184451094336 submission_runner.py:408] Time since start: 70919.76s, 	Step: 137998, 	{'train/accuracy': 0.7840625047683716, 'train/loss': 0.8761175274848938, 'validation/accuracy': 0.7105000019073486, 'validation/loss': 1.1932687759399414, 'validation/num_examples': 50000, 'test/accuracy': 0.5921000242233276, 'test/loss': 1.8124998807907104, 'test/num_examples': 10000, 'score': 63478.914189100266, 'total_duration': 70919.76289439201, 'accumulated_submission_time': 63478.914189100266, 'accumulated_eval_time': 7427.31960606575, 'accumulated_logging_time': 6.34608793258667}
I0204 07:21:35.529482 140022518892288 logging_writer.py:48] [137998] accumulated_eval_time=7427.319606, accumulated_logging_time=6.346088, accumulated_submission_time=63478.914189, global_step=137998, preemption_count=0, score=63478.914189, test/accuracy=0.592100, test/loss=1.812500, test/num_examples=10000, total_duration=70919.762894, train/accuracy=0.784063, train/loss=0.876118, validation/accuracy=0.710500, validation/loss=1.193269, validation/num_examples=50000
I0204 07:21:36.793963 140023005427456 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.8134289979934692, loss=2.916893482208252
I0204 07:22:19.747020 140022518892288 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.03330659866333, loss=2.1643683910369873
I0204 07:23:06.062522 140023005427456 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.9584801197052002, loss=4.184933662414551
I0204 07:23:52.524196 140022518892288 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.0113024711608887, loss=2.9977028369903564
I0204 07:24:38.801593 140023005427456 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.851749300956726, loss=2.6627230644226074
I0204 07:25:25.353395 140022518892288 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.9027929306030273, loss=2.0814554691314697
I0204 07:26:11.690509 140023005427456 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.158325433731079, loss=2.215921401977539
I0204 07:26:57.850973 140022518892288 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.828575849533081, loss=3.100925922393799
I0204 07:27:44.029735 140023005427456 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.350991725921631, loss=2.086362838745117
I0204 07:28:30.565062 140022518892288 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.9140113592147827, loss=3.67242431640625
I0204 07:28:35.841778 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:28:46.454591 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:29:24.293525 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:29:25.918374 140184451094336 submission_runner.py:408] Time since start: 71390.19s, 	Step: 138913, 	{'train/accuracy': 0.7923241853713989, 'train/loss': 0.8205782771110535, 'validation/accuracy': 0.714199960231781, 'validation/loss': 1.164374589920044, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.7992960214614868, 'test/num_examples': 10000, 'score': 63899.1681098938, 'total_duration': 71390.19374513626, 'accumulated_submission_time': 63899.1681098938, 'accumulated_eval_time': 7477.3962070941925, 'accumulated_logging_time': 6.399160146713257}
I0204 07:29:25.961802 140023005427456 logging_writer.py:48] [138913] accumulated_eval_time=7477.396207, accumulated_logging_time=6.399160, accumulated_submission_time=63899.168110, global_step=138913, preemption_count=0, score=63899.168110, test/accuracy=0.595800, test/loss=1.799296, test/num_examples=10000, total_duration=71390.193745, train/accuracy=0.792324, train/loss=0.820578, validation/accuracy=0.714200, validation/loss=1.164375, validation/num_examples=50000
I0204 07:30:03.225145 140022518892288 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.227069854736328, loss=1.798573613166809
I0204 07:30:49.094621 140023005427456 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.189185857772827, loss=2.528230667114258
I0204 07:31:35.690834 140022518892288 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.089672088623047, loss=1.8876979351043701
I0204 07:32:21.791617 140023005427456 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.0771965980529785, loss=2.059368133544922
I0204 07:33:08.031170 140022518892288 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.1482315063476562, loss=1.8073420524597168
I0204 07:33:54.145052 140023005427456 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.8739707469940186, loss=4.00429630279541
I0204 07:34:40.513741 140022518892288 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.9563325643539429, loss=1.904977560043335
I0204 07:35:26.607698 140023005427456 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.0515077114105225, loss=1.7487516403198242
I0204 07:36:12.884225 140022518892288 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.9599201679229736, loss=2.6257095336914062
I0204 07:36:26.203902 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:36:37.094270 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:37:12.892292 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:37:14.500221 140184451094336 submission_runner.py:408] Time since start: 71858.78s, 	Step: 139830, 	{'train/accuracy': 0.8011132478713989, 'train/loss': 0.8038817644119263, 'validation/accuracy': 0.7127000093460083, 'validation/loss': 1.1908022165298462, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.8122637271881104, 'test/num_examples': 10000, 'score': 64319.3515021801, 'total_duration': 71858.77561020851, 'accumulated_submission_time': 64319.3515021801, 'accumulated_eval_time': 7525.692511558533, 'accumulated_logging_time': 6.453774929046631}
I0204 07:37:14.543686 140023005427456 logging_writer.py:48] [139830] accumulated_eval_time=7525.692512, accumulated_logging_time=6.453775, accumulated_submission_time=64319.351502, global_step=139830, preemption_count=0, score=64319.351502, test/accuracy=0.588800, test/loss=1.812264, test/num_examples=10000, total_duration=71858.775610, train/accuracy=0.801113, train/loss=0.803882, validation/accuracy=0.712700, validation/loss=1.190802, validation/num_examples=50000
I0204 07:37:44.393918 140022518892288 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.241654634475708, loss=1.9270374774932861
I0204 07:38:30.494765 140023005427456 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.8235456943511963, loss=1.7205480337142944
I0204 07:39:17.394581 140022518892288 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.9261817932128906, loss=1.7964346408843994
I0204 07:40:04.174163 140023005427456 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.3247411251068115, loss=1.9757757186889648
I0204 07:40:50.495033 140022518892288 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.053010940551758, loss=1.7566580772399902
I0204 07:41:37.538304 140023005427456 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.024554491043091, loss=1.9600467681884766
I0204 07:42:24.033206 140022518892288 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.2591514587402344, loss=1.8345015048980713
I0204 07:43:10.495610 140023005427456 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.116654634475708, loss=2.555823564529419
I0204 07:43:56.902969 140022518892288 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.221346139907837, loss=1.714760184288025
I0204 07:44:14.771761 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:44:26.056269 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:45:02.711401 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:45:04.318532 140184451094336 submission_runner.py:408] Time since start: 72328.59s, 	Step: 140740, 	{'train/accuracy': 0.7881835699081421, 'train/loss': 0.8385329246520996, 'validation/accuracy': 0.7159799933433533, 'validation/loss': 1.167323350906372, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7915613651275635, 'test/num_examples': 10000, 'score': 64739.52413749695, 'total_duration': 72328.59392142296, 'accumulated_submission_time': 64739.52413749695, 'accumulated_eval_time': 7575.239279747009, 'accumulated_logging_time': 6.506052732467651}
I0204 07:45:04.357594 140023005427456 logging_writer.py:48] [140740] accumulated_eval_time=7575.239280, accumulated_logging_time=6.506053, accumulated_submission_time=64739.524137, global_step=140740, preemption_count=0, score=64739.524137, test/accuracy=0.598600, test/loss=1.791561, test/num_examples=10000, total_duration=72328.593921, train/accuracy=0.788184, train/loss=0.838533, validation/accuracy=0.715980, validation/loss=1.167323, validation/num_examples=50000
I0204 07:45:29.829292 140022518892288 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.1895523071289062, loss=2.0447146892547607
I0204 07:46:15.561445 140023005427456 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.1614840030670166, loss=2.9318718910217285
I0204 07:47:01.809779 140022518892288 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.214571714401245, loss=1.7903716564178467
I0204 07:47:47.884100 140023005427456 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.1323347091674805, loss=1.6295210123062134
I0204 07:48:34.250064 140022518892288 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.080793857574463, loss=2.3158297538757324
I0204 07:49:20.554468 140023005427456 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.3653993606567383, loss=1.6882569789886475
I0204 07:50:07.232687 140022518892288 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.9279204607009888, loss=3.1301755905151367
I0204 07:50:53.820055 140023005427456 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.2722551822662354, loss=3.6600282192230225
I0204 07:51:40.301780 140022518892288 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.5653727054595947, loss=1.8002516031265259
I0204 07:52:04.581138 140184451094336 spec.py:321] Evaluating on the training split.
I0204 07:52:15.113184 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 07:52:50.364164 140184451094336 spec.py:349] Evaluating on the test split.
I0204 07:52:51.982093 140184451094336 submission_runner.py:408] Time since start: 72796.26s, 	Step: 141654, 	{'train/accuracy': 0.7936913967132568, 'train/loss': 0.8237582445144653, 'validation/accuracy': 0.7181000113487244, 'validation/loss': 1.1573004722595215, 'validation/num_examples': 50000, 'test/accuracy': 0.5964000225067139, 'test/loss': 1.7926957607269287, 'test/num_examples': 10000, 'score': 65159.690633773804, 'total_duration': 72796.25745105743, 'accumulated_submission_time': 65159.690633773804, 'accumulated_eval_time': 7622.6402060985565, 'accumulated_logging_time': 6.555207014083862}
I0204 07:52:52.029134 140023005427456 logging_writer.py:48] [141654] accumulated_eval_time=7622.640206, accumulated_logging_time=6.555207, accumulated_submission_time=65159.690634, global_step=141654, preemption_count=0, score=65159.690634, test/accuracy=0.596400, test/loss=1.792696, test/num_examples=10000, total_duration=72796.257451, train/accuracy=0.793691, train/loss=0.823758, validation/accuracy=0.718100, validation/loss=1.157300, validation/num_examples=50000
I0204 07:53:11.655373 140022518892288 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.2634785175323486, loss=1.9270092248916626
I0204 07:53:56.907057 140023005427456 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.1465253829956055, loss=2.6144561767578125
I0204 07:54:43.107820 140022518892288 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.2230522632598877, loss=1.8343617916107178
I0204 07:55:29.291024 140023005427456 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.270695209503174, loss=2.3777873516082764
I0204 07:56:15.294802 140022518892288 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.203068971633911, loss=1.916932463645935
I0204 07:57:01.428286 140023005427456 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.9137927293777466, loss=2.124779224395752
I0204 07:57:47.620928 140022518892288 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.0417544841766357, loss=1.9852861166000366
I0204 07:58:34.007622 140023005427456 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.0951318740844727, loss=4.238609313964844
I0204 07:59:20.263175 140022518892288 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.10953426361084, loss=2.7767741680145264
I0204 07:59:52.306080 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:00:03.006078 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:00:41.585186 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:00:43.192365 140184451094336 submission_runner.py:408] Time since start: 73267.47s, 	Step: 142571, 	{'train/accuracy': 0.8095507621765137, 'train/loss': 0.75356125831604, 'validation/accuracy': 0.7208200097084045, 'validation/loss': 1.128256916999817, 'validation/num_examples': 50000, 'test/accuracy': 0.6039000153541565, 'test/loss': 1.7557858228683472, 'test/num_examples': 10000, 'score': 65579.90997314453, 'total_duration': 73267.46773982048, 'accumulated_submission_time': 65579.90997314453, 'accumulated_eval_time': 7673.526467323303, 'accumulated_logging_time': 6.612675666809082}
I0204 08:00:43.232743 140023005427456 logging_writer.py:48] [142571] accumulated_eval_time=7673.526467, accumulated_logging_time=6.612676, accumulated_submission_time=65579.909973, global_step=142571, preemption_count=0, score=65579.909973, test/accuracy=0.603900, test/loss=1.755786, test/num_examples=10000, total_duration=73267.467740, train/accuracy=0.809551, train/loss=0.753561, validation/accuracy=0.720820, validation/loss=1.128257, validation/num_examples=50000
I0204 08:00:55.773785 140022518892288 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.1352880001068115, loss=2.8516268730163574
I0204 08:01:40.185721 140023005427456 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.1943795680999756, loss=1.7811744213104248
I0204 08:02:26.408145 140022518892288 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.17211651802063, loss=1.8349663019180298
I0204 08:03:13.122959 140023005427456 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.281926155090332, loss=1.706272006034851
I0204 08:03:59.070610 140022518892288 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.055978298187256, loss=3.1358864307403564
I0204 08:04:45.440215 140023005427456 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.289870023727417, loss=4.240461349487305
I0204 08:05:31.941755 140022518892288 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.1272218227386475, loss=2.68155574798584
I0204 08:06:18.056865 140023005427456 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.035959482192993, loss=3.7824783325195312
I0204 08:07:04.268904 140022518892288 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.106226921081543, loss=1.805977463722229
I0204 08:07:43.640634 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:07:54.482708 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:08:31.742434 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:08:33.348993 140184451094336 submission_runner.py:408] Time since start: 73737.62s, 	Step: 143487, 	{'train/accuracy': 0.7930663824081421, 'train/loss': 0.80852872133255, 'validation/accuracy': 0.719539999961853, 'validation/loss': 1.132944107055664, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.7516762018203735, 'test/num_examples': 10000, 'score': 66000.25936365128, 'total_duration': 73737.62437415123, 'accumulated_submission_time': 66000.25936365128, 'accumulated_eval_time': 7723.234818935394, 'accumulated_logging_time': 6.664331912994385}
I0204 08:08:33.391461 140023005427456 logging_writer.py:48] [143487] accumulated_eval_time=7723.234819, accumulated_logging_time=6.664332, accumulated_submission_time=66000.259364, global_step=143487, preemption_count=0, score=66000.259364, test/accuracy=0.600000, test/loss=1.751676, test/num_examples=10000, total_duration=73737.624374, train/accuracy=0.793066, train/loss=0.808529, validation/accuracy=0.719540, validation/loss=1.132944, validation/num_examples=50000
I0204 08:08:39.238547 140022518892288 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.2900326251983643, loss=2.4279723167419434
I0204 08:09:22.986448 140023005427456 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.2387101650238037, loss=2.306234359741211
I0204 08:10:09.197079 140022518892288 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.033116102218628, loss=2.1779568195343018
I0204 08:10:55.304124 140023005427456 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.4752988815307617, loss=4.272487163543701
I0204 08:11:42.029350 140022518892288 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.036172389984131, loss=3.573580026626587
I0204 08:12:28.525109 140023005427456 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.193923234939575, loss=1.760178565979004
I0204 08:13:15.032204 140022518892288 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.076151132583618, loss=4.249564170837402
I0204 08:14:01.518405 140023005427456 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.10308837890625, loss=2.89265775680542
I0204 08:14:47.661598 140022518892288 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.301896810531616, loss=1.7413371801376343
I0204 08:15:33.639456 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:15:44.351135 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:16:22.316229 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:16:23.913817 140184451094336 submission_runner.py:408] Time since start: 74208.19s, 	Step: 144400, 	{'train/accuracy': 0.7961718440055847, 'train/loss': 0.7973366379737854, 'validation/accuracy': 0.7209199666976929, 'validation/loss': 1.1386834383010864, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.758381962776184, 'test/num_examples': 10000, 'score': 66420.45049548149, 'total_duration': 74208.18918466568, 'accumulated_submission_time': 66420.45049548149, 'accumulated_eval_time': 7773.509160041809, 'accumulated_logging_time': 6.716897964477539}
I0204 08:16:23.961233 140023005427456 logging_writer.py:48] [144400] accumulated_eval_time=7773.509160, accumulated_logging_time=6.716898, accumulated_submission_time=66420.450495, global_step=144400, preemption_count=0, score=66420.450495, test/accuracy=0.602700, test/loss=1.758382, test/num_examples=10000, total_duration=74208.189185, train/accuracy=0.796172, train/loss=0.797337, validation/accuracy=0.720920, validation/loss=1.138683, validation/num_examples=50000
I0204 08:16:24.389051 140022518892288 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.2823805809020996, loss=2.1238789558410645
I0204 08:17:07.580102 140023005427456 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.1485085487365723, loss=1.7979216575622559
I0204 08:17:53.456007 140022518892288 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.2123703956604004, loss=1.6842507123947144
I0204 08:18:40.110351 140023005427456 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.0211803913116455, loss=1.8523348569869995
I0204 08:19:26.363065 140022518892288 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.0148491859436035, loss=3.8635661602020264
I0204 08:20:12.737215 140023005427456 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.0073046684265137, loss=1.6679539680480957
I0204 08:20:58.962741 140022518892288 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.042656183242798, loss=3.812548875808716
I0204 08:21:45.467392 140023005427456 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.1413936614990234, loss=3.5266478061676025
I0204 08:22:31.930721 140022518892288 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.13681697845459, loss=2.9135072231292725
I0204 08:23:18.361400 140023005427456 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.091041326522827, loss=4.121146202087402
I0204 08:23:24.047573 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:23:34.806095 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:24:12.052835 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:24:13.663896 140184451094336 submission_runner.py:408] Time since start: 74677.94s, 	Step: 145314, 	{'train/accuracy': 0.80712890625, 'train/loss': 0.7481192350387573, 'validation/accuracy': 0.7227999567985535, 'validation/loss': 1.1151829957962036, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.7355223894119263, 'test/num_examples': 10000, 'score': 66840.47810816765, 'total_duration': 74677.93926692009, 'accumulated_submission_time': 66840.47810816765, 'accumulated_eval_time': 7823.125457286835, 'accumulated_logging_time': 6.775168180465698}
I0204 08:24:13.707861 140022518892288 logging_writer.py:48] [145314] accumulated_eval_time=7823.125457, accumulated_logging_time=6.775168, accumulated_submission_time=66840.478108, global_step=145314, preemption_count=0, score=66840.478108, test/accuracy=0.604500, test/loss=1.735522, test/num_examples=10000, total_duration=74677.939267, train/accuracy=0.807129, train/loss=0.748119, validation/accuracy=0.722800, validation/loss=1.115183, validation/num_examples=50000
I0204 08:24:50.559320 140023005427456 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.06270170211792, loss=2.0516841411590576
I0204 08:25:36.522627 140022518892288 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.894120216369629, loss=2.9789211750030518
I0204 08:26:22.906825 140023005427456 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.3392155170440674, loss=1.6304963827133179
I0204 08:27:09.346063 140022518892288 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.168779134750366, loss=2.977945327758789
I0204 08:27:55.577219 140023005427456 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.951029896736145, loss=2.2688851356506348
I0204 08:28:41.678044 140022518892288 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.1536576747894287, loss=1.723120927810669
I0204 08:29:28.135884 140023005427456 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.2673797607421875, loss=3.692368268966675
I0204 08:30:14.644470 140022518892288 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.539848566055298, loss=2.048276424407959
I0204 08:31:01.088206 140023005427456 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.2916860580444336, loss=1.7360023260116577
I0204 08:31:13.875878 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:31:24.497264 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:32:01.022622 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:32:02.633670 140184451094336 submission_runner.py:408] Time since start: 75146.91s, 	Step: 146229, 	{'train/accuracy': 0.8002148270606995, 'train/loss': 0.7895472049713135, 'validation/accuracy': 0.7212600111961365, 'validation/loss': 1.1343475580215454, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.7654008865356445, 'test/num_examples': 10000, 'score': 67260.58725810051, 'total_duration': 75146.9090578556, 'accumulated_submission_time': 67260.58725810051, 'accumulated_eval_time': 7871.883242845535, 'accumulated_logging_time': 6.829352855682373}
I0204 08:32:02.679494 140022518892288 logging_writer.py:48] [146229] accumulated_eval_time=7871.883243, accumulated_logging_time=6.829353, accumulated_submission_time=67260.587258, global_step=146229, preemption_count=0, score=67260.587258, test/accuracy=0.599700, test/loss=1.765401, test/num_examples=10000, total_duration=75146.909058, train/accuracy=0.800215, train/loss=0.789547, validation/accuracy=0.721260, validation/loss=1.134348, validation/num_examples=50000
I0204 08:32:32.809133 140023005427456 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.9065271615982056, loss=3.1333088874816895
I0204 08:33:18.999959 140022518892288 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.7316391468048096, loss=2.2430100440979004
I0204 08:34:05.783479 140023005427456 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.2944321632385254, loss=3.830066680908203
I0204 08:34:52.316878 140022518892288 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.9184653759002686, loss=3.29427433013916
I0204 08:35:38.855666 140023005427456 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.483956813812256, loss=1.9490396976470947
I0204 08:36:25.439437 140022518892288 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.2391133308410645, loss=4.043631076812744
I0204 08:37:12.052973 140023005427456 logging_writer.py:48] [146900] global_step=146900, grad_norm=1.9689687490463257, loss=3.2599868774414062
I0204 08:37:58.396484 140022518892288 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.2822153568267822, loss=1.7308601140975952
I0204 08:38:45.081277 140023005427456 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.1956899166107178, loss=1.7513813972473145
I0204 08:39:02.806354 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:39:13.358871 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:39:50.917266 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:39:52.511181 140184451094336 submission_runner.py:408] Time since start: 75616.79s, 	Step: 147140, 	{'train/accuracy': 0.7982617020606995, 'train/loss': 0.7943770289421082, 'validation/accuracy': 0.7218599915504456, 'validation/loss': 1.133278250694275, 'validation/num_examples': 50000, 'test/accuracy': 0.6002000570297241, 'test/loss': 1.7653189897537231, 'test/num_examples': 10000, 'score': 67680.65770792961, 'total_duration': 75616.78656816483, 'accumulated_submission_time': 67680.65770792961, 'accumulated_eval_time': 7921.5880670547485, 'accumulated_logging_time': 6.884565353393555}
I0204 08:39:52.548995 140022518892288 logging_writer.py:48] [147140] accumulated_eval_time=7921.588067, accumulated_logging_time=6.884565, accumulated_submission_time=67680.657708, global_step=147140, preemption_count=0, score=67680.657708, test/accuracy=0.600200, test/loss=1.765319, test/num_examples=10000, total_duration=75616.786568, train/accuracy=0.798262, train/loss=0.794377, validation/accuracy=0.721860, validation/loss=1.133278, validation/num_examples=50000
I0204 08:40:18.020410 140023005427456 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.0925841331481934, loss=1.8861454725265503
I0204 08:41:03.705749 140022518892288 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.40085506439209, loss=1.7884284257888794
I0204 08:41:50.388247 140023005427456 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.3452866077423096, loss=2.001525402069092
I0204 08:42:36.964419 140022518892288 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.2446162700653076, loss=1.71515691280365
I0204 08:43:23.506710 140023005427456 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.269122838973999, loss=1.9534244537353516
I0204 08:44:09.950761 140022518892288 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.2200698852539062, loss=1.7768924236297607
I0204 08:44:56.454664 140023005427456 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.3204221725463867, loss=2.529888153076172
I0204 08:45:42.913363 140022518892288 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.3988001346588135, loss=1.7479228973388672
I0204 08:46:29.552067 140023005427456 logging_writer.py:48] [148000] global_step=148000, grad_norm=1.9772175550460815, loss=2.950430393218994
I0204 08:46:52.529699 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:47:03.486196 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:47:39.714854 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:47:41.318142 140184451094336 submission_runner.py:408] Time since start: 76085.59s, 	Step: 148051, 	{'train/accuracy': 0.8085156083106995, 'train/loss': 0.7679116725921631, 'validation/accuracy': 0.7251200079917908, 'validation/loss': 1.131921410560608, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.749596118927002, 'test/num_examples': 10000, 'score': 68100.57943248749, 'total_duration': 76085.59352064133, 'accumulated_submission_time': 68100.57943248749, 'accumulated_eval_time': 7970.376499891281, 'accumulated_logging_time': 6.934350490570068}
I0204 08:47:41.360468 140022518892288 logging_writer.py:48] [148051] accumulated_eval_time=7970.376500, accumulated_logging_time=6.934350, accumulated_submission_time=68100.579432, global_step=148051, preemption_count=0, score=68100.579432, test/accuracy=0.602400, test/loss=1.749596, test/num_examples=10000, total_duration=76085.593521, train/accuracy=0.808516, train/loss=0.767912, validation/accuracy=0.725120, validation/loss=1.131921, validation/num_examples=50000
I0204 08:48:02.244441 140023005427456 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.1907331943511963, loss=1.8508986234664917
I0204 08:48:47.371602 140022518892288 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.199709415435791, loss=1.5966312885284424
I0204 08:49:33.935217 140023005427456 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.4176456928253174, loss=1.7914519309997559
I0204 08:50:20.376485 140022518892288 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.218797206878662, loss=1.7782421112060547
I0204 08:51:06.727764 140023005427456 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.2976837158203125, loss=1.7487599849700928
I0204 08:51:53.291017 140022518892288 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.2450473308563232, loss=3.403707981109619
I0204 08:52:39.954571 140023005427456 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.370190382003784, loss=1.645427942276001
I0204 08:53:26.104754 140022518892288 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.261439323425293, loss=1.749481439590454
I0204 08:54:12.703783 140023005427456 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.1547861099243164, loss=2.251957416534424
I0204 08:54:41.495296 140184451094336 spec.py:321] Evaluating on the training split.
I0204 08:54:52.158868 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 08:55:30.199108 140184451094336 spec.py:349] Evaluating on the test split.
I0204 08:55:31.797059 140184451094336 submission_runner.py:408] Time since start: 76556.07s, 	Step: 148964, 	{'train/accuracy': 0.8046875, 'train/loss': 0.7722216844558716, 'validation/accuracy': 0.7258399724960327, 'validation/loss': 1.1195939779281616, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.739829421043396, 'test/num_examples': 10000, 'score': 68520.65689635277, 'total_duration': 76556.07243704796, 'accumulated_submission_time': 68520.65689635277, 'accumulated_eval_time': 8020.678261995316, 'accumulated_logging_time': 6.98644495010376}
I0204 08:55:31.841809 140022518892288 logging_writer.py:48] [148964] accumulated_eval_time=8020.678262, accumulated_logging_time=6.986445, accumulated_submission_time=68520.656896, global_step=148964, preemption_count=0, score=68520.656896, test/accuracy=0.606700, test/loss=1.739829, test/num_examples=10000, total_duration=76556.072437, train/accuracy=0.804688, train/loss=0.772222, validation/accuracy=0.725840, validation/loss=1.119594, validation/num_examples=50000
I0204 08:55:47.294067 140023005427456 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.366787910461426, loss=4.111917972564697
I0204 08:56:31.628535 140022518892288 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.502070426940918, loss=1.6773457527160645
I0204 08:57:18.208156 140023005427456 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.2938730716705322, loss=1.6751123666763306
I0204 08:58:04.945138 140022518892288 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.3765556812286377, loss=1.6190654039382935
I0204 08:58:51.233304 140023005427456 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.208098888397217, loss=1.5741993188858032
I0204 08:59:37.833228 140022518892288 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.2856314182281494, loss=1.716794729232788
I0204 09:00:24.255336 140023005427456 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.4182190895080566, loss=1.7546420097351074
I0204 09:01:10.781693 140022518892288 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.3210976123809814, loss=1.8390638828277588
I0204 09:01:57.431145 140023005427456 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.1711268424987793, loss=4.246826648712158
I0204 09:02:31.996407 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:02:42.819704 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:03:19.578806 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:03:21.176708 140184451094336 submission_runner.py:408] Time since start: 77025.45s, 	Step: 149875, 	{'train/accuracy': 0.8066015243530273, 'train/loss': 0.7632541656494141, 'validation/accuracy': 0.7278800010681152, 'validation/loss': 1.111369013786316, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.7303481101989746, 'test/num_examples': 10000, 'score': 68940.75487065315, 'total_duration': 77025.45209693909, 'accumulated_submission_time': 68940.75487065315, 'accumulated_eval_time': 8069.858564853668, 'accumulated_logging_time': 7.0408594608306885}
I0204 09:03:21.217969 140022518892288 logging_writer.py:48] [149875] accumulated_eval_time=8069.858565, accumulated_logging_time=7.040859, accumulated_submission_time=68940.754871, global_step=149875, preemption_count=0, score=68940.754871, test/accuracy=0.606000, test/loss=1.730348, test/num_examples=10000, total_duration=77025.452097, train/accuracy=0.806602, train/loss=0.763254, validation/accuracy=0.727880, validation/loss=1.111369, validation/num_examples=50000
I0204 09:03:32.072421 140023005427456 logging_writer.py:48] [149900] global_step=149900, grad_norm=1.994769811630249, loss=3.560317039489746
I0204 09:04:16.071776 140022518892288 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.1448721885681152, loss=2.7947561740875244
I0204 09:05:02.370990 140023005427456 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.284287452697754, loss=1.763352870941162
I0204 09:05:48.772149 140022518892288 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.2131943702697754, loss=3.473331928253174
I0204 09:06:35.212526 140023005427456 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.5842270851135254, loss=2.580380439758301
I0204 09:07:21.674162 140022518892288 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.5305538177490234, loss=1.9656901359558105
I0204 09:08:07.953161 140023005427456 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.5673983097076416, loss=1.7111399173736572
I0204 09:08:54.224778 140022518892288 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.111856460571289, loss=2.055509567260742
I0204 09:09:40.544050 140023005427456 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.2086994647979736, loss=1.6914124488830566
I0204 09:10:21.502888 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:10:32.397830 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:11:08.595222 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:11:10.208696 140184451094336 submission_runner.py:408] Time since start: 77494.48s, 	Step: 150790, 	{'train/accuracy': 0.8119726181030273, 'train/loss': 0.7188385725021362, 'validation/accuracy': 0.7307599782943726, 'validation/loss': 1.0796045064926147, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.7081046104431152, 'test/num_examples': 10000, 'score': 69360.97912240028, 'total_duration': 77494.48408341408, 'accumulated_submission_time': 69360.97912240028, 'accumulated_eval_time': 8118.56437587738, 'accumulated_logging_time': 7.0947325229644775}
I0204 09:11:10.248613 140022518892288 logging_writer.py:48] [150790] accumulated_eval_time=8118.564376, accumulated_logging_time=7.094733, accumulated_submission_time=69360.979122, global_step=150790, preemption_count=0, score=69360.979122, test/accuracy=0.610400, test/loss=1.708105, test/num_examples=10000, total_duration=77494.484083, train/accuracy=0.811973, train/loss=0.718839, validation/accuracy=0.730760, validation/loss=1.079605, validation/num_examples=50000
I0204 09:11:14.846385 140023005427456 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.4734809398651123, loss=1.7895745038986206
I0204 09:11:58.365797 140022518892288 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.345149517059326, loss=1.8129435777664185
I0204 09:12:44.544346 140023005427456 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.1866114139556885, loss=2.0824949741363525
I0204 09:13:30.923277 140022518892288 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.1424906253814697, loss=2.032787561416626
I0204 09:14:17.180226 140023005427456 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.2742514610290527, loss=4.063957691192627
I0204 09:15:03.658940 140022518892288 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.1633667945861816, loss=3.19632887840271
I0204 09:15:50.175770 140023005427456 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.3503549098968506, loss=4.053627967834473
I0204 09:16:36.422907 140022518892288 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.1832780838012695, loss=4.086027145385742
I0204 09:17:22.792082 140023005427456 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.2682607173919678, loss=2.3389415740966797
I0204 09:18:09.425111 140022518892288 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.512709140777588, loss=1.7997373342514038
I0204 09:18:10.702523 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:18:21.400605 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:18:57.125067 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:18:58.736568 140184451094336 submission_runner.py:408] Time since start: 77963.01s, 	Step: 151704, 	{'train/accuracy': 0.8213671445846558, 'train/loss': 0.7049360275268555, 'validation/accuracy': 0.7328000068664551, 'validation/loss': 1.0796293020248413, 'validation/num_examples': 50000, 'test/accuracy': 0.6109000444412231, 'test/loss': 1.7023441791534424, 'test/num_examples': 10000, 'score': 69781.3741095066, 'total_duration': 77963.0119497776, 'accumulated_submission_time': 69781.3741095066, 'accumulated_eval_time': 8166.598405122757, 'accumulated_logging_time': 7.145893335342407}
I0204 09:18:58.775404 140023005427456 logging_writer.py:48] [151704] accumulated_eval_time=8166.598405, accumulated_logging_time=7.145893, accumulated_submission_time=69781.374110, global_step=151704, preemption_count=0, score=69781.374110, test/accuracy=0.610900, test/loss=1.702344, test/num_examples=10000, total_duration=77963.011950, train/accuracy=0.821367, train/loss=0.704936, validation/accuracy=0.732800, validation/loss=1.079629, validation/num_examples=50000
I0204 09:19:40.600311 140022518892288 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.744379997253418, loss=1.683847188949585
I0204 09:20:26.860424 140023005427456 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.6291940212249756, loss=1.9204132556915283
I0204 09:21:13.423448 140022518892288 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.655911684036255, loss=1.7029979228973389
I0204 09:21:59.952908 140023005427456 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.373553514480591, loss=1.5708526372909546
I0204 09:22:46.340860 140022518892288 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.5487282276153564, loss=2.020317316055298
I0204 09:23:32.845355 140023005427456 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.2951624393463135, loss=1.5931295156478882
I0204 09:24:19.426577 140022518892288 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.4571406841278076, loss=3.304511785507202
I0204 09:25:05.608971 140023005427456 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.0489442348480225, loss=1.8924494981765747
I0204 09:25:51.654470 140022518892288 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.4079885482788086, loss=1.6123850345611572
I0204 09:25:59.207300 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:26:09.793290 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:26:46.915429 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:26:48.519317 140184451094336 submission_runner.py:408] Time since start: 78432.79s, 	Step: 152618, 	{'train/accuracy': 0.8115234375, 'train/loss': 0.7339527010917664, 'validation/accuracy': 0.7320799827575684, 'validation/loss': 1.0806304216384888, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.709767460823059, 'test/num_examples': 10000, 'score': 70201.74805235863, 'total_duration': 78432.79470396042, 'accumulated_submission_time': 70201.74805235863, 'accumulated_eval_time': 8215.91041135788, 'accumulated_logging_time': 7.195410490036011}
I0204 09:26:48.561836 140023005427456 logging_writer.py:48] [152618] accumulated_eval_time=8215.910411, accumulated_logging_time=7.195410, accumulated_submission_time=70201.748052, global_step=152618, preemption_count=0, score=70201.748052, test/accuracy=0.610800, test/loss=1.709767, test/num_examples=10000, total_duration=78432.794704, train/accuracy=0.811523, train/loss=0.733953, validation/accuracy=0.732080, validation/loss=1.080630, validation/num_examples=50000
I0204 09:27:23.756389 140022518892288 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.562485456466675, loss=3.8572893142700195
I0204 09:28:09.771950 140023005427456 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.206007480621338, loss=3.6384377479553223
I0204 09:28:55.936211 140022518892288 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.028923511505127, loss=1.6177033185958862
I0204 09:29:42.463041 140023005427456 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.2268903255462646, loss=1.5476254224777222
I0204 09:30:28.631483 140022518892288 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.5961556434631348, loss=1.5445382595062256
I0204 09:31:14.974008 140023005427456 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.611711263656616, loss=1.676265835762024
I0204 09:32:01.156090 140022518892288 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.3857626914978027, loss=3.8256683349609375
I0204 09:32:47.454731 140023005427456 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.2719852924346924, loss=1.7813148498535156
I0204 09:33:33.538494 140022518892288 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.3926563262939453, loss=2.289454221725464
I0204 09:33:48.932002 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:33:59.635534 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:34:38.001304 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:34:39.625685 140184451094336 submission_runner.py:408] Time since start: 78903.90s, 	Step: 153535, 	{'train/accuracy': 0.815722644329071, 'train/loss': 0.7261427640914917, 'validation/accuracy': 0.7325199842453003, 'validation/loss': 1.0846915245056152, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.706923484802246, 'test/num_examples': 10000, 'score': 70622.05784344673, 'total_duration': 78903.90107440948, 'accumulated_submission_time': 70622.05784344673, 'accumulated_eval_time': 8266.604093313217, 'accumulated_logging_time': 7.249720573425293}
I0204 09:34:39.667097 140023005427456 logging_writer.py:48] [153535] accumulated_eval_time=8266.604093, accumulated_logging_time=7.249721, accumulated_submission_time=70622.057843, global_step=153535, preemption_count=0, score=70622.057843, test/accuracy=0.609600, test/loss=1.706923, test/num_examples=10000, total_duration=78903.901074, train/accuracy=0.815723, train/loss=0.726143, validation/accuracy=0.732520, validation/loss=1.084692, validation/num_examples=50000
I0204 09:35:07.236493 140022518892288 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.8565428256988525, loss=1.6121690273284912
I0204 09:35:53.073100 140023005427456 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.4625792503356934, loss=1.6213436126708984
I0204 09:36:39.550616 140022518892288 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.348799705505371, loss=2.185784101486206
I0204 09:37:25.966411 140023005427456 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.2739410400390625, loss=1.5937836170196533
I0204 09:38:12.285917 140022518892288 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.5504796504974365, loss=1.6241679191589355
I0204 09:38:58.655030 140023005427456 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.2909181118011475, loss=2.8961985111236572
I0204 09:39:45.141111 140022518892288 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.4220480918884277, loss=1.5285229682922363
I0204 09:40:31.585965 140023005427456 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.8296427726745605, loss=1.7366372346878052
I0204 09:41:18.251776 140022518892288 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.449479103088379, loss=3.6296186447143555
I0204 09:41:39.956977 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:41:51.067113 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:42:28.472256 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:42:30.064558 140184451094336 submission_runner.py:408] Time since start: 79374.34s, 	Step: 154448, 	{'train/accuracy': 0.8185155987739563, 'train/loss': 0.7208164930343628, 'validation/accuracy': 0.730239987373352, 'validation/loss': 1.097428798675537, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.7289506196975708, 'test/num_examples': 10000, 'score': 71042.29072260857, 'total_duration': 79374.33994674683, 'accumulated_submission_time': 71042.29072260857, 'accumulated_eval_time': 8316.71166753769, 'accumulated_logging_time': 7.300796031951904}
I0204 09:42:30.103198 140023005427456 logging_writer.py:48] [154448] accumulated_eval_time=8316.711668, accumulated_logging_time=7.300796, accumulated_submission_time=71042.290723, global_step=154448, preemption_count=0, score=71042.290723, test/accuracy=0.610800, test/loss=1.728951, test/num_examples=10000, total_duration=79374.339947, train/accuracy=0.818516, train/loss=0.720816, validation/accuracy=0.730240, validation/loss=1.097429, validation/num_examples=50000
I0204 09:42:52.251358 140022518892288 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.471461772918701, loss=3.4995925426483154
I0204 09:43:37.615289 140023005427456 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.3583312034606934, loss=2.3262553215026855
I0204 09:44:23.879015 140022518892288 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.3905951976776123, loss=3.2870612144470215
I0204 09:45:10.268105 140023005427456 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.2387452125549316, loss=2.230257987976074
I0204 09:45:56.767633 140022518892288 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.285902976989746, loss=4.179448127746582
I0204 09:46:43.046395 140023005427456 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.383901596069336, loss=1.529362440109253
I0204 09:47:29.647128 140022518892288 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.2462270259857178, loss=2.7131404876708984
I0204 09:48:16.171897 140023005427456 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.482985019683838, loss=1.7339611053466797
I0204 09:49:02.622872 140022518892288 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.49497652053833, loss=4.093935012817383
I0204 09:49:30.232132 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:49:40.783444 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:50:17.066247 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:50:18.667934 140184451094336 submission_runner.py:408] Time since start: 79842.94s, 	Step: 155361, 	{'train/accuracy': 0.8110156059265137, 'train/loss': 0.7516688108444214, 'validation/accuracy': 0.731440007686615, 'validation/loss': 1.091639757156372, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.7229865789413452, 'test/num_examples': 10000, 'score': 71462.36037421227, 'total_duration': 79842.9433221817, 'accumulated_submission_time': 71462.36037421227, 'accumulated_eval_time': 8365.147471904755, 'accumulated_logging_time': 7.35153603553772}
I0204 09:50:18.708309 140023005427456 logging_writer.py:48] [155361] accumulated_eval_time=8365.147472, accumulated_logging_time=7.351536, accumulated_submission_time=71462.360374, global_step=155361, preemption_count=0, score=71462.360374, test/accuracy=0.611000, test/loss=1.722987, test/num_examples=10000, total_duration=79842.943322, train/accuracy=0.811016, train/loss=0.751669, validation/accuracy=0.731440, validation/loss=1.091640, validation/num_examples=50000
I0204 09:50:35.413397 140022518892288 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.4632625579833984, loss=3.32886004447937
I0204 09:51:19.964163 140023005427456 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.686650514602661, loss=1.640819787979126
I0204 09:52:06.722538 140022518892288 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.558415412902832, loss=1.70747709274292
I0204 09:52:53.036770 140023005427456 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.4235541820526123, loss=1.776343584060669
I0204 09:53:39.558762 140022518892288 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.3617618083953857, loss=3.364598035812378
I0204 09:54:26.031262 140023005427456 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.3511102199554443, loss=2.13932728767395
I0204 09:55:12.420710 140022518892288 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.3606653213500977, loss=3.6830735206604004
I0204 09:55:58.919175 140023005427456 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.780965566635132, loss=1.9067171812057495
I0204 09:56:45.514409 140022518892288 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.258861541748047, loss=1.6247131824493408
I0204 09:57:19.065464 140184451094336 spec.py:321] Evaluating on the training split.
I0204 09:57:30.066682 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 09:58:08.832561 140184451094336 spec.py:349] Evaluating on the test split.
I0204 09:58:10.454365 140184451094336 submission_runner.py:408] Time since start: 80314.73s, 	Step: 156274, 	{'train/accuracy': 0.8199804425239563, 'train/loss': 0.7003152370452881, 'validation/accuracy': 0.7363599538803101, 'validation/loss': 1.0666792392730713, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7028956413269043, 'test/num_examples': 10000, 'score': 71882.66050243378, 'total_duration': 80314.7297205925, 'accumulated_submission_time': 71882.66050243378, 'accumulated_eval_time': 8416.536350011826, 'accumulated_logging_time': 7.401159286499023}
I0204 09:58:10.504708 140023005427456 logging_writer.py:48] [156274] accumulated_eval_time=8416.536350, accumulated_logging_time=7.401159, accumulated_submission_time=71882.660502, global_step=156274, preemption_count=0, score=71882.660502, test/accuracy=0.612100, test/loss=1.702896, test/num_examples=10000, total_duration=80314.729721, train/accuracy=0.819980, train/loss=0.700315, validation/accuracy=0.736360, validation/loss=1.066679, validation/num_examples=50000
I0204 09:58:21.775436 140022518892288 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.434148073196411, loss=2.3417656421661377
I0204 09:59:06.175539 140023005427456 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.567917823791504, loss=1.4769459962844849
I0204 09:59:52.685794 140022518892288 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.222240924835205, loss=3.484475612640381
I0204 10:00:39.307956 140023005427456 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.795682430267334, loss=3.160486936569214
I0204 10:01:25.873998 140022518892288 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.6487877368927, loss=4.066863059997559
I0204 10:02:12.566272 140023005427456 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.8367912769317627, loss=1.532810091972351
I0204 10:02:59.011625 140022518892288 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.599621295928955, loss=2.883997917175293
I0204 10:03:45.347764 140023005427456 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.5827698707580566, loss=4.031239986419678
I0204 10:04:31.842415 140022518892288 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.7899320125579834, loss=1.5264075994491577
I0204 10:05:10.919066 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:05:21.546419 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:05:59.422537 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:06:01.035536 140184451094336 submission_runner.py:408] Time since start: 80785.31s, 	Step: 157186, 	{'train/accuracy': 0.82289057970047, 'train/loss': 0.6994317173957825, 'validation/accuracy': 0.7371799945831299, 'validation/loss': 1.0725938081741333, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.7018332481384277, 'test/num_examples': 10000, 'score': 72303.01607465744, 'total_duration': 80785.31089067459, 'accumulated_submission_time': 72303.01607465744, 'accumulated_eval_time': 8466.652791976929, 'accumulated_logging_time': 7.462125539779663}
I0204 10:06:01.087663 140023005427456 logging_writer.py:48] [157186] accumulated_eval_time=8466.652792, accumulated_logging_time=7.462126, accumulated_submission_time=72303.016075, global_step=157186, preemption_count=0, score=72303.016075, test/accuracy=0.616200, test/loss=1.701833, test/num_examples=10000, total_duration=80785.310891, train/accuracy=0.822891, train/loss=0.699432, validation/accuracy=0.737180, validation/loss=1.072594, validation/num_examples=50000
I0204 10:06:07.357499 140022518892288 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.7026991844177246, loss=1.5403034687042236
I0204 10:06:50.813945 140023005427456 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.5918753147125244, loss=3.271698474884033
I0204 10:07:37.748016 140022518892288 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.685206174850464, loss=2.50779390335083
I0204 10:08:24.245660 140023005427456 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.4421069622039795, loss=2.674269676208496
I0204 10:09:10.656261 140022518892288 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.5484538078308105, loss=2.077197313308716
I0204 10:09:57.202162 140023005427456 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.8348312377929688, loss=3.170792579650879
I0204 10:10:43.640169 140022518892288 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.4332973957061768, loss=1.506779670715332
I0204 10:11:30.012154 140023005427456 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.9383583068847656, loss=4.109537124633789
I0204 10:12:16.814028 140022518892288 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.4997735023498535, loss=2.8009486198425293
I0204 10:13:01.193429 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:13:12.085701 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:13:48.490883 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:13:50.097865 140184451094336 submission_runner.py:408] Time since start: 81254.37s, 	Step: 158097, 	{'train/accuracy': 0.821582019329071, 'train/loss': 0.718077540397644, 'validation/accuracy': 0.738599956035614, 'validation/loss': 1.0715638399124146, 'validation/num_examples': 50000, 'test/accuracy': 0.6152000427246094, 'test/loss': 1.7035168409347534, 'test/num_examples': 10000, 'score': 72723.06442546844, 'total_duration': 81254.37323331833, 'accumulated_submission_time': 72723.06442546844, 'accumulated_eval_time': 8515.557217359543, 'accumulated_logging_time': 7.524913787841797}
I0204 10:13:50.146430 140023005427456 logging_writer.py:48] [158097] accumulated_eval_time=8515.557217, accumulated_logging_time=7.524914, accumulated_submission_time=72723.064425, global_step=158097, preemption_count=0, score=72723.064425, test/accuracy=0.615200, test/loss=1.703517, test/num_examples=10000, total_duration=81254.373233, train/accuracy=0.821582, train/loss=0.718078, validation/accuracy=0.738600, validation/loss=1.071564, validation/num_examples=50000
I0204 10:13:51.817632 140022518892288 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.942251443862915, loss=1.689002513885498
I0204 10:14:34.944054 140023005427456 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.6274828910827637, loss=4.039350509643555
I0204 10:15:21.055256 140022518892288 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.9427666664123535, loss=1.5783376693725586
I0204 10:16:07.466370 140023005427456 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.691185712814331, loss=3.1670303344726562
I0204 10:16:53.759668 140022518892288 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.3808839321136475, loss=3.9846572875976562
I0204 10:17:40.201411 140023005427456 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.5312139987945557, loss=1.9562407732009888
I0204 10:18:26.810085 140022518892288 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.87851619720459, loss=1.7194799184799194
I0204 10:19:13.164917 140023005427456 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.512053966522217, loss=1.5158069133758545
I0204 10:20:00.003146 140022518892288 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.5582897663116455, loss=1.523527979850769
I0204 10:20:46.386548 140023005427456 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.6544251441955566, loss=1.486925482749939
I0204 10:20:50.386347 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:21:01.358667 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:21:39.125324 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:21:40.745032 140184451094336 submission_runner.py:408] Time since start: 81725.02s, 	Step: 159010, 	{'train/accuracy': 0.8240429759025574, 'train/loss': 0.699968159198761, 'validation/accuracy': 0.7378199696540833, 'validation/loss': 1.0641032457351685, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.6965481042861938, 'test/num_examples': 10000, 'score': 73143.2449285984, 'total_duration': 81725.02041864395, 'accumulated_submission_time': 73143.2449285984, 'accumulated_eval_time': 8565.915897130966, 'accumulated_logging_time': 7.585332155227661}
I0204 10:21:40.785983 140022518892288 logging_writer.py:48] [159010] accumulated_eval_time=8565.915897, accumulated_logging_time=7.585332, accumulated_submission_time=73143.244929, global_step=159010, preemption_count=0, score=73143.244929, test/accuracy=0.617300, test/loss=1.696548, test/num_examples=10000, total_duration=81725.020419, train/accuracy=0.824043, train/loss=0.699968, validation/accuracy=0.737820, validation/loss=1.064103, validation/num_examples=50000
I0204 10:22:19.278266 140023005427456 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.3926291465759277, loss=3.4169199466705322
I0204 10:23:05.374394 140022518892288 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.3299503326416016, loss=3.7427444458007812
I0204 10:23:51.767835 140023005427456 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.546990156173706, loss=1.642695665359497
I0204 10:24:38.368552 140022518892288 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.32124662399292, loss=1.731124997138977
I0204 10:25:24.839510 140023005427456 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.7289109230041504, loss=1.7999718189239502
I0204 10:26:11.020956 140022518892288 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.6538641452789307, loss=1.5955727100372314
I0204 10:26:57.291398 140023005427456 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.6921346187591553, loss=2.8321080207824707
I0204 10:27:43.639898 140022518892288 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.240614652633667, loss=2.8062517642974854
I0204 10:28:30.397402 140023005427456 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.7193095684051514, loss=1.5313982963562012
I0204 10:28:40.748914 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:28:51.471366 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:29:26.406051 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:29:28.016243 140184451094336 submission_runner.py:408] Time since start: 82192.29s, 	Step: 159924, 	{'train/accuracy': 0.822558581829071, 'train/loss': 0.6985819935798645, 'validation/accuracy': 0.7378399968147278, 'validation/loss': 1.068437933921814, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.6972639560699463, 'test/num_examples': 10000, 'score': 73563.15134334564, 'total_duration': 82192.29157710075, 'accumulated_submission_time': 73563.15134334564, 'accumulated_eval_time': 8613.183167696, 'accumulated_logging_time': 7.635556221008301}
I0204 10:29:28.067585 140022518892288 logging_writer.py:48] [159924] accumulated_eval_time=8613.183168, accumulated_logging_time=7.635556, accumulated_submission_time=73563.151343, global_step=159924, preemption_count=0, score=73563.151343, test/accuracy=0.615300, test/loss=1.697264, test/num_examples=10000, total_duration=82192.291577, train/accuracy=0.822559, train/loss=0.698582, validation/accuracy=0.737840, validation/loss=1.068438, validation/num_examples=50000
I0204 10:30:00.595940 140023005427456 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.8202567100524902, loss=3.7340354919433594
I0204 10:30:46.665600 140022518892288 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.7007901668548584, loss=1.5744444131851196
I0204 10:31:33.435425 140023005427456 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.8558671474456787, loss=1.566286563873291
I0204 10:32:19.718072 140022518892288 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.9090211391448975, loss=1.6043980121612549
I0204 10:33:05.841861 140023005427456 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.5091514587402344, loss=1.5897094011306763
I0204 10:33:52.244460 140022518892288 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.7292428016662598, loss=1.5638630390167236
I0204 10:34:38.552026 140023005427456 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.7283363342285156, loss=1.6291112899780273
I0204 10:35:24.930449 140022518892288 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.830540418624878, loss=1.5343273878097534
I0204 10:36:11.184989 140023005427456 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.627692461013794, loss=2.934849739074707
I0204 10:36:28.018703 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:36:38.731239 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:37:13.795699 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:37:15.402219 140184451094336 submission_runner.py:408] Time since start: 82659.68s, 	Step: 160838, 	{'train/accuracy': 0.8229296803474426, 'train/loss': 0.6872890591621399, 'validation/accuracy': 0.7397199869155884, 'validation/loss': 1.054733395576477, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.6841599941253662, 'test/num_examples': 10000, 'score': 73983.04493808746, 'total_duration': 82659.67759394646, 'accumulated_submission_time': 73983.04493808746, 'accumulated_eval_time': 8660.566656589508, 'accumulated_logging_time': 7.696813583374023}
I0204 10:37:15.446306 140022518892288 logging_writer.py:48] [160838] accumulated_eval_time=8660.566657, accumulated_logging_time=7.696814, accumulated_submission_time=73983.044938, global_step=160838, preemption_count=0, score=73983.044938, test/accuracy=0.613100, test/loss=1.684160, test/num_examples=10000, total_duration=82659.677594, train/accuracy=0.822930, train/loss=0.687289, validation/accuracy=0.739720, validation/loss=1.054733, validation/num_examples=50000
I0204 10:37:41.758971 140023005427456 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.4082701206207275, loss=2.2716760635375977
I0204 10:38:27.457037 140022518892288 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.5927841663360596, loss=3.0758934020996094
I0204 10:39:13.970952 140023005427456 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.724163055419922, loss=1.8174973726272583
I0204 10:40:00.259784 140022518892288 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.428248405456543, loss=2.159400224685669
I0204 10:40:46.779129 140023005427456 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.6688811779022217, loss=1.5477941036224365
I0204 10:41:33.406430 140022518892288 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.5061190128326416, loss=2.7589852809906006
I0204 10:42:19.648405 140023005427456 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.512453079223633, loss=3.521864891052246
I0204 10:43:06.181670 140022518892288 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.0673468112945557, loss=1.5332419872283936
I0204 10:43:52.300386 140023005427456 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.5788304805755615, loss=2.4692633152008057
I0204 10:44:15.620116 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:44:26.566994 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:45:03.505826 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:45:05.106447 140184451094336 submission_runner.py:408] Time since start: 83129.38s, 	Step: 161752, 	{'train/accuracy': 0.8261327743530273, 'train/loss': 0.6801848411560059, 'validation/accuracy': 0.7404199838638306, 'validation/loss': 1.0489463806152344, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.6832082271575928, 'test/num_examples': 10000, 'score': 74403.15802598, 'total_duration': 83129.38182520866, 'accumulated_submission_time': 74403.15802598, 'accumulated_eval_time': 8710.053017377853, 'accumulated_logging_time': 7.754112958908081}
I0204 10:45:05.146357 140022518892288 logging_writer.py:48] [161752] accumulated_eval_time=8710.053017, accumulated_logging_time=7.754113, accumulated_submission_time=74403.158026, global_step=161752, preemption_count=0, score=74403.158026, test/accuracy=0.615600, test/loss=1.683208, test/num_examples=10000, total_duration=83129.381825, train/accuracy=0.826133, train/loss=0.680185, validation/accuracy=0.740420, validation/loss=1.048946, validation/num_examples=50000
I0204 10:45:25.615112 140023005427456 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.6574959754943848, loss=4.08119535446167
I0204 10:46:10.809748 140022518892288 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.682291269302368, loss=2.1936380863189697
I0204 10:46:57.091078 140023005427456 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.5598411560058594, loss=3.3697235584259033
I0204 10:47:43.435508 140022518892288 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.355973482131958, loss=2.1016931533813477
I0204 10:48:29.617703 140023005427456 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.343740224838257, loss=1.6191288232803345
I0204 10:49:15.880126 140022518892288 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.5357298851013184, loss=2.0331435203552246
I0204 10:50:02.146039 140023005427456 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.6472103595733643, loss=1.451184630393982
I0204 10:50:48.446640 140022518892288 logging_writer.py:48] [162500] global_step=162500, grad_norm=3.3371551036834717, loss=3.993269443511963
I0204 10:51:34.510371 140023005427456 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.7266130447387695, loss=2.268369436264038
I0204 10:52:05.438873 140184451094336 spec.py:321] Evaluating on the training split.
I0204 10:52:16.044925 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 10:52:52.434586 140184451094336 spec.py:349] Evaluating on the test split.
I0204 10:52:54.041438 140184451094336 submission_runner.py:408] Time since start: 83598.32s, 	Step: 162668, 	{'train/accuracy': 0.8266015648841858, 'train/loss': 0.6813116073608398, 'validation/accuracy': 0.7404999732971191, 'validation/loss': 1.0605944395065308, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.6932504177093506, 'test/num_examples': 10000, 'score': 74823.39261484146, 'total_duration': 83598.31682682037, 'accumulated_submission_time': 74823.39261484146, 'accumulated_eval_time': 8758.655586957932, 'accumulated_logging_time': 7.804761648178101}
I0204 10:52:54.086566 140022518892288 logging_writer.py:48] [162668] accumulated_eval_time=8758.655587, accumulated_logging_time=7.804762, accumulated_submission_time=74823.392615, global_step=162668, preemption_count=0, score=74823.392615, test/accuracy=0.618300, test/loss=1.693250, test/num_examples=10000, total_duration=83598.316827, train/accuracy=0.826602, train/loss=0.681312, validation/accuracy=0.740500, validation/loss=1.060594, validation/num_examples=50000
I0204 10:53:07.861000 140023005427456 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.354370594024658, loss=3.60693621635437
I0204 10:53:52.042931 140022518892288 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.6053919792175293, loss=2.30840802192688
I0204 10:54:38.398903 140023005427456 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.8256328105926514, loss=1.5561823844909668
I0204 10:55:24.991826 140022518892288 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.5143187046051025, loss=3.110680103302002
I0204 10:56:11.300877 140023005427456 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.8773603439331055, loss=4.127394199371338
I0204 10:56:57.750633 140022518892288 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.7569735050201416, loss=1.5601520538330078
I0204 10:57:44.172863 140023005427456 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.545240879058838, loss=2.6954760551452637
I0204 10:58:30.782379 140022518892288 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.6398847103118896, loss=2.00012469291687
I0204 10:59:17.367765 140023005427456 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.971863031387329, loss=1.527056097984314
I0204 10:59:54.450592 140184451094336 spec.py:321] Evaluating on the training split.
I0204 11:00:05.138039 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 11:00:44.001815 140184451094336 spec.py:349] Evaluating on the test split.
I0204 11:00:45.621658 140184451094336 submission_runner.py:408] Time since start: 84069.90s, 	Step: 163582, 	{'train/accuracy': 0.82958984375, 'train/loss': 0.6572279334068298, 'validation/accuracy': 0.7425599694252014, 'validation/loss': 1.0434695482254028, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.6695793867111206, 'test/num_examples': 10000, 'score': 75243.69893074036, 'total_duration': 84069.89703917503, 'accumulated_submission_time': 75243.69893074036, 'accumulated_eval_time': 8809.826647281647, 'accumulated_logging_time': 7.860961198806763}
I0204 11:00:45.667933 140022518892288 logging_writer.py:48] [163582] accumulated_eval_time=8809.826647, accumulated_logging_time=7.860961, accumulated_submission_time=75243.698931, global_step=163582, preemption_count=0, score=75243.698931, test/accuracy=0.620500, test/loss=1.669579, test/num_examples=10000, total_duration=84069.897039, train/accuracy=0.829590, train/loss=0.657228, validation/accuracy=0.742560, validation/loss=1.043470, validation/num_examples=50000
I0204 11:00:53.602067 140023005427456 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.6714253425598145, loss=1.4364185333251953
I0204 11:01:37.664982 140022518892288 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.5347695350646973, loss=1.7836620807647705
I0204 11:02:23.765343 140023005427456 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.5754449367523193, loss=3.9926540851593018
I0204 11:03:10.285323 140022518892288 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.871875524520874, loss=3.7883214950561523
I0204 11:03:56.531154 140023005427456 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.129883050918579, loss=1.4518547058105469
I0204 11:04:42.957083 140023005427456 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.2149133682250977, loss=1.4347596168518066
I0204 11:05:29.323036 140022518892288 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.835876703262329, loss=1.8451322317123413
I0204 11:06:15.809147 140023005427456 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.0518345832824707, loss=3.2571468353271484
I0204 11:07:02.273192 140022518892288 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.557338237762451, loss=2.0053164958953857
I0204 11:07:45.820978 140184451094336 spec.py:321] Evaluating on the training split.
I0204 11:07:56.604185 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 11:08:33.961980 140184451094336 spec.py:349] Evaluating on the test split.
I0204 11:08:35.563211 140184451094336 submission_runner.py:408] Time since start: 84539.84s, 	Step: 164496, 	{'train/accuracy': 0.8274804353713989, 'train/loss': 0.6675172448158264, 'validation/accuracy': 0.7411800026893616, 'validation/loss': 1.0367389917373657, 'validation/num_examples': 50000, 'test/accuracy': 0.6194000244140625, 'test/loss': 1.6700612306594849, 'test/num_examples': 10000, 'score': 75663.79537057877, 'total_duration': 84539.8385951519, 'accumulated_submission_time': 75663.79537057877, 'accumulated_eval_time': 8859.568863630295, 'accumulated_logging_time': 7.9168360233306885}
I0204 11:08:35.607209 140023005427456 logging_writer.py:48] [164496] accumulated_eval_time=8859.568864, accumulated_logging_time=7.916836, accumulated_submission_time=75663.795371, global_step=164496, preemption_count=0, score=75663.795371, test/accuracy=0.619400, test/loss=1.670061, test/num_examples=10000, total_duration=84539.838595, train/accuracy=0.827480, train/loss=0.667517, validation/accuracy=0.741180, validation/loss=1.036739, validation/num_examples=50000
I0204 11:08:37.694439 140022518892288 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.3351340293884277, loss=3.0634374618530273
I0204 11:09:20.854164 140023005427456 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.5841946601867676, loss=1.354889988899231
I0204 11:10:06.858285 140022518892288 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.595315456390381, loss=2.5281991958618164
I0204 11:10:53.144045 140023005427456 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.3497936725616455, loss=1.5426082611083984
I0204 11:11:39.834178 140022518892288 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.7731988430023193, loss=2.284916400909424
I0204 11:12:26.194905 140023005427456 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.615562677383423, loss=1.6178333759307861
I0204 11:13:12.608889 140022518892288 logging_writer.py:48] [165100] global_step=165100, grad_norm=7.127208232879639, loss=1.6435123682022095
I0204 11:13:59.014681 140023005427456 logging_writer.py:48] [165200] global_step=165200, grad_norm=3.132117509841919, loss=1.6713902950286865
I0204 11:14:45.171752 140022518892288 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.844982862472534, loss=1.5768874883651733
I0204 11:15:31.441109 140023005427456 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.4264230728149414, loss=3.092268705368042
I0204 11:15:35.700481 140184451094336 spec.py:321] Evaluating on the training split.
I0204 11:15:46.457121 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 11:16:24.231459 140184451094336 spec.py:349] Evaluating on the test split.
I0204 11:16:25.829393 140184451094336 submission_runner.py:408] Time since start: 85010.10s, 	Step: 165411, 	{'train/accuracy': 0.8327734470367432, 'train/loss': 0.6660227179527283, 'validation/accuracy': 0.741599977016449, 'validation/loss': 1.0521215200424194, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.6760276556015015, 'test/num_examples': 10000, 'score': 76083.82977080345, 'total_duration': 85010.10476636887, 'accumulated_submission_time': 76083.82977080345, 'accumulated_eval_time': 8909.697760820389, 'accumulated_logging_time': 7.97200608253479}
I0204 11:16:25.870854 140022518892288 logging_writer.py:48] [165411] accumulated_eval_time=8909.697761, accumulated_logging_time=7.972006, accumulated_submission_time=76083.829771, global_step=165411, preemption_count=0, score=76083.829771, test/accuracy=0.620600, test/loss=1.676028, test/num_examples=10000, total_duration=85010.104766, train/accuracy=0.832773, train/loss=0.666023, validation/accuracy=0.741600, validation/loss=1.052122, validation/num_examples=50000
I0204 11:17:04.397643 140023005427456 logging_writer.py:48] [165500] global_step=165500, grad_norm=3.2577075958251953, loss=1.5920395851135254
I0204 11:17:50.116715 140022518892288 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.783973455429077, loss=2.979346513748169
I0204 11:18:36.154332 140023005427456 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.7189836502075195, loss=3.871802806854248
I0204 11:19:22.432095 140022518892288 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.6671254634857178, loss=2.0606610774993896
I0204 11:20:08.666857 140023005427456 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.023770809173584, loss=1.4951881170272827
I0204 11:20:54.900241 140022518892288 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.7892251014709473, loss=1.4984550476074219
I0204 11:21:41.321117 140023005427456 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.885490655899048, loss=1.6151427030563354
I0204 11:22:27.432091 140022518892288 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.2040913105010986, loss=2.55228590965271
I0204 11:23:13.610024 140023005427456 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.439265489578247, loss=2.6597490310668945
I0204 11:23:26.264121 140184451094336 spec.py:321] Evaluating on the training split.
I0204 11:23:37.051703 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 11:24:14.010475 140184451094336 spec.py:349] Evaluating on the test split.
I0204 11:24:15.617928 140184451094336 submission_runner.py:408] Time since start: 85479.89s, 	Step: 166329, 	{'train/accuracy': 0.8364452719688416, 'train/loss': 0.6335233449935913, 'validation/accuracy': 0.7430599927902222, 'validation/loss': 1.0303434133529663, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.660801887512207, 'test/num_examples': 10000, 'score': 76504.16225075722, 'total_duration': 85479.89331364632, 'accumulated_submission_time': 76504.16225075722, 'accumulated_eval_time': 8959.051559209824, 'accumulated_logging_time': 8.026431798934937}
I0204 11:24:15.658620 140022518892288 logging_writer.py:48] [166329] accumulated_eval_time=8959.051559, accumulated_logging_time=8.026432, accumulated_submission_time=76504.162251, global_step=166329, preemption_count=0, score=76504.162251, test/accuracy=0.620500, test/loss=1.660802, test/num_examples=10000, total_duration=85479.893314, train/accuracy=0.836445, train/loss=0.633523, validation/accuracy=0.743060, validation/loss=1.030343, validation/num_examples=50000
I0204 11:24:45.951674 140023005427456 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.7912180423736572, loss=1.9468317031860352
I0204 11:25:31.967750 140022518892288 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.868492364883423, loss=1.3728314638137817
I0204 11:26:18.787540 140023005427456 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.7280049324035645, loss=3.8665363788604736
I0204 11:27:05.066563 140022518892288 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.937093734741211, loss=1.5148155689239502
I0204 11:27:51.322861 140023005427456 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.815889596939087, loss=1.6613937616348267
I0204 11:28:37.999378 140022518892288 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.7197751998901367, loss=1.4955825805664062
I0204 11:29:24.527972 140023005427456 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.606429100036621, loss=1.4810733795166016
I0204 11:30:10.926337 140022518892288 logging_writer.py:48] [167100] global_step=167100, grad_norm=3.1908483505249023, loss=1.6889867782592773
I0204 11:30:57.297546 140023005427456 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.718207597732544, loss=2.275214910507202
I0204 11:31:15.628190 140184451094336 spec.py:321] Evaluating on the training split.
I0204 11:31:26.182512 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 11:32:05.095617 140184451094336 spec.py:349] Evaluating on the test split.
I0204 11:32:06.694546 140184451094336 submission_runner.py:408] Time since start: 85950.97s, 	Step: 167241, 	{'train/accuracy': 0.8333203196525574, 'train/loss': 0.6550984978675842, 'validation/accuracy': 0.7444999814033508, 'validation/loss': 1.0337879657745361, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.6686664819717407, 'test/num_examples': 10000, 'score': 76924.07387590408, 'total_duration': 85950.96992588043, 'accumulated_submission_time': 76924.07387590408, 'accumulated_eval_time': 9010.117893695831, 'accumulated_logging_time': 8.077922344207764}
I0204 11:32:06.738343 140022518892288 logging_writer.py:48] [167241] accumulated_eval_time=9010.117894, accumulated_logging_time=8.077922, accumulated_submission_time=76924.073876, global_step=167241, preemption_count=0, score=76924.073876, test/accuracy=0.619100, test/loss=1.668666, test/num_examples=10000, total_duration=85950.969926, train/accuracy=0.833320, train/loss=0.655098, validation/accuracy=0.744500, validation/loss=1.033788, validation/num_examples=50000
I0204 11:32:31.785728 140023005427456 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.625915288925171, loss=3.1146984100341797
I0204 11:33:17.283069 140022518892288 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.981511116027832, loss=1.5102227926254272
I0204 11:34:04.000981 140023005427456 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.853513240814209, loss=3.0333504676818848
I0204 11:34:50.265505 140022518892288 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.8809635639190674, loss=1.4732412099838257
I0204 11:35:36.856581 140023005427456 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.029120445251465, loss=2.349555015563965
I0204 11:36:23.031821 140022518892288 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.875553607940674, loss=1.4956531524658203
I0204 11:37:09.699616 140023005427456 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.90916109085083, loss=1.4190750122070312
I0204 11:37:55.732104 140022518892288 logging_writer.py:48] [168000] global_step=168000, grad_norm=3.308028221130371, loss=1.3950797319412231
I0204 11:38:42.299185 140023005427456 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.921433925628662, loss=1.5212652683258057
I0204 11:39:07.133509 140184451094336 spec.py:321] Evaluating on the training split.
I0204 11:39:18.064488 140184451094336 spec.py:333] Evaluating on the validation split.
I0204 11:39:52.823251 140184451094336 spec.py:349] Evaluating on the test split.
I0204 11:39:54.417678 140184451094336 submission_runner.py:408] Time since start: 86418.69s, 	Step: 168155, 	{'train/accuracy': 0.8345312476158142, 'train/loss': 0.6509010195732117, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.033661127090454, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.6653497219085693, 'test/num_examples': 10000, 'score': 77344.41281080246, 'total_duration': 86418.69306540489, 'accumulated_submission_time': 77344.41281080246, 'accumulated_eval_time': 9057.402058124542, 'accumulated_logging_time': 8.131011486053467}
I0204 11:39:54.459372 140022518892288 logging_writer.py:48] [168155] accumulated_eval_time=9057.402058, accumulated_logging_time=8.131011, accumulated_submission_time=77344.412811, global_step=168155, preemption_count=0, score=77344.412811, test/accuracy=0.625500, test/loss=1.665350, test/num_examples=10000, total_duration=86418.693065, train/accuracy=0.834531, train/loss=0.650901, validation/accuracy=0.744800, validation/loss=1.033661, validation/num_examples=50000
I0204 11:40:13.678414 140023005427456 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.338310480117798, loss=1.5059235095977783
I0204 11:40:58.579618 140022518892288 logging_writer.py:48] [168300] global_step=168300, grad_norm=3.3584020137786865, loss=2.225857973098755
I0204 11:41:44.926344 140023005427456 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.0167720317840576, loss=2.4544811248779297
I0204 11:42:31.427894 140022518892288 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.714463233947754, loss=2.4550702571868896
I0204 11:42:50.321354 140023005427456 logging_writer.py:48] [168542] global_step=168542, preemption_count=0, score=77520.185804
I0204 11:42:51.012343 140184451094336 checkpoints.py:490] Saving checkpoint at step: 168542
I0204 11:42:52.242188 140184451094336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5/checkpoint_168542
I0204 11:42:52.295878 140184451094336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_vit_jax/trial_5/checkpoint_168542.
I0204 11:42:52.981360 140184451094336 submission_runner.py:583] Tuning trial 5/5
I0204 11:42:52.981579 140184451094336 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0204 11:42:52.991892 140184451094336 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007812499534338713, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 40.48073649406433, 'total_duration': 69.00026822090149, 'accumulated_submission_time': 40.48073649406433, 'accumulated_eval_time': 28.51941752433777, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (848, {'train/accuracy': 0.029550781473517418, 'train/loss': 5.966935157775879, 'validation/accuracy': 0.02531999908387661, 'validation/loss': 6.030439376831055, 'validation/num_examples': 50000, 'test/accuracy': 0.0215000007301569, 'test/loss': 6.148784637451172, 'test/num_examples': 10000, 'score': 460.5695321559906, 'total_duration': 530.42049741745, 'accumulated_submission_time': 460.5695321559906, 'accumulated_eval_time': 69.7873125076294, 'accumulated_logging_time': 0.019226789474487305, 'global_step': 848, 'preemption_count': 0}), (1761, {'train/accuracy': 0.07119140774011612, 'train/loss': 5.351015567779541, 'validation/accuracy': 0.06855999678373337, 'validation/loss': 5.379694938659668, 'validation/num_examples': 50000, 'test/accuracy': 0.054100003093481064, 'test/loss': 5.57318639755249, 'test/num_examples': 10000, 'score': 880.5223081111908, 'total_duration': 998.2155048847198, 'accumulated_submission_time': 880.5223081111908, 'accumulated_eval_time': 117.55456948280334, 'accumulated_logging_time': 0.046671152114868164, 'global_step': 1761, 'preemption_count': 0}), (2678, {'train/accuracy': 0.12623046338558197, 'train/loss': 4.751564025878906, 'validation/accuracy': 0.11949999630451202, 'validation/loss': 4.812889099121094, 'validation/num_examples': 50000, 'test/accuracy': 0.08980000764131546, 'test/loss': 5.094581604003906, 'test/num_examples': 10000, 'score': 1300.5238733291626, 'total_duration': 1465.0715363025665, 'accumulated_submission_time': 1300.5238733291626, 'accumulated_eval_time': 164.33563232421875, 'accumulated_logging_time': 0.07172465324401855, 'global_step': 2678, 'preemption_count': 0}), (3597, {'train/accuracy': 0.18238280713558197, 'train/loss': 4.224600315093994, 'validation/accuracy': 0.16603998839855194, 'validation/loss': 4.346518516540527, 'validation/num_examples': 50000, 'test/accuracy': 0.1284000128507614, 'test/loss': 4.73599910736084, 'test/num_examples': 10000, 'score': 1720.9155325889587, 'total_duration': 1934.8571481704712, 'accumulated_submission_time': 1720.9155325889587, 'accumulated_eval_time': 213.65205097198486, 'accumulated_logging_time': 0.10139894485473633, 'global_step': 3597, 'preemption_count': 0}), (4516, {'train/accuracy': 0.2391015589237213, 'train/loss': 3.7565841674804688, 'validation/accuracy': 0.22258000075817108, 'validation/loss': 3.8583083152770996, 'validation/num_examples': 50000, 'test/accuracy': 0.1705000102519989, 'test/loss': 4.336967945098877, 'test/num_examples': 10000, 'score': 2141.0058159828186, 'total_duration': 2401.094133615494, 'accumulated_submission_time': 2141.0058159828186, 'accumulated_eval_time': 259.7257878780365, 'accumulated_logging_time': 0.1269831657409668, 'global_step': 4516, 'preemption_count': 0}), (5431, {'train/accuracy': 0.27052733302116394, 'train/loss': 3.628831386566162, 'validation/accuracy': 0.24855999648571014, 'validation/loss': 3.7476320266723633, 'validation/num_examples': 50000, 'test/accuracy': 0.1940000057220459, 'test/loss': 4.203524112701416, 'test/num_examples': 10000, 'score': 2561.1740078926086, 'total_duration': 2868.2551929950714, 'accumulated_submission_time': 2561.1740078926086, 'accumulated_eval_time': 306.6399974822998, 'accumulated_logging_time': 0.15290021896362305, 'global_step': 5431, 'preemption_count': 0}), (6348, {'train/accuracy': 0.3121093809604645, 'train/loss': 3.2937893867492676, 'validation/accuracy': 0.2845599949359894, 'validation/loss': 3.4468889236450195, 'validation/num_examples': 50000, 'test/accuracy': 0.2184000164270401, 'test/loss': 3.9825544357299805, 'test/num_examples': 10000, 'score': 2981.2767839431763, 'total_duration': 3334.8607263565063, 'accumulated_submission_time': 2981.2767839431763, 'accumulated_eval_time': 353.06326150894165, 'accumulated_logging_time': 0.18476366996765137, 'global_step': 6348, 'preemption_count': 0}), (7261, {'train/accuracy': 0.3370117247104645, 'train/loss': 3.1292214393615723, 'validation/accuracy': 0.3139199912548065, 'validation/loss': 3.2608413696289062, 'validation/num_examples': 50000, 'test/accuracy': 0.24220001697540283, 'test/loss': 3.815594434738159, 'test/num_examples': 10000, 'score': 3401.2823746204376, 'total_duration': 3803.3420593738556, 'accumulated_submission_time': 3401.2823746204376, 'accumulated_eval_time': 401.46146631240845, 'accumulated_logging_time': 0.21497297286987305, 'global_step': 7261, 'preemption_count': 0}), (8174, {'train/accuracy': 0.35986328125, 'train/loss': 3.0289080142974854, 'validation/accuracy': 0.3312399983406067, 'validation/loss': 3.1762967109680176, 'validation/num_examples': 50000, 'test/accuracy': 0.25690001249313354, 'test/loss': 3.737917423248291, 'test/num_examples': 10000, 'score': 3821.274812936783, 'total_duration': 4268.951048851013, 'accumulated_submission_time': 3821.274812936783, 'accumulated_eval_time': 447.0047791004181, 'accumulated_logging_time': 0.23990464210510254, 'global_step': 8174, 'preemption_count': 0}), (9089, {'train/accuracy': 0.3876953125, 'train/loss': 2.8449742794036865, 'validation/accuracy': 0.3534199893474579, 'validation/loss': 3.0242254734039307, 'validation/num_examples': 50000, 'test/accuracy': 0.2750000059604645, 'test/loss': 3.5979087352752686, 'test/num_examples': 10000, 'score': 4241.370996952057, 'total_duration': 4737.255994081497, 'accumulated_submission_time': 4241.370996952057, 'accumulated_eval_time': 495.13693618774414, 'accumulated_logging_time': 0.269237756729126, 'global_step': 9089, 'preemption_count': 0}), (10003, {'train/accuracy': 0.40519529581069946, 'train/loss': 2.718560218811035, 'validation/accuracy': 0.3734799921512604, 'validation/loss': 2.8857717514038086, 'validation/num_examples': 50000, 'test/accuracy': 0.28700000047683716, 'test/loss': 3.4828357696533203, 'test/num_examples': 10000, 'score': 4661.446325063705, 'total_duration': 5206.459691762924, 'accumulated_submission_time': 4661.446325063705, 'accumulated_eval_time': 544.185914516449, 'accumulated_logging_time': 0.30090880393981934, 'global_step': 10003, 'preemption_count': 0}), (10919, {'train/accuracy': 0.42525389790534973, 'train/loss': 2.589329719543457, 'validation/accuracy': 0.39239999651908875, 'validation/loss': 2.7610809803009033, 'validation/num_examples': 50000, 'test/accuracy': 0.3006000220775604, 'test/loss': 3.390204429626465, 'test/num_examples': 10000, 'score': 5081.747814178467, 'total_duration': 5674.528142929077, 'accumulated_submission_time': 5081.747814178467, 'accumulated_eval_time': 591.8794357776642, 'accumulated_logging_time': 0.32680320739746094, 'global_step': 10919, 'preemption_count': 0}), (11832, {'train/accuracy': 0.4370703101158142, 'train/loss': 2.549715757369995, 'validation/accuracy': 0.398499995470047, 'validation/loss': 2.7432808876037598, 'validation/num_examples': 50000, 'test/accuracy': 0.30980002880096436, 'test/loss': 3.3450920581817627, 'test/num_examples': 10000, 'score': 5501.858088970184, 'total_duration': 6143.224822998047, 'accumulated_submission_time': 5501.858088970184, 'accumulated_eval_time': 640.3868417739868, 'accumulated_logging_time': 0.3585836887359619, 'global_step': 11832, 'preemption_count': 0}), (12746, {'train/accuracy': 0.48222655057907104, 'train/loss': 2.3162496089935303, 'validation/accuracy': 0.4213799834251404, 'validation/loss': 2.6229944229125977, 'validation/num_examples': 50000, 'test/accuracy': 0.3286000192165375, 'test/loss': 3.2327558994293213, 'test/num_examples': 10000, 'score': 5921.982423782349, 'total_duration': 6611.999529123306, 'accumulated_submission_time': 5921.982423782349, 'accumulated_eval_time': 688.9637801647186, 'accumulated_logging_time': 0.3849976062774658, 'global_step': 12746, 'preemption_count': 0}), (13662, {'train/accuracy': 0.4601757824420929, 'train/loss': 2.428586721420288, 'validation/accuracy': 0.4309999942779541, 'validation/loss': 2.577209711074829, 'validation/num_examples': 50000, 'test/accuracy': 0.33230000734329224, 'test/loss': 3.2250514030456543, 'test/num_examples': 10000, 'score': 6342.175592184067, 'total_duration': 7081.163420915604, 'accumulated_submission_time': 6342.175592184067, 'accumulated_eval_time': 737.8570251464844, 'accumulated_logging_time': 0.41483449935913086, 'global_step': 13662, 'preemption_count': 0}), (14578, {'train/accuracy': 0.46898436546325684, 'train/loss': 2.3901267051696777, 'validation/accuracy': 0.43695998191833496, 'validation/loss': 2.566824197769165, 'validation/num_examples': 50000, 'test/accuracy': 0.3360000252723694, 'test/loss': 3.2194483280181885, 'test/num_examples': 10000, 'score': 6762.429019451141, 'total_duration': 7552.847557067871, 'accumulated_submission_time': 6762.429019451141, 'accumulated_eval_time': 789.2101700305939, 'accumulated_logging_time': 0.4447028636932373, 'global_step': 14578, 'preemption_count': 0}), (15492, {'train/accuracy': 0.509960949420929, 'train/loss': 2.1471524238586426, 'validation/accuracy': 0.45419999957084656, 'validation/loss': 2.4339747428894043, 'validation/num_examples': 50000, 'test/accuracy': 0.35110002756118774, 'test/loss': 3.085043430328369, 'test/num_examples': 10000, 'score': 7182.634547472, 'total_duration': 8022.958084821701, 'accumulated_submission_time': 7182.634547472, 'accumulated_eval_time': 839.039304971695, 'accumulated_logging_time': 0.47350120544433594, 'global_step': 15492, 'preemption_count': 0}), (16408, {'train/accuracy': 0.48423826694488525, 'train/loss': 2.291804552078247, 'validation/accuracy': 0.45715999603271484, 'validation/loss': 2.4441986083984375, 'validation/num_examples': 50000, 'test/accuracy': 0.3532000184059143, 'test/loss': 3.1026744842529297, 'test/num_examples': 10000, 'score': 7602.818835020065, 'total_duration': 8492.689853906631, 'accumulated_submission_time': 7602.818835020065, 'accumulated_eval_time': 888.5021407604218, 'accumulated_logging_time': 0.5111334323883057, 'global_step': 16408, 'preemption_count': 0}), (17323, {'train/accuracy': 0.4905664026737213, 'train/loss': 2.2800023555755615, 'validation/accuracy': 0.4546799957752228, 'validation/loss': 2.4581525325775146, 'validation/num_examples': 50000, 'test/accuracy': 0.3483000099658966, 'test/loss': 3.1094908714294434, 'test/num_examples': 10000, 'score': 8022.798815488815, 'total_duration': 8961.801989793777, 'accumulated_submission_time': 8022.798815488815, 'accumulated_eval_time': 937.5597274303436, 'accumulated_logging_time': 0.5380949974060059, 'global_step': 17323, 'preemption_count': 0}), (18239, {'train/accuracy': 0.520800769329071, 'train/loss': 2.135014533996582, 'validation/accuracy': 0.46879997849464417, 'validation/loss': 2.398402452468872, 'validation/num_examples': 50000, 'test/accuracy': 0.3587000072002411, 'test/loss': 3.0502817630767822, 'test/num_examples': 10000, 'score': 8443.062378168106, 'total_duration': 9430.669814825058, 'accumulated_submission_time': 8443.062378168106, 'accumulated_eval_time': 986.0897233486176, 'accumulated_logging_time': 0.5650601387023926, 'global_step': 18239, 'preemption_count': 0}), (19155, {'train/accuracy': 0.5138086080551147, 'train/loss': 2.095219135284424, 'validation/accuracy': 0.4810999929904938, 'validation/loss': 2.2757034301757812, 'validation/num_examples': 50000, 'test/accuracy': 0.3728000223636627, 'test/loss': 2.9553446769714355, 'test/num_examples': 10000, 'score': 8863.191219329834, 'total_duration': 9902.65026807785, 'accumulated_submission_time': 8863.191219329834, 'accumulated_eval_time': 1037.8643803596497, 'accumulated_logging_time': 0.5954127311706543, 'global_step': 19155, 'preemption_count': 0}), (20070, {'train/accuracy': 0.5203906297683716, 'train/loss': 2.1330671310424805, 'validation/accuracy': 0.4761599898338318, 'validation/loss': 2.3456084728240967, 'validation/num_examples': 50000, 'test/accuracy': 0.3712000250816345, 'test/loss': 2.999906301498413, 'test/num_examples': 10000, 'score': 9283.326484203339, 'total_duration': 10370.82237124443, 'accumulated_submission_time': 9283.326484203339, 'accumulated_eval_time': 1085.8274364471436, 'accumulated_logging_time': 0.6221892833709717, 'global_step': 20070, 'preemption_count': 0}), (20987, {'train/accuracy': 0.5329296588897705, 'train/loss': 2.0133917331695557, 'validation/accuracy': 0.48829999566078186, 'validation/loss': 2.249237060546875, 'validation/num_examples': 50000, 'test/accuracy': 0.3790000081062317, 'test/loss': 2.913668394088745, 'test/num_examples': 10000, 'score': 9703.52631521225, 'total_duration': 10840.916332960129, 'accumulated_submission_time': 9703.52631521225, 'accumulated_eval_time': 1135.6430156230927, 'accumulated_logging_time': 0.6531627178192139, 'global_step': 20987, 'preemption_count': 0}), (21901, {'train/accuracy': 0.5308398604393005, 'train/loss': 2.069345474243164, 'validation/accuracy': 0.49559998512268066, 'validation/loss': 2.2566778659820557, 'validation/num_examples': 50000, 'test/accuracy': 0.3831000328063965, 'test/loss': 2.9034087657928467, 'test/num_examples': 10000, 'score': 10123.515281438828, 'total_duration': 11306.940642356873, 'accumulated_submission_time': 10123.515281438828, 'accumulated_eval_time': 1181.5994005203247, 'accumulated_logging_time': 0.6843507289886475, 'global_step': 21901, 'preemption_count': 0}), (22816, {'train/accuracy': 0.5394726395606995, 'train/loss': 2.0032269954681396, 'validation/accuracy': 0.5013399720191956, 'validation/loss': 2.2060070037841797, 'validation/num_examples': 50000, 'test/accuracy': 0.3904000222682953, 'test/loss': 2.8718252182006836, 'test/num_examples': 10000, 'score': 10543.550062179565, 'total_duration': 11775.017482995987, 'accumulated_submission_time': 10543.550062179565, 'accumulated_eval_time': 1229.5627224445343, 'accumulated_logging_time': 0.7155659198760986, 'global_step': 22816, 'preemption_count': 0}), (23732, {'train/accuracy': 0.551074206829071, 'train/loss': 1.9518203735351562, 'validation/accuracy': 0.5073400139808655, 'validation/loss': 2.178410768508911, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.8280670642852783, 'test/num_examples': 10000, 'score': 10963.586684465408, 'total_duration': 12244.024310827255, 'accumulated_submission_time': 10963.586684465408, 'accumulated_eval_time': 1278.4515182971954, 'accumulated_logging_time': 0.7490296363830566, 'global_step': 23732, 'preemption_count': 0}), (24649, {'train/accuracy': 0.5817968845367432, 'train/loss': 1.7977644205093384, 'validation/accuracy': 0.5110399723052979, 'validation/loss': 2.1437582969665527, 'validation/num_examples': 50000, 'test/accuracy': 0.39900001883506775, 'test/loss': 2.818315029144287, 'test/num_examples': 10000, 'score': 11383.736985206604, 'total_duration': 12713.149666309357, 'accumulated_submission_time': 11383.736985206604, 'accumulated_eval_time': 1327.3468503952026, 'accumulated_logging_time': 0.781226396560669, 'global_step': 24649, 'preemption_count': 0}), (25565, {'train/accuracy': 0.5573828220367432, 'train/loss': 1.887807011604309, 'validation/accuracy': 0.5200200080871582, 'validation/loss': 2.082399606704712, 'validation/num_examples': 50000, 'test/accuracy': 0.40450000762939453, 'test/loss': 2.7608931064605713, 'test/num_examples': 10000, 'score': 11803.839218378067, 'total_duration': 13181.796215295792, 'accumulated_submission_time': 11803.839218378067, 'accumulated_eval_time': 1375.8142375946045, 'accumulated_logging_time': 0.8103833198547363, 'global_step': 25565, 'preemption_count': 0}), (26481, {'train/accuracy': 0.5679296851158142, 'train/loss': 1.8611342906951904, 'validation/accuracy': 0.5214599967002869, 'validation/loss': 2.0894598960876465, 'validation/num_examples': 50000, 'test/accuracy': 0.40790000557899475, 'test/loss': 2.758775234222412, 'test/num_examples': 10000, 'score': 12223.86883687973, 'total_duration': 13651.433849334717, 'accumulated_submission_time': 12223.86883687973, 'accumulated_eval_time': 1425.337683916092, 'accumulated_logging_time': 0.8424429893493652, 'global_step': 26481, 'preemption_count': 0}), (27397, {'train/accuracy': 0.5859179496765137, 'train/loss': 1.7362204790115356, 'validation/accuracy': 0.5262599587440491, 'validation/loss': 2.0381267070770264, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.729538917541504, 'test/num_examples': 10000, 'score': 12644.12185382843, 'total_duration': 14122.03713798523, 'accumulated_submission_time': 12644.12185382843, 'accumulated_eval_time': 1475.6100606918335, 'accumulated_logging_time': 0.8734757900238037, 'global_step': 27397, 'preemption_count': 0}), (28313, {'train/accuracy': 0.5691796541213989, 'train/loss': 1.8370025157928467, 'validation/accuracy': 0.5263400077819824, 'validation/loss': 2.0492756366729736, 'validation/num_examples': 50000, 'test/accuracy': 0.4115000069141388, 'test/loss': 2.7372734546661377, 'test/num_examples': 10000, 'score': 13064.183827638626, 'total_duration': 14590.110958814621, 'accumulated_submission_time': 13064.183827638626, 'accumulated_eval_time': 1523.542355298996, 'accumulated_logging_time': 0.9049403667449951, 'global_step': 28313, 'preemption_count': 0}), (29230, {'train/accuracy': 0.5762890577316284, 'train/loss': 1.8103874921798706, 'validation/accuracy': 0.5307799577713013, 'validation/loss': 2.0295510292053223, 'validation/num_examples': 50000, 'test/accuracy': 0.4165000319480896, 'test/loss': 2.705432653427124, 'test/num_examples': 10000, 'score': 13484.137912511826, 'total_duration': 15059.425895929337, 'accumulated_submission_time': 13484.137912511826, 'accumulated_eval_time': 1572.8207762241364, 'accumulated_logging_time': 0.9397439956665039, 'global_step': 29230, 'preemption_count': 0}), (30147, {'train/accuracy': 0.586718738079071, 'train/loss': 1.7852870225906372, 'validation/accuracy': 0.5297799706459045, 'validation/loss': 2.055450677871704, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.7173359394073486, 'test/num_examples': 10000, 'score': 13904.376311302185, 'total_duration': 15528.329751729965, 'accumulated_submission_time': 13904.376311302185, 'accumulated_eval_time': 1621.401178598404, 'accumulated_logging_time': 0.9774858951568604, 'global_step': 30147, 'preemption_count': 0}), (31064, {'train/accuracy': 0.5789452791213989, 'train/loss': 1.8326478004455566, 'validation/accuracy': 0.5329399704933167, 'validation/loss': 2.0538718700408936, 'validation/num_examples': 50000, 'test/accuracy': 0.42000001668930054, 'test/loss': 2.728641986846924, 'test/num_examples': 10000, 'score': 14324.65103316307, 'total_duration': 15996.338223218918, 'accumulated_submission_time': 14324.65103316307, 'accumulated_eval_time': 1669.0584816932678, 'accumulated_logging_time': 1.0061118602752686, 'global_step': 31064, 'preemption_count': 0}), (31981, {'train/accuracy': 0.5824804306030273, 'train/loss': 1.7766609191894531, 'validation/accuracy': 0.5413399934768677, 'validation/loss': 1.9840885400772095, 'validation/num_examples': 50000, 'test/accuracy': 0.4280000329017639, 'test/loss': 2.649587631225586, 'test/num_examples': 10000, 'score': 14744.678804397583, 'total_duration': 16463.911828041077, 'accumulated_submission_time': 14744.678804397583, 'accumulated_eval_time': 1716.5272045135498, 'accumulated_logging_time': 1.0351231098175049, 'global_step': 31981, 'preemption_count': 0}), (32897, {'train/accuracy': 0.5879296660423279, 'train/loss': 1.7226225137710571, 'validation/accuracy': 0.5427199602127075, 'validation/loss': 1.9646421670913696, 'validation/num_examples': 50000, 'test/accuracy': 0.4245000183582306, 'test/loss': 2.6344099044799805, 'test/num_examples': 10000, 'score': 15164.711621046066, 'total_duration': 16931.429044008255, 'accumulated_submission_time': 15164.711621046066, 'accumulated_eval_time': 1763.92360329628, 'accumulated_logging_time': 1.0761663913726807, 'global_step': 32897, 'preemption_count': 0}), (33811, {'train/accuracy': 0.5936523079872131, 'train/loss': 1.7037583589553833, 'validation/accuracy': 0.5494799613952637, 'validation/loss': 1.9261177778244019, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.6165106296539307, 'test/num_examples': 10000, 'score': 15584.832740068436, 'total_duration': 17399.639848709106, 'accumulated_submission_time': 15584.832740068436, 'accumulated_eval_time': 1811.9239838123322, 'accumulated_logging_time': 1.1177661418914795, 'global_step': 33811, 'preemption_count': 0}), (34727, {'train/accuracy': 0.5853710770606995, 'train/loss': 1.762331485748291, 'validation/accuracy': 0.5461999773979187, 'validation/loss': 1.964341163635254, 'validation/num_examples': 50000, 'test/accuracy': 0.43240001797676086, 'test/loss': 2.64608097076416, 'test/num_examples': 10000, 'score': 16004.785893440247, 'total_duration': 17869.55842280388, 'accumulated_submission_time': 16004.785893440247, 'accumulated_eval_time': 1861.8075823783875, 'accumulated_logging_time': 1.1519536972045898, 'global_step': 34727, 'preemption_count': 0}), (35643, {'train/accuracy': 0.6008984446525574, 'train/loss': 1.6964315176010132, 'validation/accuracy': 0.5519599914550781, 'validation/loss': 1.9376044273376465, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.613935708999634, 'test/num_examples': 10000, 'score': 16425.0973944664, 'total_duration': 18338.875772714615, 'accumulated_submission_time': 16425.0973944664, 'accumulated_eval_time': 1910.7358112335205, 'accumulated_logging_time': 1.1812076568603516, 'global_step': 35643, 'preemption_count': 0}), (36555, {'train/accuracy': 0.6290624737739563, 'train/loss': 1.551759958267212, 'validation/accuracy': 0.5550999641418457, 'validation/loss': 1.8873655796051025, 'validation/num_examples': 50000, 'test/accuracy': 0.4384000301361084, 'test/loss': 2.5619187355041504, 'test/num_examples': 10000, 'score': 16845.29321050644, 'total_duration': 18808.527985811234, 'accumulated_submission_time': 16845.29321050644, 'accumulated_eval_time': 1960.112357378006, 'accumulated_logging_time': 1.2128477096557617, 'global_step': 36555, 'preemption_count': 0}), (37470, {'train/accuracy': 0.5981054306030273, 'train/loss': 1.7004003524780273, 'validation/accuracy': 0.5553799867630005, 'validation/loss': 1.9004456996917725, 'validation/num_examples': 50000, 'test/accuracy': 0.44300001859664917, 'test/loss': 2.575869560241699, 'test/num_examples': 10000, 'score': 17265.42009329796, 'total_duration': 19279.006051301956, 'accumulated_submission_time': 17265.42009329796, 'accumulated_eval_time': 2010.38028049469, 'accumulated_logging_time': 1.2492239475250244, 'global_step': 37470, 'preemption_count': 0}), (38384, {'train/accuracy': 0.5977538824081421, 'train/loss': 1.6697273254394531, 'validation/accuracy': 0.5511000156402588, 'validation/loss': 1.906893253326416, 'validation/num_examples': 50000, 'test/accuracy': 0.4426000118255615, 'test/loss': 2.580732822418213, 'test/num_examples': 10000, 'score': 17685.49142575264, 'total_duration': 19748.276788711548, 'accumulated_submission_time': 17685.49142575264, 'accumulated_eval_time': 2059.4976251125336, 'accumulated_logging_time': 1.2838959693908691, 'global_step': 38384, 'preemption_count': 0}), (39298, {'train/accuracy': 0.6231836080551147, 'train/loss': 1.5941240787506104, 'validation/accuracy': 0.5628399848937988, 'validation/loss': 1.888396143913269, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.558645725250244, 'test/num_examples': 10000, 'score': 18105.790986537933, 'total_duration': 20217.332773685455, 'accumulated_submission_time': 18105.790986537933, 'accumulated_eval_time': 2108.164050579071, 'accumulated_logging_time': 1.3266685009002686, 'global_step': 39298, 'preemption_count': 0}), (40214, {'train/accuracy': 0.5969336032867432, 'train/loss': 1.700569987297058, 'validation/accuracy': 0.55485999584198, 'validation/loss': 1.9114704132080078, 'validation/num_examples': 50000, 'test/accuracy': 0.44140002131462097, 'test/loss': 2.5693047046661377, 'test/num_examples': 10000, 'score': 18526.15670633316, 'total_duration': 20686.453429460526, 'accumulated_submission_time': 18526.15670633316, 'accumulated_eval_time': 2156.8390045166016, 'accumulated_logging_time': 1.3593740463256836, 'global_step': 40214, 'preemption_count': 0}), (41130, {'train/accuracy': 0.6115429401397705, 'train/loss': 1.6528892517089844, 'validation/accuracy': 0.5624399781227112, 'validation/loss': 1.895995855331421, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.548941135406494, 'test/num_examples': 10000, 'score': 18946.13728427887, 'total_duration': 21155.588027715683, 'accumulated_submission_time': 18946.13728427887, 'accumulated_eval_time': 2205.9153735637665, 'accumulated_logging_time': 1.389624834060669, 'global_step': 41130, 'preemption_count': 0}), (42044, {'train/accuracy': 0.6252148151397705, 'train/loss': 1.5360467433929443, 'validation/accuracy': 0.5721399784088135, 'validation/loss': 1.8095941543579102, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.498311758041382, 'test/num_examples': 10000, 'score': 19366.324320554733, 'total_duration': 21624.983068466187, 'accumulated_submission_time': 19366.324320554733, 'accumulated_eval_time': 2255.0452842712402, 'accumulated_logging_time': 1.4197359085083008, 'global_step': 42044, 'preemption_count': 0}), (42957, {'train/accuracy': 0.6100390553474426, 'train/loss': 1.6844910383224487, 'validation/accuracy': 0.5631600022315979, 'validation/loss': 1.9153586626052856, 'validation/num_examples': 50000, 'test/accuracy': 0.44850000739097595, 'test/loss': 2.561943292617798, 'test/num_examples': 10000, 'score': 19786.389555692673, 'total_duration': 22095.019829034805, 'accumulated_submission_time': 19786.389555692673, 'accumulated_eval_time': 2304.938737630844, 'accumulated_logging_time': 1.450535774230957, 'global_step': 42957, 'preemption_count': 0}), (43869, {'train/accuracy': 0.6125780940055847, 'train/loss': 1.6683449745178223, 'validation/accuracy': 0.5681599974632263, 'validation/loss': 1.8960041999816895, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.5512001514434814, 'test/num_examples': 10000, 'score': 20206.347382307053, 'total_duration': 22562.94238114357, 'accumulated_submission_time': 20206.347382307053, 'accumulated_eval_time': 2352.8230979442596, 'accumulated_logging_time': 1.4831857681274414, 'global_step': 43869, 'preemption_count': 0}), (44782, {'train/accuracy': 0.6306250095367432, 'train/loss': 1.5366846323013306, 'validation/accuracy': 0.5754599571228027, 'validation/loss': 1.7981550693511963, 'validation/num_examples': 50000, 'test/accuracy': 0.4561000168323517, 'test/loss': 2.4726850986480713, 'test/num_examples': 10000, 'score': 20626.368554592133, 'total_duration': 23032.650621652603, 'accumulated_submission_time': 20626.368554592133, 'accumulated_eval_time': 2402.4246587753296, 'accumulated_logging_time': 1.521988868713379, 'global_step': 44782, 'preemption_count': 0}), (45695, {'train/accuracy': 0.6145312190055847, 'train/loss': 1.6023259162902832, 'validation/accuracy': 0.5731599926948547, 'validation/loss': 1.8077213764190674, 'validation/num_examples': 50000, 'test/accuracy': 0.46310001611709595, 'test/loss': 2.4594175815582275, 'test/num_examples': 10000, 'score': 21046.379149913788, 'total_duration': 23501.57047510147, 'accumulated_submission_time': 21046.379149913788, 'accumulated_eval_time': 2451.248507976532, 'accumulated_logging_time': 1.560218334197998, 'global_step': 45695, 'preemption_count': 0}), (46611, {'train/accuracy': 0.6163476705551147, 'train/loss': 1.598486304283142, 'validation/accuracy': 0.5700199604034424, 'validation/loss': 1.8413797616958618, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.510585308074951, 'test/num_examples': 10000, 'score': 21466.459342956543, 'total_duration': 23970.574808597565, 'accumulated_submission_time': 21466.459342956543, 'accumulated_eval_time': 2500.084460258484, 'accumulated_logging_time': 1.5997076034545898, 'global_step': 46611, 'preemption_count': 0}), (47521, {'train/accuracy': 0.629199206829071, 'train/loss': 1.6050124168395996, 'validation/accuracy': 0.575760006904602, 'validation/loss': 1.858893871307373, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.499596118927002, 'test/num_examples': 10000, 'score': 21886.386165380478, 'total_duration': 24440.17867231369, 'accumulated_submission_time': 21886.386165380478, 'accumulated_eval_time': 2549.6821115016937, 'accumulated_logging_time': 1.631948709487915, 'global_step': 47521, 'preemption_count': 0}), (48431, {'train/accuracy': 0.6450585722923279, 'train/loss': 1.445672631263733, 'validation/accuracy': 0.5813400149345398, 'validation/loss': 1.7478997707366943, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.401552677154541, 'test/num_examples': 10000, 'score': 22306.334075450897, 'total_duration': 24908.24243426323, 'accumulated_submission_time': 22306.334075450897, 'accumulated_eval_time': 2597.7186567783356, 'accumulated_logging_time': 1.6638882160186768, 'global_step': 48431, 'preemption_count': 0}), (49343, {'train/accuracy': 0.623046875, 'train/loss': 1.623903512954712, 'validation/accuracy': 0.5805599689483643, 'validation/loss': 1.835086703300476, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.4847159385681152, 'test/num_examples': 10000, 'score': 22726.702834129333, 'total_duration': 25377.72305703163, 'accumulated_submission_time': 22726.702834129333, 'accumulated_eval_time': 2646.743688106537, 'accumulated_logging_time': 1.7032458782196045, 'global_step': 49343, 'preemption_count': 0}), (50258, {'train/accuracy': 0.640917956829071, 'train/loss': 1.4934781789779663, 'validation/accuracy': 0.5890600085258484, 'validation/loss': 1.7496922016143799, 'validation/num_examples': 50000, 'test/accuracy': 0.46650001406669617, 'test/loss': 2.422419309616089, 'test/num_examples': 10000, 'score': 23146.965981721878, 'total_duration': 25848.046773910522, 'accumulated_submission_time': 23146.965981721878, 'accumulated_eval_time': 2696.7211923599243, 'accumulated_logging_time': 1.7389581203460693, 'global_step': 50258, 'preemption_count': 0}), (51173, {'train/accuracy': 0.654101550579071, 'train/loss': 1.4827535152435303, 'validation/accuracy': 0.582260012626648, 'validation/loss': 1.8158226013183594, 'validation/num_examples': 50000, 'test/accuracy': 0.46250003576278687, 'test/loss': 2.461416482925415, 'test/num_examples': 10000, 'score': 23567.253214359283, 'total_duration': 26316.64104104042, 'accumulated_submission_time': 23567.253214359283, 'accumulated_eval_time': 2744.944550037384, 'accumulated_logging_time': 1.7745862007141113, 'global_step': 51173, 'preemption_count': 0}), (52086, {'train/accuracy': 0.6284374594688416, 'train/loss': 1.56432044506073, 'validation/accuracy': 0.5817399621009827, 'validation/loss': 1.7849321365356445, 'validation/num_examples': 50000, 'test/accuracy': 0.4642000198364258, 'test/loss': 2.44661545753479, 'test/num_examples': 10000, 'score': 23987.205542325974, 'total_duration': 26784.412437677383, 'accumulated_submission_time': 23987.205542325974, 'accumulated_eval_time': 2792.6781933307648, 'accumulated_logging_time': 1.8138036727905273, 'global_step': 52086, 'preemption_count': 0}), (53002, {'train/accuracy': 0.6423242092132568, 'train/loss': 1.4505491256713867, 'validation/accuracy': 0.5980799794197083, 'validation/loss': 1.690264344215393, 'validation/num_examples': 50000, 'test/accuracy': 0.4789000153541565, 'test/loss': 2.357414960861206, 'test/num_examples': 10000, 'score': 24407.16034078598, 'total_duration': 27253.20618200302, 'accumulated_submission_time': 24407.16034078598, 'accumulated_eval_time': 2841.4261870384216, 'accumulated_logging_time': 1.8572685718536377, 'global_step': 53002, 'preemption_count': 0}), (53916, {'train/accuracy': 0.6451367139816284, 'train/loss': 1.5169159173965454, 'validation/accuracy': 0.583579957485199, 'validation/loss': 1.813071370124817, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.465859889984131, 'test/num_examples': 10000, 'score': 24827.353207588196, 'total_duration': 27724.505935430527, 'accumulated_submission_time': 24827.353207588196, 'accumulated_eval_time': 2892.447431087494, 'accumulated_logging_time': 1.8953602313995361, 'global_step': 53916, 'preemption_count': 0}), (54832, {'train/accuracy': 0.6351171731948853, 'train/loss': 1.4957752227783203, 'validation/accuracy': 0.5937199592590332, 'validation/loss': 1.7042373418807983, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.388838529586792, 'test/num_examples': 10000, 'score': 25247.272602796555, 'total_duration': 28193.08148908615, 'accumulated_submission_time': 25247.272602796555, 'accumulated_eval_time': 2941.016970396042, 'accumulated_logging_time': 1.9336466789245605, 'global_step': 54832, 'preemption_count': 0}), (55740, {'train/accuracy': 0.64501953125, 'train/loss': 1.4755982160568237, 'validation/accuracy': 0.5958200097084045, 'validation/loss': 1.7111175060272217, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.404808759689331, 'test/num_examples': 10000, 'score': 25667.20871949196, 'total_duration': 28663.57295012474, 'accumulated_submission_time': 25667.20871949196, 'accumulated_eval_time': 2991.4879484176636, 'accumulated_logging_time': 1.9717350006103516, 'global_step': 55740, 'preemption_count': 0}), (56651, {'train/accuracy': 0.6513866782188416, 'train/loss': 1.4567426443099976, 'validation/accuracy': 0.590999960899353, 'validation/loss': 1.7368868589401245, 'validation/num_examples': 50000, 'test/accuracy': 0.4706000089645386, 'test/loss': 2.3935937881469727, 'test/num_examples': 10000, 'score': 26087.488626241684, 'total_duration': 29132.650985717773, 'accumulated_submission_time': 26087.488626241684, 'accumulated_eval_time': 3040.205552339554, 'accumulated_logging_time': 2.005126714706421, 'global_step': 56651, 'preemption_count': 0}), (57560, {'train/accuracy': 0.6385351419448853, 'train/loss': 1.554219126701355, 'validation/accuracy': 0.593239963054657, 'validation/loss': 1.763049840927124, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.4218482971191406, 'test/num_examples': 10000, 'score': 26507.458403348923, 'total_duration': 29602.409747600555, 'accumulated_submission_time': 26507.458403348923, 'accumulated_eval_time': 3089.9091703891754, 'accumulated_logging_time': 2.0419747829437256, 'global_step': 57560, 'preemption_count': 0}), (58474, {'train/accuracy': 0.6503124833106995, 'train/loss': 1.4144947528839111, 'validation/accuracy': 0.6011399626731873, 'validation/loss': 1.6524994373321533, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.323306083679199, 'test/num_examples': 10000, 'score': 26927.61052799225, 'total_duration': 30073.152743577957, 'accumulated_submission_time': 26927.61052799225, 'accumulated_eval_time': 3140.4162259101868, 'accumulated_logging_time': 2.0792157649993896, 'global_step': 58474, 'preemption_count': 0}), (59389, {'train/accuracy': 0.6558203101158142, 'train/loss': 1.472233533859253, 'validation/accuracy': 0.5989399552345276, 'validation/loss': 1.731550931930542, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.381978988647461, 'test/num_examples': 10000, 'score': 27347.89120697975, 'total_duration': 30542.191057682037, 'accumulated_submission_time': 27347.89120697975, 'accumulated_eval_time': 3189.0866141319275, 'accumulated_logging_time': 2.1189894676208496, 'global_step': 59389, 'preemption_count': 0}), (60301, {'train/accuracy': 0.64208984375, 'train/loss': 1.5133836269378662, 'validation/accuracy': 0.5927799940109253, 'validation/loss': 1.7447527647018433, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.4132392406463623, 'test/num_examples': 10000, 'score': 27767.984453201294, 'total_duration': 31011.865272283554, 'accumulated_submission_time': 27767.984453201294, 'accumulated_eval_time': 3238.5846648216248, 'accumulated_logging_time': 2.1544106006622314, 'global_step': 60301, 'preemption_count': 0}), (61215, {'train/accuracy': 0.6486132740974426, 'train/loss': 1.5043102502822876, 'validation/accuracy': 0.6002399921417236, 'validation/loss': 1.7300899028778076, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.3871309757232666, 'test/num_examples': 10000, 'score': 28188.03951358795, 'total_duration': 31482.68537425995, 'accumulated_submission_time': 28188.03951358795, 'accumulated_eval_time': 3289.2686598300934, 'accumulated_logging_time': 2.187922954559326, 'global_step': 61215, 'preemption_count': 0}), (62126, {'train/accuracy': 0.6597460508346558, 'train/loss': 1.4151033163070679, 'validation/accuracy': 0.6001200079917908, 'validation/loss': 1.6884820461273193, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.353926658630371, 'test/num_examples': 10000, 'score': 28607.97818994522, 'total_duration': 31952.919785261154, 'accumulated_submission_time': 28607.97818994522, 'accumulated_eval_time': 3339.475342273712, 'accumulated_logging_time': 2.2301175594329834, 'global_step': 62126, 'preemption_count': 0}), (63040, {'train/accuracy': 0.6842187643051147, 'train/loss': 1.2707115411758423, 'validation/accuracy': 0.6073399782180786, 'validation/loss': 1.6327868700027466, 'validation/num_examples': 50000, 'test/accuracy': 0.4918000102043152, 'test/loss': 2.2991316318511963, 'test/num_examples': 10000, 'score': 29028.256913661957, 'total_duration': 32421.085930347443, 'accumulated_submission_time': 29028.256913661957, 'accumulated_eval_time': 3387.2805788517, 'accumulated_logging_time': 2.26474666595459, 'global_step': 63040, 'preemption_count': 0}), (63953, {'train/accuracy': 0.6486914157867432, 'train/loss': 1.453892469406128, 'validation/accuracy': 0.6010400056838989, 'validation/loss': 1.6871942281723022, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.333249092102051, 'test/num_examples': 10000, 'score': 29448.54456448555, 'total_duration': 32891.12489771843, 'accumulated_submission_time': 29448.54456448555, 'accumulated_eval_time': 3436.948952436447, 'accumulated_logging_time': 2.3011348247528076, 'global_step': 63953, 'preemption_count': 0}), (64867, {'train/accuracy': 0.658203125, 'train/loss': 1.4299410581588745, 'validation/accuracy': 0.6074599623680115, 'validation/loss': 1.6725072860717773, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.3327584266662598, 'test/num_examples': 10000, 'score': 29868.568327188492, 'total_duration': 33362.58354306221, 'accumulated_submission_time': 29868.568327188492, 'accumulated_eval_time': 3488.2956540584564, 'accumulated_logging_time': 2.3419806957244873, 'global_step': 64867, 'preemption_count': 0}), (65780, {'train/accuracy': 0.6772069931030273, 'train/loss': 1.291306734085083, 'validation/accuracy': 0.6087999939918518, 'validation/loss': 1.6291512250900269, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.2988369464874268, 'test/num_examples': 10000, 'score': 30288.65084552765, 'total_duration': 33832.26501727104, 'accumulated_submission_time': 30288.65084552765, 'accumulated_eval_time': 3537.7987973690033, 'accumulated_logging_time': 2.3904261589050293, 'global_step': 65780, 'preemption_count': 0}), (66695, {'train/accuracy': 0.6553320288658142, 'train/loss': 1.4468097686767578, 'validation/accuracy': 0.6100599765777588, 'validation/loss': 1.668556571006775, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.307793378829956, 'test/num_examples': 10000, 'score': 30708.979912281036, 'total_duration': 34301.335938215256, 'accumulated_submission_time': 30708.979912281036, 'accumulated_eval_time': 3586.4557435512543, 'accumulated_logging_time': 2.428715467453003, 'global_step': 66695, 'preemption_count': 0}), (67610, {'train/accuracy': 0.6596874594688416, 'train/loss': 1.3802225589752197, 'validation/accuracy': 0.6093400120735168, 'validation/loss': 1.6242049932479858, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3031609058380127, 'test/num_examples': 10000, 'score': 31128.98797106743, 'total_duration': 34770.267876148224, 'accumulated_submission_time': 31128.98797106743, 'accumulated_eval_time': 3635.289438724518, 'accumulated_logging_time': 2.4718663692474365, 'global_step': 67610, 'preemption_count': 0}), (68524, {'train/accuracy': 0.6707226634025574, 'train/loss': 1.330780267715454, 'validation/accuracy': 0.6122999787330627, 'validation/loss': 1.6297461986541748, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.2985339164733887, 'test/num_examples': 10000, 'score': 31548.975848913193, 'total_duration': 35238.550125837326, 'accumulated_submission_time': 31548.975848913193, 'accumulated_eval_time': 3683.4893350601196, 'accumulated_logging_time': 2.515212059020996, 'global_step': 68524, 'preemption_count': 0}), (69435, {'train/accuracy': 0.6629882454872131, 'train/loss': 1.3963218927383423, 'validation/accuracy': 0.6138799786567688, 'validation/loss': 1.6306712627410889, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.291445732116699, 'test/num_examples': 10000, 'score': 31968.895731449127, 'total_duration': 35706.80040049553, 'accumulated_submission_time': 31968.895731449127, 'accumulated_eval_time': 3731.7338008880615, 'accumulated_logging_time': 2.553818941116333, 'global_step': 69435, 'preemption_count': 0}), (70350, {'train/accuracy': 0.66259765625, 'train/loss': 1.3925042152404785, 'validation/accuracy': 0.6094799637794495, 'validation/loss': 1.6426547765731812, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.2989656925201416, 'test/num_examples': 10000, 'score': 32389.219320058823, 'total_duration': 36176.90175771713, 'accumulated_submission_time': 32389.219320058823, 'accumulated_eval_time': 3781.4290795326233, 'accumulated_logging_time': 2.589449882507324, 'global_step': 70350, 'preemption_count': 0}), (71265, {'train/accuracy': 0.6683593392372131, 'train/loss': 1.397711157798767, 'validation/accuracy': 0.6119399666786194, 'validation/loss': 1.6650700569152832, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.3102755546569824, 'test/num_examples': 10000, 'score': 32809.368601322174, 'total_duration': 36646.70735788345, 'accumulated_submission_time': 32809.368601322174, 'accumulated_eval_time': 3830.9983818531036, 'accumulated_logging_time': 2.6295628547668457, 'global_step': 71265, 'preemption_count': 0}), (72179, {'train/accuracy': 0.6663671731948853, 'train/loss': 1.3786489963531494, 'validation/accuracy': 0.6188600063323975, 'validation/loss': 1.6109440326690674, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.268369674682617, 'test/num_examples': 10000, 'score': 33229.415531635284, 'total_duration': 37116.63294029236, 'accumulated_submission_time': 33229.415531635284, 'accumulated_eval_time': 3880.7879054546356, 'accumulated_logging_time': 2.671271324157715, 'global_step': 72179, 'preemption_count': 0}), (73093, {'train/accuracy': 0.6658984422683716, 'train/loss': 1.3752154111862183, 'validation/accuracy': 0.6152799725532532, 'validation/loss': 1.6217827796936035, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.2937076091766357, 'test/num_examples': 10000, 'score': 33649.62982439995, 'total_duration': 37585.90481185913, 'accumulated_submission_time': 33649.62982439995, 'accumulated_eval_time': 3929.7533457279205, 'accumulated_logging_time': 2.7157506942749023, 'global_step': 73093, 'preemption_count': 0}), (74008, {'train/accuracy': 0.6753124594688416, 'train/loss': 1.3523850440979004, 'validation/accuracy': 0.6179999709129333, 'validation/loss': 1.608821153640747, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.2665867805480957, 'test/num_examples': 10000, 'score': 34069.875893354416, 'total_duration': 38055.59846138954, 'accumulated_submission_time': 34069.875893354416, 'accumulated_eval_time': 3979.105614423752, 'accumulated_logging_time': 2.763178586959839, 'global_step': 74008, 'preemption_count': 0}), (74919, {'train/accuracy': 0.695605456829071, 'train/loss': 1.2618730068206787, 'validation/accuracy': 0.6238799691200256, 'validation/loss': 1.5817739963531494, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.2340173721313477, 'test/num_examples': 10000, 'score': 34489.79175186157, 'total_duration': 38525.39829945564, 'accumulated_submission_time': 34489.79175186157, 'accumulated_eval_time': 4028.905200958252, 'accumulated_logging_time': 2.799935817718506, 'global_step': 74919, 'preemption_count': 0}), (75832, {'train/accuracy': 0.6705663800239563, 'train/loss': 1.3746696710586548, 'validation/accuracy': 0.6190599799156189, 'validation/loss': 1.6183385848999023, 'validation/num_examples': 50000, 'test/accuracy': 0.4936000108718872, 'test/loss': 2.2937138080596924, 'test/num_examples': 10000, 'score': 34909.78695511818, 'total_duration': 38997.88926529884, 'accumulated_submission_time': 34909.78695511818, 'accumulated_eval_time': 4081.3146035671234, 'accumulated_logging_time': 2.8388900756835938, 'global_step': 75832, 'preemption_count': 0}), (76745, {'train/accuracy': 0.6803905963897705, 'train/loss': 1.297563910484314, 'validation/accuracy': 0.6237199902534485, 'validation/loss': 1.563851237297058, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.204143762588501, 'test/num_examples': 10000, 'score': 35329.75003695488, 'total_duration': 39465.47510480881, 'accumulated_submission_time': 35329.75003695488, 'accumulated_eval_time': 4128.850201368332, 'accumulated_logging_time': 2.8785297870635986, 'global_step': 76745, 'preemption_count': 0}), (77660, {'train/accuracy': 0.6931836009025574, 'train/loss': 1.2485311031341553, 'validation/accuracy': 0.6234999895095825, 'validation/loss': 1.5847680568695068, 'validation/num_examples': 50000, 'test/accuracy': 0.5022000074386597, 'test/loss': 2.245917320251465, 'test/num_examples': 10000, 'score': 35749.74824357033, 'total_duration': 39935.989151239395, 'accumulated_submission_time': 35749.74824357033, 'accumulated_eval_time': 4179.281141281128, 'accumulated_logging_time': 2.9159533977508545, 'global_step': 77660, 'preemption_count': 0}), (78574, {'train/accuracy': 0.6760546565055847, 'train/loss': 1.3154542446136475, 'validation/accuracy': 0.6225399971008301, 'validation/loss': 1.5719152688980103, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.222673177719116, 'test/num_examples': 10000, 'score': 36169.981214761734, 'total_duration': 40402.831394672394, 'accumulated_submission_time': 36169.981214761734, 'accumulated_eval_time': 4225.807715415955, 'accumulated_logging_time': 2.9519596099853516, 'global_step': 78574, 'preemption_count': 0}), (79489, {'train/accuracy': 0.6782616972923279, 'train/loss': 1.3288729190826416, 'validation/accuracy': 0.6286799907684326, 'validation/loss': 1.572872281074524, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.23213791847229, 'test/num_examples': 10000, 'score': 36590.37432670593, 'total_duration': 40870.50536131859, 'accumulated_submission_time': 36590.37432670593, 'accumulated_eval_time': 4273.000194072723, 'accumulated_logging_time': 2.9928932189941406, 'global_step': 79489, 'preemption_count': 0}), (80405, {'train/accuracy': 0.6900194883346558, 'train/loss': 1.277381420135498, 'validation/accuracy': 0.6231399774551392, 'validation/loss': 1.5857053995132446, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.244521379470825, 'test/num_examples': 10000, 'score': 37010.2838101387, 'total_duration': 41339.1589307785, 'accumulated_submission_time': 37010.2838101387, 'accumulated_eval_time': 4321.6484797000885, 'accumulated_logging_time': 3.0400874614715576, 'global_step': 80405, 'preemption_count': 0}), (81320, {'train/accuracy': 0.6727538704872131, 'train/loss': 1.3480576276779175, 'validation/accuracy': 0.6234999895095825, 'validation/loss': 1.5841037034988403, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2456095218658447, 'test/num_examples': 10000, 'score': 37430.524639606476, 'total_duration': 41809.0449860096, 'accumulated_submission_time': 37430.524639606476, 'accumulated_eval_time': 4371.208312034607, 'accumulated_logging_time': 3.0785329341888428, 'global_step': 81320, 'preemption_count': 0}), (82232, {'train/accuracy': 0.6878319978713989, 'train/loss': 1.2847343683242798, 'validation/accuracy': 0.6326599717140198, 'validation/loss': 1.5323657989501953, 'validation/num_examples': 50000, 'test/accuracy': 0.5116000175476074, 'test/loss': 2.190577745437622, 'test/num_examples': 10000, 'score': 37850.4936645031, 'total_duration': 42277.68804812431, 'accumulated_submission_time': 37850.4936645031, 'accumulated_eval_time': 4419.785451173782, 'accumulated_logging_time': 3.1283504962921143, 'global_step': 82232, 'preemption_count': 0}), (83145, {'train/accuracy': 0.6927343606948853, 'train/loss': 1.2806254625320435, 'validation/accuracy': 0.6339799761772156, 'validation/loss': 1.5582224130630493, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.209243059158325, 'test/num_examples': 10000, 'score': 38270.45635151863, 'total_duration': 42744.11470103264, 'accumulated_submission_time': 38270.45635151863, 'accumulated_eval_time': 4466.1608464717865, 'accumulated_logging_time': 3.169515371322632, 'global_step': 83145, 'preemption_count': 0}), (84058, {'train/accuracy': 0.6813867092132568, 'train/loss': 1.28969407081604, 'validation/accuracy': 0.6294599771499634, 'validation/loss': 1.5353686809539795, 'validation/num_examples': 50000, 'test/accuracy': 0.5073000192642212, 'test/loss': 2.186338186264038, 'test/num_examples': 10000, 'score': 38690.68273019791, 'total_duration': 43214.1302447319, 'accumulated_submission_time': 38690.68273019791, 'accumulated_eval_time': 4515.86549949646, 'accumulated_logging_time': 3.206382989883423, 'global_step': 84058, 'preemption_count': 0}), (84974, {'train/accuracy': 0.688281238079071, 'train/loss': 1.2811274528503418, 'validation/accuracy': 0.6375600099563599, 'validation/loss': 1.5175962448120117, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.1811633110046387, 'test/num_examples': 10000, 'score': 39111.102942466736, 'total_duration': 43683.11135816574, 'accumulated_submission_time': 39111.102942466736, 'accumulated_eval_time': 4564.3340582847595, 'accumulated_logging_time': 3.2509865760803223, 'global_step': 84974, 'preemption_count': 0}), (85887, {'train/accuracy': 0.6952148079872131, 'train/loss': 1.2292883396148682, 'validation/accuracy': 0.6356799602508545, 'validation/loss': 1.519616723060608, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.1769161224365234, 'test/num_examples': 10000, 'score': 39531.074053287506, 'total_duration': 44151.64874100685, 'accumulated_submission_time': 39531.074053287506, 'accumulated_eval_time': 4612.810303688049, 'accumulated_logging_time': 3.2939980030059814, 'global_step': 85887, 'preemption_count': 0}), (86798, {'train/accuracy': 0.6976562142372131, 'train/loss': 1.220523476600647, 'validation/accuracy': 0.6376199722290039, 'validation/loss': 1.5006834268569946, 'validation/num_examples': 50000, 'test/accuracy': 0.5117000341415405, 'test/loss': 2.1646652221679688, 'test/num_examples': 10000, 'score': 39951.19725751877, 'total_duration': 44620.83441233635, 'accumulated_submission_time': 39951.19725751877, 'accumulated_eval_time': 4661.783539533615, 'accumulated_logging_time': 3.335458278656006, 'global_step': 86798, 'preemption_count': 0}), (87713, {'train/accuracy': 0.6788867115974426, 'train/loss': 1.3864643573760986, 'validation/accuracy': 0.6287400126457214, 'validation/loss': 1.6301475763320923, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.2643375396728516, 'test/num_examples': 10000, 'score': 40371.22511386871, 'total_duration': 45092.05185699463, 'accumulated_submission_time': 40371.22511386871, 'accumulated_eval_time': 4712.8799839019775, 'accumulated_logging_time': 3.3804314136505127, 'global_step': 87713, 'preemption_count': 0}), (88626, {'train/accuracy': 0.696582019329071, 'train/loss': 1.239540934562683, 'validation/accuracy': 0.6377399563789368, 'validation/loss': 1.517307162284851, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.1748321056365967, 'test/num_examples': 10000, 'score': 40791.30937457085, 'total_duration': 45559.40066933632, 'accumulated_submission_time': 40791.30937457085, 'accumulated_eval_time': 4760.05557847023, 'accumulated_logging_time': 3.4225516319274902, 'global_step': 88626, 'preemption_count': 0}), (89538, {'train/accuracy': 0.71888667345047, 'train/loss': 1.155856728553772, 'validation/accuracy': 0.6437000036239624, 'validation/loss': 1.506482481956482, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.1656334400177, 'test/num_examples': 10000, 'score': 41211.301359415054, 'total_duration': 46029.07649850845, 'accumulated_submission_time': 41211.301359415054, 'accumulated_eval_time': 4809.654201030731, 'accumulated_logging_time': 3.460923194885254, 'global_step': 89538, 'preemption_count': 0}), (90449, {'train/accuracy': 0.6948632597923279, 'train/loss': 1.2670438289642334, 'validation/accuracy': 0.6413399577140808, 'validation/loss': 1.5187033414840698, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.1620571613311768, 'test/num_examples': 10000, 'score': 41631.52504038811, 'total_duration': 46498.67860245705, 'accumulated_submission_time': 41631.52504038811, 'accumulated_eval_time': 4858.942010641098, 'accumulated_logging_time': 3.504815101623535, 'global_step': 90449, 'preemption_count': 0}), (91365, {'train/accuracy': 0.7024804353713989, 'train/loss': 1.197124719619751, 'validation/accuracy': 0.6451199650764465, 'validation/loss': 1.4732191562652588, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.1140828132629395, 'test/num_examples': 10000, 'score': 42051.71841979027, 'total_duration': 46967.12596321106, 'accumulated_submission_time': 42051.71841979027, 'accumulated_eval_time': 4907.109016418457, 'accumulated_logging_time': 3.5443942546844482, 'global_step': 91365, 'preemption_count': 0}), (92280, {'train/accuracy': 0.7185351252555847, 'train/loss': 1.1746147871017456, 'validation/accuracy': 0.6480799913406372, 'validation/loss': 1.501075267791748, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.161663293838501, 'test/num_examples': 10000, 'score': 42471.94208693504, 'total_duration': 47435.50911140442, 'accumulated_submission_time': 42471.94208693504, 'accumulated_eval_time': 4955.174675226212, 'accumulated_logging_time': 3.5913169384002686, 'global_step': 92280, 'preemption_count': 0}), (93194, {'train/accuracy': 0.7062109112739563, 'train/loss': 1.185657262802124, 'validation/accuracy': 0.6523799896240234, 'validation/loss': 1.4381099939346313, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.1004345417022705, 'test/num_examples': 10000, 'score': 42892.12707614899, 'total_duration': 47906.19521903992, 'accumulated_submission_time': 42892.12707614899, 'accumulated_eval_time': 5005.583575248718, 'accumulated_logging_time': 3.6359968185424805, 'global_step': 93194, 'preemption_count': 0}), (94110, {'train/accuracy': 0.7049023509025574, 'train/loss': 1.2190285921096802, 'validation/accuracy': 0.6449199914932251, 'validation/loss': 1.500189185142517, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.142191171646118, 'test/num_examples': 10000, 'score': 43312.31781864166, 'total_duration': 48376.341938734055, 'accumulated_submission_time': 43312.31781864166, 'accumulated_eval_time': 5055.447685480118, 'accumulated_logging_time': 3.6795918941497803, 'global_step': 94110, 'preemption_count': 0}), (95025, {'train/accuracy': 0.7074413895606995, 'train/loss': 1.209861397743225, 'validation/accuracy': 0.6448799967765808, 'validation/loss': 1.5044829845428467, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.164458990097046, 'test/num_examples': 10000, 'score': 43732.444350004196, 'total_duration': 48846.29187488556, 'accumulated_submission_time': 43732.444350004196, 'accumulated_eval_time': 5105.180654525757, 'accumulated_logging_time': 3.722055435180664, 'global_step': 95025, 'preemption_count': 0}), (95937, {'train/accuracy': 0.7051367163658142, 'train/loss': 1.1937915086746216, 'validation/accuracy': 0.6563999652862549, 'validation/loss': 1.434855580329895, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.0843238830566406, 'test/num_examples': 10000, 'score': 44152.55052447319, 'total_duration': 49315.72231268883, 'accumulated_submission_time': 44152.55052447319, 'accumulated_eval_time': 5154.415741682053, 'accumulated_logging_time': 3.7638611793518066, 'global_step': 95937, 'preemption_count': 0}), (96852, {'train/accuracy': 0.7074413895606995, 'train/loss': 1.221797227859497, 'validation/accuracy': 0.6502400040626526, 'validation/loss': 1.4847956895828247, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.126685857772827, 'test/num_examples': 10000, 'score': 44572.624385118484, 'total_duration': 49783.660767793655, 'accumulated_submission_time': 44572.624385118484, 'accumulated_eval_time': 5202.185204267502, 'accumulated_logging_time': 3.811398506164551, 'global_step': 96852, 'preemption_count': 0}), (97764, {'train/accuracy': 0.7198437452316284, 'train/loss': 1.1401715278625488, 'validation/accuracy': 0.6556599736213684, 'validation/loss': 1.4324363470077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.0989506244659424, 'test/num_examples': 10000, 'score': 44992.955062150955, 'total_duration': 50255.37498688698, 'accumulated_submission_time': 44992.955062150955, 'accumulated_eval_time': 5253.482377767563, 'accumulated_logging_time': 3.8508574962615967, 'global_step': 97764, 'preemption_count': 0}), (98676, {'train/accuracy': 0.7141015529632568, 'train/loss': 1.1600228548049927, 'validation/accuracy': 0.6577399969100952, 'validation/loss': 1.420377254486084, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.072648525238037, 'test/num_examples': 10000, 'score': 45413.15657520294, 'total_duration': 50724.73337721825, 'accumulated_submission_time': 45413.15657520294, 'accumulated_eval_time': 5302.544720649719, 'accumulated_logging_time': 3.8982207775115967, 'global_step': 98676, 'preemption_count': 0}), (99592, {'train/accuracy': 0.7101367115974426, 'train/loss': 1.1785920858383179, 'validation/accuracy': 0.6572999954223633, 'validation/loss': 1.4439600706100464, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.08896541595459, 'test/num_examples': 10000, 'score': 45833.22182202339, 'total_duration': 51193.74008560181, 'accumulated_submission_time': 45833.22182202339, 'accumulated_eval_time': 5351.394645690918, 'accumulated_logging_time': 3.9429714679718018, 'global_step': 99592, 'preemption_count': 0}), (100506, {'train/accuracy': 0.7159960865974426, 'train/loss': 1.1765002012252808, 'validation/accuracy': 0.6542199850082397, 'validation/loss': 1.4638800621032715, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1265082359313965, 'test/num_examples': 10000, 'score': 46253.18224287033, 'total_duration': 51661.882147789, 'accumulated_submission_time': 46253.18224287033, 'accumulated_eval_time': 5399.48194694519, 'accumulated_logging_time': 3.9898531436920166, 'global_step': 100506, 'preemption_count': 0}), (101420, {'train/accuracy': 0.7417382597923279, 'train/loss': 1.0204622745513916, 'validation/accuracy': 0.6606000065803528, 'validation/loss': 1.399212121963501, 'validation/num_examples': 50000, 'test/accuracy': 0.5382000207901001, 'test/loss': 2.0457940101623535, 'test/num_examples': 10000, 'score': 46673.48130655289, 'total_duration': 52132.143812179565, 'accumulated_submission_time': 46673.48130655289, 'accumulated_eval_time': 5449.350612878799, 'accumulated_logging_time': 4.036839246749878, 'global_step': 101420, 'preemption_count': 0}), (102335, {'train/accuracy': 0.7127734422683716, 'train/loss': 1.167461633682251, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.4336830377578735, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.095097064971924, 'test/num_examples': 10000, 'score': 47093.46519160271, 'total_duration': 52601.335465192795, 'accumulated_submission_time': 47093.46519160271, 'accumulated_eval_time': 5498.46777844429, 'accumulated_logging_time': 4.080301761627197, 'global_step': 102335, 'preemption_count': 0}), (103250, {'train/accuracy': 0.7226171493530273, 'train/loss': 1.1213455200195312, 'validation/accuracy': 0.6646199822425842, 'validation/loss': 1.3939058780670166, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.0467312335968018, 'test/num_examples': 10000, 'score': 47513.68147063255, 'total_duration': 53071.09167742729, 'accumulated_submission_time': 47513.68147063255, 'accumulated_eval_time': 5547.918229103088, 'accumulated_logging_time': 4.1213812828063965, 'global_step': 103250, 'preemption_count': 0}), (104166, {'train/accuracy': 0.7356249690055847, 'train/loss': 1.0597867965698242, 'validation/accuracy': 0.6630399823188782, 'validation/loss': 1.3899242877960205, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.0377378463745117, 'test/num_examples': 10000, 'score': 47933.8608417511, 'total_duration': 53539.94333600998, 'accumulated_submission_time': 47933.8608417511, 'accumulated_eval_time': 5596.494349956512, 'accumulated_logging_time': 4.169813394546509, 'global_step': 104166, 'preemption_count': 0}), (105081, {'train/accuracy': 0.7218359112739563, 'train/loss': 1.1346908807754517, 'validation/accuracy': 0.6655399799346924, 'validation/loss': 1.3913841247558594, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.0553183555603027, 'test/num_examples': 10000, 'score': 48353.88178706169, 'total_duration': 54009.957320690155, 'accumulated_submission_time': 48353.88178706169, 'accumulated_eval_time': 5646.397901058197, 'accumulated_logging_time': 4.211780786514282, 'global_step': 105081, 'preemption_count': 0}), (105995, {'train/accuracy': 0.7276366949081421, 'train/loss': 1.0901107788085938, 'validation/accuracy': 0.6678199768066406, 'validation/loss': 1.3639365434646606, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0250608921051025, 'test/num_examples': 10000, 'score': 48774.124626636505, 'total_duration': 54482.09878492355, 'accumulated_submission_time': 48774.124626636505, 'accumulated_eval_time': 5698.203535318375, 'accumulated_logging_time': 4.257709503173828, 'global_step': 105995, 'preemption_count': 0}), (106909, {'train/accuracy': 0.7373046875, 'train/loss': 1.0468322038650513, 'validation/accuracy': 0.6692599654197693, 'validation/loss': 1.3726061582565308, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.0267672538757324, 'test/num_examples': 10000, 'score': 49194.25314736366, 'total_duration': 54950.18088555336, 'accumulated_submission_time': 49194.25314736366, 'accumulated_eval_time': 5746.058041095734, 'accumulated_logging_time': 4.303881406784058, 'global_step': 106909, 'preemption_count': 0}), (107821, {'train/accuracy': 0.7249218821525574, 'train/loss': 1.1311174631118774, 'validation/accuracy': 0.6684199571609497, 'validation/loss': 1.3853036165237427, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.035310745239258, 'test/num_examples': 10000, 'score': 49614.54778671265, 'total_duration': 55419.25144505501, 'accumulated_submission_time': 49614.54778671265, 'accumulated_eval_time': 5794.7436735630035, 'accumulated_logging_time': 4.3474390506744385, 'global_step': 107821, 'preemption_count': 0}), (108735, {'train/accuracy': 0.7259179353713989, 'train/loss': 1.0984207391738892, 'validation/accuracy': 0.6665199995040894, 'validation/loss': 1.3816251754760742, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.0237200260162354, 'test/num_examples': 10000, 'score': 50034.81283926964, 'total_duration': 55889.601029634476, 'accumulated_submission_time': 50034.81283926964, 'accumulated_eval_time': 5844.7369022369385, 'accumulated_logging_time': 4.391482353210449, 'global_step': 108735, 'preemption_count': 0}), (109650, {'train/accuracy': 0.7411913871765137, 'train/loss': 1.045373558998108, 'validation/accuracy': 0.6743599772453308, 'validation/loss': 1.3515647649765015, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 1.9914335012435913, 'test/num_examples': 10000, 'score': 50455.170258522034, 'total_duration': 56358.836493730545, 'accumulated_submission_time': 50455.170258522034, 'accumulated_eval_time': 5893.524285316467, 'accumulated_logging_time': 4.434794902801514, 'global_step': 109650, 'preemption_count': 0}), (110565, {'train/accuracy': 0.7310742139816284, 'train/loss': 1.0999258756637573, 'validation/accuracy': 0.6731199622154236, 'validation/loss': 1.3631587028503418, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.017176866531372, 'test/num_examples': 10000, 'score': 50875.440262556076, 'total_duration': 56829.10870409012, 'accumulated_submission_time': 50875.440262556076, 'accumulated_eval_time': 5943.436125278473, 'accumulated_logging_time': 4.477481842041016, 'global_step': 110565, 'preemption_count': 0}), (111479, {'train/accuracy': 0.7366992235183716, 'train/loss': 1.0674903392791748, 'validation/accuracy': 0.6771799921989441, 'validation/loss': 1.3465999364852905, 'validation/num_examples': 50000, 'test/accuracy': 0.5570000410079956, 'test/loss': 1.9866653680801392, 'test/num_examples': 10000, 'score': 51295.71018028259, 'total_duration': 57298.81796312332, 'accumulated_submission_time': 51295.71018028259, 'accumulated_eval_time': 5992.779366493225, 'accumulated_logging_time': 4.526298999786377, 'global_step': 111479, 'preemption_count': 0}), (112394, {'train/accuracy': 0.7471093535423279, 'train/loss': 1.0021445751190186, 'validation/accuracy': 0.6795799732208252, 'validation/loss': 1.3151804208755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 1.9630584716796875, 'test/num_examples': 10000, 'score': 51715.62279224396, 'total_duration': 57766.71099972725, 'accumulated_submission_time': 51715.62279224396, 'accumulated_eval_time': 6040.666308164597, 'accumulated_logging_time': 4.571993350982666, 'global_step': 112394, 'preemption_count': 0}), (113310, {'train/accuracy': 0.7491015195846558, 'train/loss': 1.0073623657226562, 'validation/accuracy': 0.6755200028419495, 'validation/loss': 1.3467522859573364, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.0030879974365234, 'test/num_examples': 10000, 'score': 52135.815131664276, 'total_duration': 58234.475531339645, 'accumulated_submission_time': 52135.815131664276, 'accumulated_eval_time': 6088.146703958511, 'accumulated_logging_time': 4.616669178009033, 'global_step': 113310, 'preemption_count': 0}), (114226, {'train/accuracy': 0.7378710508346558, 'train/loss': 1.050584316253662, 'validation/accuracy': 0.6758399605751038, 'validation/loss': 1.3396703004837036, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.981272578239441, 'test/num_examples': 10000, 'score': 52556.26023578644, 'total_duration': 58705.31205654144, 'accumulated_submission_time': 52556.26023578644, 'accumulated_eval_time': 6138.448607206345, 'accumulated_logging_time': 4.658712863922119, 'global_step': 114226, 'preemption_count': 0}), (115140, {'train/accuracy': 0.7449804544448853, 'train/loss': 1.0316308736801147, 'validation/accuracy': 0.6810399889945984, 'validation/loss': 1.3292573690414429, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 1.9811760187149048, 'test/num_examples': 10000, 'score': 52976.29587888718, 'total_duration': 59176.41819024086, 'accumulated_submission_time': 52976.29587888718, 'accumulated_eval_time': 6189.425545454025, 'accumulated_logging_time': 4.7043726444244385, 'global_step': 115140, 'preemption_count': 0}), (116054, {'train/accuracy': 0.7593359351158142, 'train/loss': 0.968052327632904, 'validation/accuracy': 0.6818599700927734, 'validation/loss': 1.3153448104858398, 'validation/num_examples': 50000, 'test/accuracy': 0.5635000467300415, 'test/loss': 1.9470332860946655, 'test/num_examples': 10000, 'score': 53396.448315382004, 'total_duration': 59648.02452993393, 'accumulated_submission_time': 53396.448315382004, 'accumulated_eval_time': 6240.787206888199, 'accumulated_logging_time': 4.749409198760986, 'global_step': 116054, 'preemption_count': 0}), (116970, {'train/accuracy': 0.7439843416213989, 'train/loss': 1.0527466535568237, 'validation/accuracy': 0.6826399564743042, 'validation/loss': 1.3293102979660034, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 1.9881199598312378, 'test/num_examples': 10000, 'score': 53816.71623015404, 'total_duration': 60119.25689291954, 'accumulated_submission_time': 53816.71623015404, 'accumulated_eval_time': 6291.6556515693665, 'accumulated_logging_time': 4.797953367233276, 'global_step': 116970, 'preemption_count': 0}), (117884, {'train/accuracy': 0.748828113079071, 'train/loss': 1.0334590673446655, 'validation/accuracy': 0.6802399754524231, 'validation/loss': 1.337364912033081, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9683139324188232, 'test/num_examples': 10000, 'score': 54236.68958616257, 'total_duration': 60588.64545035362, 'accumulated_submission_time': 54236.68958616257, 'accumulated_eval_time': 6340.977566003799, 'accumulated_logging_time': 4.843794584274292, 'global_step': 117884, 'preemption_count': 0}), (118799, {'train/accuracy': 0.7620312571525574, 'train/loss': 0.9354296922683716, 'validation/accuracy': 0.6877599954605103, 'validation/loss': 1.279233694076538, 'validation/num_examples': 50000, 'test/accuracy': 0.5672000050544739, 'test/loss': 1.9073601961135864, 'test/num_examples': 10000, 'score': 54656.7682967186, 'total_duration': 61058.40561413765, 'accumulated_submission_time': 54656.7682967186, 'accumulated_eval_time': 6390.563494682312, 'accumulated_logging_time': 4.891946077346802, 'global_step': 118799, 'preemption_count': 0}), (119714, {'train/accuracy': 0.7458398342132568, 'train/loss': 1.0181658267974854, 'validation/accuracy': 0.6830599904060364, 'validation/loss': 1.3007621765136719, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9275920391082764, 'test/num_examples': 10000, 'score': 55076.75383043289, 'total_duration': 61526.98484683037, 'accumulated_submission_time': 55076.75383043289, 'accumulated_eval_time': 6439.0599138736725, 'accumulated_logging_time': 4.941753149032593, 'global_step': 119714, 'preemption_count': 0}), (120629, {'train/accuracy': 0.7542773485183716, 'train/loss': 0.9806972146034241, 'validation/accuracy': 0.6851599812507629, 'validation/loss': 1.289842963218689, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 1.920379638671875, 'test/num_examples': 10000, 'score': 55496.691744565964, 'total_duration': 61996.933457136154, 'accumulated_submission_time': 55496.691744565964, 'accumulated_eval_time': 6488.97558426857, 'accumulated_logging_time': 4.9889538288116455, 'global_step': 120629, 'preemption_count': 0}), (121539, {'train/accuracy': 0.7638280987739563, 'train/loss': 0.9494880437850952, 'validation/accuracy': 0.6881799697875977, 'validation/loss': 1.2904366254806519, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.9232209920883179, 'test/num_examples': 10000, 'score': 55916.75475072861, 'total_duration': 62466.457062006, 'accumulated_submission_time': 55916.75475072861, 'accumulated_eval_time': 6538.339520454407, 'accumulated_logging_time': 5.038301229476929, 'global_step': 121539, 'preemption_count': 0}), (122454, {'train/accuracy': 0.753613293170929, 'train/loss': 1.002234935760498, 'validation/accuracy': 0.6880599856376648, 'validation/loss': 1.3044852018356323, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 1.932502269744873, 'test/num_examples': 10000, 'score': 56337.04533982277, 'total_duration': 62936.76914644241, 'accumulated_submission_time': 56337.04533982277, 'accumulated_eval_time': 6588.268809556961, 'accumulated_logging_time': 5.083476781845093, 'global_step': 122454, 'preemption_count': 0}), (123369, {'train/accuracy': 0.7582421898841858, 'train/loss': 0.9696675539016724, 'validation/accuracy': 0.692579984664917, 'validation/loss': 1.262717843055725, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.9009531736373901, 'test/num_examples': 10000, 'score': 56757.24954080582, 'total_duration': 63404.89881134033, 'accumulated_submission_time': 56757.24954080582, 'accumulated_eval_time': 6636.103061914444, 'accumulated_logging_time': 5.127235651016235, 'global_step': 123369, 'preemption_count': 0}), (124285, {'train/accuracy': 0.7618163824081421, 'train/loss': 0.9538049101829529, 'validation/accuracy': 0.6948999762535095, 'validation/loss': 1.2704392671585083, 'validation/num_examples': 50000, 'test/accuracy': 0.5712000131607056, 'test/loss': 1.8988127708435059, 'test/num_examples': 10000, 'score': 57177.33669304848, 'total_duration': 63871.522136449814, 'accumulated_submission_time': 57177.33669304848, 'accumulated_eval_time': 6682.54062128067, 'accumulated_logging_time': 5.174166440963745, 'global_step': 124285, 'preemption_count': 0}), (125198, {'train/accuracy': 0.7659569978713989, 'train/loss': 0.9690631628036499, 'validation/accuracy': 0.6935399770736694, 'validation/loss': 1.2955999374389648, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.9468671083450317, 'test/num_examples': 10000, 'score': 57597.292941093445, 'total_duration': 64342.43574547768, 'accumulated_submission_time': 57597.292941093445, 'accumulated_eval_time': 6733.407159566879, 'accumulated_logging_time': 5.218145370483398, 'global_step': 125198, 'preemption_count': 0}), (126112, {'train/accuracy': 0.7572070360183716, 'train/loss': 0.9875859618186951, 'validation/accuracy': 0.6926199793815613, 'validation/loss': 1.279220461845398, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9271174669265747, 'test/num_examples': 10000, 'score': 58017.22869372368, 'total_duration': 64812.9195394516, 'accumulated_submission_time': 58017.22869372368, 'accumulated_eval_time': 6783.851147174835, 'accumulated_logging_time': 5.274590492248535, 'global_step': 126112, 'preemption_count': 0}), (127025, {'train/accuracy': 0.7670117020606995, 'train/loss': 0.9423275589942932, 'validation/accuracy': 0.6972000002861023, 'validation/loss': 1.2566183805465698, 'validation/num_examples': 50000, 'test/accuracy': 0.5734000205993652, 'test/loss': 1.893884301185608, 'test/num_examples': 10000, 'score': 58437.148782491684, 'total_duration': 65282.39766430855, 'accumulated_submission_time': 58437.148782491684, 'accumulated_eval_time': 6833.316943883896, 'accumulated_logging_time': 5.319833278656006, 'global_step': 127025, 'preemption_count': 0}), (127939, {'train/accuracy': 0.7809179425239563, 'train/loss': 0.883868932723999, 'validation/accuracy': 0.6997999548912048, 'validation/loss': 1.2388687133789062, 'validation/num_examples': 50000, 'test/accuracy': 0.5772000551223755, 'test/loss': 1.8826701641082764, 'test/num_examples': 10000, 'score': 58857.462636232376, 'total_duration': 65752.2223212719, 'accumulated_submission_time': 58857.462636232376, 'accumulated_eval_time': 6882.7317888736725, 'accumulated_logging_time': 5.368030786514282, 'global_step': 127939, 'preemption_count': 0}), (128854, {'train/accuracy': 0.7710937261581421, 'train/loss': 0.9130802154541016, 'validation/accuracy': 0.7034199833869934, 'validation/loss': 1.210724949836731, 'validation/num_examples': 50000, 'test/accuracy': 0.582800030708313, 'test/loss': 1.8451893329620361, 'test/num_examples': 10000, 'score': 59277.466651916504, 'total_duration': 66218.96444773674, 'accumulated_submission_time': 59277.466651916504, 'accumulated_eval_time': 6929.378044605255, 'accumulated_logging_time': 5.412663459777832, 'global_step': 128854, 'preemption_count': 0}), (129767, {'train/accuracy': 0.7718359231948853, 'train/loss': 0.9090878367424011, 'validation/accuracy': 0.6992599964141846, 'validation/loss': 1.23419189453125, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.8769426345825195, 'test/num_examples': 10000, 'score': 59697.61399292946, 'total_duration': 66688.71971225739, 'accumulated_submission_time': 59697.61399292946, 'accumulated_eval_time': 6978.882484436035, 'accumulated_logging_time': 5.467691898345947, 'global_step': 129767, 'preemption_count': 0}), (130683, {'train/accuracy': 0.7808593511581421, 'train/loss': 0.8642115592956543, 'validation/accuracy': 0.7039200067520142, 'validation/loss': 1.2181345224380493, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.8567651510238647, 'test/num_examples': 10000, 'score': 60117.49275612831, 'total_duration': 67161.56207251549, 'accumulated_submission_time': 60117.49275612831, 'accumulated_eval_time': 7031.3348553180695, 'accumulated_logging_time': 5.931267976760864, 'global_step': 130683, 'preemption_count': 0}), (131596, {'train/accuracy': 0.7708203196525574, 'train/loss': 0.8994258642196655, 'validation/accuracy': 0.7038799524307251, 'validation/loss': 1.2020835876464844, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8415201902389526, 'test/num_examples': 10000, 'score': 60537.74565386772, 'total_duration': 67632.65162563324, 'accumulated_submission_time': 60537.74565386772, 'accumulated_eval_time': 7082.0744042396545, 'accumulated_logging_time': 5.981815576553345, 'global_step': 131596, 'preemption_count': 0}), (132511, {'train/accuracy': 0.7748632431030273, 'train/loss': 0.8906111717224121, 'validation/accuracy': 0.7036799788475037, 'validation/loss': 1.2126256227493286, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 1.8380862474441528, 'test/num_examples': 10000, 'score': 60957.78933095932, 'total_duration': 68103.68083000183, 'accumulated_submission_time': 60957.78933095932, 'accumulated_eval_time': 7132.960758924484, 'accumulated_logging_time': 6.033755540847778, 'global_step': 132511, 'preemption_count': 0}), (133426, {'train/accuracy': 0.7884570360183716, 'train/loss': 0.8362293839454651, 'validation/accuracy': 0.7095800042152405, 'validation/loss': 1.1812160015106201, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.816677451133728, 'test/num_examples': 10000, 'score': 61377.8827316761, 'total_duration': 68570.23081469536, 'accumulated_submission_time': 61377.8827316761, 'accumulated_eval_time': 7179.321115255356, 'accumulated_logging_time': 6.082520008087158, 'global_step': 133426, 'preemption_count': 0}), (134343, {'train/accuracy': 0.7745312452316284, 'train/loss': 0.8990936279296875, 'validation/accuracy': 0.7057799696922302, 'validation/loss': 1.2014926671981812, 'validation/num_examples': 50000, 'test/accuracy': 0.5829000473022461, 'test/loss': 1.8295793533325195, 'test/num_examples': 10000, 'score': 61798.15866804123, 'total_duration': 69040.90519189835, 'accumulated_submission_time': 61798.15866804123, 'accumulated_eval_time': 7229.612575292587, 'accumulated_logging_time': 6.1410746574401855, 'global_step': 134343, 'preemption_count': 0}), (135256, {'train/accuracy': 0.7787109017372131, 'train/loss': 0.886769711971283, 'validation/accuracy': 0.7080000042915344, 'validation/loss': 1.2056972980499268, 'validation/num_examples': 50000, 'test/accuracy': 0.5863000154495239, 'test/loss': 1.8409180641174316, 'test/num_examples': 10000, 'score': 62218.16717839241, 'total_duration': 69509.96885418892, 'accumulated_submission_time': 62218.16717839241, 'accumulated_eval_time': 7278.565196990967, 'accumulated_logging_time': 6.1958794593811035, 'global_step': 135256, 'preemption_count': 0}), (136170, {'train/accuracy': 0.7862108945846558, 'train/loss': 0.8529349565505981, 'validation/accuracy': 0.7099399566650391, 'validation/loss': 1.196141004562378, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8285341262817383, 'test/num_examples': 10000, 'score': 62638.280061244965, 'total_duration': 69978.45606637001, 'accumulated_submission_time': 62638.280061244965, 'accumulated_eval_time': 7326.843999385834, 'accumulated_logging_time': 6.2438578605651855, 'global_step': 136170, 'preemption_count': 0}), (137084, {'train/accuracy': 0.7843554615974426, 'train/loss': 0.8620164394378662, 'validation/accuracy': 0.7106199860572815, 'validation/loss': 1.1773674488067627, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8074219226837158, 'test/num_examples': 10000, 'score': 63058.562294483185, 'total_duration': 70450.00231456757, 'accumulated_submission_time': 63058.562294483185, 'accumulated_eval_time': 7378.012609243393, 'accumulated_logging_time': 6.291548013687134, 'global_step': 137084, 'preemption_count': 0}), (137998, {'train/accuracy': 0.7840625047683716, 'train/loss': 0.8761175274848938, 'validation/accuracy': 0.7105000019073486, 'validation/loss': 1.1932687759399414, 'validation/num_examples': 50000, 'test/accuracy': 0.5921000242233276, 'test/loss': 1.8124998807907104, 'test/num_examples': 10000, 'score': 63478.914189100266, 'total_duration': 70919.76289439201, 'accumulated_submission_time': 63478.914189100266, 'accumulated_eval_time': 7427.31960606575, 'accumulated_logging_time': 6.34608793258667, 'global_step': 137998, 'preemption_count': 0}), (138913, {'train/accuracy': 0.7923241853713989, 'train/loss': 0.8205782771110535, 'validation/accuracy': 0.714199960231781, 'validation/loss': 1.164374589920044, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.7992960214614868, 'test/num_examples': 10000, 'score': 63899.1681098938, 'total_duration': 71390.19374513626, 'accumulated_submission_time': 63899.1681098938, 'accumulated_eval_time': 7477.3962070941925, 'accumulated_logging_time': 6.399160146713257, 'global_step': 138913, 'preemption_count': 0}), (139830, {'train/accuracy': 0.8011132478713989, 'train/loss': 0.8038817644119263, 'validation/accuracy': 0.7127000093460083, 'validation/loss': 1.1908022165298462, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.8122637271881104, 'test/num_examples': 10000, 'score': 64319.3515021801, 'total_duration': 71858.77561020851, 'accumulated_submission_time': 64319.3515021801, 'accumulated_eval_time': 7525.692511558533, 'accumulated_logging_time': 6.453774929046631, 'global_step': 139830, 'preemption_count': 0}), (140740, {'train/accuracy': 0.7881835699081421, 'train/loss': 0.8385329246520996, 'validation/accuracy': 0.7159799933433533, 'validation/loss': 1.167323350906372, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7915613651275635, 'test/num_examples': 10000, 'score': 64739.52413749695, 'total_duration': 72328.59392142296, 'accumulated_submission_time': 64739.52413749695, 'accumulated_eval_time': 7575.239279747009, 'accumulated_logging_time': 6.506052732467651, 'global_step': 140740, 'preemption_count': 0}), (141654, {'train/accuracy': 0.7936913967132568, 'train/loss': 0.8237582445144653, 'validation/accuracy': 0.7181000113487244, 'validation/loss': 1.1573004722595215, 'validation/num_examples': 50000, 'test/accuracy': 0.5964000225067139, 'test/loss': 1.7926957607269287, 'test/num_examples': 10000, 'score': 65159.690633773804, 'total_duration': 72796.25745105743, 'accumulated_submission_time': 65159.690633773804, 'accumulated_eval_time': 7622.6402060985565, 'accumulated_logging_time': 6.555207014083862, 'global_step': 141654, 'preemption_count': 0}), (142571, {'train/accuracy': 0.8095507621765137, 'train/loss': 0.75356125831604, 'validation/accuracy': 0.7208200097084045, 'validation/loss': 1.128256916999817, 'validation/num_examples': 50000, 'test/accuracy': 0.6039000153541565, 'test/loss': 1.7557858228683472, 'test/num_examples': 10000, 'score': 65579.90997314453, 'total_duration': 73267.46773982048, 'accumulated_submission_time': 65579.90997314453, 'accumulated_eval_time': 7673.526467323303, 'accumulated_logging_time': 6.612675666809082, 'global_step': 142571, 'preemption_count': 0}), (143487, {'train/accuracy': 0.7930663824081421, 'train/loss': 0.80852872133255, 'validation/accuracy': 0.719539999961853, 'validation/loss': 1.132944107055664, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.7516762018203735, 'test/num_examples': 10000, 'score': 66000.25936365128, 'total_duration': 73737.62437415123, 'accumulated_submission_time': 66000.25936365128, 'accumulated_eval_time': 7723.234818935394, 'accumulated_logging_time': 6.664331912994385, 'global_step': 143487, 'preemption_count': 0}), (144400, {'train/accuracy': 0.7961718440055847, 'train/loss': 0.7973366379737854, 'validation/accuracy': 0.7209199666976929, 'validation/loss': 1.1386834383010864, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.758381962776184, 'test/num_examples': 10000, 'score': 66420.45049548149, 'total_duration': 74208.18918466568, 'accumulated_submission_time': 66420.45049548149, 'accumulated_eval_time': 7773.509160041809, 'accumulated_logging_time': 6.716897964477539, 'global_step': 144400, 'preemption_count': 0}), (145314, {'train/accuracy': 0.80712890625, 'train/loss': 0.7481192350387573, 'validation/accuracy': 0.7227999567985535, 'validation/loss': 1.1151829957962036, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.7355223894119263, 'test/num_examples': 10000, 'score': 66840.47810816765, 'total_duration': 74677.93926692009, 'accumulated_submission_time': 66840.47810816765, 'accumulated_eval_time': 7823.125457286835, 'accumulated_logging_time': 6.775168180465698, 'global_step': 145314, 'preemption_count': 0}), (146229, {'train/accuracy': 0.8002148270606995, 'train/loss': 0.7895472049713135, 'validation/accuracy': 0.7212600111961365, 'validation/loss': 1.1343475580215454, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.7654008865356445, 'test/num_examples': 10000, 'score': 67260.58725810051, 'total_duration': 75146.9090578556, 'accumulated_submission_time': 67260.58725810051, 'accumulated_eval_time': 7871.883242845535, 'accumulated_logging_time': 6.829352855682373, 'global_step': 146229, 'preemption_count': 0}), (147140, {'train/accuracy': 0.7982617020606995, 'train/loss': 0.7943770289421082, 'validation/accuracy': 0.7218599915504456, 'validation/loss': 1.133278250694275, 'validation/num_examples': 50000, 'test/accuracy': 0.6002000570297241, 'test/loss': 1.7653189897537231, 'test/num_examples': 10000, 'score': 67680.65770792961, 'total_duration': 75616.78656816483, 'accumulated_submission_time': 67680.65770792961, 'accumulated_eval_time': 7921.5880670547485, 'accumulated_logging_time': 6.884565353393555, 'global_step': 147140, 'preemption_count': 0}), (148051, {'train/accuracy': 0.8085156083106995, 'train/loss': 0.7679116725921631, 'validation/accuracy': 0.7251200079917908, 'validation/loss': 1.131921410560608, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.749596118927002, 'test/num_examples': 10000, 'score': 68100.57943248749, 'total_duration': 76085.59352064133, 'accumulated_submission_time': 68100.57943248749, 'accumulated_eval_time': 7970.376499891281, 'accumulated_logging_time': 6.934350490570068, 'global_step': 148051, 'preemption_count': 0}), (148964, {'train/accuracy': 0.8046875, 'train/loss': 0.7722216844558716, 'validation/accuracy': 0.7258399724960327, 'validation/loss': 1.1195939779281616, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.739829421043396, 'test/num_examples': 10000, 'score': 68520.65689635277, 'total_duration': 76556.07243704796, 'accumulated_submission_time': 68520.65689635277, 'accumulated_eval_time': 8020.678261995316, 'accumulated_logging_time': 6.98644495010376, 'global_step': 148964, 'preemption_count': 0}), (149875, {'train/accuracy': 0.8066015243530273, 'train/loss': 0.7632541656494141, 'validation/accuracy': 0.7278800010681152, 'validation/loss': 1.111369013786316, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.7303481101989746, 'test/num_examples': 10000, 'score': 68940.75487065315, 'total_duration': 77025.45209693909, 'accumulated_submission_time': 68940.75487065315, 'accumulated_eval_time': 8069.858564853668, 'accumulated_logging_time': 7.0408594608306885, 'global_step': 149875, 'preemption_count': 0}), (150790, {'train/accuracy': 0.8119726181030273, 'train/loss': 0.7188385725021362, 'validation/accuracy': 0.7307599782943726, 'validation/loss': 1.0796045064926147, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.7081046104431152, 'test/num_examples': 10000, 'score': 69360.97912240028, 'total_duration': 77494.48408341408, 'accumulated_submission_time': 69360.97912240028, 'accumulated_eval_time': 8118.56437587738, 'accumulated_logging_time': 7.0947325229644775, 'global_step': 150790, 'preemption_count': 0}), (151704, {'train/accuracy': 0.8213671445846558, 'train/loss': 0.7049360275268555, 'validation/accuracy': 0.7328000068664551, 'validation/loss': 1.0796293020248413, 'validation/num_examples': 50000, 'test/accuracy': 0.6109000444412231, 'test/loss': 1.7023441791534424, 'test/num_examples': 10000, 'score': 69781.3741095066, 'total_duration': 77963.0119497776, 'accumulated_submission_time': 69781.3741095066, 'accumulated_eval_time': 8166.598405122757, 'accumulated_logging_time': 7.145893335342407, 'global_step': 151704, 'preemption_count': 0}), (152618, {'train/accuracy': 0.8115234375, 'train/loss': 0.7339527010917664, 'validation/accuracy': 0.7320799827575684, 'validation/loss': 1.0806304216384888, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.709767460823059, 'test/num_examples': 10000, 'score': 70201.74805235863, 'total_duration': 78432.79470396042, 'accumulated_submission_time': 70201.74805235863, 'accumulated_eval_time': 8215.91041135788, 'accumulated_logging_time': 7.195410490036011, 'global_step': 152618, 'preemption_count': 0}), (153535, {'train/accuracy': 0.815722644329071, 'train/loss': 0.7261427640914917, 'validation/accuracy': 0.7325199842453003, 'validation/loss': 1.0846915245056152, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.706923484802246, 'test/num_examples': 10000, 'score': 70622.05784344673, 'total_duration': 78903.90107440948, 'accumulated_submission_time': 70622.05784344673, 'accumulated_eval_time': 8266.604093313217, 'accumulated_logging_time': 7.249720573425293, 'global_step': 153535, 'preemption_count': 0}), (154448, {'train/accuracy': 0.8185155987739563, 'train/loss': 0.7208164930343628, 'validation/accuracy': 0.730239987373352, 'validation/loss': 1.097428798675537, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.7289506196975708, 'test/num_examples': 10000, 'score': 71042.29072260857, 'total_duration': 79374.33994674683, 'accumulated_submission_time': 71042.29072260857, 'accumulated_eval_time': 8316.71166753769, 'accumulated_logging_time': 7.300796031951904, 'global_step': 154448, 'preemption_count': 0}), (155361, {'train/accuracy': 0.8110156059265137, 'train/loss': 0.7516688108444214, 'validation/accuracy': 0.731440007686615, 'validation/loss': 1.091639757156372, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.7229865789413452, 'test/num_examples': 10000, 'score': 71462.36037421227, 'total_duration': 79842.9433221817, 'accumulated_submission_time': 71462.36037421227, 'accumulated_eval_time': 8365.147471904755, 'accumulated_logging_time': 7.35153603553772, 'global_step': 155361, 'preemption_count': 0}), (156274, {'train/accuracy': 0.8199804425239563, 'train/loss': 0.7003152370452881, 'validation/accuracy': 0.7363599538803101, 'validation/loss': 1.0666792392730713, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7028956413269043, 'test/num_examples': 10000, 'score': 71882.66050243378, 'total_duration': 80314.7297205925, 'accumulated_submission_time': 71882.66050243378, 'accumulated_eval_time': 8416.536350011826, 'accumulated_logging_time': 7.401159286499023, 'global_step': 156274, 'preemption_count': 0}), (157186, {'train/accuracy': 0.82289057970047, 'train/loss': 0.6994317173957825, 'validation/accuracy': 0.7371799945831299, 'validation/loss': 1.0725938081741333, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.7018332481384277, 'test/num_examples': 10000, 'score': 72303.01607465744, 'total_duration': 80785.31089067459, 'accumulated_submission_time': 72303.01607465744, 'accumulated_eval_time': 8466.652791976929, 'accumulated_logging_time': 7.462125539779663, 'global_step': 157186, 'preemption_count': 0}), (158097, {'train/accuracy': 0.821582019329071, 'train/loss': 0.718077540397644, 'validation/accuracy': 0.738599956035614, 'validation/loss': 1.0715638399124146, 'validation/num_examples': 50000, 'test/accuracy': 0.6152000427246094, 'test/loss': 1.7035168409347534, 'test/num_examples': 10000, 'score': 72723.06442546844, 'total_duration': 81254.37323331833, 'accumulated_submission_time': 72723.06442546844, 'accumulated_eval_time': 8515.557217359543, 'accumulated_logging_time': 7.524913787841797, 'global_step': 158097, 'preemption_count': 0}), (159010, {'train/accuracy': 0.8240429759025574, 'train/loss': 0.699968159198761, 'validation/accuracy': 0.7378199696540833, 'validation/loss': 1.0641032457351685, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.6965481042861938, 'test/num_examples': 10000, 'score': 73143.2449285984, 'total_duration': 81725.02041864395, 'accumulated_submission_time': 73143.2449285984, 'accumulated_eval_time': 8565.915897130966, 'accumulated_logging_time': 7.585332155227661, 'global_step': 159010, 'preemption_count': 0}), (159924, {'train/accuracy': 0.822558581829071, 'train/loss': 0.6985819935798645, 'validation/accuracy': 0.7378399968147278, 'validation/loss': 1.068437933921814, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.6972639560699463, 'test/num_examples': 10000, 'score': 73563.15134334564, 'total_duration': 82192.29157710075, 'accumulated_submission_time': 73563.15134334564, 'accumulated_eval_time': 8613.183167696, 'accumulated_logging_time': 7.635556221008301, 'global_step': 159924, 'preemption_count': 0}), (160838, {'train/accuracy': 0.8229296803474426, 'train/loss': 0.6872890591621399, 'validation/accuracy': 0.7397199869155884, 'validation/loss': 1.054733395576477, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.6841599941253662, 'test/num_examples': 10000, 'score': 73983.04493808746, 'total_duration': 82659.67759394646, 'accumulated_submission_time': 73983.04493808746, 'accumulated_eval_time': 8660.566656589508, 'accumulated_logging_time': 7.696813583374023, 'global_step': 160838, 'preemption_count': 0}), (161752, {'train/accuracy': 0.8261327743530273, 'train/loss': 0.6801848411560059, 'validation/accuracy': 0.7404199838638306, 'validation/loss': 1.0489463806152344, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.6832082271575928, 'test/num_examples': 10000, 'score': 74403.15802598, 'total_duration': 83129.38182520866, 'accumulated_submission_time': 74403.15802598, 'accumulated_eval_time': 8710.053017377853, 'accumulated_logging_time': 7.754112958908081, 'global_step': 161752, 'preemption_count': 0}), (162668, {'train/accuracy': 0.8266015648841858, 'train/loss': 0.6813116073608398, 'validation/accuracy': 0.7404999732971191, 'validation/loss': 1.0605944395065308, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.6932504177093506, 'test/num_examples': 10000, 'score': 74823.39261484146, 'total_duration': 83598.31682682037, 'accumulated_submission_time': 74823.39261484146, 'accumulated_eval_time': 8758.655586957932, 'accumulated_logging_time': 7.804761648178101, 'global_step': 162668, 'preemption_count': 0}), (163582, {'train/accuracy': 0.82958984375, 'train/loss': 0.6572279334068298, 'validation/accuracy': 0.7425599694252014, 'validation/loss': 1.0434695482254028, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.6695793867111206, 'test/num_examples': 10000, 'score': 75243.69893074036, 'total_duration': 84069.89703917503, 'accumulated_submission_time': 75243.69893074036, 'accumulated_eval_time': 8809.826647281647, 'accumulated_logging_time': 7.860961198806763, 'global_step': 163582, 'preemption_count': 0}), (164496, {'train/accuracy': 0.8274804353713989, 'train/loss': 0.6675172448158264, 'validation/accuracy': 0.7411800026893616, 'validation/loss': 1.0367389917373657, 'validation/num_examples': 50000, 'test/accuracy': 0.6194000244140625, 'test/loss': 1.6700612306594849, 'test/num_examples': 10000, 'score': 75663.79537057877, 'total_duration': 84539.8385951519, 'accumulated_submission_time': 75663.79537057877, 'accumulated_eval_time': 8859.568863630295, 'accumulated_logging_time': 7.9168360233306885, 'global_step': 164496, 'preemption_count': 0}), (165411, {'train/accuracy': 0.8327734470367432, 'train/loss': 0.6660227179527283, 'validation/accuracy': 0.741599977016449, 'validation/loss': 1.0521215200424194, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.6760276556015015, 'test/num_examples': 10000, 'score': 76083.82977080345, 'total_duration': 85010.10476636887, 'accumulated_submission_time': 76083.82977080345, 'accumulated_eval_time': 8909.697760820389, 'accumulated_logging_time': 7.97200608253479, 'global_step': 165411, 'preemption_count': 0}), (166329, {'train/accuracy': 0.8364452719688416, 'train/loss': 0.6335233449935913, 'validation/accuracy': 0.7430599927902222, 'validation/loss': 1.0303434133529663, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.660801887512207, 'test/num_examples': 10000, 'score': 76504.16225075722, 'total_duration': 85479.89331364632, 'accumulated_submission_time': 76504.16225075722, 'accumulated_eval_time': 8959.051559209824, 'accumulated_logging_time': 8.026431798934937, 'global_step': 166329, 'preemption_count': 0}), (167241, {'train/accuracy': 0.8333203196525574, 'train/loss': 0.6550984978675842, 'validation/accuracy': 0.7444999814033508, 'validation/loss': 1.0337879657745361, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.6686664819717407, 'test/num_examples': 10000, 'score': 76924.07387590408, 'total_duration': 85950.96992588043, 'accumulated_submission_time': 76924.07387590408, 'accumulated_eval_time': 9010.117893695831, 'accumulated_logging_time': 8.077922344207764, 'global_step': 167241, 'preemption_count': 0}), (168155, {'train/accuracy': 0.8345312476158142, 'train/loss': 0.6509010195732117, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.033661127090454, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.6653497219085693, 'test/num_examples': 10000, 'score': 77344.41281080246, 'total_duration': 86418.69306540489, 'accumulated_submission_time': 77344.41281080246, 'accumulated_eval_time': 9057.402058124542, 'accumulated_logging_time': 8.131011486053467, 'global_step': 168155, 'preemption_count': 0})], 'global_step': 168542}
I0204 11:42:52.992998 140184451094336 submission_runner.py:586] Timing: 77520.18580436707
I0204 11:42:52.993093 140184451094336 submission_runner.py:588] Total number of evals: 185
I0204 11:42:52.993137 140184451094336 submission_runner.py:589] ====================
I0204 11:42:52.995616 140184451094336 submission_runner.py:673] Final imagenet_vit score: 77520.18580436707
