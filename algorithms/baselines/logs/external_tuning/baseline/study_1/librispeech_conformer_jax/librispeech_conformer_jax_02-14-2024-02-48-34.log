python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_1 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=2554204701 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_02-14-2024-02-48-34.log
I0214 02:48:55.473892 140399019657024 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax.
I0214 02:48:56.510845 140399019657024 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0214 02:48:56.511605 140399019657024 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0214 02:48:56.511733 140399019657024 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0214 02:48:56.512847 140399019657024 submission_runner.py:542] Using RNG seed 2554204701
I0214 02:48:57.672131 140399019657024 submission_runner.py:551] --- Tuning run 1/5 ---
I0214 02:48:57.672340 140399019657024 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1.
I0214 02:48:57.672754 140399019657024 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1/hparams.json.
I0214 02:48:57.859725 140399019657024 submission_runner.py:206] Initializing dataset.
I0214 02:48:57.859950 140399019657024 submission_runner.py:213] Initializing model.
I0214 02:49:02.881759 140399019657024 submission_runner.py:255] Initializing optimizer.
I0214 02:49:04.131569 140399019657024 submission_runner.py:262] Initializing metrics bundle.
I0214 02:49:04.131768 140399019657024 submission_runner.py:280] Initializing checkpoint and logger.
I0214 02:49:04.132947 140399019657024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0214 02:49:04.133087 140399019657024 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0214 02:49:04.133292 140399019657024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 02:49:04.133357 140399019657024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 02:49:04.445557 140399019657024 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 02:49:04.727210 140399019657024 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1/flags_0.json.
I0214 02:49:04.742677 140399019657024 submission_runner.py:314] Starting training loop.
I0214 02:49:05.045512 140399019657024 input_pipeline.py:20] Loading split = train-clean-100
I0214 02:49:05.083951 140399019657024 input_pipeline.py:20] Loading split = train-clean-360
I0214 02:49:05.506960 140399019657024 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 02:50:06.921607 140224885671680 logging_writer.py:48] [0] global_step=0, grad_norm=46.65699768066406, loss=32.59967803955078
I0214 02:50:06.959966 140399019657024 spec.py:321] Evaluating on the training split.
I0214 02:50:07.128435 140399019657024 input_pipeline.py:20] Loading split = train-clean-100
I0214 02:50:07.163148 140399019657024 input_pipeline.py:20] Loading split = train-clean-360
I0214 02:50:07.555671 140399019657024 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0214 02:51:26.825319 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 02:51:26.942553 140399019657024 input_pipeline.py:20] Loading split = dev-clean
I0214 02:51:26.947861 140399019657024 input_pipeline.py:20] Loading split = dev-other
I0214 02:52:32.257195 140399019657024 spec.py:349] Evaluating on the test split.
I0214 02:52:32.374637 140399019657024 input_pipeline.py:20] Loading split = test-clean
I0214 02:53:10.660982 140399019657024 submission_runner.py:408] Time since start: 245.92s, 	Step: 1, 	{'train/ctc_loss': Array(32.02916, dtype=float32), 'train/wer': 1.3788965703933531, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043156299178389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.27949, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 62.217230796813965, 'total_duration': 245.91571712493896, 'accumulated_submission_time': 62.217230796813965, 'accumulated_eval_time': 183.69842910766602, 'accumulated_logging_time': 0}
I0214 02:53:10.689110 140215641433856 logging_writer.py:48] [1] accumulated_eval_time=183.698429, accumulated_logging_time=0, accumulated_submission_time=62.217231, global_step=1, preemption_count=0, score=62.217231, test/ctc_loss=31.279489517211914, test/num_examples=2472, test/wer=1.097699, total_duration=245.915717, train/ctc_loss=32.02915954589844, train/wer=1.378897, validation/ctc_loss=31.163591384887695, validation/num_examples=5348, validation/wer=1.043156
I0214 02:54:50.618914 140226672416512 logging_writer.py:48] [100] global_step=100, grad_norm=23.54883575439453, loss=8.06506633758545
I0214 02:56:08.039816 140226680809216 logging_writer.py:48] [200] global_step=200, grad_norm=2.180469274520874, loss=6.207576274871826
I0214 02:57:25.603695 140226672416512 logging_writer.py:48] [300] global_step=300, grad_norm=1.2001465559005737, loss=5.863913059234619
I0214 02:58:42.770006 140226680809216 logging_writer.py:48] [400] global_step=400, grad_norm=0.3545736074447632, loss=5.814631462097168
I0214 03:00:00.131099 140226672416512 logging_writer.py:48] [500] global_step=500, grad_norm=0.2500763535499573, loss=5.81234073638916
I0214 03:01:25.183382 140226680809216 logging_writer.py:48] [600] global_step=600, grad_norm=0.3027534782886505, loss=5.811140060424805
I0214 03:02:52.991544 140226672416512 logging_writer.py:48] [700] global_step=700, grad_norm=0.8503317832946777, loss=5.817859172821045
I0214 03:04:20.431540 140226680809216 logging_writer.py:48] [800] global_step=800, grad_norm=0.58623868227005, loss=5.80345344543457
I0214 03:05:45.979988 140226672416512 logging_writer.py:48] [900] global_step=900, grad_norm=0.25471779704093933, loss=5.771727085113525
I0214 03:07:14.230871 140226680809216 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4502325654029846, loss=5.800830841064453
I0214 03:08:37.762127 140227378132736 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4998590350151062, loss=5.785528182983398
I0214 03:09:54.971991 140227369740032 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.36314165592193604, loss=5.770524024963379
I0214 03:11:13.036307 140227378132736 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.4102992117404938, loss=5.785315036773682
I0214 03:12:30.371334 140227369740032 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.2968561351299286, loss=5.772067070007324
I0214 03:13:48.653253 140227378132736 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.4834307134151459, loss=5.7769317626953125
I0214 03:15:16.481639 140227369740032 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8626517057418823, loss=5.718222618103027
I0214 03:16:40.243526 140227378132736 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7020875215530396, loss=5.580896377563477
I0214 03:17:11.322139 140399019657024 spec.py:321] Evaluating on the training split.
I0214 03:17:49.099470 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 03:18:36.168526 140399019657024 spec.py:349] Evaluating on the test split.
I0214 03:18:59.225642 140399019657024 submission_runner.py:408] Time since start: 1794.48s, 	Step: 1738, 	{'train/ctc_loss': Array(6.6277523, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.710369, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.6806073, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1502.7666385173798, 'total_duration': 1794.4777307510376, 'accumulated_submission_time': 1502.7666385173798, 'accumulated_eval_time': 291.5967798233032, 'accumulated_logging_time': 0.04290056228637695}
I0214 03:18:59.261282 140228033492736 logging_writer.py:48] [1738] accumulated_eval_time=291.596780, accumulated_logging_time=0.042901, accumulated_submission_time=1502.766639, global_step=1738, preemption_count=0, score=1502.766639, test/ctc_loss=6.680607318878174, test/num_examples=2472, test/wer=0.899580, total_duration=1794.477731, train/ctc_loss=6.627752304077148, train/wer=0.944636, validation/ctc_loss=6.710369110107422, validation/num_examples=5348, validation/wer=0.896618
I0214 03:19:48.771680 140228025100032 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.1735997200012207, loss=5.529717922210693
I0214 03:21:06.199720 140228033492736 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4659096896648407, loss=5.293018817901611
I0214 03:22:23.684683 140228025100032 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6944855451583862, loss=4.8540143966674805
I0214 03:23:45.514979 140228688852736 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7902207970619202, loss=4.220525741577148
I0214 03:25:03.531152 140228680460032 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.165855050086975, loss=3.8382928371429443
I0214 03:26:21.503618 140228688852736 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1654595136642456, loss=3.554292917251587
I0214 03:27:39.157563 140228680460032 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1425021886825562, loss=3.4257731437683105
I0214 03:28:57.005121 140228688852736 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.0388069152832031, loss=3.2409708499908447
I0214 03:30:25.073150 140228680460032 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9944076538085938, loss=3.178326368331909
I0214 03:31:53.351675 140228688852736 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0505192279815674, loss=3.0551249980926514
I0214 03:33:16.946722 140228680460032 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9341892600059509, loss=2.9381842613220215
I0214 03:34:43.223911 140228688852736 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9437026381492615, loss=2.8871500492095947
I0214 03:36:11.264561 140228680460032 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.1566054821014404, loss=2.826350688934326
I0214 03:37:39.167290 140227378132736 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9622841477394104, loss=2.6967132091522217
I0214 03:38:56.484548 140227369740032 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1070373058319092, loss=2.6773269176483154
I0214 03:40:13.493927 140227378132736 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8992457389831543, loss=2.648538827896118
I0214 03:41:30.770630 140227369740032 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9805911183357239, loss=2.589458465576172
I0214 03:42:49.267888 140227378132736 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.98311847448349, loss=2.492621660232544
I0214 03:42:59.625459 140399019657024 spec.py:321] Evaluating on the training split.
I0214 03:43:50.951192 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 03:44:41.977982 140399019657024 spec.py:349] Evaluating on the test split.
I0214 03:45:07.531507 140399019657024 submission_runner.py:408] Time since start: 3362.78s, 	Step: 3514, 	{'train/ctc_loss': Array(2.9477355, dtype=float32), 'train/wer': 0.6334869349812215, 'validation/ctc_loss': Array(3.3468678, dtype=float32), 'validation/wer': 0.6825936259980497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.023245, dtype=float32), 'test/wer': 0.625312290536835, 'test/num_examples': 2472, 'score': 2943.0448310375214, 'total_duration': 3362.7826220989227, 'accumulated_submission_time': 2943.0448310375214, 'accumulated_eval_time': 419.49665999412537, 'accumulated_logging_time': 0.09370923042297363}
I0214 03:45:07.574826 140228033492736 logging_writer.py:48] [3514] accumulated_eval_time=419.496660, accumulated_logging_time=0.093709, accumulated_submission_time=2943.044831, global_step=3514, preemption_count=0, score=2943.044831, test/ctc_loss=3.023245096206665, test/num_examples=2472, test/wer=0.625312, total_duration=3362.782622, train/ctc_loss=2.947735548019409, train/wer=0.633487, validation/ctc_loss=3.346867799758911, validation/num_examples=5348, validation/wer=0.682594
I0214 03:46:14.180166 140228025100032 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.0634602308273315, loss=2.5180745124816895
I0214 03:47:31.143600 140228033492736 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.015724778175354, loss=2.4562056064605713
I0214 03:48:49.138041 140228025100032 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1646472215652466, loss=2.4087448120117188
I0214 03:50:15.981996 140228033492736 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.979387640953064, loss=2.3711936473846436
I0214 03:51:41.457144 140228025100032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8527163863182068, loss=2.3354408740997314
I0214 03:53:10.198735 140228033492736 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0324324369430542, loss=2.3520140647888184
I0214 03:54:32.388146 140228033492736 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8242277503013611, loss=2.259089469909668
I0214 03:55:49.277623 140228025100032 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8164480924606323, loss=2.2601583003997803
I0214 03:57:06.155504 140228033492736 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8698683381080627, loss=2.1996123790740967
I0214 03:58:23.301828 140228025100032 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0098106861114502, loss=2.1804351806640625
I0214 03:59:45.766367 140228033492736 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.0828388929367065, loss=2.1094346046447754
I0214 04:01:15.368722 140228025100032 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.927733302116394, loss=2.0849087238311768
I0214 04:02:45.128436 140228033492736 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8859367370605469, loss=2.069143533706665
I0214 04:04:14.948672 140228025100032 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.005391001701355, loss=2.0766441822052
I0214 04:05:43.018125 140228033492736 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9979166984558105, loss=2.0405545234680176
I0214 04:07:10.646612 140228025100032 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8738248348236084, loss=1.9977912902832031
I0214 04:08:35.074309 140228033492736 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8808873891830444, loss=2.0188167095184326
I0214 04:09:07.789979 140399019657024 spec.py:321] Evaluating on the training split.
I0214 04:10:03.061825 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 04:10:55.816193 140399019657024 spec.py:349] Evaluating on the test split.
I0214 04:11:22.091426 140399019657024 submission_runner.py:408] Time since start: 4937.34s, 	Step: 5244, 	{'train/ctc_loss': Array(0.7954038, dtype=float32), 'train/wer': 0.26949412527476, 'validation/ctc_loss': Array(1.1628219, dtype=float32), 'validation/wer': 0.3323517769388957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.86656547, dtype=float32), 'test/wer': 0.2749172303130014, 'test/num_examples': 2472, 'score': 4383.173229217529, 'total_duration': 4937.342960596085, 'accumulated_submission_time': 4383.173229217529, 'accumulated_eval_time': 553.7924075126648, 'accumulated_logging_time': 0.1545546054840088}
I0214 04:11:22.124724 140228688852736 logging_writer.py:48] [5244] accumulated_eval_time=553.792408, accumulated_logging_time=0.154555, accumulated_submission_time=4383.173229, global_step=5244, preemption_count=0, score=4383.173229, test/ctc_loss=0.866565465927124, test/num_examples=2472, test/wer=0.274917, total_duration=4937.342961, train/ctc_loss=0.795403778553009, train/wer=0.269494, validation/ctc_loss=1.162821888923645, validation/num_examples=5348, validation/wer=0.332352
I0214 04:12:06.148302 140228680460032 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9456565976142883, loss=1.9309417009353638
I0214 04:13:23.116018 140228688852736 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7732535004615784, loss=1.9237713813781738
I0214 04:14:40.316166 140228680460032 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7488285303115845, loss=1.9416390657424927
I0214 04:15:57.182223 140228688852736 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8846495151519775, loss=1.9598321914672852
I0214 04:17:23.880199 140228680460032 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8924286961555481, loss=1.9251054525375366
I0214 04:18:52.782696 140228688852736 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7623401284217834, loss=1.8749582767486572
I0214 04:20:20.253909 140228680460032 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8640207648277283, loss=1.8576534986495972
I0214 04:21:49.451690 140228688852736 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0296663045883179, loss=1.8836772441864014
I0214 04:23:17.897399 140228680460032 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8704034090042114, loss=1.9025551080703735
I0214 04:24:47.853679 140228688852736 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7770646810531616, loss=1.7713923454284668
I0214 04:26:04.458769 140228680460032 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8187523484230042, loss=1.8574382066726685
I0214 04:27:21.660305 140228688852736 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7580314874649048, loss=1.8701978921890259
I0214 04:28:40.024996 140228680460032 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7444121837615967, loss=1.8487305641174316
I0214 04:30:02.940594 140228688852736 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7696605324745178, loss=1.7808175086975098
I0214 04:31:31.210434 140228680460032 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8684462904930115, loss=1.8210816383361816
I0214 04:33:00.149870 140228688852736 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7371543049812317, loss=1.779318928718567
I0214 04:34:25.350571 140228680460032 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7922670841217041, loss=1.8491774797439575
I0214 04:35:22.396955 140399019657024 spec.py:321] Evaluating on the training split.
I0214 04:36:18.431416 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 04:37:11.310699 140399019657024 spec.py:349] Evaluating on the test split.
I0214 04:37:38.627569 140399019657024 submission_runner.py:408] Time since start: 6513.88s, 	Step: 6967, 	{'train/ctc_loss': Array(0.52575487, dtype=float32), 'train/wer': 0.18291635685785798, 'validation/ctc_loss': Array(0.8364061, dtype=float32), 'validation/wer': 0.2505092829489172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.570568, dtype=float32), 'test/wer': 0.19348810757012572, 'test/num_examples': 2472, 'score': 5823.355607032776, 'total_duration': 6513.878301858902, 'accumulated_submission_time': 5823.355607032776, 'accumulated_eval_time': 690.0164759159088, 'accumulated_logging_time': 0.20651555061340332}
I0214 04:37:38.662809 140228688852736 logging_writer.py:48] [6967] accumulated_eval_time=690.016476, accumulated_logging_time=0.206516, accumulated_submission_time=5823.355607, global_step=6967, preemption_count=0, score=5823.355607, test/ctc_loss=0.5705680251121521, test/num_examples=2472, test/wer=0.193488, total_duration=6513.878302, train/ctc_loss=0.5257548689842224, train/wer=0.182916, validation/ctc_loss=0.8364061117172241, validation/num_examples=5348, validation/wer=0.250509
I0214 04:38:04.633390 140228680460032 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7993415594100952, loss=1.828171730041504
I0214 04:39:21.279666 140228688852736 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6924585103988647, loss=1.8055068254470825
I0214 04:40:37.951978 140228680460032 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8903378844261169, loss=1.736720085144043
I0214 04:41:58.302010 140228688852736 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7369360327720642, loss=1.7603211402893066
I0214 04:43:20.020781 140228680460032 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8583273887634277, loss=1.732661247253418
I0214 04:44:39.752776 140228688852736 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7450730204582214, loss=1.7058888673782349
I0214 04:46:01.549637 140228680460032 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7780295014381409, loss=1.750163197517395
I0214 04:47:30.834124 140228688852736 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7993589043617249, loss=1.7549914121627808
I0214 04:48:59.613406 140228680460032 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.0684489011764526, loss=1.6730756759643555
I0214 04:50:22.977532 140228688852736 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8251162767410278, loss=1.769269585609436
I0214 04:51:50.115501 140228680460032 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8624464869499207, loss=1.725078821182251
I0214 04:53:18.456360 140228688852736 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7978437542915344, loss=1.7336798906326294
I0214 04:54:47.937378 140228680460032 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.8295857906341553, loss=1.727344274520874
I0214 04:56:12.132926 140228688852736 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6864697933197021, loss=1.6806832551956177
I0214 04:57:31.058389 140228680460032 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7035327553749084, loss=1.6759274005889893
I0214 04:58:50.945706 140228688852736 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.728233814239502, loss=1.71537184715271
I0214 05:00:15.747784 140228680460032 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6827844977378845, loss=1.7282449007034302
I0214 05:01:38.772397 140399019657024 spec.py:321] Evaluating on the training split.
I0214 05:02:33.634300 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 05:03:26.407532 140399019657024 spec.py:349] Evaluating on the test split.
I0214 05:03:53.728701 140399019657024 submission_runner.py:408] Time since start: 8088.98s, 	Step: 8695, 	{'train/ctc_loss': Array(0.4505587, dtype=float32), 'train/wer': 0.160277457652917, 'validation/ctc_loss': Array(0.7544828, dtype=float32), 'validation/wer': 0.2271257132374948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49796993, dtype=float32), 'test/wer': 0.1694595088660045, 'test/num_examples': 2472, 'score': 7263.37749004364, 'total_duration': 8088.980140447617, 'accumulated_submission_time': 7263.37749004364, 'accumulated_eval_time': 824.9669606685638, 'accumulated_logging_time': 0.2596597671508789}
I0214 05:03:53.763207 140228688852736 logging_writer.py:48] [8695] accumulated_eval_time=824.966961, accumulated_logging_time=0.259660, accumulated_submission_time=7263.377490, global_step=8695, preemption_count=0, score=7263.377490, test/ctc_loss=0.49796992540359497, test/num_examples=2472, test/wer=0.169460, total_duration=8088.980140, train/ctc_loss=0.45055869221687317, train/wer=0.160277, validation/ctc_loss=0.7544828057289124, validation/num_examples=5348, validation/wer=0.227126
I0214 05:03:58.445751 140228680460032 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6476874351501465, loss=1.6563180685043335
I0214 05:05:15.630663 140228688852736 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8042441010475159, loss=1.6840150356292725
I0214 05:06:32.776675 140228680460032 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7059640288352966, loss=1.6779561042785645
I0214 05:07:58.057532 140228688852736 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.636300265789032, loss=1.6943012475967407
I0214 05:09:30.442021 140228680460032 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6720351576805115, loss=1.6531201601028442
I0214 05:11:00.857364 140228688852736 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.734546959400177, loss=1.6081570386886597
I0214 05:12:29.038704 140228688852736 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6521489024162292, loss=1.572424292564392
I0214 05:13:46.650045 140228680460032 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7572503685951233, loss=1.6194965839385986
I0214 05:15:03.168916 140228688852736 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8769221305847168, loss=1.638796329498291
I0214 05:16:25.402251 140228680460032 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6446064710617065, loss=1.627496361732483
I0214 05:17:50.970927 140228688852736 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6997306942939758, loss=1.6081194877624512
I0214 05:19:22.517373 140228680460032 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.633395791053772, loss=1.6745364665985107
I0214 05:20:49.538955 140228688852736 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6059873104095459, loss=1.6402415037155151
I0214 05:22:17.694217 140228680460032 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7110649943351746, loss=1.6650851964950562
I0214 05:23:44.416655 140228688852736 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7828207612037659, loss=1.5928210020065308
I0214 05:25:15.306902 140228680460032 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6598726511001587, loss=1.6514480113983154
I0214 05:26:48.509632 140228688852736 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.720941960811615, loss=1.6413532495498657
I0214 05:27:54.450113 140399019657024 spec.py:321] Evaluating on the training split.
I0214 05:28:49.544203 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 05:29:42.690219 140399019657024 spec.py:349] Evaluating on the test split.
I0214 05:30:10.828229 140399019657024 submission_runner.py:408] Time since start: 9666.08s, 	Step: 10387, 	{'train/ctc_loss': Array(0.4173494, dtype=float32), 'train/wer': 0.14835877109217877, 'validation/ctc_loss': Array(0.68799996, dtype=float32), 'validation/wer': 0.20828948511735232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4417418, dtype=float32), 'test/wer': 0.15079316718461194, 'test/num_examples': 2472, 'score': 8703.979434251785, 'total_duration': 9666.078685045242, 'accumulated_submission_time': 8703.979434251785, 'accumulated_eval_time': 961.3382768630981, 'accumulated_logging_time': 0.31139492988586426}
I0214 05:30:10.863241 140227966932736 logging_writer.py:48] [10387] accumulated_eval_time=961.338277, accumulated_logging_time=0.311395, accumulated_submission_time=8703.979434, global_step=10387, preemption_count=0, score=8703.979434, test/ctc_loss=0.44174179434776306, test/num_examples=2472, test/wer=0.150793, total_duration=9666.078685, train/ctc_loss=0.4173493981361389, train/wer=0.148359, validation/ctc_loss=0.687999963760376, validation/num_examples=5348, validation/wer=0.208289
I0214 05:30:21.592759 140227958540032 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.693935751914978, loss=1.5502588748931885
I0214 05:31:38.036485 140227966932736 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6216660737991333, loss=1.5944417715072632
I0214 05:32:54.731399 140227958540032 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.849493145942688, loss=1.585759162902832
I0214 05:34:12.012903 140227966932736 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6385294795036316, loss=1.5828046798706055
I0214 05:35:32.102949 140227958540032 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.7981867790222168, loss=1.5609478950500488
I0214 05:37:01.561617 140227966932736 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7393890619277954, loss=1.6343344449996948
I0214 05:38:29.569698 140227958540032 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7219037413597107, loss=1.620275855064392
I0214 05:39:58.134162 140227966932736 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7184645533561707, loss=1.5634264945983887
I0214 05:41:26.068801 140227958540032 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6403723359107971, loss=1.5332825183868408
I0214 05:42:56.271104 140227966932736 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.669880211353302, loss=1.5615410804748535
I0214 05:44:18.971245 140227639252736 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6310568451881409, loss=1.5015066862106323
I0214 05:45:35.919635 140227630860032 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6301239132881165, loss=1.5370609760284424
I0214 05:46:54.845970 140227639252736 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6411224603652954, loss=1.5521117448806763
I0214 05:48:19.237337 140227630860032 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6445788741111755, loss=1.595518708229065
I0214 05:49:45.863349 140227639252736 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6072559356689453, loss=1.511455774307251
I0214 05:51:14.255003 140227630860032 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7588467001914978, loss=1.5981223583221436
I0214 05:52:44.453793 140227639252736 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6901595592498779, loss=1.4694948196411133
I0214 05:54:11.864488 140227630860032 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6598760485649109, loss=1.5753892660140991
I0214 05:54:11.873092 140399019657024 spec.py:321] Evaluating on the training split.
I0214 05:55:07.801993 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 05:56:02.259902 140399019657024 spec.py:349] Evaluating on the test split.
I0214 05:56:28.623903 140399019657024 submission_runner.py:408] Time since start: 11243.88s, 	Step: 12101, 	{'train/ctc_loss': Array(0.35887453, dtype=float32), 'train/wer': 0.12945355539957032, 'validation/ctc_loss': Array(0.641523, dtype=float32), 'validation/wer': 0.1964142618535003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40709147, dtype=float32), 'test/wer': 0.1400483415595231, 'test/num_examples': 2472, 'score': 10144.905450105667, 'total_duration': 11243.875876426697, 'accumulated_submission_time': 10144.905450105667, 'accumulated_eval_time': 1098.0837564468384, 'accumulated_logging_time': 0.36140894889831543}
I0214 05:56:28.658617 140228397012736 logging_writer.py:48] [12101] accumulated_eval_time=1098.083756, accumulated_logging_time=0.361409, accumulated_submission_time=10144.905450, global_step=12101, preemption_count=0, score=10144.905450, test/ctc_loss=0.4070914685726166, test/num_examples=2472, test/wer=0.140048, total_duration=11243.875876, train/ctc_loss=0.35887452960014343, train/wer=0.129454, validation/ctc_loss=0.641523003578186, validation/num_examples=5348, validation/wer=0.196414
I0214 05:57:45.445981 140228388620032 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6832861304283142, loss=1.5214917659759521
I0214 05:59:02.028167 140228397012736 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6515945792198181, loss=1.5187135934829712
I0214 06:00:23.378672 140227741652736 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6560348868370056, loss=1.5320918560028076
I0214 06:01:40.250899 140227733260032 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6245310306549072, loss=1.4885573387145996
I0214 06:02:59.888560 140227741652736 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6519602537155151, loss=1.5200073719024658
I0214 06:04:22.886428 140227733260032 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6790751814842224, loss=1.5291211605072021
I0214 06:05:47.030161 140227741652736 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6397795081138611, loss=1.5274379253387451
I0214 06:07:17.747613 140227733260032 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7114217877388, loss=1.5331764221191406
I0214 06:08:45.781465 140227741652736 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7419846653938293, loss=1.554247498512268
I0214 06:10:17.376441 140227733260032 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7424975633621216, loss=1.4529657363891602
I0214 06:11:48.097385 140227741652736 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6797588467597961, loss=1.4721359014511108
I0214 06:13:15.635770 140227733260032 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.7158635258674622, loss=1.5520590543746948
I0214 06:14:46.343038 140227741652736 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6507290005683899, loss=1.4791289567947388
I0214 06:16:03.890744 140227733260032 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5620921850204468, loss=1.5343003273010254
I0214 06:17:22.417224 140227741652736 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9501355886459351, loss=1.4499832391738892
I0214 06:18:45.030276 140227733260032 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5635291934013367, loss=1.4920674562454224
I0214 06:20:07.090420 140227741652736 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5744807720184326, loss=1.4651063680648804
I0214 06:20:29.004336 140399019657024 spec.py:321] Evaluating on the training split.
I0214 06:21:24.739516 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 06:22:18.614458 140399019657024 spec.py:349] Evaluating on the test split.
I0214 06:22:46.248387 140399019657024 submission_runner.py:408] Time since start: 12821.50s, 	Step: 13827, 	{'train/ctc_loss': Array(0.30647576, dtype=float32), 'train/wer': 0.1146051639408201, 'validation/ctc_loss': Array(0.61191267, dtype=float32), 'validation/wer': 0.18591965397723434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38234127, dtype=float32), 'test/wer': 0.13007535596043304, 'test/num_examples': 2472, 'score': 11585.165503025055, 'total_duration': 12821.49998164177, 'accumulated_submission_time': 11585.165503025055, 'accumulated_eval_time': 1235.3221170902252, 'accumulated_logging_time': 0.4123513698577881}
I0214 06:22:46.284898 140227526612736 logging_writer.py:48] [13827] accumulated_eval_time=1235.322117, accumulated_logging_time=0.412351, accumulated_submission_time=11585.165503, global_step=13827, preemption_count=0, score=11585.165503, test/ctc_loss=0.38234126567840576, test/num_examples=2472, test/wer=0.130075, total_duration=12821.499982, train/ctc_loss=0.30647575855255127, train/wer=0.114605, validation/ctc_loss=0.6119126677513123, validation/num_examples=5348, validation/wer=0.185920
I0214 06:23:42.790709 140227518220032 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6559535264968872, loss=1.5090205669403076
I0214 06:25:00.342976 140227526612736 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.762895405292511, loss=1.459510087966919
I0214 06:26:18.905577 140227518220032 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6413031816482544, loss=1.488120436668396
I0214 06:27:46.200842 140227526612736 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6567457914352417, loss=1.5358716249465942
I0214 06:29:16.380008 140227518220032 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.9272944331169128, loss=1.433532476425171
I0214 06:30:45.046858 140227526612736 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6147965788841248, loss=1.4286142587661743
I0214 06:32:09.889079 140227526612736 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6773685812950134, loss=1.4477312564849854
I0214 06:33:28.436917 140227518220032 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.6784624457359314, loss=1.4201148748397827
I0214 06:34:46.534992 140227526612736 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6816737651824951, loss=1.4687366485595703
I0214 06:36:07.569241 140227518220032 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8660352826118469, loss=1.4478533267974854
I0214 06:37:36.072863 140227526612736 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6202588677406311, loss=1.4340863227844238
I0214 06:39:04.181990 140227518220032 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6624699831008911, loss=1.4967519044876099
I0214 06:40:33.499864 140227526612736 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6075104475021362, loss=1.46157968044281
I0214 06:42:06.148119 140227518220032 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6630311012268066, loss=1.5192315578460693
I0214 06:43:33.459349 140227526612736 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6754040122032166, loss=1.4935741424560547
I0214 06:44:58.954094 140227518220032 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6855206489562988, loss=1.4000130891799927
I0214 06:46:26.460115 140228397012736 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6636734008789062, loss=1.4496853351593018
I0214 06:46:46.305153 140399019657024 spec.py:321] Evaluating on the training split.
I0214 06:47:42.002868 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 06:48:35.112995 140399019657024 spec.py:349] Evaluating on the test split.
I0214 06:49:02.170306 140399019657024 submission_runner.py:408] Time since start: 14397.42s, 	Step: 15527, 	{'train/ctc_loss': Array(0.2841638, dtype=float32), 'train/wer': 0.10447999404157068, 'validation/ctc_loss': Array(0.58510756, dtype=float32), 'validation/wer': 0.17735597671297681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35999078, dtype=float32), 'test/wer': 0.122214774643024, 'test/num_examples': 2472, 'score': 13025.100800514221, 'total_duration': 14397.421264886856, 'accumulated_submission_time': 13025.100800514221, 'accumulated_eval_time': 1371.180969953537, 'accumulated_logging_time': 0.4650459289550781}
I0214 06:49:02.208589 140228397012736 logging_writer.py:48] [15527] accumulated_eval_time=1371.180970, accumulated_logging_time=0.465046, accumulated_submission_time=13025.100801, global_step=15527, preemption_count=0, score=13025.100801, test/ctc_loss=0.35999077558517456, test/num_examples=2472, test/wer=0.122215, total_duration=14397.421265, train/ctc_loss=0.28416380286216736, train/wer=0.104480, validation/ctc_loss=0.5851075649261475, validation/num_examples=5348, validation/wer=0.177356
I0214 06:49:58.734084 140228388620032 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7355413436889648, loss=1.5115588903427124
I0214 06:51:15.236190 140228397012736 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6579475998878479, loss=1.444411039352417
I0214 06:52:32.619718 140228388620032 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7455279231071472, loss=1.4667352437973022
I0214 06:53:49.684005 140228397012736 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6703409552574158, loss=1.463489294052124
I0214 06:55:14.163865 140228388620032 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6373302340507507, loss=1.4149854183197021
I0214 06:56:44.456130 140228397012736 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6187164187431335, loss=1.4924157857894897
I0214 06:58:15.203479 140228388620032 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7233363389968872, loss=1.4156334400177002
I0214 06:59:43.553081 140228397012736 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6054996252059937, loss=1.4215706586837769
I0214 07:01:11.963635 140228388620032 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.731085479259491, loss=1.4677650928497314
I0214 07:02:40.633911 140228069332736 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7030026912689209, loss=1.4439761638641357
I0214 07:03:57.822459 140228060940032 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5559350252151489, loss=1.418906569480896
I0214 07:05:14.501071 140228069332736 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6702251434326172, loss=1.3849306106567383
I0214 07:06:33.142741 140228060940032 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.1302573680877686, loss=1.4298591613769531
I0214 07:07:56.933512 140228069332736 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7321450710296631, loss=1.4481300115585327
I0214 07:09:25.441229 140228060940032 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.960886538028717, loss=1.4698617458343506
I0214 07:10:53.261569 140228069332736 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6000177264213562, loss=1.4370447397232056
I0214 07:12:24.655851 140228060940032 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.7038397789001465, loss=1.4562171697616577
I0214 07:13:03.212970 140399019657024 spec.py:321] Evaluating on the training split.
I0214 07:13:58.335242 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 07:14:52.425901 140399019657024 spec.py:349] Evaluating on the test split.
I0214 07:15:19.348045 140399019657024 submission_runner.py:408] Time since start: 15974.60s, 	Step: 17244, 	{'train/ctc_loss': Array(0.27435565, dtype=float32), 'train/wer': 0.10281175757640762, 'validation/ctc_loss': Array(0.569697, dtype=float32), 'validation/wer': 0.17216177336667407, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3494229, dtype=float32), 'test/wer': 0.11859931346860846, 'test/num_examples': 2472, 'score': 14466.018681049347, 'total_duration': 15974.599870920181, 'accumulated_submission_time': 14466.018681049347, 'accumulated_eval_time': 1507.310597896576, 'accumulated_logging_time': 0.5201177597045898}
I0214 07:15:19.382946 140227593168640 logging_writer.py:48] [17244] accumulated_eval_time=1507.310598, accumulated_logging_time=0.520118, accumulated_submission_time=14466.018681, global_step=17244, preemption_count=0, score=14466.018681, test/ctc_loss=0.3494229018688202, test/num_examples=2472, test/wer=0.118599, total_duration=15974.599871, train/ctc_loss=0.2743556499481201, train/wer=0.102812, validation/ctc_loss=0.5696970224380493, validation/num_examples=5348, validation/wer=0.172162
I0214 07:16:03.191018 140227584775936 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6631512641906738, loss=1.4300020933151245
I0214 07:17:20.310113 140227593168640 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6505115628242493, loss=1.4339663982391357
I0214 07:18:37.738539 140227584775936 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6247693300247192, loss=1.4459365606307983
I0214 07:19:59.781000 140227593168640 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6022955775260925, loss=1.4194676876068115
I0214 07:21:19.228444 140227584775936 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6147547960281372, loss=1.4697962999343872
I0214 07:22:40.168388 140227593168640 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.7068423628807068, loss=1.5104057788848877
I0214 07:24:05.906586 140227584775936 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6380869150161743, loss=1.5357602834701538
I0214 07:25:32.136868 140227593168640 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7807178497314453, loss=1.3648059368133545
I0214 07:27:00.614234 140227584775936 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6783511638641357, loss=1.4194533824920654
I0214 07:28:32.847175 140227593168640 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7664039134979248, loss=1.4697574377059937
I0214 07:30:03.222264 140227584775936 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5983495712280273, loss=1.4086371660232544
I0214 07:31:35.454665 140227593168640 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8369134664535522, loss=1.4390289783477783
I0214 07:33:03.343803 140227584775936 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7651355266571045, loss=1.4339240789413452
I0214 07:34:27.963610 140227593168640 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8265707492828369, loss=1.448756217956543
I0214 07:35:45.477213 140227584775936 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.602615237236023, loss=1.3268108367919922
I0214 07:37:04.880438 140227593168640 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6365854144096375, loss=1.3955751657485962
I0214 07:38:28.116200 140227584775936 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6265974640846252, loss=1.381296157836914
I0214 07:39:19.598640 140399019657024 spec.py:321] Evaluating on the training split.
I0214 07:40:15.073841 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 07:41:09.483484 140399019657024 spec.py:349] Evaluating on the test split.
I0214 07:41:36.195541 140399019657024 submission_runner.py:408] Time since start: 17551.45s, 	Step: 18961, 	{'train/ctc_loss': Array(0.28704607, dtype=float32), 'train/wer': 0.10458235278563685, 'validation/ctc_loss': Array(0.55894655, dtype=float32), 'validation/wer': 0.170549446305647, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34298202, dtype=float32), 'test/wer': 0.11685251762029533, 'test/num_examples': 2472, 'score': 15906.14902305603, 'total_duration': 17551.447067975998, 'accumulated_submission_time': 15906.14902305603, 'accumulated_eval_time': 1643.9017758369446, 'accumulated_logging_time': 0.5702481269836426}
I0214 07:41:36.232915 140227081164544 logging_writer.py:48] [18961] accumulated_eval_time=1643.901776, accumulated_logging_time=0.570248, accumulated_submission_time=15906.149023, global_step=18961, preemption_count=0, score=15906.149023, test/ctc_loss=0.3429820239543915, test/num_examples=2472, test/wer=0.116853, total_duration=17551.447068, train/ctc_loss=0.28704607486724854, train/wer=0.104582, validation/ctc_loss=0.5589465498924255, validation/num_examples=5348, validation/wer=0.170549
I0214 07:42:06.770962 140227072771840 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7587826251983643, loss=1.3816823959350586
I0214 07:43:23.405350 140227081164544 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6992226839065552, loss=1.4402902126312256
I0214 07:44:41.392926 140227072771840 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7441304922103882, loss=1.383013367652893
I0214 07:46:10.090245 140227081164544 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6517629027366638, loss=1.4168356657028198
I0214 07:47:38.055123 140227072771840 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.618843138217926, loss=1.3863236904144287
I0214 07:49:10.074026 140227081164544 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6043354868888855, loss=1.3977940082550049
I0214 07:50:40.665777 140227081164544 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7936393618583679, loss=1.3984053134918213
I0214 07:51:59.192184 140227072771840 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6498579978942871, loss=1.35548734664917
I0214 07:53:18.018282 140227081164544 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8777502775192261, loss=1.4205058813095093
I0214 07:54:39.701796 140227072771840 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6940650343894958, loss=1.4363244771957397
I0214 07:56:04.405991 140227081164544 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7921505570411682, loss=1.451856017112732
I0214 07:57:34.410056 140227072771840 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8279311656951904, loss=1.418373942375183
I0214 07:59:05.026751 140227081164544 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7625669836997986, loss=1.4813637733459473
I0214 08:00:33.877622 140227072771840 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8406808972358704, loss=1.4753273725509644
I0214 08:02:06.437820 140227081164544 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7177483439445496, loss=1.4251635074615479
I0214 08:03:32.168159 140227072771840 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7354947924613953, loss=1.4218674898147583
I0214 08:05:05.465121 140227081164544 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.9340161085128784, loss=1.3452908992767334
I0214 08:05:36.422994 140399019657024 spec.py:321] Evaluating on the training split.
I0214 08:06:33.070734 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 08:07:27.891304 140399019657024 spec.py:349] Evaluating on the test split.
I0214 08:07:55.590068 140399019657024 submission_runner.py:408] Time since start: 19130.84s, 	Step: 20642, 	{'train/ctc_loss': Array(0.2800363, dtype=float32), 'train/wer': 0.10345846413591157, 'validation/ctc_loss': Array(0.54050195, dtype=float32), 'validation/wer': 0.16402290083705842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32796475, dtype=float32), 'test/wer': 0.11216054272540775, 'test/num_examples': 2472, 'score': 17346.25297522545, 'total_duration': 19130.841157197952, 'accumulated_submission_time': 17346.25297522545, 'accumulated_eval_time': 1783.0626783370972, 'accumulated_logging_time': 0.6249892711639404}
I0214 08:07:55.628507 140227081164544 logging_writer.py:48] [20642] accumulated_eval_time=1783.062678, accumulated_logging_time=0.624989, accumulated_submission_time=17346.252975, global_step=20642, preemption_count=0, score=17346.252975, test/ctc_loss=0.32796475291252136, test/num_examples=2472, test/wer=0.112161, total_duration=19130.841157, train/ctc_loss=0.2800363004207611, train/wer=0.103458, validation/ctc_loss=0.5405019521713257, validation/num_examples=5348, validation/wer=0.164023
I0214 08:08:40.520250 140227072771840 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6236632466316223, loss=1.3836171627044678
I0214 08:09:57.037944 140227081164544 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5796570777893066, loss=1.3731681108474731
I0214 08:11:13.640700 140227072771840 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6726134419441223, loss=1.4338459968566895
I0214 08:12:30.024994 140227081164544 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5653206706047058, loss=1.4397327899932861
I0214 08:13:52.407752 140227072771840 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6546093821525574, loss=1.4287718534469604
I0214 08:15:24.258691 140227081164544 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6161981225013733, loss=1.4105135202407837
I0214 08:16:51.263650 140227072771840 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6978866457939148, loss=1.3836562633514404
I0214 08:18:18.546977 140227081164544 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6986268162727356, loss=1.3753349781036377
I0214 08:19:49.935143 140227072771840 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8054738640785217, loss=1.383656620979309
I0214 08:21:21.122664 140227081164544 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8286683559417725, loss=1.4096816778182983
I0214 08:22:45.978794 140226753484544 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7442474365234375, loss=1.349544644355774
I0214 08:24:04.121207 140226745091840 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6536734104156494, loss=1.4206703901290894
I0214 08:25:21.534891 140226753484544 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7984030246734619, loss=1.4498469829559326
I0214 08:26:41.171685 140226745091840 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7970600128173828, loss=1.3613766431808472
I0214 08:28:09.805254 140226753484544 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.6982113122940063, loss=1.391455888748169
I0214 08:29:39.241060 140226745091840 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6022641062736511, loss=1.3518813848495483
I0214 08:31:09.212018 140226753484544 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.671034574508667, loss=1.3952956199645996
I0214 08:31:55.962285 140399019657024 spec.py:321] Evaluating on the training split.
I0214 08:32:52.373963 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 08:33:45.229375 140399019657024 spec.py:349] Evaluating on the test split.
I0214 08:34:11.627949 140399019657024 submission_runner.py:408] Time since start: 20706.88s, 	Step: 22354, 	{'train/ctc_loss': Array(0.2611929, dtype=float32), 'train/wer': 0.09692869636793909, 'validation/ctc_loss': Array(0.5234255, dtype=float32), 'validation/wer': 0.16026724079670196, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3156809, dtype=float32), 'test/wer': 0.10909349420104401, 'test/num_examples': 2472, 'score': 18786.501445770264, 'total_duration': 20706.88010787964, 'accumulated_submission_time': 18786.501445770264, 'accumulated_eval_time': 1918.723219871521, 'accumulated_logging_time': 0.6793062686920166}
I0214 08:34:11.667167 140227081164544 logging_writer.py:48] [22354] accumulated_eval_time=1918.723220, accumulated_logging_time=0.679306, accumulated_submission_time=18786.501446, global_step=22354, preemption_count=0, score=18786.501446, test/ctc_loss=0.3156808912754059, test/num_examples=2472, test/wer=0.109093, total_duration=20706.880108, train/ctc_loss=0.2611928880214691, train/wer=0.096929, validation/ctc_loss=0.5234255194664001, validation/num_examples=5348, validation/wer=0.160267
I0214 08:34:47.683959 140227072771840 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7470589280128479, loss=1.3298885822296143
I0214 08:36:04.371898 140227081164544 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6726725101470947, loss=1.3465923070907593
I0214 08:37:22.420796 140227072771840 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.8025693893432617, loss=1.3732011318206787
I0214 08:38:49.288954 140226753484544 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7435570955276489, loss=1.3579338788986206
I0214 08:40:06.824688 140226745091840 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.609861433506012, loss=1.3893053531646729
I0214 08:41:24.421528 140226753484544 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6281980276107788, loss=1.34489107131958
I0214 08:42:46.170639 140226745091840 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6655075550079346, loss=1.3534389734268188
I0214 08:44:10.626369 140226753484544 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.8400611877441406, loss=1.405823826789856
I0214 08:45:42.304562 140226745091840 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6296952962875366, loss=1.3410085439682007
I0214 08:47:12.005850 140226753484544 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.01283860206604, loss=1.3705705404281616
I0214 08:48:41.881668 140226745091840 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9261251091957092, loss=1.3272260427474976
I0214 08:50:14.608198 140226753484544 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7415924668312073, loss=1.3846697807312012
I0214 08:51:44.603536 140226745091840 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7061557173728943, loss=1.404015302658081
I0214 08:53:17.558741 140226753484544 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6050705313682556, loss=1.3411444425582886
I0214 08:54:35.015452 140226745091840 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.7285926938056946, loss=1.3625138998031616
I0214 08:55:52.022141 140226753484544 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6311089396476746, loss=1.3166224956512451
I0214 08:57:10.799509 140226745091840 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8161279559135437, loss=1.3853756189346313
I0214 08:58:11.925081 140399019657024 spec.py:321] Evaluating on the training split.
I0214 08:59:07.502765 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 08:59:59.967851 140399019657024 spec.py:349] Evaluating on the test split.
I0214 09:00:26.779294 140399019657024 submission_runner.py:408] Time since start: 22282.03s, 	Step: 24076, 	{'train/ctc_loss': Array(0.24182494, dtype=float32), 'train/wer': 0.08912735948650817, 'validation/ctc_loss': Array(0.5162125, dtype=float32), 'validation/wer': 0.15688811222568716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3084755, dtype=float32), 'test/wer': 0.10423902666910406, 'test/num_examples': 2472, 'score': 20226.675240516663, 'total_duration': 22282.030920743942, 'accumulated_submission_time': 20226.675240516663, 'accumulated_eval_time': 2053.571811437607, 'accumulated_logging_time': 0.733173131942749}
I0214 09:00:26.817051 140228258772736 logging_writer.py:48] [24076] accumulated_eval_time=2053.571811, accumulated_logging_time=0.733173, accumulated_submission_time=20226.675241, global_step=24076, preemption_count=0, score=20226.675241, test/ctc_loss=0.3084754943847656, test/num_examples=2472, test/wer=0.104239, total_duration=22282.030921, train/ctc_loss=0.2418249398469925, train/wer=0.089127, validation/ctc_loss=0.516212522983551, validation/num_examples=5348, validation/wer=0.156888
I0214 09:00:45.939798 140228250380032 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6477687954902649, loss=1.3757505416870117
I0214 09:02:03.313709 140228258772736 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6757887601852417, loss=1.3920520544052124
I0214 09:03:20.780373 140228250380032 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7500926852226257, loss=1.2759993076324463
I0214 09:04:43.398646 140228258772736 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6436701416969299, loss=1.4004052877426147
I0214 09:06:15.006534 140228250380032 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.747874915599823, loss=1.3657207489013672
I0214 09:07:43.269802 140228258772736 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6990137100219727, loss=1.3357264995574951
I0214 09:09:14.922796 140228250380032 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6280199885368347, loss=1.2940208911895752
I0214 09:10:37.569001 140227603412736 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5671780705451965, loss=1.2699981927871704
I0214 09:11:57.511263 140227301328640 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7051537036895752, loss=1.385636329650879
I0214 09:13:17.206173 140227603412736 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6558520793914795, loss=1.390886664390564
I0214 09:14:42.365146 140227301328640 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5672531127929688, loss=1.3641067743301392
I0214 09:16:11.412021 140227603412736 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.8024519681930542, loss=1.3216105699539185
I0214 09:17:43.385056 140227301328640 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5849475264549255, loss=1.2344512939453125
I0214 09:19:10.585656 140227603412736 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6122190952301025, loss=1.366414189338684
I0214 09:20:40.921313 140227301328640 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6530482172966003, loss=1.3280644416809082
I0214 09:22:10.614758 140227603412736 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7039351463317871, loss=1.3203281164169312
I0214 09:23:39.777854 140227301328640 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.7240594625473022, loss=1.3473594188690186
I0214 09:24:26.778367 140399019657024 spec.py:321] Evaluating on the training split.
I0214 09:25:22.843119 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 09:26:15.604202 140399019657024 spec.py:349] Evaluating on the test split.
I0214 09:26:42.739531 140399019657024 submission_runner.py:408] Time since start: 23857.99s, 	Step: 25751, 	{'train/ctc_loss': Array(0.22988309, dtype=float32), 'train/wer': 0.08592295052035787, 'validation/ctc_loss': Array(0.50804406, dtype=float32), 'validation/wer': 0.15461926875657722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30117932, dtype=float32), 'test/wer': 0.102309426604107, 'test/num_examples': 2472, 'score': 21666.55222582817, 'total_duration': 23857.990137577057, 'accumulated_submission_time': 21666.55222582817, 'accumulated_eval_time': 2189.5265684127808, 'accumulated_logging_time': 0.7874441146850586}
I0214 09:26:42.783496 140227603412736 logging_writer.py:48] [25751] accumulated_eval_time=2189.526568, accumulated_logging_time=0.787444, accumulated_submission_time=21666.552226, global_step=25751, preemption_count=0, score=21666.552226, test/ctc_loss=0.30117931962013245, test/num_examples=2472, test/wer=0.102309, total_duration=23857.990138, train/ctc_loss=0.2298830896615982, train/wer=0.085923, validation/ctc_loss=0.5080440640449524, validation/num_examples=5348, validation/wer=0.154619
I0214 09:27:21.001653 140227595020032 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.675443708896637, loss=1.3192434310913086
I0214 09:28:37.320050 140227603412736 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7269005179405212, loss=1.312505841255188
I0214 09:29:53.939440 140227595020032 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6871179938316345, loss=1.329519271850586
I0214 09:31:10.714038 140227603412736 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.5810534358024597, loss=1.2706639766693115
I0214 09:32:28.436340 140227595020032 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8217728137969971, loss=1.2905372381210327
I0214 09:33:58.112258 140227603412736 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6056215167045593, loss=1.3344230651855469
I0214 09:35:24.941031 140227595020032 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.7458435893058777, loss=1.335472822189331
I0214 09:36:53.226498 140227603412736 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7703551650047302, loss=1.344016671180725
I0214 09:38:24.806654 140227595020032 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6526656150817871, loss=1.3226382732391357
I0214 09:39:53.800015 140227603412736 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5243370532989502, loss=1.2897995710372925
I0214 09:41:22.678847 140227603412736 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.63129061460495, loss=1.3208681344985962
I0214 09:42:40.020254 140227595020032 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7916168570518494, loss=1.3278356790542603
I0214 09:43:56.556437 140227603412736 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8646976947784424, loss=1.2961699962615967
I0214 09:45:17.928454 140227595020032 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6077048778533936, loss=1.2937476634979248
I0214 09:46:44.239871 140227603412736 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.609736979007721, loss=1.269303560256958
I0214 09:48:14.554919 140227595020032 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6639093160629272, loss=1.3371338844299316
I0214 09:49:43.818773 140227603412736 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7333467602729797, loss=1.2987107038497925
I0214 09:50:43.368368 140399019657024 spec.py:321] Evaluating on the training split.
I0214 09:51:39.887834 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 09:52:33.465126 140399019657024 spec.py:349] Evaluating on the test split.
I0214 09:52:59.555590 140399019657024 submission_runner.py:408] Time since start: 25434.81s, 	Step: 27470, 	{'train/ctc_loss': Array(0.22008015, dtype=float32), 'train/wer': 0.08128177066819404, 'validation/ctc_loss': Array(0.49360743, dtype=float32), 'validation/wer': 0.14966643173677555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29376552, dtype=float32), 'test/wer': 0.10098917392805638, 'test/num_examples': 2472, 'score': 23107.052596330643, 'total_duration': 25434.80774664879, 'accumulated_submission_time': 23107.052596330643, 'accumulated_eval_time': 2325.708679676056, 'accumulated_logging_time': 0.8463056087493896}
I0214 09:52:59.596606 140227603412736 logging_writer.py:48] [27470] accumulated_eval_time=2325.708680, accumulated_logging_time=0.846306, accumulated_submission_time=23107.052596, global_step=27470, preemption_count=0, score=23107.052596, test/ctc_loss=0.29376551508903503, test/num_examples=2472, test/wer=0.100989, total_duration=25434.807747, train/ctc_loss=0.2200801521539688, train/wer=0.081282, validation/ctc_loss=0.49360743165016174, validation/num_examples=5348, validation/wer=0.149666
I0214 09:53:23.223669 140227595020032 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6887239217758179, loss=1.3363620042800903
I0214 09:54:40.291927 140227603412736 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.0435338020324707, loss=1.3268016576766968
I0214 09:55:56.936099 140227595020032 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6661728620529175, loss=1.321252465248108
I0214 09:57:21.725848 140227603412736 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6990178823471069, loss=1.3003203868865967
I0214 09:58:44.257932 140227603412736 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.757053554058075, loss=1.2917190790176392
I0214 10:00:01.096915 140227595020032 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5620726346969604, loss=1.3040257692337036
I0214 10:01:24.438048 140227603412736 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7139050364494324, loss=1.3796937465667725
I0214 10:02:49.574377 140227595020032 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7236176133155823, loss=1.2707784175872803
I0214 10:04:17.530643 140227603412736 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.7412188649177551, loss=1.3006315231323242
I0214 10:05:45.293691 140227595020032 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.5628986954689026, loss=1.2812379598617554
I0214 10:07:13.560758 140227603412736 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7187665104866028, loss=1.328874111175537
I0214 10:08:40.722540 140227595020032 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7548572421073914, loss=1.360200047492981
I0214 10:10:12.211931 140227603412736 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9161545634269714, loss=1.319582462310791
I0214 10:11:42.837276 140227595020032 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8545947074890137, loss=1.3176804780960083
I0214 10:13:08.098223 140226948052736 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.7619603276252747, loss=1.2544037103652954
I0214 10:14:27.208439 140226939660032 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6421610713005066, loss=1.3079617023468018
I0214 10:15:49.861664 140226948052736 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8725623488426208, loss=1.2838127613067627
I0214 10:17:00.207785 140399019657024 spec.py:321] Evaluating on the training split.
I0214 10:17:57.283864 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 10:18:51.098081 140399019657024 spec.py:349] Evaluating on the test split.
I0214 10:19:19.116176 140399019657024 submission_runner.py:408] Time since start: 27014.37s, 	Step: 29189, 	{'train/ctc_loss': Array(0.2380161, dtype=float32), 'train/wer': 0.08831686600797366, 'validation/ctc_loss': Array(0.48860037, dtype=float32), 'validation/wer': 0.14919335373683346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2872465, dtype=float32), 'test/wer': 0.09849084963337598, 'test/num_examples': 2472, 'score': 24547.576735258102, 'total_duration': 27014.36773943901, 'accumulated_submission_time': 24547.576735258102, 'accumulated_eval_time': 2464.611401796341, 'accumulated_logging_time': 0.904517650604248}
I0214 10:19:19.153945 140227603412736 logging_writer.py:48] [29189] accumulated_eval_time=2464.611402, accumulated_logging_time=0.904518, accumulated_submission_time=24547.576735, global_step=29189, preemption_count=0, score=24547.576735, test/ctc_loss=0.2872464954853058, test/num_examples=2472, test/wer=0.098491, total_duration=27014.367739, train/ctc_loss=0.23801609873771667, train/wer=0.088317, validation/ctc_loss=0.48860037326812744, validation/num_examples=5348, validation/wer=0.149193
I0214 10:19:28.350790 140227595020032 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5719461441040039, loss=1.2952436208724976
I0214 10:20:45.506106 140227603412736 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6526393294334412, loss=1.2976572513580322
I0214 10:22:02.025997 140227595020032 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7743399739265442, loss=1.2515809535980225
I0214 10:23:25.977626 140227603412736 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7725125551223755, loss=1.3187861442565918
I0214 10:24:58.245314 140227595020032 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.9486395120620728, loss=1.302236557006836
I0214 10:26:27.080468 140227603412736 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.7037222385406494, loss=1.3171318769454956
I0214 10:27:56.134479 140227595020032 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6429136395454407, loss=1.2971644401550293
I0214 10:29:24.065582 140227603412736 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.7346222996711731, loss=1.284642219543457
I0214 10:30:41.985134 140227595020032 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6909452676773071, loss=1.272207498550415
I0214 10:31:59.034910 140227603412736 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.64590984582901, loss=1.2580242156982422
I0214 10:33:18.212875 140227595020032 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.56471186876297, loss=1.295395016670227
I0214 10:34:41.948462 140227603412736 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7913751006126404, loss=1.2674778699874878
I0214 10:36:12.124471 140227595020032 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.8003259897232056, loss=1.2895658016204834
I0214 10:37:39.273348 140227603412736 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6348038911819458, loss=1.2857235670089722
I0214 10:39:07.968806 140227595020032 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.7468615770339966, loss=1.2985435724258423
I0214 10:40:37.817684 140227603412736 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6073628067970276, loss=1.279228925704956
I0214 10:42:07.874432 140227595020032 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.6752465963363647, loss=1.2857478857040405
I0214 10:43:20.378679 140399019657024 spec.py:321] Evaluating on the training split.
I0214 10:44:14.892734 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 10:45:08.221638 140399019657024 spec.py:349] Evaluating on the test split.
I0214 10:45:35.235534 140399019657024 submission_runner.py:408] Time since start: 28590.49s, 	Step: 30881, 	{'train/ctc_loss': Array(0.2175279, dtype=float32), 'train/wer': 0.0792353729537328, 'validation/ctc_loss': Array(0.47386247, dtype=float32), 'validation/wer': 0.14348745377834848, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28259403, dtype=float32), 'test/wer': 0.09463164950338188, 'test/num_examples': 2472, 'score': 25988.718726873398, 'total_duration': 28590.486193180084, 'accumulated_submission_time': 25988.718726873398, 'accumulated_eval_time': 2599.4616689682007, 'accumulated_logging_time': 0.9569101333618164}
I0214 10:45:35.279100 140228688852736 logging_writer.py:48] [30881] accumulated_eval_time=2599.461669, accumulated_logging_time=0.956910, accumulated_submission_time=25988.718727, global_step=30881, preemption_count=0, score=25988.718727, test/ctc_loss=0.2825940251350403, test/num_examples=2472, test/wer=0.094632, total_duration=28590.486193, train/ctc_loss=0.21752789616584778, train/wer=0.079235, validation/ctc_loss=0.4738624691963196, validation/num_examples=5348, validation/wer=0.143487
I0214 10:45:54.594806 140228688852736 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7867415547370911, loss=1.242437720298767
I0214 10:47:12.230355 140228680460032 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6770761609077454, loss=1.3226269483566284
I0214 10:48:28.830102 140228688852736 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.8674089312553406, loss=1.2759007215499878
I0214 10:49:50.401479 140228680460032 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6597068905830383, loss=1.3107692003250122
I0214 10:51:14.717425 140228688852736 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.5803703665733337, loss=1.2623190879821777
I0214 10:52:44.033382 140228680460032 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6419428586959839, loss=1.2760345935821533
I0214 10:54:11.634217 140228688852736 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.749330461025238, loss=1.2144266366958618
I0214 10:55:42.373585 140228680460032 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.8144508600234985, loss=1.2987128496170044
I0214 10:57:13.766300 140228688852736 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.7500735521316528, loss=1.2610821723937988
I0214 10:58:46.002017 140228680460032 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6357227563858032, loss=1.2354590892791748
I0214 11:00:15.376396 140228688852736 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.099178671836853, loss=1.243888020515442
I0214 11:01:39.030554 140228361172736 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6666662693023682, loss=1.2729657888412476
I0214 11:02:57.715694 140228352780032 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6164184808731079, loss=1.2433016300201416
I0214 11:04:18.522740 140228361172736 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.7146986126899719, loss=1.262639045715332
I0214 11:05:44.574746 140228352780032 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6347241997718811, loss=1.2639403343200684
I0214 11:07:12.443699 140228361172736 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7120726704597473, loss=1.2839363813400269
I0214 11:08:41.219075 140228352780032 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6327534317970276, loss=1.2574734687805176
I0214 11:09:35.572643 140399019657024 spec.py:321] Evaluating on the training split.
I0214 11:10:32.786312 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 11:11:26.837041 140399019657024 spec.py:349] Evaluating on the test split.
I0214 11:11:54.544931 140399019657024 submission_runner.py:408] Time since start: 30169.80s, 	Step: 32563, 	{'train/ctc_loss': Array(0.22219987, dtype=float32), 'train/wer': 0.08049561049781938, 'validation/ctc_loss': Array(0.46052456, dtype=float32), 'validation/wer': 0.13775258986068337, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26877353, dtype=float32), 'test/wer': 0.09111774622712408, 'test/num_examples': 2472, 'score': 27428.926631212234, 'total_duration': 30169.796103715897, 'accumulated_submission_time': 27428.926631212234, 'accumulated_eval_time': 2738.427877187729, 'accumulated_logging_time': 1.0178043842315674}
I0214 11:11:54.588653 140228361172736 logging_writer.py:48] [32563] accumulated_eval_time=2738.427877, accumulated_logging_time=1.017804, accumulated_submission_time=27428.926631, global_step=32563, preemption_count=0, score=27428.926631, test/ctc_loss=0.26877352595329285, test/num_examples=2472, test/wer=0.091118, total_duration=30169.796104, train/ctc_loss=0.22219987213611603, train/wer=0.080496, validation/ctc_loss=0.4605245590209961, validation/num_examples=5348, validation/wer=0.137753
I0214 11:12:24.049827 140228352780032 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.8546767234802246, loss=1.2419360876083374
I0214 11:13:40.791781 140228361172736 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.6442260146141052, loss=1.3235408067703247
I0214 11:14:58.370014 140228352780032 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5931894183158875, loss=1.2562910318374634
I0214 11:16:27.013317 140228361172736 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6766502857208252, loss=1.3243471384048462
I0214 11:17:53.759174 140228688852736 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7413496375083923, loss=1.232430338859558
I0214 11:19:11.488060 140228680460032 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6469911932945251, loss=1.2300450801849365
I0214 11:20:29.299991 140228688852736 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.6217503547668457, loss=1.2335056066513062
I0214 11:21:49.920278 140228680460032 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.8708772659301758, loss=1.2671918869018555
I0214 11:23:17.665343 140228688852736 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6892721652984619, loss=1.2614984512329102
I0214 11:24:47.497663 140228680460032 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6331751346588135, loss=1.248151183128357
I0214 11:26:18.532222 140228688852736 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6741199493408203, loss=1.2597804069519043
I0214 11:27:46.914408 140228680460032 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.681277871131897, loss=1.2407351732254028
I0214 11:29:17.165600 140228688852736 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7125077843666077, loss=1.2446526288986206
I0214 11:30:49.302703 140228680460032 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.621716320514679, loss=1.2961664199829102
I0214 11:32:22.175630 140228688852736 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5972015857696533, loss=1.2041690349578857
I0214 11:33:38.564339 140228680460032 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7553126811981201, loss=1.2869939804077148
I0214 11:34:55.112620 140228688852736 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.7702157497406006, loss=1.2986080646514893
I0214 11:35:54.591930 140399019657024 spec.py:321] Evaluating on the training split.
I0214 11:36:50.096959 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 11:37:43.578905 140399019657024 spec.py:349] Evaluating on the test split.
I0214 11:38:11.422826 140399019657024 submission_runner.py:408] Time since start: 31746.67s, 	Step: 34277, 	{'train/ctc_loss': Array(0.21909791, dtype=float32), 'train/wer': 0.0772190074277737, 'validation/ctc_loss': Array(0.4654405, dtype=float32), 'validation/wer': 0.13907527732990915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27387086, dtype=float32), 'test/wer': 0.091930209412386, 'test/num_examples': 2472, 'score': 28868.846267938614, 'total_duration': 31746.674035787582, 'accumulated_submission_time': 28868.846267938614, 'accumulated_eval_time': 2875.2527759075165, 'accumulated_logging_time': 1.0767803192138672}
I0214 11:38:11.467210 140228688852736 logging_writer.py:48] [34277] accumulated_eval_time=2875.252776, accumulated_logging_time=1.076780, accumulated_submission_time=28868.846268, global_step=34277, preemption_count=0, score=28868.846268, test/ctc_loss=0.2738708555698395, test/num_examples=2472, test/wer=0.091930, total_duration=31746.674036, train/ctc_loss=0.21909791231155396, train/wer=0.077219, validation/ctc_loss=0.4654405117034912, validation/num_examples=5348, validation/wer=0.139075
I0214 11:38:29.968116 140228680460032 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6559391617774963, loss=1.22111976146698
I0214 11:39:47.114036 140228688852736 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6287640333175659, loss=1.1932827234268188
I0214 11:41:03.703378 140228680460032 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.8141769766807556, loss=1.2851334810256958
I0214 11:42:31.809389 140228688852736 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6877944469451904, loss=1.240684986114502
I0214 11:44:00.497644 140228680460032 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.6582378149032593, loss=1.2807340621948242
I0214 11:45:29.980306 140228688852736 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6376792788505554, loss=1.3182257413864136
I0214 11:46:57.238504 140228680460032 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6949794888496399, loss=1.2683675289154053
I0214 11:48:29.701871 140228688852736 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7541608214378357, loss=1.2274737358093262
I0214 11:49:53.340183 140228361172736 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6905398368835449, loss=1.1851967573165894
I0214 11:51:09.933595 140228352780032 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6908760070800781, loss=1.2804511785507202
I0214 11:52:30.649542 140228361172736 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7273470759391785, loss=1.2522531747817993
I0214 11:53:51.862905 140228352780032 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6793444156646729, loss=1.254272222518921
I0214 11:55:18.317985 140228361172736 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6926461458206177, loss=1.2733774185180664
I0214 11:56:44.505549 140228352780032 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7075762748718262, loss=1.229213833808899
I0214 11:58:16.021195 140228361172736 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6147692799568176, loss=1.2916630506515503
I0214 11:59:45.947095 140228352780032 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.779046893119812, loss=1.305791974067688
I0214 12:01:15.256889 140228361172736 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7950432300567627, loss=1.2494462728500366
I0214 12:02:12.048477 140399019657024 spec.py:321] Evaluating on the training split.
I0214 12:03:08.971204 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 12:04:03.248641 140399019657024 spec.py:349] Evaluating on the test split.
I0214 12:04:30.232932 140399019657024 submission_runner.py:408] Time since start: 33325.48s, 	Step: 35964, 	{'train/ctc_loss': Array(0.17266439, dtype=float32), 'train/wer': 0.06542951733751524, 'validation/ctc_loss': Array(0.45770597, dtype=float32), 'validation/wer': 0.13686436177915948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26163143, dtype=float32), 'test/wer': 0.08766477768976093, 'test/num_examples': 2472, 'score': 30309.341188192368, 'total_duration': 33325.48345398903, 'accumulated_submission_time': 30309.341188192368, 'accumulated_eval_time': 3013.4304831027985, 'accumulated_logging_time': 1.1376783847808838}
I0214 12:04:30.274374 140227675092736 logging_writer.py:48] [35964] accumulated_eval_time=3013.430483, accumulated_logging_time=1.137678, accumulated_submission_time=30309.341188, global_step=35964, preemption_count=0, score=30309.341188, test/ctc_loss=0.26163142919540405, test/num_examples=2472, test/wer=0.087665, total_duration=33325.483454, train/ctc_loss=0.17266438901424408, train/wer=0.065430, validation/ctc_loss=0.4577059745788574, validation/num_examples=5348, validation/wer=0.136864
I0214 12:04:58.672974 140227666700032 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6539139151573181, loss=1.2059725522994995
I0214 12:06:20.049178 140226692052736 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.8531903028488159, loss=1.2498834133148193
I0214 12:07:39.488012 140226683660032 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.614976704120636, loss=1.1993484497070312
I0214 12:08:59.970996 140226692052736 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7913639545440674, loss=1.2327196598052979
I0214 12:10:25.692150 140226683660032 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7518683075904846, loss=1.2160683870315552
I0214 12:11:52.380384 140226692052736 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7201353311538696, loss=1.234550952911377
I0214 12:13:23.197853 140226683660032 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6562839150428772, loss=1.2567906379699707
I0214 12:14:54.006248 140226692052736 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.717903733253479, loss=1.2140729427337646
I0214 12:16:24.834784 140226683660032 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6551864743232727, loss=1.250103235244751
I0214 12:17:57.951064 140226692052736 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7219589352607727, loss=1.2046265602111816
I0214 12:19:29.044081 140226683660032 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.664448082447052, loss=1.2352813482284546
I0214 12:20:56.760907 140227675092736 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.7316082119941711, loss=1.2478317022323608
I0214 12:22:14.889628 140227666700032 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.8284846544265747, loss=1.2819850444793701
I0214 12:23:32.133841 140227675092736 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.8177028894424438, loss=1.2707933187484741
I0214 12:24:52.094521 140227666700032 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6866759657859802, loss=1.1948158740997314
I0214 12:26:17.784073 140227675092736 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8339003324508667, loss=1.1863566637039185
I0214 12:27:47.802593 140227666700032 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7533513307571411, loss=1.2361773252487183
I0214 12:28:30.279438 140399019657024 spec.py:321] Evaluating on the training split.
I0214 12:29:25.873575 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 12:30:19.352846 140399019657024 spec.py:349] Evaluating on the test split.
I0214 12:30:46.199952 140399019657024 submission_runner.py:408] Time since start: 34901.45s, 	Step: 37649, 	{'train/ctc_loss': Array(0.19681297, dtype=float32), 'train/wer': 0.0729393254630314, 'validation/ctc_loss': Array(0.4440631, dtype=float32), 'validation/wer': 0.134247950799888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26120028, dtype=float32), 'test/wer': 0.08866004509170679, 'test/num_examples': 2472, 'score': 31749.260707616806, 'total_duration': 34901.451577186584, 'accumulated_submission_time': 31749.260707616806, 'accumulated_eval_time': 3149.3453755378723, 'accumulated_logging_time': 1.1959314346313477}
I0214 12:30:46.243977 140228105172736 logging_writer.py:48] [37649] accumulated_eval_time=3149.345376, accumulated_logging_time=1.195931, accumulated_submission_time=31749.260708, global_step=37649, preemption_count=0, score=31749.260708, test/ctc_loss=0.26120027899742126, test/num_examples=2472, test/wer=0.088660, total_duration=34901.451577, train/ctc_loss=0.1968129724264145, train/wer=0.072939, validation/ctc_loss=0.44406309723854065, validation/num_examples=5348, validation/wer=0.134248
I0214 12:31:26.152953 140228096780032 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.7031397223472595, loss=1.2500183582305908
I0214 12:32:42.508851 140228105172736 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6146105527877808, loss=1.170785903930664
I0214 12:34:03.579946 140228096780032 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7523890137672424, loss=1.2131760120391846
I0214 12:35:32.557764 140228105172736 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0638314485549927, loss=1.311802625656128
I0214 12:37:01.929137 140228096780032 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6929735541343689, loss=1.179939866065979
I0214 12:38:22.694131 140228105172736 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.7386370897293091, loss=1.207223892211914
I0214 12:39:41.927666 140228096780032 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6551814675331116, loss=1.2178442478179932
I0214 12:41:01.701252 140228105172736 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6604993939399719, loss=1.2042014598846436
I0214 12:42:25.562832 140228096780032 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7191461324691772, loss=1.2383995056152344
I0214 12:43:53.206165 140228105172736 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7124717235565186, loss=1.2052652835845947
I0214 12:45:21.810014 140228096780032 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.5953294038772583, loss=1.171445369720459
I0214 12:46:51.527810 140228105172736 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.8855478763580322, loss=1.2014538049697876
I0214 12:48:19.526599 140228096780032 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7597166299819946, loss=1.2798622846603394
I0214 12:49:49.557589 140228105172736 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7120805978775024, loss=1.1981605291366577
I0214 12:51:19.660314 140228096780032 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6063359975814819, loss=1.2221306562423706
I0214 12:52:44.492890 140228105172736 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7251811027526855, loss=1.1773329973220825
I0214 12:54:04.191489 140228096780032 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7030600309371948, loss=1.1715922355651855
I0214 12:54:46.202390 140399019657024 spec.py:321] Evaluating on the training split.
I0214 12:55:40.940926 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 12:56:33.432668 140399019657024 spec.py:349] Evaluating on the test split.
I0214 12:57:00.025309 140399019657024 submission_runner.py:408] Time since start: 36475.28s, 	Step: 39356, 	{'train/ctc_loss': Array(0.24031529, dtype=float32), 'train/wer': 0.08714872289993111, 'validation/ctc_loss': Array(0.4375541, dtype=float32), 'validation/wer': 0.12940131496374677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2517394, dtype=float32), 'test/wer': 0.08321654175045193, 'test/num_examples': 2472, 'score': 33189.13189792633, 'total_duration': 36475.276661872864, 'accumulated_submission_time': 33189.13189792633, 'accumulated_eval_time': 3283.1624019145966, 'accumulated_logging_time': 1.2573490142822266}
I0214 12:57:00.070785 140228688852736 logging_writer.py:48] [39356] accumulated_eval_time=3283.162402, accumulated_logging_time=1.257349, accumulated_submission_time=33189.131898, global_step=39356, preemption_count=0, score=33189.131898, test/ctc_loss=0.25173941254615784, test/num_examples=2472, test/wer=0.083217, total_duration=36475.276662, train/ctc_loss=0.2403152883052826, train/wer=0.087149, validation/ctc_loss=0.43755409121513367, validation/num_examples=5348, validation/wer=0.129401
I0214 12:57:34.483625 140228680460032 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9013299345970154, loss=1.2225680351257324
I0214 12:58:51.172476 140228688852736 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7306863069534302, loss=1.254085659980774
I0214 13:00:07.644819 140228680460032 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7602339386940002, loss=1.2390393018722534
I0214 13:01:32.927665 140228688852736 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6989521384239197, loss=1.1619873046875
I0214 13:03:03.396228 140228680460032 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7581925392150879, loss=1.212797999382019
I0214 13:04:34.848750 140228688852736 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7057288289070129, loss=1.2165184020996094
I0214 13:06:05.056749 140228680460032 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9407238960266113, loss=1.216699242591858
I0214 13:07:34.195142 140228688852736 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6770495772361755, loss=1.1644086837768555
I0214 13:09:03.562000 140227818452736 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6446713805198669, loss=1.1833783388137817
I0214 13:10:21.418340 140227810060032 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7077270746231079, loss=1.169899344444275
I0214 13:11:38.805751 140227818452736 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7159931659698486, loss=1.1358299255371094
I0214 13:13:02.283652 140227810060032 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7109043002128601, loss=1.155380368232727
I0214 13:14:25.762912 140227818452736 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7098260521888733, loss=1.1324084997177124
I0214 13:15:57.343470 140227810060032 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6643987894058228, loss=1.2464100122451782
I0214 13:17:23.514970 140227818452736 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7536214590072632, loss=1.234731674194336
I0214 13:18:54.409984 140227810060032 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.6479736566543579, loss=1.2169065475463867
I0214 13:20:26.877180 140227818452736 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.6956795454025269, loss=1.2109347581863403
I0214 13:21:00.405392 140399019657024 spec.py:321] Evaluating on the training split.
I0214 13:21:54.901127 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 13:22:48.734337 140399019657024 spec.py:349] Evaluating on the test split.
I0214 13:23:15.783488 140399019657024 submission_runner.py:408] Time since start: 38051.03s, 	Step: 41039, 	{'train/ctc_loss': Array(0.24005286, dtype=float32), 'train/wer': 0.08867695143141054, 'validation/ctc_loss': Array(0.4300553, dtype=float32), 'validation/wer': 0.12838757639244233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24431132, dtype=float32), 'test/wer': 0.08384620071902991, 'test/num_examples': 2472, 'score': 34629.38103199005, 'total_duration': 38051.034391880035, 'accumulated_submission_time': 34629.38103199005, 'accumulated_eval_time': 3418.5341413021088, 'accumulated_logging_time': 1.3193349838256836}
I0214 13:23:15.830482 140227818452736 logging_writer.py:48] [41039] accumulated_eval_time=3418.534141, accumulated_logging_time=1.319335, accumulated_submission_time=34629.381032, global_step=41039, preemption_count=0, score=34629.381032, test/ctc_loss=0.24431131780147552, test/num_examples=2472, test/wer=0.083846, total_duration=38051.034392, train/ctc_loss=0.24005286395549774, train/wer=0.088677, validation/ctc_loss=0.43005529046058655, validation/num_examples=5348, validation/wer=0.128388
I0214 13:24:03.841140 140227810060032 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6734299659729004, loss=1.1922091245651245
I0214 13:25:24.439861 140228688852736 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7784520387649536, loss=1.1413347721099854
I0214 13:26:41.745781 140228680460032 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7158207893371582, loss=1.1437448263168335
I0214 13:27:58.987798 140228688852736 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7463732957839966, loss=1.1467723846435547
I0214 13:29:18.895232 140228680460032 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8305433988571167, loss=1.18854820728302
I0214 13:30:45.269275 140228688852736 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6928372979164124, loss=1.1978647708892822
I0214 13:32:11.844836 140228680460032 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7242294549942017, loss=1.1840077638626099
I0214 13:33:41.156159 140228688852736 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7349388003349304, loss=1.1668425798416138
I0214 13:35:11.093765 140228680460032 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9935857653617859, loss=1.226651906967163
I0214 13:36:41.584125 140228688852736 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7645052671432495, loss=1.1686654090881348
I0214 13:38:12.743419 140228680460032 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7922699451446533, loss=1.1899974346160889
I0214 13:39:40.719105 140228688852736 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7469579577445984, loss=1.2443662881851196
I0214 13:41:07.213093 140228688852736 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7552990913391113, loss=1.176810383796692
I0214 13:42:26.242422 140228680460032 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.6706793308258057, loss=1.175824522972107
I0214 13:43:44.190404 140228688852736 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.7398829460144043, loss=1.1759130954742432
I0214 13:45:03.534223 140228680460032 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0176931619644165, loss=1.1320068836212158
I0214 13:46:30.874810 140228688852736 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7126533389091492, loss=1.130588173866272
I0214 13:47:16.299360 140399019657024 spec.py:321] Evaluating on the training split.
I0214 13:48:09.036951 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 13:49:02.555321 140399019657024 spec.py:349] Evaluating on the test split.
I0214 13:49:29.084344 140399019657024 submission_runner.py:408] Time since start: 39624.34s, 	Step: 42750, 	{'train/ctc_loss': Array(0.28034556, dtype=float32), 'train/wer': 0.10321086159191815, 'validation/ctc_loss': Array(0.42529032, dtype=float32), 'validation/wer': 0.1256649642295104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24818589, dtype=float32), 'test/wer': 0.08305404911339954, 'test/num_examples': 2472, 'score': 36069.76509022713, 'total_duration': 39624.3362300396, 'accumulated_submission_time': 36069.76509022713, 'accumulated_eval_time': 3551.3137764930725, 'accumulated_logging_time': 1.3813042640686035}
I0214 13:49:29.131577 140228258772736 logging_writer.py:48] [42750] accumulated_eval_time=3551.313776, accumulated_logging_time=1.381304, accumulated_submission_time=36069.765090, global_step=42750, preemption_count=0, score=36069.765090, test/ctc_loss=0.2481858879327774, test/num_examples=2472, test/wer=0.083054, total_duration=39624.336230, train/ctc_loss=0.2803455591201782, train/wer=0.103211, validation/ctc_loss=0.4252903163433075, validation/num_examples=5348, validation/wer=0.125665
I0214 13:50:07.962007 140228250380032 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8633154630661011, loss=1.1226909160614014
I0214 13:51:24.406742 140228258772736 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.8365446925163269, loss=1.15966796875
I0214 13:52:43.626289 140228250380032 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6244678497314453, loss=1.2205324172973633
I0214 13:54:12.020729 140228258772736 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.8447979092597961, loss=1.1470011472702026
I0214 13:55:43.825128 140228250380032 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6593980193138123, loss=1.1921204328536987
I0214 13:57:12.296657 140228258772736 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7208229303359985, loss=1.1660430431365967
I0214 13:58:29.278863 140228250380032 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.71390300989151, loss=1.1363017559051514
I0214 13:59:46.795737 140228258772736 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6952341198921204, loss=1.2025033235549927
I0214 14:01:11.479777 140228250380032 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.9184262752532959, loss=1.1060137748718262
I0214 14:02:38.028192 140228258772736 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6692973971366882, loss=1.1626332998275757
I0214 14:04:06.135191 140228250380032 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7137855291366577, loss=1.1162854433059692
I0214 14:05:33.732473 140228258772736 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8088810443878174, loss=1.1180871725082397
I0214 14:07:03.728193 140228250380032 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7358304262161255, loss=1.1812036037445068
I0214 14:08:32.045623 140228258772736 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7448328733444214, loss=1.1426178216934204
I0214 14:10:01.686969 140228250380032 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.7924395799636841, loss=1.1689894199371338
I0214 14:11:32.813402 140228258772736 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6608670353889465, loss=1.1228094100952148
I0214 14:12:49.745593 140228250380032 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.8766199946403503, loss=1.199465274810791
I0214 14:13:29.394212 140399019657024 spec.py:321] Evaluating on the training split.
I0214 14:14:21.910009 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 14:15:15.405652 140399019657024 spec.py:349] Evaluating on the test split.
I0214 14:15:43.330083 140399019657024 submission_runner.py:408] Time since start: 41198.58s, 	Step: 44453, 	{'train/ctc_loss': Array(0.24660096, dtype=float32), 'train/wer': 0.08854924079623666, 'validation/ctc_loss': Array(0.41894877, dtype=float32), 'validation/wer': 0.12466088031126601, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23739682, dtype=float32), 'test/wer': 0.08073852903540309, 'test/num_examples': 2472, 'score': 37509.9409840107, 'total_duration': 41198.581345796585, 'accumulated_submission_time': 37509.9409840107, 'accumulated_eval_time': 3685.243673801422, 'accumulated_logging_time': 1.445460557937622}
I0214 14:15:43.378424 140228258772736 logging_writer.py:48] [44453] accumulated_eval_time=3685.243674, accumulated_logging_time=1.445461, accumulated_submission_time=37509.940984, global_step=44453, preemption_count=0, score=37509.940984, test/ctc_loss=0.23739682137966156, test/num_examples=2472, test/wer=0.080739, total_duration=41198.581346, train/ctc_loss=0.2466009557247162, train/wer=0.088549, validation/ctc_loss=0.418948769569397, validation/num_examples=5348, validation/wer=0.124661
I0214 14:16:19.941123 140228250380032 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7916885614395142, loss=1.1790581941604614
I0214 14:17:36.453538 140228258772736 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.8037691116333008, loss=1.214841604232788
I0214 14:18:52.953003 140228250380032 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.8533853888511658, loss=1.1194285154342651
I0214 14:20:13.762136 140228258772736 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.8018500208854675, loss=1.143799066543579
I0214 14:21:42.040149 140228250380032 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8719842433929443, loss=1.1213037967681885
I0214 14:23:13.948465 140228258772736 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6408230662345886, loss=1.1291484832763672
I0214 14:24:43.341807 140228250380032 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.8048709630966187, loss=1.2252519130706787
I0214 14:26:14.029190 140228258772736 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.9535926580429077, loss=1.1321455240249634
I0214 14:27:44.185745 140228250380032 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6977567672729492, loss=1.1371171474456787
I0214 14:29:06.834129 140228258772736 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7689244747161865, loss=1.1642203330993652
I0214 14:30:24.326090 140228250380032 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7979820370674133, loss=1.1350111961364746
I0214 14:31:43.572077 140228258772736 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9132418632507324, loss=1.134385347366333
I0214 14:33:05.567315 140228250380032 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6953862309455872, loss=1.0936298370361328
I0214 14:34:32.804778 140228258772736 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.8390461802482605, loss=1.1644479036331177
I0214 14:36:02.798800 140228250380032 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6528084874153137, loss=1.1568841934204102
I0214 14:37:35.588095 140228258772736 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6813790798187256, loss=1.1690293550491333
I0214 14:39:06.596131 140228250380032 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7225102186203003, loss=1.1660935878753662
I0214 14:39:43.545570 140399019657024 spec.py:321] Evaluating on the training split.
I0214 14:40:37.687389 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 14:41:29.901977 140399019657024 spec.py:349] Evaluating on the test split.
I0214 14:41:57.364425 140399019657024 submission_runner.py:408] Time since start: 42772.62s, 	Step: 46142, 	{'train/ctc_loss': Array(0.22179091, dtype=float32), 'train/wer': 0.08236001469720419, 'validation/ctc_loss': Array(0.40570727, dtype=float32), 'validation/wer': 0.1204804155362677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23045741, dtype=float32), 'test/wer': 0.07738711839619768, 'test/num_examples': 2472, 'score': 38950.01946210861, 'total_duration': 42772.615599155426, 'accumulated_submission_time': 38950.01946210861, 'accumulated_eval_time': 3819.056458711624, 'accumulated_logging_time': 1.5136487483978271}
I0214 14:41:57.411756 140228258772736 logging_writer.py:48] [46142] accumulated_eval_time=3819.056459, accumulated_logging_time=1.513649, accumulated_submission_time=38950.019462, global_step=46142, preemption_count=0, score=38950.019462, test/ctc_loss=0.23045741021633148, test/num_examples=2472, test/wer=0.077387, total_duration=42772.615599, train/ctc_loss=0.22179090976715088, train/wer=0.082360, validation/ctc_loss=0.4057072699069977, validation/num_examples=5348, validation/wer=0.120480
I0214 14:42:42.619431 140228250380032 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7184640169143677, loss=1.145747423171997
I0214 14:43:59.587098 140228258772736 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.8455840945243835, loss=1.1579595804214478
I0214 14:45:21.243278 140228258772736 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7248312830924988, loss=1.1021102666854858
I0214 14:46:37.877298 140228250380032 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8282476663589478, loss=1.1701171398162842
I0214 14:47:57.645389 140228258772736 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7207801938056946, loss=1.1365491151809692
I0214 14:49:21.703426 140228250380032 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7003448605537415, loss=1.1184515953063965
I0214 14:50:48.251115 140228258772736 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7232025265693665, loss=1.1353682279586792
I0214 14:52:18.968157 140228250380032 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7582892775535583, loss=1.1122981309890747
I0214 14:53:48.427607 140228258772736 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7912436723709106, loss=1.148392915725708
I0214 14:55:18.518462 140228250380032 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7833452820777893, loss=1.1389925479888916
I0214 14:56:48.244860 140228258772736 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.7807217836380005, loss=1.1765183210372925
I0214 14:58:19.028358 140228250380032 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7975825071334839, loss=1.110915184020996
I0214 14:59:50.633879 140228258772736 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1488450765609741, loss=1.0664793252944946
I0214 15:01:09.068443 140228250380032 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7248831987380981, loss=1.1606181859970093
I0214 15:02:28.250469 140228258772736 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.8297001719474792, loss=1.1492778062820435
I0214 15:03:49.958050 140228250380032 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9769868850708008, loss=1.1051509380340576
I0214 15:05:12.850426 140228258772736 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6758508682250977, loss=1.1369677782058716
I0214 15:05:57.462664 140399019657024 spec.py:321] Evaluating on the training split.
I0214 15:06:53.088007 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 15:07:46.136997 140399019657024 spec.py:349] Evaluating on the test split.
I0214 15:08:13.321371 140399019657024 submission_runner.py:408] Time since start: 44348.57s, 	Step: 47851, 	{'train/ctc_loss': Array(0.18740244, dtype=float32), 'train/wer': 0.0704520984458776, 'validation/ctc_loss': Array(0.4032107, dtype=float32), 'validation/wer': 0.11852052096507912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22821124, dtype=float32), 'test/wer': 0.07582312676456848, 'test/num_examples': 2472, 'score': 40389.98576760292, 'total_duration': 44348.57243299484, 'accumulated_submission_time': 40389.98576760292, 'accumulated_eval_time': 3954.9089529514313, 'accumulated_logging_time': 1.5759549140930176}
I0214 15:08:13.371299 140227378128640 logging_writer.py:48] [47851] accumulated_eval_time=3954.908953, accumulated_logging_time=1.575955, accumulated_submission_time=40389.985768, global_step=47851, preemption_count=0, score=40389.985768, test/ctc_loss=0.22821123898029327, test/num_examples=2472, test/wer=0.075823, total_duration=44348.572433, train/ctc_loss=0.18740244209766388, train/wer=0.070452, validation/ctc_loss=0.40321069955825806, validation/num_examples=5348, validation/wer=0.118521
I0214 15:08:51.888810 140227369735936 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6428344249725342, loss=1.0458593368530273
I0214 15:10:08.358258 140227378128640 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0803133249282837, loss=1.12240731716156
I0214 15:11:27.391026 140227369735936 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8673198819160461, loss=1.1860495805740356
I0214 15:12:56.081403 140227378128640 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7765364646911621, loss=1.1594312191009521
I0214 15:14:27.319338 140227369735936 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7872525453567505, loss=1.1819597482681274
I0214 15:15:57.402606 140227378128640 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.980708122253418, loss=1.1625034809112549
I0214 15:17:19.982676 140226722768640 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8829730153083801, loss=1.1428614854812622
I0214 15:18:39.321913 140226714375936 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6816091537475586, loss=1.1152817010879517
I0214 15:20:01.385391 140226722768640 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7465125322341919, loss=1.109169840812683
I0214 15:21:26.164973 140226714375936 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.0123614072799683, loss=1.130367636680603
I0214 15:22:53.918703 140226722768640 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7717174887657166, loss=1.1414830684661865
I0214 15:24:21.684105 140226714375936 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.7778691649436951, loss=1.114793300628662
I0214 15:25:53.039695 140226722768640 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.7506678700447083, loss=1.1344794034957886
I0214 15:27:21.599054 140226714375936 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.7057996988296509, loss=1.1411739587783813
I0214 15:28:51.070418 140226722768640 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.914258599281311, loss=1.1049870252609253
I0214 15:30:21.251257 140226714375936 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.9884544014930725, loss=1.172163724899292
I0214 15:31:48.400067 140227378128640 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0689516067504883, loss=1.0815136432647705
I0214 15:32:13.742335 140399019657024 spec.py:321] Evaluating on the training split.
I0214 15:33:08.890710 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 15:34:02.043153 140399019657024 spec.py:349] Evaluating on the test split.
I0214 15:34:29.492163 140399019657024 submission_runner.py:408] Time since start: 45924.74s, 	Step: 49534, 	{'train/ctc_loss': Array(0.20381515, dtype=float32), 'train/wer': 0.07671335545574522, 'validation/ctc_loss': Array(0.3923336, dtype=float32), 'validation/wer': 0.11468762370024232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21855989, dtype=float32), 'test/wer': 0.07228891190867914, 'test/num_examples': 2472, 'score': 41830.2703332901, 'total_duration': 45924.74306154251, 'accumulated_submission_time': 41830.2703332901, 'accumulated_eval_time': 4090.652411699295, 'accumulated_logging_time': 1.6439871788024902}
I0214 15:34:29.533014 140228473812736 logging_writer.py:48] [49534] accumulated_eval_time=4090.652412, accumulated_logging_time=1.643987, accumulated_submission_time=41830.270333, global_step=49534, preemption_count=0, score=41830.270333, test/ctc_loss=0.2185598909854889, test/num_examples=2472, test/wer=0.072289, total_duration=45924.743062, train/ctc_loss=0.20381514728069305, train/wer=0.076713, validation/ctc_loss=0.39233359694480896, validation/num_examples=5348, validation/wer=0.114688
I0214 15:35:21.261211 140228465420032 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.7688824534416199, loss=1.0952221155166626
I0214 15:36:38.004918 140228473812736 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7706218957901001, loss=1.1417465209960938
I0214 15:37:54.534955 140228465420032 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1337559223175049, loss=1.1161625385284424
I0214 15:39:12.734567 140228473812736 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7472556829452515, loss=1.1615933179855347
I0214 15:40:43.357131 140228465420032 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.911870002746582, loss=1.0616984367370605
I0214 15:42:12.022345 140228473812736 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9531521797180176, loss=1.0911998748779297
I0214 15:43:42.334649 140228465420032 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7224175930023193, loss=1.0777047872543335
I0214 15:45:12.255184 140228473812736 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7431973218917847, loss=1.1289256811141968
I0214 15:46:44.374602 140228465420032 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.798953115940094, loss=1.1074482202529907
I0214 15:48:14.137965 140227818452736 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7625212669372559, loss=1.0747013092041016
I0214 15:49:31.244813 140227810060032 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.0430225133895874, loss=1.1275759935379028
I0214 15:50:48.217986 140227818452736 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8283535838127136, loss=1.0763449668884277
I0214 15:52:09.501993 140227810060032 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.728550910949707, loss=1.0915628671646118
I0214 15:53:33.736462 140227818452736 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.0754352807998657, loss=1.092347264289856
I0214 15:55:03.541023 140227810060032 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7798120379447937, loss=1.1027384996414185
I0214 15:56:33.465274 140227818452736 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7431000471115112, loss=1.0847067832946777
I0214 15:58:03.981925 140227810060032 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8881555795669556, loss=1.126007318496704
I0214 15:58:29.769798 140399019657024 spec.py:321] Evaluating on the training split.
I0214 15:59:25.037722 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 16:00:18.424832 140399019657024 spec.py:349] Evaluating on the test split.
I0214 16:00:45.014728 140399019657024 submission_runner.py:408] Time since start: 47500.27s, 	Step: 51230, 	{'train/ctc_loss': Array(0.17861539, dtype=float32), 'train/wer': 0.06775038653152379, 'validation/ctc_loss': Array(0.3843447, dtype=float32), 'validation/wer': 0.11259256398621316, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21695636, dtype=float32), 'test/wer': 0.0702374423658928, 'test/num_examples': 2472, 'score': 43270.42231464386, 'total_duration': 47500.26622223854, 'accumulated_submission_time': 43270.42231464386, 'accumulated_eval_time': 4225.891577243805, 'accumulated_logging_time': 1.700354814529419}
I0214 16:00:45.061162 140227818452736 logging_writer.py:48] [51230] accumulated_eval_time=4225.891577, accumulated_logging_time=1.700355, accumulated_submission_time=43270.422315, global_step=51230, preemption_count=0, score=43270.422315, test/ctc_loss=0.21695636212825775, test/num_examples=2472, test/wer=0.070237, total_duration=47500.266222, train/ctc_loss=0.17861539125442505, train/wer=0.067750, validation/ctc_loss=0.3843446969985962, validation/num_examples=5348, validation/wer=0.112593
I0214 16:01:39.625021 140227810060032 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7191764116287231, loss=1.1492396593093872
I0214 16:02:56.111253 140227818452736 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.760796070098877, loss=1.1579746007919312
I0214 16:04:20.761495 140228473812736 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.9092469811439514, loss=1.0558758974075317
I0214 16:05:39.871111 140228465420032 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.1588367223739624, loss=1.071479320526123
I0214 16:06:57.382750 140228473812736 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1888409852981567, loss=1.0516595840454102
I0214 16:08:16.653522 140228465420032 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.2503405809402466, loss=1.0976282358169556
I0214 16:09:42.984051 140228473812736 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9223368763923645, loss=1.0472381114959717
I0214 16:11:12.519643 140228465420032 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8456559777259827, loss=1.1466397047042847
I0214 16:12:41.024826 140228473812736 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8293501734733582, loss=1.0250247716903687
I0214 16:14:08.962814 140228465420032 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.826154887676239, loss=1.0796436071395874
I0214 16:15:38.101002 140228473812736 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.1047744750976562, loss=1.1129939556121826
I0214 16:17:06.507147 140228465420032 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.0822947025299072, loss=1.1000372171401978
I0214 16:18:36.298276 140228473812736 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7724050879478455, loss=1.0585049390792847
I0214 16:19:58.523220 140228473812736 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8998005390167236, loss=1.0619038343429565
I0214 16:21:16.365072 140228465420032 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.7909697890281677, loss=1.1062909364700317
I0214 16:22:36.095499 140228473812736 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.8317158222198486, loss=1.0703917741775513
I0214 16:23:58.290932 140228465420032 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.9798999428749084, loss=1.0540674924850464
I0214 16:24:45.103668 140399019657024 spec.py:321] Evaluating on the training split.
I0214 16:25:39.200232 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 16:26:32.803626 140399019657024 spec.py:349] Evaluating on the test split.
I0214 16:26:59.921871 140399019657024 submission_runner.py:408] Time since start: 49075.17s, 	Step: 52957, 	{'train/ctc_loss': Array(0.17695369, dtype=float32), 'train/wer': 0.0670163486260428, 'validation/ctc_loss': Array(0.372745, dtype=float32), 'validation/wer': 0.10919412610907828, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21086185, dtype=float32), 'test/wer': 0.07096865923262852, 'test/num_examples': 2472, 'score': 44710.28479671478, 'total_duration': 49075.17360305786, 'accumulated_submission_time': 44710.28479671478, 'accumulated_eval_time': 4360.704236030579, 'accumulated_logging_time': 1.8572721481323242}
I0214 16:26:59.961933 140228473812736 logging_writer.py:48] [52957] accumulated_eval_time=4360.704236, accumulated_logging_time=1.857272, accumulated_submission_time=44710.284797, global_step=52957, preemption_count=0, score=44710.284797, test/ctc_loss=0.21086184680461884, test/num_examples=2472, test/wer=0.070969, total_duration=49075.173603, train/ctc_loss=0.17695368826389313, train/wer=0.067016, validation/ctc_loss=0.37274500727653503, validation/num_examples=5348, validation/wer=0.109194
I0214 16:27:33.680602 140228465420032 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.8159696459770203, loss=1.076596975326538
I0214 16:28:50.739687 140228473812736 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9540120959281921, loss=1.0756611824035645
I0214 16:30:07.434908 140228465420032 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.443254828453064, loss=1.1215119361877441
I0214 16:31:35.675796 140228473812736 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8002079129219055, loss=1.0670080184936523
I0214 16:33:05.220093 140228465420032 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.9739716649055481, loss=1.084688663482666
I0214 16:34:35.438650 140228473812736 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7844946384429932, loss=1.0311509370803833
I0214 16:36:02.074540 140227163092736 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8903182744979858, loss=1.0391088724136353
I0214 16:37:19.146275 140227154700032 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1101714372634888, loss=1.0362831354141235
I0214 16:38:35.866769 140227163092736 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9102076292037964, loss=1.0966475009918213
I0214 16:39:58.320284 140227154700032 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8885846734046936, loss=1.0811078548431396
I0214 16:41:23.053410 140227163092736 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8369734287261963, loss=1.0589619874954224
I0214 16:42:52.058871 140227154700032 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.8206722736358643, loss=1.0427029132843018
I0214 16:44:22.616356 140227163092736 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.8355205655097961, loss=1.0619683265686035
I0214 16:45:50.409061 140227154700032 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9241310954093933, loss=1.0553475618362427
I0214 16:47:20.320832 140227163092736 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8205738067626953, loss=1.081674575805664
I0214 16:48:48.790306 140227154700032 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7947590947151184, loss=1.0435206890106201
I0214 16:50:21.211283 140228473812736 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7992098331451416, loss=1.0003199577331543
I0214 16:51:00.140665 140399019657024 spec.py:321] Evaluating on the training split.
I0214 16:51:54.848002 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 16:52:48.395264 140399019657024 spec.py:349] Evaluating on the test split.
I0214 16:53:16.263222 140399019657024 submission_runner.py:408] Time since start: 50651.51s, 	Step: 54652, 	{'train/ctc_loss': Array(0.16497089, dtype=float32), 'train/wer': 0.06185231193926846, 'validation/ctc_loss': Array(0.36614746, dtype=float32), 'validation/wer': 0.1077169641908918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19986272, dtype=float32), 'test/wer': 0.06739382121747609, 'test/num_examples': 2472, 'score': 46150.37541222572, 'total_duration': 50651.51419734955, 'accumulated_submission_time': 46150.37541222572, 'accumulated_eval_time': 4496.820524454117, 'accumulated_logging_time': 1.9165270328521729}
I0214 16:53:16.308348 140228473812736 logging_writer.py:48] [54652] accumulated_eval_time=4496.820524, accumulated_logging_time=1.916527, accumulated_submission_time=46150.375412, global_step=54652, preemption_count=0, score=46150.375412, test/ctc_loss=0.19986271858215332, test/num_examples=2472, test/wer=0.067394, total_duration=50651.514197, train/ctc_loss=0.16497088968753815, train/wer=0.061852, validation/ctc_loss=0.3661474585533142, validation/num_examples=5348, validation/wer=0.107717
I0214 16:53:53.687458 140228465420032 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2829906940460205, loss=1.051734447479248
I0214 16:55:11.018930 140228473812736 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9208270907402039, loss=1.062479853630066
I0214 16:56:28.122319 140228465420032 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.9145169258117676, loss=1.0684385299682617
I0214 16:57:44.979680 140228473812736 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.8673646450042725, loss=1.0155278444290161
I0214 16:59:06.588219 140228465420032 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.9574268460273743, loss=1.0380589962005615
I0214 17:00:36.355306 140228473812736 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8972716927528381, loss=1.0655937194824219
I0214 17:02:04.726037 140228465420032 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.9157742857933044, loss=1.061769723892212
I0214 17:03:34.065488 140228473812736 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.9646701812744141, loss=1.0254467725753784
I0214 17:05:01.501281 140228465420032 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7882925271987915, loss=1.0545192956924438
I0214 17:06:33.201563 140228473812736 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8137431740760803, loss=1.0090343952178955
I0214 17:07:55.631928 140228473812736 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.8581885099411011, loss=1.001617431640625
I0214 17:09:12.994289 140228465420032 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.8917946815490723, loss=1.0162400007247925
I0214 17:10:32.353308 140228473812736 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8282839059829712, loss=1.0338116884231567
I0214 17:11:54.958913 140228465420032 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9025225639343262, loss=1.08240807056427
I0214 17:13:23.256132 140228473812736 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.3267953395843506, loss=1.0421996116638184
I0214 17:14:50.619324 140228465420032 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0615904331207275, loss=1.0663281679153442
I0214 17:16:19.592538 140228473812736 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.855745792388916, loss=1.0673565864562988
I0214 17:17:16.556599 140399019657024 spec.py:321] Evaluating on the training split.
I0214 17:18:10.066420 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 17:19:04.004314 140399019657024 spec.py:349] Evaluating on the test split.
I0214 17:19:31.497404 140399019657024 submission_runner.py:408] Time since start: 52226.75s, 	Step: 56368, 	{'train/ctc_loss': Array(0.16453665, dtype=float32), 'train/wer': 0.06207433352792372, 'validation/ctc_loss': Array(0.3558999, dtype=float32), 'validation/wer': 0.10438610888517721, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1941069, dtype=float32), 'test/wer': 0.06440801901163853, 'test/num_examples': 2472, 'score': 47590.537660598755, 'total_duration': 52226.748883485794, 'accumulated_submission_time': 47590.537660598755, 'accumulated_eval_time': 4631.7555339336395, 'accumulated_logging_time': 1.9792718887329102}
I0214 17:19:31.542824 140228473812736 logging_writer.py:48] [56368] accumulated_eval_time=4631.755534, accumulated_logging_time=1.979272, accumulated_submission_time=47590.537661, global_step=56368, preemption_count=0, score=47590.537661, test/ctc_loss=0.19410690665245056, test/num_examples=2472, test/wer=0.064408, total_duration=52226.748883, train/ctc_loss=0.16453665494918823, train/wer=0.062074, validation/ctc_loss=0.3558999001979828, validation/num_examples=5348, validation/wer=0.104386
I0214 17:19:56.765495 140228465420032 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.7982880473136902, loss=0.9718461036682129
I0214 17:21:13.259954 140228473812736 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.892829954624176, loss=1.0233877897262573
I0214 17:22:30.000859 140228465420032 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.929878830909729, loss=1.0121346712112427
I0214 17:23:53.660844 140228473812736 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.8123838901519775, loss=0.9740809202194214
I0214 17:25:10.158479 140228465420032 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.8237441778182983, loss=1.0300121307373047
I0214 17:26:28.426750 140228473812736 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.9523563981056213, loss=1.0063674449920654
I0214 17:27:51.090127 140228465420032 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.0906367301940918, loss=1.0397441387176514
I0214 17:29:17.964146 140228473812736 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.9055049419403076, loss=1.0056157112121582
I0214 17:30:48.046062 140228465420032 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.8816274404525757, loss=1.029367446899414
I0214 17:32:18.908766 140228473812736 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.8484403491020203, loss=1.0553455352783203
I0214 17:33:48.049677 140228465420032 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.9144842028617859, loss=1.0439743995666504
I0214 17:35:18.791053 140228473812736 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0140914916992188, loss=1.0006171464920044
I0214 17:36:47.809068 140228465420032 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.8571702241897583, loss=0.9824726581573486
I0214 17:38:17.428625 140228473812736 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8273900151252747, loss=0.9817742705345154
I0214 17:39:35.600167 140228465420032 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.051692008972168, loss=0.9776865243911743
I0214 17:40:54.121015 140228473812736 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0379449129104614, loss=0.9985232949256897
I0214 17:42:15.031003 140228465420032 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7695802450180054, loss=0.9913784861564636
I0214 17:43:31.751748 140399019657024 spec.py:321] Evaluating on the training split.
I0214 17:44:27.199429 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 17:45:20.004650 140399019657024 spec.py:349] Evaluating on the test split.
I0214 17:45:47.432625 140399019657024 submission_runner.py:408] Time since start: 53802.68s, 	Step: 58095, 	{'train/ctc_loss': Array(0.16163893, dtype=float32), 'train/wer': 0.0600747122055348, 'validation/ctc_loss': Array(0.34983188, dtype=float32), 'validation/wer': 0.10201106423240681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19088641, dtype=float32), 'test/wer': 0.061442528385432536, 'test/num_examples': 2472, 'score': 49030.65796470642, 'total_duration': 53802.684811115265, 'accumulated_submission_time': 49030.65796470642, 'accumulated_eval_time': 4767.43133020401, 'accumulated_logging_time': 2.0423855781555176}
I0214 17:45:47.476538 140228473812736 logging_writer.py:48] [58095] accumulated_eval_time=4767.431330, accumulated_logging_time=2.042386, accumulated_submission_time=49030.657965, global_step=58095, preemption_count=0, score=49030.657965, test/ctc_loss=0.19088640809059143, test/num_examples=2472, test/wer=0.061443, total_duration=53802.684811, train/ctc_loss=0.16163893043994904, train/wer=0.060075, validation/ctc_loss=0.34983187913894653, validation/num_examples=5348, validation/wer=0.102011
I0214 17:45:52.162942 140228465420032 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.9799785017967224, loss=0.992915689945221
I0214 17:47:08.515975 140228473812736 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9179576635360718, loss=1.0265532732009888
I0214 17:48:25.676759 140228465420032 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9572349786758423, loss=1.0135585069656372
I0214 17:49:50.354462 140228473812736 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9432465434074402, loss=0.9751253128051758
I0214 17:51:17.558400 140228465420032 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8645517826080322, loss=1.037747859954834
I0214 17:52:47.633813 140228473812736 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.9343125224113464, loss=0.9940761923789978
I0214 17:54:19.638853 140228465420032 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.886579155921936, loss=0.990380048751831
I0214 17:55:39.885163 140228473812736 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9933528304100037, loss=1.0852742195129395
I0214 17:56:57.158364 140228465420032 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8105493783950806, loss=0.965888500213623
I0214 17:58:14.164577 140228473812736 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9141493439674377, loss=1.0605640411376953
I0214 17:59:37.704520 140228465420032 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.8996173739433289, loss=0.9945195317268372
I0214 18:01:04.041923 140228473812736 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.8669580817222595, loss=0.9580242037773132
I0214 18:02:34.256924 140228465420032 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.9170185923576355, loss=0.9765764474868774
I0214 18:04:04.398870 140228473812736 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.0395842790603638, loss=1.000961184501648
I0214 18:05:34.545948 140228465420032 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.8300908207893372, loss=0.9726071953773499
I0214 18:07:04.191066 140228473812736 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0071760416030884, loss=0.9972886443138123
I0214 18:08:32.531864 140228465420032 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.0539072751998901, loss=1.0127639770507812
I0214 18:09:47.873111 140399019657024 spec.py:321] Evaluating on the training split.
I0214 18:10:42.375333 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 18:11:34.563663 140399019657024 spec.py:349] Evaluating on the test split.
I0214 18:12:01.270625 140399019657024 submission_runner.py:408] Time since start: 55376.52s, 	Step: 59789, 	{'train/ctc_loss': Array(0.13394181, dtype=float32), 'train/wer': 0.05147734660479297, 'validation/ctc_loss': Array(0.34509662, dtype=float32), 'validation/wer': 0.10036011855913958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1848671, dtype=float32), 'test/wer': 0.0623159263095891, 'test/num_examples': 2472, 'score': 50470.96928143501, 'total_duration': 55376.52167439461, 'accumulated_submission_time': 50470.96928143501, 'accumulated_eval_time': 4900.822810411453, 'accumulated_logging_time': 2.103106737136841}
I0214 18:12:01.317798 140228258772736 logging_writer.py:48] [59789] accumulated_eval_time=4900.822810, accumulated_logging_time=2.103107, accumulated_submission_time=50470.969281, global_step=59789, preemption_count=0, score=50470.969281, test/ctc_loss=0.18486709892749786, test/num_examples=2472, test/wer=0.062316, total_duration=55376.521674, train/ctc_loss=0.13394181430339813, train/wer=0.051477, validation/ctc_loss=0.345096617937088, validation/num_examples=5348, validation/wer=0.100360
I0214 18:12:10.528934 140228250380032 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9130488038063049, loss=0.9585834741592407
I0214 18:13:27.031079 140228258772736 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.021126389503479, loss=1.0009160041809082
I0214 18:14:43.604542 140228250380032 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8479280471801758, loss=0.9679062366485596
I0214 18:16:00.045438 140228258772736 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9907639026641846, loss=0.9756782650947571
I0214 18:17:16.977563 140228250380032 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1596065759658813, loss=0.9911229014396667
I0214 18:18:48.376165 140228258772736 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0632497072219849, loss=0.979867696762085
I0214 18:20:20.251075 140228250380032 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.9089629054069519, loss=0.9741639494895935
I0214 18:21:47.775531 140228258772736 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9384458661079407, loss=1.0414459705352783
I0214 18:23:16.400297 140228250380032 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.8840063810348511, loss=0.984368085861206
I0214 18:24:48.771716 140228258772736 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.3340778350830078, loss=0.986942708492279
I0214 18:26:16.629358 140228258772736 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.3106201887130737, loss=1.0007858276367188
I0214 18:27:35.008560 140228250380032 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0562254190444946, loss=0.9736559391021729
I0214 18:28:55.002917 140228258772736 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.1000944375991821, loss=1.0229250192642212
I0214 18:30:19.008550 140228250380032 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0102486610412598, loss=0.9521085023880005
I0214 18:31:45.085850 140228258772736 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.0134711265563965, loss=0.9736145734786987
I0214 18:33:14.573333 140228250380032 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.0573554039001465, loss=0.9708279967308044
I0214 18:34:44.161086 140228258772736 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.9736369848251343, loss=1.0178519487380981
I0214 18:36:01.695844 140399019657024 spec.py:321] Evaluating on the training split.
I0214 18:36:55.191239 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 18:37:48.948864 140399019657024 spec.py:349] Evaluating on the test split.
I0214 18:38:16.699954 140399019657024 submission_runner.py:408] Time since start: 56951.95s, 	Step: 61491, 	{'train/ctc_loss': Array(0.13361306, dtype=float32), 'train/wer': 0.05178905572543471, 'validation/ctc_loss': Array(0.3403858, dtype=float32), 'validation/wer': 0.09937534394701526, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1836257, dtype=float32), 'test/wer': 0.06146283996506408, 'test/num_examples': 2472, 'score': 51911.2644867897, 'total_duration': 56951.9517929554, 'accumulated_submission_time': 51911.2644867897, 'accumulated_eval_time': 5035.821505784988, 'accumulated_logging_time': 2.164682149887085}
I0214 18:38:16.749073 140227966932736 logging_writer.py:48] [61491] accumulated_eval_time=5035.821506, accumulated_logging_time=2.164682, accumulated_submission_time=51911.264487, global_step=61491, preemption_count=0, score=51911.264487, test/ctc_loss=0.1836256980895996, test/num_examples=2472, test/wer=0.061463, total_duration=56951.951793, train/ctc_loss=0.13361306488513947, train/wer=0.051789, validation/ctc_loss=0.3403857946395874, validation/num_examples=5348, validation/wer=0.099375
I0214 18:38:24.514724 140227958540032 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9260745644569397, loss=0.9865331053733826
I0214 18:39:40.849685 140227966932736 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.921570897102356, loss=0.9470296502113342
I0214 18:40:57.389462 140227958540032 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9819923043251038, loss=0.9801139235496521
I0214 18:42:28.584324 140227311572736 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.0506517887115479, loss=0.9902102947235107
I0214 18:43:46.226859 140227303180032 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0514848232269287, loss=0.9224026799201965
I0214 18:45:05.478748 140227311572736 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.8736193180084229, loss=0.9705197215080261
I0214 18:46:27.935935 140227303180032 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.1516807079315186, loss=0.9778087735176086
I0214 18:47:49.463047 140227311572736 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0226807594299316, loss=0.9837820529937744
I0214 18:49:16.396694 140227303180032 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0467606782913208, loss=0.9791935682296753
I0214 18:50:45.940418 140227311572736 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.001319408416748, loss=1.0108016729354858
I0214 18:52:12.785080 140227303180032 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.077891230583191, loss=1.0098083019256592
I0214 18:53:42.783855 140227311572736 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0925477743148804, loss=1.016200065612793
I0214 18:55:13.988947 140227303180032 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0309672355651855, loss=0.9506354928016663
I0214 18:56:44.819415 140227311572736 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.4060622453689575, loss=0.9868763089179993
I0214 18:58:09.881383 140226294445824 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.3446005582809448, loss=0.9733946919441223
I0214 18:59:28.513389 140226286053120 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0111663341522217, loss=0.9295946359634399
I0214 19:00:48.150861 140226294445824 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.011589527130127, loss=0.938075065612793
I0214 19:02:10.723777 140226286053120 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.9590675234794617, loss=0.9324225783348083
I0214 19:02:17.407631 140399019657024 spec.py:321] Evaluating on the training split.
I0214 19:03:10.922744 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 19:04:03.048112 140399019657024 spec.py:349] Evaluating on the test split.
I0214 19:04:30.279683 140399019657024 submission_runner.py:408] Time since start: 58525.53s, 	Step: 63210, 	{'train/ctc_loss': Array(0.11915162, dtype=float32), 'train/wer': 0.04634981922064009, 'validation/ctc_loss': Array(0.33299387, dtype=float32), 'validation/wer': 0.09586105023315987, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17569458, dtype=float32), 'test/wer': 0.057827067211017, 'test/num_examples': 2472, 'score': 53351.83570098877, 'total_duration': 58525.53119134903, 'accumulated_submission_time': 53351.83570098877, 'accumulated_eval_time': 5168.687787055969, 'accumulated_logging_time': 2.2308127880096436}
I0214 19:04:30.324288 140228688852736 logging_writer.py:48] [63210] accumulated_eval_time=5168.687787, accumulated_logging_time=2.230813, accumulated_submission_time=53351.835701, global_step=63210, preemption_count=0, score=53351.835701, test/ctc_loss=0.17569458484649658, test/num_examples=2472, test/wer=0.057827, total_duration=58525.531191, train/ctc_loss=0.11915162205696106, train/wer=0.046350, validation/ctc_loss=0.33299386501312256, validation/num_examples=5348, validation/wer=0.095861
I0214 19:05:39.807531 140228680460032 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.9430215358734131, loss=0.9282944798469543
I0214 19:06:56.271774 140228688852736 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1725620031356812, loss=0.9896500706672668
I0214 19:08:22.008982 140228680460032 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.08451247215271, loss=0.914920449256897
I0214 19:09:50.772155 140228688852736 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8943745493888855, loss=0.9610358476638794
I0214 19:11:22.743437 140228680460032 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0521973371505737, loss=0.9349738955497742
I0214 19:12:54.429366 140228688852736 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0387356281280518, loss=0.9609672427177429
I0214 19:14:24.800820 140228361172736 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.9237136244773865, loss=0.9268755316734314
I0214 19:15:41.719598 140228352780032 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.9394518136978149, loss=0.9081584811210632
I0214 19:16:58.883587 140228361172736 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.9846113920211792, loss=0.927444338798523
I0214 19:18:18.877645 140228352780032 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.348291039466858, loss=0.9349809288978577
I0214 19:19:44.409679 140228361172736 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1258955001831055, loss=0.9124057292938232
I0214 19:21:14.201036 140228352780032 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.082822561264038, loss=0.9105319976806641
I0214 19:22:45.672422 140228361172736 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.306753396987915, loss=0.9391199350357056
I0214 19:24:16.089014 140228352780032 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1019665002822876, loss=0.9044175744056702
I0214 19:25:47.608403 140228361172736 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.4876844882965088, loss=0.9636583924293518
I0214 19:27:18.835044 140228352780032 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.287652850151062, loss=0.9435217380523682
I0214 19:28:31.255872 140399019657024 spec.py:321] Evaluating on the training split.
I0214 19:29:23.898366 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 19:30:17.811612 140399019657024 spec.py:349] Evaluating on the test split.
I0214 19:30:44.667307 140399019657024 submission_runner.py:408] Time since start: 60099.92s, 	Step: 64881, 	{'train/ctc_loss': Array(0.11503623, dtype=float32), 'train/wer': 0.04372654964500646, 'validation/ctc_loss': Array(0.32092267, dtype=float32), 'validation/wer': 0.09318671133552815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17374022, dtype=float32), 'test/wer': 0.056913046127597344, 'test/num_examples': 2472, 'score': 54792.682903051376, 'total_duration': 60099.91721081734, 'accumulated_submission_time': 54792.682903051376, 'accumulated_eval_time': 5302.091877937317, 'accumulated_logging_time': 2.2912206649780273}
I0214 19:30:44.718442 140228146132736 logging_writer.py:48] [64881] accumulated_eval_time=5302.091878, accumulated_logging_time=2.291221, accumulated_submission_time=54792.682903, global_step=64881, preemption_count=0, score=54792.682903, test/ctc_loss=0.1737402230501175, test/num_examples=2472, test/wer=0.056913, total_duration=60099.917211, train/ctc_loss=0.11503623425960541, train/wer=0.043727, validation/ctc_loss=0.3209226727485657, validation/num_examples=5348, validation/wer=0.093187
I0214 19:31:04.132533 140228688852736 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.9691528081893921, loss=0.950137734413147
I0214 19:32:20.751796 140228680460032 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.0874131917953491, loss=0.9419263005256653
I0214 19:33:40.275977 140228688852736 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.2190855741500854, loss=1.0076913833618164
I0214 19:35:01.761089 140228680460032 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1344081163406372, loss=0.913268506526947
I0214 19:36:25.632548 140228688852736 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.9725805521011353, loss=0.9069473147392273
I0214 19:37:53.189432 140228680460032 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.9177594184875488, loss=0.9307100176811218
I0214 19:39:24.484890 140228688852736 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0957309007644653, loss=0.9202322363853455
I0214 19:40:55.033186 140228680460032 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2318530082702637, loss=0.9410938024520874
I0214 19:42:25.479636 140228688852736 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.1366808414459229, loss=0.9317708015441895
I0214 19:43:53.419046 140228680460032 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.3327594995498657, loss=0.9605321288108826
I0214 19:45:24.503387 140228688852736 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0809623003005981, loss=0.9181505441665649
I0214 19:46:48.643111 140227163092736 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.8195245265960693, loss=0.9232751131057739
I0214 19:48:05.917789 140227154700032 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.2648131847381592, loss=0.9660908579826355
I0214 19:49:24.848097 140227163092736 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.028989315032959, loss=0.8734099864959717
I0214 19:50:46.731332 140227154700032 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3642462491989136, loss=0.942479133605957
I0214 19:52:12.961632 140227163092736 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.134552240371704, loss=0.9218604564666748
I0214 19:53:44.673576 140227154700032 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2124418020248413, loss=1.0007469654083252
I0214 19:54:45.119299 140399019657024 spec.py:321] Evaluating on the training split.
I0214 19:55:39.062225 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 19:56:32.783591 140399019657024 spec.py:349] Evaluating on the test split.
I0214 19:57:00.031452 140399019657024 submission_runner.py:408] Time since start: 61675.28s, 	Step: 66568, 	{'train/ctc_loss': Array(0.10550904, dtype=float32), 'train/wer': 0.0410768989959974, 'validation/ctc_loss': Array(0.3176728, dtype=float32), 'validation/wer': 0.09162265753980131, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17099647, dtype=float32), 'test/wer': 0.05496313448296874, 'test/num_examples': 2472, 'score': 56232.994379758835, 'total_duration': 61675.28289914131, 'accumulated_submission_time': 56232.994379758835, 'accumulated_eval_time': 5436.998205900192, 'accumulated_logging_time': 2.3619296550750732}
I0214 19:57:00.076121 140227163092736 logging_writer.py:48] [66568] accumulated_eval_time=5436.998206, accumulated_logging_time=2.361930, accumulated_submission_time=56232.994380, global_step=66568, preemption_count=0, score=56232.994380, test/ctc_loss=0.17099647223949432, test/num_examples=2472, test/wer=0.054963, total_duration=61675.282899, train/ctc_loss=0.10550904273986816, train/wer=0.041077, validation/ctc_loss=0.3176727890968323, validation/num_examples=5348, validation/wer=0.091623
I0214 19:57:25.389330 140227154700032 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.5585466623306274, loss=0.9548549056053162
I0214 19:58:41.889928 140227163092736 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.120110034942627, loss=0.9240759611129761
I0214 19:59:59.270471 140227154700032 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.049182415008545, loss=0.9246103167533875
I0214 20:01:28.254019 140227163092736 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.9531638026237488, loss=0.880943238735199
I0214 20:02:56.050456 140228688852736 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.0891932249069214, loss=0.9139672517776489
I0214 20:04:13.548359 140228680460032 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.357375979423523, loss=0.9218199849128723
I0214 20:05:30.629441 140228688852736 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.09451425075531, loss=0.9255275130271912
I0214 20:06:53.518635 140228680460032 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.2806607484817505, loss=0.9384945631027222
I0214 20:08:20.261838 140228688852736 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.28980553150177, loss=0.922760546207428
I0214 20:09:49.493686 140228680460032 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0845365524291992, loss=0.9390806555747986
I0214 20:11:15.253271 140228688852736 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.26773202419281, loss=0.9077492356300354
I0214 20:12:47.168298 140228680460032 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0131051540374756, loss=0.9179246425628662
I0214 20:14:17.909950 140228688852736 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1165053844451904, loss=0.9156618714332581
I0214 20:15:49.258421 140228680460032 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2005892992019653, loss=0.923764169216156
I0214 20:17:21.361346 140227163092736 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.4642961025238037, loss=0.9057742953300476
I0214 20:18:40.624612 140227154700032 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.190014362335205, loss=0.9380937218666077
I0214 20:19:59.179343 140227163092736 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1778075695037842, loss=0.9000463485717773
I0214 20:21:00.597533 140399019657024 spec.py:321] Evaluating on the training split.
I0214 20:21:55.432488 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 20:22:48.665923 140399019657024 spec.py:349] Evaluating on the test split.
I0214 20:23:16.224464 140399019657024 submission_runner.py:408] Time since start: 63251.48s, 	Step: 68277, 	{'train/ctc_loss': Array(0.09899141, dtype=float32), 'train/wer': 0.03862501798700508, 'validation/ctc_loss': Array(0.31188878, dtype=float32), 'validation/wer': 0.08942139664211167, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16618471, dtype=float32), 'test/wer': 0.053338208112444906, 'test/num_examples': 2472, 'score': 57673.43183207512, 'total_duration': 63251.47676539421, 'accumulated_submission_time': 57673.43183207512, 'accumulated_eval_time': 5572.620210170746, 'accumulated_logging_time': 2.421889066696167}
I0214 20:23:16.266504 140226330285824 logging_writer.py:48] [68277] accumulated_eval_time=5572.620210, accumulated_logging_time=2.421889, accumulated_submission_time=57673.431832, global_step=68277, preemption_count=0, score=57673.431832, test/ctc_loss=0.1661847084760666, test/num_examples=2472, test/wer=0.053338, total_duration=63251.476765, train/ctc_loss=0.09899140894412994, train/wer=0.038625, validation/ctc_loss=0.31188878417015076, validation/num_examples=5348, validation/wer=0.089421
I0214 20:23:34.904572 140226321893120 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.959237813949585, loss=0.9093014001846313
I0214 20:24:52.440927 140226330285824 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2433885335922241, loss=0.9115265607833862
I0214 20:26:09.152990 140226321893120 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.129738211631775, loss=0.886573851108551
I0214 20:27:33.520861 140226330285824 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0998448133468628, loss=0.8706010580062866
I0214 20:28:59.672046 140226321893120 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2315176725387573, loss=0.9234848618507385
I0214 20:30:27.815884 140226330285824 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.0556763410568237, loss=0.889466404914856
I0214 20:31:58.075402 140226321893120 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.286298394203186, loss=0.9656636118888855
I0214 20:33:28.154326 140226330285824 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.2087451219558716, loss=0.9011779427528381
I0214 20:34:49.589893 140227818452736 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0890482664108276, loss=0.9020521640777588
I0214 20:36:06.056470 140227810060032 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.1607637405395508, loss=0.9115152359008789
I0214 20:37:24.521961 140227818452736 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.4342647790908813, loss=0.8463960886001587
I0214 20:38:47.794365 140227810060032 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.4154956340789795, loss=0.9451528787612915
I0214 20:40:16.476842 140227818452736 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1650267839431763, loss=0.916193962097168
I0214 20:41:48.870166 140227810060032 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1172579526901245, loss=0.8848072290420532
I0214 20:43:17.586287 140227818452736 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.1764650344848633, loss=0.8307718634605408
I0214 20:44:48.076228 140227810060032 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.0964395999908447, loss=0.850146472454071
I0214 20:46:16.296476 140227818452736 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.4184627532958984, loss=0.8845568895339966
I0214 20:47:16.331236 140399019657024 spec.py:321] Evaluating on the training split.
I0214 20:48:09.791753 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 20:49:03.854994 140399019657024 spec.py:349] Evaluating on the test split.
I0214 20:49:30.339271 140399019657024 submission_runner.py:408] Time since start: 64825.59s, 	Step: 69968, 	{'train/ctc_loss': Array(0.07961264, dtype=float32), 'train/wer': 0.030598225845663272, 'validation/ctc_loss': Array(0.3068214, dtype=float32), 'validation/wer': 0.08783803354026473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16443966, dtype=float32), 'test/wer': 0.052282005971604414, 'test/num_examples': 2472, 'score': 59113.41282272339, 'total_duration': 64825.58831310272, 'accumulated_submission_time': 59113.41282272339, 'accumulated_eval_time': 5706.620025396347, 'accumulated_logging_time': 2.478905200958252}
I0214 20:49:30.388627 140228258772736 logging_writer.py:48] [69968] accumulated_eval_time=5706.620025, accumulated_logging_time=2.478905, accumulated_submission_time=59113.412823, global_step=69968, preemption_count=0, score=59113.412823, test/ctc_loss=0.16443966329097748, test/num_examples=2472, test/wer=0.052282, total_duration=64825.588313, train/ctc_loss=0.07961263507604599, train/wer=0.030598, validation/ctc_loss=0.30682140588760376, validation/num_examples=5348, validation/wer=0.087838
I0214 20:49:55.625641 140228250380032 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1232919692993164, loss=0.8834413886070251
I0214 20:51:17.067064 140228258772736 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.3568657636642456, loss=0.8633344173431396
I0214 20:52:36.576109 140228250380032 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0504541397094727, loss=0.9150654673576355
I0214 20:53:55.308747 140228258772736 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.106632709503174, loss=0.8834624290466309
I0214 20:55:18.477658 140228250380032 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1955225467681885, loss=0.8884576559066772
I0214 20:56:46.296719 140228258772736 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2659151554107666, loss=0.8530794978141785
I0214 20:58:14.426698 140228250380032 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.1952120065689087, loss=0.8913941383361816
I0214 20:59:42.484979 140228258772736 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1537823677062988, loss=0.8573444485664368
I0214 21:01:10.368698 140228250380032 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2136887311935425, loss=0.90086829662323
I0214 21:02:39.105581 140228258772736 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.3765895366668701, loss=0.8820105195045471
I0214 21:04:07.708416 140228250380032 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1300230026245117, loss=0.8722980618476868
I0214 21:05:36.848948 140228258772736 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3212252855300903, loss=0.8843960762023926
I0214 21:06:54.220220 140228250380032 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.2026357650756836, loss=0.890565812587738
I0214 21:08:14.288680 140228258772736 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.285854458808899, loss=0.8411402106285095
I0214 21:09:35.109952 140228250380032 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0564463138580322, loss=0.881151556968689
I0214 21:11:02.514244 140228258772736 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1118637323379517, loss=0.8325755000114441
I0214 21:12:28.929154 140228250380032 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.1100056171417236, loss=0.8703468441963196
I0214 21:13:30.887357 140399019657024 spec.py:321] Evaluating on the training split.
I0214 21:14:24.431642 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 21:15:16.881122 140399019657024 spec.py:349] Evaluating on the test split.
I0214 21:15:44.165807 140399019657024 submission_runner.py:408] Time since start: 66399.42s, 	Step: 71672, 	{'train/ctc_loss': Array(0.07433639, dtype=float32), 'train/wer': 0.028280459691990507, 'validation/ctc_loss': Array(0.30262092, dtype=float32), 'validation/wer': 0.0857526284792956, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16102934, dtype=float32), 'test/wer': 0.05179452806044726, 'test/num_examples': 2472, 'score': 60553.82369470596, 'total_duration': 66399.41762804985, 'accumulated_submission_time': 60553.82369470596, 'accumulated_eval_time': 5839.893024682999, 'accumulated_logging_time': 2.547360420227051}
I0214 21:15:44.208237 140228688852736 logging_writer.py:48] [71672] accumulated_eval_time=5839.893025, accumulated_logging_time=2.547360, accumulated_submission_time=60553.823695, global_step=71672, preemption_count=0, score=60553.823695, test/ctc_loss=0.16102933883666992, test/num_examples=2472, test/wer=0.051795, total_duration=66399.417628, train/ctc_loss=0.07433638721704483, train/wer=0.028280, validation/ctc_loss=0.30262091755867004, validation/num_examples=5348, validation/wer=0.085753
I0214 21:16:06.386235 140228680460032 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1809123754501343, loss=0.8762310147285461
I0214 21:17:22.979651 140228688852736 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.189234733581543, loss=0.8700083494186401
I0214 21:18:40.589522 140228680460032 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1491838693618774, loss=0.8792693614959717
I0214 21:20:07.984425 140228688852736 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1203649044036865, loss=0.8496213555335999
I0214 21:21:42.570923 140228361172736 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2911217212677002, loss=0.8725280165672302
I0214 21:22:59.043751 140228352780032 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.4187984466552734, loss=0.8391330242156982
I0214 21:24:18.601312 140228361172736 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.5979658365249634, loss=0.8858581781387329
I0214 21:24:18.630769 140228352780032 logging_writer.py:48] [72301] global_step=72301, preemption_count=0, score=61068.183178
I0214 21:24:19.168411 140399019657024 checkpoints.py:490] Saving checkpoint at step: 72301
I0214 21:24:20.766478 140399019657024 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1/checkpoint_72301
I0214 21:24:20.799536 140399019657024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_1/checkpoint_72301.
I0214 21:24:24.408122 140399019657024 submission_runner.py:583] Tuning trial 1/5
I0214 21:24:24.408377 140399019657024 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0214 21:24:24.451474 140399019657024 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.02916, dtype=float32), 'train/wer': 1.3788965703933531, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043156299178389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.27949, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 62.217230796813965, 'total_duration': 245.91571712493896, 'accumulated_submission_time': 62.217230796813965, 'accumulated_eval_time': 183.69842910766602, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1738, {'train/ctc_loss': Array(6.6277523, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.710369, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.6806073, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1502.7666385173798, 'total_duration': 1794.4777307510376, 'accumulated_submission_time': 1502.7666385173798, 'accumulated_eval_time': 291.5967798233032, 'accumulated_logging_time': 0.04290056228637695, 'global_step': 1738, 'preemption_count': 0}), (3514, {'train/ctc_loss': Array(2.9477355, dtype=float32), 'train/wer': 0.6334869349812215, 'validation/ctc_loss': Array(3.3468678, dtype=float32), 'validation/wer': 0.6825936259980497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.023245, dtype=float32), 'test/wer': 0.625312290536835, 'test/num_examples': 2472, 'score': 2943.0448310375214, 'total_duration': 3362.7826220989227, 'accumulated_submission_time': 2943.0448310375214, 'accumulated_eval_time': 419.49665999412537, 'accumulated_logging_time': 0.09370923042297363, 'global_step': 3514, 'preemption_count': 0}), (5244, {'train/ctc_loss': Array(0.7954038, dtype=float32), 'train/wer': 0.26949412527476, 'validation/ctc_loss': Array(1.1628219, dtype=float32), 'validation/wer': 0.3323517769388957, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.86656547, dtype=float32), 'test/wer': 0.2749172303130014, 'test/num_examples': 2472, 'score': 4383.173229217529, 'total_duration': 4937.342960596085, 'accumulated_submission_time': 4383.173229217529, 'accumulated_eval_time': 553.7924075126648, 'accumulated_logging_time': 0.1545546054840088, 'global_step': 5244, 'preemption_count': 0}), (6967, {'train/ctc_loss': Array(0.52575487, dtype=float32), 'train/wer': 0.18291635685785798, 'validation/ctc_loss': Array(0.8364061, dtype=float32), 'validation/wer': 0.2505092829489172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.570568, dtype=float32), 'test/wer': 0.19348810757012572, 'test/num_examples': 2472, 'score': 5823.355607032776, 'total_duration': 6513.878301858902, 'accumulated_submission_time': 5823.355607032776, 'accumulated_eval_time': 690.0164759159088, 'accumulated_logging_time': 0.20651555061340332, 'global_step': 6967, 'preemption_count': 0}), (8695, {'train/ctc_loss': Array(0.4505587, dtype=float32), 'train/wer': 0.160277457652917, 'validation/ctc_loss': Array(0.7544828, dtype=float32), 'validation/wer': 0.2271257132374948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49796993, dtype=float32), 'test/wer': 0.1694595088660045, 'test/num_examples': 2472, 'score': 7263.37749004364, 'total_duration': 8088.980140447617, 'accumulated_submission_time': 7263.37749004364, 'accumulated_eval_time': 824.9669606685638, 'accumulated_logging_time': 0.2596597671508789, 'global_step': 8695, 'preemption_count': 0}), (10387, {'train/ctc_loss': Array(0.4173494, dtype=float32), 'train/wer': 0.14835877109217877, 'validation/ctc_loss': Array(0.68799996, dtype=float32), 'validation/wer': 0.20828948511735232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4417418, dtype=float32), 'test/wer': 0.15079316718461194, 'test/num_examples': 2472, 'score': 8703.979434251785, 'total_duration': 9666.078685045242, 'accumulated_submission_time': 8703.979434251785, 'accumulated_eval_time': 961.3382768630981, 'accumulated_logging_time': 0.31139492988586426, 'global_step': 10387, 'preemption_count': 0}), (12101, {'train/ctc_loss': Array(0.35887453, dtype=float32), 'train/wer': 0.12945355539957032, 'validation/ctc_loss': Array(0.641523, dtype=float32), 'validation/wer': 0.1964142618535003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40709147, dtype=float32), 'test/wer': 0.1400483415595231, 'test/num_examples': 2472, 'score': 10144.905450105667, 'total_duration': 11243.875876426697, 'accumulated_submission_time': 10144.905450105667, 'accumulated_eval_time': 1098.0837564468384, 'accumulated_logging_time': 0.36140894889831543, 'global_step': 12101, 'preemption_count': 0}), (13827, {'train/ctc_loss': Array(0.30647576, dtype=float32), 'train/wer': 0.1146051639408201, 'validation/ctc_loss': Array(0.61191267, dtype=float32), 'validation/wer': 0.18591965397723434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38234127, dtype=float32), 'test/wer': 0.13007535596043304, 'test/num_examples': 2472, 'score': 11585.165503025055, 'total_duration': 12821.49998164177, 'accumulated_submission_time': 11585.165503025055, 'accumulated_eval_time': 1235.3221170902252, 'accumulated_logging_time': 0.4123513698577881, 'global_step': 13827, 'preemption_count': 0}), (15527, {'train/ctc_loss': Array(0.2841638, dtype=float32), 'train/wer': 0.10447999404157068, 'validation/ctc_loss': Array(0.58510756, dtype=float32), 'validation/wer': 0.17735597671297681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35999078, dtype=float32), 'test/wer': 0.122214774643024, 'test/num_examples': 2472, 'score': 13025.100800514221, 'total_duration': 14397.421264886856, 'accumulated_submission_time': 13025.100800514221, 'accumulated_eval_time': 1371.180969953537, 'accumulated_logging_time': 0.4650459289550781, 'global_step': 15527, 'preemption_count': 0}), (17244, {'train/ctc_loss': Array(0.27435565, dtype=float32), 'train/wer': 0.10281175757640762, 'validation/ctc_loss': Array(0.569697, dtype=float32), 'validation/wer': 0.17216177336667407, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3494229, dtype=float32), 'test/wer': 0.11859931346860846, 'test/num_examples': 2472, 'score': 14466.018681049347, 'total_duration': 15974.599870920181, 'accumulated_submission_time': 14466.018681049347, 'accumulated_eval_time': 1507.310597896576, 'accumulated_logging_time': 0.5201177597045898, 'global_step': 17244, 'preemption_count': 0}), (18961, {'train/ctc_loss': Array(0.28704607, dtype=float32), 'train/wer': 0.10458235278563685, 'validation/ctc_loss': Array(0.55894655, dtype=float32), 'validation/wer': 0.170549446305647, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34298202, dtype=float32), 'test/wer': 0.11685251762029533, 'test/num_examples': 2472, 'score': 15906.14902305603, 'total_duration': 17551.447067975998, 'accumulated_submission_time': 15906.14902305603, 'accumulated_eval_time': 1643.9017758369446, 'accumulated_logging_time': 0.5702481269836426, 'global_step': 18961, 'preemption_count': 0}), (20642, {'train/ctc_loss': Array(0.2800363, dtype=float32), 'train/wer': 0.10345846413591157, 'validation/ctc_loss': Array(0.54050195, dtype=float32), 'validation/wer': 0.16402290083705842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32796475, dtype=float32), 'test/wer': 0.11216054272540775, 'test/num_examples': 2472, 'score': 17346.25297522545, 'total_duration': 19130.841157197952, 'accumulated_submission_time': 17346.25297522545, 'accumulated_eval_time': 1783.0626783370972, 'accumulated_logging_time': 0.6249892711639404, 'global_step': 20642, 'preemption_count': 0}), (22354, {'train/ctc_loss': Array(0.2611929, dtype=float32), 'train/wer': 0.09692869636793909, 'validation/ctc_loss': Array(0.5234255, dtype=float32), 'validation/wer': 0.16026724079670196, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3156809, dtype=float32), 'test/wer': 0.10909349420104401, 'test/num_examples': 2472, 'score': 18786.501445770264, 'total_duration': 20706.88010787964, 'accumulated_submission_time': 18786.501445770264, 'accumulated_eval_time': 1918.723219871521, 'accumulated_logging_time': 0.6793062686920166, 'global_step': 22354, 'preemption_count': 0}), (24076, {'train/ctc_loss': Array(0.24182494, dtype=float32), 'train/wer': 0.08912735948650817, 'validation/ctc_loss': Array(0.5162125, dtype=float32), 'validation/wer': 0.15688811222568716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3084755, dtype=float32), 'test/wer': 0.10423902666910406, 'test/num_examples': 2472, 'score': 20226.675240516663, 'total_duration': 22282.030920743942, 'accumulated_submission_time': 20226.675240516663, 'accumulated_eval_time': 2053.571811437607, 'accumulated_logging_time': 0.733173131942749, 'global_step': 24076, 'preemption_count': 0}), (25751, {'train/ctc_loss': Array(0.22988309, dtype=float32), 'train/wer': 0.08592295052035787, 'validation/ctc_loss': Array(0.50804406, dtype=float32), 'validation/wer': 0.15461926875657722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30117932, dtype=float32), 'test/wer': 0.102309426604107, 'test/num_examples': 2472, 'score': 21666.55222582817, 'total_duration': 23857.990137577057, 'accumulated_submission_time': 21666.55222582817, 'accumulated_eval_time': 2189.5265684127808, 'accumulated_logging_time': 0.7874441146850586, 'global_step': 25751, 'preemption_count': 0}), (27470, {'train/ctc_loss': Array(0.22008015, dtype=float32), 'train/wer': 0.08128177066819404, 'validation/ctc_loss': Array(0.49360743, dtype=float32), 'validation/wer': 0.14966643173677555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29376552, dtype=float32), 'test/wer': 0.10098917392805638, 'test/num_examples': 2472, 'score': 23107.052596330643, 'total_duration': 25434.80774664879, 'accumulated_submission_time': 23107.052596330643, 'accumulated_eval_time': 2325.708679676056, 'accumulated_logging_time': 0.8463056087493896, 'global_step': 27470, 'preemption_count': 0}), (29189, {'train/ctc_loss': Array(0.2380161, dtype=float32), 'train/wer': 0.08831686600797366, 'validation/ctc_loss': Array(0.48860037, dtype=float32), 'validation/wer': 0.14919335373683346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2872465, dtype=float32), 'test/wer': 0.09849084963337598, 'test/num_examples': 2472, 'score': 24547.576735258102, 'total_duration': 27014.36773943901, 'accumulated_submission_time': 24547.576735258102, 'accumulated_eval_time': 2464.611401796341, 'accumulated_logging_time': 0.904517650604248, 'global_step': 29189, 'preemption_count': 0}), (30881, {'train/ctc_loss': Array(0.2175279, dtype=float32), 'train/wer': 0.0792353729537328, 'validation/ctc_loss': Array(0.47386247, dtype=float32), 'validation/wer': 0.14348745377834848, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28259403, dtype=float32), 'test/wer': 0.09463164950338188, 'test/num_examples': 2472, 'score': 25988.718726873398, 'total_duration': 28590.486193180084, 'accumulated_submission_time': 25988.718726873398, 'accumulated_eval_time': 2599.4616689682007, 'accumulated_logging_time': 0.9569101333618164, 'global_step': 30881, 'preemption_count': 0}), (32563, {'train/ctc_loss': Array(0.22219987, dtype=float32), 'train/wer': 0.08049561049781938, 'validation/ctc_loss': Array(0.46052456, dtype=float32), 'validation/wer': 0.13775258986068337, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26877353, dtype=float32), 'test/wer': 0.09111774622712408, 'test/num_examples': 2472, 'score': 27428.926631212234, 'total_duration': 30169.796103715897, 'accumulated_submission_time': 27428.926631212234, 'accumulated_eval_time': 2738.427877187729, 'accumulated_logging_time': 1.0178043842315674, 'global_step': 32563, 'preemption_count': 0}), (34277, {'train/ctc_loss': Array(0.21909791, dtype=float32), 'train/wer': 0.0772190074277737, 'validation/ctc_loss': Array(0.4654405, dtype=float32), 'validation/wer': 0.13907527732990915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27387086, dtype=float32), 'test/wer': 0.091930209412386, 'test/num_examples': 2472, 'score': 28868.846267938614, 'total_duration': 31746.674035787582, 'accumulated_submission_time': 28868.846267938614, 'accumulated_eval_time': 2875.2527759075165, 'accumulated_logging_time': 1.0767803192138672, 'global_step': 34277, 'preemption_count': 0}), (35964, {'train/ctc_loss': Array(0.17266439, dtype=float32), 'train/wer': 0.06542951733751524, 'validation/ctc_loss': Array(0.45770597, dtype=float32), 'validation/wer': 0.13686436177915948, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26163143, dtype=float32), 'test/wer': 0.08766477768976093, 'test/num_examples': 2472, 'score': 30309.341188192368, 'total_duration': 33325.48345398903, 'accumulated_submission_time': 30309.341188192368, 'accumulated_eval_time': 3013.4304831027985, 'accumulated_logging_time': 1.1376783847808838, 'global_step': 35964, 'preemption_count': 0}), (37649, {'train/ctc_loss': Array(0.19681297, dtype=float32), 'train/wer': 0.0729393254630314, 'validation/ctc_loss': Array(0.4440631, dtype=float32), 'validation/wer': 0.134247950799888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26120028, dtype=float32), 'test/wer': 0.08866004509170679, 'test/num_examples': 2472, 'score': 31749.260707616806, 'total_duration': 34901.451577186584, 'accumulated_submission_time': 31749.260707616806, 'accumulated_eval_time': 3149.3453755378723, 'accumulated_logging_time': 1.1959314346313477, 'global_step': 37649, 'preemption_count': 0}), (39356, {'train/ctc_loss': Array(0.24031529, dtype=float32), 'train/wer': 0.08714872289993111, 'validation/ctc_loss': Array(0.4375541, dtype=float32), 'validation/wer': 0.12940131496374677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2517394, dtype=float32), 'test/wer': 0.08321654175045193, 'test/num_examples': 2472, 'score': 33189.13189792633, 'total_duration': 36475.276661872864, 'accumulated_submission_time': 33189.13189792633, 'accumulated_eval_time': 3283.1624019145966, 'accumulated_logging_time': 1.2573490142822266, 'global_step': 39356, 'preemption_count': 0}), (41039, {'train/ctc_loss': Array(0.24005286, dtype=float32), 'train/wer': 0.08867695143141054, 'validation/ctc_loss': Array(0.4300553, dtype=float32), 'validation/wer': 0.12838757639244233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24431132, dtype=float32), 'test/wer': 0.08384620071902991, 'test/num_examples': 2472, 'score': 34629.38103199005, 'total_duration': 38051.034391880035, 'accumulated_submission_time': 34629.38103199005, 'accumulated_eval_time': 3418.5341413021088, 'accumulated_logging_time': 1.3193349838256836, 'global_step': 41039, 'preemption_count': 0}), (42750, {'train/ctc_loss': Array(0.28034556, dtype=float32), 'train/wer': 0.10321086159191815, 'validation/ctc_loss': Array(0.42529032, dtype=float32), 'validation/wer': 0.1256649642295104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24818589, dtype=float32), 'test/wer': 0.08305404911339954, 'test/num_examples': 2472, 'score': 36069.76509022713, 'total_duration': 39624.3362300396, 'accumulated_submission_time': 36069.76509022713, 'accumulated_eval_time': 3551.3137764930725, 'accumulated_logging_time': 1.3813042640686035, 'global_step': 42750, 'preemption_count': 0}), (44453, {'train/ctc_loss': Array(0.24660096, dtype=float32), 'train/wer': 0.08854924079623666, 'validation/ctc_loss': Array(0.41894877, dtype=float32), 'validation/wer': 0.12466088031126601, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23739682, dtype=float32), 'test/wer': 0.08073852903540309, 'test/num_examples': 2472, 'score': 37509.9409840107, 'total_duration': 41198.581345796585, 'accumulated_submission_time': 37509.9409840107, 'accumulated_eval_time': 3685.243673801422, 'accumulated_logging_time': 1.445460557937622, 'global_step': 44453, 'preemption_count': 0}), (46142, {'train/ctc_loss': Array(0.22179091, dtype=float32), 'train/wer': 0.08236001469720419, 'validation/ctc_loss': Array(0.40570727, dtype=float32), 'validation/wer': 0.1204804155362677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23045741, dtype=float32), 'test/wer': 0.07738711839619768, 'test/num_examples': 2472, 'score': 38950.01946210861, 'total_duration': 42772.615599155426, 'accumulated_submission_time': 38950.01946210861, 'accumulated_eval_time': 3819.056458711624, 'accumulated_logging_time': 1.5136487483978271, 'global_step': 46142, 'preemption_count': 0}), (47851, {'train/ctc_loss': Array(0.18740244, dtype=float32), 'train/wer': 0.0704520984458776, 'validation/ctc_loss': Array(0.4032107, dtype=float32), 'validation/wer': 0.11852052096507912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22821124, dtype=float32), 'test/wer': 0.07582312676456848, 'test/num_examples': 2472, 'score': 40389.98576760292, 'total_duration': 44348.57243299484, 'accumulated_submission_time': 40389.98576760292, 'accumulated_eval_time': 3954.9089529514313, 'accumulated_logging_time': 1.5759549140930176, 'global_step': 47851, 'preemption_count': 0}), (49534, {'train/ctc_loss': Array(0.20381515, dtype=float32), 'train/wer': 0.07671335545574522, 'validation/ctc_loss': Array(0.3923336, dtype=float32), 'validation/wer': 0.11468762370024232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21855989, dtype=float32), 'test/wer': 0.07228891190867914, 'test/num_examples': 2472, 'score': 41830.2703332901, 'total_duration': 45924.74306154251, 'accumulated_submission_time': 41830.2703332901, 'accumulated_eval_time': 4090.652411699295, 'accumulated_logging_time': 1.6439871788024902, 'global_step': 49534, 'preemption_count': 0}), (51230, {'train/ctc_loss': Array(0.17861539, dtype=float32), 'train/wer': 0.06775038653152379, 'validation/ctc_loss': Array(0.3843447, dtype=float32), 'validation/wer': 0.11259256398621316, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21695636, dtype=float32), 'test/wer': 0.0702374423658928, 'test/num_examples': 2472, 'score': 43270.42231464386, 'total_duration': 47500.26622223854, 'accumulated_submission_time': 43270.42231464386, 'accumulated_eval_time': 4225.891577243805, 'accumulated_logging_time': 1.700354814529419, 'global_step': 51230, 'preemption_count': 0}), (52957, {'train/ctc_loss': Array(0.17695369, dtype=float32), 'train/wer': 0.0670163486260428, 'validation/ctc_loss': Array(0.372745, dtype=float32), 'validation/wer': 0.10919412610907828, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21086185, dtype=float32), 'test/wer': 0.07096865923262852, 'test/num_examples': 2472, 'score': 44710.28479671478, 'total_duration': 49075.17360305786, 'accumulated_submission_time': 44710.28479671478, 'accumulated_eval_time': 4360.704236030579, 'accumulated_logging_time': 1.8572721481323242, 'global_step': 52957, 'preemption_count': 0}), (54652, {'train/ctc_loss': Array(0.16497089, dtype=float32), 'train/wer': 0.06185231193926846, 'validation/ctc_loss': Array(0.36614746, dtype=float32), 'validation/wer': 0.1077169641908918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19986272, dtype=float32), 'test/wer': 0.06739382121747609, 'test/num_examples': 2472, 'score': 46150.37541222572, 'total_duration': 50651.51419734955, 'accumulated_submission_time': 46150.37541222572, 'accumulated_eval_time': 4496.820524454117, 'accumulated_logging_time': 1.9165270328521729, 'global_step': 54652, 'preemption_count': 0}), (56368, {'train/ctc_loss': Array(0.16453665, dtype=float32), 'train/wer': 0.06207433352792372, 'validation/ctc_loss': Array(0.3558999, dtype=float32), 'validation/wer': 0.10438610888517721, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1941069, dtype=float32), 'test/wer': 0.06440801901163853, 'test/num_examples': 2472, 'score': 47590.537660598755, 'total_duration': 52226.748883485794, 'accumulated_submission_time': 47590.537660598755, 'accumulated_eval_time': 4631.7555339336395, 'accumulated_logging_time': 1.9792718887329102, 'global_step': 56368, 'preemption_count': 0}), (58095, {'train/ctc_loss': Array(0.16163893, dtype=float32), 'train/wer': 0.0600747122055348, 'validation/ctc_loss': Array(0.34983188, dtype=float32), 'validation/wer': 0.10201106423240681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19088641, dtype=float32), 'test/wer': 0.061442528385432536, 'test/num_examples': 2472, 'score': 49030.65796470642, 'total_duration': 53802.684811115265, 'accumulated_submission_time': 49030.65796470642, 'accumulated_eval_time': 4767.43133020401, 'accumulated_logging_time': 2.0423855781555176, 'global_step': 58095, 'preemption_count': 0}), (59789, {'train/ctc_loss': Array(0.13394181, dtype=float32), 'train/wer': 0.05147734660479297, 'validation/ctc_loss': Array(0.34509662, dtype=float32), 'validation/wer': 0.10036011855913958, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1848671, dtype=float32), 'test/wer': 0.0623159263095891, 'test/num_examples': 2472, 'score': 50470.96928143501, 'total_duration': 55376.52167439461, 'accumulated_submission_time': 50470.96928143501, 'accumulated_eval_time': 4900.822810411453, 'accumulated_logging_time': 2.103106737136841, 'global_step': 59789, 'preemption_count': 0}), (61491, {'train/ctc_loss': Array(0.13361306, dtype=float32), 'train/wer': 0.05178905572543471, 'validation/ctc_loss': Array(0.3403858, dtype=float32), 'validation/wer': 0.09937534394701526, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1836257, dtype=float32), 'test/wer': 0.06146283996506408, 'test/num_examples': 2472, 'score': 51911.2644867897, 'total_duration': 56951.9517929554, 'accumulated_submission_time': 51911.2644867897, 'accumulated_eval_time': 5035.821505784988, 'accumulated_logging_time': 2.164682149887085, 'global_step': 61491, 'preemption_count': 0}), (63210, {'train/ctc_loss': Array(0.11915162, dtype=float32), 'train/wer': 0.04634981922064009, 'validation/ctc_loss': Array(0.33299387, dtype=float32), 'validation/wer': 0.09586105023315987, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17569458, dtype=float32), 'test/wer': 0.057827067211017, 'test/num_examples': 2472, 'score': 53351.83570098877, 'total_duration': 58525.53119134903, 'accumulated_submission_time': 53351.83570098877, 'accumulated_eval_time': 5168.687787055969, 'accumulated_logging_time': 2.2308127880096436, 'global_step': 63210, 'preemption_count': 0}), (64881, {'train/ctc_loss': Array(0.11503623, dtype=float32), 'train/wer': 0.04372654964500646, 'validation/ctc_loss': Array(0.32092267, dtype=float32), 'validation/wer': 0.09318671133552815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17374022, dtype=float32), 'test/wer': 0.056913046127597344, 'test/num_examples': 2472, 'score': 54792.682903051376, 'total_duration': 60099.91721081734, 'accumulated_submission_time': 54792.682903051376, 'accumulated_eval_time': 5302.091877937317, 'accumulated_logging_time': 2.2912206649780273, 'global_step': 64881, 'preemption_count': 0}), (66568, {'train/ctc_loss': Array(0.10550904, dtype=float32), 'train/wer': 0.0410768989959974, 'validation/ctc_loss': Array(0.3176728, dtype=float32), 'validation/wer': 0.09162265753980131, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17099647, dtype=float32), 'test/wer': 0.05496313448296874, 'test/num_examples': 2472, 'score': 56232.994379758835, 'total_duration': 61675.28289914131, 'accumulated_submission_time': 56232.994379758835, 'accumulated_eval_time': 5436.998205900192, 'accumulated_logging_time': 2.3619296550750732, 'global_step': 66568, 'preemption_count': 0}), (68277, {'train/ctc_loss': Array(0.09899141, dtype=float32), 'train/wer': 0.03862501798700508, 'validation/ctc_loss': Array(0.31188878, dtype=float32), 'validation/wer': 0.08942139664211167, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16618471, dtype=float32), 'test/wer': 0.053338208112444906, 'test/num_examples': 2472, 'score': 57673.43183207512, 'total_duration': 63251.47676539421, 'accumulated_submission_time': 57673.43183207512, 'accumulated_eval_time': 5572.620210170746, 'accumulated_logging_time': 2.421889066696167, 'global_step': 68277, 'preemption_count': 0}), (69968, {'train/ctc_loss': Array(0.07961264, dtype=float32), 'train/wer': 0.030598225845663272, 'validation/ctc_loss': Array(0.3068214, dtype=float32), 'validation/wer': 0.08783803354026473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16443966, dtype=float32), 'test/wer': 0.052282005971604414, 'test/num_examples': 2472, 'score': 59113.41282272339, 'total_duration': 64825.58831310272, 'accumulated_submission_time': 59113.41282272339, 'accumulated_eval_time': 5706.620025396347, 'accumulated_logging_time': 2.478905200958252, 'global_step': 69968, 'preemption_count': 0}), (71672, {'train/ctc_loss': Array(0.07433639, dtype=float32), 'train/wer': 0.028280459691990507, 'validation/ctc_loss': Array(0.30262092, dtype=float32), 'validation/wer': 0.0857526284792956, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16102934, dtype=float32), 'test/wer': 0.05179452806044726, 'test/num_examples': 2472, 'score': 60553.82369470596, 'total_duration': 66399.41762804985, 'accumulated_submission_time': 60553.82369470596, 'accumulated_eval_time': 5839.893024682999, 'accumulated_logging_time': 2.547360420227051, 'global_step': 71672, 'preemption_count': 0})], 'global_step': 72301}
I0214 21:24:24.451741 140399019657024 submission_runner.py:586] Timing: 61068.183177948
I0214 21:24:24.451816 140399019657024 submission_runner.py:588] Total number of evals: 43
I0214 21:24:24.451867 140399019657024 submission_runner.py:589] ====================
I0214 21:24:24.451928 140399019657024 submission_runner.py:542] Using RNG seed 2554204701
I0214 21:24:24.454422 140399019657024 submission_runner.py:551] --- Tuning run 2/5 ---
I0214 21:24:24.454609 140399019657024 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2.
I0214 21:24:24.456681 140399019657024 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2/hparams.json.
I0214 21:24:24.458637 140399019657024 submission_runner.py:206] Initializing dataset.
I0214 21:24:24.458776 140399019657024 submission_runner.py:213] Initializing model.
I0214 21:24:28.471590 140399019657024 submission_runner.py:255] Initializing optimizer.
I0214 21:24:28.907357 140399019657024 submission_runner.py:262] Initializing metrics bundle.
I0214 21:24:28.907568 140399019657024 submission_runner.py:280] Initializing checkpoint and logger.
I0214 21:24:29.016521 140399019657024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2 with prefix checkpoint_
I0214 21:24:29.016670 140399019657024 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2/meta_data_0.json.
I0214 21:24:29.016893 140399019657024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 21:24:29.016977 140399019657024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 21:24:29.589390 140399019657024 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 21:24:30.051657 140399019657024 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2/flags_0.json.
I0214 21:24:30.181634 140399019657024 submission_runner.py:314] Starting training loop.
I0214 21:24:30.185242 140399019657024 input_pipeline.py:20] Loading split = train-clean-100
I0214 21:24:30.233891 140399019657024 input_pipeline.py:20] Loading split = train-clean-360
I0214 21:24:30.373250 140399019657024 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 21:25:05.743145 140228621711104 logging_writer.py:48] [0] global_step=0, grad_norm=44.14146041870117, loss=31.829971313476562
I0214 21:25:05.759613 140399019657024 spec.py:321] Evaluating on the training split.
I0214 21:26:04.805128 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 21:26:56.806026 140399019657024 spec.py:349] Evaluating on the test split.
I0214 21:27:24.743723 140399019657024 submission_runner.py:408] Time since start: 174.56s, 	Step: 1, 	{'train/ctc_loss': Array(32.387848, dtype=float32), 'train/wer': 1.3955485058197952, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043146644525329, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.279486, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 35.57791781425476, 'total_duration': 174.5587763786316, 'accumulated_submission_time': 35.57791781425476, 'accumulated_eval_time': 138.98080229759216, 'accumulated_logging_time': 0}
I0214 21:27:24.765170 140318077859584 logging_writer.py:48] [1] accumulated_eval_time=138.980802, accumulated_logging_time=0, accumulated_submission_time=35.577918, global_step=1, preemption_count=0, score=35.577918, test/ctc_loss=31.27948570251465, test/num_examples=2472, test/wer=1.097699, total_duration=174.558776, train/ctc_loss=32.387847900390625, train/wer=1.395549, validation/ctc_loss=31.163591384887695, validation/num_examples=5348, validation/wer=1.043147
I0214 21:29:10.379079 140228630103808 logging_writer.py:48] [100] global_step=100, grad_norm=2.568187713623047, loss=6.649985313415527
I0214 21:30:28.716521 140228638496512 logging_writer.py:48] [200] global_step=200, grad_norm=0.6175643801689148, loss=5.932361602783203
I0214 21:31:46.982272 140228630103808 logging_writer.py:48] [300] global_step=300, grad_norm=0.47367212176322937, loss=5.836099147796631
I0214 21:33:05.119560 140228638496512 logging_writer.py:48] [400] global_step=400, grad_norm=0.5760883688926697, loss=5.806021690368652
I0214 21:34:23.779591 140228630103808 logging_writer.py:48] [500] global_step=500, grad_norm=0.570950984954834, loss=5.810318946838379
I0214 21:35:41.766463 140228638496512 logging_writer.py:48] [600] global_step=600, grad_norm=0.5810579657554626, loss=5.793973922729492
I0214 21:37:09.852410 140228630103808 logging_writer.py:48] [700] global_step=700, grad_norm=1.9093612432479858, loss=5.7824602127075195
I0214 21:38:39.422395 140228638496512 logging_writer.py:48] [800] global_step=800, grad_norm=1.6847188472747803, loss=5.788020133972168
I0214 21:40:07.295202 140228630103808 logging_writer.py:48] [900] global_step=900, grad_norm=3.6546554565429688, loss=5.798965930938721
I0214 21:41:37.851802 140228638496512 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9925527572631836, loss=5.77406644821167
I0214 21:43:00.726121 140318077859584 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.241114139556885, loss=5.730327606201172
I0214 21:44:20.942809 140318069466880 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.541102647781372, loss=5.534539222717285
I0214 21:45:40.535201 140318077859584 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.226884603500366, loss=5.49166202545166
I0214 21:47:09.601348 140318069466880 logging_writer.py:48] [1400] global_step=1400, grad_norm=8.061100959777832, loss=5.586886405944824
I0214 21:48:37.941478 140318077859584 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.130719780921936, loss=4.747255325317383
I0214 21:50:09.619511 140318069466880 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7833146452903748, loss=4.16571044921875
I0214 21:51:25.232706 140399019657024 spec.py:321] Evaluating on the training split.
I0214 21:52:04.976356 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 21:52:52.974670 140399019657024 spec.py:349] Evaluating on the test split.
I0214 21:53:18.250251 140399019657024 submission_runner.py:408] Time since start: 1728.06s, 	Step: 1685, 	{'train/ctc_loss': Array(6.3702335, dtype=float32), 'train/wer': 0.936353811149033, 'validation/ctc_loss': Array(6.3203735, dtype=float32), 'validation/wer': 0.895440107359742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.330534, dtype=float32), 'test/wer': 0.8973046533828936, 'test/num_examples': 2472, 'score': 1475.9603667259216, 'total_duration': 1728.0618832111359, 'accumulated_submission_time': 1475.9603667259216, 'accumulated_eval_time': 251.99166750907898, 'accumulated_logging_time': 0.03572487831115723}
I0214 21:53:18.284849 140318077859584 logging_writer.py:48] [1685] accumulated_eval_time=251.991668, accumulated_logging_time=0.035725, accumulated_submission_time=1475.960367, global_step=1685, preemption_count=0, score=1475.960367, test/ctc_loss=6.330533981323242, test/num_examples=2472, test/wer=0.897305, total_duration=1728.061883, train/ctc_loss=6.370233535766602, train/wer=0.936354, validation/ctc_loss=6.32037353515625, validation/num_examples=5348, validation/wer=0.895440
I0214 21:53:30.740347 140318069466880 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.208687663078308, loss=3.854560136795044
I0214 21:54:48.138977 140318077859584 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9246078729629517, loss=3.6348648071289062
I0214 21:56:05.212005 140318069466880 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.6033844947814941, loss=3.4123950004577637
I0214 21:57:34.332204 140318077859584 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0228086709976196, loss=3.2941462993621826
I0214 21:59:03.838605 140318077859584 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.063293218612671, loss=3.1404669284820557
I0214 22:00:22.540163 140318069466880 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.2216461896896362, loss=3.0859105587005615
I0214 22:01:42.556585 140318077859584 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9238247871398926, loss=2.97415828704834
I0214 22:03:04.247955 140318069466880 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.342693567276001, loss=2.9254965782165527
I0214 22:04:32.193956 140318077859584 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.3516727685928345, loss=2.8708930015563965
I0214 22:06:04.241790 140318069466880 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9757182598114014, loss=2.727196216583252
I0214 22:07:36.611102 140318077859584 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.80474054813385, loss=2.6853654384613037
I0214 22:09:06.753246 140318069466880 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8620672225952148, loss=2.572885513305664
I0214 22:10:36.228426 140318077859584 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1347734928131104, loss=2.615403175354004
I0214 22:12:06.982004 140318069466880 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.907249093055725, loss=2.4839513301849365
I0214 22:13:41.126014 140318077859584 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8526654839515686, loss=2.4275741577148438
I0214 22:14:59.364100 140318069466880 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0511534214019775, loss=2.3777027130126953
I0214 22:16:17.121209 140318077859584 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7725561261177063, loss=2.335498571395874
I0214 22:17:18.833125 140399019657024 spec.py:321] Evaluating on the training split.
I0214 22:18:10.660537 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 22:19:02.441694 140399019657024 spec.py:349] Evaluating on the test split.
I0214 22:19:30.676780 140399019657024 submission_runner.py:408] Time since start: 3300.49s, 	Step: 3376, 	{'train/ctc_loss': Array(2.8703103, dtype=float32), 'train/wer': 0.5950224159334732, 'validation/ctc_loss': Array(2.8294756, dtype=float32), 'validation/wer': 0.5699334794404163, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5039368, dtype=float32), 'test/wer': 0.5188999248471554, 'test/num_examples': 2472, 'score': 2916.4242436885834, 'total_duration': 3300.489486694336, 'accumulated_submission_time': 2916.4242436885834, 'accumulated_eval_time': 383.8297390937805, 'accumulated_logging_time': 0.08650851249694824}
I0214 22:19:30.710818 140318077859584 logging_writer.py:48] [3376] accumulated_eval_time=383.829739, accumulated_logging_time=0.086509, accumulated_submission_time=2916.424244, global_step=3376, preemption_count=0, score=2916.424244, test/ctc_loss=2.503936767578125, test/num_examples=2472, test/wer=0.518900, total_duration=3300.489487, train/ctc_loss=2.8703103065490723, train/wer=0.595022, validation/ctc_loss=2.8294756412506104, validation/num_examples=5348, validation/wer=0.569933
I0214 22:19:49.875062 140318069466880 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8101664781570435, loss=2.2195212841033936
I0214 22:21:07.225760 140318077859584 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0958051681518555, loss=2.243652105331421
I0214 22:22:24.128043 140318069466880 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7777608036994934, loss=2.173610210418701
I0214 22:23:50.560376 140318077859584 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7964881658554077, loss=2.149174451828003
I0214 22:25:17.572280 140318069466880 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8138518929481506, loss=2.0864429473876953
I0214 22:26:49.569093 140318077859584 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.743278443813324, loss=2.1805262565612793
I0214 22:28:19.094784 140318069466880 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0745735168457031, loss=2.045133113861084
I0214 22:29:50.657153 140318077859584 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.739308774471283, loss=2.0551164150238037
I0214 22:31:13.496166 140318077859584 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6468826532363892, loss=1.9634243249893188
I0214 22:32:31.722652 140318069466880 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.027560830116272, loss=1.9907804727554321
I0214 22:33:52.672508 140318077859584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.784538209438324, loss=1.9503257274627686
I0214 22:35:18.764956 140318069466880 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7749285101890564, loss=1.926270604133606
I0214 22:36:46.968433 140318077859584 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6815732717514038, loss=1.9688024520874023
I0214 22:38:15.464409 140318069466880 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.801300585269928, loss=1.837080717086792
I0214 22:39:44.377842 140318077859584 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7469475865364075, loss=1.8580245971679688
I0214 22:41:15.236548 140318069466880 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8009631633758545, loss=1.8886014223098755
I0214 22:42:47.112446 140318077859584 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8308871984481812, loss=1.8308898210525513
I0214 22:43:30.897356 140399019657024 spec.py:321] Evaluating on the training split.
I0214 22:44:27.375433 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 22:45:21.593742 140399019657024 spec.py:349] Evaluating on the test split.
I0214 22:45:48.661887 140399019657024 submission_runner.py:408] Time since start: 4878.47s, 	Step: 5050, 	{'train/ctc_loss': Array(0.83288306, dtype=float32), 'train/wer': 0.2686575402958711, 'validation/ctc_loss': Array(0.9431758, dtype=float32), 'validation/wer': 0.28080558425132995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66521096, dtype=float32), 'test/wer': 0.2190197631669815, 'test/num_examples': 2472, 'score': 4356.522769451141, 'total_duration': 4878.47324180603, 'accumulated_submission_time': 4356.522769451141, 'accumulated_eval_time': 521.587308883667, 'accumulated_logging_time': 0.14109587669372559}
I0214 22:45:48.694569 140318077859584 logging_writer.py:48] [5050] accumulated_eval_time=521.587309, accumulated_logging_time=0.141096, accumulated_submission_time=4356.522769, global_step=5050, preemption_count=0, score=4356.522769, test/ctc_loss=0.6652109622955322, test/num_examples=2472, test/wer=0.219020, total_duration=4878.473242, train/ctc_loss=0.8328830599784851, train/wer=0.268658, validation/ctc_loss=0.9431757926940918, validation/num_examples=5348, validation/wer=0.280806
I0214 22:46:28.174611 140318069466880 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7977325320243835, loss=1.8254657983779907
I0214 22:47:48.937286 140318077859584 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5961374640464783, loss=1.8154493570327759
I0214 22:49:08.945385 140318069466880 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7553263902664185, loss=1.7908775806427002
I0214 22:50:28.681768 140318077859584 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5686492323875427, loss=1.7761549949645996
I0214 22:51:52.718266 140318069466880 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9451816082000732, loss=1.802667498588562
I0214 22:53:18.897117 140318077859584 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9536869525909424, loss=1.809611439704895
I0214 22:54:48.655443 140318069466880 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6777125597000122, loss=1.860455870628357
I0214 22:56:19.040847 140318077859584 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.627730667591095, loss=1.758818507194519
I0214 22:57:51.386687 140318069466880 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6930453777313232, loss=1.7801867723464966
I0214 22:59:20.720884 140318077859584 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7556576728820801, loss=1.6990594863891602
I0214 23:00:50.713212 140318069466880 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9305669069290161, loss=1.7312062978744507
I0214 23:02:21.560760 140318077859584 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9417332410812378, loss=1.622070074081421
I0214 23:03:39.501826 140318069466880 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6209583282470703, loss=1.672452449798584
I0214 23:04:56.375291 140318077859584 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6890050172805786, loss=1.699312686920166
I0214 23:06:19.409786 140318069466880 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5915859341621399, loss=1.6492919921875
I0214 23:07:43.498350 140318077859584 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.5282402038574219, loss=1.6143410205841064
I0214 23:09:13.575251 140318069466880 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6808627843856812, loss=1.7023600339889526
I0214 23:09:49.279215 140399019657024 spec.py:321] Evaluating on the training split.
I0214 23:10:42.013972 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 23:11:34.356874 140399019657024 spec.py:349] Evaluating on the test split.
I0214 23:12:01.824676 140399019657024 submission_runner.py:408] Time since start: 6451.64s, 	Step: 6740, 	{'train/ctc_loss': Array(0.6040004, dtype=float32), 'train/wer': 0.2079973718639594, 'validation/ctc_loss': Array(0.74008816, dtype=float32), 'validation/wer': 0.22464446740106395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48961702, dtype=float32), 'test/wer': 0.16718461194727113, 'test/num_examples': 2472, 'score': 5797.023282766342, 'total_duration': 6451.637514591217, 'accumulated_submission_time': 5797.023282766342, 'accumulated_eval_time': 654.1273121833801, 'accumulated_logging_time': 0.18926143646240234}
I0214 23:12:01.858658 140318077859584 logging_writer.py:48] [6740] accumulated_eval_time=654.127312, accumulated_logging_time=0.189261, accumulated_submission_time=5797.023283, global_step=6740, preemption_count=0, score=5797.023283, test/ctc_loss=0.4896170198917389, test/num_examples=2472, test/wer=0.167185, total_duration=6451.637515, train/ctc_loss=0.6040003895759583, train/wer=0.207997, validation/ctc_loss=0.740088164806366, validation/num_examples=5348, validation/wer=0.224644
I0214 23:12:48.509372 140318069466880 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6146959662437439, loss=1.6188918352127075
I0214 23:14:05.504541 140318077859584 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7072887420654297, loss=1.6057281494140625
I0214 23:15:25.739555 140318069466880 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8878753185272217, loss=1.6760151386260986
I0214 23:16:56.442730 140318077859584 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6473897695541382, loss=1.6414943933486938
I0214 23:18:23.879712 140318069466880 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7163247466087341, loss=1.626686453819275
I0214 23:19:46.353156 140318077859584 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.695534884929657, loss=1.5956937074661255
I0214 23:21:05.302838 140318069466880 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7931658625602722, loss=1.572737693786621
I0214 23:22:23.045534 140318077859584 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5809848308563232, loss=1.6387916803359985
I0214 23:23:43.723628 140318069466880 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6445445418357849, loss=1.5391358137130737
I0214 23:25:11.712745 140318077859584 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8437666893005371, loss=1.608250379562378
I0214 23:26:40.844185 140318069466880 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5587358474731445, loss=1.578989863395691
I0214 23:28:08.862467 140318077859584 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.594143271446228, loss=1.5358824729919434
I0214 23:29:41.177204 140318069466880 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6434637904167175, loss=1.5579911470413208
I0214 23:31:12.494582 140318077859584 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6871382594108582, loss=1.5638163089752197
I0214 23:32:42.203455 140318069466880 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6619915962219238, loss=1.6175086498260498
I0214 23:34:07.232527 140318077859584 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7158822417259216, loss=1.4916267395019531
I0214 23:35:24.527518 140318069466880 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.809025228023529, loss=1.5376118421554565
I0214 23:36:01.894520 140399019657024 spec.py:321] Evaluating on the training split.
I0214 23:36:55.969492 140399019657024 spec.py:333] Evaluating on the validation split.
I0214 23:37:47.729807 140399019657024 spec.py:349] Evaluating on the test split.
I0214 23:38:14.684905 140399019657024 submission_runner.py:408] Time since start: 8024.50s, 	Step: 8450, 	{'train/ctc_loss': Array(0.49352032, dtype=float32), 'train/wer': 0.17106892623027514, 'validation/ctc_loss': Array(0.6651629, dtype=float32), 'validation/wer': 0.20105814997538063, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4210605, dtype=float32), 'test/wer': 0.14262791217272966, 'test/num_examples': 2472, 'score': 7236.972342252731, 'total_duration': 8024.497707366943, 'accumulated_submission_time': 7236.972342252731, 'accumulated_eval_time': 786.9121985435486, 'accumulated_logging_time': 0.2403411865234375}
I0214 23:38:14.721290 140318077859584 logging_writer.py:48] [8450] accumulated_eval_time=786.912199, accumulated_logging_time=0.240341, accumulated_submission_time=7236.972342, global_step=8450, preemption_count=0, score=7236.972342, test/ctc_loss=0.4210605025291443, test/num_examples=2472, test/wer=0.142628, total_duration=8024.497707, train/ctc_loss=0.4935203194618225, train/wer=0.171069, validation/ctc_loss=0.6651629209518433, validation/num_examples=5348, validation/wer=0.201058
I0214 23:38:53.911885 140318069466880 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5791571140289307, loss=1.5627813339233398
I0214 23:40:10.458531 140318077859584 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6594583988189697, loss=1.557826280593872
I0214 23:41:27.212857 140318069466880 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6456313133239746, loss=1.536027193069458
I0214 23:42:52.779783 140318077859584 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7480075359344482, loss=1.5876312255859375
I0214 23:44:21.432344 140318069466880 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6192821860313416, loss=1.5321160554885864
I0214 23:45:50.997894 140318077859584 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6413133144378662, loss=1.5383702516555786
I0214 23:47:19.672235 140318069466880 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5799463987350464, loss=1.520531415939331
I0214 23:48:51.135078 140318077859584 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6044724583625793, loss=1.4993400573730469
I0214 23:50:20.764808 140318077859584 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5766881108283997, loss=1.4801340103149414
I0214 23:51:37.711854 140318069466880 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7715793251991272, loss=1.4726605415344238
I0214 23:52:54.628313 140318077859584 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7179312705993652, loss=1.5282732248306274
I0214 23:54:14.780329 140318069466880 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6554887890815735, loss=1.4908275604248047
I0214 23:55:38.915025 140318077859584 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5683542490005493, loss=1.5435113906860352
I0214 23:57:06.067006 140318069466880 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6083745360374451, loss=1.5138225555419922
I0214 23:58:32.536031 140318077859584 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7902871370315552, loss=1.4380199909210205
I0215 00:00:01.606843 140318069466880 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6530944108963013, loss=1.4586806297302246
I0215 00:01:33.179225 140318077859584 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6256141066551208, loss=1.4871912002563477
I0215 00:02:14.963868 140399019657024 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0215 00:03:36.070751 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 00:04:29.174124 140399019657024 spec.py:349] Evaluating on the test split.
I0215 00:04:55.870134 140399019657024 submission_runner.py:408] Time since start: 9625.68s, 	Step: 10149, 	{'train/ctc_loss': Array(0.29691428, dtype=float32), 'train/wer': 0.11104616561809216, 'validation/ctc_loss': Array(0.6043157, dtype=float32), 'validation/wer': 0.18619963891597555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3722067, dtype=float32), 'test/wer': 0.12629740214896512, 'test/num_examples': 2472, 'score': 8677.129787445068, 'total_duration': 9625.68328166008, 'accumulated_submission_time': 8677.129787445068, 'accumulated_eval_time': 947.8132953643799, 'accumulated_logging_time': 0.2935667037963867}
I0215 00:04:55.902222 140318077859584 logging_writer.py:48] [10149] accumulated_eval_time=947.813295, accumulated_logging_time=0.293567, accumulated_submission_time=8677.129787, global_step=10149, preemption_count=0, score=8677.129787, test/ctc_loss=0.3722066879272461, test/num_examples=2472, test/wer=0.126297, total_duration=9625.683282, train/ctc_loss=0.296914279460907, train/wer=0.111046, validation/ctc_loss=0.6043156981468201, validation/num_examples=5348, validation/wer=0.186200
I0215 00:05:35.499083 140318069466880 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6532445549964905, loss=1.5412415266036987
I0215 00:06:55.845572 140318077859584 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6393014788627625, loss=1.4745161533355713
I0215 00:08:12.717550 140318069466880 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6540030837059021, loss=1.460999608039856
I0215 00:09:29.766912 140318077859584 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7166895866394043, loss=1.4056190252304077
I0215 00:10:48.367152 140318069466880 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6118433475494385, loss=1.4414573907852173
I0215 00:12:14.220394 140318077859584 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6226298809051514, loss=1.50558602809906
I0215 00:13:38.634998 140318069466880 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6062005758285522, loss=1.4393846988677979
I0215 00:15:08.676706 140318077859584 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6869046688079834, loss=1.4776208400726318
I0215 00:16:36.552179 140318069466880 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6156816482543945, loss=1.3880268335342407
I0215 00:18:06.289441 140318077859584 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6046046018600464, loss=1.4202390909194946
I0215 00:19:35.384949 140318069466880 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6326717734336853, loss=1.4322031736373901
I0215 00:21:00.386371 140318077859584 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7399338483810425, loss=1.4541325569152832
I0215 00:22:23.436994 140318077859584 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7022589445114136, loss=1.4272041320800781
I0215 00:23:41.864130 140318069466880 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6236221194267273, loss=1.3986022472381592
I0215 00:25:04.000429 140318077859584 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6560986042022705, loss=1.4002331495285034
I0215 00:26:25.117049 140318069466880 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6523135900497437, loss=1.499404788017273
I0215 00:27:50.277614 140318077859584 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5571064352989197, loss=1.3949061632156372
I0215 00:28:56.601874 140399019657024 spec.py:321] Evaluating on the training split.
I0215 00:29:52.780771 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 00:30:48.085858 140399019657024 spec.py:349] Evaluating on the test split.
I0215 00:31:15.181158 140399019657024 submission_runner.py:408] Time since start: 11204.99s, 	Step: 11875, 	{'train/ctc_loss': Array(0.27041242, dtype=float32), 'train/wer': 0.09958803696757804, 'validation/ctc_loss': Array(0.57668215, dtype=float32), 'validation/wer': 0.17535746352954806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35247084, dtype=float32), 'test/wer': 0.1216257388337091, 'test/num_examples': 2472, 'score': 10117.744185447693, 'total_duration': 11204.992297172546, 'accumulated_submission_time': 10117.744185447693, 'accumulated_eval_time': 1086.3854405879974, 'accumulated_logging_time': 0.3410470485687256}
I0215 00:31:15.217210 140318077859584 logging_writer.py:48] [11875] accumulated_eval_time=1086.385441, accumulated_logging_time=0.341047, accumulated_submission_time=10117.744185, global_step=11875, preemption_count=0, score=10117.744185, test/ctc_loss=0.35247084498405457, test/num_examples=2472, test/wer=0.121626, total_duration=11204.992297, train/ctc_loss=0.270412415266037, train/wer=0.099588, validation/ctc_loss=0.5766821503639221, validation/num_examples=5348, validation/wer=0.175357
I0215 00:31:35.097423 140318069466880 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5649073719978333, loss=1.42826509475708
I0215 00:32:51.743531 140318077859584 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7619122266769409, loss=1.4007656574249268
I0215 00:34:10.211318 140318069466880 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6506183743476868, loss=1.4266341924667358
I0215 00:35:37.412314 140318077859584 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6069251894950867, loss=1.420901894569397
I0215 00:37:07.999511 140318069466880 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7160285115242004, loss=1.4044089317321777
I0215 00:38:36.844449 140318077859584 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5581914782524109, loss=1.385873794555664
I0215 00:39:53.998546 140318069466880 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6287034749984741, loss=1.4071937799453735
I0215 00:41:11.058681 140318077859584 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7285196185112, loss=1.4084707498550415
I0215 00:42:35.055464 140318069466880 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.624608039855957, loss=1.4360862970352173
I0215 00:44:00.338163 140318077859584 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6814197301864624, loss=1.402243733406067
I0215 00:45:27.846321 140318069466880 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5652175545692444, loss=1.3622941970825195
I0215 00:46:55.417694 140318077859584 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5772730708122253, loss=1.4098708629608154
I0215 00:48:28.703216 140318069466880 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6490570902824402, loss=1.3943848609924316
I0215 00:49:58.174483 140318077859584 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5901668667793274, loss=1.4157342910766602
I0215 00:51:28.784964 140318069466880 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6099916100502014, loss=1.3607027530670166
I0215 00:53:01.215446 140318077859584 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.545141875743866, loss=1.3545047044754028
I0215 00:54:18.399145 140318069466880 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5584402084350586, loss=1.3736859560012817
I0215 00:55:15.393815 140399019657024 spec.py:321] Evaluating on the training split.
I0215 00:56:10.750314 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 00:57:03.525619 140399019657024 spec.py:349] Evaluating on the test split.
I0215 00:57:31.228666 140399019657024 submission_runner.py:408] Time since start: 12781.04s, 	Step: 13572, 	{'train/ctc_loss': Array(0.26092592, dtype=float32), 'train/wer': 0.0966991728111208, 'validation/ctc_loss': Array(0.54714733, dtype=float32), 'validation/wer': 0.1656352278980855, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33199945, dtype=float32), 'test/wer': 0.11339954908293218, 'test/num_examples': 2472, 'score': 11557.835375070572, 'total_duration': 12781.041088342667, 'accumulated_submission_time': 11557.835375070572, 'accumulated_eval_time': 1222.2144269943237, 'accumulated_logging_time': 0.39313745498657227}
I0215 00:57:31.268985 140318077859584 logging_writer.py:48] [13572] accumulated_eval_time=1222.214427, accumulated_logging_time=0.393137, accumulated_submission_time=11557.835375, global_step=13572, preemption_count=0, score=11557.835375, test/ctc_loss=0.33199945092201233, test/num_examples=2472, test/wer=0.113400, total_duration=12781.041088, train/ctc_loss=0.26092591881752014, train/wer=0.096699, validation/ctc_loss=0.5471473336219788, validation/num_examples=5348, validation/wer=0.165635
I0215 00:57:53.454805 140318069466880 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6274126172065735, loss=1.3994710445404053
I0215 00:59:09.991520 140318077859584 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6681174039840698, loss=1.3845901489257812
I0215 01:00:26.830513 140318069466880 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5459533929824829, loss=1.3785340785980225
I0215 01:01:52.663614 140318077859584 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.556720495223999, loss=1.4035288095474243
I0215 01:03:24.899263 140318069466880 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7021304965019226, loss=1.3173919916152954
I0215 01:04:53.454765 140318077859584 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6478960514068604, loss=1.4116461277008057
I0215 01:06:26.628815 140318069466880 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6285641193389893, loss=1.3357642889022827
I0215 01:07:56.424063 140318077859584 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5339750647544861, loss=1.395058274269104
I0215 01:09:25.734526 140318069466880 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6511242389678955, loss=1.4009284973144531
I0215 01:10:51.277040 140318077859584 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8016378283500671, loss=1.393634557723999
I0215 01:12:09.086969 140318069466880 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.688715934753418, loss=1.3364871740341187
I0215 01:13:31.285549 140318077859584 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5887088775634766, loss=1.3671766519546509
I0215 01:14:58.341682 140318069466880 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7282829284667969, loss=1.3544836044311523
I0215 01:16:25.891536 140318077859584 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.5472930669784546, loss=1.3281714916229248
I0215 01:17:56.326372 140318069466880 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5988735556602478, loss=1.347278356552124
I0215 01:19:26.783408 140318077859584 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6436053514480591, loss=1.3250184059143066
I0215 01:20:59.521069 140318069466880 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6297594308853149, loss=1.3997645378112793
I0215 01:21:31.235164 140399019657024 spec.py:321] Evaluating on the training split.
I0215 01:22:27.667101 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 01:23:22.548021 140399019657024 spec.py:349] Evaluating on the test split.
I0215 01:23:49.208004 140399019657024 submission_runner.py:408] Time since start: 14359.02s, 	Step: 15238, 	{'train/ctc_loss': Array(0.2348248, dtype=float32), 'train/wer': 0.08926558978211871, 'validation/ctc_loss': Array(0.5383248, dtype=float32), 'validation/wer': 0.16437046834721994, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3189678, dtype=float32), 'test/wer': 0.11104340584567261, 'test/num_examples': 2472, 'score': 12997.719169139862, 'total_duration': 14359.018620729446, 'accumulated_submission_time': 12997.719169139862, 'accumulated_eval_time': 1360.179630279541, 'accumulated_logging_time': 0.4491429328918457}
I0215 01:23:49.243670 140318077859584 logging_writer.py:48] [15238] accumulated_eval_time=1360.179630, accumulated_logging_time=0.449143, accumulated_submission_time=12997.719169, global_step=15238, preemption_count=0, score=12997.719169, test/ctc_loss=0.3189677894115448, test/num_examples=2472, test/wer=0.111043, total_duration=14359.018621, train/ctc_loss=0.23482480645179749, train/wer=0.089266, validation/ctc_loss=0.538324773311615, validation/num_examples=5348, validation/wer=0.164370
I0215 01:24:37.488129 140318069466880 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7665472030639648, loss=1.3755332231521606
I0215 01:25:53.966681 140318077859584 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7130540013313293, loss=1.3771647214889526
I0215 01:27:16.227438 140318077859584 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.5453541874885559, loss=1.3220245838165283
I0215 01:28:36.540279 140318069466880 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5486115217208862, loss=1.3868619203567505
I0215 01:29:55.539681 140318077859584 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5812245011329651, loss=1.3536269664764404
I0215 01:31:17.399332 140318069466880 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8608642816543579, loss=1.3522238731384277
I0215 01:32:44.849292 140318077859584 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5031298398971558, loss=1.3149833679199219
I0215 01:34:14.627962 140318069466880 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6011531352996826, loss=1.3330752849578857
I0215 01:35:47.611930 140318077859584 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5234991312026978, loss=1.3476723432540894
I0215 01:37:17.940399 140318069466880 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6813105344772339, loss=1.328158974647522
I0215 01:38:45.733645 140318077859584 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.4925764799118042, loss=1.3638485670089722
I0215 01:40:13.302237 140318069466880 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5056812763214111, loss=1.337511420249939
I0215 01:41:43.872393 140318077859584 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6346429586410522, loss=1.3216807842254639
I0215 01:43:00.385512 140318069466880 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5463924407958984, loss=1.2987278699874878
I0215 01:44:19.567186 140318077859584 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5398378968238831, loss=1.3289690017700195
I0215 01:45:42.236558 140318069466880 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.720730721950531, loss=1.3266452550888062
I0215 01:47:06.083599 140318077859584 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5923729538917542, loss=1.3376781940460205
I0215 01:47:49.764689 140399019657024 spec.py:321] Evaluating on the training split.
I0215 01:48:45.799787 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 01:49:38.933017 140399019657024 spec.py:349] Evaluating on the test split.
I0215 01:50:07.242662 140399019657024 submission_runner.py:408] Time since start: 15937.06s, 	Step: 16950, 	{'train/ctc_loss': Array(0.22629037, dtype=float32), 'train/wer': 0.0866338613111835, 'validation/ctc_loss': Array(0.51311344, dtype=float32), 'validation/wer': 0.15726464369502882, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30607492, dtype=float32), 'test/wer': 0.10618893831373266, 'test/num_examples': 2472, 'score': 14438.15297293663, 'total_duration': 15937.055542230606, 'accumulated_submission_time': 14438.15297293663, 'accumulated_eval_time': 1497.6522102355957, 'accumulated_logging_time': 0.5024521350860596}
I0215 01:50:07.278316 140318077859584 logging_writer.py:48] [16950] accumulated_eval_time=1497.652210, accumulated_logging_time=0.502452, accumulated_submission_time=14438.152973, global_step=16950, preemption_count=0, score=14438.152973, test/ctc_loss=0.30607491731643677, test/num_examples=2472, test/wer=0.106189, total_duration=15937.055542, train/ctc_loss=0.22629037499427795, train/wer=0.086634, validation/ctc_loss=0.5131134390830994, validation/num_examples=5348, validation/wer=0.157265
I0215 01:50:46.549175 140318069466880 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8076064586639404, loss=1.3715914487838745
I0215 01:52:03.167996 140318077859584 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6021768450737, loss=1.3600492477416992
I0215 01:53:20.215523 140318069466880 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.582601010799408, loss=1.3481451272964478
I0215 01:54:46.738483 140318077859584 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6801526546478271, loss=1.3393031358718872
I0215 01:56:14.912870 140318069466880 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6545800566673279, loss=1.3718408346176147
I0215 01:57:42.683191 140318077859584 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5853484869003296, loss=1.3642486333847046
I0215 01:59:05.441361 140318077859584 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5788617730140686, loss=1.2886461019515991
I0215 02:00:23.983437 140318069466880 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6192957758903503, loss=1.3574358224868774
I0215 02:01:46.276985 140318077859584 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5698546171188354, loss=1.2958022356033325
I0215 02:03:09.159991 140318069466880 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6192912459373474, loss=1.3773514032363892
I0215 02:04:38.988664 140318077859584 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.567866861820221, loss=1.2702070474624634
I0215 02:06:11.232686 140318069466880 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5205221772193909, loss=1.2757800817489624
I0215 02:07:39.272345 140318077859584 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5342163443565369, loss=1.3053230047225952
I0215 02:09:08.613892 140318069466880 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6188620924949646, loss=1.3006126880645752
I0215 02:10:41.747347 140318077859584 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.614208459854126, loss=1.3554970026016235
I0215 02:12:10.042304 140318069466880 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7695962190628052, loss=1.3659344911575317
I0215 02:13:37.257325 140318077859584 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7727637887001038, loss=1.3018149137496948
I0215 02:14:07.898122 140399019657024 spec.py:321] Evaluating on the training split.
I0215 02:15:03.214338 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 02:15:55.564234 140399019657024 spec.py:349] Evaluating on the test split.
I0215 02:16:22.764717 140399019657024 submission_runner.py:408] Time since start: 17512.58s, 	Step: 18641, 	{'train/ctc_loss': Array(0.20851934, dtype=float32), 'train/wer': 0.08079113552095911, 'validation/ctc_loss': Array(0.4994007, dtype=float32), 'validation/wer': 0.15180976471610494, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2961128, dtype=float32), 'test/wer': 0.10027826864095221, 'test/num_examples': 2472, 'score': 15878.68928694725, 'total_duration': 17512.57652759552, 'accumulated_submission_time': 15878.68928694725, 'accumulated_eval_time': 1632.5123000144958, 'accumulated_logging_time': 0.5541045665740967}
I0215 02:16:22.802425 140318077859584 logging_writer.py:48] [18641] accumulated_eval_time=1632.512300, accumulated_logging_time=0.554105, accumulated_submission_time=15878.689287, global_step=18641, preemption_count=0, score=15878.689287, test/ctc_loss=0.2961128056049347, test/num_examples=2472, test/wer=0.100278, total_duration=17512.576528, train/ctc_loss=0.2085193395614624, train/wer=0.080791, validation/ctc_loss=0.49940070509910583, validation/num_examples=5348, validation/wer=0.151810
I0215 02:17:08.529109 140318069466880 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6704474091529846, loss=1.2555433511734009
I0215 02:18:25.365875 140318077859584 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6259558796882629, loss=1.322767734527588
I0215 02:19:42.190824 140318069466880 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.723837673664093, loss=1.2749602794647217
I0215 02:21:01.746908 140318077859584 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6157892346382141, loss=1.2956901788711548
I0215 02:22:30.145016 140318069466880 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6066767573356628, loss=1.2998424768447876
I0215 02:24:01.288570 140318077859584 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7298597097396851, loss=1.2458865642547607
I0215 02:25:28.887192 140318069466880 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5688266754150391, loss=1.3450208902359009
I0215 02:26:59.635696 140318077859584 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5793768167495728, loss=1.3078038692474365
I0215 02:28:30.228360 140318069466880 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5384697914123535, loss=1.2739390134811401
I0215 02:30:00.692001 140318077859584 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5538669228553772, loss=1.2699943780899048
I0215 02:31:19.310260 140318069466880 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6259413361549377, loss=1.2719082832336426
I0215 02:32:38.510519 140318077859584 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5669018030166626, loss=1.2547181844711304
I0215 02:34:00.894824 140318069466880 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5205289125442505, loss=1.251676321029663
I0215 02:35:27.187970 140318077859584 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6761798858642578, loss=1.2924630641937256
I0215 02:36:58.596731 140318069466880 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5658971667289734, loss=1.2704282999038696
I0215 02:38:28.540328 140318077859584 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.5586851835250854, loss=1.2961221933364868
I0215 02:39:59.109753 140318069466880 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6640005707740784, loss=1.298548936843872
I0215 02:40:23.330433 140399019657024 spec.py:321] Evaluating on the training split.
I0215 02:41:19.133430 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 02:42:12.566436 140399019657024 spec.py:349] Evaluating on the test split.
I0215 02:42:39.698477 140399019657024 submission_runner.py:408] Time since start: 19089.51s, 	Step: 20329, 	{'train/ctc_loss': Array(0.22626519, dtype=float32), 'train/wer': 0.08245879474633015, 'validation/ctc_loss': Array(0.4777417, dtype=float32), 'validation/wer': 0.14654797879838188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28475207, dtype=float32), 'test/wer': 0.09780025592590336, 'test/num_examples': 2472, 'score': 17319.13134288788, 'total_duration': 19089.508982419968, 'accumulated_submission_time': 17319.13134288788, 'accumulated_eval_time': 1768.8725280761719, 'accumulated_logging_time': 0.6105937957763672}
I0215 02:42:39.737010 140318077859584 logging_writer.py:48] [20329] accumulated_eval_time=1768.872528, accumulated_logging_time=0.610594, accumulated_submission_time=17319.131343, global_step=20329, preemption_count=0, score=17319.131343, test/ctc_loss=0.2847520709037781, test/num_examples=2472, test/wer=0.097800, total_duration=19089.508982, train/ctc_loss=0.22626519203186035, train/wer=0.082459, validation/ctc_loss=0.47774168848991394, validation/num_examples=5348, validation/wer=0.146548
I0215 02:43:34.848511 140318069466880 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.5496190190315247, loss=1.2894028425216675
I0215 02:44:51.370087 140318077859584 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5486974120140076, loss=1.325713038444519
I0215 02:46:16.291407 140318077859584 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6151789426803589, loss=1.3026717901229858
I0215 02:47:33.847477 140318069466880 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6362324953079224, loss=1.2388824224472046
I0215 02:48:55.215297 140318077859584 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5757290124893188, loss=1.2504686117172241
I0215 02:50:14.181596 140318069466880 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6090482473373413, loss=1.26429283618927
I0215 02:51:38.935097 140318077859584 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5963753461837769, loss=1.300038456916809
I0215 02:53:08.572052 140318069466880 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5901792645454407, loss=1.2320785522460938
I0215 02:54:40.591903 140318077859584 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6237740516662598, loss=1.3070861101150513
I0215 02:56:11.708390 140318069466880 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6361007690429688, loss=1.323804259300232
I0215 02:57:44.654952 140318077859584 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.5523571372032166, loss=1.2577519416809082
I0215 02:59:17.892796 140318069466880 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7356838583946228, loss=1.3264353275299072
I0215 03:00:46.900186 140318077859584 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6126459836959839, loss=1.1979089975357056
I0215 03:02:11.168161 140318077859584 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.49698761105537415, loss=1.2178062200546265
I0215 03:03:30.327377 140318069466880 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6814865469932556, loss=1.246323823928833
I0215 03:04:50.069490 140318077859584 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6697343587875366, loss=1.2832486629486084
I0215 03:06:12.114098 140318069466880 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.652657151222229, loss=1.2249691486358643
I0215 03:06:40.375064 140399019657024 spec.py:321] Evaluating on the training split.
I0215 03:07:35.561063 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 03:08:27.793176 140399019657024 spec.py:349] Evaluating on the test split.
I0215 03:08:54.768515 140399019657024 submission_runner.py:408] Time since start: 20664.58s, 	Step: 22034, 	{'train/ctc_loss': Array(0.20196249, dtype=float32), 'train/wer': 0.078254877014419, 'validation/ctc_loss': Array(0.47147408, dtype=float32), 'validation/wer': 0.1439701864313506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27821675, dtype=float32), 'test/wer': 0.09503788109601284, 'test/num_examples': 2472, 'score': 18759.682195663452, 'total_duration': 20664.581153154373, 'accumulated_submission_time': 18759.682195663452, 'accumulated_eval_time': 1903.2603197097778, 'accumulated_logging_time': 0.6675965785980225}
I0215 03:08:54.809849 140318077859584 logging_writer.py:48] [22034] accumulated_eval_time=1903.260320, accumulated_logging_time=0.667597, accumulated_submission_time=18759.682196, global_step=22034, preemption_count=0, score=18759.682196, test/ctc_loss=0.27821674942970276, test/num_examples=2472, test/wer=0.095038, total_duration=20664.581153, train/ctc_loss=0.20196248590946198, train/wer=0.078255, validation/ctc_loss=0.4714740812778473, validation/num_examples=5348, validation/wer=0.143970
I0215 03:09:46.335890 140318069466880 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5805971026420593, loss=1.2293646335601807
I0215 03:11:03.628141 140318077859584 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6592899560928345, loss=1.2613475322723389
I0215 03:12:22.612127 140318069466880 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6623836755752563, loss=1.263863444328308
I0215 03:13:50.957958 140318077859584 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4855347275733948, loss=1.209713101387024
I0215 03:15:20.845241 140318069466880 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5401884317398071, loss=1.238011360168457
I0215 03:16:49.503765 140318077859584 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6186894774436951, loss=1.234271764755249
I0215 03:18:18.452950 140318077859584 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5472319722175598, loss=1.212265133857727
I0215 03:19:35.894410 140318069466880 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6059436798095703, loss=1.2325778007507324
I0215 03:20:54.578281 140318077859584 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7606172561645508, loss=1.2521249055862427
I0215 03:22:15.974786 140318069466880 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6487554311752319, loss=1.2149337530136108
I0215 03:23:44.236183 140318077859584 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5627292990684509, loss=1.1984444856643677
I0215 03:25:11.977446 140318069466880 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.608025312423706, loss=1.1904102563858032
I0215 03:26:44.013501 140318077859584 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6645564436912537, loss=1.2379915714263916
I0215 03:28:17.042251 140318069466880 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5895131826400757, loss=1.2288808822631836
I0215 03:29:46.406736 140318077859584 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5152938365936279, loss=1.2475974559783936
I0215 03:31:16.802684 140318069466880 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5813013315200806, loss=1.2829335927963257
I0215 03:32:48.123960 140318077859584 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5856808423995972, loss=1.198237419128418
I0215 03:32:54.806952 140399019657024 spec.py:321] Evaluating on the training split.
I0215 03:33:50.546655 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 03:34:42.867340 140399019657024 spec.py:349] Evaluating on the test split.
I0215 03:35:09.247727 140399019657024 submission_runner.py:408] Time since start: 22239.06s, 	Step: 23710, 	{'train/ctc_loss': Array(0.16809237, dtype=float32), 'train/wer': 0.06681527265893358, 'validation/ctc_loss': Array(0.46198797, dtype=float32), 'validation/wer': 0.13954835532985121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26943246, dtype=float32), 'test/wer': 0.09207239046980684, 'test/num_examples': 2472, 'score': 20199.590856790543, 'total_duration': 22239.059130191803, 'accumulated_submission_time': 20199.590856790543, 'accumulated_eval_time': 2037.694198846817, 'accumulated_logging_time': 0.728062629699707}
I0215 03:35:09.287839 140318077859584 logging_writer.py:48] [23710] accumulated_eval_time=2037.694199, accumulated_logging_time=0.728063, accumulated_submission_time=20199.590857, global_step=23710, preemption_count=0, score=20199.590857, test/ctc_loss=0.2694324553012848, test/num_examples=2472, test/wer=0.092072, total_duration=22239.059130, train/ctc_loss=0.16809237003326416, train/wer=0.066815, validation/ctc_loss=0.4619879722595215, validation/num_examples=5348, validation/wer=0.139548
I0215 03:36:18.969571 140318069466880 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.6969749331474304, loss=1.2081137895584106
I0215 03:37:35.411204 140318077859584 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5174456238746643, loss=1.1583940982818604
I0215 03:38:51.818734 140318069466880 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7043508887290955, loss=1.2344223260879517
I0215 03:40:09.152639 140318077859584 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7880745530128479, loss=1.259700059890747
I0215 03:41:39.401607 140318069466880 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6363590359687805, loss=1.2532036304473877
I0215 03:43:10.540093 140318077859584 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.49207833409309387, loss=1.2014859914779663
I0215 03:44:40.805016 140318069466880 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6409050226211548, loss=1.2076855897903442
I0215 03:46:11.041818 140318077859584 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5567182898521423, loss=1.2122111320495605
I0215 03:47:42.071320 140318069466880 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5148727297782898, loss=1.2285887002944946
I0215 03:49:15.385814 140318077859584 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6092706918716431, loss=1.1529406309127808
I0215 03:50:39.514895 140318077859584 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5184392929077148, loss=1.1736111640930176
I0215 03:51:56.452167 140318069466880 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7512158155441284, loss=1.241682767868042
I0215 03:53:18.142156 140318077859584 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5687258839607239, loss=1.2057050466537476
I0215 03:54:44.232578 140318069466880 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6117996573448181, loss=1.2200698852539062
I0215 03:56:10.711582 140318077859584 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.5016045570373535, loss=1.2071117162704468
I0215 03:57:38.027248 140318069466880 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5530965924263, loss=1.1854289770126343
I0215 03:59:09.277829 140318077859584 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6139163374900818, loss=1.2098208665847778
I0215 03:59:09.287889 140399019657024 spec.py:321] Evaluating on the training split.
I0215 04:00:06.490024 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 04:00:58.440000 140399019657024 spec.py:349] Evaluating on the test split.
I0215 04:01:25.581499 140399019657024 submission_runner.py:408] Time since start: 23815.39s, 	Step: 25401, 	{'train/ctc_loss': Array(0.1630345, dtype=float32), 'train/wer': 0.06337915430425409, 'validation/ctc_loss': Array(0.4471465, dtype=float32), 'validation/wer': 0.13672919663631888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26178136, dtype=float32), 'test/wer': 0.0884163061361282, 'test/num_examples': 2472, 'score': 21639.50814318657, 'total_duration': 23815.393007278442, 'accumulated_submission_time': 21639.50814318657, 'accumulated_eval_time': 2173.9809906482697, 'accumulated_logging_time': 0.7831501960754395}
I0215 04:01:25.619078 140318077859584 logging_writer.py:48] [25401] accumulated_eval_time=2173.980991, accumulated_logging_time=0.783150, accumulated_submission_time=21639.508143, global_step=25401, preemption_count=0, score=21639.508143, test/ctc_loss=0.26178136467933655, test/num_examples=2472, test/wer=0.088416, total_duration=23815.393007, train/ctc_loss=0.16303449869155884, train/wer=0.063379, validation/ctc_loss=0.4471465051174164, validation/num_examples=5348, validation/wer=0.136729
I0215 04:02:42.243019 140318069466880 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9166492819786072, loss=1.2235790491104126
I0215 04:03:59.103393 140318077859584 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6337612867355347, loss=1.2194550037384033
I0215 04:05:25.753373 140318069466880 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8079302906990051, loss=1.251444935798645
I0215 04:06:53.867253 140318077859584 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6179318428039551, loss=1.1342788934707642
I0215 04:08:11.967352 140318069466880 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6445990800857544, loss=1.2269030809402466
I0215 04:09:31.930777 140318077859584 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7441360950469971, loss=1.218714952468872
I0215 04:10:56.064992 140318069466880 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6263691186904907, loss=1.2148691415786743
I0215 04:12:22.834818 140318077859584 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.532058835029602, loss=1.1701549291610718
I0215 04:13:48.039082 140318069466880 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5953626036643982, loss=1.2562576532363892
I0215 04:15:17.724176 140318077859584 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.582611083984375, loss=1.2286120653152466
I0215 04:16:47.991637 140318069466880 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6505217552185059, loss=1.1926922798156738
I0215 04:18:16.835409 140318077859584 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5224595069885254, loss=1.1947323083877563
I0215 04:19:46.348459 140318069466880 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8191063404083252, loss=1.2239328622817993
I0215 04:21:15.846491 140318077859584 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.533758282661438, loss=1.178704023361206
I0215 04:22:33.854839 140318069466880 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.507439911365509, loss=1.2026995420455933
I0215 04:23:55.962823 140318077859584 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6644409894943237, loss=1.1399754285812378
I0215 04:25:20.685048 140318069466880 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7891465425491333, loss=1.1951346397399902
I0215 04:25:25.805614 140399019657024 spec.py:321] Evaluating on the training split.
I0215 04:26:20.828637 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 04:27:14.474100 140399019657024 spec.py:349] Evaluating on the test split.
I0215 04:27:41.260305 140399019657024 submission_runner.py:408] Time since start: 25391.07s, 	Step: 27108, 	{'train/ctc_loss': Array(0.15664186, dtype=float32), 'train/wer': 0.06248922878069797, 'validation/ctc_loss': Array(0.43819365, dtype=float32), 'validation/wer': 0.13243287602460005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25654113, dtype=float32), 'test/wer': 0.08666951028781508, 'test/num_examples': 2472, 'score': 23079.609487771988, 'total_duration': 25391.072680950165, 'accumulated_submission_time': 23079.609487771988, 'accumulated_eval_time': 2309.429753303528, 'accumulated_logging_time': 0.8377275466918945}
I0215 04:27:41.299907 140318077859584 logging_writer.py:48] [27108] accumulated_eval_time=2309.429753, accumulated_logging_time=0.837728, accumulated_submission_time=23079.609488, global_step=27108, preemption_count=0, score=23079.609488, test/ctc_loss=0.2565411329269409, test/num_examples=2472, test/wer=0.086670, total_duration=25391.072681, train/ctc_loss=0.1566418558359146, train/wer=0.062489, validation/ctc_loss=0.4381936490535736, validation/num_examples=5348, validation/wer=0.132433
I0215 04:28:52.355111 140318069466880 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5659081339836121, loss=1.1772429943084717
I0215 04:30:09.094000 140318077859584 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6738590002059937, loss=1.2023680210113525
I0215 04:31:33.124031 140318069466880 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0111292600631714, loss=1.1681674718856812
I0215 04:33:02.855887 140318077859584 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7704409956932068, loss=1.206737995147705
I0215 04:34:31.277280 140318069466880 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.512262225151062, loss=1.1551685333251953
I0215 04:36:02.749945 140318077859584 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5687717795372009, loss=1.1914198398590088
I0215 04:37:32.310843 140318069466880 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5744908452033997, loss=1.1566071510314941
I0215 04:38:54.242808 140318077859584 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.580592930316925, loss=1.2583866119384766
I0215 04:40:14.284774 140318069466880 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.643100380897522, loss=1.194828748703003
I0215 04:41:37.200978 140318077859584 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.5906961560249329, loss=1.1785674095153809
I0215 04:43:04.860888 140318069466880 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6205371022224426, loss=1.1583547592163086
I0215 04:44:34.427474 140318077859584 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5870755314826965, loss=1.1656395196914673
I0215 04:46:07.661616 140318069466880 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.595537543296814, loss=1.1342791318893433
I0215 04:47:38.535754 140318077859584 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5762792825698853, loss=1.1292073726654053
I0215 04:49:09.426517 140318069466880 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6509549021720886, loss=1.1859451532363892
I0215 04:50:39.354760 140318077859584 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.8833521604537964, loss=1.1758180856704712
I0215 04:51:41.254813 140399019657024 spec.py:321] Evaluating on the training split.
I0215 04:52:36.840569 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 04:53:29.508491 140399019657024 spec.py:349] Evaluating on the test split.
I0215 04:53:57.013883 140399019657024 submission_runner.py:408] Time since start: 26966.82s, 	Step: 28772, 	{'train/ctc_loss': Array(0.15958466, dtype=float32), 'train/wer': 0.061006696685088645, 'validation/ctc_loss': Array(0.42708495, dtype=float32), 'validation/wer': 0.12850343222916286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24707821, dtype=float32), 'test/wer': 0.08382588913939837, 'test/num_examples': 2472, 'score': 24519.481394052505, 'total_duration': 26966.824649333954, 'accumulated_submission_time': 24519.481394052505, 'accumulated_eval_time': 2445.1812682151794, 'accumulated_logging_time': 0.892916202545166}
I0215 04:53:57.053491 140318077859584 logging_writer.py:48] [28772] accumulated_eval_time=2445.181268, accumulated_logging_time=0.892916, accumulated_submission_time=24519.481394, global_step=28772, preemption_count=0, score=24519.481394, test/ctc_loss=0.24707821011543274, test/num_examples=2472, test/wer=0.083826, total_duration=26966.824649, train/ctc_loss=0.1595846563577652, train/wer=0.061007, validation/ctc_loss=0.42708495259284973, validation/num_examples=5348, validation/wer=0.128503
I0215 04:54:19.434700 140318069466880 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7151203155517578, loss=1.1603480577468872
I0215 04:55:41.512380 140318077859584 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.7451648712158203, loss=1.1311615705490112
I0215 04:57:00.667176 140318069466880 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6235782504081726, loss=1.1933850049972534
I0215 04:58:19.685009 140318077859584 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.649257481098175, loss=1.162980079650879
I0215 04:59:43.029629 140318069466880 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5272591710090637, loss=1.1639025211334229
I0215 05:01:11.407423 140318077859584 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6746460795402527, loss=1.1353309154510498
I0215 05:02:41.625646 140318069466880 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.6322565078735352, loss=1.1438570022583008
I0215 05:04:14.555470 140318077859584 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9013431072235107, loss=1.1513458490371704
I0215 05:05:44.697012 140318069466880 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5815827250480652, loss=1.2165449857711792
I0215 05:07:13.665944 140318077859584 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5478888154029846, loss=1.186950922012329
I0215 05:08:41.810674 140318069466880 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5589346289634705, loss=1.2185171842575073
I0215 05:10:11.383876 140318077859584 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6255772709846497, loss=1.1289464235305786
I0215 05:11:28.260283 140318069466880 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.5170186758041382, loss=1.1530284881591797
I0215 05:12:45.877682 140318077859584 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6018232703208923, loss=1.1679840087890625
I0215 05:14:05.799531 140318069466880 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.5793225765228271, loss=1.162077784538269
I0215 05:15:30.442190 140318077859584 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.6026954054832458, loss=1.123415231704712
I0215 05:17:01.795134 140318069466880 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.6150431632995605, loss=1.145140528678894
I0215 05:17:57.141302 140399019657024 spec.py:321] Evaluating on the training split.
I0215 05:18:53.194485 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 05:19:46.283475 140399019657024 spec.py:349] Evaluating on the test split.
I0215 05:20:13.927436 140399019657024 submission_runner.py:408] Time since start: 28543.74s, 	Step: 30461, 	{'train/ctc_loss': Array(0.15726769, dtype=float32), 'train/wer': 0.0611525223303841, 'validation/ctc_loss': Array(0.43288928, dtype=float32), 'validation/wer': 0.1284261950046825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24584797, dtype=float32), 'test/wer': 0.08362277334308289, 'test/num_examples': 2472, 'score': 25959.484798431396, 'total_duration': 28543.738730192184, 'accumulated_submission_time': 25959.484798431396, 'accumulated_eval_time': 2581.9604048728943, 'accumulated_logging_time': 0.948739767074585}
I0215 05:20:13.965816 140318077859584 logging_writer.py:48] [30461] accumulated_eval_time=2581.960405, accumulated_logging_time=0.948740, accumulated_submission_time=25959.484798, global_step=30461, preemption_count=0, score=25959.484798, test/ctc_loss=0.24584797024726868, test/num_examples=2472, test/wer=0.083623, total_duration=28543.738730, train/ctc_loss=0.15726768970489502, train/wer=0.061153, validation/ctc_loss=0.43288928270339966, validation/num_examples=5348, validation/wer=0.128426
I0215 05:20:44.437957 140318069466880 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6186425089836121, loss=1.1493138074874878
I0215 05:22:01.400331 140318077859584 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.5884019732475281, loss=1.1788854598999023
I0215 05:23:18.051546 140318069466880 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6125967502593994, loss=1.1656160354614258
I0215 05:24:47.961159 140318077859584 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.7471274137496948, loss=1.1678577661514282
I0215 05:26:18.306458 140318077859584 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5579226016998291, loss=1.0987516641616821
I0215 05:27:36.513565 140318069466880 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6676721572875977, loss=1.1333532333374023
I0215 05:28:54.227905 140318077859584 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.572510302066803, loss=1.133091926574707
I0215 05:30:16.494647 140318069466880 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.5768108367919922, loss=1.111146092414856
I0215 05:31:41.396390 140318077859584 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7054029703140259, loss=1.1568520069122314
I0215 05:33:09.440003 140318069466880 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6264090538024902, loss=1.1471418142318726
I0215 05:34:36.938927 140318077859584 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6106365323066711, loss=1.1895813941955566
I0215 05:36:05.718754 140318069466880 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.706859290599823, loss=1.1523336172103882
I0215 05:37:35.698935 140318077859584 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.5767804384231567, loss=1.1745308637619019
I0215 05:39:04.185363 140318069466880 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6632310152053833, loss=1.1571202278137207
I0215 05:40:30.717676 140318077859584 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.5986566543579102, loss=1.1194850206375122
I0215 05:41:53.718938 140318077859584 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6241698861122131, loss=1.1710443496704102
I0215 05:43:10.743868 140318069466880 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.5689603090286255, loss=1.1676713228225708
I0215 05:44:14.200226 140399019657024 spec.py:321] Evaluating on the training split.
I0215 05:45:10.676804 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 05:46:04.271859 140399019657024 spec.py:349] Evaluating on the test split.
I0215 05:46:31.241870 140399019657024 submission_runner.py:408] Time since start: 30121.05s, 	Step: 32179, 	{'train/ctc_loss': Array(0.14783192, dtype=float32), 'train/wer': 0.059060555431988286, 'validation/ctc_loss': Array(0.42411798, dtype=float32), 'validation/wer': 0.12668835745387488, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23941892, dtype=float32), 'test/wer': 0.08090102167245547, 'test/num_examples': 2472, 'score': 27399.637142419815, 'total_duration': 30121.05331158638, 'accumulated_submission_time': 27399.637142419815, 'accumulated_eval_time': 2718.995190382004, 'accumulated_logging_time': 1.0019807815551758}
I0215 05:46:31.281261 140318077859584 logging_writer.py:48] [32179] accumulated_eval_time=2718.995190, accumulated_logging_time=1.001981, accumulated_submission_time=27399.637142, global_step=32179, preemption_count=0, score=27399.637142, test/ctc_loss=0.23941892385482788, test/num_examples=2472, test/wer=0.080901, total_duration=30121.053312, train/ctc_loss=0.14783191680908203, train/wer=0.059061, validation/ctc_loss=0.4241179823875427, validation/num_examples=5348, validation/wer=0.126688
I0215 05:46:48.081354 140318069466880 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5602403283119202, loss=1.1009678840637207
I0215 05:48:05.556877 140318077859584 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6286982297897339, loss=1.0760570764541626
I0215 05:49:22.067709 140318069466880 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7193448543548584, loss=1.1187108755111694
I0215 05:50:46.507932 140318077859584 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.731877863407135, loss=1.199894905090332
I0215 05:52:15.475597 140318069466880 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6057908535003662, loss=1.1008986234664917
I0215 05:53:45.933923 140318077859584 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7353489398956299, loss=1.1950095891952515
I0215 05:55:15.482290 140318069466880 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5975509285926819, loss=1.1603152751922607
I0215 05:56:43.421127 140318077859584 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6834257245063782, loss=1.1973754167556763
I0215 05:58:11.473172 140318077859584 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5620355606079102, loss=1.1207937002182007
I0215 05:59:28.060357 140318069466880 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5895258784294128, loss=1.1142442226409912
I0215 06:00:45.884305 140318077859584 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.5721510052680969, loss=1.0917493104934692
I0215 06:02:08.465251 140318069466880 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.5859267115592957, loss=1.1702996492385864
I0215 06:03:36.576894 140318077859584 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.5946757197380066, loss=1.1524958610534668
I0215 06:05:03.761245 140318069466880 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5474313497543335, loss=1.1177715063095093
I0215 06:06:31.812704 140318077859584 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5956205129623413, loss=1.1733745336532593
I0215 06:07:58.143249 140318069466880 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7441468238830566, loss=1.0929900407791138
I0215 06:09:28.774719 140318077859584 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6428477764129639, loss=1.1558912992477417
I0215 06:10:31.608139 140399019657024 spec.py:321] Evaluating on the training split.
I0215 06:11:27.445997 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 06:12:19.497614 140399019657024 spec.py:349] Evaluating on the test split.
I0215 06:12:46.571368 140399019657024 submission_runner.py:408] Time since start: 31696.38s, 	Step: 33870, 	{'train/ctc_loss': Array(0.14385405, dtype=float32), 'train/wer': 0.0571283357173921, 'validation/ctc_loss': Array(0.41215566, dtype=float32), 'validation/wer': 0.12275891365843769, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23393744, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 28839.877410888672, 'total_duration': 31696.38317656517, 'accumulated_submission_time': 28839.877410888672, 'accumulated_eval_time': 2853.9519302845, 'accumulated_logging_time': 1.0594327449798584}
I0215 06:12:46.612832 140318077859584 logging_writer.py:48] [33870] accumulated_eval_time=2853.951930, accumulated_logging_time=1.059433, accumulated_submission_time=28839.877411, global_step=33870, preemption_count=0, score=28839.877411, test/ctc_loss=0.23393744230270386, test/num_examples=2472, test/wer=0.077631, total_duration=31696.383177, train/ctc_loss=0.1438540518283844, train/wer=0.057128, validation/ctc_loss=0.4121556580066681, validation/num_examples=5348, validation/wer=0.122759
I0215 06:13:10.495813 140318069466880 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5874848961830139, loss=1.1540943384170532
I0215 06:14:30.856083 140318077859584 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5564159154891968, loss=1.100569725036621
I0215 06:15:49.537324 140318069466880 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7158393263816833, loss=1.1348007917404175
I0215 06:17:06.960783 140318077859584 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6251711845397949, loss=1.1557233333587646
I0215 06:18:27.676066 140318069466880 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6819305419921875, loss=1.0730416774749756
I0215 06:19:57.612552 140318077859584 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7075429558753967, loss=1.1234135627746582
I0215 06:21:28.651713 140318069466880 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7384839653968811, loss=1.0718626976013184
I0215 06:22:57.315724 140318077859584 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6994331479072571, loss=1.1113293170928955
I0215 06:24:29.942327 140318069466880 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.5924099087715149, loss=1.125802755355835
I0215 06:25:59.421923 140318077859584 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6302767992019653, loss=1.1796027421951294
I0215 06:27:28.665693 140318069466880 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.739606499671936, loss=1.064972162246704
I0215 06:29:00.906482 140318077859584 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6740014553070068, loss=1.066704273223877
I0215 06:30:23.824575 140318077859584 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7006248831748962, loss=1.096205711364746
I0215 06:31:40.939943 140318069466880 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6387808918952942, loss=1.082116961479187
I0215 06:33:01.150787 140318077859584 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6733516454696655, loss=1.1248047351837158
I0215 06:34:26.630091 140318069466880 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7638164162635803, loss=1.1701246500015259
I0215 06:35:55.744461 140318077859584 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7275606393814087, loss=1.168168544769287
I0215 06:36:47.213349 140399019657024 spec.py:321] Evaluating on the training split.
I0215 06:37:43.520018 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 06:38:36.916323 140399019657024 spec.py:349] Evaluating on the test split.
I0215 06:39:04.219985 140399019657024 submission_runner.py:408] Time since start: 33274.03s, 	Step: 35559, 	{'train/ctc_loss': Array(0.1265491, dtype=float32), 'train/wer': 0.05118336487918554, 'validation/ctc_loss': Array(0.4120602, dtype=float32), 'validation/wer': 0.12215067051565502, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23113732, dtype=float32), 'test/wer': 0.07594499624235777, 'test/num_examples': 2472, 'score': 30280.395092010498, 'total_duration': 33274.03078866005, 'accumulated_submission_time': 30280.395092010498, 'accumulated_eval_time': 2990.9510576725006, 'accumulated_logging_time': 1.1162869930267334}
I0215 06:39:04.260319 140318077859584 logging_writer.py:48] [35559] accumulated_eval_time=2990.951058, accumulated_logging_time=1.116287, accumulated_submission_time=30280.395092, global_step=35559, preemption_count=0, score=30280.395092, test/ctc_loss=0.23113732039928436, test/num_examples=2472, test/wer=0.075945, total_duration=33274.030789, train/ctc_loss=0.12654909491539001, train/wer=0.051183, validation/ctc_loss=0.4120602011680603, validation/num_examples=5348, validation/wer=0.122151
I0215 06:39:36.349185 140318069466880 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6440275311470032, loss=1.0847818851470947
I0215 06:40:53.254678 140318077859584 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.5681765079498291, loss=1.167685627937317
I0215 06:42:11.567183 140318069466880 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.642223060131073, loss=1.1518256664276123
I0215 06:43:42.919524 140318077859584 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.6022183895111084, loss=1.085978388786316
I0215 06:45:12.651319 140318069466880 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.8285364508628845, loss=1.1510168313980103
I0215 06:46:40.760309 140318077859584 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6728099584579468, loss=1.1296067237854004
I0215 06:47:58.041807 140318069466880 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.540316104888916, loss=1.0922285318374634
I0215 06:49:18.815587 140318077859584 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6269600987434387, loss=1.097775936126709
I0215 06:50:39.892914 140318069466880 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6099674105644226, loss=1.1241040229797363
I0215 06:52:06.666446 140318077859584 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6572386622428894, loss=1.1299529075622559
I0215 06:53:36.763802 140318069466880 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.5854713320732117, loss=1.1819790601730347
I0215 06:55:08.338951 140318077859584 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6420729756355286, loss=1.0541224479675293
I0215 06:56:36.646436 140318069466880 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6387783288955688, loss=1.0942925214767456
I0215 06:58:05.341332 140318077859584 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6335304975509644, loss=1.106424331665039
I0215 06:59:32.865798 140318069466880 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7026818990707397, loss=1.0942388772964478
I0215 07:01:04.549970 140318077859584 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.7149100303649902, loss=1.0772761106491089
I0215 07:02:22.111035 140318069466880 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7280397415161133, loss=1.0703788995742798
I0215 07:03:05.026945 140399019657024 spec.py:321] Evaluating on the training split.
I0215 07:04:01.553326 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 07:04:55.540993 140399019657024 spec.py:349] Evaluating on the test split.
I0215 07:05:22.436814 140399019657024 submission_runner.py:408] Time since start: 34852.25s, 	Step: 37254, 	{'train/ctc_loss': Array(0.12382909, dtype=float32), 'train/wer': 0.04804198627371821, 'validation/ctc_loss': Array(0.40429664, dtype=float32), 'validation/wer': 0.12002664684244571, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22469771, dtype=float32), 'test/wer': 0.07527471411451668, 'test/num_examples': 2472, 'score': 31721.076458215714, 'total_duration': 34852.249284267426, 'accumulated_submission_time': 31721.076458215714, 'accumulated_eval_time': 3128.3551211357117, 'accumulated_logging_time': 1.1727495193481445}
I0215 07:05:22.477874 140318077859584 logging_writer.py:48] [37254] accumulated_eval_time=3128.355121, accumulated_logging_time=1.172750, accumulated_submission_time=31721.076458, global_step=37254, preemption_count=0, score=31721.076458, test/ctc_loss=0.22469770908355713, test/num_examples=2472, test/wer=0.075275, total_duration=34852.249284, train/ctc_loss=0.12382908910512924, train/wer=0.048042, validation/ctc_loss=0.4042966365814209, validation/num_examples=5348, validation/wer=0.120027
I0215 07:05:58.853666 140318069466880 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6736642122268677, loss=1.132284164428711
I0215 07:07:15.808314 140318077859584 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.7124752998352051, loss=1.0797014236450195
I0215 07:08:32.677683 140318069466880 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.692510187625885, loss=1.0951653718948364
I0215 07:09:58.763734 140318077859584 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6790232062339783, loss=1.1229808330535889
I0215 07:11:27.673572 140318069466880 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6412305235862732, loss=1.0861202478408813
I0215 07:12:59.679524 140318077859584 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5913736820220947, loss=1.0847699642181396
I0215 07:14:31.359696 140318069466880 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6070224046707153, loss=1.063866376876831
I0215 07:16:00.684903 140318077859584 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6138049364089966, loss=1.1839646100997925
I0215 07:17:32.109195 140318069466880 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6335560083389282, loss=1.102089762687683
I0215 07:18:54.428798 140318077859584 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.7276349067687988, loss=1.0824882984161377
I0215 07:20:14.128382 140318069466880 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6451632976531982, loss=1.0475212335586548
I0215 07:21:34.106474 140318077859584 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.5655562281608582, loss=1.085194706916809
I0215 07:22:59.200350 140318069466880 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.789814829826355, loss=1.1051380634307861
I0215 07:24:26.100871 140318077859584 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.639085590839386, loss=1.0840116739273071
I0215 07:25:58.075484 140318069466880 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6128403544425964, loss=1.0736578702926636
I0215 07:27:27.747651 140318077859584 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.747597336769104, loss=1.0666613578796387
I0215 07:28:58.121110 140318069466880 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5904409885406494, loss=1.0969477891921997
I0215 07:29:22.917203 140399019657024 spec.py:321] Evaluating on the training split.
I0215 07:30:18.525603 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 07:31:13.024305 140399019657024 spec.py:349] Evaluating on the test split.
I0215 07:31:40.407824 140399019657024 submission_runner.py:408] Time since start: 36430.22s, 	Step: 38930, 	{'train/ctc_loss': Array(0.13134545, dtype=float32), 'train/wer': 0.05106037654187405, 'validation/ctc_loss': Array(0.39354557, dtype=float32), 'validation/wer': 0.11500622725122372, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2211299, dtype=float32), 'test/wer': 0.07440131619036013, 'test/num_examples': 2472, 'score': 33161.42921876907, 'total_duration': 36430.21849322319, 'accumulated_submission_time': 33161.42921876907, 'accumulated_eval_time': 3265.838086128235, 'accumulated_logging_time': 1.232497215270996}
I0215 07:31:40.453002 140318077859584 logging_writer.py:48] [38930] accumulated_eval_time=3265.838086, accumulated_logging_time=1.232497, accumulated_submission_time=33161.429219, global_step=38930, preemption_count=0, score=33161.429219, test/ctc_loss=0.2211298942565918, test/num_examples=2472, test/wer=0.074401, total_duration=36430.218493, train/ctc_loss=0.1313454508781433, train/wer=0.051060, validation/ctc_loss=0.39354556798934937, validation/num_examples=5348, validation/wer=0.115006
I0215 07:32:34.646250 140318069466880 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7055196762084961, loss=1.0932577848434448
I0215 07:33:51.210620 140318077859584 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6466261744499207, loss=1.0961802005767822
I0215 07:35:11.907258 140318077859584 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6878513693809509, loss=1.0711495876312256
I0215 07:36:28.931984 140318069466880 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6330759525299072, loss=1.0602473020553589
I0215 07:37:49.032489 140318077859584 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.5716040730476379, loss=1.0205352306365967
I0215 07:39:13.441724 140318069466880 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6410467028617859, loss=1.096338152885437
I0215 07:40:41.412657 140318077859584 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.8332738280296326, loss=1.1258740425109863
I0215 07:42:11.613743 140318069466880 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.67276531457901, loss=1.060705304145813
I0215 07:43:43.296099 140318077859584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.6547374129295349, loss=1.07029390335083
I0215 07:45:14.736979 140318069466880 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7900665402412415, loss=1.070068359375
I0215 07:46:46.663561 140318077859584 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7917740941047668, loss=1.0845922231674194
I0215 07:48:16.809373 140318069466880 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.8075674772262573, loss=1.0625208616256714
I0215 07:49:47.489980 140318077859584 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6170562505722046, loss=1.0504131317138672
I0215 07:51:04.933530 140318069466880 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.5847125053405762, loss=1.0457606315612793
I0215 07:52:27.275391 140318077859584 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7247006297111511, loss=1.0419775247573853
I0215 07:53:51.534438 140318069466880 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.64833664894104, loss=1.0667310953140259
I0215 07:55:15.158372 140318077859584 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8280734419822693, loss=1.066703200340271
I0215 07:55:40.535841 140399019657024 spec.py:321] Evaluating on the training split.
I0215 07:56:35.394581 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 07:57:29.981263 140399019657024 spec.py:349] Evaluating on the test split.
I0215 07:57:58.363546 140399019657024 submission_runner.py:408] Time since start: 38008.18s, 	Step: 40629, 	{'train/ctc_loss': Array(0.11861537, dtype=float32), 'train/wer': 0.046957308071536356, 'validation/ctc_loss': Array(0.38333464, dtype=float32), 'validation/wer': 0.11374146770035819, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21496634, dtype=float32), 'test/wer': 0.07261389718278391, 'test/num_examples': 2472, 'score': 34601.42512226105, 'total_duration': 38008.17625498772, 'accumulated_submission_time': 34601.42512226105, 'accumulated_eval_time': 3403.660226583481, 'accumulated_logging_time': 1.2956340312957764}
I0215 07:57:58.400900 140318077859584 logging_writer.py:48] [40629] accumulated_eval_time=3403.660227, accumulated_logging_time=1.295634, accumulated_submission_time=34601.425122, global_step=40629, preemption_count=0, score=34601.425122, test/ctc_loss=0.21496634185314178, test/num_examples=2472, test/wer=0.072614, total_duration=38008.176255, train/ctc_loss=0.11861536651849747, train/wer=0.046957, validation/ctc_loss=0.3833346366882324, validation/num_examples=5348, validation/wer=0.113741
I0215 07:58:53.912231 140318069466880 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.564811110496521, loss=1.0666698217391968
I0215 08:00:10.576783 140318077859584 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.6018242239952087, loss=1.0974657535552979
I0215 08:01:31.654484 140318069466880 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.619990348815918, loss=1.0833138227462769
I0215 08:03:01.542087 140318077859584 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.6153241395950317, loss=1.0737501382827759
I0215 08:04:31.102884 140318069466880 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.6704471111297607, loss=1.0916776657104492
I0215 08:06:05.948186 140318077859584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.5510500073432922, loss=1.0444490909576416
I0215 08:07:24.978344 140318069466880 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6118537783622742, loss=1.0676203966140747
I0215 08:08:44.103994 140318077859584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5964966416358948, loss=1.0289134979248047
I0215 08:10:07.427235 140318069466880 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7711579203605652, loss=1.0646804571151733
I0215 08:11:30.696354 140318077859584 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6921854615211487, loss=1.1147533655166626
I0215 08:13:00.705403 140318069466880 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.5566399693489075, loss=1.059415340423584
I0215 08:14:30.150065 140318077859584 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6492468118667603, loss=1.0233479738235474
I0215 08:15:59.505516 140318069466880 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6617255210876465, loss=1.030798077583313
I0215 08:17:30.970705 140318077859584 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6601221561431885, loss=1.0555119514465332
I0215 08:18:59.974365 140318069466880 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.5705921053886414, loss=1.0841256380081177
I0215 08:20:32.223894 140318077859584 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6340205073356628, loss=1.0785523653030396
I0215 08:21:55.157617 140318077859584 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.6191521286964417, loss=1.0240260362625122
I0215 08:21:58.679326 140399019657024 spec.py:321] Evaluating on the training split.
I0215 08:22:55.179851 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 08:23:48.168559 140399019657024 spec.py:349] Evaluating on the test split.
I0215 08:24:15.033198 140399019657024 submission_runner.py:408] Time since start: 39584.84s, 	Step: 42306, 	{'train/ctc_loss': Array(0.10947188, dtype=float32), 'train/wer': 0.045603775520673025, 'validation/ctc_loss': Array(0.3845979, dtype=float32), 'validation/wer': 0.11188777431283006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21081606, dtype=float32), 'test/wer': 0.06944529076026243, 'test/num_examples': 2472, 'score': 36041.619644880295, 'total_duration': 39584.84460020065, 'accumulated_submission_time': 36041.619644880295, 'accumulated_eval_time': 3540.007214784622, 'accumulated_logging_time': 1.3486223220825195}
I0215 08:24:15.076372 140318077859584 logging_writer.py:48] [42306] accumulated_eval_time=3540.007215, accumulated_logging_time=1.348622, accumulated_submission_time=36041.619645, global_step=42306, preemption_count=0, score=36041.619645, test/ctc_loss=0.21081605553627014, test/num_examples=2472, test/wer=0.069445, total_duration=39584.844600, train/ctc_loss=0.1094718798995018, train/wer=0.045604, validation/ctc_loss=0.38459789752960205, validation/num_examples=5348, validation/wer=0.111888
I0215 08:25:28.627089 140318069466880 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.6550483107566833, loss=1.0852423906326294
I0215 08:26:45.184408 140318077859584 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6655775308609009, loss=1.072510838508606
I0215 08:28:01.869956 140318069466880 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5935478210449219, loss=1.0419191122055054
I0215 08:29:22.322558 140318077859584 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.650627851486206, loss=1.0218751430511475
I0215 08:30:51.746136 140318069466880 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.682774543762207, loss=1.0443165302276611
I0215 08:32:20.618606 140318077859584 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5896550416946411, loss=1.050698161125183
I0215 08:33:48.731068 140318069466880 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6760251522064209, loss=1.0692822933197021
I0215 08:35:20.261020 140318077859584 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.63400799036026, loss=1.0346927642822266
I0215 08:36:52.128538 140318069466880 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.9602574110031128, loss=1.1055684089660645
I0215 08:38:20.734709 140318077859584 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.5880307555198669, loss=1.0334523916244507
I0215 08:39:38.422236 140318069466880 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5670264959335327, loss=1.0183972120285034
I0215 08:40:58.164500 140318077859584 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8022103309631348, loss=1.0963311195373535
I0215 08:42:16.940764 140318069466880 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.665657639503479, loss=0.9814571142196655
I0215 08:43:42.143349 140318077859584 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.689149022102356, loss=1.0527271032333374
I0215 08:45:09.786170 140318069466880 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6176578998565674, loss=1.0720407962799072
I0215 08:46:40.913853 140318077859584 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.6758609414100647, loss=1.0528584718704224
I0215 08:48:11.434854 140318069466880 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7638954520225525, loss=1.041246771812439
I0215 08:48:15.103925 140399019657024 spec.py:321] Evaluating on the training split.
I0215 08:49:11.002057 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 08:50:05.303766 140399019657024 spec.py:349] Evaluating on the test split.
I0215 08:50:31.730050 140399019657024 submission_runner.py:408] Time since start: 41161.54s, 	Step: 44006, 	{'train/ctc_loss': Array(0.13150723, dtype=float32), 'train/wer': 0.04866141229777594, 'validation/ctc_loss': Array(0.38188517, dtype=float32), 'validation/wer': 0.11146296957818821, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21202654, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 37481.56066060066, 'total_duration': 41161.54116153717, 'accumulated_submission_time': 37481.56066060066, 'accumulated_eval_time': 3676.6261253356934, 'accumulated_logging_time': 1.4088342189788818}
I0215 08:50:31.770431 140318077859584 logging_writer.py:48] [44006] accumulated_eval_time=3676.626125, accumulated_logging_time=1.408834, accumulated_submission_time=37481.560661, global_step=44006, preemption_count=0, score=37481.560661, test/ctc_loss=0.21202653646469116, test/num_examples=2472, test/wer=0.070867, total_duration=41161.541162, train/ctc_loss=0.13150723278522491, train/wer=0.048661, validation/ctc_loss=0.3818851709365845, validation/num_examples=5348, validation/wer=0.111463
I0215 08:51:44.848019 140318069466880 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6754454970359802, loss=1.0002623796463013
I0215 08:53:01.376666 140318077859584 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6104853749275208, loss=1.035953164100647
I0215 08:54:26.313991 140318077859584 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6405009627342224, loss=1.0333245992660522
I0215 08:55:43.963581 140318069466880 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7305730581283569, loss=1.0471267700195312
I0215 08:57:05.120790 140318077859584 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.8392003178596497, loss=1.0429835319519043
I0215 08:58:27.899568 140318069466880 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.6984158754348755, loss=1.0614992380142212
I0215 08:59:53.379563 140318077859584 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6027175784111023, loss=1.049507975578308
I0215 09:01:20.218514 140318069466880 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.638457179069519, loss=1.0719910860061646
I0215 09:02:49.746064 140318077859584 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.9941308498382568, loss=1.0071841478347778
I0215 09:04:18.053264 140318069466880 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.5659535527229309, loss=1.0118149518966675
I0215 09:05:48.576483 140318077859584 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6387746930122375, loss=1.0693179368972778
I0215 09:07:17.528983 140318069466880 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5984248518943787, loss=1.0649524927139282
I0215 09:08:46.333585 140318077859584 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5752027034759521, loss=0.9858237504959106
I0215 09:10:08.862508 140318077859584 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6488970518112183, loss=1.0252436399459839
I0215 09:11:30.963291 140318069466880 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6746940016746521, loss=1.0336333513259888
I0215 09:12:49.791366 140318077859584 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.833198070526123, loss=1.0293967723846436
I0215 09:14:14.906868 140318069466880 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.687183678150177, loss=1.0258846282958984
I0215 09:14:31.748079 140399019657024 spec.py:321] Evaluating on the training split.
I0215 09:15:28.337449 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 09:16:22.388586 140399019657024 spec.py:349] Evaluating on the test split.
I0215 09:16:49.346084 140399019657024 submission_runner.py:408] Time since start: 42739.16s, 	Step: 45720, 	{'train/ctc_loss': Array(0.08845052, dtype=float32), 'train/wer': 0.03545131981048875, 'validation/ctc_loss': Array(0.37528655, dtype=float32), 'validation/wer': 0.10923274472131844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20303267, dtype=float32), 'test/wer': 0.06775942965084394, 'test/num_examples': 2472, 'score': 38921.45214056969, 'total_duration': 42739.1587574482, 'accumulated_submission_time': 38921.45214056969, 'accumulated_eval_time': 3814.218501806259, 'accumulated_logging_time': 1.4656569957733154}
I0215 09:16:49.390211 140318077859584 logging_writer.py:48] [45720] accumulated_eval_time=3814.218502, accumulated_logging_time=1.465657, accumulated_submission_time=38921.452141, global_step=45720, preemption_count=0, score=38921.452141, test/ctc_loss=0.20303267240524292, test/num_examples=2472, test/wer=0.067759, total_duration=42739.158757, train/ctc_loss=0.08845052123069763, train/wer=0.035451, validation/ctc_loss=0.3752865493297577, validation/num_examples=5348, validation/wer=0.109233
I0215 09:17:51.064630 140318069466880 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6814514398574829, loss=1.0594959259033203
I0215 09:19:07.545379 140318077859584 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.8667367100715637, loss=1.045301079750061
I0215 09:20:31.315672 140318069466880 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7303603887557983, loss=1.0531303882598877
I0215 09:22:02.383714 140318077859584 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7329360842704773, loss=1.0169618129730225
I0215 09:23:33.777682 140318069466880 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6070754528045654, loss=1.0187593698501587
I0215 09:25:01.287478 140318077859584 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6459404230117798, loss=0.9813911318778992
I0215 09:26:27.428031 140318077859584 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7027564644813538, loss=1.0184496641159058
I0215 09:27:45.226856 140318069466880 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6385241150856018, loss=0.9928856492042542
I0215 09:29:04.317387 140318077859584 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.6043490767478943, loss=0.9812965989112854
I0215 09:30:28.739767 140318069466880 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.0369595289230347, loss=1.02598237991333
I0215 09:31:53.293797 140318077859584 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6280257701873779, loss=1.0012671947479248
I0215 09:33:24.309655 140318069466880 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.603476881980896, loss=1.0586533546447754
I0215 09:34:55.885705 140318077859584 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.218542218208313, loss=1.0353082418441772
I0215 09:36:25.600810 140318069466880 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.6150560975074768, loss=1.0339200496673584
I0215 09:37:54.370778 140318077859584 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8610762357711792, loss=1.0527198314666748
I0215 09:39:26.108192 140318069466880 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.6614577174186707, loss=1.045888066291809
I0215 09:40:50.059452 140399019657024 spec.py:321] Evaluating on the training split.
I0215 09:41:46.003476 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 09:42:39.768167 140399019657024 spec.py:349] Evaluating on the test split.
I0215 09:43:06.488010 140399019657024 submission_runner.py:408] Time since start: 44316.30s, 	Step: 47396, 	{'train/ctc_loss': Array(0.09425022, dtype=float32), 'train/wer': 0.03792730295315416, 'validation/ctc_loss': Array(0.37133262, dtype=float32), 'validation/wer': 0.1077169641908918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20532721, dtype=float32), 'test/wer': 0.06745475595637072, 'test/num_examples': 2472, 'score': 40362.0363907814, 'total_duration': 44316.299813985825, 'accumulated_submission_time': 40362.0363907814, 'accumulated_eval_time': 3950.640738248825, 'accumulated_logging_time': 1.5266199111938477}
I0215 09:43:06.529534 140318077859584 logging_writer.py:48] [47396] accumulated_eval_time=3950.640738, accumulated_logging_time=1.526620, accumulated_submission_time=40362.036391, global_step=47396, preemption_count=0, score=40362.036391, test/ctc_loss=0.20532721281051636, test/num_examples=2472, test/wer=0.067455, total_duration=44316.299814, train/ctc_loss=0.09425021708011627, train/wer=0.037927, validation/ctc_loss=0.3713326156139374, validation/num_examples=5348, validation/wer=0.107717
I0215 09:43:10.431768 140318069466880 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6439027786254883, loss=0.9953566193580627
I0215 09:44:27.110558 140318077859584 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7257837653160095, loss=1.0446826219558716
I0215 09:45:43.993072 140318069466880 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7720357775688171, loss=1.0005663633346558
I0215 09:47:01.695286 140318077859584 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7284092903137207, loss=1.010101556777954
I0215 09:48:18.911176 140318069466880 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6370104551315308, loss=1.0397382974624634
I0215 09:49:47.493260 140318077859584 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7197430729866028, loss=0.9958561062812805
I0215 09:51:17.881441 140318069466880 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6257081031799316, loss=0.9951083660125732
I0215 09:52:45.332152 140318077859584 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8215084075927734, loss=1.0382249355316162
I0215 09:54:17.479600 140318069466880 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6041012406349182, loss=0.988941490650177
I0215 09:55:48.344269 140318077859584 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.6483383178710938, loss=0.985750138759613
I0215 09:57:18.159678 140318069466880 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7168595790863037, loss=1.0951788425445557
I0215 09:58:38.702659 140318077859584 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7190542817115784, loss=1.0371378660202026
I0215 09:59:58.818280 140318069466880 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.668345034122467, loss=1.0188319683074951
I0215 10:01:19.721301 140318077859584 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6760896444320679, loss=0.996172308921814
I0215 10:02:45.863735 140318069466880 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.6389229893684387, loss=1.04292631149292
I0215 10:04:15.649352 140318077859584 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7563754320144653, loss=1.0458335876464844
I0215 10:05:46.146025 140318069466880 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.627461850643158, loss=1.0054874420166016
I0215 10:07:06.534842 140399019657024 spec.py:321] Evaluating on the training split.
I0215 10:08:01.247134 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 10:08:54.705540 140399019657024 spec.py:349] Evaluating on the test split.
I0215 10:09:21.608455 140399019657024 submission_runner.py:408] Time since start: 45891.42s, 	Step: 49093, 	{'train/ctc_loss': Array(0.11438162, dtype=float32), 'train/wer': 0.04541957042671532, 'validation/ctc_loss': Array(0.36344072, dtype=float32), 'validation/wer': 0.10611429178292478, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19545701, dtype=float32), 'test/wer': 0.06552515589137367, 'test/num_examples': 2472, 'score': 41801.95665073395, 'total_duration': 45891.41930747032, 'accumulated_submission_time': 41801.95665073395, 'accumulated_eval_time': 4085.706921339035, 'accumulated_logging_time': 1.5839388370513916}
I0215 10:09:21.651478 140318077859584 logging_writer.py:48] [49093] accumulated_eval_time=4085.706921, accumulated_logging_time=1.583939, accumulated_submission_time=41801.956651, global_step=49093, preemption_count=0, score=41801.956651, test/ctc_loss=0.19545701146125793, test/num_examples=2472, test/wer=0.065525, total_duration=45891.419307, train/ctc_loss=0.11438161879777908, train/wer=0.045420, validation/ctc_loss=0.36344072222709656, validation/num_examples=5348, validation/wer=0.106114
I0215 10:09:27.843422 140318069466880 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.8706823587417603, loss=0.9874899983406067
I0215 10:10:44.434700 140318077859584 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.6514729857444763, loss=0.9725657105445862
I0215 10:12:01.519764 140318069466880 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.580624520778656, loss=1.000670075416565
I0215 10:13:27.882509 140318077859584 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6655046939849854, loss=1.0165823698043823
I0215 10:14:55.583778 140318077859584 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.8799347877502441, loss=0.9816451072692871
I0215 10:16:14.138449 140318069466880 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6782148480415344, loss=0.9810541868209839
I0215 10:17:34.077320 140318077859584 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6564465165138245, loss=0.9868859648704529
I0215 10:18:58.826364 140318069466880 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7544103860855103, loss=1.0144046545028687
I0215 10:20:26.539330 140318077859584 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7191867828369141, loss=1.0120192766189575
I0215 10:21:57.745419 140318069466880 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6536495685577393, loss=0.961922824382782
I0215 10:23:30.328592 140318077859584 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8403379321098328, loss=1.0004621744155884
I0215 10:25:02.803895 140318069466880 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.6994442939758301, loss=0.9404385685920715
I0215 10:26:31.857935 140318077859584 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.8394764065742493, loss=1.032361388206482
I0215 10:28:03.594857 140318069466880 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6155309677124023, loss=0.9430158138275146
I0215 10:29:32.593415 140318077859584 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.6946716904640198, loss=1.0241080522537231
I0215 10:30:50.780519 140318069466880 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.604130744934082, loss=0.9649918675422668
I0215 10:32:10.715776 140318077859584 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.7920092344284058, loss=0.9645569920539856
I0215 10:33:21.814768 140399019657024 spec.py:321] Evaluating on the training split.
I0215 10:34:16.099539 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 10:35:10.205396 140399019657024 spec.py:349] Evaluating on the test split.
I0215 10:35:37.665205 140399019657024 submission_runner.py:408] Time since start: 47467.48s, 	Step: 50786, 	{'train/ctc_loss': Array(0.1178861, dtype=float32), 'train/wer': 0.04611315356079384, 'validation/ctc_loss': Array(0.360941, dtype=float32), 'validation/wer': 0.10384544831381484, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19716543, dtype=float32), 'test/wer': 0.06408303373753377, 'test/num_examples': 2472, 'score': 43242.034012556076, 'total_duration': 47467.480123758316, 'accumulated_submission_time': 43242.034012556076, 'accumulated_eval_time': 4221.55396938324, 'accumulated_logging_time': 1.6436846256256104}
I0215 10:35:37.704910 140318077859584 logging_writer.py:48] [50786] accumulated_eval_time=4221.553969, accumulated_logging_time=1.643685, accumulated_submission_time=43242.034013, global_step=50786, preemption_count=0, score=43242.034013, test/ctc_loss=0.19716542959213257, test/num_examples=2472, test/wer=0.064083, total_duration=47467.480124, train/ctc_loss=0.11788609623908997, train/wer=0.046113, validation/ctc_loss=0.36094099283218384, validation/num_examples=5348, validation/wer=0.103845
I0215 10:35:49.221847 140318069466880 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7483908534049988, loss=1.0084370374679565
I0215 10:37:05.970497 140318077859584 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.7152193784713745, loss=0.9669314622879028
I0215 10:38:23.373350 140318069466880 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6541395783424377, loss=0.9638397693634033
I0215 10:39:50.026307 140318077859584 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.6703545451164246, loss=0.9016806483268738
I0215 10:41:21.480196 140318069466880 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.7173901200294495, loss=0.9883438348770142
I0215 10:42:54.638010 140318077859584 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7521890997886658, loss=1.015458106994629
I0215 10:44:26.093434 140318069466880 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6621181964874268, loss=1.005190372467041
I0215 10:45:57.952154 140318077859584 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7020325660705566, loss=0.9709141850471497
I0215 10:47:16.158062 140318069466880 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.5675042271614075, loss=0.978766918182373
I0215 10:48:38.359693 140318077859584 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.7016860246658325, loss=1.0182428359985352
I0215 10:50:00.595351 140318069466880 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.5560743808746338, loss=0.9707650542259216
I0215 10:51:28.607829 140318077859584 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.6799880266189575, loss=0.9515567421913147
I0215 10:52:59.485985 140318069466880 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8154773116111755, loss=1.0341588258743286
I0215 10:54:27.788537 140318077859584 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.684682309627533, loss=0.9486355781555176
I0215 10:55:58.273040 140318069466880 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6491516828536987, loss=0.9386008381843567
I0215 10:57:27.108006 140318077859584 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.5960045456886292, loss=0.9599130153656006
I0215 10:58:58.950164 140318069466880 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.7364766001701355, loss=0.9829809665679932
I0215 10:59:38.316892 140399019657024 spec.py:321] Evaluating on the training split.
I0215 11:00:31.711252 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 11:01:24.602378 140399019657024 spec.py:349] Evaluating on the test split.
I0215 11:01:52.073490 140399019657024 submission_runner.py:408] Time since start: 49041.89s, 	Step: 52446, 	{'train/ctc_loss': Array(0.12866937, dtype=float32), 'train/wer': 0.05156511045108595, 'validation/ctc_loss': Array(0.3551715, dtype=float32), 'validation/wer': 0.10201106423240681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19476175, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 44682.5635433197, 'total_duration': 49041.885318517685, 'accumulated_submission_time': 44682.5635433197, 'accumulated_eval_time': 4355.304067373276, 'accumulated_logging_time': 1.6991455554962158}
I0215 11:01:52.117784 140318077859584 logging_writer.py:48] [52446] accumulated_eval_time=4355.304067, accumulated_logging_time=1.699146, accumulated_submission_time=44682.563543, global_step=52446, preemption_count=0, score=44682.563543, test/ctc_loss=0.1947617530822754, test/num_examples=2472, test/wer=0.063007, total_duration=49041.885319, train/ctc_loss=0.1286693662405014, train/wer=0.051565, validation/ctc_loss=0.3551715016365051, validation/num_examples=5348, validation/wer=0.102011
I0215 11:02:34.111750 140318069466880 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7141628265380859, loss=0.9492729902267456
I0215 11:03:55.959300 140318077859584 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8146719336509705, loss=0.9781568050384521
I0215 11:05:13.705442 140318069466880 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.7907870411872864, loss=0.9828594923019409
I0215 11:06:32.895394 140318077859584 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.7901434302330017, loss=0.972495436668396
I0215 11:07:58.178641 140318069466880 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.904410719871521, loss=0.9316099882125854
I0215 11:09:27.652307 140318077859584 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6946229934692383, loss=0.9945843815803528
I0215 11:10:59.970788 140318069466880 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.7494658827781677, loss=0.9945934414863586
I0215 11:12:30.561154 140318077859584 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7071800231933594, loss=0.9522135257720947
I0215 11:14:02.875501 140318069466880 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.6443193554878235, loss=0.9797658920288086
I0215 11:15:33.667990 140318077859584 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.859067976474762, loss=1.0183290243148804
I0215 11:17:05.285052 140318069466880 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8009480237960815, loss=0.9696847200393677
I0215 11:18:34.141256 140318077859584 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.7610978484153748, loss=0.9256882667541504
I0215 11:19:54.473720 140318069466880 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8341487050056458, loss=0.9138068556785583
I0215 11:21:14.632785 140318077859584 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.729362428188324, loss=0.9875704646110535
I0215 11:22:36.725985 140318069466880 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.726456344127655, loss=0.9894569516181946
I0215 11:24:04.411124 140318077859584 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8120272159576416, loss=0.9808908104896545
I0215 11:25:35.179850 140318069466880 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.0013293027877808, loss=0.9399704337120056
I0215 11:25:52.840672 140399019657024 spec.py:321] Evaluating on the training split.
I0215 11:26:45.656166 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 11:27:39.166198 140399019657024 spec.py:349] Evaluating on the test split.
I0215 11:28:07.465231 140399019657024 submission_runner.py:408] Time since start: 50617.28s, 	Step: 54122, 	{'train/ctc_loss': Array(0.11104356, dtype=float32), 'train/wer': 0.042604765817584225, 'validation/ctc_loss': Array(0.3560681, dtype=float32), 'validation/wer': 0.10204002819158693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19035426, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 46123.199763059616, 'total_duration': 50617.27509188652, 'accumulated_submission_time': 46123.199763059616, 'accumulated_eval_time': 4489.92017698288, 'accumulated_logging_time': 1.7626543045043945}
I0215 11:28:07.515324 140318077859584 logging_writer.py:48] [54122] accumulated_eval_time=4489.920177, accumulated_logging_time=1.762654, accumulated_submission_time=46123.199763, global_step=54122, preemption_count=0, score=46123.199763, test/ctc_loss=0.19035425782203674, test/num_examples=2472, test/wer=0.063007, total_duration=50617.275092, train/ctc_loss=0.11104356497526169, train/wer=0.042605, validation/ctc_loss=0.35606810450553894, validation/num_examples=5348, validation/wer=0.102040
I0215 11:29:07.830847 140318069466880 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.6491814255714417, loss=0.9485059976577759
I0215 11:30:24.508547 140318077859584 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.7582406401634216, loss=0.9293857216835022
I0215 11:31:48.665584 140318069466880 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.5910125970840454, loss=0.9336622357368469
I0215 11:33:17.391802 140318077859584 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7435044050216675, loss=0.9880056977272034
I0215 11:34:50.840204 140318077859584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7254305481910706, loss=0.8923486471176147
I0215 11:36:08.647281 140318069466880 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8900308609008789, loss=0.9177565574645996
I0215 11:37:29.539829 140318077859584 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.7172265648841858, loss=0.9926499128341675
I0215 11:38:51.199813 140318069466880 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.7421567440032959, loss=0.9312800765037537
I0215 11:40:19.538628 140318077859584 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6167478561401367, loss=0.9312388300895691
I0215 11:41:48.749747 140318069466880 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.5923295021057129, loss=0.9369236826896667
I0215 11:43:18.905353 140318077859584 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.6283603310585022, loss=0.9326363801956177
I0215 11:44:50.065544 140318069466880 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.7177403569221497, loss=1.0329333543777466
I0215 11:46:23.886580 140318077859584 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8643996119499207, loss=0.93483567237854
I0215 11:47:56.436978 140318069466880 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7426765561103821, loss=0.9827816486358643
I0215 11:49:26.680892 140318077859584 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.7112722992897034, loss=0.9555249214172363
I0215 11:50:49.768316 140318077859584 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.8354775309562683, loss=0.9731650948524475
I0215 11:52:08.022032 140318069466880 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.7142565250396729, loss=0.9400156736373901
I0215 11:52:08.030488 140399019657024 spec.py:321] Evaluating on the training split.
I0215 11:53:02.153478 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 11:53:54.058397 140399019657024 spec.py:349] Evaluating on the test split.
I0215 11:54:21.548475 140399019657024 submission_runner.py:408] Time since start: 52191.36s, 	Step: 55801, 	{'train/ctc_loss': Array(0.09821409, dtype=float32), 'train/wer': 0.039273090992869145, 'validation/ctc_loss': Array(0.3484534, dtype=float32), 'validation/wer': 0.09916294157969434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18869571, dtype=float32), 'test/wer': 0.06150346312432718, 'test/num_examples': 2472, 'score': 47563.62722706795, 'total_duration': 52191.360865831375, 'accumulated_submission_time': 47563.62722706795, 'accumulated_eval_time': 4623.43222784996, 'accumulated_logging_time': 1.8321900367736816}
I0215 11:54:21.588146 140318077859584 logging_writer.py:48] [55801] accumulated_eval_time=4623.432228, accumulated_logging_time=1.832190, accumulated_submission_time=47563.627227, global_step=55801, preemption_count=0, score=47563.627227, test/ctc_loss=0.18869571387767792, test/num_examples=2472, test/wer=0.061503, total_duration=52191.360866, train/ctc_loss=0.09821408987045288, train/wer=0.039273, validation/ctc_loss=0.3484534025192261, validation/num_examples=5348, validation/wer=0.099163
I0215 11:55:37.795798 140318069466880 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.733892023563385, loss=0.9600018262863159
I0215 11:56:54.548081 140318077859584 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.6327806115150452, loss=0.9018851518630981
I0215 11:58:11.993000 140318069466880 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.8838161826133728, loss=1.0228824615478516
I0215 11:59:42.306031 140318077859584 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.8099607229232788, loss=0.9389404654502869
I0215 12:01:13.302479 140318069466880 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7454162836074829, loss=0.9248285889625549
I0215 12:02:44.565727 140318077859584 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.7733306884765625, loss=0.914847195148468
I0215 12:04:14.248110 140318069466880 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.6198350191116333, loss=0.9341850876808167
I0215 12:05:44.577828 140318077859584 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.7473694682121277, loss=0.924689531326294
I0215 12:07:10.759883 140318077859584 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.6528234481811523, loss=0.8877513408660889
I0215 12:08:32.643829 140318069466880 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.7135820388793945, loss=0.9247570037841797
I0215 12:09:52.810288 140318077859584 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6890888810157776, loss=0.9359341263771057
I0215 12:11:19.021808 140318069466880 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.6799377202987671, loss=0.885797917842865
I0215 12:12:48.559993 140318077859584 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6668310165405273, loss=0.9381340742111206
I0215 12:14:17.435202 140318069466880 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.8646013736724854, loss=0.9493427872657776
I0215 12:15:45.778028 140318077859584 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2323813438415527, loss=0.9614601135253906
I0215 12:17:16.038364 140318069466880 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.6268784403800964, loss=0.9723669290542603
I0215 12:18:22.658665 140399019657024 spec.py:321] Evaluating on the training split.
I0215 12:19:18.054449 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 12:20:11.656114 140399019657024 spec.py:349] Evaluating on the test split.
I0215 12:20:39.152562 140399019657024 submission_runner.py:408] Time since start: 53768.96s, 	Step: 57473, 	{'train/ctc_loss': Array(0.08137985, dtype=float32), 'train/wer': 0.03258301206317622, 'validation/ctc_loss': Array(0.34315622, dtype=float32), 'validation/wer': 0.09662376782490321, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18406248, dtype=float32), 'test/wer': 0.05987853675380334, 'test/num_examples': 2472, 'score': 49004.61514925957, 'total_duration': 53768.963953733444, 'accumulated_submission_time': 49004.61514925957, 'accumulated_eval_time': 4759.919205904007, 'accumulated_logging_time': 1.8870255947113037}
I0215 12:20:39.200308 140318077859584 logging_writer.py:48] [57473] accumulated_eval_time=4759.919206, accumulated_logging_time=1.887026, accumulated_submission_time=49004.615149, global_step=57473, preemption_count=0, score=49004.615149, test/ctc_loss=0.18406248092651367, test/num_examples=2472, test/wer=0.059879, total_duration=53768.963954, train/ctc_loss=0.08137984573841095, train/wer=0.032583, validation/ctc_loss=0.34315621852874756, validation/num_examples=5348, validation/wer=0.096624
I0215 12:21:00.653005 140318069466880 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.684048056602478, loss=0.928276002407074
I0215 12:22:17.309222 140318077859584 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7460336089134216, loss=0.9316577315330505
I0215 12:23:37.740565 140318077859584 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8378241658210754, loss=0.8808188438415527
I0215 12:24:55.281719 140318069466880 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.770605742931366, loss=0.8956276178359985
I0215 12:26:12.559392 140318077859584 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.262583613395691, loss=0.9206997156143188
I0215 12:27:34.877200 140318069466880 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9257460236549377, loss=0.9305537343025208
I0215 12:29:01.276616 140318077859584 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8941729664802551, loss=0.9210774302482605
I0215 12:30:31.800341 140318069466880 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.096169352531433, loss=0.9458211660385132
I0215 12:32:00.589335 140318077859584 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.764832079410553, loss=1.0015417337417603
I0215 12:33:30.329736 140318069466880 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.7530907392501831, loss=0.9263453483581543
I0215 12:35:00.408164 140318077859584 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7535281181335449, loss=0.9058208465576172
I0215 12:36:32.547206 140318069466880 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.6890089511871338, loss=0.9273069500923157
I0215 12:38:03.696851 140318077859584 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8868057727813721, loss=0.9018334746360779
I0215 12:39:26.677024 140318077859584 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7575355172157288, loss=0.9466740489006042
I0215 12:40:46.737019 140318069466880 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7714675068855286, loss=0.8829291462898254
I0215 12:42:05.486690 140318077859584 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.7340235114097595, loss=0.943004310131073
I0215 12:43:30.724418 140318069466880 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.7282586097717285, loss=0.8858295679092407
I0215 12:44:39.631820 140399019657024 spec.py:321] Evaluating on the training split.
I0215 12:45:34.819795 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 12:46:29.060456 140399019657024 spec.py:349] Evaluating on the test split.
I0215 12:46:57.095245 140399019657024 submission_runner.py:408] Time since start: 55346.91s, 	Step: 59177, 	{'train/ctc_loss': Array(0.09032433, dtype=float32), 'train/wer': 0.036708405139503936, 'validation/ctc_loss': Array(0.34330922, dtype=float32), 'validation/wer': 0.09648860268206262, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18465194, dtype=float32), 'test/wer': 0.05902545040927833, 'test/num_examples': 2472, 'score': 50444.962716817856, 'total_duration': 55346.90739274025, 'accumulated_submission_time': 50444.962716817856, 'accumulated_eval_time': 4897.376502037048, 'accumulated_logging_time': 1.9495971202850342}
I0215 12:46:57.145715 140318077859584 logging_writer.py:48] [59177] accumulated_eval_time=4897.376502, accumulated_logging_time=1.949597, accumulated_submission_time=50444.962717, global_step=59177, preemption_count=0, score=50444.962717, test/ctc_loss=0.1846519410610199, test/num_examples=2472, test/wer=0.059025, total_duration=55346.907393, train/ctc_loss=0.09032432734966278, train/wer=0.036708, validation/ctc_loss=0.343309223651886, validation/num_examples=5348, validation/wer=0.096489
I0215 12:47:15.541938 140318069466880 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.7674368619918823, loss=0.9196761846542358
I0215 12:48:32.220981 140318077859584 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.7053400874137878, loss=0.9840580224990845
I0215 12:49:49.482035 140318069466880 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.667693018913269, loss=0.9778828024864197
I0215 12:51:18.615658 140318077859584 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.6191672086715698, loss=0.9079375863075256
I0215 12:52:49.427019 140318069466880 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.8216525912284851, loss=0.9409453272819519
I0215 12:54:19.530731 140318077859584 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7780764698982239, loss=0.8819811940193176
I0215 12:55:45.587031 140318077859584 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.6944193243980408, loss=0.8758272528648376
I0215 12:57:04.628954 140318069466880 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.7173200845718384, loss=0.9499778151512146
I0215 12:58:24.462129 140318077859584 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.2869772911071777, loss=0.9153295159339905
I0215 12:59:46.773895 140318069466880 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.907280445098877, loss=0.8994709849357605
I0215 13:01:14.490257 140318077859584 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.7928958535194397, loss=0.9104862809181213
I0215 13:02:45.191498 140318069466880 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.704888641834259, loss=0.9088611006736755
I0215 13:04:16.762648 140318077859584 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.7391437292098999, loss=0.9410955905914307
I0215 13:05:49.588704 140318069466880 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.6588364839553833, loss=0.9274331331253052
I0215 13:07:18.786405 140318077859584 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.7174003720283508, loss=0.91829913854599
I0215 13:08:45.177596 140318069466880 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7438734173774719, loss=0.9374275207519531
I0215 13:10:16.636612 140318077859584 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9669827222824097, loss=0.9426463842391968
I0215 13:10:57.496682 140399019657024 spec.py:321] Evaluating on the training split.
I0215 13:11:54.952706 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 13:12:49.614931 140399019657024 spec.py:349] Evaluating on the test split.
I0215 13:13:17.444911 140399019657024 submission_runner.py:408] Time since start: 56927.26s, 	Step: 60853, 	{'train/ctc_loss': Array(0.07665253, dtype=float32), 'train/wer': 0.030312266067800235, 'validation/ctc_loss': Array(0.33885056, dtype=float32), 'validation/wer': 0.09492454888633577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18213382, dtype=float32), 'test/wer': 0.05790831352954319, 'test/num_examples': 2472, 'score': 51885.22932291031, 'total_duration': 56927.25577759743, 'accumulated_submission_time': 51885.22932291031, 'accumulated_eval_time': 5037.317294836044, 'accumulated_logging_time': 2.0173354148864746}
I0215 13:13:17.495613 140318077859584 logging_writer.py:48] [60853] accumulated_eval_time=5037.317295, accumulated_logging_time=2.017335, accumulated_submission_time=51885.229323, global_step=60853, preemption_count=0, score=51885.229323, test/ctc_loss=0.18213382363319397, test/num_examples=2472, test/wer=0.057908, total_duration=56927.255778, train/ctc_loss=0.07665253430604935, train/wer=0.030312, validation/ctc_loss=0.33885055780410767, validation/num_examples=5348, validation/wer=0.094925
I0215 13:13:54.394864 140318069466880 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7002134323120117, loss=0.8876658082008362
I0215 13:15:11.407924 140318077859584 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.6573428511619568, loss=0.9353995323181152
I0215 13:16:28.140299 140318069466880 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.8057031631469727, loss=0.888843297958374
I0215 13:17:44.989885 140318077859584 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.6819365620613098, loss=0.8920338749885559
I0215 13:19:14.019873 140318069466880 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.8622531890869141, loss=0.8867531418800354
I0215 13:20:45.807062 140318077859584 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.7286635637283325, loss=0.8506912589073181
I0215 13:22:14.687079 140318069466880 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.6772958040237427, loss=0.8948138356208801
I0215 13:23:47.112222 140318077859584 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.760317862033844, loss=0.9166473746299744
I0215 13:25:17.354152 140318069466880 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7055618762969971, loss=0.9062063694000244
I0215 13:26:50.093339 140318077859584 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.673538327217102, loss=0.9166004657745361
I0215 13:28:07.524952 140318069466880 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.7018378376960754, loss=0.859205424785614
I0215 13:29:24.923510 140318077859584 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.7851539254188538, loss=0.8718181252479553
I0215 13:30:44.445104 140318069466880 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7328035235404968, loss=0.8725340962409973
I0215 13:32:09.610364 140318077859584 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.7361255288124084, loss=0.9164847135543823
I0215 13:33:41.004446 140318069466880 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0747716426849365, loss=0.8836795091629028
I0215 13:35:09.352520 140318077859584 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7357288002967834, loss=0.8890021443367004
I0215 13:36:39.615698 140318069466880 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.7209094166755676, loss=0.8932754397392273
I0215 13:37:18.435330 140399019657024 spec.py:321] Evaluating on the training split.
I0215 13:38:13.643126 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 13:39:06.667687 140399019657024 spec.py:349] Evaluating on the test split.
I0215 13:39:33.143383 140399019657024 submission_runner.py:408] Time since start: 58502.96s, 	Step: 62543, 	{'train/ctc_loss': Array(0.07401633, dtype=float32), 'train/wer': 0.031146656787486855, 'validation/ctc_loss': Array(0.3358002, dtype=float32), 'validation/wer': 0.09390115566197128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17903255, dtype=float32), 'test/wer': 0.05677086507017651, 'test/num_examples': 2472, 'score': 53326.08283543587, 'total_duration': 58502.95514130592, 'accumulated_submission_time': 53326.08283543587, 'accumulated_eval_time': 5172.018817186356, 'accumulated_logging_time': 2.085672378540039}
I0215 13:39:33.193855 140318077859584 logging_writer.py:48] [62543] accumulated_eval_time=5172.018817, accumulated_logging_time=2.085672, accumulated_submission_time=53326.082835, global_step=62543, preemption_count=0, score=53326.082835, test/ctc_loss=0.17903254926204681, test/num_examples=2472, test/wer=0.056771, total_duration=58502.955141, train/ctc_loss=0.07401633262634277, train/wer=0.031147, validation/ctc_loss=0.3358002007007599, validation/num_examples=5348, validation/wer=0.093901
I0215 13:40:17.409454 140318069466880 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.878772497177124, loss=0.8914157152175903
I0215 13:41:34.160720 140318077859584 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.8611963987350464, loss=0.9117104411125183
I0215 13:42:55.036470 140318069466880 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.7521567940711975, loss=0.8749845623970032
I0215 13:44:19.677862 140318077859584 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.7542635798454285, loss=0.912868082523346
I0215 13:45:38.046488 140318069466880 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.8482347130775452, loss=0.8812664151191711
I0215 13:47:00.151780 140318077859584 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.7307758927345276, loss=0.8817806839942932
I0215 13:48:26.021671 140318069466880 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.8783411979675293, loss=0.8997959494590759
I0215 13:49:54.678314 140318077859584 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.7013534307479858, loss=0.8944932222366333
I0215 13:51:23.991790 140318069466880 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7796947956085205, loss=0.9156655669212341
I0215 13:52:55.214049 140318077859584 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.71412593126297, loss=0.8752168416976929
I0215 13:54:26.461532 140318069466880 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8355613946914673, loss=0.9167114496231079
I0215 13:55:59.844570 140318077859584 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.8081640601158142, loss=0.8567183613777161
I0215 13:57:27.212020 140318069466880 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.7203989624977112, loss=0.8387845158576965
I0215 13:58:56.689085 140318077859584 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.6769847869873047, loss=0.9243060946464539
I0215 14:00:14.731362 140318069466880 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.704890251159668, loss=0.872491180896759
I0215 14:01:32.349298 140318077859584 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.6716960668563843, loss=0.8781216144561768
I0215 14:02:54.581597 140318069466880 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.6577449440956116, loss=0.8768471479415894
I0215 14:03:33.665121 140399019657024 spec.py:321] Evaluating on the training split.
I0215 14:04:28.822648 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 14:05:21.785165 140399019657024 spec.py:349] Evaluating on the test split.
I0215 14:05:49.287448 140399019657024 submission_runner.py:408] Time since start: 60079.10s, 	Step: 64246, 	{'train/ctc_loss': Array(0.06872757, dtype=float32), 'train/wer': 0.02779087231546751, 'validation/ctc_loss': Array(0.33404937, dtype=float32), 'validation/wer': 0.09387219170279117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17863517, dtype=float32), 'test/wer': 0.05679117664980805, 'test/num_examples': 2472, 'score': 54766.469485759735, 'total_duration': 60079.09964752197, 'accumulated_submission_time': 54766.469485759735, 'accumulated_eval_time': 5307.635042190552, 'accumulated_logging_time': 2.1523077487945557}
I0215 14:05:49.330947 140318077859584 logging_writer.py:48] [64246] accumulated_eval_time=5307.635042, accumulated_logging_time=2.152308, accumulated_submission_time=54766.469486, global_step=64246, preemption_count=0, score=54766.469486, test/ctc_loss=0.17863516509532928, test/num_examples=2472, test/wer=0.056791, total_duration=60079.099648, train/ctc_loss=0.06872756779193878, train/wer=0.027791, validation/ctc_loss=0.33404937386512756, validation/num_examples=5348, validation/wer=0.093872
I0215 14:06:31.605214 140318069466880 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.6633666157722473, loss=0.8960525393486023
I0215 14:07:48.142638 140318077859584 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9713062047958374, loss=0.8781189322471619
I0215 14:09:08.487079 140318069466880 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.7175240516662598, loss=0.8777287006378174
I0215 14:10:37.176192 140318077859584 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0949591398239136, loss=0.9105643033981323
I0215 14:12:04.603521 140318069466880 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.9474323391914368, loss=0.9118548631668091
I0215 14:13:37.081585 140318077859584 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.786084771156311, loss=0.8697984218597412
I0215 14:15:10.107409 140318077859584 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.9248729944229126, loss=0.9201256632804871
I0215 14:16:27.725383 140318069466880 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.0340639352798462, loss=0.9051635265350342
I0215 14:17:47.148228 140318077859584 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9024569392204285, loss=0.946519672870636
I0215 14:19:06.363639 140318069466880 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.847184419631958, loss=0.8924044370651245
I0215 14:20:34.323165 140318077859584 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.64472496509552, loss=0.8866326808929443
I0215 14:22:06.516112 140318069466880 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.8029538989067078, loss=0.8610491752624512
I0215 14:23:35.765114 140318077859584 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7331193685531616, loss=0.8429205417633057
I0215 14:25:06.080549 140318069466880 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.11884605884552, loss=0.8599912524223328
I0215 14:26:37.033669 140318077859584 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.8511390089988708, loss=0.88673996925354
I0215 14:28:09.275038 140318069466880 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.3565806150436401, loss=0.9514309167861938
I0215 14:29:37.491678 140318077859584 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8480263352394104, loss=0.9149795174598694
I0215 14:29:49.432197 140399019657024 spec.py:321] Evaluating on the training split.
I0215 14:30:43.679919 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 14:31:38.554692 140399019657024 spec.py:349] Evaluating on the test split.
I0215 14:32:05.181915 140399019657024 submission_runner.py:408] Time since start: 61654.99s, 	Step: 65915, 	{'train/ctc_loss': Array(0.07059194, dtype=float32), 'train/wer': 0.027976686094920898, 'validation/ctc_loss': Array(0.32888624, dtype=float32), 'validation/wer': 0.09201849831526304, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17421615, dtype=float32), 'test/wer': 0.05530843133670506, 'test/num_examples': 2472, 'score': 56206.484684705734, 'total_duration': 61654.99394798279, 'accumulated_submission_time': 56206.484684705734, 'accumulated_eval_time': 5443.378481864929, 'accumulated_logging_time': 2.2151148319244385}
I0215 14:32:05.226595 140318077859584 logging_writer.py:48] [65915] accumulated_eval_time=5443.378482, accumulated_logging_time=2.215115, accumulated_submission_time=56206.484685, global_step=65915, preemption_count=0, score=56206.484685, test/ctc_loss=0.1742161512374878, test/num_examples=2472, test/wer=0.055308, total_duration=61654.993948, train/ctc_loss=0.07059194147586823, train/wer=0.027977, validation/ctc_loss=0.3288862407207489, validation/num_examples=5348, validation/wer=0.092018
I0215 14:33:16.583903 140318077859584 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.8882062435150146, loss=0.8665449619293213
I0215 14:34:35.817587 140318069466880 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.7209200859069824, loss=0.8684730529785156
I0215 14:35:55.379900 140318077859584 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.973314642906189, loss=0.8231772780418396
I0215 14:37:20.464824 140318069466880 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3257278203964233, loss=0.8425854444503784
I0215 14:38:49.949518 140318077859584 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.6604135036468506, loss=0.8621370196342468
I0215 14:40:21.676855 140318069466880 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9200162887573242, loss=0.8924304246902466
I0215 14:41:55.219463 140318077859584 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.8258633613586426, loss=0.8791916966438293
I0215 14:43:25.377449 140318069466880 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8675269484519958, loss=0.8355906009674072
I0215 14:44:54.081874 140318077859584 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.6979774236679077, loss=0.8364405632019043
I0215 14:46:25.269862 140318069466880 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.8558644652366638, loss=0.8583459854125977
I0215 14:47:51.833752 140318077859584 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.693602979183197, loss=0.8302990794181824
I0215 14:49:08.746229 140318069466880 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.8825996518135071, loss=0.8308689594268799
I0215 14:50:29.512242 140318077859584 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.9183952808380127, loss=0.8572434186935425
I0215 14:51:53.256821 140318069466880 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.8759182691574097, loss=0.8609035015106201
I0215 14:53:17.611164 140318077859584 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.9403867721557617, loss=0.8853174448013306
I0215 14:54:48.023394 140318069466880 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.8703572154045105, loss=0.8539512157440186
I0215 14:56:05.881505 140399019657024 spec.py:321] Evaluating on the training split.
I0215 14:57:00.836658 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 14:57:54.760199 140399019657024 spec.py:349] Evaluating on the test split.
I0215 14:58:21.860041 140399019657024 submission_runner.py:408] Time since start: 63231.67s, 	Step: 67587, 	{'train/ctc_loss': Array(0.07291302, dtype=float32), 'train/wer': 0.02871408002715383, 'validation/ctc_loss': Array(0.32705754, dtype=float32), 'validation/wer': 0.09051237243789645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17488703, dtype=float32), 'test/wer': 0.05520687343854731, 'test/num_examples': 2472, 'score': 57647.05547738075, 'total_duration': 63231.67115473747, 'accumulated_submission_time': 57647.05547738075, 'accumulated_eval_time': 5579.349866390228, 'accumulated_logging_time': 2.2771389484405518}
I0215 14:58:21.904247 140318077859584 logging_writer.py:48] [67587] accumulated_eval_time=5579.349866, accumulated_logging_time=2.277139, accumulated_submission_time=57647.055477, global_step=67587, preemption_count=0, score=57647.055477, test/ctc_loss=0.1748870313167572, test/num_examples=2472, test/wer=0.055207, total_duration=63231.671155, train/ctc_loss=0.0729130208492279, train/wer=0.028714, validation/ctc_loss=0.32705754041671753, validation/num_examples=5348, validation/wer=0.090512
I0215 14:58:32.645021 140318069466880 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9667240381240845, loss=0.8775531053543091
I0215 14:59:49.192224 140318077859584 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0544637441635132, loss=0.8913003206253052
I0215 15:01:05.816570 140318069466880 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.8277770280838013, loss=0.8760582208633423
I0215 15:02:34.961494 140318077859584 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.1711329221725464, loss=0.8296403884887695
I0215 15:04:03.423390 140318077859584 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0182245969772339, loss=0.8348369002342224
I0215 15:05:22.274307 140318069466880 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.7767882943153381, loss=0.8939505815505981
I0215 15:06:42.829691 140318077859584 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.974936842918396, loss=0.8635233044624329
I0215 15:08:03.629138 140318069466880 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.9533007740974426, loss=0.8702400326728821
I0215 15:09:31.826846 140318077859584 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.8792659640312195, loss=0.8529648184776306
I0215 15:11:02.511753 140318069466880 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.7683837413787842, loss=0.8208501935005188
I0215 15:12:34.254649 140318077859584 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7636569142341614, loss=0.8620133996009827
I0215 15:14:05.561367 140318069466880 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.0836554765701294, loss=0.8189705610275269
I0215 15:15:37.001541 140318077859584 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.9803311228752136, loss=0.8633831739425659
I0215 15:17:08.543338 140318069466880 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.7916733026504517, loss=0.8662330508232117
I0215 15:18:37.291393 140318077859584 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.7488300800323486, loss=0.8350363373756409
I0215 15:19:59.633567 140318077859584 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.6704771518707275, loss=0.8555130362510681
I0215 15:21:17.629275 140318069466880 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.12843918800354, loss=0.8836228251457214
I0215 15:22:22.238743 140399019657024 spec.py:321] Evaluating on the training split.
I0215 15:23:16.507797 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 15:24:10.220380 140399019657024 spec.py:349] Evaluating on the test split.
I0215 15:24:37.582333 140399019657024 submission_runner.py:408] Time since start: 64807.40s, 	Step: 69280, 	{'train/ctc_loss': Array(0.06055355, dtype=float32), 'train/wer': 0.024073547531039713, 'validation/ctc_loss': Array(0.329274, dtype=float32), 'validation/wer': 0.09141025517248037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1747135, dtype=float32), 'test/wer': 0.05545061239412589, 'test/num_examples': 2472, 'score': 59087.30037307739, 'total_duration': 64807.39521574974, 'accumulated_submission_time': 59087.30037307739, 'accumulated_eval_time': 5714.688037395477, 'accumulated_logging_time': 2.3428428173065186}
I0215 15:24:37.623295 140318077859584 logging_writer.py:48] [69280] accumulated_eval_time=5714.688037, accumulated_logging_time=2.342843, accumulated_submission_time=59087.300373, global_step=69280, preemption_count=0, score=59087.300373, test/ctc_loss=0.17471350729465485, test/num_examples=2472, test/wer=0.055451, total_duration=64807.395216, train/ctc_loss=0.06055355444550514, train/wer=0.024074, validation/ctc_loss=0.3292739987373352, validation/num_examples=5348, validation/wer=0.091410
I0215 15:24:53.942505 140318069466880 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.830269992351532, loss=0.827545702457428
I0215 15:26:10.672043 140318077859584 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.8021144270896912, loss=0.8841527700424194
I0215 15:27:27.659693 140318069466880 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.9960487484931946, loss=0.8674522638320923
I0215 15:28:54.273090 140318077859584 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.7070279121398926, loss=0.8587295413017273
I0215 15:30:25.433861 140318069466880 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.9579351544380188, loss=0.8268392086029053
I0215 15:31:55.474007 140318077859584 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.7233072519302368, loss=0.8032741546630859
I0215 15:33:27.234373 140318069466880 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.8440852761268616, loss=0.8377341628074646
I0215 15:34:54.416556 140318077859584 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.8128156661987305, loss=0.8129535913467407
I0215 15:36:19.313985 140318077859584 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.8709161281585693, loss=0.8502106666564941
I0215 15:37:38.485567 140318069466880 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.823554277420044, loss=0.8360995650291443
I0215 15:38:59.177751 140318077859584 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.055304765701294, loss=0.841635525226593
I0215 15:40:25.746455 140318069466880 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.7193841338157654, loss=0.8559319972991943
I0215 15:41:53.636251 140318077859584 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.7650008797645569, loss=0.8862863779067993
I0215 15:43:23.533838 140318069466880 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.6697266697883606, loss=0.8624745607376099
I0215 15:44:49.168885 140318077859584 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0020464658737183, loss=0.8625534772872925
I0215 15:46:16.833705 140318069466880 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.6776235699653625, loss=0.8824595212936401
I0215 15:47:47.794995 140318077859584 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8440788388252258, loss=0.8457359671592712
I0215 15:48:38.336321 140399019657024 spec.py:321] Evaluating on the training split.
I0215 15:49:32.021466 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 15:50:25.711569 140399019657024 spec.py:349] Evaluating on the test split.
I0215 15:50:52.999653 140399019657024 submission_runner.py:408] Time since start: 66382.81s, 	Step: 70958, 	{'train/ctc_loss': Array(0.06268328, dtype=float32), 'train/wer': 0.024154338909627173, 'validation/ctc_loss': Array(0.3256981, dtype=float32), 'validation/wer': 0.08989447464205373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17322181, dtype=float32), 'test/wer': 0.054617837629232426, 'test/num_examples': 2472, 'score': 60527.92716932297, 'total_duration': 66382.81089735031, 'accumulated_submission_time': 60527.92716932297, 'accumulated_eval_time': 5849.344304800034, 'accumulated_logging_time': 2.4029757976531982}
I0215 15:50:53.048061 140318077859584 logging_writer.py:48] [70958] accumulated_eval_time=5849.344305, accumulated_logging_time=2.402976, accumulated_submission_time=60527.927169, global_step=70958, preemption_count=0, score=60527.927169, test/ctc_loss=0.17322181165218353, test/num_examples=2472, test/wer=0.054618, total_duration=66382.810897, train/ctc_loss=0.06268327683210373, train/wer=0.024154, validation/ctc_loss=0.3256981074810028, validation/num_examples=5348, validation/wer=0.089894
I0215 15:51:25.945923 140318069466880 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1638240814208984, loss=0.8861420154571533
I0215 15:52:47.001770 140318077859584 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.7560493350028992, loss=0.8408684730529785
I0215 15:54:05.868376 140318069466880 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.0618784427642822, loss=0.8796219229698181
I0215 15:55:23.891196 140318077859584 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8368021845817566, loss=0.8500995635986328
I0215 15:56:46.523658 140318069466880 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.7517539858818054, loss=0.8646754622459412
I0215 15:58:15.348733 140318077859584 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.7519028186798096, loss=0.8658142685890198
I0215 15:59:44.881542 140318069466880 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.4136654138565063, loss=0.8144399523735046
I0215 15:59:54.096492 140318077859584 logging_writer.py:48] [71611] global_step=71611, preemption_count=0, score=61068.909403
I0215 15:59:54.988656 140399019657024 checkpoints.py:490] Saving checkpoint at step: 71611
I0215 15:59:56.640582 140399019657024 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2/checkpoint_71611
I0215 15:59:56.677823 140399019657024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_2/checkpoint_71611.
I0215 16:00:01.460480 140399019657024 submission_runner.py:583] Tuning trial 2/5
I0215 16:00:01.460744 140399019657024 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0215 16:00:01.484284 140399019657024 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.387848, dtype=float32), 'train/wer': 1.3955485058197952, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043146644525329, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.279486, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 35.57791781425476, 'total_duration': 174.5587763786316, 'accumulated_submission_time': 35.57791781425476, 'accumulated_eval_time': 138.98080229759216, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1685, {'train/ctc_loss': Array(6.3702335, dtype=float32), 'train/wer': 0.936353811149033, 'validation/ctc_loss': Array(6.3203735, dtype=float32), 'validation/wer': 0.895440107359742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.330534, dtype=float32), 'test/wer': 0.8973046533828936, 'test/num_examples': 2472, 'score': 1475.9603667259216, 'total_duration': 1728.0618832111359, 'accumulated_submission_time': 1475.9603667259216, 'accumulated_eval_time': 251.99166750907898, 'accumulated_logging_time': 0.03572487831115723, 'global_step': 1685, 'preemption_count': 0}), (3376, {'train/ctc_loss': Array(2.8703103, dtype=float32), 'train/wer': 0.5950224159334732, 'validation/ctc_loss': Array(2.8294756, dtype=float32), 'validation/wer': 0.5699334794404163, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5039368, dtype=float32), 'test/wer': 0.5188999248471554, 'test/num_examples': 2472, 'score': 2916.4242436885834, 'total_duration': 3300.489486694336, 'accumulated_submission_time': 2916.4242436885834, 'accumulated_eval_time': 383.8297390937805, 'accumulated_logging_time': 0.08650851249694824, 'global_step': 3376, 'preemption_count': 0}), (5050, {'train/ctc_loss': Array(0.83288306, dtype=float32), 'train/wer': 0.2686575402958711, 'validation/ctc_loss': Array(0.9431758, dtype=float32), 'validation/wer': 0.28080558425132995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66521096, dtype=float32), 'test/wer': 0.2190197631669815, 'test/num_examples': 2472, 'score': 4356.522769451141, 'total_duration': 4878.47324180603, 'accumulated_submission_time': 4356.522769451141, 'accumulated_eval_time': 521.587308883667, 'accumulated_logging_time': 0.14109587669372559, 'global_step': 5050, 'preemption_count': 0}), (6740, {'train/ctc_loss': Array(0.6040004, dtype=float32), 'train/wer': 0.2079973718639594, 'validation/ctc_loss': Array(0.74008816, dtype=float32), 'validation/wer': 0.22464446740106395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48961702, dtype=float32), 'test/wer': 0.16718461194727113, 'test/num_examples': 2472, 'score': 5797.023282766342, 'total_duration': 6451.637514591217, 'accumulated_submission_time': 5797.023282766342, 'accumulated_eval_time': 654.1273121833801, 'accumulated_logging_time': 0.18926143646240234, 'global_step': 6740, 'preemption_count': 0}), (8450, {'train/ctc_loss': Array(0.49352032, dtype=float32), 'train/wer': 0.17106892623027514, 'validation/ctc_loss': Array(0.6651629, dtype=float32), 'validation/wer': 0.20105814997538063, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4210605, dtype=float32), 'test/wer': 0.14262791217272966, 'test/num_examples': 2472, 'score': 7236.972342252731, 'total_duration': 8024.497707366943, 'accumulated_submission_time': 7236.972342252731, 'accumulated_eval_time': 786.9121985435486, 'accumulated_logging_time': 0.2403411865234375, 'global_step': 8450, 'preemption_count': 0}), (10149, {'train/ctc_loss': Array(0.29691428, dtype=float32), 'train/wer': 0.11104616561809216, 'validation/ctc_loss': Array(0.6043157, dtype=float32), 'validation/wer': 0.18619963891597555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3722067, dtype=float32), 'test/wer': 0.12629740214896512, 'test/num_examples': 2472, 'score': 8677.129787445068, 'total_duration': 9625.68328166008, 'accumulated_submission_time': 8677.129787445068, 'accumulated_eval_time': 947.8132953643799, 'accumulated_logging_time': 0.2935667037963867, 'global_step': 10149, 'preemption_count': 0}), (11875, {'train/ctc_loss': Array(0.27041242, dtype=float32), 'train/wer': 0.09958803696757804, 'validation/ctc_loss': Array(0.57668215, dtype=float32), 'validation/wer': 0.17535746352954806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35247084, dtype=float32), 'test/wer': 0.1216257388337091, 'test/num_examples': 2472, 'score': 10117.744185447693, 'total_duration': 11204.992297172546, 'accumulated_submission_time': 10117.744185447693, 'accumulated_eval_time': 1086.3854405879974, 'accumulated_logging_time': 0.3410470485687256, 'global_step': 11875, 'preemption_count': 0}), (13572, {'train/ctc_loss': Array(0.26092592, dtype=float32), 'train/wer': 0.0966991728111208, 'validation/ctc_loss': Array(0.54714733, dtype=float32), 'validation/wer': 0.1656352278980855, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33199945, dtype=float32), 'test/wer': 0.11339954908293218, 'test/num_examples': 2472, 'score': 11557.835375070572, 'total_duration': 12781.041088342667, 'accumulated_submission_time': 11557.835375070572, 'accumulated_eval_time': 1222.2144269943237, 'accumulated_logging_time': 0.39313745498657227, 'global_step': 13572, 'preemption_count': 0}), (15238, {'train/ctc_loss': Array(0.2348248, dtype=float32), 'train/wer': 0.08926558978211871, 'validation/ctc_loss': Array(0.5383248, dtype=float32), 'validation/wer': 0.16437046834721994, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3189678, dtype=float32), 'test/wer': 0.11104340584567261, 'test/num_examples': 2472, 'score': 12997.719169139862, 'total_duration': 14359.018620729446, 'accumulated_submission_time': 12997.719169139862, 'accumulated_eval_time': 1360.179630279541, 'accumulated_logging_time': 0.4491429328918457, 'global_step': 15238, 'preemption_count': 0}), (16950, {'train/ctc_loss': Array(0.22629037, dtype=float32), 'train/wer': 0.0866338613111835, 'validation/ctc_loss': Array(0.51311344, dtype=float32), 'validation/wer': 0.15726464369502882, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30607492, dtype=float32), 'test/wer': 0.10618893831373266, 'test/num_examples': 2472, 'score': 14438.15297293663, 'total_duration': 15937.055542230606, 'accumulated_submission_time': 14438.15297293663, 'accumulated_eval_time': 1497.6522102355957, 'accumulated_logging_time': 0.5024521350860596, 'global_step': 16950, 'preemption_count': 0}), (18641, {'train/ctc_loss': Array(0.20851934, dtype=float32), 'train/wer': 0.08079113552095911, 'validation/ctc_loss': Array(0.4994007, dtype=float32), 'validation/wer': 0.15180976471610494, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2961128, dtype=float32), 'test/wer': 0.10027826864095221, 'test/num_examples': 2472, 'score': 15878.68928694725, 'total_duration': 17512.57652759552, 'accumulated_submission_time': 15878.68928694725, 'accumulated_eval_time': 1632.5123000144958, 'accumulated_logging_time': 0.5541045665740967, 'global_step': 18641, 'preemption_count': 0}), (20329, {'train/ctc_loss': Array(0.22626519, dtype=float32), 'train/wer': 0.08245879474633015, 'validation/ctc_loss': Array(0.4777417, dtype=float32), 'validation/wer': 0.14654797879838188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28475207, dtype=float32), 'test/wer': 0.09780025592590336, 'test/num_examples': 2472, 'score': 17319.13134288788, 'total_duration': 19089.508982419968, 'accumulated_submission_time': 17319.13134288788, 'accumulated_eval_time': 1768.8725280761719, 'accumulated_logging_time': 0.6105937957763672, 'global_step': 20329, 'preemption_count': 0}), (22034, {'train/ctc_loss': Array(0.20196249, dtype=float32), 'train/wer': 0.078254877014419, 'validation/ctc_loss': Array(0.47147408, dtype=float32), 'validation/wer': 0.1439701864313506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27821675, dtype=float32), 'test/wer': 0.09503788109601284, 'test/num_examples': 2472, 'score': 18759.682195663452, 'total_duration': 20664.581153154373, 'accumulated_submission_time': 18759.682195663452, 'accumulated_eval_time': 1903.2603197097778, 'accumulated_logging_time': 0.6675965785980225, 'global_step': 22034, 'preemption_count': 0}), (23710, {'train/ctc_loss': Array(0.16809237, dtype=float32), 'train/wer': 0.06681527265893358, 'validation/ctc_loss': Array(0.46198797, dtype=float32), 'validation/wer': 0.13954835532985121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26943246, dtype=float32), 'test/wer': 0.09207239046980684, 'test/num_examples': 2472, 'score': 20199.590856790543, 'total_duration': 22239.059130191803, 'accumulated_submission_time': 20199.590856790543, 'accumulated_eval_time': 2037.694198846817, 'accumulated_logging_time': 0.728062629699707, 'global_step': 23710, 'preemption_count': 0}), (25401, {'train/ctc_loss': Array(0.1630345, dtype=float32), 'train/wer': 0.06337915430425409, 'validation/ctc_loss': Array(0.4471465, dtype=float32), 'validation/wer': 0.13672919663631888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26178136, dtype=float32), 'test/wer': 0.0884163061361282, 'test/num_examples': 2472, 'score': 21639.50814318657, 'total_duration': 23815.393007278442, 'accumulated_submission_time': 21639.50814318657, 'accumulated_eval_time': 2173.9809906482697, 'accumulated_logging_time': 0.7831501960754395, 'global_step': 25401, 'preemption_count': 0}), (27108, {'train/ctc_loss': Array(0.15664186, dtype=float32), 'train/wer': 0.06248922878069797, 'validation/ctc_loss': Array(0.43819365, dtype=float32), 'validation/wer': 0.13243287602460005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25654113, dtype=float32), 'test/wer': 0.08666951028781508, 'test/num_examples': 2472, 'score': 23079.609487771988, 'total_duration': 25391.072680950165, 'accumulated_submission_time': 23079.609487771988, 'accumulated_eval_time': 2309.429753303528, 'accumulated_logging_time': 0.8377275466918945, 'global_step': 27108, 'preemption_count': 0}), (28772, {'train/ctc_loss': Array(0.15958466, dtype=float32), 'train/wer': 0.061006696685088645, 'validation/ctc_loss': Array(0.42708495, dtype=float32), 'validation/wer': 0.12850343222916286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24707821, dtype=float32), 'test/wer': 0.08382588913939837, 'test/num_examples': 2472, 'score': 24519.481394052505, 'total_duration': 26966.824649333954, 'accumulated_submission_time': 24519.481394052505, 'accumulated_eval_time': 2445.1812682151794, 'accumulated_logging_time': 0.892916202545166, 'global_step': 28772, 'preemption_count': 0}), (30461, {'train/ctc_loss': Array(0.15726769, dtype=float32), 'train/wer': 0.0611525223303841, 'validation/ctc_loss': Array(0.43288928, dtype=float32), 'validation/wer': 0.1284261950046825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24584797, dtype=float32), 'test/wer': 0.08362277334308289, 'test/num_examples': 2472, 'score': 25959.484798431396, 'total_duration': 28543.738730192184, 'accumulated_submission_time': 25959.484798431396, 'accumulated_eval_time': 2581.9604048728943, 'accumulated_logging_time': 0.948739767074585, 'global_step': 30461, 'preemption_count': 0}), (32179, {'train/ctc_loss': Array(0.14783192, dtype=float32), 'train/wer': 0.059060555431988286, 'validation/ctc_loss': Array(0.42411798, dtype=float32), 'validation/wer': 0.12668835745387488, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23941892, dtype=float32), 'test/wer': 0.08090102167245547, 'test/num_examples': 2472, 'score': 27399.637142419815, 'total_duration': 30121.05331158638, 'accumulated_submission_time': 27399.637142419815, 'accumulated_eval_time': 2718.995190382004, 'accumulated_logging_time': 1.0019807815551758, 'global_step': 32179, 'preemption_count': 0}), (33870, {'train/ctc_loss': Array(0.14385405, dtype=float32), 'train/wer': 0.0571283357173921, 'validation/ctc_loss': Array(0.41215566, dtype=float32), 'validation/wer': 0.12275891365843769, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23393744, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 28839.877410888672, 'total_duration': 31696.38317656517, 'accumulated_submission_time': 28839.877410888672, 'accumulated_eval_time': 2853.9519302845, 'accumulated_logging_time': 1.0594327449798584, 'global_step': 33870, 'preemption_count': 0}), (35559, {'train/ctc_loss': Array(0.1265491, dtype=float32), 'train/wer': 0.05118336487918554, 'validation/ctc_loss': Array(0.4120602, dtype=float32), 'validation/wer': 0.12215067051565502, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23113732, dtype=float32), 'test/wer': 0.07594499624235777, 'test/num_examples': 2472, 'score': 30280.395092010498, 'total_duration': 33274.03078866005, 'accumulated_submission_time': 30280.395092010498, 'accumulated_eval_time': 2990.9510576725006, 'accumulated_logging_time': 1.1162869930267334, 'global_step': 35559, 'preemption_count': 0}), (37254, {'train/ctc_loss': Array(0.12382909, dtype=float32), 'train/wer': 0.04804198627371821, 'validation/ctc_loss': Array(0.40429664, dtype=float32), 'validation/wer': 0.12002664684244571, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22469771, dtype=float32), 'test/wer': 0.07527471411451668, 'test/num_examples': 2472, 'score': 31721.076458215714, 'total_duration': 34852.249284267426, 'accumulated_submission_time': 31721.076458215714, 'accumulated_eval_time': 3128.3551211357117, 'accumulated_logging_time': 1.1727495193481445, 'global_step': 37254, 'preemption_count': 0}), (38930, {'train/ctc_loss': Array(0.13134545, dtype=float32), 'train/wer': 0.05106037654187405, 'validation/ctc_loss': Array(0.39354557, dtype=float32), 'validation/wer': 0.11500622725122372, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2211299, dtype=float32), 'test/wer': 0.07440131619036013, 'test/num_examples': 2472, 'score': 33161.42921876907, 'total_duration': 36430.21849322319, 'accumulated_submission_time': 33161.42921876907, 'accumulated_eval_time': 3265.838086128235, 'accumulated_logging_time': 1.232497215270996, 'global_step': 38930, 'preemption_count': 0}), (40629, {'train/ctc_loss': Array(0.11861537, dtype=float32), 'train/wer': 0.046957308071536356, 'validation/ctc_loss': Array(0.38333464, dtype=float32), 'validation/wer': 0.11374146770035819, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21496634, dtype=float32), 'test/wer': 0.07261389718278391, 'test/num_examples': 2472, 'score': 34601.42512226105, 'total_duration': 38008.17625498772, 'accumulated_submission_time': 34601.42512226105, 'accumulated_eval_time': 3403.660226583481, 'accumulated_logging_time': 1.2956340312957764, 'global_step': 40629, 'preemption_count': 0}), (42306, {'train/ctc_loss': Array(0.10947188, dtype=float32), 'train/wer': 0.045603775520673025, 'validation/ctc_loss': Array(0.3845979, dtype=float32), 'validation/wer': 0.11188777431283006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21081606, dtype=float32), 'test/wer': 0.06944529076026243, 'test/num_examples': 2472, 'score': 36041.619644880295, 'total_duration': 39584.84460020065, 'accumulated_submission_time': 36041.619644880295, 'accumulated_eval_time': 3540.007214784622, 'accumulated_logging_time': 1.3486223220825195, 'global_step': 42306, 'preemption_count': 0}), (44006, {'train/ctc_loss': Array(0.13150723, dtype=float32), 'train/wer': 0.04866141229777594, 'validation/ctc_loss': Array(0.38188517, dtype=float32), 'validation/wer': 0.11146296957818821, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21202654, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 37481.56066060066, 'total_duration': 41161.54116153717, 'accumulated_submission_time': 37481.56066060066, 'accumulated_eval_time': 3676.6261253356934, 'accumulated_logging_time': 1.4088342189788818, 'global_step': 44006, 'preemption_count': 0}), (45720, {'train/ctc_loss': Array(0.08845052, dtype=float32), 'train/wer': 0.03545131981048875, 'validation/ctc_loss': Array(0.37528655, dtype=float32), 'validation/wer': 0.10923274472131844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20303267, dtype=float32), 'test/wer': 0.06775942965084394, 'test/num_examples': 2472, 'score': 38921.45214056969, 'total_duration': 42739.1587574482, 'accumulated_submission_time': 38921.45214056969, 'accumulated_eval_time': 3814.218501806259, 'accumulated_logging_time': 1.4656569957733154, 'global_step': 45720, 'preemption_count': 0}), (47396, {'train/ctc_loss': Array(0.09425022, dtype=float32), 'train/wer': 0.03792730295315416, 'validation/ctc_loss': Array(0.37133262, dtype=float32), 'validation/wer': 0.1077169641908918, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20532721, dtype=float32), 'test/wer': 0.06745475595637072, 'test/num_examples': 2472, 'score': 40362.0363907814, 'total_duration': 44316.299813985825, 'accumulated_submission_time': 40362.0363907814, 'accumulated_eval_time': 3950.640738248825, 'accumulated_logging_time': 1.5266199111938477, 'global_step': 47396, 'preemption_count': 0}), (49093, {'train/ctc_loss': Array(0.11438162, dtype=float32), 'train/wer': 0.04541957042671532, 'validation/ctc_loss': Array(0.36344072, dtype=float32), 'validation/wer': 0.10611429178292478, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19545701, dtype=float32), 'test/wer': 0.06552515589137367, 'test/num_examples': 2472, 'score': 41801.95665073395, 'total_duration': 45891.41930747032, 'accumulated_submission_time': 41801.95665073395, 'accumulated_eval_time': 4085.706921339035, 'accumulated_logging_time': 1.5839388370513916, 'global_step': 49093, 'preemption_count': 0}), (50786, {'train/ctc_loss': Array(0.1178861, dtype=float32), 'train/wer': 0.04611315356079384, 'validation/ctc_loss': Array(0.360941, dtype=float32), 'validation/wer': 0.10384544831381484, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19716543, dtype=float32), 'test/wer': 0.06408303373753377, 'test/num_examples': 2472, 'score': 43242.034012556076, 'total_duration': 47467.480123758316, 'accumulated_submission_time': 43242.034012556076, 'accumulated_eval_time': 4221.55396938324, 'accumulated_logging_time': 1.6436846256256104, 'global_step': 50786, 'preemption_count': 0}), (52446, {'train/ctc_loss': Array(0.12866937, dtype=float32), 'train/wer': 0.05156511045108595, 'validation/ctc_loss': Array(0.3551715, dtype=float32), 'validation/wer': 0.10201106423240681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19476175, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 44682.5635433197, 'total_duration': 49041.885318517685, 'accumulated_submission_time': 44682.5635433197, 'accumulated_eval_time': 4355.304067373276, 'accumulated_logging_time': 1.6991455554962158, 'global_step': 52446, 'preemption_count': 0}), (54122, {'train/ctc_loss': Array(0.11104356, dtype=float32), 'train/wer': 0.042604765817584225, 'validation/ctc_loss': Array(0.3560681, dtype=float32), 'validation/wer': 0.10204002819158693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19035426, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 46123.199763059616, 'total_duration': 50617.27509188652, 'accumulated_submission_time': 46123.199763059616, 'accumulated_eval_time': 4489.92017698288, 'accumulated_logging_time': 1.7626543045043945, 'global_step': 54122, 'preemption_count': 0}), (55801, {'train/ctc_loss': Array(0.09821409, dtype=float32), 'train/wer': 0.039273090992869145, 'validation/ctc_loss': Array(0.3484534, dtype=float32), 'validation/wer': 0.09916294157969434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18869571, dtype=float32), 'test/wer': 0.06150346312432718, 'test/num_examples': 2472, 'score': 47563.62722706795, 'total_duration': 52191.360865831375, 'accumulated_submission_time': 47563.62722706795, 'accumulated_eval_time': 4623.43222784996, 'accumulated_logging_time': 1.8321900367736816, 'global_step': 55801, 'preemption_count': 0}), (57473, {'train/ctc_loss': Array(0.08137985, dtype=float32), 'train/wer': 0.03258301206317622, 'validation/ctc_loss': Array(0.34315622, dtype=float32), 'validation/wer': 0.09662376782490321, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18406248, dtype=float32), 'test/wer': 0.05987853675380334, 'test/num_examples': 2472, 'score': 49004.61514925957, 'total_duration': 53768.963953733444, 'accumulated_submission_time': 49004.61514925957, 'accumulated_eval_time': 4759.919205904007, 'accumulated_logging_time': 1.8870255947113037, 'global_step': 57473, 'preemption_count': 0}), (59177, {'train/ctc_loss': Array(0.09032433, dtype=float32), 'train/wer': 0.036708405139503936, 'validation/ctc_loss': Array(0.34330922, dtype=float32), 'validation/wer': 0.09648860268206262, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18465194, dtype=float32), 'test/wer': 0.05902545040927833, 'test/num_examples': 2472, 'score': 50444.962716817856, 'total_duration': 55346.90739274025, 'accumulated_submission_time': 50444.962716817856, 'accumulated_eval_time': 4897.376502037048, 'accumulated_logging_time': 1.9495971202850342, 'global_step': 59177, 'preemption_count': 0}), (60853, {'train/ctc_loss': Array(0.07665253, dtype=float32), 'train/wer': 0.030312266067800235, 'validation/ctc_loss': Array(0.33885056, dtype=float32), 'validation/wer': 0.09492454888633577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18213382, dtype=float32), 'test/wer': 0.05790831352954319, 'test/num_examples': 2472, 'score': 51885.22932291031, 'total_duration': 56927.25577759743, 'accumulated_submission_time': 51885.22932291031, 'accumulated_eval_time': 5037.317294836044, 'accumulated_logging_time': 2.0173354148864746, 'global_step': 60853, 'preemption_count': 0}), (62543, {'train/ctc_loss': Array(0.07401633, dtype=float32), 'train/wer': 0.031146656787486855, 'validation/ctc_loss': Array(0.3358002, dtype=float32), 'validation/wer': 0.09390115566197128, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17903255, dtype=float32), 'test/wer': 0.05677086507017651, 'test/num_examples': 2472, 'score': 53326.08283543587, 'total_duration': 58502.95514130592, 'accumulated_submission_time': 53326.08283543587, 'accumulated_eval_time': 5172.018817186356, 'accumulated_logging_time': 2.085672378540039, 'global_step': 62543, 'preemption_count': 0}), (64246, {'train/ctc_loss': Array(0.06872757, dtype=float32), 'train/wer': 0.02779087231546751, 'validation/ctc_loss': Array(0.33404937, dtype=float32), 'validation/wer': 0.09387219170279117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17863517, dtype=float32), 'test/wer': 0.05679117664980805, 'test/num_examples': 2472, 'score': 54766.469485759735, 'total_duration': 60079.09964752197, 'accumulated_submission_time': 54766.469485759735, 'accumulated_eval_time': 5307.635042190552, 'accumulated_logging_time': 2.1523077487945557, 'global_step': 64246, 'preemption_count': 0}), (65915, {'train/ctc_loss': Array(0.07059194, dtype=float32), 'train/wer': 0.027976686094920898, 'validation/ctc_loss': Array(0.32888624, dtype=float32), 'validation/wer': 0.09201849831526304, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17421615, dtype=float32), 'test/wer': 0.05530843133670506, 'test/num_examples': 2472, 'score': 56206.484684705734, 'total_duration': 61654.99394798279, 'accumulated_submission_time': 56206.484684705734, 'accumulated_eval_time': 5443.378481864929, 'accumulated_logging_time': 2.2151148319244385, 'global_step': 65915, 'preemption_count': 0}), (67587, {'train/ctc_loss': Array(0.07291302, dtype=float32), 'train/wer': 0.02871408002715383, 'validation/ctc_loss': Array(0.32705754, dtype=float32), 'validation/wer': 0.09051237243789645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17488703, dtype=float32), 'test/wer': 0.05520687343854731, 'test/num_examples': 2472, 'score': 57647.05547738075, 'total_duration': 63231.67115473747, 'accumulated_submission_time': 57647.05547738075, 'accumulated_eval_time': 5579.349866390228, 'accumulated_logging_time': 2.2771389484405518, 'global_step': 67587, 'preemption_count': 0}), (69280, {'train/ctc_loss': Array(0.06055355, dtype=float32), 'train/wer': 0.024073547531039713, 'validation/ctc_loss': Array(0.329274, dtype=float32), 'validation/wer': 0.09141025517248037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1747135, dtype=float32), 'test/wer': 0.05545061239412589, 'test/num_examples': 2472, 'score': 59087.30037307739, 'total_duration': 64807.39521574974, 'accumulated_submission_time': 59087.30037307739, 'accumulated_eval_time': 5714.688037395477, 'accumulated_logging_time': 2.3428428173065186, 'global_step': 69280, 'preemption_count': 0}), (70958, {'train/ctc_loss': Array(0.06268328, dtype=float32), 'train/wer': 0.024154338909627173, 'validation/ctc_loss': Array(0.3256981, dtype=float32), 'validation/wer': 0.08989447464205373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17322181, dtype=float32), 'test/wer': 0.054617837629232426, 'test/num_examples': 2472, 'score': 60527.92716932297, 'total_duration': 66382.81089735031, 'accumulated_submission_time': 60527.92716932297, 'accumulated_eval_time': 5849.344304800034, 'accumulated_logging_time': 2.4029757976531982, 'global_step': 70958, 'preemption_count': 0})], 'global_step': 71611}
I0215 16:00:01.484559 140399019657024 submission_runner.py:586] Timing: 61068.90940284729
I0215 16:00:01.484621 140399019657024 submission_runner.py:588] Total number of evals: 43
I0215 16:00:01.484664 140399019657024 submission_runner.py:589] ====================
I0215 16:00:01.484727 140399019657024 submission_runner.py:542] Using RNG seed 2554204701
I0215 16:00:01.486907 140399019657024 submission_runner.py:551] --- Tuning run 3/5 ---
I0215 16:00:01.487030 140399019657024 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3.
I0215 16:00:01.488545 140399019657024 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3/hparams.json.
I0215 16:00:01.490469 140399019657024 submission_runner.py:206] Initializing dataset.
I0215 16:00:01.490589 140399019657024 submission_runner.py:213] Initializing model.
I0215 16:00:05.081439 140399019657024 submission_runner.py:255] Initializing optimizer.
I0215 16:00:05.515737 140399019657024 submission_runner.py:262] Initializing metrics bundle.
I0215 16:00:05.515952 140399019657024 submission_runner.py:280] Initializing checkpoint and logger.
I0215 16:00:05.520165 140399019657024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3 with prefix checkpoint_
I0215 16:00:05.520308 140399019657024 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3/meta_data_0.json.
I0215 16:00:05.520539 140399019657024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 16:00:05.520618 140399019657024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 16:00:06.089454 140399019657024 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 16:00:06.600342 140399019657024 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3/flags_0.json.
I0215 16:00:06.621801 140399019657024 submission_runner.py:314] Starting training loop.
I0215 16:00:06.625083 140399019657024 input_pipeline.py:20] Loading split = train-clean-100
I0215 16:00:06.671823 140399019657024 input_pipeline.py:20] Loading split = train-clean-360
I0215 16:00:07.196816 140399019657024 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 16:00:40.768358 140228579747584 logging_writer.py:48] [0] global_step=0, grad_norm=46.786991119384766, loss=32.64909744262695
I0215 16:00:40.789885 140399019657024 spec.py:321] Evaluating on the training split.
I0215 16:01:41.062136 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 16:02:36.263073 140399019657024 spec.py:349] Evaluating on the test split.
I0215 16:03:04.020590 140399019657024 submission_runner.py:408] Time since start: 177.40s, 	Step: 1, 	{'train/ctc_loss': Array(32.681545, dtype=float32), 'train/wer': 1.395902589872439, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043156299178389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.279491, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 34.168030738830566, 'total_duration': 177.3963179588318, 'accumulated_submission_time': 34.168030738830566, 'accumulated_eval_time': 143.2282350063324, 'accumulated_logging_time': 0}
I0215 16:03:04.036629 140318077859584 logging_writer.py:48] [1] accumulated_eval_time=143.228235, accumulated_logging_time=0, accumulated_submission_time=34.168031, global_step=1, preemption_count=0, score=34.168031, test/ctc_loss=31.279491424560547, test/num_examples=2472, test/wer=1.097699, total_duration=177.396318, train/ctc_loss=32.68154525756836, train/wer=1.395903, validation/ctc_loss=31.163591384887695, validation/num_examples=5348, validation/wer=1.043156
I0215 16:04:46.175084 140228579747584 logging_writer.py:48] [100] global_step=100, grad_norm=23.842931747436523, loss=8.101322174072266
I0215 16:06:03.598701 140228588140288 logging_writer.py:48] [200] global_step=200, grad_norm=1.5977551937103271, loss=6.215599060058594
I0215 16:07:21.973783 140228579747584 logging_writer.py:48] [300] global_step=300, grad_norm=0.955781877040863, loss=5.8622636795043945
I0215 16:08:39.059635 140228588140288 logging_writer.py:48] [400] global_step=400, grad_norm=0.7608523368835449, loss=5.853573799133301
I0215 16:10:03.495518 140228579747584 logging_writer.py:48] [500] global_step=500, grad_norm=0.6266219019889832, loss=5.818777084350586
I0215 16:11:35.296930 140228588140288 logging_writer.py:48] [600] global_step=600, grad_norm=0.4293850064277649, loss=5.806005001068115
I0215 16:13:06.725460 140228579747584 logging_writer.py:48] [700] global_step=700, grad_norm=0.35267284512519836, loss=5.825949192047119
I0215 16:14:36.590046 140228588140288 logging_writer.py:48] [800] global_step=800, grad_norm=0.2705027461051941, loss=5.820455074310303
I0215 16:16:09.366067 140228579747584 logging_writer.py:48] [900] global_step=900, grad_norm=0.3991773724555969, loss=5.816932678222656
I0215 16:17:37.116494 140228588140288 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.29937615990638733, loss=5.794167995452881
I0215 16:19:01.199764 140318077859584 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.509655237197876, loss=5.784133434295654
I0215 16:20:20.016208 140318069466880 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6082611083984375, loss=5.7863640785217285
I0215 16:21:38.732759 140318077859584 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5157970190048218, loss=5.769628524780273
I0215 16:23:03.103961 140318069466880 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.29569461941719055, loss=5.77276611328125
I0215 16:24:33.458381 140318077859584 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8285871744155884, loss=5.786098003387451
I0215 16:26:04.112681 140318069466880 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.33940473198890686, loss=5.735036849975586
I0215 16:27:04.715104 140399019657024 spec.py:321] Evaluating on the training split.
I0215 16:27:43.518810 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 16:28:31.066519 140399019657024 spec.py:349] Evaluating on the test split.
I0215 16:28:55.185318 140399019657024 submission_runner.py:408] Time since start: 1728.56s, 	Step: 1670, 	{'train/ctc_loss': Array(6.3159585, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.4463353, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4492598, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1474.7652645111084, 'total_duration': 1728.55646276474, 'accumulated_submission_time': 1474.7652645111084, 'accumulated_eval_time': 253.69146490097046, 'accumulated_logging_time': 0.029105186462402344}
I0215 16:28:55.220522 140318077859584 logging_writer.py:48] [1670] accumulated_eval_time=253.691465, accumulated_logging_time=0.029105, accumulated_submission_time=1474.765265, global_step=1670, preemption_count=0, score=1474.765265, test/ctc_loss=6.4492597579956055, test/num_examples=2472, test/wer=0.899580, total_duration=1728.556463, train/ctc_loss=6.315958499908447, train/wer=0.939190, validation/ctc_loss=6.446335315704346, validation/num_examples=5348, validation/wer=0.896618
I0215 16:29:19.115531 140318069466880 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7465608716011047, loss=5.594280242919922
I0215 16:30:36.610667 140318077859584 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6947289705276489, loss=5.520155906677246
I0215 16:31:53.997726 140318069466880 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6107088327407837, loss=5.442639350891113
I0215 16:33:22.549105 140318077859584 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3527674674987793, loss=5.212207794189453
I0215 16:34:52.713151 140318077859584 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9351999163627625, loss=4.593229293823242
I0215 16:36:10.638979 140318069466880 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7975956201553345, loss=4.087401390075684
I0215 16:37:30.480764 140318077859584 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.6879997253417969, loss=3.722606897354126
I0215 16:38:54.584797 140318069466880 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.227220058441162, loss=3.497296094894409
I0215 16:40:22.440362 140318077859584 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2280826568603516, loss=3.318556308746338
I0215 16:41:50.927469 140318069466880 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8972972631454468, loss=3.221446990966797
I0215 16:43:20.120585 140318077859584 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2256724834442139, loss=3.0626096725463867
I0215 16:44:48.080707 140318069466880 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.121219277381897, loss=2.9777731895446777
I0215 16:46:18.021144 140318077859584 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.057206153869629, loss=2.9305810928344727
I0215 16:47:49.193041 140318069466880 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.64157235622406, loss=2.838029623031616
I0215 16:49:20.924656 140318077859584 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0922638177871704, loss=2.6764256954193115
I0215 16:50:39.816323 140318069466880 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9207181334495544, loss=2.6800663471221924
I0215 16:51:57.960457 140318077859584 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.4875884056091309, loss=2.6679818630218506
I0215 16:52:55.340833 140399019657024 spec.py:321] Evaluating on the training split.
I0215 16:53:37.267209 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 16:54:26.664122 140399019657024 spec.py:349] Evaluating on the test split.
I0215 16:54:52.451690 140399019657024 submission_runner.py:408] Time since start: 3285.82s, 	Step: 3370, 	{'train/ctc_loss': Array(3.8715672, dtype=float32), 'train/wer': 0.8640947888589398, 'validation/ctc_loss': Array(3.8034496, dtype=float32), 'validation/wer': 0.8164843546347162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.4893599, dtype=float32), 'test/wer': 0.776105457721447, 'test/num_examples': 2472, 'score': 2914.80074095726, 'total_duration': 3285.8231496810913, 'accumulated_submission_time': 2914.80074095726, 'accumulated_eval_time': 370.795667886734, 'accumulated_logging_time': 0.07997775077819824}
I0215 16:54:52.485913 140318077859584 logging_writer.py:48] [3370] accumulated_eval_time=370.795668, accumulated_logging_time=0.079978, accumulated_submission_time=2914.800741, global_step=3370, preemption_count=0, score=2914.800741, test/ctc_loss=3.4893598556518555, test/num_examples=2472, test/wer=0.776105, total_duration=3285.823150, train/ctc_loss=3.8715672492980957, train/wer=0.864095, validation/ctc_loss=3.8034496307373047, validation/num_examples=5348, validation/wer=0.816484
I0215 16:55:16.431063 140318069466880 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0349045991897583, loss=2.5931758880615234
I0215 16:56:33.719070 140318077859584 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.966709554195404, loss=2.526113271713257
I0215 16:57:50.595085 140318069466880 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.9361410140991211, loss=2.4748847484588623
I0215 16:59:21.704645 140318077859584 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8985722064971924, loss=2.4248077869415283
I0215 17:00:52.530755 140318069466880 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9440783858299255, loss=2.381579875946045
I0215 17:02:20.884377 140318077859584 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9336588382720947, loss=2.3708910942077637
I0215 17:03:52.608228 140318069466880 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8798872232437134, loss=2.3743889331817627
I0215 17:05:23.060423 140318077859584 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9812135100364685, loss=2.2498278617858887
I0215 17:06:48.238587 140318077859584 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.1101270914077759, loss=2.3113510608673096
I0215 17:08:07.129034 140318069466880 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8731418251991272, loss=2.1667957305908203
I0215 17:09:27.548082 140318077859584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8138108849525452, loss=2.1893808841705322
I0215 17:10:52.192246 140318069466880 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9839704632759094, loss=2.158259868621826
I0215 17:12:21.663275 140318077859584 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8755568861961365, loss=2.099456787109375
I0215 17:13:53.689856 140318069466880 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8403400778770447, loss=2.112424612045288
I0215 17:15:22.144549 140318077859584 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.0418511629104614, loss=2.09732723236084
I0215 17:16:53.814059 140318069466880 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8843159675598145, loss=2.022787570953369
I0215 17:18:26.363267 140318077859584 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8334366679191589, loss=2.050264835357666
I0215 17:18:52.491311 140399019657024 spec.py:321] Evaluating on the training split.
I0215 17:19:47.022865 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 17:20:39.858539 140399019657024 spec.py:349] Evaluating on the test split.
I0215 17:21:06.980129 140399019657024 submission_runner.py:408] Time since start: 4860.35s, 	Step: 5031, 	{'train/ctc_loss': Array(1.1874629, dtype=float32), 'train/wer': 0.35988786356734026, 'validation/ctc_loss': Array(1.2110256, dtype=float32), 'validation/wer': 0.34403390714154686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.91928077, dtype=float32), 'test/wer': 0.2884244307679808, 'test/num_examples': 2472, 'score': 4354.720586776733, 'total_duration': 4860.35196018219, 'accumulated_submission_time': 4354.720586776733, 'accumulated_eval_time': 505.2781677246094, 'accumulated_logging_time': 0.13262176513671875}
I0215 17:21:07.016596 140318077859584 logging_writer.py:48] [5031] accumulated_eval_time=505.278168, accumulated_logging_time=0.132622, accumulated_submission_time=4354.720587, global_step=5031, preemption_count=0, score=4354.720587, test/ctc_loss=0.9192807674407959, test/num_examples=2472, test/wer=0.288424, total_duration=4860.351960, train/ctc_loss=1.1874629259109497, train/wer=0.359888, validation/ctc_loss=1.211025595664978, validation/num_examples=5348, validation/wer=0.344034
I0215 17:22:01.096850 140318069466880 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8416881561279297, loss=2.0518863201141357
I0215 17:23:22.480464 140318077859584 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7888950705528259, loss=1.9543776512145996
I0215 17:24:43.710807 140318069466880 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9085081219673157, loss=1.9463889598846436
I0215 17:26:07.119070 140318077859584 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8095585107803345, loss=1.9769697189331055
I0215 17:27:33.213773 140318069466880 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.783735454082489, loss=1.8846269845962524
I0215 17:29:02.497139 140318077859584 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7233462929725647, loss=1.8961855173110962
I0215 17:30:29.684425 140318069466880 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8453081250190735, loss=1.88548743724823
I0215 17:32:02.357714 140318077859584 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8949396014213562, loss=1.8803685903549194
I0215 17:33:32.993446 140318069466880 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.0032356977462769, loss=1.864533543586731
I0215 17:35:04.162365 140318077859584 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8096173405647278, loss=1.8359622955322266
I0215 17:36:33.065852 140318069466880 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7631608843803406, loss=1.8318912982940674
I0215 17:38:05.142345 140318077859584 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7173158526420593, loss=1.777987003326416
I0215 17:39:25.193844 140318069466880 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8935853242874146, loss=1.7956154346466064
I0215 17:40:43.131941 140318077859584 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7955781817436218, loss=1.9107195138931274
I0215 17:42:07.675051 140318069466880 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7805130481719971, loss=1.7652190923690796
I0215 17:43:35.110530 140318077859584 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7269234657287598, loss=1.787377119064331
I0215 17:45:06.400086 140318069466880 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.9405531883239746, loss=1.7877634763717651
I0215 17:45:07.632672 140399019657024 spec.py:321] Evaluating on the training split.
I0215 17:46:01.677814 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 17:46:56.330837 140399019657024 spec.py:349] Evaluating on the test split.
I0215 17:47:25.066839 140399019657024 submission_runner.py:408] Time since start: 6438.44s, 	Step: 6703, 	{'train/ctc_loss': Array(0.7139498, dtype=float32), 'train/wer': 0.23824286482052442, 'validation/ctc_loss': Array(0.84304017, dtype=float32), 'validation/wer': 0.2546994023769756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5802292, dtype=float32), 'test/wer': 0.19596612028517457, 'test/num_examples': 2472, 'score': 5795.25229716301, 'total_duration': 6438.438871383667, 'accumulated_submission_time': 5795.25229716301, 'accumulated_eval_time': 642.7062382698059, 'accumulated_logging_time': 0.18616890907287598}
I0215 17:47:25.101264 140318077859584 logging_writer.py:48] [6703] accumulated_eval_time=642.706238, accumulated_logging_time=0.186169, accumulated_submission_time=5795.252297, global_step=6703, preemption_count=0, score=5795.252297, test/ctc_loss=0.5802292227745056, test/num_examples=2472, test/wer=0.195966, total_duration=6438.438871, train/ctc_loss=0.7139497995376587, train/wer=0.238243, validation/ctc_loss=0.8430401682853699, validation/num_examples=5348, validation/wer=0.254699
I0215 17:48:39.937033 140318069466880 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.800032913684845, loss=1.7531193494796753
I0215 17:49:56.813671 140318077859584 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8314924240112305, loss=1.800502896308899
I0215 17:51:24.812738 140318069466880 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7098900079727173, loss=1.818471074104309
I0215 17:52:55.373906 140318077859584 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7508831024169922, loss=1.778793215751648
I0215 17:54:25.164232 140318069466880 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7550416588783264, loss=1.7621279954910278
I0215 17:55:47.869789 140318077859584 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7873170375823975, loss=1.7627689838409424
I0215 17:57:08.526440 140318069466880 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7659558057785034, loss=1.681787371635437
I0215 17:58:29.887511 140318077859584 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8133402466773987, loss=1.6556217670440674
I0215 17:59:56.955706 140318069466880 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7040309309959412, loss=1.7689752578735352
I0215 18:01:28.214227 140318077859584 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7822552919387817, loss=1.7638523578643799
I0215 18:03:00.318940 140318069466880 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7937811613082886, loss=1.68510103225708
I0215 18:04:30.548745 140318077859584 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8324292302131653, loss=1.6837705373764038
I0215 18:06:02.280451 140318069466880 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.658238410949707, loss=1.7488266229629517
I0215 18:07:34.179775 140318077859584 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8161956667900085, loss=1.6843318939208984
I0215 18:09:03.148955 140318069466880 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7597209811210632, loss=1.7456097602844238
I0215 18:10:29.503990 140318077859584 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7330775260925293, loss=1.6698945760726929
I0215 18:11:25.806963 140399019657024 spec.py:321] Evaluating on the training split.
I0215 18:12:21.154453 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 18:13:14.754705 140399019657024 spec.py:349] Evaluating on the test split.
I0215 18:13:42.478397 140399019657024 submission_runner.py:408] Time since start: 8015.85s, 	Step: 8372, 	{'train/ctc_loss': Array(0.5976577, dtype=float32), 'train/wer': 0.19920432590627965, 'validation/ctc_loss': Array(0.7593585, dtype=float32), 'validation/wer': 0.22556165944176795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50354314, dtype=float32), 'test/wer': 0.16952044360489915, 'test/num_examples': 2472, 'score': 7235.874894618988, 'total_duration': 8015.850210905075, 'accumulated_submission_time': 7235.874894618988, 'accumulated_eval_time': 779.3713698387146, 'accumulated_logging_time': 0.2369072437286377}
I0215 18:13:42.515819 140318077859584 logging_writer.py:48] [8372] accumulated_eval_time=779.371370, accumulated_logging_time=0.236907, accumulated_submission_time=7235.874895, global_step=8372, preemption_count=0, score=7235.874895, test/ctc_loss=0.5035431385040283, test/num_examples=2472, test/wer=0.169520, total_duration=8015.850211, train/ctc_loss=0.5976576805114746, train/wer=0.199204, validation/ctc_loss=0.7593585252761841, validation/num_examples=5348, validation/wer=0.225562
I0215 18:14:04.652645 140318069466880 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7188286185264587, loss=1.704270839691162
I0215 18:15:21.972935 140318077859584 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9253222346305847, loss=1.7343745231628418
I0215 18:16:38.617532 140318069466880 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6833286881446838, loss=1.6472933292388916
I0215 18:17:55.965506 140318077859584 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8359657526016235, loss=1.6629655361175537
I0215 18:19:25.526858 140318069466880 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.676901638507843, loss=1.654685378074646
I0215 18:20:54.490272 140318077859584 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6650519371032715, loss=1.6082133054733276
I0215 18:22:25.604480 140318069466880 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6815189719200134, loss=1.6903800964355469
I0215 18:23:54.862315 140318077859584 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7620971202850342, loss=1.624045729637146
I0215 18:25:24.431693 140318069466880 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7185758352279663, loss=1.6159895658493042
I0215 18:26:54.702307 140318077859584 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7922748327255249, loss=1.5915263891220093
I0215 18:28:12.084429 140318069466880 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6653057932853699, loss=1.6481324434280396
I0215 18:29:31.160696 140318077859584 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7292106747627258, loss=1.6394637823104858
I0215 18:30:56.120807 140318069466880 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6932204365730286, loss=1.6088039875030518
I0215 18:32:25.000054 140318077859584 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7756368517875671, loss=1.6543779373168945
I0215 18:33:53.201240 140318069466880 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6422344446182251, loss=1.643856406211853
I0215 18:35:21.951233 140318077859584 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6368726491928101, loss=1.5980451107025146
I0215 18:36:49.783555 140318069466880 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8176311254501343, loss=1.5962820053100586
I0215 18:37:43.178438 140399019657024 spec.py:321] Evaluating on the training split.
I0215 18:38:39.224743 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 18:39:33.970979 140399019657024 spec.py:349] Evaluating on the test split.
I0215 18:40:01.419871 140399019657024 submission_runner.py:408] Time since start: 9594.79s, 	Step: 10060, 	{'train/ctc_loss': Array(0.58726174, dtype=float32), 'train/wer': 0.2015751037064896, 'validation/ctc_loss': Array(0.6942416, dtype=float32), 'validation/wer': 0.2111665717292449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44782707, dtype=float32), 'test/wer': 0.15134157983466373, 'test/num_examples': 2472, 'score': 8676.452405929565, 'total_duration': 9594.790989875793, 'accumulated_submission_time': 8676.452405929565, 'accumulated_eval_time': 917.6057934761047, 'accumulated_logging_time': 0.29051780700683594}
I0215 18:40:01.453671 140318077859584 logging_writer.py:48] [10060] accumulated_eval_time=917.605793, accumulated_logging_time=0.290518, accumulated_submission_time=8676.452406, global_step=10060, preemption_count=0, score=8676.452406, test/ctc_loss=0.4478270709514618, test/num_examples=2472, test/wer=0.151342, total_duration=9594.790990, train/ctc_loss=0.5872617363929749, train/wer=0.201575, validation/ctc_loss=0.6942415833473206, validation/num_examples=5348, validation/wer=0.211167
I0215 18:40:33.110357 140318069466880 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8285123705863953, loss=1.565261721611023
I0215 18:41:50.089690 140318077859584 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.651665210723877, loss=1.595873475074768
I0215 18:43:10.457818 140318077859584 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9634540677070618, loss=1.5656226873397827
I0215 18:44:29.361472 140318069466880 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7791209816932678, loss=1.5770127773284912
I0215 18:45:47.606600 140318077859584 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7917353510856628, loss=1.5734564065933228
I0215 18:47:09.367670 140318069466880 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8572839498519897, loss=1.566748857498169
I0215 18:48:33.204374 140318077859584 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5848513245582581, loss=1.5607144832611084
I0215 18:50:01.513038 140318069466880 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6834343671798706, loss=1.578920841217041
I0215 18:51:29.985837 140318077859584 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7524286508560181, loss=1.5841974020004272
I0215 18:53:01.383455 140318069466880 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6696750521659851, loss=1.5670398473739624
I0215 18:54:32.573368 140318077859584 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.797606348991394, loss=1.5305849313735962
I0215 18:55:59.080447 140318069466880 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6733385920524597, loss=1.5952609777450562
I0215 18:57:31.212889 140318077859584 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6554524302482605, loss=1.572416067123413
I0215 18:58:55.513548 140318077859584 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7284442782402039, loss=1.5026130676269531
I0215 19:00:14.293941 140318069466880 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7295677661895752, loss=1.533846378326416
I0215 19:01:36.095540 140318077859584 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.6954596042633057, loss=1.5205914974212646
I0215 19:02:57.463038 140318069466880 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.685673713684082, loss=1.5793496370315552
I0215 19:04:01.748769 140399019657024 spec.py:321] Evaluating on the training split.
I0215 19:04:55.515914 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 19:05:48.790761 140399019657024 spec.py:349] Evaluating on the test split.
I0215 19:06:16.623489 140399019657024 submission_runner.py:408] Time since start: 11170.00s, 	Step: 11776, 	{'train/ctc_loss': Array(0.5025373, dtype=float32), 'train/wer': 0.1714755275350288, 'validation/ctc_loss': Array(0.6410851, dtype=float32), 'validation/wer': 0.1946764243026927, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40460023, dtype=float32), 'test/wer': 0.13726565515000103, 'test/num_examples': 2472, 'score': 10116.661395788193, 'total_duration': 11169.995291233063, 'accumulated_submission_time': 10116.661395788193, 'accumulated_eval_time': 1052.4742050170898, 'accumulated_logging_time': 0.3399777412414551}
I0215 19:06:16.658896 140318077859584 logging_writer.py:48] [11776] accumulated_eval_time=1052.474205, accumulated_logging_time=0.339978, accumulated_submission_time=10116.661396, global_step=11776, preemption_count=0, score=10116.661396, test/ctc_loss=0.40460023283958435, test/num_examples=2472, test/wer=0.137266, total_duration=11169.995291, train/ctc_loss=0.5025373101234436, train/wer=0.171476, validation/ctc_loss=0.6410850882530212, validation/num_examples=5348, validation/wer=0.194676
I0215 19:06:35.759550 140318069466880 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6329883337020874, loss=1.5284966230392456
I0215 19:07:52.325177 140318077859584 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7352865934371948, loss=1.5864063501358032
I0215 19:09:09.160276 140318069466880 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8301129341125488, loss=1.493343710899353
I0215 19:10:36.133076 140318077859584 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7746291756629944, loss=1.5785411596298218
I0215 19:12:06.649223 140318069466880 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6743797659873962, loss=1.4941649436950684
I0215 19:13:35.187481 140318077859584 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6367987990379333, loss=1.4982178211212158
I0215 19:15:02.417965 140318077859584 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6797558069229126, loss=1.517641544342041
I0215 19:16:20.498409 140318069466880 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8760135173797607, loss=1.481139063835144
I0215 19:17:37.808441 140318077859584 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6455142498016357, loss=1.5760759115219116
I0215 19:19:01.184996 140318069466880 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.581166684627533, loss=1.5022436380386353
I0215 19:20:27.365375 140318077859584 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.751125693321228, loss=1.5053199529647827
I0215 19:21:54.337715 140318069466880 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7359002828598022, loss=1.49910569190979
I0215 19:23:27.107674 140318077859584 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.69638592004776, loss=1.5150835514068604
I0215 19:24:58.039706 140318069466880 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7065181732177734, loss=1.497083067893982
I0215 19:26:25.489328 140318077859584 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.6885000467300415, loss=1.4640721082687378
I0215 19:27:52.904973 140318069466880 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6030451059341431, loss=1.4616410732269287
I0215 19:29:25.750860 140318077859584 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.7995355725288391, loss=1.4831949472427368
I0215 19:30:16.813033 140399019657024 spec.py:321] Evaluating on the training split.
I0215 19:31:12.209305 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 19:32:07.124152 140399019657024 spec.py:349] Evaluating on the test split.
I0215 19:32:34.937954 140399019657024 submission_runner.py:408] Time since start: 12748.31s, 	Step: 13468, 	{'train/ctc_loss': Array(0.49499294, dtype=float32), 'train/wer': 0.16616104992415748, 'validation/ctc_loss': Array(0.6108735, dtype=float32), 'validation/wer': 0.18658582503837726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38324997, dtype=float32), 'test/wer': 0.13253305709585034, 'test/num_examples': 2472, 'score': 11556.729298353195, 'total_duration': 12748.30952501297, 'accumulated_submission_time': 11556.729298353195, 'accumulated_eval_time': 1190.592571258545, 'accumulated_logging_time': 0.39217233657836914}
I0215 19:32:34.976064 140318077859584 logging_writer.py:48] [13468] accumulated_eval_time=1190.592571, accumulated_logging_time=0.392172, accumulated_submission_time=11556.729298, global_step=13468, preemption_count=0, score=11556.729298, test/ctc_loss=0.383249968290329, test/num_examples=2472, test/wer=0.132533, total_duration=12748.309525, train/ctc_loss=0.4949929416179657, train/wer=0.166161, validation/ctc_loss=0.6108735203742981, validation/num_examples=5348, validation/wer=0.186586
I0215 19:33:00.244822 140318069466880 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.712529182434082, loss=1.5687198638916016
I0215 19:34:17.030054 140318077859584 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6945111751556396, loss=1.5347295999526978
I0215 19:35:33.535148 140318069466880 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6978768110275269, loss=1.458138108253479
I0215 19:36:50.661352 140318077859584 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6464115977287292, loss=1.50575852394104
I0215 19:38:20.962120 140318069466880 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.749180257320404, loss=1.4690574407577515
I0215 19:39:47.182265 140318077859584 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6674143671989441, loss=1.4749064445495605
I0215 19:41:18.150799 140318069466880 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.6612767577171326, loss=1.5788182020187378
I0215 19:42:47.455374 140318077859584 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6512405276298523, loss=1.499131202697754
I0215 19:44:17.527470 140318069466880 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.716259777545929, loss=1.436645746231079
I0215 19:45:47.651943 140318077859584 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7996319532394409, loss=1.447797417640686
I0215 19:47:11.311466 140318077859584 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7182483673095703, loss=1.4221370220184326
I0215 19:48:29.725673 140318069466880 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8072180151939392, loss=1.3961098194122314
I0215 19:49:49.147960 140318077859584 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6054241061210632, loss=1.4793858528137207
I0215 19:51:12.059528 140318069466880 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7526004314422607, loss=1.4564684629440308
I0215 19:52:39.044338 140318077859584 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6503022909164429, loss=1.4446866512298584
I0215 19:54:09.128520 140318069466880 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6953244209289551, loss=1.4554073810577393
I0215 19:55:42.864347 140318077859584 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6872403621673584, loss=1.4358044862747192
I0215 19:56:35.057829 140399019657024 spec.py:321] Evaluating on the training split.
I0215 19:57:29.562459 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 19:58:22.504270 140399019657024 spec.py:349] Evaluating on the test split.
I0215 19:58:49.779350 140399019657024 submission_runner.py:408] Time since start: 14323.15s, 	Step: 15157, 	{'train/ctc_loss': Array(0.41451362, dtype=float32), 'train/wer': 0.14536975496146726, 'validation/ctc_loss': Array(0.5853169, dtype=float32), 'validation/wer': 0.17944138177394595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3680178, dtype=float32), 'test/wer': 0.12524120000812464, 'test/num_examples': 2472, 'score': 12996.726741552353, 'total_duration': 14323.149782896042, 'accumulated_submission_time': 12996.726741552353, 'accumulated_eval_time': 1325.306384563446, 'accumulated_logging_time': 0.4475064277648926}
I0215 19:58:49.820867 140318077859584 logging_writer.py:48] [15157] accumulated_eval_time=1325.306385, accumulated_logging_time=0.447506, accumulated_submission_time=12996.726742, global_step=15157, preemption_count=0, score=12996.726742, test/ctc_loss=0.3680177927017212, test/num_examples=2472, test/wer=0.125241, total_duration=14323.149783, train/ctc_loss=0.41451361775398254, train/wer=0.145370, validation/ctc_loss=0.5853168964385986, validation/num_examples=5348, validation/wer=0.179441
I0215 19:59:23.677878 140318069466880 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6542710661888123, loss=1.47471284866333
I0215 20:00:40.737043 140318077859584 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.6338046193122864, loss=1.4670883417129517
I0215 20:01:57.567816 140318069466880 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6700263619422913, loss=1.4391978979110718
I0215 20:03:22.856276 140318077859584 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.672262966632843, loss=1.4279803037643433
I0215 20:04:42.457155 140318069466880 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7322268486022949, loss=1.4502888917922974
I0215 20:06:01.968039 140318077859584 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.606436014175415, loss=1.475012183189392
I0215 20:07:24.549730 140318069466880 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6697537899017334, loss=1.4610803127288818
I0215 20:08:50.455891 140318077859584 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.5758814215660095, loss=1.4054322242736816
I0215 20:10:21.273301 140318069466880 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6214486956596375, loss=1.404184341430664
I0215 20:11:50.765439 140318077859584 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0029364824295044, loss=1.4638948440551758
I0215 20:13:19.974809 140318069466880 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6415061354637146, loss=1.4233695268630981
I0215 20:14:49.882726 140318077859584 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.743472158908844, loss=1.4120160341262817
I0215 20:16:19.131369 140318069466880 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.7049001455307007, loss=1.455217957496643
I0215 20:17:50.041507 140318077859584 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7315532565116882, loss=1.3845423460006714
I0215 20:19:08.854006 140318069466880 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.8774452805519104, loss=1.4062845706939697
I0215 20:20:30.198987 140318077859584 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6650069355964661, loss=1.4152127504348755
I0215 20:21:51.546860 140318069466880 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6832048296928406, loss=1.4285602569580078
I0215 20:22:49.931732 140399019657024 spec.py:321] Evaluating on the training split.
I0215 20:23:42.969254 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 20:24:37.217035 140399019657024 spec.py:349] Evaluating on the test split.
I0215 20:25:05.721763 140399019657024 submission_runner.py:408] Time since start: 15899.09s, 	Step: 16871, 	{'train/ctc_loss': Array(0.41015854, dtype=float32), 'train/wer': 0.1461925696453384, 'validation/ctc_loss': Array(0.56820166, dtype=float32), 'validation/wer': 0.17308862006043813, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34979352, dtype=float32), 'test/wer': 0.12081327564844718, 'test/num_examples': 2472, 'score': 14436.753607988358, 'total_duration': 15899.094260454178, 'accumulated_submission_time': 14436.753607988358, 'accumulated_eval_time': 1461.0907878875732, 'accumulated_logging_time': 0.5039346218109131}
I0215 20:25:05.764199 140318077859584 logging_writer.py:48] [16871] accumulated_eval_time=1461.090788, accumulated_logging_time=0.503935, accumulated_submission_time=14436.753608, global_step=16871, preemption_count=0, score=14436.753608, test/ctc_loss=0.34979352355003357, test/num_examples=2472, test/wer=0.120813, total_duration=15899.094260, train/ctc_loss=0.41015854477882385, train/wer=0.146193, validation/ctc_loss=0.5682016611099243, validation/num_examples=5348, validation/wer=0.173089
I0215 20:25:28.637696 140318069466880 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8452373147010803, loss=1.4549269676208496
I0215 20:26:45.122750 140318077859584 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6576420664787292, loss=1.4540215730667114
I0215 20:28:02.370889 140318069466880 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6267492771148682, loss=1.452509880065918
I0215 20:29:33.039212 140318077859584 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6524232029914856, loss=1.4094222784042358
I0215 20:31:03.582655 140318069466880 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.565197229385376, loss=1.4074703454971313
I0215 20:32:32.692210 140318077859584 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6797502636909485, loss=1.3913273811340332
I0215 20:34:01.381094 140318069466880 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6775586009025574, loss=1.4446547031402588
I0215 20:35:22.176092 140318077859584 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6289498805999756, loss=1.4163668155670166
I0215 20:36:39.978804 140318069466880 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.6953133344650269, loss=1.3718411922454834
I0215 20:38:01.684004 140318077859584 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6822423338890076, loss=1.4517713785171509
I0215 20:39:24.970777 140318069466880 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6854548454284668, loss=1.4801303148269653
I0215 20:40:52.947219 140318077859584 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.5925898551940918, loss=1.423879623413086
I0215 20:42:22.500876 140318069466880 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7227901220321655, loss=1.3823401927947998
I0215 20:43:52.367318 140318077859584 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6185590028762817, loss=1.4506537914276123
I0215 20:45:20.241867 140318069466880 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7346195578575134, loss=1.4354009628295898
I0215 20:46:49.471885 140318077859584 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6631560921669006, loss=1.4589179754257202
I0215 20:48:21.970643 140318069466880 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.634319007396698, loss=1.4333266019821167
I0215 20:49:06.267878 140399019657024 spec.py:321] Evaluating on the training split.
I0215 20:50:00.131446 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 20:50:54.276092 140399019657024 spec.py:349] Evaluating on the test split.
I0215 20:51:21.855220 140399019657024 submission_runner.py:408] Time since start: 17475.23s, 	Step: 18548, 	{'train/ctc_loss': Array(0.3982619, dtype=float32), 'train/wer': 0.14110618034674327, 'validation/ctc_loss': Array(0.55043125, dtype=float32), 'validation/wer': 0.1676240864284542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33841434, dtype=float32), 'test/wer': 0.11526821440903459, 'test/num_examples': 2472, 'score': 15877.172675848007, 'total_duration': 17475.227288007736, 'accumulated_submission_time': 15877.172675848007, 'accumulated_eval_time': 1596.6722741127014, 'accumulated_logging_time': 0.5620999336242676}
I0215 20:51:21.891441 140318077859584 logging_writer.py:48] [18548] accumulated_eval_time=1596.672274, accumulated_logging_time=0.562100, accumulated_submission_time=15877.172676, global_step=18548, preemption_count=0, score=15877.172676, test/ctc_loss=0.33841434121131897, test/num_examples=2472, test/wer=0.115268, total_duration=17475.227288, train/ctc_loss=0.3982619047164917, train/wer=0.141106, validation/ctc_loss=0.5504312515258789, validation/num_examples=5348, validation/wer=0.167624
I0215 20:52:02.517365 140318069466880 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.7009545564651489, loss=1.4449506998062134
I0215 20:53:18.928688 140318077859584 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.5931548476219177, loss=1.3965795040130615
I0215 20:54:35.616050 140318069466880 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6521071195602417, loss=1.489007830619812
I0215 20:55:52.235615 140318077859584 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6280736327171326, loss=1.3750420808792114
I0215 20:57:08.647725 140318069466880 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6740172505378723, loss=1.397704839706421
I0215 20:58:36.202453 140318077859584 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6363664865493774, loss=1.4226937294006348
I0215 21:00:07.451876 140318069466880 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6064721345901489, loss=1.3976104259490967
I0215 21:01:34.603172 140318077859584 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.605885922908783, loss=1.4009507894515991
I0215 21:03:05.506316 140318069466880 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7178438305854797, loss=1.4160902500152588
I0215 21:04:35.230813 140318077859584 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6104678511619568, loss=1.4077991247177124
I0215 21:06:04.543933 140318077859584 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6726545691490173, loss=1.3983588218688965
I0215 21:07:20.855913 140318069466880 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.70142662525177, loss=1.3908287286758423
I0215 21:08:37.618515 140318077859584 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6849599480628967, loss=1.3491027355194092
I0215 21:09:55.182044 140318069466880 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6143937706947327, loss=1.4061554670333862
I0215 21:11:17.318640 140318077859584 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7925535440444946, loss=1.4457758665084839
I0215 21:12:46.560486 140318069466880 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6354328989982605, loss=1.432072401046753
I0215 21:14:16.746739 140318077859584 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6583008766174316, loss=1.4272327423095703
I0215 21:15:22.727895 140399019657024 spec.py:321] Evaluating on the training split.
I0215 21:16:30.139771 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 21:17:22.730500 140399019657024 spec.py:349] Evaluating on the test split.
I0215 21:17:49.729908 140399019657024 submission_runner.py:408] Time since start: 19063.10s, 	Step: 20273, 	{'train/ctc_loss': Array(0.26937568, dtype=float32), 'train/wer': 0.0986536762934611, 'validation/ctc_loss': Array(0.54442644, dtype=float32), 'validation/wer': 0.1646794172451413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32714456, dtype=float32), 'test/wer': 0.1112871448012512, 'test/num_examples': 2472, 'score': 17317.922025680542, 'total_duration': 19063.102712631226, 'accumulated_submission_time': 17317.922025680542, 'accumulated_eval_time': 1743.6689743995667, 'accumulated_logging_time': 0.6150286197662354}
I0215 21:17:49.773493 140318077859584 logging_writer.py:48] [20273] accumulated_eval_time=1743.668974, accumulated_logging_time=0.615029, accumulated_submission_time=17317.922026, global_step=20273, preemption_count=0, score=17317.922026, test/ctc_loss=0.3271445631980896, test/num_examples=2472, test/wer=0.111287, total_duration=19063.102713, train/ctc_loss=0.26937568187713623, train/wer=0.098654, validation/ctc_loss=0.544426441192627, validation/num_examples=5348, validation/wer=0.164679
I0215 21:18:11.175867 140318069466880 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.620207667350769, loss=1.4066948890686035
I0215 21:19:28.033993 140318077859584 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6873127222061157, loss=1.443410873413086
I0215 21:20:44.613726 140318069466880 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9269428849220276, loss=1.389415979385376
I0215 21:22:07.554040 140318077859584 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7690062522888184, loss=1.3989741802215576
I0215 21:23:23.971297 140318069466880 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6260830760002136, loss=1.3804373741149902
I0215 21:24:40.481048 140318077859584 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6692454218864441, loss=1.3880499601364136
I0215 21:25:59.658272 140318069466880 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.5672233700752258, loss=1.386344075202942
I0215 21:27:23.232107 140318077859584 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7020584940910339, loss=1.391864538192749
I0215 21:28:51.003222 140318069466880 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6514584422111511, loss=1.4038161039352417
I0215 21:30:22.556769 140318077859584 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5992951989173889, loss=1.3747105598449707
I0215 21:31:51.995632 140318069466880 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7276586890220642, loss=1.369992733001709
I0215 21:33:23.240959 140318077859584 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6685503125190735, loss=1.3911068439483643
I0215 21:34:54.256386 140318069466880 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7100992798805237, loss=1.4054629802703857
I0215 21:36:22.790653 140318077859584 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6943071484565735, loss=1.4104700088500977
I0215 21:37:43.729290 140318077859584 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6736903786659241, loss=1.3946466445922852
I0215 21:39:00.448333 140318069466880 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7810034155845642, loss=1.377102255821228
I0215 21:40:17.325045 140318077859584 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7610592842102051, loss=1.3943191766738892
I0215 21:41:34.239860 140318069466880 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8320460915565491, loss=1.3590731620788574
I0215 21:41:50.046776 140399019657024 spec.py:321] Evaluating on the training split.
I0215 21:42:42.927078 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 21:43:33.035870 140399019657024 spec.py:349] Evaluating on the test split.
I0215 21:43:57.838108 140399019657024 submission_runner.py:408] Time since start: 20631.21s, 	Step: 22022, 	{'train/ctc_loss': Array(0.25267893, dtype=float32), 'train/wer': 0.09203062521310561, 'validation/ctc_loss': Array(0.5279306, dtype=float32), 'validation/wer': 0.16062446295992353, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31976706, dtype=float32), 'test/wer': 0.10915442893993865, 'test/num_examples': 2472, 'score': 18758.112550258636, 'total_duration': 20631.210894346237, 'accumulated_submission_time': 18758.112550258636, 'accumulated_eval_time': 1871.4549486637115, 'accumulated_logging_time': 0.674034595489502}
I0215 21:43:57.874998 140318077859584 logging_writer.py:48] [22022] accumulated_eval_time=1871.454949, accumulated_logging_time=0.674035, accumulated_submission_time=18758.112550, global_step=22022, preemption_count=0, score=18758.112550, test/ctc_loss=0.3197670578956604, test/num_examples=2472, test/wer=0.109154, total_duration=20631.210894, train/ctc_loss=0.25267893075942993, train/wer=0.092031, validation/ctc_loss=0.5279306173324585, validation/num_examples=5348, validation/wer=0.160624
I0215 21:44:58.390970 140318069466880 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.646491527557373, loss=1.3609563112258911
I0215 21:46:15.185256 140318077859584 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.8186667561531067, loss=1.3694015741348267
I0215 21:47:31.987677 140318069466880 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.7041110992431641, loss=1.4245169162750244
I0215 21:48:48.901237 140318077859584 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6394612789154053, loss=1.3162013292312622
I0215 21:50:05.765407 140318069466880 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7083041071891785, loss=1.3462246656417847
I0215 21:51:22.542870 140318077859584 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6578941941261292, loss=1.3217593431472778
I0215 21:52:42.890407 140318077859584 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7152170538902283, loss=1.3840492963790894
I0215 21:53:59.823976 140318069466880 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6145561933517456, loss=1.4024320840835571
I0215 21:55:16.696697 140318077859584 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6163428425788879, loss=1.3450944423675537
I0215 21:56:33.589401 140318069466880 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6609082818031311, loss=1.348659634590149
I0215 21:57:50.440167 140318077859584 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5962278842926025, loss=1.3635003566741943
I0215 21:59:07.433462 140318069466880 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.7097951173782349, loss=1.298777461051941
I0215 22:00:24.164687 140318077859584 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6920523643493652, loss=1.3088637590408325
I0215 22:01:40.907901 140318069466880 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5888375043869019, loss=1.3460537195205688
I0215 22:03:00.104235 140318077859584 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6716622710227966, loss=1.345658302307129
I0215 22:04:21.105276 140318069466880 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7589436173439026, loss=1.3821306228637695
I0215 22:05:45.618250 140318077859584 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6210842728614807, loss=1.3555612564086914
I0215 22:07:02.752367 140318069466880 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.662810742855072, loss=1.338282585144043
I0215 22:07:58.529691 140399019657024 spec.py:321] Evaluating on the training split.
I0215 22:08:50.686588 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 22:09:39.703999 140399019657024 spec.py:349] Evaluating on the test split.
I0215 22:10:04.885346 140399019657024 submission_runner.py:408] Time since start: 22198.26s, 	Step: 23874, 	{'train/ctc_loss': Array(0.24335396, dtype=float32), 'train/wer': 0.09045162715126276, 'validation/ctc_loss': Array(0.51649487, dtype=float32), 'validation/wer': 0.1568688029195671, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31226346, dtype=float32), 'test/wer': 0.1043405845672618, 'test/num_examples': 2472, 'score': 20198.68664932251, 'total_duration': 22198.257422208786, 'accumulated_submission_time': 20198.68664932251, 'accumulated_eval_time': 1997.8045163154602, 'accumulated_logging_time': 0.7282936573028564}
I0215 22:10:04.923820 140318077859584 logging_writer.py:48] [23874] accumulated_eval_time=1997.804516, accumulated_logging_time=0.728294, accumulated_submission_time=20198.686649, global_step=23874, preemption_count=0, score=20198.686649, test/ctc_loss=0.31226345896720886, test/num_examples=2472, test/wer=0.104341, total_duration=22198.257422, train/ctc_loss=0.2433539628982544, train/wer=0.090452, validation/ctc_loss=0.516494870185852, validation/num_examples=5348, validation/wer=0.156869
I0215 22:10:25.609815 140318069466880 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6583777070045471, loss=1.3065897226333618
I0215 22:11:42.324233 140318077859584 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7792830467224121, loss=1.3449721336364746
I0215 22:12:59.091356 140318069466880 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.7694207429885864, loss=1.361669898033142
I0215 22:14:15.960274 140318077859584 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6680805087089539, loss=1.3267388343811035
I0215 22:15:32.951386 140318069466880 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5796101093292236, loss=1.3569656610488892
I0215 22:16:49.648848 140318077859584 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6988324522972107, loss=1.3182209730148315
I0215 22:18:06.287821 140318069466880 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6842089891433716, loss=1.273413062095642
I0215 22:19:22.957954 140318077859584 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6600571870803833, loss=1.345771074295044
I0215 22:20:43.442008 140318069466880 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6546673774719238, loss=1.3090888261795044
I0215 22:22:04.356709 140318077859584 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8658584952354431, loss=1.3388867378234863
I0215 22:23:21.064645 140318069466880 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8633527159690857, loss=1.3141189813613892
I0215 22:24:37.825918 140318077859584 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6270954012870789, loss=1.3210488557815552
I0215 22:25:54.476504 140318069466880 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7247506976127625, loss=1.3640024662017822
I0215 22:27:11.083087 140318077859584 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.631966769695282, loss=1.350338339805603
I0215 22:28:27.652858 140318069466880 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6563513875007629, loss=1.3165241479873657
I0215 22:29:44.480815 140318077859584 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7435508966445923, loss=1.3262073993682861
I0215 22:31:01.817816 140318069466880 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6897022128105164, loss=1.3213975429534912
I0215 22:32:22.735213 140318077859584 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7283837199211121, loss=1.3957113027572632
I0215 22:33:44.337308 140318069466880 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.754583477973938, loss=1.2842555046081543
I0215 22:34:05.150272 140399019657024 spec.py:321] Evaluating on the training split.
I0215 22:34:58.110573 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 22:35:47.696573 140399019657024 spec.py:349] Evaluating on the test split.
I0215 22:36:12.850869 140399019657024 submission_runner.py:408] Time since start: 23766.22s, 	Step: 25727, 	{'train/ctc_loss': Array(0.22757375, dtype=float32), 'train/wer': 0.08577388314710384, 'validation/ctc_loss': Array(0.5035488, dtype=float32), 'validation/wer': 0.15267868349150873, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2979179, dtype=float32), 'test/wer': 0.10245160766152783, 'test/num_examples': 2472, 'score': 21638.83458685875, 'total_duration': 23766.222700834274, 'accumulated_submission_time': 21638.83458685875, 'accumulated_eval_time': 2125.4988000392914, 'accumulated_logging_time': 0.7810320854187012}
I0215 22:36:12.885239 140318077859584 logging_writer.py:48] [25727] accumulated_eval_time=2125.498800, accumulated_logging_time=0.781032, accumulated_submission_time=21638.834587, global_step=25727, preemption_count=0, score=21638.834587, test/ctc_loss=0.297917902469635, test/num_examples=2472, test/wer=0.102452, total_duration=23766.222701, train/ctc_loss=0.22757375240325928, train/wer=0.085774, validation/ctc_loss=0.503548800945282, validation/num_examples=5348, validation/wer=0.152679
I0215 22:37:12.609071 140318077859584 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6176450848579407, loss=1.2967032194137573
I0215 22:38:29.138831 140318069466880 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6600366234779358, loss=1.3448148965835571
I0215 22:39:45.943130 140318077859584 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6639295816421509, loss=1.322507619857788
I0215 22:41:03.019185 140318069466880 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.783871054649353, loss=1.265163779258728
I0215 22:42:19.792919 140318077859584 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6354123950004578, loss=1.3282796144485474
I0215 22:43:36.677546 140318069466880 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6868427395820618, loss=1.3207111358642578
I0215 22:44:53.378058 140318077859584 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6695525646209717, loss=1.3092957735061646
I0215 22:46:10.209586 140318069466880 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6938046813011169, loss=1.3580952882766724
I0215 22:47:29.204577 140318077859584 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6843889951705933, loss=1.2860157489776611
I0215 22:48:49.968388 140318069466880 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7112466096878052, loss=1.362890362739563
I0215 22:50:13.682538 140318077859584 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6454710960388184, loss=1.291595220565796
I0215 22:51:30.487524 140318069466880 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7697586417198181, loss=1.3693737983703613
I0215 22:52:47.378695 140318077859584 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.61076420545578, loss=1.289136290550232
I0215 22:54:04.295866 140318069466880 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7588974237442017, loss=1.3122416734695435
I0215 22:55:21.038837 140318077859584 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7833588719367981, loss=1.300795316696167
I0215 22:56:37.985365 140318069466880 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6973719596862793, loss=1.3111931085586548
I0215 22:57:54.831225 140318077859584 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.6910973191261292, loss=1.358229398727417
I0215 22:59:11.666610 140318069466880 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7508319616317749, loss=1.3425068855285645
I0215 23:00:13.624060 140399019657024 spec.py:321] Evaluating on the training split.
I0215 23:01:06.730313 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 23:01:56.272945 140399019657024 spec.py:349] Evaluating on the test split.
I0215 23:02:21.765083 140399019657024 submission_runner.py:408] Time since start: 25335.14s, 	Step: 27582, 	{'train/ctc_loss': Array(0.22518203, dtype=float32), 'train/wer': 0.08362828084689025, 'validation/ctc_loss': Array(0.49105188, dtype=float32), 'validation/wer': 0.1481313419002288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2894307, dtype=float32), 'test/wer': 0.09792212540369265, 'test/num_examples': 2472, 'score': 23079.494574785233, 'total_duration': 25335.13685107231, 'accumulated_submission_time': 23079.494574785233, 'accumulated_eval_time': 2253.6334249973297, 'accumulated_logging_time': 0.829434871673584}
I0215 23:02:21.807576 140318077859584 logging_writer.py:48] [27582] accumulated_eval_time=2253.633425, accumulated_logging_time=0.829435, accumulated_submission_time=23079.494575, global_step=27582, preemption_count=0, score=23079.494575, test/ctc_loss=0.2894307076931, test/num_examples=2472, test/wer=0.097922, total_duration=25335.136851, train/ctc_loss=0.22518202662467957, train/wer=0.083628, validation/ctc_loss=0.49105188250541687, validation/num_examples=5348, validation/wer=0.148131
I0215 23:02:36.380838 140318069466880 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6723331212997437, loss=1.2718803882598877
I0215 23:03:52.971211 140318077859584 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.625445544719696, loss=1.289551019668579
I0215 23:05:09.699275 140318069466880 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6587042808532715, loss=1.3494350910186768
I0215 23:06:29.625324 140318077859584 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8289671540260315, loss=1.2918606996536255
I0215 23:07:46.298772 140318069466880 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6811484694480896, loss=1.2803467512130737
I0215 23:09:03.098704 140318077859584 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7242686748504639, loss=1.282234787940979
I0215 23:10:19.636837 140318069466880 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6001266837120056, loss=1.318926453590393
I0215 23:11:36.363517 140318077859584 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.7212267518043518, loss=1.2796783447265625
I0215 23:12:53.406579 140318069466880 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.7191643714904785, loss=1.3212331533432007
I0215 23:14:10.188552 140318077859584 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7093722820281982, loss=1.325445294380188
I0215 23:15:26.887956 140318069466880 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7222259640693665, loss=1.321609377861023
I0215 23:16:47.999032 140318077859584 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6902807354927063, loss=1.3663620948791504
I0215 23:18:10.429411 140318069466880 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7131938338279724, loss=1.3042690753936768
I0215 23:19:32.017253 140318077859584 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.7627225518226624, loss=1.2457571029663086
I0215 23:20:48.645683 140318069466880 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6666013598442078, loss=1.3381870985031128
I0215 23:22:05.339249 140318077859584 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.6187447309494019, loss=1.2880430221557617
I0215 23:23:22.028391 140318069466880 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6940769553184509, loss=1.2610023021697998
I0215 23:24:38.716751 140318077859584 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6735996007919312, loss=1.300513744354248
I0215 23:25:55.298357 140318069466880 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7143581509590149, loss=1.3048290014266968
I0215 23:26:22.506108 140399019657024 spec.py:321] Evaluating on the training split.
I0215 23:27:16.349576 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 23:28:06.085140 140399019657024 spec.py:349] Evaluating on the test split.
I0215 23:28:31.911050 140399019657024 submission_runner.py:408] Time since start: 26905.28s, 	Step: 29437, 	{'train/ctc_loss': Array(0.20949112, dtype=float32), 'train/wer': 0.07906515745021853, 'validation/ctc_loss': Array(0.47871414, dtype=float32), 'validation/wer': 0.14474255867615396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27920473, dtype=float32), 'test/wer': 0.09585034428127476, 'test/num_examples': 2472, 'score': 24520.111387252808, 'total_duration': 26905.28270840645, 'accumulated_submission_time': 24520.111387252808, 'accumulated_eval_time': 2383.0318739414215, 'accumulated_logging_time': 0.8874096870422363}
I0215 23:28:31.949273 140318077859584 logging_writer.py:48] [29437] accumulated_eval_time=2383.031874, accumulated_logging_time=0.887410, accumulated_submission_time=24520.111387, global_step=29437, preemption_count=0, score=24520.111387, test/ctc_loss=0.27920472621917725, test/num_examples=2472, test/wer=0.095850, total_duration=26905.282708, train/ctc_loss=0.20949111878871918, train/wer=0.079065, validation/ctc_loss=0.47871413826942444, validation/num_examples=5348, validation/wer=0.144743
I0215 23:29:20.990541 140318069466880 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7564393281936646, loss=1.2571556568145752
I0215 23:30:38.103654 140318077859584 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6790400743484497, loss=1.3175091743469238
I0215 23:31:54.781139 140318069466880 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6048758029937744, loss=1.3183495998382568
I0215 23:33:11.648009 140318077859584 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.6740269660949707, loss=1.275915503501892
I0215 23:34:31.445592 140318077859584 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5847899913787842, loss=1.1944769620895386
I0215 23:35:47.986419 140318069466880 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6608981490135193, loss=1.3041714429855347
I0215 23:37:04.697794 140318077859584 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6838600039482117, loss=1.2285661697387695
I0215 23:38:21.359653 140318069466880 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.7326360940933228, loss=1.3276125192642212
I0215 23:39:38.064419 140318077859584 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.6652138233184814, loss=1.3111906051635742
I0215 23:40:54.614492 140318069466880 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.750657320022583, loss=1.2883363962173462
I0215 23:42:11.267401 140318077859584 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7040404081344604, loss=1.31357741355896
I0215 23:43:28.160874 140318069466880 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6883068084716797, loss=1.3565624952316284
I0215 23:44:49.540074 140318077859584 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.7604355216026306, loss=1.2929508686065674
I0215 23:46:10.974463 140318069466880 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.8088429570198059, loss=1.2525204420089722
I0215 23:47:35.164559 140318077859584 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7060028910636902, loss=1.3037537336349487
I0215 23:48:51.966699 140318069466880 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7762553095817566, loss=1.2927006483078003
I0215 23:50:08.705334 140318077859584 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.85698002576828, loss=1.3158881664276123
I0215 23:51:25.565351 140318069466880 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.671505868434906, loss=1.2652794122695923
I0215 23:52:32.666391 140399019657024 spec.py:321] Evaluating on the training split.
I0215 23:53:25.029786 140399019657024 spec.py:333] Evaluating on the validation split.
I0215 23:54:15.014031 140399019657024 spec.py:349] Evaluating on the test split.
I0215 23:54:40.090452 140399019657024 submission_runner.py:408] Time since start: 28473.46s, 	Step: 31289, 	{'train/ctc_loss': Array(0.24946217, dtype=float32), 'train/wer': 0.08724518370698234, 'validation/ctc_loss': Array(0.4818028, dtype=float32), 'validation/wer': 0.14407638761501104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28004083, dtype=float32), 'test/wer': 0.09499725793674973, 'test/num_examples': 2472, 'score': 25960.74817752838, 'total_duration': 28473.462604045868, 'accumulated_submission_time': 25960.74817752838, 'accumulated_eval_time': 2510.4499428272247, 'accumulated_logging_time': 0.9405355453491211}
I0215 23:54:40.128150 140318077859584 logging_writer.py:48] [31289] accumulated_eval_time=2510.449943, accumulated_logging_time=0.940536, accumulated_submission_time=25960.748178, global_step=31289, preemption_count=0, score=25960.748178, test/ctc_loss=0.28004083037376404, test/num_examples=2472, test/wer=0.094997, total_duration=28473.462604, train/ctc_loss=0.24946217238903046, train/wer=0.087245, validation/ctc_loss=0.4818027913570404, validation/num_examples=5348, validation/wer=0.144076
I0215 23:54:49.352680 140318069466880 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6722198724746704, loss=1.3170335292816162
I0215 23:56:05.771621 140318077859584 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6913654804229736, loss=1.2947885990142822
I0215 23:57:22.456907 140318069466880 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.60698002576828, loss=1.2267162799835205
I0215 23:58:39.257185 140318077859584 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6722037196159363, loss=1.2715805768966675
I0215 23:59:55.979481 140318069466880 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6667638421058655, loss=1.2984724044799805
I0216 00:01:13.072193 140318077859584 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6989762187004089, loss=1.2550029754638672
I0216 00:02:29.767387 140318069466880 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7568316459655762, loss=1.2739959955215454
I0216 00:03:49.606051 140318077859584 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6661316156387329, loss=1.2700517177581787
I0216 00:05:06.518445 140318069466880 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.7305579781532288, loss=1.297749638557434
I0216 00:06:23.395759 140318077859584 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.7125236392021179, loss=1.2861254215240479
I0216 00:07:40.310132 140318069466880 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6600634455680847, loss=1.2593756914138794
I0216 00:08:57.230574 140318077859584 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6118910908699036, loss=1.2585489749908447
I0216 00:10:14.164766 140318069466880 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6158874034881592, loss=1.3272310495376587
I0216 00:11:31.036918 140318077859584 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6900498867034912, loss=1.2270023822784424
I0216 00:12:50.378857 140318069466880 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.6864883899688721, loss=1.2925349473953247
I0216 00:14:12.360329 140318077859584 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.7312477827072144, loss=1.2207787036895752
I0216 00:15:33.852845 140318069466880 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6687701940536499, loss=1.306286334991455
I0216 00:16:55.988779 140318077859584 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.6812468767166138, loss=1.2433288097381592
I0216 00:18:12.781175 140318069466880 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6992727518081665, loss=1.252845048904419
I0216 00:18:40.091566 140399019657024 spec.py:321] Evaluating on the training split.
I0216 00:19:33.600501 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 00:20:24.126502 140399019657024 spec.py:349] Evaluating on the test split.
I0216 00:20:49.737339 140399019657024 submission_runner.py:408] Time since start: 30043.11s, 	Step: 33137, 	{'train/ctc_loss': Array(0.20576166, dtype=float32), 'train/wer': 0.07758233987197145, 'validation/ctc_loss': Array(0.46390292, dtype=float32), 'validation/wer': 0.13770431659538315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27050698, dtype=float32), 'test/wer': 0.09119899254565028, 'test/num_examples': 2472, 'score': 27400.628918409348, 'total_duration': 30043.10976457596, 'accumulated_submission_time': 27400.628918409348, 'accumulated_eval_time': 2640.090026140213, 'accumulated_logging_time': 0.9931390285491943}
I0216 00:20:49.775096 140318077859584 logging_writer.py:48] [33137] accumulated_eval_time=2640.090026, accumulated_logging_time=0.993139, accumulated_submission_time=27400.628918, global_step=33137, preemption_count=0, score=27400.628918, test/ctc_loss=0.27050697803497314, test/num_examples=2472, test/wer=0.091199, total_duration=30043.109765, train/ctc_loss=0.20576165616512299, train/wer=0.077582, validation/ctc_loss=0.46390292048454285, validation/num_examples=5348, validation/wer=0.137704
I0216 00:21:38.838484 140318069466880 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.593965470790863, loss=1.2209185361862183
I0216 00:22:55.602185 140318077859584 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6244324445724487, loss=1.2503694295883179
I0216 00:24:12.576516 140318069466880 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.792904794216156, loss=1.2424126863479614
I0216 00:25:29.419797 140318077859584 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7535510063171387, loss=1.2680555582046509
I0216 00:26:46.076951 140318069466880 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.7060666680335999, loss=1.2981951236724854
I0216 00:28:02.687211 140318077859584 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6662034392356873, loss=1.2652862071990967
I0216 00:29:19.497287 140318069466880 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.7101031541824341, loss=1.2928707599639893
I0216 00:30:42.144273 140318077859584 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.744944155216217, loss=1.2844911813735962
I0216 00:32:06.544919 140318077859584 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7938626408576965, loss=1.1986833810806274
I0216 00:33:23.574338 140318069466880 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.6132859587669373, loss=1.2322430610656738
I0216 00:34:40.419196 140318077859584 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8346476554870605, loss=1.2290410995483398
I0216 00:35:57.344214 140318069466880 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.7562848329544067, loss=1.2085492610931396
I0216 00:37:14.220881 140318077859584 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6535923480987549, loss=1.205195665359497
I0216 00:38:31.131667 140318069466880 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7871757745742798, loss=1.2578116655349731
I0216 00:39:48.119050 140318077859584 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.787817120552063, loss=1.212296962738037
I0216 00:41:07.306488 140318069466880 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7497804164886475, loss=1.294726014137268
I0216 00:42:28.852458 140318077859584 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6951817870140076, loss=1.2766586542129517
I0216 00:43:50.585335 140318069466880 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6386498808860779, loss=1.2488051652908325
I0216 00:44:50.406688 140399019657024 spec.py:321] Evaluating on the training split.
I0216 00:45:44.620854 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 00:46:35.547797 140399019657024 spec.py:349] Evaluating on the test split.
I0216 00:47:01.286195 140399019657024 submission_runner.py:408] Time since start: 31614.66s, 	Step: 34975, 	{'train/ctc_loss': Array(0.18593837, dtype=float32), 'train/wer': 0.07066081236328156, 'validation/ctc_loss': Array(0.44671303, dtype=float32), 'validation/wer': 0.13543547312627321, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26218984, dtype=float32), 'test/wer': 0.08945219669733716, 'test/num_examples': 2472, 'score': 28841.17727303505, 'total_duration': 31614.658063411713, 'accumulated_submission_time': 28841.17727303505, 'accumulated_eval_time': 2770.963233947754, 'accumulated_logging_time': 1.047239065170288}
I0216 00:47:01.323827 140318077859584 logging_writer.py:48] [34975] accumulated_eval_time=2770.963234, accumulated_logging_time=1.047239, accumulated_submission_time=28841.177273, global_step=34975, preemption_count=0, score=28841.177273, test/ctc_loss=0.2621898353099823, test/num_examples=2472, test/wer=0.089452, total_duration=31614.658063, train/ctc_loss=0.18593837320804596, train/wer=0.070661, validation/ctc_loss=0.44671303033828735, validation/num_examples=5348, validation/wer=0.135435
I0216 00:47:21.209523 140318069466880 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7082468867301941, loss=1.232174038887024
I0216 00:48:41.152081 140318077859584 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7567536234855652, loss=1.2413443326950073
I0216 00:49:57.964141 140318069466880 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.7064571976661682, loss=1.217642068862915
I0216 00:51:14.960436 140318077859584 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7843534350395203, loss=1.2324953079223633
I0216 00:52:31.771499 140318069466880 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6376121044158936, loss=1.2312755584716797
I0216 00:53:48.659492 140318077859584 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6974985599517822, loss=1.2974461317062378
I0216 00:55:05.640246 140318069466880 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6544180512428284, loss=1.246166706085205
I0216 00:56:23.749215 140318077859584 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7237750887870789, loss=1.2980122566223145
I0216 00:57:45.550591 140318069466880 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.7314149141311646, loss=1.2274092435836792
I0216 00:59:06.875981 140318077859584 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.8160827159881592, loss=1.2260361909866333
I0216 01:00:28.795965 140318069466880 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7024013996124268, loss=1.3022849559783936
I0216 01:01:51.038591 140318077859584 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.623155951499939, loss=1.2491271495819092
I0216 01:03:07.763482 140318069466880 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.7807477116584778, loss=1.1676677465438843
I0216 01:04:24.732083 140318077859584 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7690277695655823, loss=1.2053472995758057
I0216 01:05:41.863997 140318069466880 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6537609696388245, loss=1.1850636005401611
I0216 01:06:58.689582 140318077859584 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6796656250953674, loss=1.2301294803619385
I0216 01:08:15.537092 140318069466880 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6935345530509949, loss=1.3263083696365356
I0216 01:09:32.356256 140318077859584 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.8113266229629517, loss=1.224022388458252
I0216 01:10:51.963317 140318069466880 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6423764824867249, loss=1.2165930271148682
I0216 01:11:01.448488 140399019657024 spec.py:321] Evaluating on the training split.
I0216 01:11:55.576770 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 01:12:45.715276 140399019657024 spec.py:349] Evaluating on the test split.
I0216 01:13:11.511649 140399019657024 submission_runner.py:408] Time since start: 33184.88s, 	Step: 36813, 	{'train/ctc_loss': Array(0.1798886, dtype=float32), 'train/wer': 0.0659792170515853, 'validation/ctc_loss': Array(0.4445087, dtype=float32), 'validation/wer': 0.1318246328818174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25743183, dtype=float32), 'test/wer': 0.08673044502670972, 'test/num_examples': 2472, 'score': 30281.21803665161, 'total_duration': 33184.88313293457, 'accumulated_submission_time': 30281.21803665161, 'accumulated_eval_time': 2901.019725084305, 'accumulated_logging_time': 1.099900484085083}
I0216 01:13:11.554883 140318077859584 logging_writer.py:48] [36813] accumulated_eval_time=2901.019725, accumulated_logging_time=1.099900, accumulated_submission_time=30281.218037, global_step=36813, preemption_count=0, score=30281.218037, test/ctc_loss=0.25743183493614197, test/num_examples=2472, test/wer=0.086730, total_duration=33184.883133, train/ctc_loss=0.17988860607147217, train/wer=0.065979, validation/ctc_loss=0.44450870156288147, validation/num_examples=5348, validation/wer=0.131825
I0216 01:14:18.859744 140318069466880 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.6841295957565308, loss=1.2129781246185303
I0216 01:15:35.696655 140318077859584 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7559390068054199, loss=1.2410101890563965
I0216 01:16:55.671937 140318077859584 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.7586527466773987, loss=1.1704540252685547
I0216 01:18:12.191640 140318069466880 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.738538384437561, loss=1.1832175254821777
I0216 01:19:28.847874 140318077859584 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.7278438210487366, loss=1.2568718194961548
I0216 01:20:45.550287 140318069466880 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.797356903553009, loss=1.1409660577774048
I0216 01:22:02.373672 140318077859584 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6985639929771423, loss=1.2139471769332886
I0216 01:23:19.452368 140318069466880 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6753871440887451, loss=1.250171184539795
I0216 01:24:36.369877 140318077859584 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.759800910949707, loss=1.2196797132492065
I0216 01:25:56.776300 140318069466880 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.7511217594146729, loss=1.2005600929260254
I0216 01:27:18.490718 140318077859584 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6215812563896179, loss=1.2214024066925049
I0216 01:28:40.044279 140318069466880 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8050113320350647, loss=1.2557446956634521
I0216 01:30:02.064866 140318077859584 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.7752983570098877, loss=1.2304631471633911
I0216 01:31:22.353667 140318077859584 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.7992035150527954, loss=1.2020912170410156
I0216 01:32:38.987991 140318069466880 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7247003316879272, loss=1.1585499048233032
I0216 01:33:55.594135 140318077859584 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6865262389183044, loss=1.2190892696380615
I0216 01:35:12.350080 140318069466880 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7661754488945007, loss=1.2014044523239136
I0216 01:36:29.204835 140318077859584 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7577794790267944, loss=1.1827138662338257
I0216 01:37:12.087598 140399019657024 spec.py:321] Evaluating on the training split.
I0216 01:38:05.311220 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 01:38:55.659995 140399019657024 spec.py:349] Evaluating on the test split.
I0216 01:39:21.102620 140399019657024 submission_runner.py:408] Time since start: 34754.47s, 	Step: 38657, 	{'train/ctc_loss': Array(0.17082553, dtype=float32), 'train/wer': 0.06603682170542635, 'validation/ctc_loss': Array(0.43620944, dtype=float32), 'validation/wer': 0.13068538382073241, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24864013, dtype=float32), 'test/wer': 0.08439461336908172, 'test/num_examples': 2472, 'score': 31721.667903661728, 'total_duration': 34754.47449827194, 'accumulated_submission_time': 31721.667903661728, 'accumulated_eval_time': 3030.0284612178802, 'accumulated_logging_time': 1.1584982872009277}
I0216 01:39:21.145262 140318077859584 logging_writer.py:48] [38657] accumulated_eval_time=3030.028461, accumulated_logging_time=1.158498, accumulated_submission_time=31721.667904, global_step=38657, preemption_count=0, score=31721.667904, test/ctc_loss=0.24864013493061066, test/num_examples=2472, test/wer=0.084395, total_duration=34754.474498, train/ctc_loss=0.1708255261182785, train/wer=0.066037, validation/ctc_loss=0.43620944023132324, validation/num_examples=5348, validation/wer=0.130685
I0216 01:39:54.848011 140318069466880 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6942969560623169, loss=1.2052528858184814
I0216 01:41:11.486686 140318077859584 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6684218049049377, loss=1.1865146160125732
I0216 01:42:28.231886 140318069466880 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8528493642807007, loss=1.2318555116653442
I0216 01:43:44.983117 140318077859584 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.682968258857727, loss=1.1933902502059937
I0216 01:45:01.673061 140318069466880 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7818791270256042, loss=1.1832249164581299
I0216 01:46:21.770539 140318077859584 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.793753445148468, loss=1.1399321556091309
I0216 01:47:38.482357 140318069466880 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6548539996147156, loss=1.1673485040664673
I0216 01:48:55.136223 140318077859584 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6834238767623901, loss=1.1606903076171875
I0216 01:50:11.807427 140318069466880 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7194457650184631, loss=1.2042242288589478
I0216 01:51:28.356544 140318077859584 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.718863844871521, loss=1.2069026231765747
I0216 01:52:45.027766 140318069466880 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7070655226707458, loss=1.2311359643936157
I0216 01:54:02.077193 140318077859584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7681974172592163, loss=1.1976839303970337
I0216 01:55:22.471242 140318069466880 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.803270161151886, loss=1.209082841873169
I0216 01:56:43.875933 140318077859584 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.0053801536560059, loss=1.1749180555343628
I0216 01:58:05.884971 140318069466880 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.7335168719291687, loss=1.1780351400375366
I0216 01:59:29.871729 140318077859584 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7207489609718323, loss=1.1559934616088867
I0216 02:00:46.461874 140318069466880 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7561041116714478, loss=1.146889328956604
I0216 02:02:03.201296 140318077859584 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7191098928451538, loss=1.177567481994629
I0216 02:03:20.003759 140318069466880 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7621909976005554, loss=1.167729377746582
I0216 02:03:21.260228 140399019657024 spec.py:321] Evaluating on the training split.
I0216 02:04:14.889895 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 02:05:04.939938 140399019657024 spec.py:349] Evaluating on the test split.
I0216 02:05:30.303431 140399019657024 submission_runner.py:408] Time since start: 36323.68s, 	Step: 40503, 	{'train/ctc_loss': Array(0.18386379, dtype=float32), 'train/wer': 0.06808385219025163, 'validation/ctc_loss': Array(0.43003973, dtype=float32), 'validation/wer': 0.12768278671905925, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2497961, dtype=float32), 'test/wer': 0.08378526598013528, 'test/num_examples': 2472, 'score': 33161.69714832306, 'total_duration': 36323.67623138428, 'accumulated_submission_time': 33161.69714832306, 'accumulated_eval_time': 3159.066329717636, 'accumulated_logging_time': 1.2179243564605713}
I0216 02:05:30.340037 140318077859584 logging_writer.py:48] [40503] accumulated_eval_time=3159.066330, accumulated_logging_time=1.217924, accumulated_submission_time=33161.697148, global_step=40503, preemption_count=0, score=33161.697148, test/ctc_loss=0.24979610741138458, test/num_examples=2472, test/wer=0.083785, total_duration=36323.676231, train/ctc_loss=0.1838637888431549, train/wer=0.068084, validation/ctc_loss=0.43003973364830017, validation/num_examples=5348, validation/wer=0.127683
I0216 02:06:45.274840 140318069466880 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6909943222999573, loss=1.1841665506362915
I0216 02:08:01.970293 140318077859584 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7680529952049255, loss=1.2112340927124023
I0216 02:09:18.833266 140318069466880 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.8198391795158386, loss=1.1885039806365967
I0216 02:10:35.724336 140318077859584 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7338718175888062, loss=1.247901201248169
I0216 02:11:52.846546 140318069466880 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.9702578186988831, loss=1.2146252393722534
I0216 02:13:10.294250 140318077859584 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.7432695031166077, loss=1.14816415309906
I0216 02:14:36.014335 140318077859584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.7214194536209106, loss=1.1443105936050415
I0216 02:15:52.638882 140318069466880 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.8531312346458435, loss=1.1708524227142334
I0216 02:17:09.323349 140318077859584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7107512950897217, loss=1.157067894935608
I0216 02:18:26.081584 140318069466880 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9080288410186768, loss=1.1205334663391113
I0216 02:19:42.871421 140318077859584 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7139307260513306, loss=1.1921690702438354
I0216 02:20:59.573305 140318069466880 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8059962391853333, loss=1.191133975982666
I0216 02:22:16.262756 140318077859584 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7090581059455872, loss=1.098134994506836
I0216 02:23:36.077012 140318069466880 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.7809035778045654, loss=1.232119083404541
I0216 02:24:58.288719 140318077859584 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6958547234535217, loss=1.1745617389678955
I0216 02:26:19.994755 140318069466880 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.8640108108520508, loss=1.212990403175354
I0216 02:27:42.748910 140318077859584 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7395622730255127, loss=1.2396619319915771
I0216 02:29:04.466582 140318077859584 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7590945363044739, loss=1.169291377067566
I0216 02:29:30.910071 140399019657024 spec.py:321] Evaluating on the training split.
I0216 02:30:24.588521 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 02:31:14.884559 140399019657024 spec.py:349] Evaluating on the test split.
I0216 02:31:40.607040 140399019657024 submission_runner.py:408] Time since start: 37893.98s, 	Step: 42336, 	{'train/ctc_loss': Array(0.17193142, dtype=float32), 'train/wer': 0.06422414473995988, 'validation/ctc_loss': Array(0.41626227, dtype=float32), 'validation/wer': 0.12483466406634677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23499578, dtype=float32), 'test/wer': 0.07974326163345724, 'test/num_examples': 2472, 'score': 34602.18103837967, 'total_duration': 37893.97904467583, 'accumulated_submission_time': 34602.18103837967, 'accumulated_eval_time': 3288.7571585178375, 'accumulated_logging_time': 1.272374153137207}
I0216 02:31:40.648363 140318077859584 logging_writer.py:48] [42336] accumulated_eval_time=3288.757159, accumulated_logging_time=1.272374, accumulated_submission_time=34602.181038, global_step=42336, preemption_count=0, score=34602.181038, test/ctc_loss=0.2349957823753357, test/num_examples=2472, test/wer=0.079743, total_duration=37893.979045, train/ctc_loss=0.1719314157962799, train/wer=0.064224, validation/ctc_loss=0.41626226902008057, validation/num_examples=5348, validation/wer=0.124835
I0216 02:32:30.389125 140318069466880 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7749879360198975, loss=1.1729295253753662
I0216 02:33:47.183029 140318077859584 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6428431868553162, loss=1.1893335580825806
I0216 02:35:03.838864 140318069466880 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.8076350092887878, loss=1.1434777975082397
I0216 02:36:20.602503 140318077859584 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7106236815452576, loss=1.1612310409545898
I0216 02:37:37.342035 140318069466880 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8043331503868103, loss=1.1845365762710571
I0216 02:38:54.089802 140318077859584 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7409247159957886, loss=1.1809394359588623
I0216 02:40:10.937758 140318069466880 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.80646812915802, loss=1.197669267654419
I0216 02:41:27.825851 140318077859584 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.8355267643928528, loss=1.1385849714279175
I0216 02:42:49.770771 140318069466880 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.7073142528533936, loss=1.1591778993606567
I0216 02:44:13.827406 140318077859584 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7947713732719421, loss=1.1667375564575195
I0216 02:45:30.509626 140318069466880 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7440198659896851, loss=1.155946969985962
I0216 02:46:47.422752 140318077859584 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7462365627288818, loss=1.1271800994873047
I0216 02:48:04.105297 140318069466880 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.6688265204429626, loss=1.118556261062622
I0216 02:49:20.800773 140318077859584 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.73691725730896, loss=1.1846129894256592
I0216 02:50:37.603161 140318069466880 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7239336371421814, loss=1.1477959156036377
I0216 02:51:54.652011 140318077859584 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7190935015678406, loss=1.158347487449646
I0216 02:53:17.065260 140318069466880 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8162619471549988, loss=1.1812283992767334
I0216 02:54:39.687211 140318077859584 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7263202667236328, loss=1.15603506565094
I0216 02:55:41.589630 140399019657024 spec.py:321] Evaluating on the training split.
I0216 02:56:36.181163 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 02:57:26.645751 140399019657024 spec.py:349] Evaluating on the test split.
I0216 02:57:52.079704 140399019657024 submission_runner.py:408] Time since start: 39465.45s, 	Step: 44177, 	{'train/ctc_loss': Array(0.15760088, dtype=float32), 'train/wer': 0.06053693084604457, 'validation/ctc_loss': Array(0.41124475, dtype=float32), 'validation/wer': 0.12171621112795311, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23374555, dtype=float32), 'test/wer': 0.08006824690756201, 'test/num_examples': 2472, 'score': 36043.03625369072, 'total_duration': 39465.451370716095, 'accumulated_submission_time': 36043.03625369072, 'accumulated_eval_time': 3419.2407870292664, 'accumulated_logging_time': 1.3307726383209229}
I0216 02:57:52.120617 140318077859584 logging_writer.py:48] [44177] accumulated_eval_time=3419.240787, accumulated_logging_time=1.330773, accumulated_submission_time=36043.036254, global_step=44177, preemption_count=0, score=36043.036254, test/ctc_loss=0.2337455451488495, test/num_examples=2472, test/wer=0.080068, total_duration=39465.451371, train/ctc_loss=0.15760087966918945, train/wer=0.060537, validation/ctc_loss=0.4112447500228882, validation/num_examples=5348, validation/wer=0.121716
I0216 02:58:10.495198 140318069466880 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.7277355194091797, loss=1.1870672702789307
I0216 02:59:30.426997 140318077859584 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7216832041740417, loss=1.112457275390625
I0216 03:00:47.265659 140318069466880 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7542033791542053, loss=1.1193526983261108
I0216 03:02:04.082005 140318077859584 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.8932663202285767, loss=1.1408511400222778
I0216 03:03:20.861199 140318069466880 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9239368438720703, loss=1.1299915313720703
I0216 03:04:37.508140 140318077859584 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.9283345341682434, loss=1.1632643938064575
I0216 03:05:54.321004 140318069466880 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7968250513076782, loss=1.1119489669799805
I0216 03:07:11.034373 140318077859584 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8991844654083252, loss=1.138134479522705
I0216 03:08:33.150274 140318069466880 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6730247735977173, loss=1.118065595626831
I0216 03:09:55.873087 140318077859584 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.8506942987442017, loss=1.1965827941894531
I0216 03:11:18.991333 140318069466880 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7733684778213501, loss=1.1471985578536987
I0216 03:12:41.480920 140318077859584 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7876927256584167, loss=1.1705055236816406
I0216 03:14:02.327754 140318077859584 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6828874945640564, loss=1.1550801992416382
I0216 03:15:18.900269 140318069466880 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7880399227142334, loss=1.1552214622497559
I0216 03:16:35.871038 140318077859584 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.8020418286323547, loss=1.1924525499343872
I0216 03:17:52.561247 140318069466880 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.7414286136627197, loss=1.1132960319519043
I0216 03:19:09.423208 140318077859584 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.8992453217506409, loss=1.1885721683502197
I0216 03:20:26.152314 140318069466880 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7281700968742371, loss=1.1510123014450073
I0216 03:21:44.665193 140318077859584 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7819080352783203, loss=1.1797170639038086
I0216 03:21:52.495035 140399019657024 spec.py:321] Evaluating on the training split.
I0216 03:22:46.864156 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 03:23:37.082047 140399019657024 spec.py:349] Evaluating on the test split.
I0216 03:24:03.121786 140399019657024 submission_runner.py:408] Time since start: 41036.49s, 	Step: 46011, 	{'train/ctc_loss': Array(0.14866316, dtype=float32), 'train/wer': 0.05755842151100635, 'validation/ctc_loss': Array(0.39693525, dtype=float32), 'validation/wer': 0.11826949998551801, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22595222, dtype=float32), 'test/wer': 0.07543720675156906, 'test/num_examples': 2472, 'score': 37483.32582259178, 'total_duration': 41036.49385213852, 'accumulated_submission_time': 37483.32582259178, 'accumulated_eval_time': 3549.8614530563354, 'accumulated_logging_time': 1.3871524333953857}
I0216 03:24:03.164874 140318077859584 logging_writer.py:48] [46011] accumulated_eval_time=3549.861453, accumulated_logging_time=1.387152, accumulated_submission_time=37483.325823, global_step=46011, preemption_count=0, score=37483.325823, test/ctc_loss=0.22595222294330597, test/num_examples=2472, test/wer=0.075437, total_duration=41036.493852, train/ctc_loss=0.14866316318511963, train/wer=0.057558, validation/ctc_loss=0.39693525433540344, validation/num_examples=5348, validation/wer=0.118269
I0216 03:25:11.973576 140318069466880 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7678725719451904, loss=1.1663434505462646
I0216 03:26:28.776824 140318077859584 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.785430371761322, loss=1.105647087097168
I0216 03:27:45.446916 140318069466880 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.803503692150116, loss=1.1432344913482666
I0216 03:29:05.536775 140318077859584 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7756986021995544, loss=1.079140543937683
I0216 03:30:22.173041 140318069466880 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7452372908592224, loss=1.0989024639129639
I0216 03:31:38.900202 140318077859584 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.8827630281448364, loss=1.1267033815383911
I0216 03:32:55.857893 140318069466880 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7962564826011658, loss=1.1640855073928833
I0216 03:34:12.661233 140318077859584 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9251668453216553, loss=1.126313328742981
I0216 03:35:29.530592 140318069466880 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8408591151237488, loss=1.1116689443588257
I0216 03:36:46.352573 140318077859584 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7654665112495422, loss=1.1443952322006226
I0216 03:38:06.789076 140318069466880 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.9309431910514832, loss=1.174083948135376
I0216 03:39:29.351789 140318077859584 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.6927544474601746, loss=1.1322216987609863
I0216 03:40:51.259010 140318069466880 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.8244504928588867, loss=1.1728800535202026
I0216 03:42:15.326170 140318077859584 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7592051029205322, loss=1.0999408960342407
I0216 03:43:32.207819 140318069466880 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8338730931282043, loss=1.1340821981430054
I0216 03:44:49.190386 140318077859584 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.8638030886650085, loss=1.083849310874939
I0216 03:46:06.090533 140318069466880 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.8159700036048889, loss=1.040225625038147
I0216 03:47:23.022971 140318077859584 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.9819729328155518, loss=1.1004446744918823
I0216 03:48:03.702647 140399019657024 spec.py:321] Evaluating on the training split.
I0216 03:48:57.598181 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 03:49:47.671932 140399019657024 spec.py:349] Evaluating on the test split.
I0216 03:50:13.476274 140399019657024 submission_runner.py:408] Time since start: 42606.85s, 	Step: 47854, 	{'train/ctc_loss': Array(0.1336802, dtype=float32), 'train/wer': 0.051433701219834184, 'validation/ctc_loss': Array(0.39036846, dtype=float32), 'validation/wer': 0.11559516108788631, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.219991, dtype=float32), 'test/wer': 0.072654520342047, 'test/num_examples': 2472, 'score': 38923.7791454792, 'total_duration': 42606.84801912308, 'accumulated_submission_time': 38923.7791454792, 'accumulated_eval_time': 3679.6286759376526, 'accumulated_logging_time': 1.4453561305999756}
I0216 03:50:13.516371 140318077859584 logging_writer.py:48] [47854] accumulated_eval_time=3679.628676, accumulated_logging_time=1.445356, accumulated_submission_time=38923.779145, global_step=47854, preemption_count=0, score=38923.779145, test/ctc_loss=0.21999099850654602, test/num_examples=2472, test/wer=0.072655, total_duration=42606.848019, train/ctc_loss=0.13368019461631775, train/wer=0.051434, validation/ctc_loss=0.3903684616088867, validation/num_examples=5348, validation/wer=0.115595
I0216 03:50:49.508858 140318069466880 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7147005796432495, loss=1.0940672159194946
I0216 03:52:06.340519 140318077859584 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9748744368553162, loss=1.1096516847610474
I0216 03:53:23.275947 140318069466880 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.7882195115089417, loss=1.1325798034667969
I0216 03:54:40.260844 140318077859584 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7958832383155823, loss=1.1036304235458374
I0216 03:55:57.289518 140318069466880 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7917050123214722, loss=1.1361775398254395
I0216 03:57:14.207029 140318077859584 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8365122675895691, loss=1.1706095933914185
I0216 03:58:34.362714 140318077859584 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8875406384468079, loss=1.1038472652435303
I0216 03:59:51.148845 140318069466880 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.8812978267669678, loss=1.1265283823013306
I0216 04:01:07.986245 140318077859584 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.843451738357544, loss=1.1277248859405518
I0216 04:02:24.866923 140318069466880 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.9220736026763916, loss=1.1022121906280518
I0216 04:03:41.787856 140318077859584 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.9221126437187195, loss=1.086681604385376
I0216 04:04:58.910619 140318069466880 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.879130482673645, loss=1.0911427736282349
I0216 04:06:15.925918 140318077859584 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.9902719855308533, loss=1.1391249895095825
I0216 04:07:38.103815 140318069466880 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.878991425037384, loss=1.1261157989501953
I0216 04:09:00.630654 140318077859584 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.7984960079193115, loss=1.150356411933899
I0216 04:10:22.069586 140318069466880 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7801352739334106, loss=1.0943598747253418
I0216 04:11:43.862285 140318077859584 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.972960889339447, loss=1.0767611265182495
I0216 04:13:00.442854 140318069466880 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9067950248718262, loss=1.0702975988388062
I0216 04:14:13.749221 140399019657024 spec.py:321] Evaluating on the training split.
I0216 04:15:08.315519 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 04:15:58.753238 140399019657024 spec.py:349] Evaluating on the test split.
I0216 04:16:24.308797 140399019657024 submission_runner.py:408] Time since start: 44177.68s, 	Step: 49697, 	{'train/ctc_loss': Array(0.13503304, dtype=float32), 'train/wer': 0.051360241911591935, 'validation/ctc_loss': Array(0.38563406, dtype=float32), 'validation/wer': 0.11369319443505797, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21653153, dtype=float32), 'test/wer': 0.07100928239189162, 'test/num_examples': 2472, 'score': 40363.92171168327, 'total_duration': 44177.68133687973, 'accumulated_submission_time': 40363.92171168327, 'accumulated_eval_time': 3810.1826572418213, 'accumulated_logging_time': 1.5044760704040527}
I0216 04:16:24.350912 140318077859584 logging_writer.py:48] [49697] accumulated_eval_time=3810.182657, accumulated_logging_time=1.504476, accumulated_submission_time=40363.921712, global_step=49697, preemption_count=0, score=40363.921712, test/ctc_loss=0.21653153002262115, test/num_examples=2472, test/wer=0.071009, total_duration=44177.681337, train/ctc_loss=0.1350330412387848, train/wer=0.051360, validation/ctc_loss=0.38563406467437744, validation/num_examples=5348, validation/wer=0.113693
I0216 04:16:27.501212 140318069466880 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.862876832485199, loss=1.084426760673523
I0216 04:17:44.120077 140318077859584 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8129486441612244, loss=1.106514811515808
I0216 04:19:00.868410 140318069466880 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7839441299438477, loss=1.0472447872161865
I0216 04:20:17.488983 140318077859584 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.8794249892234802, loss=1.1037870645523071
I0216 04:21:34.478878 140318069466880 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8744217157363892, loss=1.1533020734786987
I0216 04:22:51.218978 140318077859584 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7713534832000732, loss=1.083925724029541
I0216 04:24:08.156698 140318069466880 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.8995761275291443, loss=1.0824609994888306
I0216 04:25:29.953575 140318077859584 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.7609866857528687, loss=1.0997909307479858
I0216 04:26:53.367040 140318077859584 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.8641220331192017, loss=1.0458967685699463
I0216 04:28:10.142299 140318069466880 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.8690351843833923, loss=1.048248052597046
I0216 04:29:26.795489 140318077859584 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8432497978210449, loss=1.0691251754760742
I0216 04:30:43.676056 140318069466880 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.8110873699188232, loss=1.104477047920227
I0216 04:32:00.610144 140318077859584 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.8053159713745117, loss=1.0854415893554688
I0216 04:33:17.559498 140318069466880 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.887010931968689, loss=1.1012382507324219
I0216 04:34:34.464045 140318077859584 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.9579979181289673, loss=1.0384310483932495
I0216 04:35:52.866982 140318069466880 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.9297699332237244, loss=1.1116690635681152
I0216 04:37:15.108240 140318077859584 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.9499332308769226, loss=1.1316224336624146
I0216 04:38:37.320384 140318069466880 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8370690941810608, loss=1.0856815576553345
I0216 04:40:02.405986 140318077859584 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.8246861696243286, loss=1.0610209703445435
I0216 04:40:24.361397 140399019657024 spec.py:321] Evaluating on the training split.
I0216 04:41:17.930942 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 04:42:08.684236 140399019657024 spec.py:349] Evaluating on the test split.
I0216 04:42:34.286559 140399019657024 submission_runner.py:408] Time since start: 45747.66s, 	Step: 51530, 	{'train/ctc_loss': Array(0.13680682, dtype=float32), 'train/wer': 0.052348666972799225, 'validation/ctc_loss': Array(0.37866125, dtype=float32), 'validation/wer': 0.11185881035364995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20964031, dtype=float32), 'test/wer': 0.07163894136046961, 'test/num_examples': 2472, 'score': 41803.847143411636, 'total_duration': 45747.658468961716, 'accumulated_submission_time': 41803.847143411636, 'accumulated_eval_time': 3940.1016092300415, 'accumulated_logging_time': 1.5616471767425537}
I0216 04:42:34.329289 140318077859584 logging_writer.py:48] [51530] accumulated_eval_time=3940.101609, accumulated_logging_time=1.561647, accumulated_submission_time=41803.847143, global_step=51530, preemption_count=0, score=41803.847143, test/ctc_loss=0.20964030921459198, test/num_examples=2472, test/wer=0.071639, total_duration=45747.658469, train/ctc_loss=0.13680681586265564, train/wer=0.052349, validation/ctc_loss=0.37866124510765076, validation/num_examples=5348, validation/wer=0.111859
I0216 04:43:28.604672 140318069466880 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8367267847061157, loss=1.0807163715362549
I0216 04:44:45.461799 140318077859584 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.857660174369812, loss=1.0887326002120972
I0216 04:46:02.269334 140318069466880 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.9489659667015076, loss=1.0854541063308716
I0216 04:47:19.201954 140318077859584 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9470546245574951, loss=1.081274390220642
I0216 04:48:35.926497 140318069466880 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8684353232383728, loss=1.1074074506759644
I0216 04:49:52.890481 140318077859584 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.9545453786849976, loss=1.0363610982894897
I0216 04:51:09.785322 140318069466880 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.9441809058189392, loss=1.0364587306976318
I0216 04:52:26.613287 140318077859584 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.8610424399375916, loss=1.1250770092010498
I0216 04:53:46.512568 140318069466880 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9593634009361267, loss=1.1208860874176025
I0216 04:55:09.526183 140318077859584 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7841105461120605, loss=1.0609071254730225
I0216 04:56:30.645156 140318077859584 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.7576240301132202, loss=1.0500496625900269
I0216 04:57:47.316257 140318069466880 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.0610724687576294, loss=1.007016897201538
I0216 04:59:04.146776 140318077859584 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.8769444823265076, loss=1.0285520553588867
I0216 05:00:20.953752 140318069466880 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.786694347858429, loss=1.1142120361328125
I0216 05:01:37.688884 140318077859584 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.8639577031135559, loss=1.0985468626022339
I0216 05:02:54.462982 140318069466880 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.8579890727996826, loss=1.083150863647461
I0216 05:04:11.409854 140318077859584 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.9700130224227905, loss=1.0690970420837402
I0216 05:05:35.700128 140318069466880 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8059594035148621, loss=1.0901240110397339
I0216 05:06:34.534343 140399019657024 spec.py:321] Evaluating on the training split.
I0216 05:07:28.076692 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 05:08:18.735821 140399019657024 spec.py:349] Evaluating on the test split.
I0216 05:08:44.251877 140399019657024 submission_runner.py:408] Time since start: 47317.62s, 	Step: 53373, 	{'train/ctc_loss': Array(0.11914682, dtype=float32), 'train/wer': 0.04524203680836034, 'validation/ctc_loss': Array(0.36934325, dtype=float32), 'validation/wer': 0.10898172374175734, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2012541, dtype=float32), 'test/wer': 0.06818597282310646, 'test/num_examples': 2472, 'score': 43243.965631484985, 'total_duration': 47317.6235909462, 'accumulated_submission_time': 43243.965631484985, 'accumulated_eval_time': 4069.81272816658, 'accumulated_logging_time': 1.6211626529693604}
I0216 05:08:44.290712 140318077859584 logging_writer.py:48] [53373] accumulated_eval_time=4069.812728, accumulated_logging_time=1.621163, accumulated_submission_time=43243.965631, global_step=53373, preemption_count=0, score=43243.965631, test/ctc_loss=0.20125409960746765, test/num_examples=2472, test/wer=0.068186, total_duration=47317.623591, train/ctc_loss=0.11914681643247604, train/wer=0.045242, validation/ctc_loss=0.36934325098991394, validation/num_examples=5348, validation/wer=0.108982
I0216 05:09:05.668571 140318069466880 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.8299843668937683, loss=1.0890787839889526
I0216 05:10:22.447529 140318077859584 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9972151517868042, loss=1.0516003370285034
I0216 05:11:42.661419 140318077859584 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.1019829511642456, loss=1.0290119647979736
I0216 05:12:59.366259 140318069466880 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7813413143157959, loss=1.0192984342575073
I0216 05:14:16.173415 140318077859584 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.2331860065460205, loss=1.0237995386123657
I0216 05:15:32.870104 140318069466880 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9174719452857971, loss=1.0563157796859741
I0216 05:16:49.729579 140318077859584 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8624023199081421, loss=1.0781627893447876
I0216 05:18:06.615107 140318069466880 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.8870331048965454, loss=1.048244833946228
I0216 05:19:23.774849 140318077859584 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.9374495148658752, loss=1.069388747215271
I0216 05:20:46.255692 140318069466880 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0183099508285522, loss=1.0255855321884155
I0216 05:22:09.044554 140318077859584 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8172447085380554, loss=1.0708398818969727
I0216 05:23:30.895069 140318069466880 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.856959879398346, loss=1.085278868675232
I0216 05:24:56.361712 140318077859584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.8404115438461304, loss=0.9785565733909607
I0216 05:26:13.542788 140318069466880 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8511019945144653, loss=1.0062034130096436
I0216 05:27:30.517976 140318077859584 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8619586825370789, loss=1.0058156251907349
I0216 05:28:47.422481 140318069466880 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.981955349445343, loss=1.0365917682647705
I0216 05:30:04.374636 140318077859584 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1148042678833008, loss=1.0422383546829224
I0216 05:31:21.454401 140318069466880 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.042490005493164, loss=1.039373755455017
I0216 05:32:38.489564 140318077859584 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8442502021789551, loss=1.0590753555297852
I0216 05:32:44.377213 140399019657024 spec.py:321] Evaluating on the training split.
I0216 05:33:39.229624 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 05:34:30.349010 140399019657024 spec.py:349] Evaluating on the test split.
I0216 05:34:56.301075 140399019657024 submission_runner.py:408] Time since start: 48889.67s, 	Step: 55209, 	{'train/ctc_loss': Array(0.11970915, dtype=float32), 'train/wer': 0.047040435353802854, 'validation/ctc_loss': Array(0.35692453, dtype=float32), 'validation/wer': 0.10478194966063895, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19781065, dtype=float32), 'test/wer': 0.06615481485995166, 'test/num_examples': 2472, 'score': 44683.96534562111, 'total_duration': 48889.67320728302, 'accumulated_submission_time': 44683.96534562111, 'accumulated_eval_time': 4201.730570077896, 'accumulated_logging_time': 1.6765682697296143}
I0216 05:34:56.340628 140318077859584 logging_writer.py:48] [55209] accumulated_eval_time=4201.730570, accumulated_logging_time=1.676568, accumulated_submission_time=44683.965346, global_step=55209, preemption_count=0, score=44683.965346, test/ctc_loss=0.19781064987182617, test/num_examples=2472, test/wer=0.066155, total_duration=48889.673207, train/ctc_loss=0.11970914900302887, train/wer=0.047040, validation/ctc_loss=0.35692453384399414, validation/num_examples=5348, validation/wer=0.104782
I0216 05:36:06.640151 140318069466880 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.0644036531448364, loss=1.0839887857437134
I0216 05:37:23.396450 140318077859584 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.7704659700393677, loss=1.0280574560165405
I0216 05:38:40.239581 140318069466880 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0734281539916992, loss=1.0403766632080078
I0216 05:39:57.220236 140318077859584 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.845028817653656, loss=1.038537621498108
I0216 05:41:17.326822 140318077859584 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.9509706497192383, loss=1.0289599895477295
I0216 05:42:34.111211 140318069466880 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0291301012039185, loss=1.0257939100265503
I0216 05:43:51.043452 140318077859584 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8931413292884827, loss=1.0306254625320435
I0216 05:45:07.962442 140318069466880 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.860925555229187, loss=1.0318207740783691
I0216 05:46:24.624861 140318077859584 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0744342803955078, loss=1.0443966388702393
I0216 05:47:41.452460 140318069466880 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9355976581573486, loss=1.0671908855438232
I0216 05:49:00.183551 140318077859584 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.84015953540802, loss=1.0388017892837524
I0216 05:50:21.804076 140318069466880 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.8931009769439697, loss=1.0280826091766357
I0216 05:51:45.076604 140318077859584 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.081320881843567, loss=1.0579040050506592
I0216 05:53:07.186514 140318069466880 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.317070484161377, loss=0.993363618850708
I0216 05:54:30.252423 140318077859584 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0381584167480469, loss=0.9676966071128845
I0216 05:55:47.211043 140318069466880 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.187518835067749, loss=1.0790228843688965
I0216 05:57:04.128848 140318077859584 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.9121759533882141, loss=0.9759401679039001
I0216 05:58:21.325098 140318069466880 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1568543910980225, loss=1.0044336318969727
I0216 05:58:56.455091 140399019657024 spec.py:321] Evaluating on the training split.
I0216 05:59:50.145843 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 06:00:41.058408 140399019657024 spec.py:349] Evaluating on the test split.
I0216 06:01:06.909313 140399019657024 submission_runner.py:408] Time since start: 50460.28s, 	Step: 57047, 	{'train/ctc_loss': Array(0.13444948, dtype=float32), 'train/wer': 0.04753825451953813, 'validation/ctc_loss': Array(0.3579548, dtype=float32), 'validation/wer': 0.10444403680353746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19295095, dtype=float32), 'test/wer': 0.06302683159669327, 'test/num_examples': 2472, 'score': 46123.9932820797, 'total_duration': 50460.28098845482, 'accumulated_submission_time': 46123.9932820797, 'accumulated_eval_time': 4332.178327083588, 'accumulated_logging_time': 1.7337017059326172}
I0216 06:01:06.952697 140318077859584 logging_writer.py:48] [57047] accumulated_eval_time=4332.178327, accumulated_logging_time=1.733702, accumulated_submission_time=46123.993282, global_step=57047, preemption_count=0, score=46123.993282, test/ctc_loss=0.19295094907283783, test/num_examples=2472, test/wer=0.063027, total_duration=50460.280988, train/ctc_loss=0.13444948196411133, train/wer=0.047538, validation/ctc_loss=0.35795480012893677, validation/num_examples=5348, validation/wer=0.104444
I0216 06:01:48.237740 140318069466880 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.8897597789764404, loss=1.0219290256500244
I0216 06:03:04.997301 140318077859584 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9778531193733215, loss=1.0034078359603882
I0216 06:04:21.585794 140318069466880 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1383405923843384, loss=1.0189740657806396
I0216 06:05:38.381450 140318077859584 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.012656331062317, loss=1.0771458148956299
I0216 06:06:55.115506 140318069466880 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.8871691823005676, loss=1.0269023180007935
I0216 06:08:15.033949 140318077859584 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9022248387336731, loss=1.0126076936721802
I0216 06:09:40.109137 140318077859584 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8037542104721069, loss=0.9701108932495117
I0216 06:10:56.798873 140318069466880 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.9147074222564697, loss=0.9641884565353394
I0216 06:12:13.599118 140318077859584 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9897016286849976, loss=0.9992073774337769
I0216 06:13:30.397470 140318069466880 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.90131676197052, loss=1.0455958843231201
I0216 06:14:47.048642 140318077859584 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.046655297279358, loss=0.9897639155387878
I0216 06:16:04.001294 140318069466880 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.8799836039543152, loss=1.0174756050109863
I0216 06:17:20.984912 140318077859584 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0225194692611694, loss=1.0363446474075317
I0216 06:18:43.990744 140318069466880 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9877582788467407, loss=1.0160930156707764
I0216 06:20:05.851102 140318077859584 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1373775005340576, loss=1.01728093624115
I0216 06:21:28.862940 140318069466880 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.9335747957229614, loss=0.9975900053977966
I0216 06:22:52.686181 140318077859584 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9306210279464722, loss=0.9752311110496521
I0216 06:24:13.097623 140318077859584 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9076797962188721, loss=1.0089472532272339
I0216 06:25:07.160783 140399019657024 spec.py:321] Evaluating on the training split.
I0216 06:26:02.498878 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 06:26:52.996875 140399019657024 spec.py:349] Evaluating on the test split.
I0216 06:27:19.327489 140399019657024 submission_runner.py:408] Time since start: 52032.70s, 	Step: 58872, 	{'train/ctc_loss': Array(0.08747087, dtype=float32), 'train/wer': 0.034387704817050634, 'validation/ctc_loss': Array(0.34598675, dtype=float32), 'validation/wer': 0.09919190553887446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18680844, dtype=float32), 'test/wer': 0.06142221680580099, 'test/num_examples': 2472, 'score': 47564.11428070068, 'total_duration': 52032.69997930527, 'accumulated_submission_time': 47564.11428070068, 'accumulated_eval_time': 4464.339418172836, 'accumulated_logging_time': 1.7940006256103516}
I0216 06:27:19.367098 140318077859584 logging_writer.py:48] [58872] accumulated_eval_time=4464.339418, accumulated_logging_time=1.794001, accumulated_submission_time=47564.114281, global_step=58872, preemption_count=0, score=47564.114281, test/ctc_loss=0.18680843710899353, test/num_examples=2472, test/wer=0.061422, total_duration=52032.699979, train/ctc_loss=0.08747086673974991, train/wer=0.034388, validation/ctc_loss=0.3459867537021637, validation/num_examples=5348, validation/wer=0.099192
I0216 06:27:41.603238 140318069466880 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8647006750106812, loss=0.956342875957489
I0216 06:28:58.375705 140318077859584 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.1309456825256348, loss=1.0025906562805176
I0216 06:30:15.097173 140318069466880 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.1283822059631348, loss=1.0196646451950073
I0216 06:31:31.860965 140318077859584 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0005617141723633, loss=1.0277717113494873
I0216 06:32:49.068788 140318069466880 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0571762323379517, loss=1.012567400932312
I0216 06:34:05.884087 140318077859584 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.978550910949707, loss=1.078799843788147
I0216 06:35:22.713235 140318069466880 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1328316926956177, loss=1.0256285667419434
I0216 06:36:45.939089 140318077859584 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0624358654022217, loss=1.0174272060394287
I0216 06:38:09.051588 140318069466880 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2624396085739136, loss=1.026427984237671
I0216 06:39:31.061689 140318077859584 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.8875782489776611, loss=0.9601297378540039
I0216 06:40:47.678809 140318069466880 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1681815385818481, loss=1.0581645965576172
I0216 06:42:04.326534 140318077859584 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0184619426727295, loss=0.999233603477478
I0216 06:43:21.079791 140318069466880 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9245445132255554, loss=0.9766335487365723
I0216 06:44:37.904938 140318077859584 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.2872846126556396, loss=0.9870239496231079
I0216 06:45:54.691551 140318069466880 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0051980018615723, loss=0.9888767600059509
I0216 06:47:12.977510 140318077859584 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.8995648622512817, loss=0.9861339926719666
I0216 06:48:36.710724 140318069466880 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.0465165376663208, loss=1.023384928703308
I0216 06:50:00.173988 140318077859584 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.2537120580673218, loss=0.9855141639709473
I0216 06:51:20.107098 140399019657024 spec.py:321] Evaluating on the training split.
I0216 06:52:15.126827 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 06:53:06.254436 140399019657024 spec.py:349] Evaluating on the test split.
I0216 06:53:32.388465 140399019657024 submission_runner.py:408] Time since start: 53605.76s, 	Step: 60697, 	{'train/ctc_loss': Array(0.08663601, dtype=float32), 'train/wer': 0.03332674462153052, 'validation/ctc_loss': Array(0.32997072, dtype=float32), 'validation/wer': 0.09642102011064232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17877539, dtype=float32), 'test/wer': 0.06046757256311824, 'test/num_examples': 2472, 'score': 49004.76940536499, 'total_duration': 53605.760281562805, 'accumulated_submission_time': 49004.76940536499, 'accumulated_eval_time': 4596.614463329315, 'accumulated_logging_time': 1.8496484756469727}
I0216 06:53:32.432432 140318077859584 logging_writer.py:48] [60697] accumulated_eval_time=4596.614463, accumulated_logging_time=1.849648, accumulated_submission_time=49004.769405, global_step=60697, preemption_count=0, score=49004.769405, test/ctc_loss=0.1787753850221634, test/num_examples=2472, test/wer=0.060468, total_duration=53605.760282, train/ctc_loss=0.0866360142827034, train/wer=0.033327, validation/ctc_loss=0.32997071743011475, validation/num_examples=5348, validation/wer=0.096421
I0216 06:53:35.584325 140318069466880 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.0687605142593384, loss=0.9951133728027344
I0216 06:54:55.467440 140318077859584 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.9477448463439941, loss=0.961020290851593
I0216 06:56:12.098224 140318069466880 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0228925943374634, loss=0.9651505947113037
I0216 06:57:28.791309 140318077859584 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0830590724945068, loss=0.968051552772522
I0216 06:58:45.659861 140318069466880 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.116489291191101, loss=1.0083576440811157
I0216 07:00:02.450746 140318077859584 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9748411774635315, loss=0.8865963816642761
I0216 07:01:19.297102 140318069466880 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.9009918570518494, loss=0.9257217049598694
I0216 07:02:41.453167 140318077859584 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.3794782161712646, loss=1.0020081996917725
I0216 07:04:03.650486 140318069466880 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.180051326751709, loss=1.0149115324020386
I0216 07:05:25.414809 140318077859584 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.9462869167327881, loss=0.9207482933998108
I0216 07:06:47.568792 140318069466880 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.1728423833847046, loss=0.9628440737724304
I0216 07:08:13.335803 140318077859584 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.9516202807426453, loss=0.9591583013534546
I0216 07:09:30.056513 140318069466880 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.9736809134483337, loss=0.9236164689064026
I0216 07:10:46.850986 140318077859584 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0009914636611938, loss=0.9534807205200195
I0216 07:12:03.730568 140318069466880 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0259593725204468, loss=0.9888346791267395
I0216 07:13:20.541943 140318077859584 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.9733660221099854, loss=0.9487317204475403
I0216 07:14:37.351983 140318069466880 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.1303590536117554, loss=0.9018502235412598
I0216 07:15:54.008571 140318077859584 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.9854815006256104, loss=0.9859266877174377
I0216 07:17:15.133071 140318069466880 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.3477696180343628, loss=0.9681355953216553
I0216 07:17:32.690742 140399019657024 spec.py:321] Evaluating on the training split.
I0216 07:18:25.747352 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 07:19:16.414411 140399019657024 spec.py:349] Evaluating on the test split.
I0216 07:19:42.266731 140399019657024 submission_runner.py:408] Time since start: 55175.64s, 	Step: 62523, 	{'train/ctc_loss': Array(0.10608669, dtype=float32), 'train/wer': 0.04166254111796869, 'validation/ctc_loss': Array(0.32664424, dtype=float32), 'validation/wer': 0.09537831758015776, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17875016, dtype=float32), 'test/wer': 0.0592691893648569, 'test/num_examples': 2472, 'score': 50444.94024038315, 'total_duration': 55175.63874554634, 'accumulated_submission_time': 50444.94024038315, 'accumulated_eval_time': 4726.184319496155, 'accumulated_logging_time': 1.9108588695526123}
I0216 07:19:42.314063 140318077859584 logging_writer.py:48] [62523] accumulated_eval_time=4726.184319, accumulated_logging_time=1.910859, accumulated_submission_time=50444.940240, global_step=62523, preemption_count=0, score=50444.940240, test/ctc_loss=0.1787501573562622, test/num_examples=2472, test/wer=0.059269, total_duration=55175.638746, train/ctc_loss=0.10608669370412827, train/wer=0.041663, validation/ctc_loss=0.32664424180984497, validation/num_examples=5348, validation/wer=0.095378
I0216 07:20:42.052507 140318069466880 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0429264307022095, loss=0.9159502387046814
I0216 07:21:58.992671 140318077859584 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.050337314605713, loss=0.9597012400627136
I0216 07:23:16.223177 140318069466880 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.0046107769012451, loss=0.9567191004753113
I0216 07:24:36.272076 140318077859584 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.0541338920593262, loss=0.9498622417449951
I0216 07:25:53.038855 140318069466880 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.9621306657791138, loss=0.8978964686393738
I0216 07:27:09.856758 140318077859584 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1945769786834717, loss=0.8965875506401062
I0216 07:28:26.631245 140318069466880 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.9176838994026184, loss=0.9328134059906006
I0216 07:29:43.476969 140318077859584 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0287225246429443, loss=0.9318545460700989
I0216 07:31:00.379973 140318069466880 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.2583577632904053, loss=0.9926198124885559
I0216 07:32:20.195153 140318077859584 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.3174697160720825, loss=0.932339608669281
I0216 07:33:42.254239 140318069466880 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.4770631790161133, loss=0.946749210357666
I0216 07:35:05.626395 140318077859584 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.9841175079345703, loss=0.9070106744766235
I0216 07:36:29.226871 140318069466880 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0336459875106812, loss=0.9521272778511047
I0216 07:37:53.389708 140318077859584 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0564919710159302, loss=0.9157764911651611
I0216 07:39:09.874254 140318069466880 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1564559936523438, loss=0.9140179753303528
I0216 07:40:26.624635 140318077859584 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0062049627304077, loss=0.9472829103469849
I0216 07:41:43.362911 140318069466880 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1457242965698242, loss=0.9657447338104248
I0216 07:43:00.089177 140318077859584 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.077858567237854, loss=0.9237114191055298
I0216 07:43:42.616491 140399019657024 spec.py:321] Evaluating on the training split.
I0216 07:44:35.596873 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 07:45:27.135207 140399019657024 spec.py:349] Evaluating on the test split.
I0216 07:45:52.645358 140399019657024 submission_runner.py:408] Time since start: 56746.02s, 	Step: 64357, 	{'train/ctc_loss': Array(0.10365024, dtype=float32), 'train/wer': 0.039632183402028384, 'validation/ctc_loss': Array(0.31951347, dtype=float32), 'validation/wer': 0.09191229713160258, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17223309, dtype=float32), 'test/wer': 0.056994292446123536, 'test/num_examples': 2472, 'score': 51885.154266119, 'total_duration': 56746.015449762344, 'accumulated_submission_time': 51885.154266119, 'accumulated_eval_time': 4856.205128669739, 'accumulated_logging_time': 1.9761414527893066}
I0216 07:45:52.687285 140318077859584 logging_writer.py:48] [64357] accumulated_eval_time=4856.205129, accumulated_logging_time=1.976141, accumulated_submission_time=51885.154266, global_step=64357, preemption_count=0, score=51885.154266, test/ctc_loss=0.17223308980464935, test/num_examples=2472, test/wer=0.056994, total_duration=56746.015450, train/ctc_loss=0.10365024209022522, train/wer=0.039632, validation/ctc_loss=0.3195134699344635, validation/num_examples=5348, validation/wer=0.091912
I0216 07:46:26.259624 140318069466880 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1330801248550415, loss=0.9160029888153076
I0216 07:47:42.887262 140318077859584 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0605010986328125, loss=0.9210708737373352
I0216 07:48:59.682716 140318069466880 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1642717123031616, loss=0.9664251804351807
I0216 07:50:16.430090 140318077859584 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1474391222000122, loss=0.9682564735412598
I0216 07:51:33.355263 140318069466880 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.0443758964538574, loss=0.8858717679977417
I0216 07:52:55.252603 140318077859584 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.117234706878662, loss=0.9661949276924133
I0216 07:54:11.991156 140318069466880 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.077635407447815, loss=0.936897873878479
I0216 07:55:29.060550 140318077859584 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0470471382141113, loss=0.9451746940612793
I0216 07:56:45.946238 140318069466880 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1146116256713867, loss=0.9137642979621887
I0216 07:58:02.786988 140318077859584 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.121134877204895, loss=0.9605975151062012
I0216 07:59:19.398625 140318069466880 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0603326559066772, loss=0.9227087497711182
I0216 08:00:36.193534 140318077859584 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3726741075515747, loss=0.9257145524024963
I0216 08:02:00.404973 140318069466880 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.1007916927337646, loss=0.9293107390403748
I0216 08:03:25.425114 140318077859584 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2811732292175293, loss=0.9427323937416077
I0216 08:04:48.921826 140318069466880 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1236586570739746, loss=0.928857684135437
I0216 08:06:12.966251 140318077859584 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.4718998670578003, loss=0.9315620064735413
I0216 08:07:34.341536 140318077859584 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.257012963294983, loss=0.9572890996932983
I0216 08:08:51.094150 140318069466880 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.4048607349395752, loss=0.9345817565917969
I0216 08:09:53.189458 140399019657024 spec.py:321] Evaluating on the training split.
I0216 08:10:44.444374 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 08:11:35.477602 140399019657024 spec.py:349] Evaluating on the test split.
I0216 08:12:01.429611 140399019657024 submission_runner.py:408] Time since start: 58314.80s, 	Step: 66182, 	{'train/ctc_loss': Array(0.11834669, dtype=float32), 'train/wer': 0.04559072856910607, 'validation/ctc_loss': Array(0.3103893, dtype=float32), 'validation/wer': 0.0891317570503104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16681975, dtype=float32), 'test/wer': 0.054739707107021716, 'test/num_examples': 2472, 'score': 53325.569303274155, 'total_duration': 58314.800549030304, 'accumulated_submission_time': 53325.569303274155, 'accumulated_eval_time': 4984.438056707382, 'accumulated_logging_time': 2.034407615661621}
I0216 08:12:01.496414 140318077859584 logging_writer.py:48] [66182] accumulated_eval_time=4984.438057, accumulated_logging_time=2.034408, accumulated_submission_time=53325.569303, global_step=66182, preemption_count=0, score=53325.569303, test/ctc_loss=0.1668197512626648, test/num_examples=2472, test/wer=0.054740, total_duration=58314.800549, train/ctc_loss=0.1183466911315918, train/wer=0.045591, validation/ctc_loss=0.31038931012153625, validation/num_examples=5348, validation/wer=0.089132
I0216 08:12:16.010472 140318069466880 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.2711328268051147, loss=0.8780363202095032
I0216 08:13:32.683550 140318077859584 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.0557770729064941, loss=0.8925943374633789
I0216 08:14:49.393887 140318069466880 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.114030122756958, loss=0.8794668912887573
I0216 08:16:06.153685 140318077859584 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2090659141540527, loss=0.9478158354759216
I0216 08:17:22.804959 140318069466880 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1631019115447998, loss=0.9922530055046082
I0216 08:18:39.461197 140318077859584 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.1217433214187622, loss=0.9160013198852539
I0216 08:20:01.230707 140318069466880 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.341418981552124, loss=0.8973509073257446
I0216 08:21:24.328924 140318077859584 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.170090913772583, loss=0.9258825778961182
I0216 08:22:47.724136 140318077859584 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1283279657363892, loss=0.8865359425544739
I0216 08:24:04.721034 140318069466880 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.4616447687149048, loss=0.9089054465293884
I0216 08:25:21.646548 140318077859584 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0125030279159546, loss=0.910716712474823
I0216 08:26:38.724972 140318069466880 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1741585731506348, loss=0.9407488703727722
I0216 08:27:55.621019 140318077859584 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.1944212913513184, loss=0.9317840337753296
I0216 08:29:12.622317 140318069466880 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.238609790802002, loss=0.9116644859313965
I0216 08:30:30.887082 140318077859584 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.2009177207946777, loss=0.8882662057876587
I0216 08:31:53.649134 140318069466880 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1851919889450073, loss=0.8856838345527649
I0216 08:33:16.697658 140318077859584 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.3943333625793457, loss=0.8912885785102844
I0216 08:34:40.452190 140318069466880 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.0959163904190063, loss=0.8850753307342529
I0216 08:36:01.581672 140399019657024 spec.py:321] Evaluating on the training split.
I0216 08:36:54.023280 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 08:37:45.051786 140399019657024 spec.py:349] Evaluating on the test split.
I0216 08:38:10.939027 140399019657024 submission_runner.py:408] Time since start: 59884.31s, 	Step: 67997, 	{'train/ctc_loss': Array(0.09386895, dtype=float32), 'train/wer': 0.03463274325935973, 'validation/ctc_loss': Array(0.30321878, dtype=float32), 'validation/wer': 0.08691118684650068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16329446, dtype=float32), 'test/wer': 0.05331789653281336, 'test/num_examples': 2472, 'score': 54765.563039541245, 'total_duration': 59884.31038093567, 'accumulated_submission_time': 54765.563039541245, 'accumulated_eval_time': 5113.788911581039, 'accumulated_logging_time': 2.122683525085449}
I0216 08:38:10.980653 140318077859584 logging_writer.py:48] [67997] accumulated_eval_time=5113.788912, accumulated_logging_time=2.122684, accumulated_submission_time=54765.563040, global_step=67997, preemption_count=0, score=54765.563040, test/ctc_loss=0.1632944643497467, test/num_examples=2472, test/wer=0.053318, total_duration=59884.310381, train/ctc_loss=0.09386894851922989, train/wer=0.034633, validation/ctc_loss=0.3032187819480896, validation/num_examples=5348, validation/wer=0.086911
I0216 08:38:14.146317 140318069466880 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0736113786697388, loss=0.8654730916023254
I0216 08:39:30.770501 140318077859584 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2361732721328735, loss=0.8931989073753357
I0216 08:40:47.641074 140318069466880 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1668074131011963, loss=0.8517748713493347
I0216 08:42:04.442861 140318077859584 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3694372177124023, loss=0.8656327128410339
I0216 08:43:21.125874 140318069466880 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2237677574157715, loss=0.9118543863296509
I0216 08:44:38.151510 140318077859584 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.2001266479492188, loss=0.9231603741645813
I0216 08:45:54.980597 140318069466880 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.491181492805481, loss=0.8852968811988831
I0216 08:47:11.791831 140318077859584 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.423649787902832, loss=0.9072725176811218
I0216 08:48:34.078440 140318069466880 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.155269980430603, loss=0.8942986130714417
I0216 08:49:57.641150 140318077859584 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2309523820877075, loss=0.9621830582618713
I0216 08:51:21.700237 140318069466880 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8767741918563843, loss=0.8786612153053284
I0216 08:52:42.084192 140318077859584 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.084822654724121, loss=0.8404083251953125
I0216 08:53:58.863672 140318069466880 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.180137276649475, loss=0.910312294960022
I0216 08:55:15.647322 140318077859584 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2355194091796875, loss=0.8852952718734741
I0216 08:56:32.479681 140318069466880 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.3093547821044922, loss=0.945375919342041
I0216 08:57:49.343816 140318077859584 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1570401191711426, loss=0.929091215133667
I0216 08:59:08.255220 140318069466880 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.055487036705017, loss=0.8663939833641052
I0216 09:00:32.321896 140318077859584 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.3778619766235352, loss=0.9062727689743042
I0216 09:01:56.359309 140318069466880 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1511515378952026, loss=0.8483765125274658
I0216 09:02:11.007273 140399019657024 spec.py:321] Evaluating on the training split.
I0216 09:03:04.022683 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 09:03:55.046690 140399019657024 spec.py:349] Evaluating on the test split.
I0216 09:04:20.843678 140399019657024 submission_runner.py:408] Time since start: 61454.21s, 	Step: 69819, 	{'train/ctc_loss': Array(0.09016636, dtype=float32), 'train/wer': 0.035446594101522955, 'validation/ctc_loss': Array(0.30152643, dtype=float32), 'validation/wer': 0.08541471562219412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1599037, dtype=float32), 'test/wer': 0.05252574492718299, 'test/num_examples': 2472, 'score': 56205.50395298004, 'total_duration': 61454.21480035782, 'accumulated_submission_time': 56205.50395298004, 'accumulated_eval_time': 5243.618288755417, 'accumulated_logging_time': 2.180286169052124}
I0216 09:04:20.893929 140318077859584 logging_writer.py:48] [69819] accumulated_eval_time=5243.618289, accumulated_logging_time=2.180286, accumulated_submission_time=56205.503953, global_step=69819, preemption_count=0, score=56205.503953, test/ctc_loss=0.15990370512008667, test/num_examples=2472, test/wer=0.052526, total_duration=61454.214800, train/ctc_loss=0.0901663601398468, train/wer=0.035447, validation/ctc_loss=0.30152642726898193, validation/num_examples=5348, validation/wer=0.085415
I0216 09:05:23.726337 140318069466880 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.386717438697815, loss=0.8825404047966003
I0216 09:06:40.465013 140318077859584 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1578936576843262, loss=0.892440915107727
I0216 09:08:00.418521 140318077859584 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1883200407028198, loss=0.9049084186553955
I0216 09:09:17.185982 140318069466880 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.1265547275543213, loss=0.9167453050613403
I0216 09:10:33.880669 140318077859584 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.2229214906692505, loss=0.8442012667655945
I0216 09:11:50.771401 140318069466880 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.2288364171981812, loss=0.8760631680488586
I0216 09:13:07.632502 140318077859584 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1011236906051636, loss=0.8187019228935242
I0216 09:14:25.250421 140318069466880 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.2321916818618774, loss=0.9015860557556152
I0216 09:15:49.615086 140318077859584 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.3610076904296875, loss=0.8629688620567322
I0216 09:17:12.055238 140318069466880 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2074302434921265, loss=0.8686960339546204
I0216 09:18:36.008477 140318077859584 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.060539484024048, loss=0.8892099857330322
I0216 09:19:59.842202 140318069466880 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.1094348430633545, loss=0.8483028411865234
I0216 09:21:24.214868 140318077859584 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3455352783203125, loss=0.8692485094070435
I0216 09:22:40.853015 140318069466880 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.265045404434204, loss=0.8494107127189636
I0216 09:23:57.550150 140318077859584 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.6785719394683838, loss=0.8699132800102234
I0216 09:25:14.475296 140318069466880 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.4639688730239868, loss=0.8818840980529785
I0216 09:26:31.309342 140318077859584 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.166214108467102, loss=0.8749281167984009
I0216 09:27:48.228311 140318069466880 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2108538150787354, loss=0.8793675899505615
I0216 09:28:21.298196 140399019657024 spec.py:321] Evaluating on the training split.
I0216 09:29:15.893364 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 09:30:07.844770 140399019657024 spec.py:349] Evaluating on the test split.
I0216 09:30:34.383291 140399019657024 submission_runner.py:408] Time since start: 63027.75s, 	Step: 71644, 	{'train/ctc_loss': Array(0.0650331, dtype=float32), 'train/wer': 0.024735723700137182, 'validation/ctc_loss': Array(0.29494342, dtype=float32), 'validation/wer': 0.08378307925504697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15629764, dtype=float32), 'test/wer': 0.05081957223813296, 'test/num_examples': 2472, 'score': 57645.81762099266, 'total_duration': 63027.75369310379, 'accumulated_submission_time': 57645.81762099266, 'accumulated_eval_time': 5376.695669412613, 'accumulated_logging_time': 2.2511696815490723}
I0216 09:30:34.430232 140318077859584 logging_writer.py:48] [71644] accumulated_eval_time=5376.695669, accumulated_logging_time=2.251170, accumulated_submission_time=57645.817621, global_step=71644, preemption_count=0, score=57645.817621, test/ctc_loss=0.15629763901233673, test/num_examples=2472, test/wer=0.050820, total_duration=63027.753693, train/ctc_loss=0.06503310054540634, train/wer=0.024736, validation/ctc_loss=0.2949434220790863, validation/num_examples=5348, validation/wer=0.083783
I0216 09:31:17.993566 140318069466880 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1358215808868408, loss=0.867027759552002
I0216 09:32:34.757214 140318077859584 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0885952711105347, loss=0.8288592100143433
I0216 09:33:51.730818 140318069466880 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.0668848752975464, loss=0.8909739255905151
I0216 09:35:08.448937 140318077859584 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4609688520431519, loss=0.8887289762496948
I0216 09:36:29.355782 140318077859584 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.4075487852096558, loss=0.8440254926681519
I0216 09:37:46.203568 140318069466880 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.5816328525543213, loss=0.8336171507835388
I0216 09:39:03.233894 140318077859584 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3800729513168335, loss=0.8307159543037415
I0216 09:40:20.261532 140318069466880 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.3153198957443237, loss=0.849263608455658
I0216 09:41:37.107037 140318077859584 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2151260375976562, loss=0.8498291373252869
I0216 09:42:53.969204 140318069466880 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.2693997621536255, loss=0.859889030456543
I0216 09:44:14.008817 140318077859584 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.841736912727356, loss=0.898359477519989
I0216 09:45:36.533188 140318069466880 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3652549982070923, loss=0.868371844291687
I0216 09:46:59.803852 140318077859584 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.3738970756530762, loss=0.8714017868041992
I0216 09:48:23.800051 140318069466880 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.2763386964797974, loss=0.8642856478691101
I0216 09:49:46.188673 140318077859584 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3260228633880615, loss=0.906093418598175
I0216 09:51:07.513697 140318077859584 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.3631350994110107, loss=0.8973908424377441
I0216 09:52:24.157237 140318069466880 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.390795111656189, loss=0.849138617515564
I0216 09:53:40.731139 140318077859584 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.5189582109451294, loss=0.8755449056625366
I0216 09:54:34.780427 140399019657024 spec.py:321] Evaluating on the training split.
I0216 09:55:28.032699 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 09:56:18.922833 140399019657024 spec.py:349] Evaluating on the test split.
I0216 09:56:44.627700 140399019657024 submission_runner.py:408] Time since start: 64598.00s, 	Step: 73472, 	{'train/ctc_loss': Array(0.07749713, dtype=float32), 'train/wer': 0.030339686258196587, 'validation/ctc_loss': Array(0.29196256, dtype=float32), 'validation/wer': 0.08276934068374253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1558389, dtype=float32), 'test/wer': 0.049864927995450205, 'test/num_examples': 2472, 'score': 59086.080971241, 'total_duration': 64598.00075960159, 'accumulated_submission_time': 59086.080971241, 'accumulated_eval_time': 5506.537875413895, 'accumulated_logging_time': 2.314791440963745}
I0216 09:56:44.672467 140318077859584 logging_writer.py:48] [73472] accumulated_eval_time=5506.537875, accumulated_logging_time=2.314791, accumulated_submission_time=59086.080971, global_step=73472, preemption_count=0, score=59086.080971, test/ctc_loss=0.15583890676498413, test/num_examples=2472, test/wer=0.049865, total_duration=64598.000760, train/ctc_loss=0.07749713212251663, train/wer=0.030340, validation/ctc_loss=0.29196256399154663, validation/num_examples=5348, validation/wer=0.082769
I0216 09:57:06.866875 140318069466880 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2838975191116333, loss=0.8419793844223022
I0216 09:58:23.600588 140318077859584 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.1653554439544678, loss=0.8662919402122498
I0216 09:59:40.465978 140318069466880 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3678056001663208, loss=0.871192991733551
I0216 10:00:57.264410 140318077859584 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.4189373254776, loss=0.876309335231781
I0216 10:02:13.985816 140318069466880 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.2556995153427124, loss=0.8753572702407837
I0216 10:03:33.809671 140318077859584 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.5882757902145386, loss=0.8168124556541443
I0216 10:04:56.193638 140318069466880 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.183408260345459, loss=0.8665789365768433
I0216 10:06:20.136569 140318077859584 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1879996061325073, loss=0.8757891654968262
I0216 10:07:36.767307 140318069466880 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.999173641204834, loss=0.8004096746444702
I0216 10:08:53.448359 140318077859584 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.4596298933029175, loss=0.8956756591796875
I0216 10:10:10.221498 140318069466880 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.4539636373519897, loss=0.8672323822975159
I0216 10:11:26.990154 140318077859584 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2301117181777954, loss=0.8049559593200684
I0216 10:12:43.749151 140318069466880 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.807015061378479, loss=0.8568390607833862
I0216 10:14:06.083863 140318077859584 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.3737255334854126, loss=0.8432125449180603
I0216 10:15:28.986669 140318069466880 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.1504602432250977, loss=0.8448282480239868
I0216 10:16:51.697550 140318077859584 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3228458166122437, loss=0.8710594773292542
I0216 10:18:14.325967 140318069466880 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.5146815776824951, loss=0.841068685054779
I0216 10:19:40.047532 140318077859584 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.268646478652954, loss=0.8262738585472107
I0216 10:20:45.019373 140399019657024 spec.py:321] Evaluating on the training split.
I0216 10:21:39.137200 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 10:22:30.642070 140399019657024 spec.py:349] Evaluating on the test split.
I0216 10:22:56.639192 140399019657024 submission_runner.py:408] Time since start: 66170.01s, 	Step: 75286, 	{'train/ctc_loss': Array(0.06773014, dtype=float32), 'train/wer': 0.024790915163660655, 'validation/ctc_loss': Array(0.289029, dtype=float32), 'validation/wer': 0.08147561717369686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15373042, dtype=float32), 'test/wer': 0.04909308796945138, 'test/num_examples': 2472, 'score': 60526.34147930145, 'total_duration': 66170.01026082039, 'accumulated_submission_time': 60526.34147930145, 'accumulated_eval_time': 5638.150643587112, 'accumulated_logging_time': 2.375697612762451}
I0216 10:22:56.696988 140318077859584 logging_writer.py:48] [75286] accumulated_eval_time=5638.150644, accumulated_logging_time=2.375698, accumulated_submission_time=60526.341479, global_step=75286, preemption_count=0, score=60526.341479, test/ctc_loss=0.15373042225837708, test/num_examples=2472, test/wer=0.049093, total_duration=66170.010261, train/ctc_loss=0.0677301362156868, train/wer=0.024791, validation/ctc_loss=0.28902900218963623, validation/num_examples=5348, validation/wer=0.081476
I0216 10:23:08.277371 140318069466880 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2475255727767944, loss=0.8396607637405396
I0216 10:24:25.023609 140318077859584 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.3449480533599854, loss=0.8530619144439697
I0216 10:25:41.643885 140318069466880 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.3094218969345093, loss=0.8545390963554382
I0216 10:26:58.472754 140318077859584 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.470487356185913, loss=0.8222560286521912
I0216 10:28:15.162466 140318069466880 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.6100786924362183, loss=0.8409357666969299
I0216 10:29:31.786622 140318077859584 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.2674452066421509, loss=0.8581689596176147
I0216 10:30:48.640188 140318069466880 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3048020601272583, loss=0.8473644256591797
I0216 10:31:59.060508 140318077859584 logging_writer.py:48] [75992] global_step=75992, preemption_count=0, score=61068.641757
I0216 10:31:59.864062 140399019657024 checkpoints.py:490] Saving checkpoint at step: 75992
I0216 10:32:01.416733 140399019657024 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3/checkpoint_75992
I0216 10:32:01.477392 140399019657024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_3/checkpoint_75992.
I0216 10:32:05.427531 140399019657024 submission_runner.py:583] Tuning trial 3/5
I0216 10:32:05.427777 140399019657024 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0216 10:32:05.453315 140399019657024 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.681545, dtype=float32), 'train/wer': 1.395902589872439, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043156299178389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.279491, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 34.168030738830566, 'total_duration': 177.3963179588318, 'accumulated_submission_time': 34.168030738830566, 'accumulated_eval_time': 143.2282350063324, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1670, {'train/ctc_loss': Array(6.3159585, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.4463353, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4492598, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1474.7652645111084, 'total_duration': 1728.55646276474, 'accumulated_submission_time': 1474.7652645111084, 'accumulated_eval_time': 253.69146490097046, 'accumulated_logging_time': 0.029105186462402344, 'global_step': 1670, 'preemption_count': 0}), (3370, {'train/ctc_loss': Array(3.8715672, dtype=float32), 'train/wer': 0.8640947888589398, 'validation/ctc_loss': Array(3.8034496, dtype=float32), 'validation/wer': 0.8164843546347162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.4893599, dtype=float32), 'test/wer': 0.776105457721447, 'test/num_examples': 2472, 'score': 2914.80074095726, 'total_duration': 3285.8231496810913, 'accumulated_submission_time': 2914.80074095726, 'accumulated_eval_time': 370.795667886734, 'accumulated_logging_time': 0.07997775077819824, 'global_step': 3370, 'preemption_count': 0}), (5031, {'train/ctc_loss': Array(1.1874629, dtype=float32), 'train/wer': 0.35988786356734026, 'validation/ctc_loss': Array(1.2110256, dtype=float32), 'validation/wer': 0.34403390714154686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.91928077, dtype=float32), 'test/wer': 0.2884244307679808, 'test/num_examples': 2472, 'score': 4354.720586776733, 'total_duration': 4860.35196018219, 'accumulated_submission_time': 4354.720586776733, 'accumulated_eval_time': 505.2781677246094, 'accumulated_logging_time': 0.13262176513671875, 'global_step': 5031, 'preemption_count': 0}), (6703, {'train/ctc_loss': Array(0.7139498, dtype=float32), 'train/wer': 0.23824286482052442, 'validation/ctc_loss': Array(0.84304017, dtype=float32), 'validation/wer': 0.2546994023769756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5802292, dtype=float32), 'test/wer': 0.19596612028517457, 'test/num_examples': 2472, 'score': 5795.25229716301, 'total_duration': 6438.438871383667, 'accumulated_submission_time': 5795.25229716301, 'accumulated_eval_time': 642.7062382698059, 'accumulated_logging_time': 0.18616890907287598, 'global_step': 6703, 'preemption_count': 0}), (8372, {'train/ctc_loss': Array(0.5976577, dtype=float32), 'train/wer': 0.19920432590627965, 'validation/ctc_loss': Array(0.7593585, dtype=float32), 'validation/wer': 0.22556165944176795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50354314, dtype=float32), 'test/wer': 0.16952044360489915, 'test/num_examples': 2472, 'score': 7235.874894618988, 'total_duration': 8015.850210905075, 'accumulated_submission_time': 7235.874894618988, 'accumulated_eval_time': 779.3713698387146, 'accumulated_logging_time': 0.2369072437286377, 'global_step': 8372, 'preemption_count': 0}), (10060, {'train/ctc_loss': Array(0.58726174, dtype=float32), 'train/wer': 0.2015751037064896, 'validation/ctc_loss': Array(0.6942416, dtype=float32), 'validation/wer': 0.2111665717292449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44782707, dtype=float32), 'test/wer': 0.15134157983466373, 'test/num_examples': 2472, 'score': 8676.452405929565, 'total_duration': 9594.790989875793, 'accumulated_submission_time': 8676.452405929565, 'accumulated_eval_time': 917.6057934761047, 'accumulated_logging_time': 0.29051780700683594, 'global_step': 10060, 'preemption_count': 0}), (11776, {'train/ctc_loss': Array(0.5025373, dtype=float32), 'train/wer': 0.1714755275350288, 'validation/ctc_loss': Array(0.6410851, dtype=float32), 'validation/wer': 0.1946764243026927, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40460023, dtype=float32), 'test/wer': 0.13726565515000103, 'test/num_examples': 2472, 'score': 10116.661395788193, 'total_duration': 11169.995291233063, 'accumulated_submission_time': 10116.661395788193, 'accumulated_eval_time': 1052.4742050170898, 'accumulated_logging_time': 0.3399777412414551, 'global_step': 11776, 'preemption_count': 0}), (13468, {'train/ctc_loss': Array(0.49499294, dtype=float32), 'train/wer': 0.16616104992415748, 'validation/ctc_loss': Array(0.6108735, dtype=float32), 'validation/wer': 0.18658582503837726, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38324997, dtype=float32), 'test/wer': 0.13253305709585034, 'test/num_examples': 2472, 'score': 11556.729298353195, 'total_duration': 12748.30952501297, 'accumulated_submission_time': 11556.729298353195, 'accumulated_eval_time': 1190.592571258545, 'accumulated_logging_time': 0.39217233657836914, 'global_step': 13468, 'preemption_count': 0}), (15157, {'train/ctc_loss': Array(0.41451362, dtype=float32), 'train/wer': 0.14536975496146726, 'validation/ctc_loss': Array(0.5853169, dtype=float32), 'validation/wer': 0.17944138177394595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3680178, dtype=float32), 'test/wer': 0.12524120000812464, 'test/num_examples': 2472, 'score': 12996.726741552353, 'total_duration': 14323.149782896042, 'accumulated_submission_time': 12996.726741552353, 'accumulated_eval_time': 1325.306384563446, 'accumulated_logging_time': 0.4475064277648926, 'global_step': 15157, 'preemption_count': 0}), (16871, {'train/ctc_loss': Array(0.41015854, dtype=float32), 'train/wer': 0.1461925696453384, 'validation/ctc_loss': Array(0.56820166, dtype=float32), 'validation/wer': 0.17308862006043813, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34979352, dtype=float32), 'test/wer': 0.12081327564844718, 'test/num_examples': 2472, 'score': 14436.753607988358, 'total_duration': 15899.094260454178, 'accumulated_submission_time': 14436.753607988358, 'accumulated_eval_time': 1461.0907878875732, 'accumulated_logging_time': 0.5039346218109131, 'global_step': 16871, 'preemption_count': 0}), (18548, {'train/ctc_loss': Array(0.3982619, dtype=float32), 'train/wer': 0.14110618034674327, 'validation/ctc_loss': Array(0.55043125, dtype=float32), 'validation/wer': 0.1676240864284542, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33841434, dtype=float32), 'test/wer': 0.11526821440903459, 'test/num_examples': 2472, 'score': 15877.172675848007, 'total_duration': 17475.227288007736, 'accumulated_submission_time': 15877.172675848007, 'accumulated_eval_time': 1596.6722741127014, 'accumulated_logging_time': 0.5620999336242676, 'global_step': 18548, 'preemption_count': 0}), (20273, {'train/ctc_loss': Array(0.26937568, dtype=float32), 'train/wer': 0.0986536762934611, 'validation/ctc_loss': Array(0.54442644, dtype=float32), 'validation/wer': 0.1646794172451413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32714456, dtype=float32), 'test/wer': 0.1112871448012512, 'test/num_examples': 2472, 'score': 17317.922025680542, 'total_duration': 19063.102712631226, 'accumulated_submission_time': 17317.922025680542, 'accumulated_eval_time': 1743.6689743995667, 'accumulated_logging_time': 0.6150286197662354, 'global_step': 20273, 'preemption_count': 0}), (22022, {'train/ctc_loss': Array(0.25267893, dtype=float32), 'train/wer': 0.09203062521310561, 'validation/ctc_loss': Array(0.5279306, dtype=float32), 'validation/wer': 0.16062446295992353, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31976706, dtype=float32), 'test/wer': 0.10915442893993865, 'test/num_examples': 2472, 'score': 18758.112550258636, 'total_duration': 20631.210894346237, 'accumulated_submission_time': 18758.112550258636, 'accumulated_eval_time': 1871.4549486637115, 'accumulated_logging_time': 0.674034595489502, 'global_step': 22022, 'preemption_count': 0}), (23874, {'train/ctc_loss': Array(0.24335396, dtype=float32), 'train/wer': 0.09045162715126276, 'validation/ctc_loss': Array(0.51649487, dtype=float32), 'validation/wer': 0.1568688029195671, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31226346, dtype=float32), 'test/wer': 0.1043405845672618, 'test/num_examples': 2472, 'score': 20198.68664932251, 'total_duration': 22198.257422208786, 'accumulated_submission_time': 20198.68664932251, 'accumulated_eval_time': 1997.8045163154602, 'accumulated_logging_time': 0.7282936573028564, 'global_step': 23874, 'preemption_count': 0}), (25727, {'train/ctc_loss': Array(0.22757375, dtype=float32), 'train/wer': 0.08577388314710384, 'validation/ctc_loss': Array(0.5035488, dtype=float32), 'validation/wer': 0.15267868349150873, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2979179, dtype=float32), 'test/wer': 0.10245160766152783, 'test/num_examples': 2472, 'score': 21638.83458685875, 'total_duration': 23766.222700834274, 'accumulated_submission_time': 21638.83458685875, 'accumulated_eval_time': 2125.4988000392914, 'accumulated_logging_time': 0.7810320854187012, 'global_step': 25727, 'preemption_count': 0}), (27582, {'train/ctc_loss': Array(0.22518203, dtype=float32), 'train/wer': 0.08362828084689025, 'validation/ctc_loss': Array(0.49105188, dtype=float32), 'validation/wer': 0.1481313419002288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2894307, dtype=float32), 'test/wer': 0.09792212540369265, 'test/num_examples': 2472, 'score': 23079.494574785233, 'total_duration': 25335.13685107231, 'accumulated_submission_time': 23079.494574785233, 'accumulated_eval_time': 2253.6334249973297, 'accumulated_logging_time': 0.829434871673584, 'global_step': 27582, 'preemption_count': 0}), (29437, {'train/ctc_loss': Array(0.20949112, dtype=float32), 'train/wer': 0.07906515745021853, 'validation/ctc_loss': Array(0.47871414, dtype=float32), 'validation/wer': 0.14474255867615396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27920473, dtype=float32), 'test/wer': 0.09585034428127476, 'test/num_examples': 2472, 'score': 24520.111387252808, 'total_duration': 26905.28270840645, 'accumulated_submission_time': 24520.111387252808, 'accumulated_eval_time': 2383.0318739414215, 'accumulated_logging_time': 0.8874096870422363, 'global_step': 29437, 'preemption_count': 0}), (31289, {'train/ctc_loss': Array(0.24946217, dtype=float32), 'train/wer': 0.08724518370698234, 'validation/ctc_loss': Array(0.4818028, dtype=float32), 'validation/wer': 0.14407638761501104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28004083, dtype=float32), 'test/wer': 0.09499725793674973, 'test/num_examples': 2472, 'score': 25960.74817752838, 'total_duration': 28473.462604045868, 'accumulated_submission_time': 25960.74817752838, 'accumulated_eval_time': 2510.4499428272247, 'accumulated_logging_time': 0.9405355453491211, 'global_step': 31289, 'preemption_count': 0}), (33137, {'train/ctc_loss': Array(0.20576166, dtype=float32), 'train/wer': 0.07758233987197145, 'validation/ctc_loss': Array(0.46390292, dtype=float32), 'validation/wer': 0.13770431659538315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27050698, dtype=float32), 'test/wer': 0.09119899254565028, 'test/num_examples': 2472, 'score': 27400.628918409348, 'total_duration': 30043.10976457596, 'accumulated_submission_time': 27400.628918409348, 'accumulated_eval_time': 2640.090026140213, 'accumulated_logging_time': 0.9931390285491943, 'global_step': 33137, 'preemption_count': 0}), (34975, {'train/ctc_loss': Array(0.18593837, dtype=float32), 'train/wer': 0.07066081236328156, 'validation/ctc_loss': Array(0.44671303, dtype=float32), 'validation/wer': 0.13543547312627321, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26218984, dtype=float32), 'test/wer': 0.08945219669733716, 'test/num_examples': 2472, 'score': 28841.17727303505, 'total_duration': 31614.658063411713, 'accumulated_submission_time': 28841.17727303505, 'accumulated_eval_time': 2770.963233947754, 'accumulated_logging_time': 1.047239065170288, 'global_step': 34975, 'preemption_count': 0}), (36813, {'train/ctc_loss': Array(0.1798886, dtype=float32), 'train/wer': 0.0659792170515853, 'validation/ctc_loss': Array(0.4445087, dtype=float32), 'validation/wer': 0.1318246328818174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25743183, dtype=float32), 'test/wer': 0.08673044502670972, 'test/num_examples': 2472, 'score': 30281.21803665161, 'total_duration': 33184.88313293457, 'accumulated_submission_time': 30281.21803665161, 'accumulated_eval_time': 2901.019725084305, 'accumulated_logging_time': 1.099900484085083, 'global_step': 36813, 'preemption_count': 0}), (38657, {'train/ctc_loss': Array(0.17082553, dtype=float32), 'train/wer': 0.06603682170542635, 'validation/ctc_loss': Array(0.43620944, dtype=float32), 'validation/wer': 0.13068538382073241, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24864013, dtype=float32), 'test/wer': 0.08439461336908172, 'test/num_examples': 2472, 'score': 31721.667903661728, 'total_duration': 34754.47449827194, 'accumulated_submission_time': 31721.667903661728, 'accumulated_eval_time': 3030.0284612178802, 'accumulated_logging_time': 1.1584982872009277, 'global_step': 38657, 'preemption_count': 0}), (40503, {'train/ctc_loss': Array(0.18386379, dtype=float32), 'train/wer': 0.06808385219025163, 'validation/ctc_loss': Array(0.43003973, dtype=float32), 'validation/wer': 0.12768278671905925, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2497961, dtype=float32), 'test/wer': 0.08378526598013528, 'test/num_examples': 2472, 'score': 33161.69714832306, 'total_duration': 36323.67623138428, 'accumulated_submission_time': 33161.69714832306, 'accumulated_eval_time': 3159.066329717636, 'accumulated_logging_time': 1.2179243564605713, 'global_step': 40503, 'preemption_count': 0}), (42336, {'train/ctc_loss': Array(0.17193142, dtype=float32), 'train/wer': 0.06422414473995988, 'validation/ctc_loss': Array(0.41626227, dtype=float32), 'validation/wer': 0.12483466406634677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23499578, dtype=float32), 'test/wer': 0.07974326163345724, 'test/num_examples': 2472, 'score': 34602.18103837967, 'total_duration': 37893.97904467583, 'accumulated_submission_time': 34602.18103837967, 'accumulated_eval_time': 3288.7571585178375, 'accumulated_logging_time': 1.272374153137207, 'global_step': 42336, 'preemption_count': 0}), (44177, {'train/ctc_loss': Array(0.15760088, dtype=float32), 'train/wer': 0.06053693084604457, 'validation/ctc_loss': Array(0.41124475, dtype=float32), 'validation/wer': 0.12171621112795311, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23374555, dtype=float32), 'test/wer': 0.08006824690756201, 'test/num_examples': 2472, 'score': 36043.03625369072, 'total_duration': 39465.451370716095, 'accumulated_submission_time': 36043.03625369072, 'accumulated_eval_time': 3419.2407870292664, 'accumulated_logging_time': 1.3307726383209229, 'global_step': 44177, 'preemption_count': 0}), (46011, {'train/ctc_loss': Array(0.14866316, dtype=float32), 'train/wer': 0.05755842151100635, 'validation/ctc_loss': Array(0.39693525, dtype=float32), 'validation/wer': 0.11826949998551801, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22595222, dtype=float32), 'test/wer': 0.07543720675156906, 'test/num_examples': 2472, 'score': 37483.32582259178, 'total_duration': 41036.49385213852, 'accumulated_submission_time': 37483.32582259178, 'accumulated_eval_time': 3549.8614530563354, 'accumulated_logging_time': 1.3871524333953857, 'global_step': 46011, 'preemption_count': 0}), (47854, {'train/ctc_loss': Array(0.1336802, dtype=float32), 'train/wer': 0.051433701219834184, 'validation/ctc_loss': Array(0.39036846, dtype=float32), 'validation/wer': 0.11559516108788631, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.219991, dtype=float32), 'test/wer': 0.072654520342047, 'test/num_examples': 2472, 'score': 38923.7791454792, 'total_duration': 42606.84801912308, 'accumulated_submission_time': 38923.7791454792, 'accumulated_eval_time': 3679.6286759376526, 'accumulated_logging_time': 1.4453561305999756, 'global_step': 47854, 'preemption_count': 0}), (49697, {'train/ctc_loss': Array(0.13503304, dtype=float32), 'train/wer': 0.051360241911591935, 'validation/ctc_loss': Array(0.38563406, dtype=float32), 'validation/wer': 0.11369319443505797, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21653153, dtype=float32), 'test/wer': 0.07100928239189162, 'test/num_examples': 2472, 'score': 40363.92171168327, 'total_duration': 44177.68133687973, 'accumulated_submission_time': 40363.92171168327, 'accumulated_eval_time': 3810.1826572418213, 'accumulated_logging_time': 1.5044760704040527, 'global_step': 49697, 'preemption_count': 0}), (51530, {'train/ctc_loss': Array(0.13680682, dtype=float32), 'train/wer': 0.052348666972799225, 'validation/ctc_loss': Array(0.37866125, dtype=float32), 'validation/wer': 0.11185881035364995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20964031, dtype=float32), 'test/wer': 0.07163894136046961, 'test/num_examples': 2472, 'score': 41803.847143411636, 'total_duration': 45747.658468961716, 'accumulated_submission_time': 41803.847143411636, 'accumulated_eval_time': 3940.1016092300415, 'accumulated_logging_time': 1.5616471767425537, 'global_step': 51530, 'preemption_count': 0}), (53373, {'train/ctc_loss': Array(0.11914682, dtype=float32), 'train/wer': 0.04524203680836034, 'validation/ctc_loss': Array(0.36934325, dtype=float32), 'validation/wer': 0.10898172374175734, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2012541, dtype=float32), 'test/wer': 0.06818597282310646, 'test/num_examples': 2472, 'score': 43243.965631484985, 'total_duration': 47317.6235909462, 'accumulated_submission_time': 43243.965631484985, 'accumulated_eval_time': 4069.81272816658, 'accumulated_logging_time': 1.6211626529693604, 'global_step': 53373, 'preemption_count': 0}), (55209, {'train/ctc_loss': Array(0.11970915, dtype=float32), 'train/wer': 0.047040435353802854, 'validation/ctc_loss': Array(0.35692453, dtype=float32), 'validation/wer': 0.10478194966063895, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19781065, dtype=float32), 'test/wer': 0.06615481485995166, 'test/num_examples': 2472, 'score': 44683.96534562111, 'total_duration': 48889.67320728302, 'accumulated_submission_time': 44683.96534562111, 'accumulated_eval_time': 4201.730570077896, 'accumulated_logging_time': 1.6765682697296143, 'global_step': 55209, 'preemption_count': 0}), (57047, {'train/ctc_loss': Array(0.13444948, dtype=float32), 'train/wer': 0.04753825451953813, 'validation/ctc_loss': Array(0.3579548, dtype=float32), 'validation/wer': 0.10444403680353746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19295095, dtype=float32), 'test/wer': 0.06302683159669327, 'test/num_examples': 2472, 'score': 46123.9932820797, 'total_duration': 50460.28098845482, 'accumulated_submission_time': 46123.9932820797, 'accumulated_eval_time': 4332.178327083588, 'accumulated_logging_time': 1.7337017059326172, 'global_step': 57047, 'preemption_count': 0}), (58872, {'train/ctc_loss': Array(0.08747087, dtype=float32), 'train/wer': 0.034387704817050634, 'validation/ctc_loss': Array(0.34598675, dtype=float32), 'validation/wer': 0.09919190553887446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18680844, dtype=float32), 'test/wer': 0.06142221680580099, 'test/num_examples': 2472, 'score': 47564.11428070068, 'total_duration': 52032.69997930527, 'accumulated_submission_time': 47564.11428070068, 'accumulated_eval_time': 4464.339418172836, 'accumulated_logging_time': 1.7940006256103516, 'global_step': 58872, 'preemption_count': 0}), (60697, {'train/ctc_loss': Array(0.08663601, dtype=float32), 'train/wer': 0.03332674462153052, 'validation/ctc_loss': Array(0.32997072, dtype=float32), 'validation/wer': 0.09642102011064232, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17877539, dtype=float32), 'test/wer': 0.06046757256311824, 'test/num_examples': 2472, 'score': 49004.76940536499, 'total_duration': 53605.760281562805, 'accumulated_submission_time': 49004.76940536499, 'accumulated_eval_time': 4596.614463329315, 'accumulated_logging_time': 1.8496484756469727, 'global_step': 60697, 'preemption_count': 0}), (62523, {'train/ctc_loss': Array(0.10608669, dtype=float32), 'train/wer': 0.04166254111796869, 'validation/ctc_loss': Array(0.32664424, dtype=float32), 'validation/wer': 0.09537831758015776, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17875016, dtype=float32), 'test/wer': 0.0592691893648569, 'test/num_examples': 2472, 'score': 50444.94024038315, 'total_duration': 55175.63874554634, 'accumulated_submission_time': 50444.94024038315, 'accumulated_eval_time': 4726.184319496155, 'accumulated_logging_time': 1.9108588695526123, 'global_step': 62523, 'preemption_count': 0}), (64357, {'train/ctc_loss': Array(0.10365024, dtype=float32), 'train/wer': 0.039632183402028384, 'validation/ctc_loss': Array(0.31951347, dtype=float32), 'validation/wer': 0.09191229713160258, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17223309, dtype=float32), 'test/wer': 0.056994292446123536, 'test/num_examples': 2472, 'score': 51885.154266119, 'total_duration': 56746.015449762344, 'accumulated_submission_time': 51885.154266119, 'accumulated_eval_time': 4856.205128669739, 'accumulated_logging_time': 1.9761414527893066, 'global_step': 64357, 'preemption_count': 0}), (66182, {'train/ctc_loss': Array(0.11834669, dtype=float32), 'train/wer': 0.04559072856910607, 'validation/ctc_loss': Array(0.3103893, dtype=float32), 'validation/wer': 0.0891317570503104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16681975, dtype=float32), 'test/wer': 0.054739707107021716, 'test/num_examples': 2472, 'score': 53325.569303274155, 'total_duration': 58314.800549030304, 'accumulated_submission_time': 53325.569303274155, 'accumulated_eval_time': 4984.438056707382, 'accumulated_logging_time': 2.034407615661621, 'global_step': 66182, 'preemption_count': 0}), (67997, {'train/ctc_loss': Array(0.09386895, dtype=float32), 'train/wer': 0.03463274325935973, 'validation/ctc_loss': Array(0.30321878, dtype=float32), 'validation/wer': 0.08691118684650068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16329446, dtype=float32), 'test/wer': 0.05331789653281336, 'test/num_examples': 2472, 'score': 54765.563039541245, 'total_duration': 59884.31038093567, 'accumulated_submission_time': 54765.563039541245, 'accumulated_eval_time': 5113.788911581039, 'accumulated_logging_time': 2.122683525085449, 'global_step': 67997, 'preemption_count': 0}), (69819, {'train/ctc_loss': Array(0.09016636, dtype=float32), 'train/wer': 0.035446594101522955, 'validation/ctc_loss': Array(0.30152643, dtype=float32), 'validation/wer': 0.08541471562219412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1599037, dtype=float32), 'test/wer': 0.05252574492718299, 'test/num_examples': 2472, 'score': 56205.50395298004, 'total_duration': 61454.21480035782, 'accumulated_submission_time': 56205.50395298004, 'accumulated_eval_time': 5243.618288755417, 'accumulated_logging_time': 2.180286169052124, 'global_step': 69819, 'preemption_count': 0}), (71644, {'train/ctc_loss': Array(0.0650331, dtype=float32), 'train/wer': 0.024735723700137182, 'validation/ctc_loss': Array(0.29494342, dtype=float32), 'validation/wer': 0.08378307925504697, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15629764, dtype=float32), 'test/wer': 0.05081957223813296, 'test/num_examples': 2472, 'score': 57645.81762099266, 'total_duration': 63027.75369310379, 'accumulated_submission_time': 57645.81762099266, 'accumulated_eval_time': 5376.695669412613, 'accumulated_logging_time': 2.2511696815490723, 'global_step': 71644, 'preemption_count': 0}), (73472, {'train/ctc_loss': Array(0.07749713, dtype=float32), 'train/wer': 0.030339686258196587, 'validation/ctc_loss': Array(0.29196256, dtype=float32), 'validation/wer': 0.08276934068374253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1558389, dtype=float32), 'test/wer': 0.049864927995450205, 'test/num_examples': 2472, 'score': 59086.080971241, 'total_duration': 64598.00075960159, 'accumulated_submission_time': 59086.080971241, 'accumulated_eval_time': 5506.537875413895, 'accumulated_logging_time': 2.314791440963745, 'global_step': 73472, 'preemption_count': 0}), (75286, {'train/ctc_loss': Array(0.06773014, dtype=float32), 'train/wer': 0.024790915163660655, 'validation/ctc_loss': Array(0.289029, dtype=float32), 'validation/wer': 0.08147561717369686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15373042, dtype=float32), 'test/wer': 0.04909308796945138, 'test/num_examples': 2472, 'score': 60526.34147930145, 'total_duration': 66170.01026082039, 'accumulated_submission_time': 60526.34147930145, 'accumulated_eval_time': 5638.150643587112, 'accumulated_logging_time': 2.375697612762451, 'global_step': 75286, 'preemption_count': 0})], 'global_step': 75992}
I0216 10:32:05.453561 140399019657024 submission_runner.py:586] Timing: 61068.64175653458
I0216 10:32:05.453619 140399019657024 submission_runner.py:588] Total number of evals: 43
I0216 10:32:05.453661 140399019657024 submission_runner.py:589] ====================
I0216 10:32:05.453706 140399019657024 submission_runner.py:542] Using RNG seed 2554204701
I0216 10:32:05.455734 140399019657024 submission_runner.py:551] --- Tuning run 4/5 ---
I0216 10:32:05.455869 140399019657024 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4.
I0216 10:32:05.457351 140399019657024 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4/hparams.json.
I0216 10:32:05.459267 140399019657024 submission_runner.py:206] Initializing dataset.
I0216 10:32:05.459386 140399019657024 submission_runner.py:213] Initializing model.
I0216 10:32:09.131162 140399019657024 submission_runner.py:255] Initializing optimizer.
I0216 10:32:09.878471 140399019657024 submission_runner.py:262] Initializing metrics bundle.
I0216 10:32:09.878697 140399019657024 submission_runner.py:280] Initializing checkpoint and logger.
I0216 10:32:09.884410 140399019657024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4 with prefix checkpoint_
I0216 10:32:09.884543 140399019657024 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4/meta_data_0.json.
I0216 10:32:09.884736 140399019657024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 10:32:09.884798 140399019657024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 10:32:10.519927 140399019657024 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 10:32:11.115166 140399019657024 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4/flags_0.json.
I0216 10:32:11.134893 140399019657024 submission_runner.py:314] Starting training loop.
I0216 10:32:11.138064 140399019657024 input_pipeline.py:20] Loading split = train-clean-100
I0216 10:32:11.182966 140399019657024 input_pipeline.py:20] Loading split = train-clean-360
I0216 10:32:11.315563 140399019657024 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 10:32:46.065094 140228621711104 logging_writer.py:48] [0] global_step=0, grad_norm=47.990047454833984, loss=31.799711227416992
I0216 10:32:46.086074 140399019657024 spec.py:321] Evaluating on the training split.
I0216 10:33:44.591713 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 10:34:35.508595 140399019657024 spec.py:349] Evaluating on the test split.
I0216 10:35:01.525639 140399019657024 submission_runner.py:408] Time since start: 170.39s, 	Step: 1, 	{'train/ctc_loss': Array(31.906748, dtype=float32), 'train/wer': 1.3798149413691736, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043146644525329, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.27949, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 34.95112609863281, 'total_duration': 170.38803958892822, 'accumulated_submission_time': 34.95112609863281, 'accumulated_eval_time': 135.43686819076538, 'accumulated_logging_time': 0}
I0216 10:35:01.543952 140318077859584 logging_writer.py:48] [1] accumulated_eval_time=135.436868, accumulated_logging_time=0, accumulated_submission_time=34.951126, global_step=1, preemption_count=0, score=34.951126, test/ctc_loss=31.279489517211914, test/num_examples=2472, test/wer=1.097699, total_duration=170.388040, train/ctc_loss=31.906747817993164, train/wer=1.379815, validation/ctc_loss=31.163591384887695, validation/num_examples=5348, validation/wer=1.043147
I0216 10:36:43.692665 140228621711104 logging_writer.py:48] [100] global_step=100, grad_norm=1.7971957921981812, loss=5.828058242797852
I0216 10:38:00.497709 140228630103808 logging_writer.py:48] [200] global_step=200, grad_norm=3.557501792907715, loss=5.7912163734436035
I0216 10:39:17.638977 140228621711104 logging_writer.py:48] [300] global_step=300, grad_norm=2.050166368484497, loss=5.61349630355835
I0216 10:40:34.462328 140228630103808 logging_writer.py:48] [400] global_step=400, grad_norm=1.855629324913025, loss=5.602569103240967
I0216 10:41:51.223526 140228621711104 logging_writer.py:48] [500] global_step=500, grad_norm=0.48225632309913635, loss=5.530081748962402
I0216 10:43:07.836910 140228630103808 logging_writer.py:48] [600] global_step=600, grad_norm=2.836517333984375, loss=5.533095359802246
I0216 10:44:24.640402 140228621711104 logging_writer.py:48] [700] global_step=700, grad_norm=3.3659467697143555, loss=5.548333168029785
I0216 10:45:46.149855 140228630103808 logging_writer.py:48] [800] global_step=800, grad_norm=2.476891279220581, loss=5.530943870544434
I0216 10:47:10.491261 140228621711104 logging_writer.py:48] [900] global_step=900, grad_norm=0.40034061670303345, loss=5.517429828643799
I0216 10:48:35.079363 140228630103808 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.9653763175010681, loss=5.488396644592285
I0216 10:49:56.669865 140318077859584 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.9369191527366638, loss=5.510887145996094
I0216 10:51:13.559696 140318069466880 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.23325252532959, loss=5.49557638168335
I0216 10:52:30.268397 140318077859584 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.6158816814422607, loss=5.524514198303223
I0216 10:53:46.898820 140318069466880 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.1830166578292847, loss=5.516254901885986
I0216 10:55:03.554335 140318077859584 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.149776577949524, loss=5.505721569061279
I0216 10:56:23.182090 140318069466880 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4102194309234619, loss=5.491453647613525
I0216 10:57:47.871373 140318077859584 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.32719746232032776, loss=5.484055519104004
I0216 10:59:02.045153 140399019657024 spec.py:321] Evaluating on the training split.
I0216 10:59:40.094548 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 11:00:25.471142 140399019657024 spec.py:349] Evaluating on the test split.
I0216 11:00:49.364450 140399019657024 submission_runner.py:408] Time since start: 1718.22s, 	Step: 1789, 	{'train/ctc_loss': Array(6.4432144, dtype=float32), 'train/wer': 0.9413900245298447, 'validation/ctc_loss': Array(6.3511953, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.313715, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.3679308891296, 'total_duration': 1718.2228519916534, 'accumulated_submission_time': 1475.3679308891296, 'accumulated_eval_time': 242.74953532218933, 'accumulated_logging_time': 0.031334638595581055}
I0216 11:00:49.400415 140318077859584 logging_writer.py:48] [1789] accumulated_eval_time=242.749535, accumulated_logging_time=0.031335, accumulated_submission_time=1475.367931, global_step=1789, preemption_count=0, score=1475.367931, test/ctc_loss=6.313714981079102, test/num_examples=2472, test/wer=0.899580, total_duration=1718.222852, train/ctc_loss=6.443214416503906, train/wer=0.941390, validation/ctc_loss=6.351195335388184, validation/num_examples=5348, validation/wer=0.896618
I0216 11:00:58.635480 140318069466880 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.404207944869995, loss=5.479971885681152
I0216 11:02:15.100159 140318077859584 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.777377963066101, loss=5.474774360656738
I0216 11:03:31.736958 140318069466880 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.4483275413513184, loss=5.46088171005249
I0216 11:04:52.622270 140318077859584 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0409414768218994, loss=5.429305553436279
I0216 11:06:09.088310 140318069466880 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.1780447959899902, loss=5.454649925231934
I0216 11:07:25.627631 140318077859584 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1742948293685913, loss=5.425667762756348
I0216 11:08:41.969148 140318069466880 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.27814239263534546, loss=5.399230003356934
I0216 11:09:58.475502 140318077859584 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6527988910675049, loss=5.366570472717285
I0216 11:11:19.008586 140318069466880 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9782567620277405, loss=5.0698394775390625
I0216 11:12:43.572915 140318077859584 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.946795105934143, loss=4.7017340660095215
I0216 11:14:08.999000 140318069466880 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.7856717109680176, loss=4.405939102172852
I0216 11:15:33.317134 140318077859584 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0435396432876587, loss=4.255269527435303
I0216 11:16:57.428587 140318069466880 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7672303318977356, loss=4.175038814544678
I0216 11:18:24.242961 140318077859584 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.622727870941162, loss=4.078666687011719
I0216 11:19:40.695213 140318069466880 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2683684825897217, loss=3.914809465408325
I0216 11:20:57.377235 140318077859584 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1427569389343262, loss=3.796116590499878
I0216 11:22:14.118202 140318069466880 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7252543568611145, loss=3.6290323734283447
I0216 11:23:30.622467 140318077859584 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8351207971572876, loss=3.591513156890869
I0216 11:24:47.105368 140318069466880 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.37105393409729, loss=3.5333094596862793
I0216 11:24:49.885341 140399019657024 spec.py:321] Evaluating on the training split.
I0216 11:25:28.022705 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 11:26:14.033275 140399019657024 spec.py:349] Evaluating on the test split.
I0216 11:26:37.621797 140399019657024 submission_runner.py:408] Time since start: 3266.48s, 	Step: 3605, 	{'train/ctc_loss': Array(6.894968, dtype=float32), 'train/wer': 0.9380587920410929, 'validation/ctc_loss': Array(6.6507244, dtype=float32), 'validation/wer': 0.8960097318902845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4707236, dtype=float32), 'test/wer': 0.898990514492312, 'test/num_examples': 2472, 'score': 2915.763386487961, 'total_duration': 3266.4796857833862, 'accumulated_submission_time': 2915.763386487961, 'accumulated_eval_time': 350.4788267612457, 'accumulated_logging_time': 0.08646297454833984}
I0216 11:26:37.663087 140318077859584 logging_writer.py:48] [3605] accumulated_eval_time=350.478827, accumulated_logging_time=0.086463, accumulated_submission_time=2915.763386, global_step=3605, preemption_count=0, score=2915.763386, test/ctc_loss=6.470723628997803, test/num_examples=2472, test/wer=0.898991, total_duration=3266.479686, train/ctc_loss=6.894968032836914, train/wer=0.938059, validation/ctc_loss=6.650724411010742, validation/num_examples=5348, validation/wer=0.896010
I0216 11:27:50.760620 140318069466880 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.089275598526001, loss=3.4843316078186035
I0216 11:29:07.170253 140318077859584 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0130615234375, loss=3.4728634357452393
I0216 11:30:23.458120 140318069466880 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.4717267751693726, loss=3.423153877258301
I0216 11:31:39.838599 140318077859584 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7802446484565735, loss=3.4120113849639893
I0216 11:33:03.902493 140318069466880 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.7581377029418945, loss=3.3860929012298584
I0216 11:34:24.651559 140318077859584 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.9783083200454712, loss=3.263258695602417
I0216 11:35:40.973504 140318069466880 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1166883707046509, loss=3.1929924488067627
I0216 11:36:57.351822 140318077859584 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.1559302806854248, loss=3.1606369018554688
I0216 11:38:13.992100 140318069466880 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.8635404109954834, loss=3.097039222717285
I0216 11:39:30.290698 140318077859584 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.3862478733062744, loss=3.114089250564575
I0216 11:40:50.692776 140318069466880 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5786532759666443, loss=2.972456932067871
I0216 11:42:14.739077 140318077859584 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.100679874420166, loss=3.0100913047790527
I0216 11:43:38.280717 140318069466880 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.7594900131225586, loss=3.0199155807495117
I0216 11:45:02.332834 140318077859584 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.730225563049316, loss=2.99676775932312
I0216 11:46:26.409304 140318069466880 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.0375510454177856, loss=2.944042682647705
I0216 11:47:49.921311 140318077859584 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.9334303140640259, loss=2.9384078979492188
I0216 11:49:06.337171 140318069466880 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6467679142951965, loss=2.925690174102783
I0216 11:50:22.642464 140318077859584 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.3192192316055298, loss=2.9303436279296875
I0216 11:50:37.623209 140399019657024 spec.py:321] Evaluating on the training split.
I0216 11:51:23.437072 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 11:52:12.692986 140399019657024 spec.py:349] Evaluating on the test split.
I0216 11:52:37.997598 140399019657024 submission_runner.py:408] Time since start: 4826.86s, 	Step: 5421, 	{'train/ctc_loss': Array(3.7265964, dtype=float32), 'train/wer': 0.7729606676212637, 'validation/ctc_loss': Array(3.6029704, dtype=float32), 'validation/wer': 0.7371327611342287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.293352, dtype=float32), 'test/wer': 0.7108037292060203, 'test/num_examples': 2472, 'score': 4355.635868310928, 'total_duration': 4826.856513261795, 'accumulated_submission_time': 4355.635868310928, 'accumulated_eval_time': 470.84709548950195, 'accumulated_logging_time': 0.14482760429382324}
I0216 11:52:38.036192 140318077859584 logging_writer.py:48] [5421] accumulated_eval_time=470.847095, accumulated_logging_time=0.144828, accumulated_submission_time=4355.635868, global_step=5421, preemption_count=0, score=4355.635868, test/ctc_loss=3.293351888656616, test/num_examples=2472, test/wer=0.710804, total_duration=4826.856513, train/ctc_loss=3.7265963554382324, train/wer=0.772961, validation/ctc_loss=3.6029703617095947, validation/num_examples=5348, validation/wer=0.737133
I0216 11:53:38.903142 140318069466880 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.2428308725357056, loss=2.867487907409668
I0216 11:54:55.415395 140318077859584 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.2650973796844482, loss=2.8395705223083496
I0216 11:56:11.793647 140318069466880 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.3715519905090332, loss=2.8384737968444824
I0216 11:57:28.092210 140318077859584 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5263240933418274, loss=2.8035383224487305
I0216 11:58:45.821576 140318069466880 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9648697376251221, loss=2.791638135910034
I0216 12:00:09.283277 140318077859584 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0487254858016968, loss=2.7889628410339355
I0216 12:01:32.914331 140318069466880 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7466350197792053, loss=2.7478420734405518
I0216 12:02:58.995212 140318077859584 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.99365234375, loss=2.6937663555145264
I0216 12:04:15.317546 140318069466880 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9917470216751099, loss=2.759340524673462
I0216 12:05:31.706512 140318077859584 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.2460472583770752, loss=2.6997413635253906
I0216 12:06:48.124005 140318069466880 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7948673367500305, loss=2.5759360790252686
I0216 12:08:04.488287 140318077859584 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8552670478820801, loss=2.625788450241089
I0216 12:09:20.819065 140318069466880 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8481550216674805, loss=2.6705141067504883
I0216 12:10:42.901057 140318077859584 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7498916983604431, loss=2.664234161376953
I0216 12:12:06.236387 140318069466880 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7069638967514038, loss=2.6557772159576416
I0216 12:13:29.301309 140318077859584 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6761559247970581, loss=2.564612627029419
I0216 12:14:53.497117 140318069466880 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6837539672851562, loss=2.6615493297576904
I0216 12:16:16.983898 140318077859584 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.5054329633712769, loss=2.600264072418213
I0216 12:16:38.194562 140399019657024 spec.py:321] Evaluating on the training split.
I0216 12:17:31.440892 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 12:18:22.624270 140399019657024 spec.py:349] Evaluating on the test split.
I0216 12:18:48.989193 140399019657024 submission_runner.py:408] Time since start: 6397.85s, 	Step: 7224, 	{'train/ctc_loss': Array(1.7894272, dtype=float32), 'train/wer': 0.50639147729467, 'validation/ctc_loss': Array(1.8419197, dtype=float32), 'validation/wer': 0.48790754704229705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.4695274, dtype=float32), 'test/wer': 0.4360693031097028, 'test/num_examples': 2472, 'score': 5795.708523511887, 'total_duration': 6397.847925901413, 'accumulated_submission_time': 5795.708523511887, 'accumulated_eval_time': 601.6357429027557, 'accumulated_logging_time': 0.1990370750427246}
I0216 12:18:49.024421 140318077859584 logging_writer.py:48] [7224] accumulated_eval_time=601.635743, accumulated_logging_time=0.199037, accumulated_submission_time=5795.708524, global_step=7224, preemption_count=0, score=5795.708524, test/ctc_loss=1.4695273637771606, test/num_examples=2472, test/wer=0.436069, total_duration=6397.847926, train/ctc_loss=1.7894271612167358, train/wer=0.506391, validation/ctc_loss=1.8419196605682373, validation/num_examples=5348, validation/wer=0.487908
I0216 12:19:47.506119 140318069466880 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.5594072341918945, loss=2.5769131183624268
I0216 12:21:03.936098 140318077859584 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5272040963172913, loss=2.5492167472839355
I0216 12:22:20.329583 140318069466880 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.2343406677246094, loss=2.502817153930664
I0216 12:23:36.660286 140318077859584 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9603368043899536, loss=2.5034613609313965
I0216 12:24:52.935918 140318069466880 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8676791191101074, loss=2.510706901550293
I0216 12:26:09.247534 140318077859584 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.2041007280349731, loss=2.5085630416870117
I0216 12:27:25.798788 140318069466880 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.2141669988632202, loss=2.4780659675598145
I0216 12:28:49.418668 140318077859584 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7987601161003113, loss=2.5561089515686035
I0216 12:30:13.731751 140318069466880 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7761017084121704, loss=2.525735378265381
I0216 12:31:37.447486 140318077859584 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.7079665660858154, loss=2.5579402446746826
I0216 12:32:59.957547 140318077859584 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6559898257255554, loss=2.471656084060669
I0216 12:34:16.691042 140318069466880 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.3325380086898804, loss=2.6472854614257812
I0216 12:35:33.341124 140318077859584 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8548782467842102, loss=2.48298716545105
I0216 12:36:49.884285 140318069466880 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.6723774671554565, loss=2.4683268070220947
I0216 12:38:06.475542 140318077859584 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.436277985572815, loss=2.5589792728424072
I0216 12:39:24.959969 140318069466880 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.2569650411605835, loss=2.502485990524292
I0216 12:40:48.081119 140318077859584 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6629098653793335, loss=2.411445140838623
I0216 12:42:12.278326 140318069466880 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.681471824645996, loss=2.4380178451538086
I0216 12:42:49.782234 140399019657024 spec.py:321] Evaluating on the training split.
I0216 12:43:42.306144 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 12:44:33.518415 140399019657024 spec.py:349] Evaluating on the test split.
I0216 12:44:59.697587 140399019657024 submission_runner.py:408] Time since start: 7968.56s, 	Step: 9046, 	{'train/ctc_loss': Array(1.5100107, dtype=float32), 'train/wer': 0.4490485031329775, 'validation/ctc_loss': Array(1.4825138, dtype=float32), 'validation/wer': 0.4142908174594746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1427629, dtype=float32), 'test/wer': 0.35490423090203727, 'test/num_examples': 2472, 'score': 7236.371410369873, 'total_duration': 7968.555783987045, 'accumulated_submission_time': 7236.371410369873, 'accumulated_eval_time': 731.5442821979523, 'accumulated_logging_time': 0.2577028274536133}
I0216 12:44:59.734943 140318077859584 logging_writer.py:48] [9046] accumulated_eval_time=731.544282, accumulated_logging_time=0.257703, accumulated_submission_time=7236.371410, global_step=9046, preemption_count=0, score=7236.371410, test/ctc_loss=1.1427628993988037, test/num_examples=2472, test/wer=0.354904, total_duration=7968.555784, train/ctc_loss=1.5100107192993164, train/wer=0.449049, validation/ctc_loss=1.4825137853622437, validation/num_examples=5348, validation/wer=0.414291
I0216 12:45:41.626870 140318069466880 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6061814427375793, loss=2.3846275806427
I0216 12:46:58.180064 140318077859584 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.2492961883544922, loss=2.377823829650879
I0216 12:48:18.043410 140318077859584 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.136347770690918, loss=2.42276930809021
I0216 12:49:34.676413 140318069466880 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7609217166900635, loss=2.3212804794311523
I0216 12:50:51.299150 140318077859584 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5602255463600159, loss=2.371729612350464
I0216 12:52:07.806867 140318069466880 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8516286015510559, loss=2.354182243347168
I0216 12:53:24.344942 140318077859584 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6461583971977234, loss=2.420304536819458
I0216 12:54:43.024135 140318069466880 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.0426652431488037, loss=2.461765766143799
I0216 12:56:07.008092 140318077859584 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8433135151863098, loss=2.291698932647705
I0216 12:57:30.352020 140318069466880 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.2334012985229492, loss=2.3091259002685547
I0216 12:58:53.047452 140318077859584 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.1733875274658203, loss=2.3042538166046143
I0216 13:00:16.961240 140318069466880 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6207188367843628, loss=2.3286986351013184
I0216 13:01:44.328178 140318077859584 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6146501302719116, loss=2.292606830596924
I0216 13:03:00.719911 140318069466880 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7698262333869934, loss=2.250614643096924
I0216 13:04:17.175431 140318077859584 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.811875581741333, loss=2.3212082386016846
I0216 13:05:33.690982 140318069466880 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.593155026435852, loss=2.2402613162994385
I0216 13:06:50.262465 140318077859584 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7343029975891113, loss=2.2720305919647217
I0216 13:08:07.592851 140318069466880 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6103416681289673, loss=2.224912166595459
I0216 13:08:59.954870 140399019657024 spec.py:321] Evaluating on the training split.
I0216 13:09:53.798114 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 13:10:44.777018 140399019657024 spec.py:349] Evaluating on the test split.
I0216 13:11:11.545439 140399019657024 submission_runner.py:408] Time since start: 9540.40s, 	Step: 10864, 	{'train/ctc_loss': Array(1.2075412, dtype=float32), 'train/wer': 0.37236854396887586, 'validation/ctc_loss': Array(1.3426595, dtype=float32), 'validation/wer': 0.3834924741979397, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.98521, dtype=float32), 'test/wer': 0.31511384640383483, 'test/num_examples': 2472, 'score': 8676.504511356354, 'total_duration': 9540.403466939926, 'accumulated_submission_time': 8676.504511356354, 'accumulated_eval_time': 863.1278550624847, 'accumulated_logging_time': 0.31041407585144043}
I0216 13:11:11.580911 140318077859584 logging_writer.py:48] [10864] accumulated_eval_time=863.127855, accumulated_logging_time=0.310414, accumulated_submission_time=8676.504511, global_step=10864, preemption_count=0, score=8676.504511, test/ctc_loss=0.9852100014686584, test/num_examples=2472, test/wer=0.315114, total_duration=9540.403467, train/ctc_loss=1.2075412273406982, train/wer=0.372369, validation/ctc_loss=1.3426594734191895, validation/num_examples=5348, validation/wer=0.383492
I0216 13:11:39.906229 140318069466880 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6564507484436035, loss=2.300323724746704
I0216 13:12:56.253978 140318077859584 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8413944244384766, loss=2.256880283355713
I0216 13:14:12.782682 140318069466880 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.6674991846084595, loss=2.221614360809326
I0216 13:15:29.291563 140318077859584 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.7911816239356995, loss=2.192770004272461
I0216 13:16:46.141991 140318069466880 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9029830098152161, loss=2.170701265335083
I0216 13:18:07.487845 140318077859584 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.8363430500030518, loss=2.1449573040008545
I0216 13:19:24.175300 140318069466880 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.1297796964645386, loss=2.1409027576446533
I0216 13:20:40.499981 140318077859584 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.096916913986206, loss=2.184453010559082
I0216 13:21:56.903236 140318069466880 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8248646855354309, loss=2.236151695251465
I0216 13:23:13.263910 140318077859584 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.2916110754013062, loss=2.1706905364990234
I0216 13:24:30.849264 140318069466880 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8454276919364929, loss=2.217222213745117
I0216 13:25:54.918264 140318077859584 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7169333100318909, loss=2.225898504257202
I0216 13:27:18.388555 140318069466880 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0116252899169922, loss=2.1818275451660156
I0216 13:28:42.144810 140318077859584 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7673553824424744, loss=2.1331405639648438
I0216 13:30:05.222562 140318069466880 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0718803405761719, loss=2.1683847904205322
I0216 13:31:29.964516 140318077859584 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0826213359832764, loss=2.1542611122131348
I0216 13:32:46.369603 140318069466880 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8272910118103027, loss=2.1897146701812744
I0216 13:34:02.882807 140318077859584 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7139835357666016, loss=2.2111587524414062
I0216 13:35:11.598556 140399019657024 spec.py:321] Evaluating on the training split.
I0216 13:36:04.409634 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 13:36:55.827011 140399019657024 spec.py:349] Evaluating on the test split.
I0216 13:37:21.983213 140399019657024 submission_runner.py:408] Time since start: 11110.84s, 	Step: 12691, 	{'train/ctc_loss': Array(1.1640844, dtype=float32), 'train/wer': 0.35710851551231654, 'validation/ctc_loss': Array(1.218482, dtype=float32), 'validation/wer': 0.352742404201705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8880885, dtype=float32), 'test/wer': 0.2892775171125058, 'test/num_examples': 2472, 'score': 10116.434507131577, 'total_duration': 11110.8425116539, 'accumulated_submission_time': 10116.434507131577, 'accumulated_eval_time': 993.5067636966705, 'accumulated_logging_time': 0.3614308834075928}
I0216 13:37:22.018782 140318077859584 logging_writer.py:48] [12691] accumulated_eval_time=993.506764, accumulated_logging_time=0.361431, accumulated_submission_time=10116.434507, global_step=12691, preemption_count=0, score=10116.434507, test/ctc_loss=0.8880885243415833, test/num_examples=2472, test/wer=0.289278, total_duration=11110.842512, train/ctc_loss=1.1640844345092773, train/wer=0.357109, validation/ctc_loss=1.2184820175170898, validation/num_examples=5348, validation/wer=0.352742
I0216 13:37:29.727215 140318069466880 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7909093499183655, loss=2.1975507736206055
I0216 13:38:46.138717 140318077859584 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0602902173995972, loss=2.158266067504883
I0216 13:40:02.625988 140318069466880 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7473101019859314, loss=2.1202714443206787
I0216 13:41:19.232886 140318077859584 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.1155918836593628, loss=2.1485788822174072
I0216 13:42:35.822024 140318069466880 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0072606801986694, loss=2.0738320350646973
I0216 13:43:57.145116 140318077859584 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.8765395879745483, loss=2.112366199493408
I0216 13:45:19.927992 140318069466880 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9533262252807617, loss=2.133063793182373
I0216 13:46:45.291438 140318077859584 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5483020544052124, loss=2.1158716678619385
I0216 13:48:01.598456 140318069466880 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0713584423065186, loss=2.1132965087890625
I0216 13:49:17.886614 140318077859584 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8979986310005188, loss=2.1087911128997803
I0216 13:50:34.250259 140318069466880 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.351258397102356, loss=2.1093640327453613
I0216 13:51:50.591596 140318077859584 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6431105136871338, loss=2.074889898300171
I0216 13:53:07.268718 140318069466880 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8418500423431396, loss=2.103564739227295
I0216 13:54:29.423177 140318077859584 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7113473415374756, loss=2.0337986946105957
I0216 13:55:52.502453 140318069466880 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.69558185338974, loss=2.106762409210205
I0216 13:57:15.258169 140318077859584 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.8613766431808472, loss=2.1183459758758545
I0216 13:58:38.549120 140318069466880 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.1205092668533325, loss=2.0986757278442383
I0216 14:00:01.957349 140318077859584 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7717962265014648, loss=2.029049873352051
I0216 14:01:22.096168 140399019657024 spec.py:321] Evaluating on the training split.
I0216 14:02:14.796337 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 14:03:06.593711 140399019657024 spec.py:349] Evaluating on the test split.
I0216 14:03:32.863476 140399019657024 submission_runner.py:408] Time since start: 12681.72s, 	Step: 14500, 	{'train/ctc_loss': Array(1.1236786, dtype=float32), 'train/wer': 0.3462997832604231, 'validation/ctc_loss': Array(1.16027, dtype=float32), 'validation/wer': 0.33419581567336376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8417588, dtype=float32), 'test/wer': 0.272825137610952, 'test/num_examples': 2472, 'score': 11556.422718286514, 'total_duration': 12681.722338914871, 'accumulated_submission_time': 11556.422718286514, 'accumulated_eval_time': 1124.2681069374084, 'accumulated_logging_time': 0.4147167205810547}
I0216 14:03:32.896992 140318077859584 logging_writer.py:48] [14500] accumulated_eval_time=1124.268107, accumulated_logging_time=0.414717, accumulated_submission_time=11556.422718, global_step=14500, preemption_count=0, score=11556.422718, test/ctc_loss=0.8417587876319885, test/num_examples=2472, test/wer=0.272825, total_duration=12681.722339, train/ctc_loss=1.1236785650253296, train/wer=0.346300, validation/ctc_loss=1.1602699756622314, validation/num_examples=5348, validation/wer=0.334196
I0216 14:03:33.802242 140318069466880 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9914752244949341, loss=2.0371198654174805
I0216 14:04:50.059010 140318077859584 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.6203263998031616, loss=2.029621124267578
I0216 14:06:06.387332 140318069466880 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9877211451530457, loss=2.0536575317382812
I0216 14:07:22.934815 140318077859584 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.4602633714675903, loss=2.051616668701172
I0216 14:08:39.636634 140318069466880 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.2248774766921997, loss=2.058241367340088
I0216 14:09:56.380257 140318077859584 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6351203918457031, loss=2.0846333503723145
I0216 14:11:12.908617 140318069466880 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6058076024055481, loss=2.094991683959961
I0216 14:12:32.337304 140318077859584 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8544021844863892, loss=2.0758612155914307
I0216 14:13:55.871926 140318069466880 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8896887302398682, loss=2.0393598079681396
I0216 14:15:20.125030 140318077859584 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6770583987236023, loss=2.035060405731201
I0216 14:16:42.847171 140318077859584 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.1043285131454468, loss=2.0521092414855957
I0216 14:17:59.474962 140318069466880 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7367693781852722, loss=2.0202243328094482
I0216 14:19:15.982810 140318077859584 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6718745827674866, loss=2.083627700805664
I0216 14:20:32.328332 140318069466880 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9226022362709045, loss=2.0405325889587402
I0216 14:21:48.934070 140318077859584 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.9085419774055481, loss=2.04884672164917
I0216 14:23:05.473478 140318069466880 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6743316054344177, loss=2.054577112197876
I0216 14:24:28.634035 140318077859584 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7976261973381042, loss=2.0593907833099365
I0216 14:25:51.876021 140318069466880 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7525418400764465, loss=2.0585622787475586
I0216 14:27:15.226012 140318077859584 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8897834420204163, loss=2.025441884994507
I0216 14:27:33.130934 140399019657024 spec.py:321] Evaluating on the training split.
I0216 14:28:27.419517 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 14:29:19.482831 140399019657024 spec.py:349] Evaluating on the test split.
I0216 14:29:45.696568 140399019657024 submission_runner.py:408] Time since start: 14254.56s, 	Step: 16323, 	{'train/ctc_loss': Array(1.0246935, dtype=float32), 'train/wer': 0.3256618002726353, 'validation/ctc_loss': Array(1.0873188, dtype=float32), 'validation/wer': 0.32038966179750333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7645127, dtype=float32), 'test/wer': 0.2534682022220868, 'test/num_examples': 2472, 'score': 12996.567366361618, 'total_duration': 14254.555345058441, 'accumulated_submission_time': 12996.567366361618, 'accumulated_eval_time': 1256.8274652957916, 'accumulated_logging_time': 0.46446824073791504}
I0216 14:29:45.730452 140318077859584 logging_writer.py:48] [16323] accumulated_eval_time=1256.827465, accumulated_logging_time=0.464468, accumulated_submission_time=12996.567366, global_step=16323, preemption_count=0, score=12996.567366, test/ctc_loss=0.7645127177238464, test/num_examples=2472, test/wer=0.253468, total_duration=14254.555345, train/ctc_loss=1.024693489074707, train/wer=0.325662, validation/ctc_loss=1.087318778038025, validation/num_examples=5348, validation/wer=0.320390
I0216 14:30:45.247111 140318069466880 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9404994249343872, loss=2.081594705581665
I0216 14:32:05.335770 140318077859584 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7645379900932312, loss=1.9779052734375
I0216 14:33:21.607022 140318069466880 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5018900036811829, loss=2.0012762546539307
I0216 14:34:37.997582 140318077859584 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.550514280796051, loss=2.0334835052490234
I0216 14:35:54.414394 140318069466880 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6894729137420654, loss=2.0136990547180176
I0216 14:37:10.945531 140318077859584 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.6339304447174072, loss=2.0403459072113037
I0216 14:38:27.431780 140318069466880 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6733617782592773, loss=2.0103373527526855
I0216 14:39:51.361383 140318077859584 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.1454460620880127, loss=2.039584159851074
I0216 14:41:14.739509 140318069466880 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9090185761451721, loss=2.083440065383911
I0216 14:42:38.117291 140318077859584 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6206532716751099, loss=1.9406394958496094
I0216 14:44:01.513426 140318069466880 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7196873426437378, loss=1.9753788709640503
I0216 14:45:24.855999 140318077859584 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7842304110527039, loss=1.9854662418365479
I0216 14:46:45.182245 140318077859584 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6388236284255981, loss=1.9689812660217285
I0216 14:48:01.827879 140318069466880 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9758303165435791, loss=2.0813958644866943
I0216 14:49:18.429038 140318077859584 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.983497142791748, loss=1.9924288988113403
I0216 14:50:35.022531 140318069466880 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6179916858673096, loss=2.050166606903076
I0216 14:51:51.469312 140318077859584 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1710331439971924, loss=2.0069162845611572
I0216 14:53:10.442096 140318069466880 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.8315314650535583, loss=1.9956169128417969
I0216 14:53:46.006068 140399019657024 spec.py:321] Evaluating on the training split.
I0216 14:54:39.401525 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 14:55:30.938928 140399019657024 spec.py:349] Evaluating on the test split.
I0216 14:55:57.309136 140399019657024 submission_runner.py:408] Time since start: 15826.17s, 	Step: 18144, 	{'train/ctc_loss': Array(0.91025984, dtype=float32), 'train/wer': 0.29429087216518107, 'validation/ctc_loss': Array(1.035664, dtype=float32), 'validation/wer': 0.3064097241665621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7306505, dtype=float32), 'test/wer': 0.24229683342473543, 'test/num_examples': 2472, 'score': 14436.753766775131, 'total_duration': 15826.1676633358, 'accumulated_submission_time': 14436.753766775131, 'accumulated_eval_time': 1388.1240470409393, 'accumulated_logging_time': 0.5152781009674072}
I0216 14:55:57.348763 140318077859584 logging_writer.py:48] [18144] accumulated_eval_time=1388.124047, accumulated_logging_time=0.515278, accumulated_submission_time=14436.753767, global_step=18144, preemption_count=0, score=14436.753767, test/ctc_loss=0.7306504845619202, test/num_examples=2472, test/wer=0.242297, total_duration=15826.167663, train/ctc_loss=0.9102598428726196, train/wer=0.294291, validation/ctc_loss=1.0356639623641968, validation/num_examples=5348, validation/wer=0.306410
I0216 14:56:40.766384 140318069466880 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6347141265869141, loss=2.0183768272399902
I0216 14:57:57.056044 140318077859584 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7520990371704102, loss=2.0117831230163574
I0216 14:59:13.560739 140318069466880 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8158491849899292, loss=1.995423674583435
I0216 15:00:30.144011 140318077859584 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5737250447273254, loss=1.9589293003082275
I0216 15:01:50.195791 140318077859584 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.592482328414917, loss=1.9806840419769287
I0216 15:03:06.704986 140318069466880 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7499009370803833, loss=1.932827353477478
I0216 15:04:23.314951 140318077859584 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.966012716293335, loss=1.9712165594100952
I0216 15:05:39.905757 140318069466880 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7572579383850098, loss=1.9261690378189087
I0216 15:06:56.546908 140318077859584 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6285003423690796, loss=1.894441843032837
I0216 15:08:13.135984 140318069466880 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5241323709487915, loss=1.963072419166565
I0216 15:09:35.417745 140318077859584 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6744915843009949, loss=1.9110419750213623
I0216 15:10:59.168581 140318069466880 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5854474902153015, loss=1.9280364513397217
I0216 15:12:23.374083 140318077859584 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5945611596107483, loss=1.9529346227645874
I0216 15:13:46.788373 140318069466880 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0290675163269043, loss=2.05722713470459
I0216 15:15:10.779661 140318077859584 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.825464129447937, loss=1.8870500326156616
I0216 15:16:27.350537 140318069466880 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0919058322906494, loss=1.8978092670440674
I0216 15:17:43.796118 140318077859584 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.655825138092041, loss=1.9040465354919434
I0216 15:19:00.270465 140318069466880 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0159258842468262, loss=1.9798216819763184
I0216 15:19:57.961046 140399019657024 spec.py:321] Evaluating on the training split.
I0216 15:20:50.158092 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 15:21:41.809715 140399019657024 spec.py:349] Evaluating on the test split.
I0216 15:22:08.213673 140399019657024 submission_runner.py:408] Time since start: 17397.07s, 	Step: 19977, 	{'train/ctc_loss': Array(0.83924824, dtype=float32), 'train/wer': 0.27583577477224386, 'validation/ctc_loss': Array(0.9875356, dtype=float32), 'validation/wer': 0.2946600113924906, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68959343, dtype=float32), 'test/wer': 0.23031300144212216, 'test/num_examples': 2472, 'score': 15877.275433778763, 'total_duration': 17397.07205915451, 'accumulated_submission_time': 15877.275433778763, 'accumulated_eval_time': 1518.3700096607208, 'accumulated_logging_time': 0.5735807418823242}
I0216 15:22:08.254553 140318077859584 logging_writer.py:48] [19977] accumulated_eval_time=1518.370010, accumulated_logging_time=0.573581, accumulated_submission_time=15877.275434, global_step=19977, preemption_count=0, score=15877.275434, test/ctc_loss=0.6895934343338013, test/num_examples=2472, test/wer=0.230313, total_duration=17397.072059, train/ctc_loss=0.8392482399940491, train/wer=0.275836, validation/ctc_loss=0.9875355958938599, validation/num_examples=5348, validation/wer=0.294660
I0216 15:22:26.550875 140318069466880 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6225357055664062, loss=2.0096895694732666
I0216 15:23:43.045642 140318077859584 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.47715315222740173, loss=1.932436227798462
I0216 15:25:00.192747 140318069466880 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6505087018013, loss=1.9354768991470337
I0216 15:26:16.695986 140318077859584 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5566874146461487, loss=1.912387728691101
I0216 15:27:33.089703 140318069466880 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7519626021385193, loss=1.9406718015670776
I0216 15:28:53.128856 140318077859584 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.84285569190979, loss=1.8969727754592896
I0216 15:30:20.080486 140318077859584 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8655305504798889, loss=1.9048657417297363
I0216 15:31:36.469363 140318069466880 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5630475282669067, loss=1.8961098194122314
I0216 15:32:53.040419 140318077859584 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8028578162193298, loss=1.9170889854431152
I0216 15:34:09.883898 140318069466880 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.633895754814148, loss=1.8990494012832642
I0216 15:35:26.465409 140318077859584 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6719015836715698, loss=1.957260251045227
I0216 15:36:43.028099 140318069466880 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7041085958480835, loss=1.8779083490371704
I0216 15:38:02.182088 140318077859584 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.8877154588699341, loss=1.822176456451416
I0216 15:39:25.336761 140318069466880 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7203246355056763, loss=1.9159637689590454
I0216 15:40:47.735717 140318077859584 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.540668785572052, loss=1.8546717166900635
I0216 15:42:10.660508 140318069466880 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.663131833076477, loss=1.9719500541687012
I0216 15:43:33.294779 140318077859584 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6189072132110596, loss=1.883744716644287
I0216 15:44:54.661944 140318077859584 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7813801169395447, loss=1.8142019510269165
I0216 15:46:08.292890 140399019657024 spec.py:321] Evaluating on the training split.
I0216 15:47:00.692447 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 15:47:52.100131 140399019657024 spec.py:349] Evaluating on the test split.
I0216 15:48:18.536297 140399019657024 submission_runner.py:408] Time since start: 18967.40s, 	Step: 21798, 	{'train/ctc_loss': Array(0.86420834, dtype=float32), 'train/wer': 0.2851232256183706, 'validation/ctc_loss': Array(0.9465712, dtype=float32), 'validation/wer': 0.2811821157206716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6596471, dtype=float32), 'test/wer': 0.22094936323197856, 'test/num_examples': 2472, 'score': 17317.22550392151, 'total_duration': 18967.396344423294, 'accumulated_submission_time': 17317.22550392151, 'accumulated_eval_time': 1648.6084377765656, 'accumulated_logging_time': 0.6313190460205078}
I0216 15:48:18.574113 140318077859584 logging_writer.py:48] [21798] accumulated_eval_time=1648.608438, accumulated_logging_time=0.631319, accumulated_submission_time=17317.225504, global_step=21798, preemption_count=0, score=17317.225504, test/ctc_loss=0.6596471071243286, test/num_examples=2472, test/wer=0.220949, total_duration=18967.396344, train/ctc_loss=0.8642083406448364, train/wer=0.285123, validation/ctc_loss=0.9465711712837219, validation/num_examples=5348, validation/wer=0.281182
I0216 15:48:20.976019 140318069466880 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.49342307448387146, loss=1.8751579523086548
I0216 15:49:37.359716 140318077859584 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5014433264732361, loss=1.9057382345199585
I0216 15:50:53.997142 140318069466880 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5308739542961121, loss=1.8491120338439941
I0216 15:52:10.795537 140318077859584 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7851022481918335, loss=1.865404486656189
I0216 15:53:27.233703 140318069466880 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.47018885612487793, loss=1.8770846128463745
I0216 15:54:43.735293 140318077859584 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6357419490814209, loss=1.8267261981964111
I0216 15:56:03.239800 140318069466880 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6268789172172546, loss=1.8145408630371094
I0216 15:57:27.210494 140318077859584 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5167617797851562, loss=1.8607268333435059
I0216 15:58:51.513652 140318069466880 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9181633591651917, loss=1.7652920484542847
I0216 16:00:16.018608 140318077859584 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.1205854415893555, loss=1.9076985120773315
I0216 16:01:32.562586 140318069466880 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.801718533039093, loss=1.9599965810775757
I0216 16:02:49.175394 140318077859584 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.698889434337616, loss=1.9070117473602295
I0216 16:04:05.877470 140318069466880 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6063162088394165, loss=1.8699965476989746
I0216 16:05:22.727334 140318077859584 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5809265971183777, loss=1.8813982009887695
I0216 16:06:39.791283 140318069466880 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.5248732566833496, loss=1.8008112907409668
I0216 16:08:00.466577 140318077859584 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5597579479217529, loss=1.8772073984146118
I0216 16:09:24.358382 140318069466880 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6070333123207092, loss=1.8537712097167969
I0216 16:10:48.397086 140318077859584 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6700071096420288, loss=1.8404662609100342
I0216 16:12:13.292635 140318069466880 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.579146146774292, loss=1.870063066482544
I0216 16:12:18.637497 140399019657024 spec.py:321] Evaluating on the training split.
I0216 16:13:11.506479 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 16:14:02.943617 140399019657024 spec.py:349] Evaluating on the test split.
I0216 16:14:28.959751 140399019657024 submission_runner.py:408] Time since start: 20537.82s, 	Step: 23608, 	{'train/ctc_loss': Array(0.8158463, dtype=float32), 'train/wer': 0.26482833320170146, 'validation/ctc_loss': Array(0.9147472, dtype=float32), 'validation/wer': 0.2765285729457312, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.631796, dtype=float32), 'test/wer': 0.21128105132736172, 'test/num_examples': 2472, 'score': 18757.19909286499, 'total_duration': 20537.81848526001, 'accumulated_submission_time': 18757.19909286499, 'accumulated_eval_time': 1778.9243762493134, 'accumulated_logging_time': 0.6872842311859131}
I0216 16:14:28.995397 140318077859584 logging_writer.py:48] [23608] accumulated_eval_time=1778.924376, accumulated_logging_time=0.687284, accumulated_submission_time=18757.199093, global_step=23608, preemption_count=0, score=18757.199093, test/ctc_loss=0.6317960023880005, test/num_examples=2472, test/wer=0.211281, total_duration=20537.818485, train/ctc_loss=0.81584632396698, train/wer=0.264828, validation/ctc_loss=0.9147471785545349, validation/num_examples=5348, validation/wer=0.276529
I0216 16:15:43.544841 140318077859584 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.573305606842041, loss=1.8317208290100098
I0216 16:16:59.977261 140318069466880 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8416121602058411, loss=1.7771121263504028
I0216 16:18:16.514405 140318077859584 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4702281355857849, loss=1.8155437707901
I0216 16:19:32.893597 140318069466880 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7932222485542297, loss=1.8517969846725464
I0216 16:20:49.382403 140318077859584 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5317465662956238, loss=1.8203868865966797
I0216 16:22:06.535674 140318069466880 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6051099896430969, loss=1.905196189880371
I0216 16:23:29.576660 140318077859584 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9222738146781921, loss=1.8276029825210571
I0216 16:24:52.414015 140318069466880 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.787787914276123, loss=1.8625766038894653
I0216 16:26:16.335269 140318077859584 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6756392121315002, loss=1.7950023412704468
I0216 16:27:39.394510 140318069466880 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.470341295003891, loss=1.7872949838638306
I0216 16:29:02.468872 140318077859584 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5268097519874573, loss=1.8133002519607544
I0216 16:30:23.550272 140318077859584 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.729073166847229, loss=1.771663784980774
I0216 16:31:40.114282 140318069466880 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8008146286010742, loss=1.8293256759643555
I0216 16:32:56.736743 140318077859584 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7182572484016418, loss=1.8428062200546265
I0216 16:34:13.253765 140318069466880 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7247819304466248, loss=1.859818696975708
I0216 16:35:29.810507 140318077859584 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.7339502573013306, loss=1.858005166053772
I0216 16:36:48.301424 140318069466880 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.0018898248672485, loss=1.7440085411071777
I0216 16:38:12.116735 140318077859584 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.768724262714386, loss=1.7754360437393188
I0216 16:38:29.683192 140399019657024 spec.py:321] Evaluating on the training split.
I0216 16:39:23.957209 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 16:40:15.569667 140399019657024 spec.py:349] Evaluating on the test split.
I0216 16:40:42.422736 140399019657024 submission_runner.py:408] Time since start: 22111.28s, 	Step: 25422, 	{'train/ctc_loss': Array(0.8221751, dtype=float32), 'train/wer': 0.26447396579254945, 'validation/ctc_loss': Array(0.89193165, dtype=float32), 'validation/wer': 0.2666228989061278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6027977, dtype=float32), 'test/wer': 0.20333922369142649, 'test/num_examples': 2472, 'score': 20197.799550056458, 'total_duration': 22111.28186249733, 'accumulated_submission_time': 20197.799550056458, 'accumulated_eval_time': 1911.6580095291138, 'accumulated_logging_time': 0.73909592628479}
I0216 16:40:42.464940 140318077859584 logging_writer.py:48] [25422] accumulated_eval_time=1911.658010, accumulated_logging_time=0.739096, accumulated_submission_time=20197.799550, global_step=25422, preemption_count=0, score=20197.799550, test/ctc_loss=0.6027976870536804, test/num_examples=2472, test/wer=0.203339, total_duration=22111.281862, train/ctc_loss=0.8221750855445862, train/wer=0.264474, validation/ctc_loss=0.8919316530227661, validation/num_examples=5348, validation/wer=0.266623
I0216 16:41:42.720506 140318069466880 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.668506383895874, loss=1.7644684314727783
I0216 16:42:59.394679 140318077859584 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6096992492675781, loss=1.8824678659439087
I0216 16:44:15.769525 140318069466880 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5232219099998474, loss=1.816408634185791
I0216 16:45:35.644807 140318077859584 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1834657192230225, loss=1.804792881011963
I0216 16:46:52.104670 140318069466880 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5422954559326172, loss=1.8241634368896484
I0216 16:48:08.657957 140318077859584 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8038445711135864, loss=1.8334788084030151
I0216 16:49:25.295744 140318069466880 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6053676605224609, loss=1.7678546905517578
I0216 16:50:42.011306 140318077859584 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.8320865035057068, loss=1.7968356609344482
I0216 16:51:58.840526 140318069466880 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7678136229515076, loss=1.8235360383987427
I0216 16:53:21.184973 140318077859584 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6441933512687683, loss=1.779788851737976
I0216 16:54:44.713403 140318069466880 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5960933566093445, loss=1.91965651512146
I0216 16:56:08.492972 140318077859584 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.578373372554779, loss=1.8151216506958008
I0216 16:57:31.513491 140318069466880 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0730942487716675, loss=1.8112519979476929
I0216 16:58:56.448105 140318077859584 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8691790103912354, loss=1.7337058782577515
I0216 17:00:12.726389 140318069466880 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1654362678527832, loss=1.8021107912063599
I0216 17:01:29.122556 140318077859584 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5367714166641235, loss=1.7355570793151855
I0216 17:02:45.652812 140318069466880 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5292297005653381, loss=1.825087308883667
I0216 17:04:02.152866 140318077859584 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9352917075157166, loss=1.7390124797821045
I0216 17:04:43.074443 140399019657024 spec.py:321] Evaluating on the training split.
I0216 17:05:36.964995 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 17:06:29.060057 140399019657024 spec.py:349] Evaluating on the test split.
I0216 17:06:55.188266 140399019657024 submission_runner.py:408] Time since start: 23684.05s, 	Step: 27255, 	{'train/ctc_loss': Array(0.7159185, dtype=float32), 'train/wer': 0.2397716148297229, 'validation/ctc_loss': Array(0.87400496, dtype=float32), 'validation/wer': 0.2637458122942352, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.59717554, dtype=float32), 'test/wer': 0.20195803627648123, 'test/num_examples': 2472, 'score': 21638.320074796677, 'total_duration': 23684.04691696167, 'accumulated_submission_time': 21638.320074796677, 'accumulated_eval_time': 2043.7654702663422, 'accumulated_logging_time': 0.7973823547363281}
I0216 17:06:55.225934 140318077859584 logging_writer.py:48] [27255] accumulated_eval_time=2043.765470, accumulated_logging_time=0.797382, accumulated_submission_time=21638.320075, global_step=27255, preemption_count=0, score=21638.320075, test/ctc_loss=0.5971755385398865, test/num_examples=2472, test/wer=0.201958, total_duration=23684.046917, train/ctc_loss=0.7159184813499451, train/wer=0.239772, validation/ctc_loss=0.8740049600601196, validation/num_examples=5348, validation/wer=0.263746
I0216 17:07:30.382922 140318069466880 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.72934490442276, loss=1.7821143865585327
I0216 17:08:46.817150 140318077859584 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.4868135154247284, loss=1.7948309183120728
I0216 17:10:03.327841 140318069466880 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7172272801399231, loss=1.8320844173431396
I0216 17:11:19.969171 140318077859584 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8756737112998962, loss=1.736904263496399
I0216 17:12:36.629427 140318069466880 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6995590925216675, loss=1.8174303770065308
I0216 17:13:59.000158 140318077859584 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.805782675743103, loss=1.7816660404205322
I0216 17:15:20.409555 140318077859584 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0739017724990845, loss=1.810053825378418
I0216 17:16:36.953755 140318069466880 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6997458338737488, loss=1.7458891868591309
I0216 17:17:53.537482 140318077859584 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.661502480506897, loss=1.8085558414459229
I0216 17:19:10.219475 140318069466880 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.5315448641777039, loss=1.784895420074463
I0216 17:20:26.749176 140318077859584 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.8232817649841309, loss=1.7425479888916016
I0216 17:21:45.924430 140318069466880 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.5642226338386536, loss=1.7196831703186035
I0216 17:23:08.847700 140318077859584 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.693244993686676, loss=1.7206605672836304
I0216 17:24:32.409948 140318069466880 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6941606402397156, loss=1.8042975664138794
I0216 17:25:55.238879 140318077859584 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.865766167640686, loss=1.8319370746612549
I0216 17:27:17.772908 140318069466880 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8227420449256897, loss=1.7855219841003418
I0216 17:28:40.505810 140318077859584 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5039201378822327, loss=1.7389062643051147
I0216 17:29:57.013605 140318069466880 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6430670022964478, loss=1.80655038356781
I0216 17:30:56.081378 140399019657024 spec.py:321] Evaluating on the training split.
I0216 17:31:48.985543 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 17:32:40.553669 140399019657024 spec.py:349] Evaluating on the test split.
I0216 17:33:06.870462 140399019657024 submission_runner.py:408] Time since start: 25255.73s, 	Step: 29078, 	{'train/ctc_loss': Array(0.7162538, dtype=float32), 'train/wer': 0.24312656722086015, 'validation/ctc_loss': Array(0.8402705, dtype=float32), 'validation/wer': 0.2538691022138119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5756875, dtype=float32), 'test/wer': 0.19549895395364897, 'test/num_examples': 2472, 'score': 23079.08738541603, 'total_duration': 25255.72882080078, 'accumulated_submission_time': 23079.08738541603, 'accumulated_eval_time': 2174.5478515625, 'accumulated_logging_time': 0.8513691425323486}
I0216 17:33:06.911776 140318077859584 logging_writer.py:48] [29078] accumulated_eval_time=2174.547852, accumulated_logging_time=0.851369, accumulated_submission_time=23079.087385, global_step=29078, preemption_count=0, score=23079.087385, test/ctc_loss=0.5756875276565552, test/num_examples=2472, test/wer=0.195499, total_duration=25255.728821, train/ctc_loss=0.7162538170814514, train/wer=0.243127, validation/ctc_loss=0.8402705192565918, validation/num_examples=5348, validation/wer=0.253869
I0216 17:33:24.496472 140318069466880 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5939390063285828, loss=1.7591753005981445
I0216 17:34:40.993448 140318077859584 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5337049961090088, loss=1.769671082496643
I0216 17:35:57.351645 140318069466880 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6843605041503906, loss=1.742413878440857
I0216 17:37:13.826661 140318077859584 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9724767804145813, loss=1.7483835220336914
I0216 17:38:30.411537 140318069466880 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9785260558128357, loss=1.7400590181350708
I0216 17:39:48.216270 140318077859584 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.7750324606895447, loss=1.7530953884124756
I0216 17:41:11.659705 140318069466880 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6786696910858154, loss=1.724162220954895
I0216 17:42:34.873853 140318077859584 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5377708077430725, loss=1.719757080078125
I0216 17:43:59.241175 140318077859584 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.565913736820221, loss=1.6810981035232544
I0216 17:45:15.932775 140318069466880 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9447967410087585, loss=1.7652835845947266
I0216 17:46:32.719213 140318077859584 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.7759436368942261, loss=1.747890830039978
I0216 17:47:49.439612 140318069466880 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6010521054267883, loss=1.7085386514663696
I0216 17:49:06.467688 140318077859584 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7115821838378906, loss=1.7578998804092407
I0216 17:50:23.024087 140318069466880 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5728752017021179, loss=1.7711970806121826
I0216 17:51:41.323900 140318077859584 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8212091326713562, loss=1.753998875617981
I0216 17:53:04.685124 140318069466880 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.7365617156028748, loss=1.8032050132751465
I0216 17:54:27.552692 140318077859584 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.7696197628974915, loss=1.7023059129714966
I0216 17:55:50.865809 140318069466880 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5743801593780518, loss=1.7447569370269775
I0216 17:57:07.149802 140399019657024 spec.py:321] Evaluating on the training split.
I0216 17:57:58.289882 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 17:58:49.886077 140399019657024 spec.py:349] Evaluating on the test split.
I0216 17:59:16.111604 140399019657024 submission_runner.py:408] Time since start: 26824.97s, 	Step: 30893, 	{'train/ctc_loss': Array(0.7572224, dtype=float32), 'train/wer': 0.2548112793816958, 'validation/ctc_loss': Array(0.86291736, dtype=float32), 'validation/wer': 0.26156386070266563, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57978195, dtype=float32), 'test/wer': 0.1974894887575407, 'test/num_examples': 2472, 'score': 24519.23925757408, 'total_duration': 26824.97067308426, 'accumulated_submission_time': 24519.23925757408, 'accumulated_eval_time': 2303.503675699234, 'accumulated_logging_time': 0.9082772731781006}
I0216 17:59:16.148795 140318077859584 logging_writer.py:48] [30893] accumulated_eval_time=2303.503676, accumulated_logging_time=0.908277, accumulated_submission_time=24519.239258, global_step=30893, preemption_count=0, score=24519.239258, test/ctc_loss=0.5797819495201111, test/num_examples=2472, test/wer=0.197489, total_duration=26824.970673, train/ctc_loss=0.7572224140167236, train/wer=0.254811, validation/ctc_loss=0.8629173636436462, validation/num_examples=5348, validation/wer=0.261564
I0216 17:59:25.887017 140318077859584 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.681300938129425, loss=1.7653121948242188
I0216 18:00:42.159932 140318069466880 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6502472162246704, loss=1.7563269138336182
I0216 18:01:58.697812 140318077859584 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5402238965034485, loss=1.723715901374817
I0216 18:03:15.221747 140318069466880 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.5815833806991577, loss=1.7457280158996582
I0216 18:04:31.684738 140318077859584 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.573732852935791, loss=1.7203714847564697
I0216 18:05:48.204136 140318069466880 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.6074140667915344, loss=1.710980772972107
I0216 18:07:05.065711 140318077859584 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6026396155357361, loss=1.7305773496627808
I0216 18:08:24.274065 140318069466880 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6598178744316101, loss=1.7462517023086548
I0216 18:09:46.969346 140318077859584 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.5698369145393372, loss=1.6997969150543213
I0216 18:11:09.898038 140318069466880 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.9240608215332031, loss=1.7748231887817383
I0216 18:12:32.981485 140318077859584 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.6593015789985657, loss=1.6996349096298218
I0216 18:13:54.537056 140318077859584 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5964513421058655, loss=1.734614610671997
I0216 18:15:11.227693 140318069466880 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.4849143922328949, loss=1.736167073249817
I0216 18:16:27.858667 140318077859584 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6671906113624573, loss=1.7703818082809448
I0216 18:17:44.388591 140318069466880 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6624642014503479, loss=1.7070988416671753
I0216 18:19:01.053075 140318077859584 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6211881041526794, loss=1.6950403451919556
I0216 18:20:17.751629 140318069466880 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7728651762008667, loss=1.7581031322479248
I0216 18:21:34.285839 140318077859584 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.8524649143218994, loss=1.7117890119552612
I0216 18:22:54.041125 140318069466880 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.556297242641449, loss=1.702217936515808
I0216 18:23:16.858720 140399019657024 spec.py:321] Evaluating on the training split.
I0216 18:24:20.528940 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 18:25:11.986799 140399019657024 spec.py:349] Evaluating on the test split.
I0216 18:25:38.747063 140399019657024 submission_runner.py:408] Time since start: 28407.61s, 	Step: 32729, 	{'train/ctc_loss': Array(0.508243, dtype=float32), 'train/wer': 0.1794177239877347, 'validation/ctc_loss': Array(0.7982695, dtype=float32), 'validation/wer': 0.24097048572559546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5389743, dtype=float32), 'test/wer': 0.18296670932098388, 'test/num_examples': 2472, 'score': 25959.860438346863, 'total_duration': 28407.60689687729, 'accumulated_submission_time': 25959.860438346863, 'accumulated_eval_time': 2445.386803150177, 'accumulated_logging_time': 0.9621689319610596}
I0216 18:25:38.788624 140318077859584 logging_writer.py:48] [32729] accumulated_eval_time=2445.386803, accumulated_logging_time=0.962169, accumulated_submission_time=25959.860438, global_step=32729, preemption_count=0, score=25959.860438, test/ctc_loss=0.5389742851257324, test/num_examples=2472, test/wer=0.182967, total_duration=28407.606897, train/ctc_loss=0.5082430243492126, train/wer=0.179418, validation/ctc_loss=0.798269510269165, validation/num_examples=5348, validation/wer=0.240970
I0216 18:26:33.794836 140318069466880 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5990757346153259, loss=1.7191568613052368
I0216 18:27:50.327593 140318077859584 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7755163908004761, loss=1.8157711029052734
I0216 18:29:10.376920 140318077859584 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8709996342658997, loss=1.7189429998397827
I0216 18:30:26.791507 140318069466880 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.7115641236305237, loss=1.6715089082717896
I0216 18:31:43.385318 140318077859584 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.790292501449585, loss=1.6958287954330444
I0216 18:33:00.061732 140318069466880 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.5384896993637085, loss=1.7141209840774536
I0216 18:34:16.537730 140318077859584 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.0748212337493896, loss=1.7654767036437988
I0216 18:35:33.100337 140318069466880 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7317633628845215, loss=1.7401031255722046
I0216 18:36:49.753000 140318077859584 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.785830020904541, loss=1.715484380722046
I0216 18:38:12.490848 140318069466880 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7154394388198853, loss=1.6882630586624146
I0216 18:39:36.004705 140318077859584 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6735951900482178, loss=1.7287325859069824
I0216 18:40:58.832413 140318069466880 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.7758084535598755, loss=1.7680243253707886
I0216 18:42:25.330084 140318077859584 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6288278102874756, loss=1.6996347904205322
I0216 18:43:41.823571 140318069466880 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.6914697885513306, loss=1.6432852745056152
I0216 18:44:58.338536 140318077859584 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6112858653068542, loss=1.7020642757415771
I0216 18:46:15.049479 140318069466880 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.8674173951148987, loss=1.6849623918533325
I0216 18:47:31.746682 140318077859584 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7357326745986938, loss=1.6706260442733765
I0216 18:48:48.173057 140318069466880 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.5433098673820496, loss=1.7103748321533203
I0216 18:49:38.959538 140399019657024 spec.py:321] Evaluating on the training split.
I0216 18:50:34.084581 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 18:51:26.001754 140399019657024 spec.py:349] Evaluating on the test split.
I0216 18:51:51.926909 140399019657024 submission_runner.py:408] Time since start: 29980.79s, 	Step: 34568, 	{'train/ctc_loss': Array(0.45416874, dtype=float32), 'train/wer': 0.1624045821292815, 'validation/ctc_loss': Array(0.7872792, dtype=float32), 'validation/wer': 0.23718586172605888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51708585, dtype=float32), 'test/wer': 0.17520768590173258, 'test/num_examples': 2472, 'score': 27399.94140648842, 'total_duration': 29980.78520011902, 'accumulated_submission_time': 27399.94140648842, 'accumulated_eval_time': 2578.3474531173706, 'accumulated_logging_time': 1.02097749710083}
I0216 18:51:51.974152 140318077859584 logging_writer.py:48] [34568] accumulated_eval_time=2578.347453, accumulated_logging_time=1.020977, accumulated_submission_time=27399.941406, global_step=34568, preemption_count=0, score=27399.941406, test/ctc_loss=0.5170858502388, test/num_examples=2472, test/wer=0.175208, total_duration=29980.785200, train/ctc_loss=0.45416873693466187, train/wer=0.162405, validation/ctc_loss=0.7872791886329651, validation/num_examples=5348, validation/wer=0.237186
I0216 18:52:17.132627 140318069466880 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.5825762748718262, loss=1.6753041744232178
I0216 18:53:33.459941 140318077859584 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.8044264912605286, loss=1.7632418870925903
I0216 18:54:49.714833 140318069466880 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.7565004825592041, loss=1.7022979259490967
I0216 18:56:06.249625 140318077859584 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6092188358306885, loss=1.7238436937332153
I0216 18:57:22.957237 140318069466880 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5862923860549927, loss=1.6860055923461914
I0216 18:58:42.915144 140318077859584 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6689545512199402, loss=1.659178614616394
I0216 18:59:59.311334 140318069466880 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.7179226875305176, loss=1.6808887720108032
I0216 19:01:15.967669 140318077859584 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6005929112434387, loss=1.6849546432495117
I0216 19:02:32.474921 140318069466880 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6640045642852783, loss=1.6938055753707886
I0216 19:03:49.065537 140318077859584 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.5800777673721313, loss=1.6478447914123535
I0216 19:05:05.739225 140318069466880 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.586627721786499, loss=1.7058180570602417
I0216 19:06:28.292954 140318077859584 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.9494989514350891, loss=1.7841297388076782
I0216 19:07:51.575484 140318069466880 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.81955885887146, loss=1.667948603630066
I0216 19:09:14.708311 140318077859584 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.5592827200889587, loss=1.645756483078003
I0216 19:10:37.607651 140318069466880 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.901223361492157, loss=1.6778035163879395
I0216 19:12:00.066351 140318077859584 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.5193734169006348, loss=1.7248777151107788
I0216 19:13:16.835454 140318069466880 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.5802269577980042, loss=1.6988794803619385
I0216 19:14:33.225666 140318077859584 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.6137917041778564, loss=1.6227957010269165
I0216 19:15:49.798068 140318069466880 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6827548742294312, loss=1.59929621219635
I0216 19:15:52.584490 140399019657024 spec.py:321] Evaluating on the training split.
I0216 19:16:46.751160 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 19:17:38.000681 140399019657024 spec.py:349] Evaluating on the test split.
I0216 19:18:04.211578 140399019657024 submission_runner.py:408] Time since start: 31553.07s, 	Step: 36405, 	{'train/ctc_loss': Array(0.4568602, dtype=float32), 'train/wer': 0.1632162807164774, 'validation/ctc_loss': Array(0.7698689, dtype=float32), 'validation/wer': 0.2332853818898018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51699764, dtype=float32), 'test/wer': 0.1756342290739951, 'test/num_examples': 2472, 'score': 28840.462290525436, 'total_duration': 31553.070999383926, 'accumulated_submission_time': 28840.462290525436, 'accumulated_eval_time': 2709.968914270401, 'accumulated_logging_time': 1.0840272903442383}
I0216 19:18:04.249964 140318077859584 logging_writer.py:48] [36405] accumulated_eval_time=2709.968914, accumulated_logging_time=1.084027, accumulated_submission_time=28840.462291, global_step=36405, preemption_count=0, score=28840.462291, test/ctc_loss=0.5169976353645325, test/num_examples=2472, test/wer=0.175634, total_duration=31553.070999, train/ctc_loss=0.456860214471817, train/wer=0.163216, validation/ctc_loss=0.7698689103126526, validation/num_examples=5348, validation/wer=0.233285
I0216 19:19:17.538897 140318069466880 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5023345351219177, loss=1.7017205953598022
I0216 19:20:34.296768 140318077859584 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.690142035484314, loss=1.6708534955978394
I0216 19:21:50.978822 140318069466880 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.7538831830024719, loss=1.696635127067566
I0216 19:23:07.705163 140318077859584 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6446850895881653, loss=1.6290956735610962
I0216 19:24:26.377610 140318069466880 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8354960680007935, loss=1.714800238609314
I0216 19:25:48.096629 140318077859584 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6550785899162292, loss=1.6177029609680176
I0216 19:27:12.877313 140318077859584 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.5834556818008423, loss=1.6672253608703613
I0216 19:28:29.579780 140318069466880 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6879333853721619, loss=1.644267201423645
I0216 19:29:46.354964 140318077859584 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.5910216569900513, loss=1.6766188144683838
I0216 19:31:03.197420 140318069466880 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.5686523914337158, loss=1.6334902048110962
I0216 19:32:19.976614 140318077859584 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6110672950744629, loss=1.6474380493164062
I0216 19:33:36.688746 140318069466880 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7007962465286255, loss=1.6654558181762695
I0216 19:34:53.518723 140318077859584 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.65981525182724, loss=1.6783323287963867
I0216 19:36:16.944195 140318069466880 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6696851849555969, loss=1.6801304817199707
I0216 19:37:39.415925 140318077859584 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.5769227743148804, loss=1.6511280536651611
I0216 19:39:01.904516 140318069466880 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.5760728716850281, loss=1.6665961742401123
I0216 19:40:24.532698 140318077859584 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.736481785774231, loss=1.6283392906188965
I0216 19:41:44.761634 140318077859584 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6465566158294678, loss=1.6631087064743042
I0216 19:42:04.420603 140399019657024 spec.py:321] Evaluating on the training split.
I0216 19:42:58.903692 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 19:43:50.261399 140399019657024 spec.py:349] Evaluating on the test split.
I0216 19:44:16.536132 140399019657024 submission_runner.py:408] Time since start: 33125.40s, 	Step: 38227, 	{'train/ctc_loss': Array(0.41335335, dtype=float32), 'train/wer': 0.15208877622287179, 'validation/ctc_loss': Array(0.74211925, dtype=float32), 'validation/wer': 0.22561958736012822, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.487146, dtype=float32), 'test/wer': 0.16645339508053542, 'test/num_examples': 2472, 'score': 30280.543273448944, 'total_duration': 33125.39553499222, 'accumulated_submission_time': 30280.543273448944, 'accumulated_eval_time': 2842.078820705414, 'accumulated_logging_time': 1.1394822597503662}
I0216 19:44:16.577666 140318077859584 logging_writer.py:48] [38227] accumulated_eval_time=2842.078821, accumulated_logging_time=1.139482, accumulated_submission_time=30280.543273, global_step=38227, preemption_count=0, score=30280.543273, test/ctc_loss=0.4871459901332855, test/num_examples=2472, test/wer=0.166453, total_duration=33125.395535, train/ctc_loss=0.4133533537387848, train/wer=0.152089, validation/ctc_loss=0.7421192526817322, validation/num_examples=5348, validation/wer=0.225620
I0216 19:45:13.022386 140318069466880 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.885356605052948, loss=1.6291736364364624
I0216 19:46:29.757100 140318077859584 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.8139669299125671, loss=1.5936341285705566
I0216 19:47:46.584774 140318069466880 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.6480854153633118, loss=1.6719262599945068
I0216 19:49:03.258402 140318077859584 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5479812026023865, loss=1.6901875734329224
I0216 19:50:20.008531 140318069466880 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6174305081367493, loss=1.6235976219177246
I0216 19:51:36.542975 140318077859584 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6291804909706116, loss=1.607541799545288
I0216 19:52:56.996680 140318069466880 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.6666167378425598, loss=1.6958236694335938
I0216 19:54:20.748861 140318077859584 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6120157837867737, loss=1.612678050994873
I0216 19:55:44.200639 140318069466880 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5610724091529846, loss=1.6055372953414917
I0216 19:57:06.332929 140318077859584 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6389895677566528, loss=1.5945614576339722
I0216 19:58:23.011320 140318069466880 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.656743049621582, loss=1.6276779174804688
I0216 19:59:39.955274 140318077859584 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.842563271522522, loss=1.6133931875228882
I0216 20:00:56.667835 140318069466880 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6618646383285522, loss=1.6745994091033936
I0216 20:02:13.442301 140318077859584 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7017630934715271, loss=1.6403942108154297
I0216 20:03:30.246801 140318069466880 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7185665369033813, loss=1.6112128496170044
I0216 20:04:51.934439 140318077859584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5255746841430664, loss=1.62567138671875
I0216 20:06:15.477983 140318069466880 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.5791138410568237, loss=1.5799400806427002
I0216 20:07:38.304362 140318077859584 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6451241970062256, loss=1.621469497680664
I0216 20:08:17.490615 140399019657024 spec.py:321] Evaluating on the training split.
I0216 20:09:11.321403 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 20:10:03.303555 140399019657024 spec.py:349] Evaluating on the test split.
I0216 20:10:29.272475 140399019657024 submission_runner.py:408] Time since start: 34698.13s, 	Step: 40049, 	{'train/ctc_loss': Array(0.41621852, dtype=float32), 'train/wer': 0.15325430697262854, 'validation/ctc_loss': Array(0.7324096, dtype=float32), 'validation/wer': 0.2226749181768153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4805117, dtype=float32), 'test/wer': 0.16440192553774907, 'test/num_examples': 2472, 'score': 31721.368850708008, 'total_duration': 34698.13117194176, 'accumulated_submission_time': 31721.368850708008, 'accumulated_eval_time': 2973.8543276786804, 'accumulated_logging_time': 1.196692943572998}
I0216 20:10:29.311720 140318077859584 logging_writer.py:48] [40049] accumulated_eval_time=2973.854328, accumulated_logging_time=1.196693, accumulated_submission_time=31721.368851, global_step=40049, preemption_count=0, score=31721.368851, test/ctc_loss=0.48051169514656067, test/num_examples=2472, test/wer=0.164402, total_duration=34698.131172, train/ctc_loss=0.41621851921081543, train/wer=0.153254, validation/ctc_loss=0.7324095964431763, validation/num_examples=5348, validation/wer=0.222675
I0216 20:11:08.986319 140318069466880 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.8225160837173462, loss=1.5662569999694824
I0216 20:12:28.997464 140318077859584 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7054793238639832, loss=1.5919904708862305
I0216 20:13:45.301206 140318069466880 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.9769739508628845, loss=1.658752679824829
I0216 20:15:01.726894 140318077859584 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6387102007865906, loss=1.5845803022384644
I0216 20:16:18.344506 140318069466880 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6939124464988708, loss=1.5920748710632324
I0216 20:17:34.902683 140318077859584 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5300265550613403, loss=1.618479609489441
I0216 20:18:51.472334 140318069466880 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.7110536694526672, loss=1.6391140222549438
I0216 20:20:14.483686 140318077859584 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.716951847076416, loss=1.544124722480774
I0216 20:21:38.042918 140318069466880 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.5548173189163208, loss=1.5609636306762695
I0216 20:23:01.965728 140318077859584 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.6169568300247192, loss=1.635495662689209
I0216 20:24:24.557360 140318069466880 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.8957411646842957, loss=1.599554419517517
I0216 20:25:50.905657 140318077859584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6049424409866333, loss=1.5695278644561768
I0216 20:27:07.505864 140318069466880 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6806119084358215, loss=1.583164930343628
I0216 20:28:24.125192 140318077859584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.641132652759552, loss=1.6120028495788574
I0216 20:29:40.830147 140318069466880 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7225812077522278, loss=1.6620292663574219
I0216 20:30:57.350993 140318077859584 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7563454508781433, loss=1.5766029357910156
I0216 20:32:14.120455 140318069466880 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.658286988735199, loss=1.591398000717163
I0216 20:33:31.748071 140318077859584 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6773257851600647, loss=1.5400876998901367
I0216 20:34:29.753231 140399019657024 spec.py:321] Evaluating on the training split.
I0216 20:35:24.996300 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 20:36:17.261785 140399019657024 spec.py:349] Evaluating on the test split.
I0216 20:36:43.721535 140399019657024 submission_runner.py:408] Time since start: 36272.58s, 	Step: 41871, 	{'train/ctc_loss': Array(0.38685045, dtype=float32), 'train/wer': 0.14329412258518107, 'validation/ctc_loss': Array(0.69860196, dtype=float32), 'validation/wer': 0.21291406393311257, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45834982, dtype=float32), 'test/wer': 0.1583084516482847, 'test/num_examples': 2472, 'score': 33161.7200114727, 'total_duration': 36272.580060482025, 'accumulated_submission_time': 33161.7200114727, 'accumulated_eval_time': 3107.816128730774, 'accumulated_logging_time': 1.2540216445922852}
I0216 20:36:43.770113 140318077859584 logging_writer.py:48] [41871] accumulated_eval_time=3107.816129, accumulated_logging_time=1.254022, accumulated_submission_time=33161.720011, global_step=41871, preemption_count=0, score=33161.720011, test/ctc_loss=0.4583498239517212, test/num_examples=2472, test/wer=0.158308, total_duration=36272.580060, train/ctc_loss=0.3868504464626312, train/wer=0.143294, validation/ctc_loss=0.6986019611358643, validation/num_examples=5348, validation/wer=0.212914
I0216 20:37:06.690137 140318069466880 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6329399347305298, loss=1.6465200185775757
I0216 20:38:23.407512 140318077859584 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6752738952636719, loss=1.6009246110916138
I0216 20:39:40.038658 140318069466880 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.610365092754364, loss=1.6592739820480347
I0216 20:40:56.773929 140318077859584 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.8680101633071899, loss=1.6564240455627441
I0216 20:42:16.728576 140318077859584 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.5578710436820984, loss=1.5161105394363403
I0216 20:43:33.156535 140318069466880 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5606358051300049, loss=1.6113959550857544
I0216 20:44:49.745121 140318077859584 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5720862150192261, loss=1.5621283054351807
I0216 20:46:06.248580 140318069466880 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7013218998908997, loss=1.5867199897766113
I0216 20:47:22.812489 140318077859584 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7641324400901794, loss=1.5727351903915405
I0216 20:48:39.337668 140318069466880 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8045552372932434, loss=1.546751618385315
I0216 20:50:02.286827 140318077859584 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6160247325897217, loss=1.5891907215118408
I0216 20:51:25.345645 140318069466880 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6731836199760437, loss=1.5592306852340698
I0216 20:52:48.820533 140318077859584 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6841853857040405, loss=1.5336698293685913
I0216 20:54:11.944414 140318069466880 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.7181046009063721, loss=1.6234996318817139
I0216 20:55:34.998675 140318077859584 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7381171584129333, loss=1.5734264850616455
I0216 20:56:51.598309 140318069466880 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5963999629020691, loss=1.5453970432281494
I0216 20:58:08.064037 140318077859584 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7607952952384949, loss=1.6092288494110107
I0216 20:59:24.660874 140318069466880 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5473247170448303, loss=1.550899863243103
I0216 21:00:41.254133 140318077859584 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7614345550537109, loss=1.6715244054794312
I0216 21:00:44.023897 140399019657024 spec.py:321] Evaluating on the training split.
I0216 21:01:37.878862 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 21:02:29.604466 140399019657024 spec.py:349] Evaluating on the test split.
I0216 21:02:56.099284 140399019657024 submission_runner.py:408] Time since start: 37844.96s, 	Step: 43705, 	{'train/ctc_loss': Array(0.4322496, dtype=float32), 'train/wer': 0.15385792600608872, 'validation/ctc_loss': Array(0.68366253, dtype=float32), 'validation/wer': 0.2103845448313815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43958446, dtype=float32), 'test/wer': 0.15253996303292508, 'test/num_examples': 2472, 'score': 34601.8858397007, 'total_duration': 37844.95906472206, 'accumulated_submission_time': 34601.8858397007, 'accumulated_eval_time': 3239.886257171631, 'accumulated_logging_time': 1.3179125785827637}
I0216 21:02:56.139590 140318077859584 logging_writer.py:48] [43705] accumulated_eval_time=3239.886257, accumulated_logging_time=1.317913, accumulated_submission_time=34601.885840, global_step=43705, preemption_count=0, score=34601.885840, test/ctc_loss=0.4395844638347626, test/num_examples=2472, test/wer=0.152540, total_duration=37844.959065, train/ctc_loss=0.43224960565567017, train/wer=0.153858, validation/ctc_loss=0.6836625337600708, validation/num_examples=5348, validation/wer=0.210385
I0216 21:04:09.402387 140318069466880 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6680459380149841, loss=1.6041748523712158
I0216 21:05:25.839241 140318077859584 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.777462363243103, loss=1.573533296585083
I0216 21:06:42.282407 140318069466880 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6715850234031677, loss=1.5454912185668945
I0216 21:07:58.943884 140318077859584 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6649878025054932, loss=1.5462111234664917
I0216 21:09:17.889634 140318069466880 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8755143284797668, loss=1.6026721000671387
I0216 21:10:43.540196 140318077859584 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6747888326644897, loss=1.490121603012085
I0216 21:12:00.191298 140318069466880 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6477472186088562, loss=1.5597341060638428
I0216 21:13:16.744098 140318077859584 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6443104147911072, loss=1.5607188940048218
I0216 21:14:33.286907 140318069466880 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.670612633228302, loss=1.6181598901748657
I0216 21:15:49.965627 140318077859584 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6718258857727051, loss=1.5379141569137573
I0216 21:17:06.577461 140318069466880 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6331212520599365, loss=1.5591319799423218
I0216 21:18:26.723547 140318077859584 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.7221083045005798, loss=1.5346404314041138
I0216 21:19:49.832493 140318069466880 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6056849956512451, loss=1.5512315034866333
I0216 21:21:12.365174 140318077859584 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7542350888252258, loss=1.55472993850708
I0216 21:22:36.141006 140318069466880 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5988997220993042, loss=1.54526948928833
I0216 21:23:59.057516 140318077859584 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7215484976768494, loss=1.5620365142822266
I0216 21:25:20.074573 140318077859584 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7538939118385315, loss=1.5894243717193604
I0216 21:26:36.756073 140318069466880 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6274266242980957, loss=1.5337876081466675
I0216 21:26:56.310720 140399019657024 spec.py:321] Evaluating on the training split.
I0216 21:27:50.497265 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 21:28:41.516307 140399019657024 spec.py:349] Evaluating on the test split.
I0216 21:29:08.161707 140399019657024 submission_runner.py:408] Time since start: 39417.02s, 	Step: 45527, 	{'train/ctc_loss': Array(0.3753733, dtype=float32), 'train/wer': 0.14004018039322375, 'validation/ctc_loss': Array(0.67537385, dtype=float32), 'validation/wer': 0.20578892997480136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43343684, dtype=float32), 'test/wer': 0.14851827026587858, 'test/num_examples': 2472, 'score': 36041.96706032753, 'total_duration': 39417.02039551735, 'accumulated_submission_time': 36041.96706032753, 'accumulated_eval_time': 3371.7308938503265, 'accumulated_logging_time': 1.3760406970977783}
I0216 21:29:08.203558 140318077859584 logging_writer.py:48] [45527] accumulated_eval_time=3371.730894, accumulated_logging_time=1.376041, accumulated_submission_time=36041.967060, global_step=45527, preemption_count=0, score=36041.967060, test/ctc_loss=0.4334368407726288, test/num_examples=2472, test/wer=0.148518, total_duration=39417.020396, train/ctc_loss=0.37537330389022827, train/wer=0.140040, validation/ctc_loss=0.6753738522529602, validation/num_examples=5348, validation/wer=0.205789
I0216 21:30:04.602931 140318069466880 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.6978996992111206, loss=1.55991792678833
I0216 21:31:21.028068 140318077859584 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6815261244773865, loss=1.5337324142456055
I0216 21:32:37.606745 140318069466880 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6842942833900452, loss=1.5899708271026611
I0216 21:33:54.158021 140318077859584 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.6931018233299255, loss=1.5129412412643433
I0216 21:35:10.521831 140318069466880 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7875953912734985, loss=1.5485143661499023
I0216 21:36:28.073884 140318077859584 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6395436525344849, loss=1.5413086414337158
I0216 21:37:50.600172 140318069466880 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7533561587333679, loss=1.5596967935562134
I0216 21:39:13.683455 140318077859584 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.7696961164474487, loss=1.4718865156173706
I0216 21:40:36.864376 140318077859584 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6804271340370178, loss=1.532083511352539
I0216 21:41:53.419878 140318069466880 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6213971376419067, loss=1.5036125183105469
I0216 21:43:10.047667 140318077859584 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7323379516601562, loss=1.5277739763259888
I0216 21:44:26.936994 140318069466880 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.6246863603591919, loss=1.547773838043213
I0216 21:45:43.290002 140318077859584 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6891490817070007, loss=1.5018693208694458
I0216 21:46:59.721166 140318069466880 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.6035306453704834, loss=1.5606762170791626
I0216 21:48:22.045623 140318077859584 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.5514547228813171, loss=1.5290184020996094
I0216 21:49:45.897008 140318069466880 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.655509352684021, loss=1.5486356019973755
I0216 21:51:10.032673 140318077859584 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.6541526913642883, loss=1.5017186403274536
I0216 21:52:33.156323 140318069466880 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7337026000022888, loss=1.5610556602478027
I0216 21:53:08.211967 140399019657024 spec.py:321] Evaluating on the training split.
I0216 21:54:03.131390 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 21:54:54.636372 140399019657024 spec.py:349] Evaluating on the test split.
I0216 21:55:20.915690 140399019657024 submission_runner.py:408] Time since start: 40989.77s, 	Step: 47343, 	{'train/ctc_loss': Array(0.34525725, dtype=float32), 'train/wer': 0.12580803211581926, 'validation/ctc_loss': Array(0.6423175, dtype=float32), 'validation/wer': 0.19709008756770324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41392162, dtype=float32), 'test/wer': 0.1407795584262588, 'test/num_examples': 2472, 'score': 37481.884147167206, 'total_duration': 40989.77420902252, 'accumulated_submission_time': 37481.884147167206, 'accumulated_eval_time': 3504.4280862808228, 'accumulated_logging_time': 1.436626672744751}
I0216 21:55:20.997345 140318077859584 logging_writer.py:48] [47343] accumulated_eval_time=3504.428086, accumulated_logging_time=1.436627, accumulated_submission_time=37481.884147, global_step=47343, preemption_count=0, score=37481.884147, test/ctc_loss=0.41392162442207336, test/num_examples=2472, test/wer=0.140780, total_duration=40989.774209, train/ctc_loss=0.3452572524547577, train/wer=0.125808, validation/ctc_loss=0.6423174738883972, validation/num_examples=5348, validation/wer=0.197090
I0216 21:56:08.830753 140318077859584 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6313338279724121, loss=1.4801416397094727
I0216 21:57:25.309781 140318069466880 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.630323052406311, loss=1.5007262229919434
I0216 21:58:41.881711 140318077859584 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.6924363374710083, loss=1.4888348579406738
I0216 21:59:58.426554 140318069466880 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5840367674827576, loss=1.5184139013290405
I0216 22:01:14.983027 140318077859584 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7267032861709595, loss=1.5219396352767944
I0216 22:02:31.906440 140318069466880 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6469380259513855, loss=1.4720395803451538
I0216 22:03:53.627648 140318077859584 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7749398350715637, loss=1.5419493913650513
I0216 22:05:16.837055 140318069466880 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.590713620185852, loss=1.5425595045089722
I0216 22:06:39.532324 140318077859584 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.7330371141433716, loss=1.5184649229049683
I0216 22:08:02.178947 140318069466880 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7197943925857544, loss=1.4915215969085693
I0216 22:09:25.771647 140318077859584 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.6766782999038696, loss=1.5563955307006836
I0216 22:10:46.230641 140318077859584 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.5814250707626343, loss=1.4913288354873657
I0216 22:12:02.813305 140318069466880 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6915575861930847, loss=1.5284852981567383
I0216 22:13:19.429485 140318077859584 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6756347417831421, loss=1.4927890300750732
I0216 22:14:36.132259 140318069466880 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7166862487792969, loss=1.4722306728363037
I0216 22:15:52.584330 140318077859584 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.6782323122024536, loss=1.4622843265533447
I0216 22:17:09.779663 140318069466880 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6653295159339905, loss=1.5001834630966187
I0216 22:18:33.177981 140318077859584 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.7079967856407166, loss=1.4984334707260132
I0216 22:19:21.555386 140399019657024 spec.py:321] Evaluating on the training split.
I0216 22:20:16.724919 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 22:21:08.142634 140399019657024 spec.py:349] Evaluating on the test split.
I0216 22:21:34.470969 140399019657024 submission_runner.py:408] Time since start: 42563.33s, 	Step: 49160, 	{'train/ctc_loss': Array(0.31973082, dtype=float32), 'train/wer': 0.12011086017126205, 'validation/ctc_loss': Array(0.6218428, dtype=float32), 'validation/wer': 0.192697220425384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40082654, dtype=float32), 'test/wer': 0.13797656043710518, 'test/num_examples': 2472, 'score': 38922.35270643234, 'total_duration': 42563.329916238785, 'accumulated_submission_time': 38922.35270643234, 'accumulated_eval_time': 3637.337597131729, 'accumulated_logging_time': 1.5352272987365723}
I0216 22:21:34.511245 140318077859584 logging_writer.py:48] [49160] accumulated_eval_time=3637.337597, accumulated_logging_time=1.535227, accumulated_submission_time=38922.352706, global_step=49160, preemption_count=0, score=38922.352706, test/ctc_loss=0.4008265435695648, test/num_examples=2472, test/wer=0.137977, total_duration=42563.329916, train/ctc_loss=0.31973081827163696, train/wer=0.120111, validation/ctc_loss=0.6218428015708923, validation/num_examples=5348, validation/wer=0.192697
I0216 22:22:05.839734 140318069466880 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.6167037487030029, loss=1.4260369539260864
I0216 22:23:22.237690 140318077859584 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.7016167044639587, loss=1.506561040878296
I0216 22:24:38.726971 140318069466880 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6866987347602844, loss=1.4920032024383545
I0216 22:25:58.811245 140318077859584 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.6432608962059021, loss=1.4527803659439087
I0216 22:27:15.483237 140318069466880 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6904144883155823, loss=1.469609022140503
I0216 22:28:31.937414 140318077859584 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6028577089309692, loss=1.5126352310180664
I0216 22:29:48.395436 140318069466880 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7533127069473267, loss=1.4695098400115967
I0216 22:31:04.958415 140318077859584 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7300757169723511, loss=1.526455283164978
I0216 22:32:23.589414 140318069466880 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.584677517414093, loss=1.4474869966506958
I0216 22:33:47.540841 140318077859584 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.6706398129463196, loss=1.5476351976394653
I0216 22:35:10.725614 140318069466880 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9799729585647583, loss=1.516174554824829
I0216 22:36:33.653065 140318077859584 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6694144606590271, loss=1.5155104398727417
I0216 22:37:55.993566 140318069466880 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6712644696235657, loss=1.4805676937103271
I0216 22:39:19.775319 140318077859584 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7117358446121216, loss=1.4675652980804443
I0216 22:40:36.127303 140318069466880 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6224422454833984, loss=1.4166373014450073
I0216 22:41:52.566832 140318077859584 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8196722269058228, loss=1.4526021480560303
I0216 22:43:09.022235 140318069466880 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7051791548728943, loss=1.4540199041366577
I0216 22:44:25.698274 140318077859584 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.6452496647834778, loss=1.4132741689682007
I0216 22:45:35.113134 140399019657024 spec.py:321] Evaluating on the training split.
I0216 22:46:30.101405 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 22:47:21.483174 140399019657024 spec.py:349] Evaluating on the test split.
I0216 22:47:47.739485 140399019657024 submission_runner.py:408] Time since start: 44136.60s, 	Step: 50992, 	{'train/ctc_loss': Array(0.30350104, dtype=float32), 'train/wer': 0.11631830085310799, 'validation/ctc_loss': Array(0.6097256, dtype=float32), 'validation/wer': 0.18753198103826138, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38197964, dtype=float32), 'test/wer': 0.13397517924969024, 'test/num_examples': 2472, 'score': 40362.86497974396, 'total_duration': 44136.597618341446, 'accumulated_submission_time': 40362.86497974396, 'accumulated_eval_time': 3769.9570519924164, 'accumulated_logging_time': 1.5926856994628906}
I0216 22:47:47.780205 140318077859584 logging_writer.py:48] [50992] accumulated_eval_time=3769.957052, accumulated_logging_time=1.592686, accumulated_submission_time=40362.864980, global_step=50992, preemption_count=0, score=40362.864980, test/ctc_loss=0.38197964429855347, test/num_examples=2472, test/wer=0.133975, total_duration=44136.597618, train/ctc_loss=0.30350103974342346, train/wer=0.116318, validation/ctc_loss=0.6097255945205688, validation/num_examples=5348, validation/wer=0.187532
I0216 22:47:54.745763 140318069466880 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.003810167312622, loss=1.5382288694381714
I0216 22:49:11.162973 140318077859584 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.5696868896484375, loss=1.4557520151138306
I0216 22:50:27.576918 140318069466880 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.5898461937904358, loss=1.4775623083114624
I0216 22:51:44.200382 140318077859584 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.6902663707733154, loss=1.4759939908981323
I0216 22:53:00.990833 140318069466880 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6908596754074097, loss=1.4648241996765137
I0216 22:54:24.020800 140318077859584 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7616575956344604, loss=1.4449104070663452
I0216 22:55:40.854765 140318069466880 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.7082787156105042, loss=1.4122854471206665
I0216 22:56:57.651974 140318077859584 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6909918785095215, loss=1.4761053323745728
I0216 22:58:14.555788 140318069466880 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.6346449255943298, loss=1.4923934936523438
I0216 22:59:31.504020 140318077859584 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.628714919090271, loss=1.4741178750991821
I0216 23:00:48.361323 140318069466880 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.6937593221664429, loss=1.4877746105194092
I0216 23:02:05.349836 140318077859584 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.6780913472175598, loss=1.4329785108566284
I0216 23:03:27.514218 140318069466880 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.6744104027748108, loss=1.5127203464508057
I0216 23:04:51.080598 140318077859584 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.8703407645225525, loss=1.4163368940353394
I0216 23:06:15.286531 140318069466880 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.6465511918067932, loss=1.4740264415740967
I0216 23:07:39.398780 140318077859584 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.5962334871292114, loss=1.3862700462341309
I0216 23:09:01.741330 140318077859584 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6077380180358887, loss=1.4209331274032593
I0216 23:10:18.241706 140318069466880 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6174322366714478, loss=1.3770486116409302
I0216 23:11:34.718355 140318077859584 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.7116469740867615, loss=1.4458872079849243
I0216 23:11:48.173087 140399019657024 spec.py:321] Evaluating on the training split.
I0216 23:12:42.099725 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 23:13:32.934198 140399019657024 spec.py:349] Evaluating on the test split.
I0216 23:13:58.765455 140399019657024 submission_runner.py:408] Time since start: 45707.63s, 	Step: 52819, 	{'train/ctc_loss': Array(0.31450352, dtype=float32), 'train/wer': 0.11548549434380698, 'validation/ctc_loss': Array(0.587471, dtype=float32), 'validation/wer': 0.1810054355696728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3687143, dtype=float32), 'test/wer': 0.12524120000812464, 'test/num_examples': 2472, 'score': 41803.16771149635, 'total_duration': 45707.625512599945, 'accumulated_submission_time': 41803.16771149635, 'accumulated_eval_time': 3900.5444436073303, 'accumulated_logging_time': 1.651547908782959}
I0216 23:13:58.803214 140318077859584 logging_writer.py:48] [52819] accumulated_eval_time=3900.544444, accumulated_logging_time=1.651548, accumulated_submission_time=41803.167711, global_step=52819, preemption_count=0, score=41803.167711, test/ctc_loss=0.368714302778244, test/num_examples=2472, test/wer=0.125241, total_duration=45707.625513, train/ctc_loss=0.3145035207271576, train/wer=0.115485, validation/ctc_loss=0.5874710083007812, validation/num_examples=5348, validation/wer=0.181005
I0216 23:15:01.392060 140318069466880 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.6610875129699707, loss=1.403504729270935
I0216 23:16:18.030892 140318077859584 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.680152177810669, loss=1.400097131729126
I0216 23:17:34.730218 140318069466880 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.674371600151062, loss=1.4218716621398926
I0216 23:18:51.482244 140318077859584 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.611299991607666, loss=1.411020040512085
I0216 23:20:08.203669 140318069466880 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.6394579410552979, loss=1.359480619430542
I0216 23:21:30.556822 140318077859584 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.5882161259651184, loss=1.4365167617797852
I0216 23:22:52.679364 140318069466880 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7746914029121399, loss=1.448016881942749
I0216 23:24:15.656355 140318077859584 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8125666379928589, loss=1.4033734798431396
I0216 23:25:32.620448 140318069466880 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.744697630405426, loss=1.3976856470108032
I0216 23:26:49.092131 140318077859584 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.5994197130203247, loss=1.3920315504074097
I0216 23:28:05.827510 140318069466880 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7549394369125366, loss=1.457018494606018
I0216 23:29:22.586861 140318077859584 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.7125042080879211, loss=1.4325916767120361
I0216 23:30:39.101371 140318069466880 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.7436382174491882, loss=1.4366430044174194
I0216 23:31:56.686003 140318077859584 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.6613109707832336, loss=1.4144965410232544
I0216 23:33:20.428267 140318069466880 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.6989790797233582, loss=1.3739056587219238
I0216 23:34:43.145155 140318077859584 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.6896942853927612, loss=1.4124531745910645
I0216 23:36:05.723440 140318069466880 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.6060500144958496, loss=1.446781039237976
I0216 23:37:32.622731 140318077859584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.588360607624054, loss=1.350591778755188
I0216 23:37:59.031095 140399019657024 spec.py:321] Evaluating on the training split.
I0216 23:38:53.679804 140399019657024 spec.py:333] Evaluating on the validation split.
I0216 23:39:45.081380 140399019657024 spec.py:349] Evaluating on the test split.
I0216 23:40:11.101756 140399019657024 submission_runner.py:408] Time since start: 47279.96s, 	Step: 54636, 	{'train/ctc_loss': Array(0.29329264, dtype=float32), 'train/wer': 0.10794605852775967, 'validation/ctc_loss': Array(0.56704783, dtype=float32), 'validation/wer': 0.17521264373364742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35127002, dtype=float32), 'test/wer': 0.1210367030243942, 'test/num_examples': 2472, 'score': 43243.308198451996, 'total_duration': 47279.961285591125, 'accumulated_submission_time': 43243.308198451996, 'accumulated_eval_time': 4032.609624862671, 'accumulated_logging_time': 1.7056090831756592}
I0216 23:40:11.147890 140318077859584 logging_writer.py:48] [54636] accumulated_eval_time=4032.609625, accumulated_logging_time=1.705609, accumulated_submission_time=43243.308198, global_step=54636, preemption_count=0, score=43243.308198, test/ctc_loss=0.35127002000808716, test/num_examples=2472, test/wer=0.121037, total_duration=47279.961286, train/ctc_loss=0.2932926416397095, train/wer=0.107946, validation/ctc_loss=0.5670478343963623, validation/num_examples=5348, validation/wer=0.175213
I0216 23:41:00.756847 140318069466880 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.9011544585227966, loss=1.3999489545822144
I0216 23:42:17.327575 140318077859584 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.7262571454048157, loss=1.3999096155166626
I0216 23:43:34.088748 140318069466880 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6211032271385193, loss=1.3775835037231445
I0216 23:44:50.571526 140318077859584 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6045776605606079, loss=1.386926293373108
I0216 23:46:06.956775 140318069466880 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8100683093070984, loss=1.3360999822616577
I0216 23:47:23.511082 140318077859584 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7865427136421204, loss=1.4156581163406372
I0216 23:48:40.160634 140318069466880 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.6989081501960754, loss=1.3802053928375244
I0216 23:50:00.783460 140318077859584 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.7876555323600769, loss=1.3919516801834106
I0216 23:51:24.498293 140318069466880 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6065444350242615, loss=1.3893074989318848
I0216 23:52:47.129330 140318077859584 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.6473718285560608, loss=1.318069577217102
I0216 23:54:07.866911 140318077859584 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.724524736404419, loss=1.4129855632781982
I0216 23:55:24.508082 140318069466880 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6630906462669373, loss=1.3506830930709839
I0216 23:56:41.082353 140318077859584 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.6587861776351929, loss=1.4191803932189941
I0216 23:57:57.520598 140318069466880 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.5957654118537903, loss=1.3743137121200562
I0216 23:59:14.400157 140318077859584 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.8675460815429688, loss=1.3340712785720825
I0217 00:00:31.000066 140318069466880 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.6961351037025452, loss=1.4163856506347656
I0217 00:01:52.702549 140318077859584 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6343739032745361, loss=1.386849284172058
I0217 00:03:16.560857 140318069466880 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.6880417466163635, loss=1.3470139503479004
I0217 00:04:11.834555 140399019657024 spec.py:321] Evaluating on the training split.
I0217 00:05:07.251724 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 00:05:58.654315 140399019657024 spec.py:349] Evaluating on the test split.
I0217 00:06:24.847100 140399019657024 submission_runner.py:408] Time since start: 48853.71s, 	Step: 56468, 	{'train/ctc_loss': Array(0.2854557, dtype=float32), 'train/wer': 0.10497089164045106, 'validation/ctc_loss': Array(0.5525805, dtype=float32), 'validation/wer': 0.16984465663226392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33652857, dtype=float32), 'test/wer': 0.11801027765929356, 'test/num_examples': 2472, 'score': 44683.90547633171, 'total_duration': 48853.7058467865, 'accumulated_submission_time': 44683.90547633171, 'accumulated_eval_time': 4165.615864276886, 'accumulated_logging_time': 1.7686245441436768}
I0217 00:06:24.965861 140318077859584 logging_writer.py:48] [56468] accumulated_eval_time=4165.615864, accumulated_logging_time=1.768625, accumulated_submission_time=44683.905476, global_step=56468, preemption_count=0, score=44683.905476, test/ctc_loss=0.33652856945991516, test/num_examples=2472, test/wer=0.118010, total_duration=48853.705847, train/ctc_loss=0.28545570373535156, train/wer=0.104971, validation/ctc_loss=0.5525804758071899, validation/num_examples=5348, validation/wer=0.169845
I0217 00:06:50.211422 140318069466880 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.7122464776039124, loss=1.3617392778396606
I0217 00:08:06.676627 140318077859584 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.7218477725982666, loss=1.3818695545196533
I0217 00:09:26.725814 140318077859584 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.6030175685882568, loss=1.3129760026931763
I0217 00:10:43.062561 140318069466880 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.7091848254203796, loss=1.3619569540023804
I0217 00:11:59.397172 140318077859584 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7566845417022705, loss=1.36043381690979
I0217 00:13:16.071445 140318069466880 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7023253440856934, loss=1.3251172304153442
I0217 00:14:32.780256 140318077859584 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.7735983729362488, loss=1.3628486394882202
I0217 00:15:49.322439 140318069466880 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.714228630065918, loss=1.3689320087432861
I0217 00:17:12.079941 140318077859584 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.6855915784835815, loss=1.3332751989364624
I0217 00:18:36.544523 140318069466880 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.7511267066001892, loss=1.416284203529358
I0217 00:20:00.252272 140318077859584 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.7310926914215088, loss=1.3838999271392822
I0217 00:21:24.120544 140318069466880 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.6950824856758118, loss=1.3960708379745483
I0217 00:22:48.603287 140318077859584 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.6315636038780212, loss=1.3064013719558716
I0217 00:24:04.992536 140318069466880 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.6920550465583801, loss=1.288862943649292
I0217 00:25:21.427083 140318077859584 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.8746942281723022, loss=1.320548415184021
I0217 00:26:38.035803 140318069466880 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.6879971623420715, loss=1.3060325384140015
I0217 00:27:54.552261 140318077859584 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8640822768211365, loss=1.329245924949646
I0217 00:29:11.161062 140318069466880 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.6348674893379211, loss=1.3692214488983154
I0217 00:30:25.486686 140399019657024 spec.py:321] Evaluating on the training split.
I0217 00:31:20.816641 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 00:32:12.520778 140399019657024 spec.py:349] Evaluating on the test split.
I0217 00:32:38.534052 140399019657024 submission_runner.py:408] Time since start: 50427.39s, 	Step: 58296, 	{'train/ctc_loss': Array(0.25223732, dtype=float32), 'train/wer': 0.09496106685760315, 'validation/ctc_loss': Array(0.5288609, dtype=float32), 'validation/wer': 0.16171543875570832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32527593, dtype=float32), 'test/wer': 0.11073873215119939, 'test/num_examples': 2472, 'score': 46124.3394947052, 'total_duration': 50427.39268708229, 'accumulated_submission_time': 46124.3394947052, 'accumulated_eval_time': 4298.656836748123, 'accumulated_logging_time': 1.9026412963867188}
I0217 00:32:38.578823 140318077859584 logging_writer.py:48] [58296] accumulated_eval_time=4298.656837, accumulated_logging_time=1.902641, accumulated_submission_time=46124.339495, global_step=58296, preemption_count=0, score=46124.339495, test/ctc_loss=0.3252759277820587, test/num_examples=2472, test/wer=0.110739, total_duration=50427.392687, train/ctc_loss=0.25223731994628906, train/wer=0.094961, validation/ctc_loss=0.5288609266281128, validation/num_examples=5348, validation/wer=0.161715
I0217 00:32:42.488552 140318069466880 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.7700087428092957, loss=1.388235330581665
I0217 00:33:59.215045 140318077859584 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.6695617437362671, loss=1.2821905612945557
I0217 00:35:15.701544 140318069466880 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.6768844127655029, loss=1.3158987760543823
I0217 00:36:32.390535 140318077859584 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.8134163022041321, loss=1.3202400207519531
I0217 00:37:48.944145 140318069466880 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.7158609628677368, loss=1.3402594327926636
I0217 00:39:08.847786 140318077859584 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7308234572410583, loss=1.3505990505218506
I0217 00:40:25.333824 140318069466880 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7141064405441284, loss=1.3265266418457031
I0217 00:41:41.787280 140318077859584 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.7387362122535706, loss=1.3295011520385742
I0217 00:42:58.354374 140318069466880 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6992409825325012, loss=1.3545676469802856
I0217 00:44:14.921581 140318077859584 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.8910446166992188, loss=1.350646734237671
I0217 00:45:31.491876 140318069466880 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8618992567062378, loss=1.362899661064148
I0217 00:46:54.540803 140318077859584 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.6864007115364075, loss=1.3719309568405151
I0217 00:48:16.966669 140318069466880 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7358430624008179, loss=1.331986904144287
I0217 00:49:39.763072 140318077859584 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.8264341354370117, loss=1.3815560340881348
I0217 00:51:01.894752 140318069466880 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.710252046585083, loss=1.2951104640960693
I0217 00:52:24.117716 140318077859584 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.6878033876419067, loss=1.26282799243927
I0217 00:53:40.524651 140318069466880 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.6957873702049255, loss=1.30385422706604
I0217 00:54:56.961644 140318077859584 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.78715580701828, loss=1.2779839038848877
I0217 00:56:13.439076 140318069466880 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.8601160049438477, loss=1.2879397869110107
I0217 00:56:39.102339 140399019657024 spec.py:321] Evaluating on the training split.
I0217 00:57:33.565752 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 00:58:24.908078 140399019657024 spec.py:349] Evaluating on the test split.
I0217 00:58:50.895200 140399019657024 submission_runner.py:408] Time since start: 51999.75s, 	Step: 60135, 	{'train/ctc_loss': Array(0.2326758, dtype=float32), 'train/wer': 0.08732094034687694, 'validation/ctc_loss': Array(0.5112117, dtype=float32), 'validation/wer': 0.1576701391235506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30733588, dtype=float32), 'test/wer': 0.10464525826173501, 'test/num_examples': 2472, 'score': 47564.77532219887, 'total_duration': 51999.75342416763, 'accumulated_submission_time': 47564.77532219887, 'accumulated_eval_time': 4430.442895412445, 'accumulated_logging_time': 1.963092565536499}
I0217 00:58:50.938465 140318077859584 logging_writer.py:48] [60135] accumulated_eval_time=4430.442895, accumulated_logging_time=1.963093, accumulated_submission_time=47564.775322, global_step=60135, preemption_count=0, score=47564.775322, test/ctc_loss=0.30733588337898254, test/num_examples=2472, test/wer=0.104645, total_duration=51999.753424, train/ctc_loss=0.23267580568790436, train/wer=0.087321, validation/ctc_loss=0.5112116932868958, validation/num_examples=5348, validation/wer=0.157670
I0217 00:59:41.266261 140318069466880 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.7414891719818115, loss=1.3113871812820435
I0217 01:00:57.674302 140318077859584 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.6869248151779175, loss=1.336320400238037
I0217 01:02:14.360779 140318069466880 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.8337653279304504, loss=1.3203755617141724
I0217 01:03:30.751744 140318077859584 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.7177907228469849, loss=1.3048322200775146
I0217 01:04:48.590295 140318069466880 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.8090581893920898, loss=1.2947254180908203
I0217 01:06:11.111963 140318077859584 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.6886002421379089, loss=1.2774475812911987
I0217 01:07:35.319710 140318077859584 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.7008119225502014, loss=1.3141082525253296
I0217 01:08:51.825542 140318069466880 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7872861623764038, loss=1.249367356300354
I0217 01:10:08.386758 140318077859584 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.8386160731315613, loss=1.3061615228652954
I0217 01:11:25.166683 140318069466880 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.7897480726242065, loss=1.2935614585876465
I0217 01:12:41.819033 140318077859584 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.8604633212089539, loss=1.2759346961975098
I0217 01:13:58.463914 140318069466880 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7785679697990417, loss=1.3072561025619507
I0217 01:15:17.657507 140318077859584 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.7369140386581421, loss=1.324047327041626
I0217 01:16:39.995230 140318069466880 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.8277222514152527, loss=1.3175383806228638
I0217 01:18:02.874240 140318077859584 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.780157744884491, loss=1.23556649684906
I0217 01:19:26.722449 140318069466880 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.7055259346961975, loss=1.29058837890625
I0217 01:20:53.122095 140318077859584 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.7039667367935181, loss=1.3264378309249878
I0217 01:22:09.623266 140318069466880 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.6632777452468872, loss=1.249269723892212
I0217 01:22:50.916610 140399019657024 spec.py:321] Evaluating on the training split.
I0217 01:23:46.173999 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 01:24:37.524406 140399019657024 spec.py:349] Evaluating on the test split.
I0217 01:25:03.425359 140399019657024 submission_runner.py:408] Time since start: 53572.28s, 	Step: 61955, 	{'train/ctc_loss': Array(0.21656099, dtype=float32), 'train/wer': 0.08025868627948263, 'validation/ctc_loss': Array(0.49202898, dtype=float32), 'validation/wer': 0.15026502022649815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29639518, dtype=float32), 'test/wer': 0.10021733390205756, 'test/num_examples': 2472, 'score': 49004.667399168015, 'total_duration': 53572.284828424454, 'accumulated_submission_time': 49004.667399168015, 'accumulated_eval_time': 4562.946071147919, 'accumulated_logging_time': 2.0217151641845703}
I0217 01:25:03.468086 140318077859584 logging_writer.py:48] [61955] accumulated_eval_time=4562.946071, accumulated_logging_time=2.021715, accumulated_submission_time=49004.667399, global_step=61955, preemption_count=0, score=49004.667399, test/ctc_loss=0.2963951826095581, test/num_examples=2472, test/wer=0.100217, total_duration=53572.284828, train/ctc_loss=0.2165609896183014, train/wer=0.080259, validation/ctc_loss=0.49202898144721985, validation/num_examples=5348, validation/wer=0.150265
I0217 01:25:38.520735 140318069466880 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.7505834698677063, loss=1.286289930343628
I0217 01:26:55.029925 140318077859584 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.8650599718093872, loss=1.2342225313186646
I0217 01:28:11.777279 140318069466880 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.036153793334961, loss=1.2645564079284668
I0217 01:29:28.580967 140318077859584 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.049433708190918, loss=1.229975700378418
I0217 01:30:45.268875 140318069466880 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7800342440605164, loss=1.294792890548706
I0217 01:32:01.973202 140318077859584 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.7735604643821716, loss=1.2938297986984253
I0217 01:33:20.034052 140318069466880 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7744523882865906, loss=1.2877434492111206
I0217 01:34:42.218193 140318077859584 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.7482393383979797, loss=1.2977114915847778
I0217 01:36:04.552005 140318069466880 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.889630913734436, loss=1.2679530382156372
I0217 01:37:26.470670 140318077859584 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.8863282799720764, loss=1.2555558681488037
I0217 01:38:42.937495 140318069466880 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.7221459150314331, loss=1.2382569313049316
I0217 01:39:59.830592 140318077859584 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.7959858179092407, loss=1.2351012229919434
I0217 01:41:16.494442 140318069466880 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.8212536573410034, loss=1.2943793535232544
I0217 01:42:33.129586 140318077859584 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.8200283646583557, loss=1.2457948923110962
I0217 01:43:49.809568 140318069466880 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.766659677028656, loss=1.2728084325790405
I0217 01:45:10.943318 140318077859584 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8279948830604553, loss=1.2393420934677124
I0217 01:46:34.047247 140318069466880 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8163345456123352, loss=1.2732863426208496
I0217 01:47:56.367223 140318077859584 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.713254451751709, loss=1.2434368133544922
I0217 01:49:03.718304 140399019657024 spec.py:321] Evaluating on the training split.
I0217 01:49:58.076390 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 01:50:49.484488 140399019657024 spec.py:349] Evaluating on the test split.
I0217 01:51:15.830664 140399019657024 submission_runner.py:408] Time since start: 55144.69s, 	Step: 63784, 	{'train/ctc_loss': Array(0.21301003, dtype=float32), 'train/wer': 0.07957331428293095, 'validation/ctc_loss': Array(0.4686298, dtype=float32), 'validation/wer': 0.1450225436148952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28154856, dtype=float32), 'test/wer': 0.09633782219243191, 'test/num_examples': 2472, 'score': 50444.830134153366, 'total_duration': 55144.689125299454, 'accumulated_submission_time': 50444.830134153366, 'accumulated_eval_time': 4695.051886081696, 'accumulated_logging_time': 2.079317331314087}
I0217 01:51:15.880920 140318077859584 logging_writer.py:48] [63784] accumulated_eval_time=4695.051886, accumulated_logging_time=2.079317, accumulated_submission_time=50444.830134, global_step=63784, preemption_count=0, score=50444.830134, test/ctc_loss=0.28154855966567993, test/num_examples=2472, test/wer=0.096338, total_duration=55144.689125, train/ctc_loss=0.2130100280046463, train/wer=0.079573, validation/ctc_loss=0.4686298072338104, validation/num_examples=5348, validation/wer=0.145023
I0217 01:51:28.901419 140318069466880 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.7684534788131714, loss=1.263310194015503
I0217 01:52:48.857156 140318077859584 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.8467965126037598, loss=1.2268764972686768
I0217 01:54:05.489786 140318069466880 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.7420194745063782, loss=1.243384838104248
I0217 01:55:22.168417 140318077859584 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.8295000195503235, loss=1.2598693370819092
I0217 01:56:38.824669 140318069466880 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.7987565994262695, loss=1.2598315477371216
I0217 01:57:55.665071 140318077859584 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.8320934772491455, loss=1.260114073753357
I0217 01:59:12.357347 140318069466880 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9072144627571106, loss=1.2613286972045898
I0217 02:00:29.332515 140318077859584 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.7381200194358826, loss=1.2002300024032593
I0217 02:01:53.168015 140318069466880 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.91705322265625, loss=1.213093876838684
I0217 02:03:15.723755 140318077859584 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.8209591507911682, loss=1.3176450729370117
I0217 02:04:38.073089 140318069466880 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.8204345703125, loss=1.2110040187835693
I0217 02:06:04.116518 140318077859584 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.873225212097168, loss=1.2535555362701416
I0217 02:07:20.753153 140318069466880 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.8060597777366638, loss=1.2265311479568481
I0217 02:08:37.371259 140318077859584 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.7053632736206055, loss=1.2537627220153809
I0217 02:09:54.100093 140318069466880 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.881904125213623, loss=1.199938416481018
I0217 02:11:10.667166 140318077859584 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.928612470626831, loss=1.2311975955963135
I0217 02:12:27.369973 140318069466880 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.6875888705253601, loss=1.1939582824707031
I0217 02:13:44.437645 140318077859584 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.8143144845962524, loss=1.1824384927749634
I0217 02:15:05.470049 140318069466880 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.8243593573570251, loss=1.2064218521118164
I0217 02:15:16.400305 140399019657024 spec.py:321] Evaluating on the training split.
I0217 02:16:10.834903 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 02:17:02.772578 140399019657024 spec.py:349] Evaluating on the test split.
I0217 02:17:29.473953 140399019657024 submission_runner.py:408] Time since start: 56718.33s, 	Step: 65615, 	{'train/ctc_loss': Array(0.18987854, dtype=float32), 'train/wer': 0.06969517767861944, 'validation/ctc_loss': Array(0.45930022, dtype=float32), 'validation/wer': 0.14046554737055525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27360567, dtype=float32), 'test/wer': 0.09225519468649077, 'test/num_examples': 2472, 'score': 51885.259873628616, 'total_duration': 56718.33281850815, 'accumulated_submission_time': 51885.259873628616, 'accumulated_eval_time': 4828.119354486465, 'accumulated_logging_time': 2.1474575996398926}
I0217 02:17:29.522595 140318077859584 logging_writer.py:48] [65615] accumulated_eval_time=4828.119354, accumulated_logging_time=2.147458, accumulated_submission_time=51885.259874, global_step=65615, preemption_count=0, score=51885.259874, test/ctc_loss=0.27360567450523376, test/num_examples=2472, test/wer=0.092255, total_duration=56718.332819, train/ctc_loss=0.18987853825092316, train/wer=0.069695, validation/ctc_loss=0.4593002200126648, validation/num_examples=5348, validation/wer=0.140466
I0217 02:18:35.177339 140318069466880 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.7942513823509216, loss=1.2542650699615479
I0217 02:19:51.661436 140318077859584 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.9941204786300659, loss=1.218614935874939
I0217 02:21:08.423566 140318069466880 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8645747303962708, loss=1.2644155025482178
I0217 02:22:28.504419 140318077859584 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.8346899747848511, loss=1.2182888984680176
I0217 02:23:44.968869 140318069466880 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.7737469673156738, loss=1.2160385847091675
I0217 02:25:01.375106 140318077859584 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.7973622679710388, loss=1.2030035257339478
I0217 02:26:17.843634 140318069466880 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.6961489915847778, loss=1.1756852865219116
I0217 02:27:34.276087 140318077859584 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7493355870246887, loss=1.2272244691848755
I0217 02:28:51.047243 140318069466880 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8434037566184998, loss=1.2003930807113647
I0217 02:30:13.701008 140318077859584 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.0452592372894287, loss=1.2433477640151978
I0217 02:31:36.618974 140318069466880 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.8287253379821777, loss=1.164771318435669
I0217 02:32:59.741671 140318077859584 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.9874321222305298, loss=1.181631088256836
I0217 02:34:21.618880 140318069466880 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.8564304113388062, loss=1.241716980934143
I0217 02:35:43.932555 140318077859584 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8430660963058472, loss=1.1905860900878906
I0217 02:37:00.602060 140318069466880 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.8592970371246338, loss=1.178748369216919
I0217 02:38:17.284253 140318077859584 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.8180535435676575, loss=1.1912579536437988
I0217 02:39:34.084520 140318069466880 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.8596141934394836, loss=1.197411298751831
I0217 02:40:50.751361 140318077859584 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.9645732641220093, loss=1.1514915227890015
I0217 02:41:29.571619 140399019657024 spec.py:321] Evaluating on the training split.
I0217 02:42:24.547967 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 02:43:16.713997 140399019657024 spec.py:349] Evaluating on the test split.
I0217 02:43:42.683160 140399019657024 submission_runner.py:408] Time since start: 58291.54s, 	Step: 67452, 	{'train/ctc_loss': Array(0.18605858, dtype=float32), 'train/wer': 0.0703033976144076, 'validation/ctc_loss': Array(0.43531832, dtype=float32), 'validation/wer': 0.1353389265956728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2592684, dtype=float32), 'test/wer': 0.08819287876018118, 'test/num_examples': 2472, 'score': 53325.22155690193, 'total_duration': 58291.54190802574, 'accumulated_submission_time': 53325.22155690193, 'accumulated_eval_time': 4961.224667787552, 'accumulated_logging_time': 2.211995840072632}
I0217 02:43:42.733427 140318077859584 logging_writer.py:48] [67452] accumulated_eval_time=4961.224668, accumulated_logging_time=2.211996, accumulated_submission_time=53325.221557, global_step=67452, preemption_count=0, score=53325.221557, test/ctc_loss=0.2592684030532837, test/num_examples=2472, test/wer=0.088193, total_duration=58291.541908, train/ctc_loss=0.18605858087539673, train/wer=0.070303, validation/ctc_loss=0.43531832098960876, validation/num_examples=5348, validation/wer=0.135339
I0217 02:44:20.121484 140318069466880 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.7414058446884155, loss=1.2016521692276
I0217 02:45:36.563071 140318077859584 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.8467651605606079, loss=1.2302494049072266
I0217 02:46:53.178365 140318069466880 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.9384753704071045, loss=1.170484185218811
I0217 02:48:10.061580 140318077859584 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.9001181125640869, loss=1.1709097623825073
I0217 02:49:26.577408 140318069466880 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.7979869246482849, loss=1.1812413930892944
I0217 02:50:47.880354 140318077859584 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8295623064041138, loss=1.117010235786438
I0217 02:52:04.384306 140318069466880 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.8335073590278625, loss=1.2113826274871826
I0217 02:53:20.909543 140318077859584 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.861870527267456, loss=1.1592165231704712
I0217 02:54:37.542913 140318069466880 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.8023298978805542, loss=1.1233071088790894
I0217 02:55:54.143727 140318077859584 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.7724860906600952, loss=1.1291354894638062
I0217 02:57:10.744679 140318069466880 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.8807806372642517, loss=1.1592155694961548
I0217 02:58:27.377982 140318077859584 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7586756944656372, loss=1.1538565158843994
I0217 02:59:49.146608 140318069466880 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.7916499972343445, loss=1.1275192499160767
I0217 03:01:12.552902 140318077859584 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.7948573231697083, loss=1.1514135599136353
I0217 03:02:36.163684 140318069466880 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.9744678139686584, loss=1.1923949718475342
I0217 03:03:59.712466 140318077859584 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.7403018474578857, loss=1.142475962638855
I0217 03:05:19.933306 140318077859584 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.8255773782730103, loss=1.151144027709961
I0217 03:06:36.581678 140318069466880 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.7284367084503174, loss=1.1377614736557007
I0217 03:07:42.857440 140399019657024 spec.py:321] Evaluating on the training split.
I0217 03:08:37.395529 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 03:09:28.707131 140399019657024 spec.py:349] Evaluating on the test split.
I0217 03:09:54.660966 140399019657024 submission_runner.py:408] Time since start: 59863.52s, 	Step: 69288, 	{'train/ctc_loss': Array(0.1981465, dtype=float32), 'train/wer': 0.06849597580193119, 'validation/ctc_loss': Array(0.42125234, dtype=float32), 'validation/wer': 0.12994197553510914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24975313, dtype=float32), 'test/wer': 0.08547112708955376, 'test/num_examples': 2472, 'score': 54765.25285768509, 'total_duration': 59863.520375967026, 'accumulated_submission_time': 54765.25285768509, 'accumulated_eval_time': 5093.022603034973, 'accumulated_logging_time': 2.2826132774353027}
I0217 03:09:54.707880 140318077859584 logging_writer.py:48] [69288] accumulated_eval_time=5093.022603, accumulated_logging_time=2.282613, accumulated_submission_time=54765.252858, global_step=69288, preemption_count=0, score=54765.252858, test/ctc_loss=0.24975313246250153, test/num_examples=2472, test/wer=0.085471, total_duration=59863.520376, train/ctc_loss=0.1981465071439743, train/wer=0.068496, validation/ctc_loss=0.4212523400783539, validation/num_examples=5348, validation/wer=0.129942
I0217 03:10:04.659145 140318069466880 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.773081362247467, loss=1.1199887990951538
I0217 03:11:21.112314 140318077859584 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.9792198538780212, loss=1.208099126815796
I0217 03:12:37.392600 140318069466880 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.8492528796195984, loss=1.1635230779647827
I0217 03:13:53.965041 140318077859584 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.9727568030357361, loss=1.1660383939743042
I0217 03:15:10.559298 140318069466880 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.8199180960655212, loss=1.1671427488327026
I0217 03:16:27.192871 140318077859584 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.8387100696563721, loss=1.1748336553573608
I0217 03:17:50.857135 140318069466880 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.868910551071167, loss=1.1488715410232544
I0217 03:19:14.151967 140318077859584 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.8281271457672119, loss=1.116245150566101
I0217 03:20:37.048316 140318077859584 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.8074803948402405, loss=1.1657025814056396
I0217 03:21:53.988058 140318069466880 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.9842815399169922, loss=1.1232908964157104
I0217 03:23:10.431396 140318077859584 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.8919305205345154, loss=1.148029088973999
I0217 03:24:27.028316 140318069466880 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.843220055103302, loss=1.1690514087677002
I0217 03:25:43.556375 140318077859584 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.9410386085510254, loss=1.1116461753845215
I0217 03:26:59.994737 140318069466880 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.8596023917198181, loss=1.1626685857772827
I0217 03:28:20.477230 140318077859584 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0358357429504395, loss=1.1326407194137573
I0217 03:29:43.688135 140318069466880 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.8719261884689331, loss=1.1103605031967163
I0217 03:31:06.347678 140318077859584 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.9321148991584778, loss=1.1318438053131104
I0217 03:32:30.259282 140318069466880 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.9858232140541077, loss=1.1502020359039307
I0217 03:33:54.321620 140318077859584 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.8754363656044006, loss=1.1184430122375488
I0217 03:33:54.814972 140399019657024 spec.py:321] Evaluating on the training split.
I0217 03:34:50.593270 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 03:35:41.981000 140399019657024 spec.py:349] Evaluating on the test split.
I0217 03:36:08.069228 140399019657024 submission_runner.py:408] Time since start: 61436.93s, 	Step: 71102, 	{'train/ctc_loss': Array(0.13899502, dtype=float32), 'train/wer': 0.05172848981862576, 'validation/ctc_loss': Array(0.41104484, dtype=float32), 'validation/wer': 0.12531739671934888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23893896, dtype=float32), 'test/wer': 0.0804947900798245, 'test/num_examples': 2472, 'score': 56205.272963523865, 'total_duration': 61436.927609205246, 'accumulated_submission_time': 56205.272963523865, 'accumulated_eval_time': 5226.270207643509, 'accumulated_logging_time': 2.345404863357544}
I0217 03:36:08.123206 140318077859584 logging_writer.py:48] [71102] accumulated_eval_time=5226.270208, accumulated_logging_time=2.345405, accumulated_submission_time=56205.272964, global_step=71102, preemption_count=0, score=56205.272964, test/ctc_loss=0.23893895745277405, test/num_examples=2472, test/wer=0.080495, total_duration=61436.927609, train/ctc_loss=0.13899502158164978, train/wer=0.051728, validation/ctc_loss=0.4110448360443115, validation/num_examples=5348, validation/wer=0.125317
I0217 03:37:23.723926 140318069466880 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.923180341720581, loss=1.1390055418014526
I0217 03:38:40.538928 140318077859584 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.9286639094352722, loss=1.110034704208374
I0217 03:39:57.230101 140318069466880 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.8240388631820679, loss=1.1366021633148193
I0217 03:41:13.901732 140318077859584 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.8942087292671204, loss=1.1398597955703735
I0217 03:42:30.601826 140318069466880 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.8549559116363525, loss=1.16095769405365
I0217 03:43:47.191370 140318077859584 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.8039098978042603, loss=1.1212486028671265
I0217 03:45:03.853101 140318069466880 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.8679946660995483, loss=1.1101267337799072
I0217 03:46:23.459840 140318077859584 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.9931250214576721, loss=1.158257246017456
I0217 03:47:46.092744 140318069466880 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.8440722227096558, loss=1.1026016473770142
I0217 03:49:12.677829 140318077859584 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.7855169177055359, loss=1.0879777669906616
I0217 03:50:29.162973 140318069466880 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.9407644867897034, loss=1.0673907995224
I0217 03:51:45.934051 140318077859584 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.9216168522834778, loss=1.10074782371521
I0217 03:53:02.550392 140318069466880 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.84110426902771, loss=1.1458231210708618
I0217 03:54:19.451446 140318077859584 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.8235985636711121, loss=1.0981709957122803
I0217 03:55:35.983443 140318069466880 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.010286808013916, loss=1.1400443315505981
I0217 03:56:52.586779 140318077859584 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.76727694272995, loss=1.1013039350509644
I0217 03:58:12.714397 140318069466880 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.9694185853004456, loss=1.10981023311615
I0217 03:59:36.193962 140318077859584 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.399838924407959, loss=1.1038568019866943
I0217 04:00:08.478197 140399019657024 spec.py:321] Evaluating on the training split.
I0217 04:01:03.677679 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 04:01:55.022889 140399019657024 spec.py:349] Evaluating on the test split.
I0217 04:02:20.945750 140399019657024 submission_runner.py:408] Time since start: 63009.80s, 	Step: 72941, 	{'train/ctc_loss': Array(0.13666794, dtype=float32), 'train/wer': 0.050745426327423224, 'validation/ctc_loss': Array(0.39726907, dtype=float32), 'validation/wer': 0.1221699798217751, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2305465, dtype=float32), 'test/wer': 0.07706213312209291, 'test/num_examples': 2472, 'score': 57645.54037809372, 'total_duration': 63009.80436420441, 'accumulated_submission_time': 57645.54037809372, 'accumulated_eval_time': 5358.731348514557, 'accumulated_logging_time': 2.414510488510132}
I0217 04:02:20.990053 140318077859584 logging_writer.py:48] [72941] accumulated_eval_time=5358.731349, accumulated_logging_time=2.414510, accumulated_submission_time=57645.540378, global_step=72941, preemption_count=0, score=57645.540378, test/ctc_loss=0.2305465042591095, test/num_examples=2472, test/wer=0.077062, total_duration=63009.804364, train/ctc_loss=0.13666793704032898, train/wer=0.050745, validation/ctc_loss=0.397269070148468, validation/num_examples=5348, validation/wer=0.122170
I0217 04:03:06.802992 140318069466880 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.8181557655334473, loss=1.069453239440918
I0217 04:04:23.473032 140318077859584 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.048447608947754, loss=1.1366649866104126
I0217 04:05:43.431398 140318077859584 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.95489501953125, loss=1.1431804895401
I0217 04:06:59.888277 140318069466880 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.964716911315918, loss=1.1332188844680786
I0217 04:08:16.648636 140318077859584 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.9798088669776917, loss=1.1399792432785034
I0217 04:09:33.397294 140318069466880 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.0270953178405762, loss=1.100795865058899
I0217 04:10:49.987103 140318077859584 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9547717571258545, loss=1.0840187072753906
I0217 04:12:06.743575 140318069466880 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.9089665412902832, loss=1.0880811214447021
I0217 04:13:27.396919 140318077859584 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.8477736115455627, loss=1.1071875095367432
I0217 04:14:50.257833 140318069466880 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.0739344358444214, loss=1.0887495279312134
I0217 04:16:13.386860 140318077859584 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.176677942276001, loss=1.107483983039856
I0217 04:17:36.264945 140318069466880 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.9789103865623474, loss=1.101093053817749
I0217 04:18:59.975622 140318077859584 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.8835875988006592, loss=1.0477485656738281
I0217 04:20:16.563189 140318069466880 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.945073127746582, loss=1.039287805557251
I0217 04:21:33.259752 140318077859584 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.8525701761245728, loss=1.0934629440307617
I0217 04:22:50.087978 140318069466880 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.9754416942596436, loss=1.1083769798278809
I0217 04:24:06.734791 140318077859584 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.8964645266532898, loss=1.0311046838760376
I0217 04:25:23.419575 140318069466880 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.8679556250572205, loss=1.0850696563720703
I0217 04:26:20.950726 140399019657024 spec.py:321] Evaluating on the training split.
I0217 04:27:15.065346 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 04:28:06.167163 140399019657024 spec.py:349] Evaluating on the test split.
I0217 04:28:32.309519 140399019657024 submission_runner.py:408] Time since start: 64581.17s, 	Step: 74775, 	{'train/ctc_loss': Array(0.16922173, dtype=float32), 'train/wer': 0.06387893051640622, 'validation/ctc_loss': Array(0.38635293, dtype=float32), 'validation/wer': 0.11791227782229645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22601831, dtype=float32), 'test/wer': 0.07606686572014705, 'test/num_examples': 2472, 'score': 59085.41255426407, 'total_duration': 64581.168640613556, 'accumulated_submission_time': 59085.41255426407, 'accumulated_eval_time': 5490.084238290787, 'accumulated_logging_time': 2.4744374752044678}
I0217 04:28:32.358045 140318077859584 logging_writer.py:48] [74775] accumulated_eval_time=5490.084238, accumulated_logging_time=2.474437, accumulated_submission_time=59085.412554, global_step=74775, preemption_count=0, score=59085.412554, test/ctc_loss=0.22601830959320068, test/num_examples=2472, test/wer=0.076067, total_duration=64581.168641, train/ctc_loss=0.16922172904014587, train/wer=0.063879, validation/ctc_loss=0.38635292649269104, validation/num_examples=5348, validation/wer=0.117912
I0217 04:28:52.207411 140318069466880 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2283563613891602, loss=1.0566734075546265
I0217 04:30:09.080599 140318077859584 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.8070144057273865, loss=1.0373735427856445
I0217 04:31:25.703758 140318069466880 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.9939883947372437, loss=1.1203913688659668
I0217 04:32:42.347438 140318077859584 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.0539004802703857, loss=1.087257742881775
I0217 04:34:02.566462 140318077859584 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.9859312772750854, loss=1.0761229991912842
I0217 04:35:18.947882 140318069466880 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.9106588363647461, loss=1.0968997478485107
I0217 04:36:35.634847 140318077859584 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.9473142623901367, loss=1.1126922369003296
I0217 04:37:52.430034 140318069466880 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0513107776641846, loss=1.1112011671066284
I0217 04:39:09.028460 140318077859584 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.8551008105278015, loss=1.0861177444458008
I0217 04:40:25.852610 140318069466880 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.9354265928268433, loss=1.0808602571487427
I0217 04:41:42.463495 140318077859584 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.9093264937400818, loss=1.075571060180664
I0217 04:43:04.233299 140318069466880 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.023676872253418, loss=1.0079939365386963
I0217 04:44:26.523592 140318077859584 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.9721314311027527, loss=1.0807087421417236
I0217 04:45:48.846902 140318069466880 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.0783789157867432, loss=1.1080782413482666
I0217 04:47:11.169780 140318077859584 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.0914472341537476, loss=1.091148853302002
I0217 04:48:31.969942 140318077859584 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.9273084998130798, loss=1.0793704986572266
I0217 04:49:48.523589 140318069466880 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.9638446569442749, loss=1.0497947931289673
I0217 04:51:04.957138 140318077859584 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.9304236769676208, loss=1.0409420728683472
I0217 04:52:21.671687 140318069466880 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.8864930868148804, loss=1.02815580368042
I0217 04:52:32.838694 140399019657024 spec.py:321] Evaluating on the training split.
I0217 04:53:26.494459 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 04:54:18.053834 140399019657024 spec.py:349] Evaluating on the test split.
I0217 04:54:44.028734 140399019657024 submission_runner.py:408] Time since start: 66152.89s, 	Step: 76616, 	{'train/ctc_loss': Array(0.1668892, dtype=float32), 'train/wer': 0.06057212775371304, 'validation/ctc_loss': Array(0.3823363, dtype=float32), 'validation/wer': 0.1158654913735675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22135468, dtype=float32), 'test/wer': 0.07438100461072858, 'test/num_examples': 2472, 'score': 60525.80380296707, 'total_duration': 66152.88785719872, 'accumulated_submission_time': 60525.80380296707, 'accumulated_eval_time': 5621.268357515335, 'accumulated_logging_time': 2.5391104221343994}
I0217 04:54:44.081938 140318077859584 logging_writer.py:48] [76616] accumulated_eval_time=5621.268358, accumulated_logging_time=2.539110, accumulated_submission_time=60525.803803, global_step=76616, preemption_count=0, score=60525.803803, test/ctc_loss=0.221354678273201, test/num_examples=2472, test/wer=0.074381, total_duration=66152.887857, train/ctc_loss=0.16688920557498932, train/wer=0.060572, validation/ctc_loss=0.382336288690567, validation/num_examples=5348, validation/wer=0.115865
I0217 04:55:48.984394 140318069466880 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.956818699836731, loss=1.077467679977417
I0217 04:57:05.481628 140318077859584 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0368988513946533, loss=1.067598819732666
I0217 04:58:21.906281 140318069466880 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.8687114715576172, loss=1.0583115816116333
I0217 04:59:38.286648 140318077859584 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.866843044757843, loss=1.0579707622528076
I0217 05:00:54.981355 140318069466880 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.0356873273849487, loss=1.0296885967254639
I0217 05:02:14.326021 140318077859584 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.9916616082191467, loss=1.0959378480911255
I0217 05:03:37.586734 140318077859584 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.9182778596878052, loss=1.0889925956726074
I0217 05:03:46.530398 140318069466880 logging_writer.py:48] [77313] global_step=77313, preemption_count=0, score=61068.185802
I0217 05:03:47.398055 140399019657024 checkpoints.py:490] Saving checkpoint at step: 77313
I0217 05:03:48.884802 140399019657024 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4/checkpoint_77313
I0217 05:03:48.916469 140399019657024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_4/checkpoint_77313.
I0217 05:03:52.316606 140399019657024 submission_runner.py:583] Tuning trial 4/5
I0217 05:03:52.316869 140399019657024 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0217 05:03:52.344154 140399019657024 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.906748, dtype=float32), 'train/wer': 1.3798149413691736, 'validation/ctc_loss': Array(31.163591, dtype=float32), 'validation/wer': 1.043146644525329, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.27949, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 34.95112609863281, 'total_duration': 170.38803958892822, 'accumulated_submission_time': 34.95112609863281, 'accumulated_eval_time': 135.43686819076538, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1789, {'train/ctc_loss': Array(6.4432144, dtype=float32), 'train/wer': 0.9413900245298447, 'validation/ctc_loss': Array(6.3511953, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.313715, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1475.3679308891296, 'total_duration': 1718.2228519916534, 'accumulated_submission_time': 1475.3679308891296, 'accumulated_eval_time': 242.74953532218933, 'accumulated_logging_time': 0.031334638595581055, 'global_step': 1789, 'preemption_count': 0}), (3605, {'train/ctc_loss': Array(6.894968, dtype=float32), 'train/wer': 0.9380587920410929, 'validation/ctc_loss': Array(6.6507244, dtype=float32), 'validation/wer': 0.8960097318902845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.4707236, dtype=float32), 'test/wer': 0.898990514492312, 'test/num_examples': 2472, 'score': 2915.763386487961, 'total_duration': 3266.4796857833862, 'accumulated_submission_time': 2915.763386487961, 'accumulated_eval_time': 350.4788267612457, 'accumulated_logging_time': 0.08646297454833984, 'global_step': 3605, 'preemption_count': 0}), (5421, {'train/ctc_loss': Array(3.7265964, dtype=float32), 'train/wer': 0.7729606676212637, 'validation/ctc_loss': Array(3.6029704, dtype=float32), 'validation/wer': 0.7371327611342287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.293352, dtype=float32), 'test/wer': 0.7108037292060203, 'test/num_examples': 2472, 'score': 4355.635868310928, 'total_duration': 4826.856513261795, 'accumulated_submission_time': 4355.635868310928, 'accumulated_eval_time': 470.84709548950195, 'accumulated_logging_time': 0.14482760429382324, 'global_step': 5421, 'preemption_count': 0}), (7224, {'train/ctc_loss': Array(1.7894272, dtype=float32), 'train/wer': 0.50639147729467, 'validation/ctc_loss': Array(1.8419197, dtype=float32), 'validation/wer': 0.48790754704229705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.4695274, dtype=float32), 'test/wer': 0.4360693031097028, 'test/num_examples': 2472, 'score': 5795.708523511887, 'total_duration': 6397.847925901413, 'accumulated_submission_time': 5795.708523511887, 'accumulated_eval_time': 601.6357429027557, 'accumulated_logging_time': 0.1990370750427246, 'global_step': 7224, 'preemption_count': 0}), (9046, {'train/ctc_loss': Array(1.5100107, dtype=float32), 'train/wer': 0.4490485031329775, 'validation/ctc_loss': Array(1.4825138, dtype=float32), 'validation/wer': 0.4142908174594746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1427629, dtype=float32), 'test/wer': 0.35490423090203727, 'test/num_examples': 2472, 'score': 7236.371410369873, 'total_duration': 7968.555783987045, 'accumulated_submission_time': 7236.371410369873, 'accumulated_eval_time': 731.5442821979523, 'accumulated_logging_time': 0.2577028274536133, 'global_step': 9046, 'preemption_count': 0}), (10864, {'train/ctc_loss': Array(1.2075412, dtype=float32), 'train/wer': 0.37236854396887586, 'validation/ctc_loss': Array(1.3426595, dtype=float32), 'validation/wer': 0.3834924741979397, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.98521, dtype=float32), 'test/wer': 0.31511384640383483, 'test/num_examples': 2472, 'score': 8676.504511356354, 'total_duration': 9540.403466939926, 'accumulated_submission_time': 8676.504511356354, 'accumulated_eval_time': 863.1278550624847, 'accumulated_logging_time': 0.31041407585144043, 'global_step': 10864, 'preemption_count': 0}), (12691, {'train/ctc_loss': Array(1.1640844, dtype=float32), 'train/wer': 0.35710851551231654, 'validation/ctc_loss': Array(1.218482, dtype=float32), 'validation/wer': 0.352742404201705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8880885, dtype=float32), 'test/wer': 0.2892775171125058, 'test/num_examples': 2472, 'score': 10116.434507131577, 'total_duration': 11110.8425116539, 'accumulated_submission_time': 10116.434507131577, 'accumulated_eval_time': 993.5067636966705, 'accumulated_logging_time': 0.3614308834075928, 'global_step': 12691, 'preemption_count': 0}), (14500, {'train/ctc_loss': Array(1.1236786, dtype=float32), 'train/wer': 0.3462997832604231, 'validation/ctc_loss': Array(1.16027, dtype=float32), 'validation/wer': 0.33419581567336376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8417588, dtype=float32), 'test/wer': 0.272825137610952, 'test/num_examples': 2472, 'score': 11556.422718286514, 'total_duration': 12681.722338914871, 'accumulated_submission_time': 11556.422718286514, 'accumulated_eval_time': 1124.2681069374084, 'accumulated_logging_time': 0.4147167205810547, 'global_step': 14500, 'preemption_count': 0}), (16323, {'train/ctc_loss': Array(1.0246935, dtype=float32), 'train/wer': 0.3256618002726353, 'validation/ctc_loss': Array(1.0873188, dtype=float32), 'validation/wer': 0.32038966179750333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7645127, dtype=float32), 'test/wer': 0.2534682022220868, 'test/num_examples': 2472, 'score': 12996.567366361618, 'total_duration': 14254.555345058441, 'accumulated_submission_time': 12996.567366361618, 'accumulated_eval_time': 1256.8274652957916, 'accumulated_logging_time': 0.46446824073791504, 'global_step': 16323, 'preemption_count': 0}), (18144, {'train/ctc_loss': Array(0.91025984, dtype=float32), 'train/wer': 0.29429087216518107, 'validation/ctc_loss': Array(1.035664, dtype=float32), 'validation/wer': 0.3064097241665621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7306505, dtype=float32), 'test/wer': 0.24229683342473543, 'test/num_examples': 2472, 'score': 14436.753766775131, 'total_duration': 15826.1676633358, 'accumulated_submission_time': 14436.753766775131, 'accumulated_eval_time': 1388.1240470409393, 'accumulated_logging_time': 0.5152781009674072, 'global_step': 18144, 'preemption_count': 0}), (19977, {'train/ctc_loss': Array(0.83924824, dtype=float32), 'train/wer': 0.27583577477224386, 'validation/ctc_loss': Array(0.9875356, dtype=float32), 'validation/wer': 0.2946600113924906, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.68959343, dtype=float32), 'test/wer': 0.23031300144212216, 'test/num_examples': 2472, 'score': 15877.275433778763, 'total_duration': 17397.07205915451, 'accumulated_submission_time': 15877.275433778763, 'accumulated_eval_time': 1518.3700096607208, 'accumulated_logging_time': 0.5735807418823242, 'global_step': 19977, 'preemption_count': 0}), (21798, {'train/ctc_loss': Array(0.86420834, dtype=float32), 'train/wer': 0.2851232256183706, 'validation/ctc_loss': Array(0.9465712, dtype=float32), 'validation/wer': 0.2811821157206716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6596471, dtype=float32), 'test/wer': 0.22094936323197856, 'test/num_examples': 2472, 'score': 17317.22550392151, 'total_duration': 18967.396344423294, 'accumulated_submission_time': 17317.22550392151, 'accumulated_eval_time': 1648.6084377765656, 'accumulated_logging_time': 0.6313190460205078, 'global_step': 21798, 'preemption_count': 0}), (23608, {'train/ctc_loss': Array(0.8158463, dtype=float32), 'train/wer': 0.26482833320170146, 'validation/ctc_loss': Array(0.9147472, dtype=float32), 'validation/wer': 0.2765285729457312, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.631796, dtype=float32), 'test/wer': 0.21128105132736172, 'test/num_examples': 2472, 'score': 18757.19909286499, 'total_duration': 20537.81848526001, 'accumulated_submission_time': 18757.19909286499, 'accumulated_eval_time': 1778.9243762493134, 'accumulated_logging_time': 0.6872842311859131, 'global_step': 23608, 'preemption_count': 0}), (25422, {'train/ctc_loss': Array(0.8221751, dtype=float32), 'train/wer': 0.26447396579254945, 'validation/ctc_loss': Array(0.89193165, dtype=float32), 'validation/wer': 0.2666228989061278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6027977, dtype=float32), 'test/wer': 0.20333922369142649, 'test/num_examples': 2472, 'score': 20197.799550056458, 'total_duration': 22111.28186249733, 'accumulated_submission_time': 20197.799550056458, 'accumulated_eval_time': 1911.6580095291138, 'accumulated_logging_time': 0.73909592628479, 'global_step': 25422, 'preemption_count': 0}), (27255, {'train/ctc_loss': Array(0.7159185, dtype=float32), 'train/wer': 0.2397716148297229, 'validation/ctc_loss': Array(0.87400496, dtype=float32), 'validation/wer': 0.2637458122942352, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.59717554, dtype=float32), 'test/wer': 0.20195803627648123, 'test/num_examples': 2472, 'score': 21638.320074796677, 'total_duration': 23684.04691696167, 'accumulated_submission_time': 21638.320074796677, 'accumulated_eval_time': 2043.7654702663422, 'accumulated_logging_time': 0.7973823547363281, 'global_step': 27255, 'preemption_count': 0}), (29078, {'train/ctc_loss': Array(0.7162538, dtype=float32), 'train/wer': 0.24312656722086015, 'validation/ctc_loss': Array(0.8402705, dtype=float32), 'validation/wer': 0.2538691022138119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5756875, dtype=float32), 'test/wer': 0.19549895395364897, 'test/num_examples': 2472, 'score': 23079.08738541603, 'total_duration': 25255.72882080078, 'accumulated_submission_time': 23079.08738541603, 'accumulated_eval_time': 2174.5478515625, 'accumulated_logging_time': 0.8513691425323486, 'global_step': 29078, 'preemption_count': 0}), (30893, {'train/ctc_loss': Array(0.7572224, dtype=float32), 'train/wer': 0.2548112793816958, 'validation/ctc_loss': Array(0.86291736, dtype=float32), 'validation/wer': 0.26156386070266563, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57978195, dtype=float32), 'test/wer': 0.1974894887575407, 'test/num_examples': 2472, 'score': 24519.23925757408, 'total_duration': 26824.97067308426, 'accumulated_submission_time': 24519.23925757408, 'accumulated_eval_time': 2303.503675699234, 'accumulated_logging_time': 0.9082772731781006, 'global_step': 30893, 'preemption_count': 0}), (32729, {'train/ctc_loss': Array(0.508243, dtype=float32), 'train/wer': 0.1794177239877347, 'validation/ctc_loss': Array(0.7982695, dtype=float32), 'validation/wer': 0.24097048572559546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5389743, dtype=float32), 'test/wer': 0.18296670932098388, 'test/num_examples': 2472, 'score': 25959.860438346863, 'total_duration': 28407.60689687729, 'accumulated_submission_time': 25959.860438346863, 'accumulated_eval_time': 2445.386803150177, 'accumulated_logging_time': 0.9621689319610596, 'global_step': 32729, 'preemption_count': 0}), (34568, {'train/ctc_loss': Array(0.45416874, dtype=float32), 'train/wer': 0.1624045821292815, 'validation/ctc_loss': Array(0.7872792, dtype=float32), 'validation/wer': 0.23718586172605888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51708585, dtype=float32), 'test/wer': 0.17520768590173258, 'test/num_examples': 2472, 'score': 27399.94140648842, 'total_duration': 29980.78520011902, 'accumulated_submission_time': 27399.94140648842, 'accumulated_eval_time': 2578.3474531173706, 'accumulated_logging_time': 1.02097749710083, 'global_step': 34568, 'preemption_count': 0}), (36405, {'train/ctc_loss': Array(0.4568602, dtype=float32), 'train/wer': 0.1632162807164774, 'validation/ctc_loss': Array(0.7698689, dtype=float32), 'validation/wer': 0.2332853818898018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51699764, dtype=float32), 'test/wer': 0.1756342290739951, 'test/num_examples': 2472, 'score': 28840.462290525436, 'total_duration': 31553.070999383926, 'accumulated_submission_time': 28840.462290525436, 'accumulated_eval_time': 2709.968914270401, 'accumulated_logging_time': 1.0840272903442383, 'global_step': 36405, 'preemption_count': 0}), (38227, {'train/ctc_loss': Array(0.41335335, dtype=float32), 'train/wer': 0.15208877622287179, 'validation/ctc_loss': Array(0.74211925, dtype=float32), 'validation/wer': 0.22561958736012822, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.487146, dtype=float32), 'test/wer': 0.16645339508053542, 'test/num_examples': 2472, 'score': 30280.543273448944, 'total_duration': 33125.39553499222, 'accumulated_submission_time': 30280.543273448944, 'accumulated_eval_time': 2842.078820705414, 'accumulated_logging_time': 1.1394822597503662, 'global_step': 38227, 'preemption_count': 0}), (40049, {'train/ctc_loss': Array(0.41621852, dtype=float32), 'train/wer': 0.15325430697262854, 'validation/ctc_loss': Array(0.7324096, dtype=float32), 'validation/wer': 0.2226749181768153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4805117, dtype=float32), 'test/wer': 0.16440192553774907, 'test/num_examples': 2472, 'score': 31721.368850708008, 'total_duration': 34698.13117194176, 'accumulated_submission_time': 31721.368850708008, 'accumulated_eval_time': 2973.8543276786804, 'accumulated_logging_time': 1.196692943572998, 'global_step': 40049, 'preemption_count': 0}), (41871, {'train/ctc_loss': Array(0.38685045, dtype=float32), 'train/wer': 0.14329412258518107, 'validation/ctc_loss': Array(0.69860196, dtype=float32), 'validation/wer': 0.21291406393311257, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45834982, dtype=float32), 'test/wer': 0.1583084516482847, 'test/num_examples': 2472, 'score': 33161.7200114727, 'total_duration': 36272.580060482025, 'accumulated_submission_time': 33161.7200114727, 'accumulated_eval_time': 3107.816128730774, 'accumulated_logging_time': 1.2540216445922852, 'global_step': 41871, 'preemption_count': 0}), (43705, {'train/ctc_loss': Array(0.4322496, dtype=float32), 'train/wer': 0.15385792600608872, 'validation/ctc_loss': Array(0.68366253, dtype=float32), 'validation/wer': 0.2103845448313815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43958446, dtype=float32), 'test/wer': 0.15253996303292508, 'test/num_examples': 2472, 'score': 34601.8858397007, 'total_duration': 37844.95906472206, 'accumulated_submission_time': 34601.8858397007, 'accumulated_eval_time': 3239.886257171631, 'accumulated_logging_time': 1.3179125785827637, 'global_step': 43705, 'preemption_count': 0}), (45527, {'train/ctc_loss': Array(0.3753733, dtype=float32), 'train/wer': 0.14004018039322375, 'validation/ctc_loss': Array(0.67537385, dtype=float32), 'validation/wer': 0.20578892997480136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43343684, dtype=float32), 'test/wer': 0.14851827026587858, 'test/num_examples': 2472, 'score': 36041.96706032753, 'total_duration': 39417.02039551735, 'accumulated_submission_time': 36041.96706032753, 'accumulated_eval_time': 3371.7308938503265, 'accumulated_logging_time': 1.3760406970977783, 'global_step': 45527, 'preemption_count': 0}), (47343, {'train/ctc_loss': Array(0.34525725, dtype=float32), 'train/wer': 0.12580803211581926, 'validation/ctc_loss': Array(0.6423175, dtype=float32), 'validation/wer': 0.19709008756770324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41392162, dtype=float32), 'test/wer': 0.1407795584262588, 'test/num_examples': 2472, 'score': 37481.884147167206, 'total_duration': 40989.77420902252, 'accumulated_submission_time': 37481.884147167206, 'accumulated_eval_time': 3504.4280862808228, 'accumulated_logging_time': 1.436626672744751, 'global_step': 47343, 'preemption_count': 0}), (49160, {'train/ctc_loss': Array(0.31973082, dtype=float32), 'train/wer': 0.12011086017126205, 'validation/ctc_loss': Array(0.6218428, dtype=float32), 'validation/wer': 0.192697220425384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40082654, dtype=float32), 'test/wer': 0.13797656043710518, 'test/num_examples': 2472, 'score': 38922.35270643234, 'total_duration': 42563.329916238785, 'accumulated_submission_time': 38922.35270643234, 'accumulated_eval_time': 3637.337597131729, 'accumulated_logging_time': 1.5352272987365723, 'global_step': 49160, 'preemption_count': 0}), (50992, {'train/ctc_loss': Array(0.30350104, dtype=float32), 'train/wer': 0.11631830085310799, 'validation/ctc_loss': Array(0.6097256, dtype=float32), 'validation/wer': 0.18753198103826138, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38197964, dtype=float32), 'test/wer': 0.13397517924969024, 'test/num_examples': 2472, 'score': 40362.86497974396, 'total_duration': 44136.597618341446, 'accumulated_submission_time': 40362.86497974396, 'accumulated_eval_time': 3769.9570519924164, 'accumulated_logging_time': 1.5926856994628906, 'global_step': 50992, 'preemption_count': 0}), (52819, {'train/ctc_loss': Array(0.31450352, dtype=float32), 'train/wer': 0.11548549434380698, 'validation/ctc_loss': Array(0.587471, dtype=float32), 'validation/wer': 0.1810054355696728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3687143, dtype=float32), 'test/wer': 0.12524120000812464, 'test/num_examples': 2472, 'score': 41803.16771149635, 'total_duration': 45707.625512599945, 'accumulated_submission_time': 41803.16771149635, 'accumulated_eval_time': 3900.5444436073303, 'accumulated_logging_time': 1.651547908782959, 'global_step': 52819, 'preemption_count': 0}), (54636, {'train/ctc_loss': Array(0.29329264, dtype=float32), 'train/wer': 0.10794605852775967, 'validation/ctc_loss': Array(0.56704783, dtype=float32), 'validation/wer': 0.17521264373364742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35127002, dtype=float32), 'test/wer': 0.1210367030243942, 'test/num_examples': 2472, 'score': 43243.308198451996, 'total_duration': 47279.961285591125, 'accumulated_submission_time': 43243.308198451996, 'accumulated_eval_time': 4032.609624862671, 'accumulated_logging_time': 1.7056090831756592, 'global_step': 54636, 'preemption_count': 0}), (56468, {'train/ctc_loss': Array(0.2854557, dtype=float32), 'train/wer': 0.10497089164045106, 'validation/ctc_loss': Array(0.5525805, dtype=float32), 'validation/wer': 0.16984465663226392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33652857, dtype=float32), 'test/wer': 0.11801027765929356, 'test/num_examples': 2472, 'score': 44683.90547633171, 'total_duration': 48853.7058467865, 'accumulated_submission_time': 44683.90547633171, 'accumulated_eval_time': 4165.615864276886, 'accumulated_logging_time': 1.7686245441436768, 'global_step': 56468, 'preemption_count': 0}), (58296, {'train/ctc_loss': Array(0.25223732, dtype=float32), 'train/wer': 0.09496106685760315, 'validation/ctc_loss': Array(0.5288609, dtype=float32), 'validation/wer': 0.16171543875570832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32527593, dtype=float32), 'test/wer': 0.11073873215119939, 'test/num_examples': 2472, 'score': 46124.3394947052, 'total_duration': 50427.39268708229, 'accumulated_submission_time': 46124.3394947052, 'accumulated_eval_time': 4298.656836748123, 'accumulated_logging_time': 1.9026412963867188, 'global_step': 58296, 'preemption_count': 0}), (60135, {'train/ctc_loss': Array(0.2326758, dtype=float32), 'train/wer': 0.08732094034687694, 'validation/ctc_loss': Array(0.5112117, dtype=float32), 'validation/wer': 0.1576701391235506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30733588, dtype=float32), 'test/wer': 0.10464525826173501, 'test/num_examples': 2472, 'score': 47564.77532219887, 'total_duration': 51999.75342416763, 'accumulated_submission_time': 47564.77532219887, 'accumulated_eval_time': 4430.442895412445, 'accumulated_logging_time': 1.963092565536499, 'global_step': 60135, 'preemption_count': 0}), (61955, {'train/ctc_loss': Array(0.21656099, dtype=float32), 'train/wer': 0.08025868627948263, 'validation/ctc_loss': Array(0.49202898, dtype=float32), 'validation/wer': 0.15026502022649815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29639518, dtype=float32), 'test/wer': 0.10021733390205756, 'test/num_examples': 2472, 'score': 49004.667399168015, 'total_duration': 53572.284828424454, 'accumulated_submission_time': 49004.667399168015, 'accumulated_eval_time': 4562.946071147919, 'accumulated_logging_time': 2.0217151641845703, 'global_step': 61955, 'preemption_count': 0}), (63784, {'train/ctc_loss': Array(0.21301003, dtype=float32), 'train/wer': 0.07957331428293095, 'validation/ctc_loss': Array(0.4686298, dtype=float32), 'validation/wer': 0.1450225436148952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28154856, dtype=float32), 'test/wer': 0.09633782219243191, 'test/num_examples': 2472, 'score': 50444.830134153366, 'total_duration': 55144.689125299454, 'accumulated_submission_time': 50444.830134153366, 'accumulated_eval_time': 4695.051886081696, 'accumulated_logging_time': 2.079317331314087, 'global_step': 63784, 'preemption_count': 0}), (65615, {'train/ctc_loss': Array(0.18987854, dtype=float32), 'train/wer': 0.06969517767861944, 'validation/ctc_loss': Array(0.45930022, dtype=float32), 'validation/wer': 0.14046554737055525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27360567, dtype=float32), 'test/wer': 0.09225519468649077, 'test/num_examples': 2472, 'score': 51885.259873628616, 'total_duration': 56718.33281850815, 'accumulated_submission_time': 51885.259873628616, 'accumulated_eval_time': 4828.119354486465, 'accumulated_logging_time': 2.1474575996398926, 'global_step': 65615, 'preemption_count': 0}), (67452, {'train/ctc_loss': Array(0.18605858, dtype=float32), 'train/wer': 0.0703033976144076, 'validation/ctc_loss': Array(0.43531832, dtype=float32), 'validation/wer': 0.1353389265956728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2592684, dtype=float32), 'test/wer': 0.08819287876018118, 'test/num_examples': 2472, 'score': 53325.22155690193, 'total_duration': 58291.54190802574, 'accumulated_submission_time': 53325.22155690193, 'accumulated_eval_time': 4961.224667787552, 'accumulated_logging_time': 2.211995840072632, 'global_step': 67452, 'preemption_count': 0}), (69288, {'train/ctc_loss': Array(0.1981465, dtype=float32), 'train/wer': 0.06849597580193119, 'validation/ctc_loss': Array(0.42125234, dtype=float32), 'validation/wer': 0.12994197553510914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24975313, dtype=float32), 'test/wer': 0.08547112708955376, 'test/num_examples': 2472, 'score': 54765.25285768509, 'total_duration': 59863.520375967026, 'accumulated_submission_time': 54765.25285768509, 'accumulated_eval_time': 5093.022603034973, 'accumulated_logging_time': 2.2826132774353027, 'global_step': 69288, 'preemption_count': 0}), (71102, {'train/ctc_loss': Array(0.13899502, dtype=float32), 'train/wer': 0.05172848981862576, 'validation/ctc_loss': Array(0.41104484, dtype=float32), 'validation/wer': 0.12531739671934888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23893896, dtype=float32), 'test/wer': 0.0804947900798245, 'test/num_examples': 2472, 'score': 56205.272963523865, 'total_duration': 61436.927609205246, 'accumulated_submission_time': 56205.272963523865, 'accumulated_eval_time': 5226.270207643509, 'accumulated_logging_time': 2.345404863357544, 'global_step': 71102, 'preemption_count': 0}), (72941, {'train/ctc_loss': Array(0.13666794, dtype=float32), 'train/wer': 0.050745426327423224, 'validation/ctc_loss': Array(0.39726907, dtype=float32), 'validation/wer': 0.1221699798217751, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2305465, dtype=float32), 'test/wer': 0.07706213312209291, 'test/num_examples': 2472, 'score': 57645.54037809372, 'total_duration': 63009.80436420441, 'accumulated_submission_time': 57645.54037809372, 'accumulated_eval_time': 5358.731348514557, 'accumulated_logging_time': 2.414510488510132, 'global_step': 72941, 'preemption_count': 0}), (74775, {'train/ctc_loss': Array(0.16922173, dtype=float32), 'train/wer': 0.06387893051640622, 'validation/ctc_loss': Array(0.38635293, dtype=float32), 'validation/wer': 0.11791227782229645, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22601831, dtype=float32), 'test/wer': 0.07606686572014705, 'test/num_examples': 2472, 'score': 59085.41255426407, 'total_duration': 64581.168640613556, 'accumulated_submission_time': 59085.41255426407, 'accumulated_eval_time': 5490.084238290787, 'accumulated_logging_time': 2.4744374752044678, 'global_step': 74775, 'preemption_count': 0}), (76616, {'train/ctc_loss': Array(0.1668892, dtype=float32), 'train/wer': 0.06057212775371304, 'validation/ctc_loss': Array(0.3823363, dtype=float32), 'validation/wer': 0.1158654913735675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22135468, dtype=float32), 'test/wer': 0.07438100461072858, 'test/num_examples': 2472, 'score': 60525.80380296707, 'total_duration': 66152.88785719872, 'accumulated_submission_time': 60525.80380296707, 'accumulated_eval_time': 5621.268357515335, 'accumulated_logging_time': 2.5391104221343994, 'global_step': 76616, 'preemption_count': 0})], 'global_step': 77313}
I0217 05:03:52.344394 140399019657024 submission_runner.py:586] Timing: 61068.18580174446
I0217 05:03:52.344452 140399019657024 submission_runner.py:588] Total number of evals: 43
I0217 05:03:52.344507 140399019657024 submission_runner.py:589] ====================
I0217 05:03:52.344554 140399019657024 submission_runner.py:542] Using RNG seed 2554204701
I0217 05:03:52.346814 140399019657024 submission_runner.py:551] --- Tuning run 5/5 ---
I0217 05:03:52.346933 140399019657024 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5.
I0217 05:03:52.349828 140399019657024 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5/hparams.json.
I0217 05:03:52.351672 140399019657024 submission_runner.py:206] Initializing dataset.
I0217 05:03:52.351794 140399019657024 submission_runner.py:213] Initializing model.
I0217 05:03:56.188970 140399019657024 submission_runner.py:255] Initializing optimizer.
I0217 05:03:56.629705 140399019657024 submission_runner.py:262] Initializing metrics bundle.
I0217 05:03:56.629914 140399019657024 submission_runner.py:280] Initializing checkpoint and logger.
I0217 05:03:56.634433 140399019657024 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5 with prefix checkpoint_
I0217 05:03:56.634569 140399019657024 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5/meta_data_0.json.
I0217 05:03:56.634820 140399019657024 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 05:03:56.634898 140399019657024 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 05:03:57.276380 140399019657024 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 05:03:57.866451 140399019657024 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5/flags_0.json.
I0217 05:03:57.887012 140399019657024 submission_runner.py:314] Starting training loop.
I0217 05:03:57.890305 140399019657024 input_pipeline.py:20] Loading split = train-clean-100
I0217 05:03:57.936695 140399019657024 input_pipeline.py:20] Loading split = train-clean-360
I0217 05:03:58.472377 140399019657024 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0217 05:04:34.543938 140228596020992 logging_writer.py:48] [0] global_step=0, grad_norm=40.9215202331543, loss=32.769222259521484
I0217 05:04:34.573441 140399019657024 spec.py:321] Evaluating on the training split.
I0217 05:05:31.665174 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 05:06:23.564583 140399019657024 spec.py:349] Evaluating on the test split.
I0217 05:06:50.398233 140399019657024 submission_runner.py:408] Time since start: 172.51s, 	Step: 1, 	{'train/ctc_loss': Array(32.477173, dtype=float32), 'train/wer': 1.3851183971898189, 'validation/ctc_loss': Array(31.16359, dtype=float32), 'validation/wer': 1.043156299178389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.27949, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 36.68635892868042, 'total_duration': 172.50863242149353, 'accumulated_submission_time': 36.68635892868042, 'accumulated_eval_time': 135.82221722602844, 'accumulated_logging_time': 0}
I0217 05:06:50.415231 140318077859584 logging_writer.py:48] [1] accumulated_eval_time=135.822217, accumulated_logging_time=0, accumulated_submission_time=36.686359, global_step=1, preemption_count=0, score=36.686359, test/ctc_loss=31.279489517211914, test/num_examples=2472, test/wer=1.097699, total_duration=172.508632, train/ctc_loss=32.4771728515625, train/wer=1.385118, validation/ctc_loss=31.163589477539062, validation/num_examples=5348, validation/wer=1.043156
I0217 05:08:35.397031 140228562450176 logging_writer.py:48] [100] global_step=100, grad_norm=0.6174293160438538, loss=5.956640720367432
I0217 05:09:53.141156 140228570842880 logging_writer.py:48] [200] global_step=200, grad_norm=0.7378325462341309, loss=5.8400092124938965
I0217 05:11:10.966805 140228562450176 logging_writer.py:48] [300] global_step=300, grad_norm=0.3470637798309326, loss=5.799478054046631
I0217 05:12:28.790548 140228570842880 logging_writer.py:48] [400] global_step=400, grad_norm=0.7253602147102356, loss=5.807891845703125
I0217 05:13:46.843663 140228562450176 logging_writer.py:48] [500] global_step=500, grad_norm=0.5908394455909729, loss=5.815213680267334
I0217 05:15:04.832509 140228570842880 logging_writer.py:48] [600] global_step=600, grad_norm=3.230295181274414, loss=5.775944232940674
I0217 05:16:22.715473 140228562450176 logging_writer.py:48] [700] global_step=700, grad_norm=0.3917125463485718, loss=5.610754489898682
I0217 05:17:40.356852 140228570842880 logging_writer.py:48] [800] global_step=800, grad_norm=0.6314805150032043, loss=5.494297027587891
I0217 05:18:58.916619 140228562450176 logging_writer.py:48] [900] global_step=900, grad_norm=1.5350781679153442, loss=5.390009880065918
I0217 05:20:21.873200 140228570842880 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7643104791641235, loss=4.552538871765137
I0217 05:21:44.664842 140318077859584 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.8247264623641968, loss=3.7731027603149414
I0217 05:23:02.136057 140318069466880 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.082258701324463, loss=3.304964065551758
I0217 05:24:19.982783 140318077859584 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2084554433822632, loss=3.116936206817627
I0217 05:25:37.388086 140318069466880 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.9344533085823059, loss=2.907552480697632
I0217 05:26:54.754810 140318077859584 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0038889646530151, loss=2.7078888416290283
I0217 05:28:11.995750 140318069466880 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7705405950546265, loss=2.632661819458008
I0217 05:29:33.657622 140318077859584 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6356576085090637, loss=2.515411853790283
I0217 05:30:50.985587 140399019657024 spec.py:321] Evaluating on the training split.
I0217 05:31:40.579196 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 05:32:31.668261 140399019657024 spec.py:349] Evaluating on the test split.
I0217 05:32:57.948297 140399019657024 submission_runner.py:408] Time since start: 1740.05s, 	Step: 1795, 	{'train/ctc_loss': Array(3.4333935, dtype=float32), 'train/wer': 0.631694270799768, 'validation/ctc_loss': Array(3.3112328, dtype=float32), 'validation/wer': 0.6119022562924201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.9762988, dtype=float32), 'test/wer': 0.5722178213799687, 'test/num_examples': 2472, 'score': 1477.175707578659, 'total_duration': 1740.0544860363007, 'accumulated_submission_time': 1477.175707578659, 'accumulated_eval_time': 262.77818775177, 'accumulated_logging_time': 0.02735280990600586}
I0217 05:32:57.981497 140318077859584 logging_writer.py:48] [1795] accumulated_eval_time=262.778188, accumulated_logging_time=0.027353, accumulated_submission_time=1477.175708, global_step=1795, preemption_count=0, score=1477.175708, test/ctc_loss=2.9762988090515137, test/num_examples=2472, test/wer=0.572218, total_duration=1740.054486, train/ctc_loss=3.4333934783935547, train/wer=0.631694, validation/ctc_loss=3.311232805252075, validation/num_examples=5348, validation/wer=0.611902
I0217 05:33:02.703514 140318069466880 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6833059787750244, loss=2.4284274578094482
I0217 05:34:19.738242 140318077859584 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6339967846870422, loss=2.326014757156372
I0217 05:35:36.806719 140318069466880 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7004031538963318, loss=2.328547477722168
I0217 05:36:57.516456 140318077859584 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.3911454677581787, loss=2.1922247409820557
I0217 05:38:14.626362 140318069466880 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8706017732620239, loss=2.1899962425231934
I0217 05:39:31.775140 140318077859584 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5660646557807922, loss=2.1687819957733154
I0217 05:40:48.832569 140318069466880 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6704100370407104, loss=2.1517040729522705
I0217 05:42:06.172828 140318077859584 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5577226877212524, loss=2.076599359512329
I0217 05:43:23.241482 140318069466880 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5907902121543884, loss=2.0335209369659424
I0217 05:44:43.688447 140318077859584 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6167941093444824, loss=2.0100815296173096
I0217 05:46:06.104536 140318069466880 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5543130040168762, loss=1.9567475318908691
I0217 05:47:28.622028 140318077859584 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6573586463928223, loss=2.028155565261841
I0217 05:48:52.230518 140318069466880 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5789649486541748, loss=1.962192177772522
I0217 05:50:18.905213 140318077859584 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6007558703422546, loss=1.9147711992263794
I0217 05:51:35.735714 140318069466880 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6032015085220337, loss=1.9343620538711548
I0217 05:52:52.800933 140318077859584 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5486841201782227, loss=1.887518286705017
I0217 05:54:09.972495 140318069466880 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5550408959388733, loss=1.846382737159729
I0217 05:55:26.930864 140318077859584 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5639113187789917, loss=1.8914295434951782
I0217 05:56:43.936751 140318069466880 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5347540378570557, loss=1.7900176048278809
I0217 05:56:58.240738 140399019657024 spec.py:321] Evaluating on the training split.
I0217 05:57:50.206444 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 05:58:42.564007 140399019657024 spec.py:349] Evaluating on the test split.
I0217 05:59:08.993879 140399019657024 submission_runner.py:408] Time since start: 3311.10s, 	Step: 3620, 	{'train/ctc_loss': Array(0.8712847, dtype=float32), 'train/wer': 0.27545211274931947, 'validation/ctc_loss': Array(0.91919994, dtype=float32), 'validation/wer': 0.2688917423752377, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64018565, dtype=float32), 'test/wer': 0.20916864704568075, 'test/num_examples': 2472, 'score': 2917.3443970680237, 'total_duration': 3311.0987231731415, 'accumulated_submission_time': 2917.3443970680237, 'accumulated_eval_time': 393.52325439453125, 'accumulated_logging_time': 0.0794527530670166}
I0217 05:59:09.028657 140318077859584 logging_writer.py:48] [3620] accumulated_eval_time=393.523254, accumulated_logging_time=0.079453, accumulated_submission_time=2917.344397, global_step=3620, preemption_count=0, score=2917.344397, test/ctc_loss=0.6401856541633606, test/num_examples=2472, test/wer=0.209169, total_duration=3311.098723, train/ctc_loss=0.8712847232818604, train/wer=0.275452, validation/ctc_loss=0.9191999435424805, validation/num_examples=5348, validation/wer=0.268892
I0217 06:00:11.601232 140318069466880 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.497056245803833, loss=1.8584030866622925
I0217 06:01:28.663819 140318077859584 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.498198926448822, loss=1.8075177669525146
I0217 06:02:45.837497 140318069466880 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5873246788978577, loss=1.8030191659927368
I0217 06:04:02.904211 140318077859584 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5185176134109497, loss=1.7675631046295166
I0217 06:05:19.962880 140318069466880 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5356746315956116, loss=1.7883613109588623
I0217 06:06:40.731344 140318077859584 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6285895705223083, loss=1.7556886672973633
I0217 06:07:57.572423 140318069466880 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.742584228515625, loss=1.7804983854293823
I0217 06:09:14.446637 140318077859584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.4398585855960846, loss=1.6911283731460571
I0217 06:10:31.459964 140318069466880 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.4780101478099823, loss=1.7544841766357422
I0217 06:11:48.429648 140318077859584 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4860260486602783, loss=1.7524198293685913
I0217 06:13:05.476039 140318069466880 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5130316615104675, loss=1.7378218173980713
I0217 06:14:29.376749 140318077859584 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7425146698951721, loss=1.6789486408233643
I0217 06:15:53.975082 140318069466880 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5201636552810669, loss=1.7080883979797363
I0217 06:17:17.614644 140318077859584 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9044380187988281, loss=1.7248414754867554
I0217 06:18:41.089782 140318069466880 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.667692244052887, loss=1.7543065547943115
I0217 06:20:04.893464 140318077859584 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.587531328201294, loss=1.6907405853271484
I0217 06:21:21.968284 140318069466880 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4629128873348236, loss=1.6584718227386475
I0217 06:22:39.176138 140318077859584 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5212445259094238, loss=1.6633371114730835
I0217 06:23:09.664839 140399019657024 spec.py:321] Evaluating on the training split.
I0217 06:24:03.404408 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 06:24:54.910404 140399019657024 spec.py:349] Evaluating on the test split.
I0217 06:25:21.021322 140399019657024 submission_runner.py:408] Time since start: 4883.13s, 	Step: 5441, 	{'train/ctc_loss': Array(0.5577522, dtype=float32), 'train/wer': 0.18933290277441403, 'validation/ctc_loss': Array(0.7517841, dtype=float32), 'validation/wer': 0.224654122054124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49422714, dtype=float32), 'test/wer': 0.1648894034489062, 'test/num_examples': 2472, 'score': 4357.893091678619, 'total_duration': 4883.128641843796, 'accumulated_submission_time': 4357.893091678619, 'accumulated_eval_time': 524.8741371631622, 'accumulated_logging_time': 0.1293325424194336}
I0217 06:25:21.055819 140318077859584 logging_writer.py:48] [5441] accumulated_eval_time=524.874137, accumulated_logging_time=0.129333, accumulated_submission_time=4357.893092, global_step=5441, preemption_count=0, score=4357.893092, test/ctc_loss=0.4942271411418915, test/num_examples=2472, test/wer=0.164889, total_duration=4883.128642, train/ctc_loss=0.5577521920204163, train/wer=0.189333, validation/ctc_loss=0.751784086227417, validation/num_examples=5348, validation/wer=0.224654
I0217 06:26:07.093037 140318069466880 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.49743592739105225, loss=1.6885082721710205
I0217 06:27:23.949740 140318077859584 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.501211404800415, loss=1.700405240058899
I0217 06:28:40.834039 140318069466880 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5829448103904724, loss=1.7032763957977295
I0217 06:29:57.600908 140318077859584 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.4783479869365692, loss=1.6247966289520264
I0217 06:31:15.594461 140318069466880 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5469033122062683, loss=1.655803918838501
I0217 06:32:38.860539 140318077859584 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4820966124534607, loss=1.633430004119873
I0217 06:34:02.297325 140318069466880 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.660788893699646, loss=1.7734121084213257
I0217 06:35:27.653083 140318077859584 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4811696410179138, loss=1.6161147356033325
I0217 06:36:44.568080 140318069466880 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5281223058700562, loss=1.6336079835891724
I0217 06:38:01.520636 140318077859584 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6071143746376038, loss=1.6635167598724365
I0217 06:39:18.584250 140318069466880 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.603286623954773, loss=1.6296275854110718
I0217 06:40:35.520938 140318077859584 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6279100179672241, loss=1.5790005922317505
I0217 06:41:52.577142 140318069466880 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5626879334449768, loss=1.6631345748901367
I0217 06:43:10.958018 140318077859584 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5173377990722656, loss=1.6504712104797363
I0217 06:44:34.589548 140318069466880 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.46133291721343994, loss=1.5775113105773926
I0217 06:45:58.344789 140318077859584 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.47029492259025574, loss=1.6033633947372437
I0217 06:47:22.037584 140318069466880 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5308753848075867, loss=1.618910312652588
I0217 06:48:45.856576 140318077859584 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4328993558883667, loss=1.5831300020217896
I0217 06:49:21.569875 140399019657024 spec.py:321] Evaluating on the training split.
I0217 06:50:16.003935 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 06:51:07.468280 140399019657024 spec.py:349] Evaluating on the test split.
I0217 06:51:34.300961 140399019657024 submission_runner.py:408] Time since start: 6456.41s, 	Step: 7243, 	{'train/ctc_loss': Array(0.548139, dtype=float32), 'train/wer': 0.18435576427215572, 'validation/ctc_loss': Array(0.6662565, dtype=float32), 'validation/wer': 0.20172432103652355, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4213605, dtype=float32), 'test/wer': 0.14240448479678264, 'test/num_examples': 2472, 'score': 5798.3205742836, 'total_duration': 6456.4077224731445, 'accumulated_submission_time': 5798.3205742836, 'accumulated_eval_time': 657.5992331504822, 'accumulated_logging_time': 0.17917275428771973}
I0217 06:51:34.339184 140318077859584 logging_writer.py:48] [7243] accumulated_eval_time=657.599233, accumulated_logging_time=0.179173, accumulated_submission_time=5798.320574, global_step=7243, preemption_count=0, score=5798.320574, test/ctc_loss=0.42136049270629883, test/num_examples=2472, test/wer=0.142404, total_duration=6456.407722, train/ctc_loss=0.5481389760971069, train/wer=0.184356, validation/ctc_loss=0.6662564873695374, validation/num_examples=5348, validation/wer=0.201724
I0217 06:52:18.706153 140318069466880 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5598119497299194, loss=1.612923264503479
I0217 06:53:35.698224 140318077859584 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5354738235473633, loss=1.570871114730835
I0217 06:54:52.710051 140318069466880 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5470500588417053, loss=1.5838736295700073
I0217 06:56:09.709002 140318077859584 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4687937796115875, loss=1.580596685409546
I0217 06:57:26.798634 140318069466880 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5640313625335693, loss=1.6125407218933105
I0217 06:58:43.792408 140318077859584 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5574276447296143, loss=1.5590851306915283
I0217 07:00:00.807536 140318069466880 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.522193431854248, loss=1.5988364219665527
I0217 07:01:21.560181 140318077859584 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4945785403251648, loss=1.640150547027588
I0217 07:02:46.190006 140318069466880 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.42577165365219116, loss=1.5464895963668823
I0217 07:04:09.291145 140318077859584 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6565156579017639, loss=1.6098796129226685
I0217 07:05:32.016113 140318077859584 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5749343037605286, loss=1.5818370580673218
I0217 07:06:48.972426 140318069466880 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.474282443523407, loss=1.5697436332702637
I0217 07:08:06.250060 140318077859584 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5295934081077576, loss=1.5829824209213257
I0217 07:09:23.352045 140318069466880 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.46580660343170166, loss=1.4649603366851807
I0217 07:10:40.353496 140318077859584 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6120036244392395, loss=1.5483733415603638
I0217 07:11:57.570075 140318069466880 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.45377784967422485, loss=1.5764814615249634
I0217 07:13:21.273829 140318077859584 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5559700727462769, loss=1.527591586112976
I0217 07:14:45.080282 140318069466880 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6263996958732605, loss=1.5122870206832886
I0217 07:15:34.798675 140399019657024 spec.py:321] Evaluating on the training split.
I0217 07:16:29.853889 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 07:17:21.804925 140399019657024 spec.py:349] Evaluating on the test split.
I0217 07:17:48.287719 140399019657024 submission_runner.py:408] Time since start: 8030.39s, 	Step: 9061, 	{'train/ctc_loss': Array(0.44564787, dtype=float32), 'train/wer': 0.15706012794537594, 'validation/ctc_loss': Array(0.6208973, dtype=float32), 'validation/wer': 0.1878312752831227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.383486, dtype=float32), 'test/wer': 0.13027847175674853, 'test/num_examples': 2472, 'score': 7238.6869995594025, 'total_duration': 8030.393237113953, 'accumulated_submission_time': 7238.6869995594025, 'accumulated_eval_time': 791.0809001922607, 'accumulated_logging_time': 0.23787641525268555}
I0217 07:17:48.326762 140318077859584 logging_writer.py:48] [9061] accumulated_eval_time=791.080900, accumulated_logging_time=0.237876, accumulated_submission_time=7238.687000, global_step=9061, preemption_count=0, score=7238.687000, test/ctc_loss=0.3834860026836395, test/num_examples=2472, test/wer=0.130278, total_duration=8030.393237, train/ctc_loss=0.44564786553382874, train/wer=0.157060, validation/ctc_loss=0.6208972930908203, validation/num_examples=5348, validation/wer=0.187831
I0217 07:18:18.976104 140318069466880 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.5460142493247986, loss=1.4891926050186157
I0217 07:19:35.833413 140318077859584 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.46347111463546753, loss=1.46305251121521
I0217 07:20:56.289046 140318077859584 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.673370361328125, loss=1.5178606510162354
I0217 07:22:12.980427 140318069466880 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.42013314366340637, loss=1.4713068008422852
I0217 07:23:29.754239 140318077859584 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5681378245353699, loss=1.547019124031067
I0217 07:24:46.584990 140318069466880 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.5358821749687195, loss=1.4960896968841553
I0217 07:26:03.818727 140318077859584 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5094670057296753, loss=1.5283409357070923
I0217 07:27:20.763440 140318069466880 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.46600353717803955, loss=1.5527657270431519
I0217 07:28:41.957003 140318077859584 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.5330974459648132, loss=1.46058988571167
I0217 07:30:05.212160 140318069466880 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.541991651058197, loss=1.4665813446044922
I0217 07:31:28.353855 140318077859584 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.4375677704811096, loss=1.503968596458435
I0217 07:32:51.727351 140318069466880 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4230181872844696, loss=1.5275410413742065
I0217 07:34:18.571511 140318077859584 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6465640068054199, loss=1.463857889175415
I0217 07:35:35.536576 140318069466880 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.5597640872001648, loss=1.4509564638137817
I0217 07:36:52.522618 140318077859584 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.47865402698516846, loss=1.4819979667663574
I0217 07:38:09.523513 140318069466880 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4249667823314667, loss=1.4925670623779297
I0217 07:39:26.425598 140318077859584 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4975161850452423, loss=1.5053222179412842
I0217 07:40:43.173526 140318069466880 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.518686830997467, loss=1.542535424232483
I0217 07:41:48.370584 140399019657024 spec.py:321] Evaluating on the training split.
I0217 07:42:43.110524 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 07:43:35.231331 140399019657024 spec.py:349] Evaluating on the test split.
I0217 07:44:01.814095 140399019657024 submission_runner.py:408] Time since start: 9603.92s, 	Step: 10883, 	{'train/ctc_loss': Array(0.4268927, dtype=float32), 'train/wer': 0.15219139417031405, 'validation/ctc_loss': Array(0.58681077, dtype=float32), 'validation/wer': 0.17565675777440937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36785704, dtype=float32), 'test/wer': 0.12526151158775617, 'test/num_examples': 2472, 'score': 8678.63923573494, 'total_duration': 9603.92083120346, 'accumulated_submission_time': 8678.63923573494, 'accumulated_eval_time': 924.5182108879089, 'accumulated_logging_time': 0.2955894470214844}
I0217 07:44:01.854717 140318077859584 logging_writer.py:48] [10883] accumulated_eval_time=924.518211, accumulated_logging_time=0.295589, accumulated_submission_time=8678.639236, global_step=10883, preemption_count=0, score=8678.639236, test/ctc_loss=0.36785703897476196, test/num_examples=2472, test/wer=0.125262, total_duration=9603.920831, train/ctc_loss=0.4268926978111267, train/wer=0.152191, validation/ctc_loss=0.5868107676506042, validation/num_examples=5348, validation/wer=0.175657
I0217 07:44:15.746626 140318069466880 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5349567532539368, loss=1.5211621522903442
I0217 07:45:32.581389 140318077859584 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6277861595153809, loss=1.5057361125946045
I0217 07:46:49.529657 140318069466880 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5158647298812866, loss=1.5386112928390503
I0217 07:48:06.465935 140318077859584 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.4589218497276306, loss=1.4147281646728516
I0217 07:49:23.471132 140318069466880 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4948062598705292, loss=1.509511113166809
I0217 07:50:44.047410 140318077859584 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6057600378990173, loss=1.4161794185638428
I0217 07:52:00.981905 140318069466880 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.4560348391532898, loss=1.4952672719955444
I0217 07:53:18.021279 140318077859584 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.47676822543144226, loss=1.464640498161316
I0217 07:54:35.092057 140318069466880 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7312710285186768, loss=1.5606255531311035
I0217 07:55:52.156295 140318077859584 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4688708186149597, loss=1.4578908681869507
I0217 07:57:09.195466 140318069466880 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5789615511894226, loss=1.4655604362487793
I0217 07:58:28.935293 140318077859584 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6012147068977356, loss=1.4422087669372559
I0217 07:59:51.856214 140318069466880 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5988485217094421, loss=1.4808845520019531
I0217 08:01:14.374573 140318077859584 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5370060801506042, loss=1.4515159130096436
I0217 08:02:38.360921 140318069466880 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.4821636974811554, loss=1.4951707124710083
I0217 08:04:02.029933 140318077859584 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5928345918655396, loss=1.4673041105270386
I0217 08:05:18.946843 140318069466880 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5183935165405273, loss=1.4617908000946045
I0217 08:06:36.046547 140318077859584 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.46373334527015686, loss=1.449267029762268
I0217 08:07:53.108936 140318069466880 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.49564579129219055, loss=1.4860327243804932
I0217 08:08:02.065672 140399019657024 spec.py:321] Evaluating on the training split.
I0217 08:08:56.098261 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 08:09:48.025997 140399019657024 spec.py:349] Evaluating on the test split.
I0217 08:10:14.361490 140399019657024 submission_runner.py:408] Time since start: 11176.47s, 	Step: 12713, 	{'train/ctc_loss': Array(0.40124416, dtype=float32), 'train/wer': 0.13860109289617487, 'validation/ctc_loss': Array(0.5638952, dtype=float32), 'validation/wer': 0.16918814022418105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3447162, dtype=float32), 'test/wer': 0.11577600389982329, 'test/num_examples': 2472, 'score': 10118.762867212296, 'total_duration': 11176.468502998352, 'accumulated_submission_time': 10118.762867212296, 'accumulated_eval_time': 1056.8081135749817, 'accumulated_logging_time': 0.352435827255249}
I0217 08:10:14.395654 140318077859584 logging_writer.py:48] [12713] accumulated_eval_time=1056.808114, accumulated_logging_time=0.352436, accumulated_submission_time=10118.762867, global_step=12713, preemption_count=0, score=10118.762867, test/ctc_loss=0.3447161912918091, test/num_examples=2472, test/wer=0.115776, total_duration=11176.468503, train/ctc_loss=0.4012441635131836, train/wer=0.138601, validation/ctc_loss=0.5638952255249023, validation/num_examples=5348, validation/wer=0.169188
I0217 08:11:21.998978 140318069466880 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.46205708384513855, loss=1.443047285079956
I0217 08:12:39.020307 140318077859584 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.4321750998497009, loss=1.4071245193481445
I0217 08:13:56.117311 140318069466880 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5234189629554749, loss=1.4605209827423096
I0217 08:15:13.258856 140318077859584 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.495847225189209, loss=1.4397226572036743
I0217 08:16:30.408159 140318069466880 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.4367692172527313, loss=1.3662514686584473
I0217 08:17:52.289087 140318077859584 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.65772545337677, loss=1.5014867782592773
I0217 08:19:18.334510 140318077859584 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.4980431795120239, loss=1.453550934791565
I0217 08:20:35.304235 140318069466880 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5243974328041077, loss=1.3922170400619507
I0217 08:21:52.107610 140318077859584 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5607277154922485, loss=1.4063465595245361
I0217 08:23:09.124424 140318069466880 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.44642388820648193, loss=1.4009193181991577
I0217 08:24:26.262205 140318077859584 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.4943644404411316, loss=1.453177809715271
I0217 08:25:43.329587 140318069466880 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.46182629466056824, loss=1.4350895881652832
I0217 08:27:00.409007 140318077859584 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5994762182235718, loss=1.4312350749969482
I0217 08:28:23.066270 140318069466880 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5793516635894775, loss=1.4559317827224731
I0217 08:29:46.344175 140318077859584 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5186571478843689, loss=1.4221984148025513
I0217 08:31:09.604795 140318069466880 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.45420438051223755, loss=1.377828598022461
I0217 08:32:31.981370 140318077859584 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.4073721170425415, loss=1.3565911054611206
I0217 08:33:53.764889 140318077859584 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.45369261503219604, loss=1.4196120500564575
I0217 08:34:15.030961 140399019657024 spec.py:321] Evaluating on the training split.
I0217 08:35:07.334995 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 08:35:58.563026 140399019657024 spec.py:349] Evaluating on the test split.
I0217 08:36:24.509528 140399019657024 submission_runner.py:408] Time since start: 12746.62s, 	Step: 14529, 	{'train/ctc_loss': Array(0.3907849, dtype=float32), 'train/wer': 0.13760074023726432, 'validation/ctc_loss': Array(0.5494791, dtype=float32), 'validation/wer': 0.1650462940614229, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33356386, dtype=float32), 'test/wer': 0.11238397010135479, 'test/num_examples': 2472, 'score': 11559.307371377945, 'total_duration': 12746.616312265396, 'accumulated_submission_time': 11559.307371377945, 'accumulated_eval_time': 1186.2805242538452, 'accumulated_logging_time': 0.4058682918548584}
I0217 08:36:24.548214 140318077859584 logging_writer.py:48] [14529] accumulated_eval_time=1186.280524, accumulated_logging_time=0.405868, accumulated_submission_time=11559.307371, global_step=14529, preemption_count=0, score=11559.307371, test/ctc_loss=0.3335638642311096, test/num_examples=2472, test/wer=0.112384, total_duration=12746.616312, train/ctc_loss=0.39078488945961, train/wer=0.137601, validation/ctc_loss=0.5494791269302368, validation/num_examples=5348, validation/wer=0.165046
I0217 08:37:19.780085 140318069466880 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.44008636474609375, loss=1.3966455459594727
I0217 08:38:36.776040 140318077859584 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5631322860717773, loss=1.383562445640564
I0217 08:39:53.939438 140318069466880 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5602694153785706, loss=1.4360944032669067
I0217 08:41:10.884190 140318077859584 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.4784669578075409, loss=1.3797191381454468
I0217 08:42:27.822635 140318069466880 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4913518726825714, loss=1.410807728767395
I0217 08:43:44.937760 140318077859584 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.5219578146934509, loss=1.3934608697891235
I0217 08:45:04.743336 140318069466880 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6133162975311279, loss=1.4160394668579102
I0217 08:46:29.350429 140318077859584 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5742173790931702, loss=1.4398401975631714
I0217 08:47:52.486914 140318069466880 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.41326892375946045, loss=1.3898041248321533
I0217 08:49:16.144550 140318077859584 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.48500171303749084, loss=1.403219223022461
I0217 08:50:33.258478 140318069466880 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5670033097267151, loss=1.4576913118362427
I0217 08:51:50.622484 140318077859584 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6175221800804138, loss=1.434815764427185
I0217 08:53:07.842396 140318069466880 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5544519424438477, loss=1.3732879161834717
I0217 08:54:24.772887 140318077859584 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.45704150199890137, loss=1.388156771659851
I0217 08:55:41.760257 140318069466880 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.44018954038619995, loss=1.3784211874008179
I0217 08:57:01.761393 140318077859584 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5451453924179077, loss=1.4298027753829956
I0217 08:58:24.408027 140318069466880 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.5223969221115112, loss=1.3819221258163452
I0217 08:59:48.512502 140318077859584 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.50843346118927, loss=1.366204857826233
I0217 09:00:25.232419 140399019657024 spec.py:321] Evaluating on the training split.
I0217 09:01:18.871757 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 09:02:10.997240 140399019657024 spec.py:349] Evaluating on the test split.
I0217 09:02:37.375952 140399019657024 submission_runner.py:408] Time since start: 14319.48s, 	Step: 16345, 	{'train/ctc_loss': Array(0.3892411, dtype=float32), 'train/wer': 0.1341954468019204, 'validation/ctc_loss': Array(0.5266665, dtype=float32), 'validation/wer': 0.1597748534906398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31609333, dtype=float32), 'test/wer': 0.10793573416204578, 'test/num_examples': 2472, 'score': 12999.903662443161, 'total_duration': 14319.482456922531, 'accumulated_submission_time': 12999.903662443161, 'accumulated_eval_time': 1318.4176275730133, 'accumulated_logging_time': 0.46091723442077637}
I0217 09:02:37.409409 140318077859584 logging_writer.py:48] [16345] accumulated_eval_time=1318.417628, accumulated_logging_time=0.460917, accumulated_submission_time=12999.903662, global_step=16345, preemption_count=0, score=12999.903662, test/ctc_loss=0.3160933256149292, test/num_examples=2472, test/wer=0.107936, total_duration=14319.482457, train/ctc_loss=0.389241099357605, train/wer=0.134195, validation/ctc_loss=0.526666522026062, validation/num_examples=5348, validation/wer=0.159775
I0217 09:03:20.433514 140318069466880 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6200915575027466, loss=1.427527904510498
I0217 09:04:40.925599 140318077859584 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.524933397769928, loss=1.4107403755187988
I0217 09:05:57.599121 140318069466880 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.48246413469314575, loss=1.3838368654251099
I0217 09:07:14.413391 140318077859584 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4819072186946869, loss=1.323990821838379
I0217 09:08:31.255594 140318069466880 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.4000815451145172, loss=1.3886585235595703
I0217 09:09:48.328634 140318077859584 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5649181604385376, loss=1.3902052640914917
I0217 09:11:05.222661 140318069466880 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.4866648018360138, loss=1.3727644681930542
I0217 09:12:26.615064 140318077859584 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5252251029014587, loss=1.3828753232955933
I0217 09:13:50.114749 140318069466880 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5616649985313416, loss=1.3994086980819702
I0217 09:15:13.044077 140318077859584 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.5130771994590759, loss=1.3830831050872803
I0217 09:16:37.271194 140318069466880 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5104988217353821, loss=1.4059808254241943
I0217 09:18:00.637067 140318077859584 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.44276687502861023, loss=1.3509594202041626
I0217 09:19:21.573646 140318077859584 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5348255634307861, loss=1.428588628768921
I0217 09:20:38.684233 140318069466880 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.621760368347168, loss=1.4332126379013062
I0217 09:21:55.699977 140318077859584 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5242112278938293, loss=1.4248921871185303
I0217 09:23:12.880498 140318069466880 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5054223537445068, loss=1.3752433061599731
I0217 09:24:30.044471 140318077859584 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.47945308685302734, loss=1.3869796991348267
I0217 09:25:47.495740 140318069466880 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5336434245109558, loss=1.3371288776397705
I0217 09:26:37.606647 140399019657024 spec.py:321] Evaluating on the training split.
I0217 09:27:31.095829 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 09:28:22.343271 140399019657024 spec.py:349] Evaluating on the test split.
I0217 09:28:48.688006 140399019657024 submission_runner.py:408] Time since start: 15890.79s, 	Step: 18162, 	{'train/ctc_loss': Array(0.37243292, dtype=float32), 'train/wer': 0.13075699375315253, 'validation/ctc_loss': Array(0.5188964, dtype=float32), 'validation/wer': 0.15627021442984446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30815408, dtype=float32), 'test/wer': 0.10507180143399752, 'test/num_examples': 2472, 'score': 14440.013365745544, 'total_duration': 15890.79479265213, 'accumulated_submission_time': 14440.013365745544, 'accumulated_eval_time': 1449.492838382721, 'accumulated_logging_time': 0.5100352764129639}
I0217 09:28:48.723360 140318077859584 logging_writer.py:48] [18162] accumulated_eval_time=1449.492838, accumulated_logging_time=0.510035, accumulated_submission_time=14440.013366, global_step=18162, preemption_count=0, score=14440.013366, test/ctc_loss=0.30815407633781433, test/num_examples=2472, test/wer=0.105072, total_duration=15890.794793, train/ctc_loss=0.3724329173564911, train/wer=0.130757, validation/ctc_loss=0.5188964009284973, validation/num_examples=5348, validation/wer=0.156270
I0217 09:29:18.780990 140318069466880 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.4671039283275604, loss=1.3875584602355957
I0217 09:30:35.795386 140318077859584 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.505517303943634, loss=1.3625812530517578
I0217 09:31:52.822289 140318069466880 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.540189266204834, loss=1.4500164985656738
I0217 09:33:09.826591 140318077859584 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4502512812614441, loss=1.4292473793029785
I0217 09:34:30.270372 140318077859584 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5262981057167053, loss=1.3984675407409668
I0217 09:35:47.306227 140318069466880 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.47021761536598206, loss=1.3681350946426392
I0217 09:37:04.346354 140318077859584 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.44852444529533386, loss=1.3480333089828491
I0217 09:38:21.486773 140318069466880 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5010057091712952, loss=1.35166597366333
I0217 09:39:38.518989 140318077859584 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4525274932384491, loss=1.3524318933486938
I0217 09:40:56.847358 140318069466880 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5669684410095215, loss=1.3610888719558716
I0217 09:42:21.351580 140318077859584 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5270318388938904, loss=1.3672059774398804
I0217 09:43:43.864580 140318069466880 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5420354008674622, loss=1.3863799571990967
I0217 09:45:07.607249 140318077859584 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4546755850315094, loss=1.375398874282837
I0217 09:46:32.558221 140318069466880 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5179109573364258, loss=1.415932059288025
I0217 09:47:57.966228 140318077859584 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5117954611778259, loss=1.2992702722549438
I0217 09:49:14.891847 140318069466880 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.5780967473983765, loss=1.3656779527664185
I0217 09:50:31.707930 140318077859584 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.47086620330810547, loss=1.3304377794265747
I0217 09:51:48.789649 140318069466880 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.4866541624069214, loss=1.3816587924957275
I0217 09:52:49.243520 140399019657024 spec.py:321] Evaluating on the training split.
I0217 09:53:40.800948 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 09:54:32.317964 140399019657024 spec.py:349] Evaluating on the test split.
I0217 09:54:58.234459 140399019657024 submission_runner.py:408] Time since start: 17460.34s, 	Step: 19980, 	{'train/ctc_loss': Array(0.37521964, dtype=float32), 'train/wer': 0.13682820361485915, 'validation/ctc_loss': Array(0.50475585, dtype=float32), 'validation/wer': 0.1534027824710119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30493766, dtype=float32), 'test/wer': 0.1033656287449475, 'test/num_examples': 2472, 'score': 15880.443354845047, 'total_duration': 17460.340955257416, 'accumulated_submission_time': 15880.443354845047, 'accumulated_eval_time': 1578.4773724079132, 'accumulated_logging_time': 0.5645277500152588}
I0217 09:54:58.275020 140318077859584 logging_writer.py:48] [19980] accumulated_eval_time=1578.477372, accumulated_logging_time=0.564528, accumulated_submission_time=15880.443355, global_step=19980, preemption_count=0, score=15880.443355, test/ctc_loss=0.3049376606941223, test/num_examples=2472, test/wer=0.103366, total_duration=17460.340955, train/ctc_loss=0.3752196431159973, train/wer=0.136828, validation/ctc_loss=0.5047558546066284, validation/num_examples=5348, validation/wer=0.153403
I0217 09:55:14.433707 140318069466880 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6218963265419006, loss=1.3765233755111694
I0217 09:56:31.423011 140318077859584 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.4416561424732208, loss=1.3290096521377563
I0217 09:57:48.227347 140318069466880 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.4912661612033844, loss=1.3612405061721802
I0217 09:59:05.174473 140318077859584 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.46741557121276855, loss=1.3802125453948975
I0217 10:00:22.178764 140318069466880 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.5541832447052002, loss=1.3551117181777954
I0217 10:01:43.121874 140318077859584 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5147175192832947, loss=1.3409329652786255
I0217 10:03:10.545696 140318077859584 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.505012571811676, loss=1.392060399055481
I0217 10:04:27.390840 140318069466880 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7085443735122681, loss=1.293029546737671
I0217 10:05:44.227647 140318077859584 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5039740800857544, loss=1.3256244659423828
I0217 10:07:01.355185 140318069466880 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6824082136154175, loss=1.3782322406768799
I0217 10:08:18.363585 140318077859584 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4956330358982086, loss=1.3843899965286255
I0217 10:09:35.372191 140318069466880 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5640525221824646, loss=1.2852084636688232
I0217 10:10:55.837699 140318077859584 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.5656205415725708, loss=1.3295215368270874
I0217 10:12:19.274093 140318069466880 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.5076944231987, loss=1.334743857383728
I0217 10:13:43.152896 140318077859584 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.4988258481025696, loss=1.3444162607192993
I0217 10:15:06.984282 140318069466880 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5717852115631104, loss=1.392827033996582
I0217 10:16:31.377307 140318077859584 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5888099670410156, loss=1.3672707080841064
I0217 10:17:53.397234 140318077859584 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.4883575141429901, loss=1.30577552318573
I0217 10:18:58.311908 140399019657024 spec.py:321] Evaluating on the training split.
I0217 10:19:51.421587 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 10:20:42.602856 140399019657024 spec.py:349] Evaluating on the test split.
I0217 10:21:08.650000 140399019657024 submission_runner.py:408] Time since start: 19030.76s, 	Step: 21786, 	{'train/ctc_loss': Array(0.31174734, dtype=float32), 'train/wer': 0.11060887183355264, 'validation/ctc_loss': Array(0.49118516, dtype=float32), 'validation/wer': 0.1480830686349286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29324007, dtype=float32), 'test/wer': 0.09936424755753255, 'test/num_examples': 2472, 'score': 17320.393052101135, 'total_duration': 19030.757175445557, 'accumulated_submission_time': 17320.393052101135, 'accumulated_eval_time': 1708.8097307682037, 'accumulated_logging_time': 0.6217460632324219}
I0217 10:21:08.683297 140318077859584 logging_writer.py:48] [21786] accumulated_eval_time=1708.809731, accumulated_logging_time=0.621746, accumulated_submission_time=17320.393052, global_step=21786, preemption_count=0, score=17320.393052, test/ctc_loss=0.2932400703430176, test/num_examples=2472, test/wer=0.099364, total_duration=19030.757175, train/ctc_loss=0.31174734234809875, train/wer=0.110609, validation/ctc_loss=0.49118515849113464, validation/num_examples=5348, validation/wer=0.148083
I0217 10:21:20.234051 140318069466880 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5234445333480835, loss=1.340672254562378
I0217 10:22:37.123948 140318077859584 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5626363158226013, loss=1.350466012954712
I0217 10:23:54.099320 140318069466880 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6793691515922546, loss=1.2979975938796997
I0217 10:25:11.045889 140318077859584 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.4812432527542114, loss=1.3510584831237793
I0217 10:26:28.072157 140318069466880 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5278606414794922, loss=1.3513003587722778
I0217 10:27:44.971686 140318077859584 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6770262718200684, loss=1.3246586322784424
I0217 10:29:04.148302 140318069466880 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5559818744659424, loss=1.2649492025375366
I0217 10:30:28.095764 140318077859584 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.4867064952850342, loss=1.3443137407302856
I0217 10:31:51.595606 140318069466880 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.49072161316871643, loss=1.3002567291259766
I0217 10:33:15.892614 140318077859584 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5282993316650391, loss=1.3477920293807983
I0217 10:34:32.667447 140318069466880 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.487205445766449, loss=1.3064239025115967
I0217 10:35:49.938570 140318077859584 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5538396835327148, loss=1.3781737089157104
I0217 10:37:06.876226 140318069466880 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5114514827728271, loss=1.3199827671051025
I0217 10:38:23.755605 140318077859584 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5104154944419861, loss=1.375534176826477
I0217 10:39:41.272572 140318069466880 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.45816653966903687, loss=1.3394073247909546
I0217 10:41:05.906497 140318077859584 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.4831753075122833, loss=1.3139363527297974
I0217 10:42:29.537839 140318069466880 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.6377648115158081, loss=1.323343276977539
I0217 10:43:52.933456 140318077859584 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4589725434780121, loss=1.343438744544983
I0217 10:45:08.698346 140399019657024 spec.py:321] Evaluating on the training split.
I0217 10:46:01.656133 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 10:46:53.416774 140399019657024 spec.py:349] Evaluating on the test split.
I0217 10:47:19.246415 140399019657024 submission_runner.py:408] Time since start: 20601.35s, 	Step: 23592, 	{'train/ctc_loss': Array(0.32217106, dtype=float32), 'train/wer': 0.11466811358292639, 'validation/ctc_loss': Array(0.4742398, dtype=float32), 'validation/wer': 0.1427440454927252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2847809, dtype=float32), 'test/wer': 0.09617532955537952, 'test/num_examples': 2472, 'score': 18760.32162618637, 'total_duration': 20601.35322713852, 'accumulated_submission_time': 18760.32162618637, 'accumulated_eval_time': 1839.3516829013824, 'accumulated_logging_time': 0.6704885959625244}
I0217 10:47:19.286473 140318077859584 logging_writer.py:48] [23592] accumulated_eval_time=1839.351683, accumulated_logging_time=0.670489, accumulated_submission_time=18760.321626, global_step=23592, preemption_count=0, score=18760.321626, test/ctc_loss=0.284780889749527, test/num_examples=2472, test/wer=0.096175, total_duration=20601.353227, train/ctc_loss=0.32217106223106384, train/wer=0.114668, validation/ctc_loss=0.4742397964000702, validation/num_examples=5348, validation/wer=0.142744
I0217 10:47:26.272557 140318069466880 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.536148190498352, loss=1.3468974828720093
I0217 10:48:46.475996 140318077859584 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.4683661162853241, loss=1.2784777879714966
I0217 10:50:03.397602 140318069466880 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5268681645393372, loss=1.3489576578140259
I0217 10:51:20.491881 140318077859584 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5634402632713318, loss=1.298071026802063
I0217 10:52:37.715668 140318069466880 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5522945523262024, loss=1.322389841079712
I0217 10:53:55.245011 140318077859584 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5681219696998596, loss=1.3373141288757324
I0217 10:55:12.486396 140318069466880 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5108779072761536, loss=1.2853177785873413
I0217 10:56:33.272817 140318077859584 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5313935279846191, loss=1.3176870346069336
I0217 10:57:56.885710 140318069466880 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4620307385921478, loss=1.338579773902893
I0217 10:59:21.546612 140318077859584 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5315977931022644, loss=1.3009886741638184
I0217 11:00:45.276069 140318069466880 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5398705005645752, loss=1.3295774459838867
I0217 11:02:08.584810 140318077859584 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.5139981508255005, loss=1.2717938423156738
I0217 11:03:29.773964 140318077859584 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6272029280662537, loss=1.291123390197754
I0217 11:04:46.723345 140318069466880 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.5864884257316589, loss=1.2380876541137695
I0217 11:06:03.629538 140318077859584 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5325382947921753, loss=1.311069130897522
I0217 11:07:20.514058 140318069466880 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5424439907073975, loss=1.3003545999526978
I0217 11:08:37.454620 140318077859584 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4765140414237976, loss=1.360187292098999
I0217 11:09:54.768882 140318069466880 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5029034614562988, loss=1.2830991744995117
I0217 11:11:17.078135 140318077859584 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.47229066491127014, loss=1.2707301378250122
I0217 11:11:19.973340 140399019657024 spec.py:321] Evaluating on the training split.
I0217 11:12:12.849683 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 11:13:04.520444 140399019657024 spec.py:349] Evaluating on the test split.
I0217 11:13:30.796624 140399019657024 submission_runner.py:408] Time since start: 22172.90s, 	Step: 25405, 	{'train/ctc_loss': Array(0.31540608, dtype=float32), 'train/wer': 0.11191176220662463, 'validation/ctc_loss': Array(0.46801168, dtype=float32), 'validation/wer': 0.1400021240236732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2735938, dtype=float32), 'test/wer': 0.09195052099201755, 'test/num_examples': 2472, 'score': 20200.92087650299, 'total_duration': 22172.902831554413, 'accumulated_submission_time': 20200.92087650299, 'accumulated_eval_time': 1970.1682348251343, 'accumulated_logging_time': 0.7266736030578613}
I0217 11:13:30.835474 140318077859584 logging_writer.py:48] [25405] accumulated_eval_time=1970.168235, accumulated_logging_time=0.726674, accumulated_submission_time=20200.920877, global_step=25405, preemption_count=0, score=20200.920877, test/ctc_loss=0.27359381318092346, test/num_examples=2472, test/wer=0.091951, total_duration=22172.902832, train/ctc_loss=0.31540608406066895, train/wer=0.111912, validation/ctc_loss=0.46801167726516724, validation/num_examples=5348, validation/wer=0.140002
I0217 11:14:44.214510 140318069466880 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5270792245864868, loss=1.3052252531051636
I0217 11:16:00.996171 140318077859584 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.43984749913215637, loss=1.3234119415283203
I0217 11:17:17.696286 140318069466880 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6303153038024902, loss=1.276565432548523
I0217 11:18:37.940813 140318077859584 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.621785581111908, loss=1.2760080099105835
I0217 11:19:54.868115 140318069466880 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5523543357849121, loss=1.356884241104126
I0217 11:21:11.808907 140318077859584 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.44353339076042175, loss=1.319658875465393
I0217 11:22:28.677770 140318069466880 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.5335564017295837, loss=1.2618464231491089
I0217 11:23:45.560883 140318077859584 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7459508180618286, loss=1.2648991346359253
I0217 11:25:02.694366 140318069466880 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4926580488681793, loss=1.3226827383041382
I0217 11:26:27.467665 140318077859584 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6314964890480042, loss=1.3478164672851562
I0217 11:27:50.597172 140318069466880 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5960708856582642, loss=1.335530161857605
I0217 11:29:14.021341 140318077859584 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5867694616317749, loss=1.2954801321029663
I0217 11:30:37.946143 140318069466880 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.4360189437866211, loss=1.3167619705200195
I0217 11:32:03.877189 140318077859584 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5862035751342773, loss=1.246971845626831
I0217 11:33:20.818249 140318069466880 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5823401808738708, loss=1.3143130540847778
I0217 11:34:37.823955 140318077859584 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4630213975906372, loss=1.221570372581482
I0217 11:35:54.734317 140318069466880 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.5458061099052429, loss=1.2974995374679565
I0217 11:37:11.741853 140318077859584 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.4946345388889313, loss=1.288437843322754
I0217 11:37:31.447028 140399019657024 spec.py:321] Evaluating on the training split.
I0217 11:38:25.368834 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 11:39:17.040269 140399019657024 spec.py:349] Evaluating on the test split.
I0217 11:39:43.198891 140399019657024 submission_runner.py:408] Time since start: 23745.31s, 	Step: 27227, 	{'train/ctc_loss': Array(0.3136039, dtype=float32), 'train/wer': 0.11127893054472357, 'validation/ctc_loss': Array(0.45962992, dtype=float32), 'validation/wer': 0.13821601320756538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26831707, dtype=float32), 'test/wer': 0.0913411736030711, 'test/num_examples': 2472, 'score': 21641.441780090332, 'total_duration': 23745.30660867691, 'accumulated_submission_time': 21641.441780090332, 'accumulated_eval_time': 2101.914893388748, 'accumulated_logging_time': 0.7841992378234863}
I0217 11:39:43.238920 140318077859584 logging_writer.py:48] [27227] accumulated_eval_time=2101.914893, accumulated_logging_time=0.784199, accumulated_submission_time=21641.441780, global_step=27227, preemption_count=0, score=21641.441780, test/ctc_loss=0.2683170735836029, test/num_examples=2472, test/wer=0.091341, total_duration=23745.306609, train/ctc_loss=0.3136039078235626, train/wer=0.111279, validation/ctc_loss=0.45962992310523987, validation/num_examples=5348, validation/wer=0.138216
I0217 11:40:40.024709 140318069466880 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5180797576904297, loss=1.2751013040542603
I0217 11:41:56.960646 140318077859584 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.4207763075828552, loss=1.2886884212493896
I0217 11:43:13.868561 140318069466880 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5944457054138184, loss=1.3066273927688599
I0217 11:44:30.904171 140318077859584 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6453847885131836, loss=1.2792171239852905
I0217 11:45:48.176865 140318069466880 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4705139994621277, loss=1.2954808473587036
I0217 11:47:12.058812 140318077859584 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.49434763193130493, loss=1.3075904846191406
I0217 11:48:32.875913 140318077859584 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5033373236656189, loss=1.2274364233016968
I0217 11:49:49.772111 140318069466880 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6332231760025024, loss=1.2670272588729858
I0217 11:51:06.726354 140318077859584 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.5277062058448792, loss=1.3152498006820679
I0217 11:52:23.645716 140318069466880 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.7626911401748657, loss=1.2716842889785767
I0217 11:53:40.606060 140318077859584 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.531117856502533, loss=1.2744035720825195
I0217 11:54:57.679744 140318069466880 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.511853039264679, loss=1.2763398885726929
I0217 11:56:21.029793 140318077859584 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.4848726689815521, loss=1.24309241771698
I0217 11:57:45.242376 140318069466880 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5180492401123047, loss=1.2987347841262817
I0217 11:59:08.951253 140318077859584 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.5892643928527832, loss=1.3087960481643677
I0217 12:00:32.641273 140318069466880 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.6367952227592468, loss=1.2576271295547485
I0217 12:01:56.181356 140318077859584 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5619215965270996, loss=1.2451560497283936
I0217 12:03:13.064270 140318069466880 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5459970235824585, loss=1.2895482778549194
I0217 12:03:43.506984 140399019657024 spec.py:321] Evaluating on the training split.
I0217 12:04:36.028471 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 12:05:27.570152 140399019657024 spec.py:349] Evaluating on the test split.
I0217 12:05:53.525030 140399019657024 submission_runner.py:408] Time since start: 25315.63s, 	Step: 29041, 	{'train/ctc_loss': Array(0.27701193, dtype=float32), 'train/wer': 0.1009942350132865, 'validation/ctc_loss': Array(0.46328136, dtype=float32), 'validation/wer': 0.1380132654933045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2715291, dtype=float32), 'test/wer': 0.0898990514492312, 'test/num_examples': 2472, 'score': 23081.622501134872, 'total_duration': 25315.63108062744, 'accumulated_submission_time': 23081.622501134872, 'accumulated_eval_time': 2231.9260606765747, 'accumulated_logging_time': 0.8400917053222656}
I0217 12:05:53.570017 140318077859584 logging_writer.py:48] [29041] accumulated_eval_time=2231.926061, accumulated_logging_time=0.840092, accumulated_submission_time=23081.622501, global_step=29041, preemption_count=0, score=23081.622501, test/ctc_loss=0.27152910828590393, test/num_examples=2472, test/wer=0.089899, total_duration=25315.631081, train/ctc_loss=0.2770119309425354, train/wer=0.100994, validation/ctc_loss=0.4632813632488251, validation/num_examples=5348, validation/wer=0.138013
I0217 12:06:39.550310 140318069466880 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.7447112202644348, loss=1.2998969554901123
I0217 12:07:56.430144 140318077859584 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5266550183296204, loss=1.3036274909973145
I0217 12:09:13.335524 140318069466880 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4882904887199402, loss=1.2449300289154053
I0217 12:10:30.225093 140318077859584 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5639708042144775, loss=1.2811815738677979
I0217 12:11:47.087670 140318069466880 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.4793267548084259, loss=1.265586018562317
I0217 12:13:06.630541 140318077859584 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6198352575302124, loss=1.2243746519088745
I0217 12:14:30.153577 140318069466880 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5248856544494629, loss=1.2777694463729858
I0217 12:15:53.722395 140318077859584 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5195674300193787, loss=1.2199128866195679
I0217 12:17:18.155772 140318077859584 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5417166352272034, loss=1.2169215679168701
I0217 12:18:34.989242 140318069466880 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6231756806373596, loss=1.2410122156143188
I0217 12:19:52.152589 140318077859584 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.4313189685344696, loss=1.2184035778045654
I0217 12:21:08.937925 140318069466880 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.511148989200592, loss=1.267137050628662
I0217 12:22:25.856424 140318077859584 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.6331291794776917, loss=1.2495965957641602
I0217 12:23:42.731209 140318069466880 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5901983380317688, loss=1.2061576843261719
I0217 12:25:04.233117 140318077859584 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5711410045623779, loss=1.291738510131836
I0217 12:26:27.737273 140318069466880 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.5116032958030701, loss=1.279524564743042
I0217 12:27:52.048792 140318077859584 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.4883231818675995, loss=1.2880514860153198
I0217 12:29:15.269333 140318069466880 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5377368927001953, loss=1.3068002462387085
I0217 12:29:53.856980 140399019657024 spec.py:321] Evaluating on the training split.
I0217 12:30:46.774486 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 12:31:38.192762 140399019657024 spec.py:349] Evaluating on the test split.
I0217 12:32:04.290792 140399019657024 submission_runner.py:408] Time since start: 26886.40s, 	Step: 30847, 	{'train/ctc_loss': Array(0.24538893, dtype=float32), 'train/wer': 0.08977366783685818, 'validation/ctc_loss': Array(0.44567695, dtype=float32), 'validation/wer': 0.13357212508568506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25223556, dtype=float32), 'test/wer': 0.08516645339508054, 'test/num_examples': 2472, 'score': 24521.818974256516, 'total_duration': 26886.398028612137, 'accumulated_submission_time': 24521.818974256516, 'accumulated_eval_time': 2362.35418009758, 'accumulated_logging_time': 0.9039266109466553}
I0217 12:32:04.328791 140318077859584 logging_writer.py:48] [30847] accumulated_eval_time=2362.354180, accumulated_logging_time=0.903927, accumulated_submission_time=24521.818974, global_step=30847, preemption_count=0, score=24521.818974, test/ctc_loss=0.2522355616092682, test/num_examples=2472, test/wer=0.085166, total_duration=26886.398029, train/ctc_loss=0.245388925075531, train/wer=0.089774, validation/ctc_loss=0.4456769526004791, validation/num_examples=5348, validation/wer=0.133572
I0217 12:32:49.290791 140318077859584 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.6859163045883179, loss=1.1895358562469482
I0217 12:34:05.943346 140318069466880 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5245588421821594, loss=1.2412058115005493
I0217 12:35:22.945149 140318077859584 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5613193511962891, loss=1.261071801185608
I0217 12:36:39.905772 140318069466880 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.4976043701171875, loss=1.2353636026382446
I0217 12:37:57.087005 140318077859584 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.532753050327301, loss=1.2433522939682007
I0217 12:39:13.954918 140318069466880 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.5944575667381287, loss=1.2427610158920288
I0217 12:40:34.411682 140318077859584 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4748789370059967, loss=1.2274692058563232
I0217 12:41:59.273493 140318069466880 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.5447267293930054, loss=1.2181174755096436
I0217 12:43:23.112347 140318077859584 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.48973262310028076, loss=1.2039400339126587
I0217 12:44:46.302642 140318069466880 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.5432184934616089, loss=1.2283419370651245
I0217 12:46:10.144077 140318077859584 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.4966801702976227, loss=1.2092034816741943
I0217 12:47:32.106020 140318077859584 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6551945805549622, loss=1.2896651029586792
I0217 12:48:49.139267 140318069466880 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.5257543325424194, loss=1.2713768482208252
I0217 12:50:06.185299 140318077859584 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5279648900032043, loss=1.2504199743270874
I0217 12:51:23.112202 140318069466880 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5361770391464233, loss=1.2619177103042603
I0217 12:52:40.144560 140318077859584 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.484102725982666, loss=1.24898099899292
I0217 12:53:57.863553 140318069466880 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.4841374158859253, loss=1.240551233291626
I0217 12:55:21.253496 140318077859584 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.4822602868080139, loss=1.185925841331482
I0217 12:56:04.427898 140399019657024 spec.py:321] Evaluating on the training split.
I0217 12:56:57.494516 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 12:57:48.597564 140399019657024 spec.py:349] Evaluating on the test split.
I0217 12:58:15.039321 140399019657024 submission_runner.py:408] Time since start: 28457.15s, 	Step: 32654, 	{'train/ctc_loss': Array(0.27409798, dtype=float32), 'train/wer': 0.09958766735105967, 'validation/ctc_loss': Array(0.43247974, dtype=float32), 'validation/wer': 0.1298164650453286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25065094, dtype=float32), 'test/wer': 0.08248532488371621, 'test/num_examples': 2472, 'score': 25961.83100414276, 'total_duration': 28457.146093845367, 'accumulated_submission_time': 25961.83100414276, 'accumulated_eval_time': 2492.959435224533, 'accumulated_logging_time': 0.9592113494873047}
I0217 12:58:15.081512 140318077859584 logging_writer.py:48] [32654] accumulated_eval_time=2492.959435, accumulated_logging_time=0.959211, accumulated_submission_time=25961.831004, global_step=32654, preemption_count=0, score=25961.831004, test/ctc_loss=0.25065094232559204, test/num_examples=2472, test/wer=0.082485, total_duration=28457.146094, train/ctc_loss=0.2740979790687561, train/wer=0.099588, validation/ctc_loss=0.43247973918914795, validation/num_examples=5348, validation/wer=0.129816
I0217 12:58:51.064501 140318069466880 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5925766229629517, loss=1.3121062517166138
I0217 13:00:07.970733 140318077859584 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5463106036186218, loss=1.2063453197479248
I0217 13:01:25.081006 140318069466880 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6949563026428223, loss=1.3152093887329102
I0217 13:02:45.479603 140318077859584 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.48715439438819885, loss=1.1833927631378174
I0217 13:04:02.397357 140318069466880 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5858908891677856, loss=1.1851803064346313
I0217 13:05:19.313373 140318077859584 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.4888845682144165, loss=1.1989160776138306
I0217 13:06:36.176962 140318069466880 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.6205967664718628, loss=1.2757068872451782
I0217 13:07:52.941418 140318077859584 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.5266062617301941, loss=1.2332457304000854
I0217 13:09:09.982096 140318069466880 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5029565691947937, loss=1.2637050151824951
I0217 13:10:33.042662 140318077859584 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5515879392623901, loss=1.2507063150405884
I0217 13:11:56.711422 140318069466880 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.4842630624771118, loss=1.2010812759399414
I0217 13:13:20.877797 140318077859584 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6305697560310364, loss=1.2550140619277954
I0217 13:14:44.083785 140318069466880 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6267136931419373, loss=1.2826684713363647
I0217 13:16:11.638014 140318077859584 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.5033215284347534, loss=1.2110400199890137
I0217 13:17:28.443897 140318069466880 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.4700656831264496, loss=1.1747812032699585
I0217 13:18:45.353332 140318077859584 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5754958987236023, loss=1.277120590209961
I0217 13:20:02.140417 140318069466880 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.506596028804779, loss=1.1822717189788818
I0217 13:21:19.089841 140318077859584 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5217476487159729, loss=1.2049317359924316
I0217 13:22:15.577836 140399019657024 spec.py:321] Evaluating on the training split.
I0217 13:23:08.752763 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 13:24:00.189039 140399019657024 spec.py:349] Evaluating on the test split.
I0217 13:24:26.216549 140399019657024 submission_runner.py:408] Time since start: 30028.32s, 	Step: 34475, 	{'train/ctc_loss': Array(0.25031212, dtype=float32), 'train/wer': 0.09101841078258352, 'validation/ctc_loss': Array(0.42782575, dtype=float32), 'validation/wer': 0.12651457369879412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24289979, dtype=float32), 'test/wer': 0.08191660065403286, 'test/num_examples': 2472, 'score': 27402.239865779877, 'total_duration': 30028.323790550232, 'accumulated_submission_time': 27402.239865779877, 'accumulated_eval_time': 2623.5924849510193, 'accumulated_logging_time': 1.017035722732544}
I0217 13:24:26.254940 140318077859584 logging_writer.py:48] [34475] accumulated_eval_time=2623.592485, accumulated_logging_time=1.017036, accumulated_submission_time=27402.239866, global_step=34475, preemption_count=0, score=27402.239866, test/ctc_loss=0.2428997904062271, test/num_examples=2472, test/wer=0.081917, total_duration=30028.323791, train/ctc_loss=0.25031211972236633, train/wer=0.091018, validation/ctc_loss=0.4278257489204407, validation/num_examples=5348, validation/wer=0.126515
I0217 13:24:46.181676 140318069466880 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4807351529598236, loss=1.2287956476211548
I0217 13:26:03.084059 140318077859584 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.46697139739990234, loss=1.17076575756073
I0217 13:27:20.039543 140318069466880 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.5377672910690308, loss=1.286077618598938
I0217 13:28:36.988120 140318077859584 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.5339683890342712, loss=1.258212685585022
I0217 13:29:54.071123 140318069466880 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6155853271484375, loss=1.24497652053833
I0217 13:31:17.431174 140318077859584 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6531868577003479, loss=1.2248128652572632
I0217 13:32:38.630220 140318077859584 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.48599138855934143, loss=1.1968709230422974
I0217 13:33:55.489482 140318069466880 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.5292055606842041, loss=1.210838794708252
I0217 13:35:12.510671 140318077859584 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.5021029114723206, loss=1.2123476266860962
I0217 13:36:29.393839 140318069466880 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.5138754844665527, loss=1.23322594165802
I0217 13:37:46.421654 140318077859584 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.513431966304779, loss=1.1924114227294922
I0217 13:39:04.646656 140318069466880 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6248747706413269, loss=1.1558806896209717
I0217 13:40:28.666238 140318077859584 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.5834314823150635, loss=1.2445340156555176
I0217 13:41:54.448388 140318069466880 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.5687820315361023, loss=1.2144241333007812
I0217 13:43:18.479065 140318077859584 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.5069947242736816, loss=1.2480391263961792
I0217 13:44:42.867654 140318069466880 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5653430223464966, loss=1.266762137413025
I0217 13:46:06.339363 140318077859584 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.5426172018051147, loss=1.1731617450714111
I0217 13:47:23.319505 140318069466880 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.6209399700164795, loss=1.1765756607055664
I0217 13:48:26.925259 140399019657024 spec.py:321] Evaluating on the training split.
I0217 13:49:19.848310 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 13:50:11.016106 140399019657024 spec.py:349] Evaluating on the test split.
I0217 13:50:37.576536 140399019657024 submission_runner.py:408] Time since start: 31599.68s, 	Step: 36284, 	{'train/ctc_loss': Array(0.26957318, dtype=float32), 'train/wer': 0.09373863515562493, 'validation/ctc_loss': Array(0.42037314, dtype=float32), 'validation/wer': 0.126253898066173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24147299, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 28842.823963165283, 'total_duration': 31599.68351483345, 'accumulated_submission_time': 28842.823963165283, 'accumulated_eval_time': 2754.237830877304, 'accumulated_logging_time': 1.070730209350586}
I0217 13:50:37.615714 140318077859584 logging_writer.py:48] [36284] accumulated_eval_time=2754.237831, accumulated_logging_time=1.070730, accumulated_submission_time=28842.823963, global_step=36284, preemption_count=0, score=28842.823963, test/ctc_loss=0.241472989320755, test/num_examples=2472, test/wer=0.080312, total_duration=31599.683515, train/ctc_loss=0.2695731818675995, train/wer=0.093739, validation/ctc_loss=0.4203731417655945, validation/num_examples=5348, validation/wer=0.126254
I0217 13:50:50.692370 140318069466880 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5278950929641724, loss=1.2177963256835938
I0217 13:52:07.438358 140318077859584 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.524631679058075, loss=1.2455439567565918
I0217 13:53:24.197424 140318069466880 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6758835911750793, loss=1.224051833152771
I0217 13:54:41.086243 140318077859584 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.616679847240448, loss=1.2489348649978638
I0217 13:55:57.925131 140318069466880 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.4426015615463257, loss=1.1864994764328003
I0217 13:57:17.526281 140318077859584 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.537592351436615, loss=1.187712550163269
I0217 13:58:41.770319 140318069466880 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.5282986164093018, loss=1.2470831871032715
I0217 14:00:05.508149 140318077859584 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.5109037756919861, loss=1.2041022777557373
I0217 14:01:31.576000 140318077859584 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.5674362778663635, loss=1.181703805923462
I0217 14:02:48.500402 140318069466880 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.5164867043495178, loss=1.1909480094909668
I0217 14:04:05.810432 140318077859584 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.5265241265296936, loss=1.1994898319244385
I0217 14:05:23.032212 140318069466880 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.534378170967102, loss=1.1477657556533813
I0217 14:06:40.223275 140318077859584 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5486375093460083, loss=1.1577147245407104
I0217 14:07:57.353031 140318069466880 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5900335311889648, loss=1.2484968900680542
I0217 14:09:17.451879 140318077859584 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5006832480430603, loss=1.201154112815857
I0217 14:10:40.759351 140318069466880 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.45612189173698425, loss=1.1764532327651978
I0217 14:12:05.565780 140318077859584 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.5493229031562805, loss=1.2308545112609863
I0217 14:13:29.247997 140318069466880 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.5445654392242432, loss=1.2394438982009888
I0217 14:14:38.115604 140399019657024 spec.py:321] Evaluating on the training split.
I0217 14:15:32.024510 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 14:16:23.495702 140399019657024 spec.py:349] Evaluating on the test split.
I0217 14:16:49.562206 140399019657024 submission_runner.py:408] Time since start: 33171.67s, 	Step: 38084, 	{'train/ctc_loss': Array(0.21194768, dtype=float32), 'train/wer': 0.07836498490351376, 'validation/ctc_loss': Array(0.41112727, dtype=float32), 'validation/wer': 0.12005561080162584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2352042, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 30283.236042499542, 'total_duration': 33171.66923260689, 'accumulated_submission_time': 30283.236042499542, 'accumulated_eval_time': 2885.6785418987274, 'accumulated_logging_time': 1.1268370151519775}
I0217 14:16:49.599586 140318077859584 logging_writer.py:48] [38084] accumulated_eval_time=2885.678542, accumulated_logging_time=1.126837, accumulated_submission_time=30283.236042, global_step=38084, preemption_count=0, score=30283.236042, test/ctc_loss=0.23520420491695404, test/num_examples=2472, test/wer=0.077631, total_duration=33171.669233, train/ctc_loss=0.21194767951965332, train/wer=0.078365, validation/ctc_loss=0.4111272692680359, validation/num_examples=5348, validation/wer=0.120056
I0217 14:17:02.707973 140318069466880 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.47145700454711914, loss=1.1811139583587646
I0217 14:18:23.233230 140318077859584 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.4664892256259918, loss=1.178236722946167
I0217 14:19:40.187745 140318069466880 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5162140727043152, loss=1.1569100618362427
I0217 14:20:57.075176 140318077859584 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.4971446692943573, loss=1.1874059438705444
I0217 14:22:14.464678 140318069466880 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5004444122314453, loss=1.2257533073425293
I0217 14:23:31.519251 140318077859584 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5835195183753967, loss=1.1389563083648682
I0217 14:24:52.775476 140318069466880 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6361364722251892, loss=1.1593859195709229
I0217 14:26:16.731124 140318077859584 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.5774990916252136, loss=1.1841073036193848
I0217 14:27:41.104065 140318069466880 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5697404742240906, loss=1.2019630670547485
I0217 14:29:04.579116 140318077859584 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7560729384422302, loss=1.1678696870803833
I0217 14:30:27.445108 140318069466880 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5479460954666138, loss=1.1348539590835571
I0217 14:31:50.639485 140318077859584 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.49255773425102234, loss=1.0804792642593384
I0217 14:33:07.538923 140318069466880 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.5555692315101624, loss=1.1662179231643677
I0217 14:34:24.482463 140318077859584 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4571707546710968, loss=1.1548062562942505
I0217 14:35:41.377967 140318069466880 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.604314923286438, loss=1.1995042562484741
I0217 14:36:58.242208 140318077859584 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6043339371681213, loss=1.214878797531128
I0217 14:38:15.519439 140318069466880 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.6103856563568115, loss=1.2199077606201172
I0217 14:39:35.810296 140318077859584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5889798402786255, loss=1.2121070623397827
I0217 14:40:50.155655 140399019657024 spec.py:321] Evaluating on the training split.
I0217 14:41:43.320277 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 14:42:34.683836 140399019657024 spec.py:349] Evaluating on the test split.
I0217 14:43:00.766374 140399019657024 submission_runner.py:408] Time since start: 34742.87s, 	Step: 39891, 	{'train/ctc_loss': Array(0.20022973, dtype=float32), 'train/wer': 0.07365950031632573, 'validation/ctc_loss': Array(0.40568483, dtype=float32), 'validation/wer': 0.11783504059781612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22744554, dtype=float32), 'test/wer': 0.07549814149046372, 'test/num_examples': 2472, 'score': 31723.705225467682, 'total_duration': 34742.872770786285, 'accumulated_submission_time': 31723.705225467682, 'accumulated_eval_time': 3016.282743215561, 'accumulated_logging_time': 1.1808912754058838}
I0217 14:43:00.808345 140318077859584 logging_writer.py:48] [39891] accumulated_eval_time=3016.282743, accumulated_logging_time=1.180891, accumulated_submission_time=31723.705225, global_step=39891, preemption_count=0, score=31723.705225, test/ctc_loss=0.2274455428123474, test/num_examples=2472, test/wer=0.075498, total_duration=34742.872771, train/ctc_loss=0.2002297341823578, train/wer=0.073660, validation/ctc_loss=0.40568482875823975, validation/num_examples=5348, validation/wer=0.117835
I0217 14:43:08.565719 140318069466880 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.5241679549217224, loss=1.1364058256149292
I0217 14:44:25.291677 140318077859584 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5972309112548828, loss=1.2516322135925293
I0217 14:45:42.143615 140318069466880 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.5592268705368042, loss=1.1466565132141113
I0217 14:47:02.494601 140318077859584 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6012254953384399, loss=1.128888726234436
I0217 14:48:19.241911 140318069466880 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.5312941670417786, loss=1.1778255701065063
I0217 14:49:36.135336 140318077859584 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.5943804979324341, loss=1.1387451887130737
I0217 14:50:53.171474 140318069466880 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.5376927852630615, loss=1.1521930694580078
I0217 14:52:10.086362 140318077859584 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6976845264434814, loss=1.1925628185272217
I0217 14:53:27.077444 140318069466880 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.5972752571105957, loss=1.201494812965393
I0217 14:54:44.071489 140318077859584 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.585878312587738, loss=1.2098313570022583
I0217 14:56:07.093613 140318069466880 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.5482323169708252, loss=1.1606143712997437
I0217 14:57:31.140013 140318077859584 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.5346837639808655, loss=1.1838955879211426
I0217 14:58:54.957074 140318069466880 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5232462882995605, loss=1.1657119989395142
I0217 15:00:21.851462 140318077859584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.5299032926559448, loss=1.1378767490386963
I0217 15:01:38.651042 140318069466880 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.5109442472457886, loss=1.182403326034546
I0217 15:02:55.453725 140318077859584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.5236579775810242, loss=1.1067827939987183
I0217 15:04:12.284459 140318069466880 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.5533567070960999, loss=1.1247527599334717
I0217 15:05:29.047028 140318077859584 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.5767412185668945, loss=1.155837059020996
I0217 15:06:45.824167 140318069466880 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.5583735108375549, loss=1.1113470792770386
I0217 15:07:00.900934 140399019657024 spec.py:321] Evaluating on the training split.
I0217 15:07:53.079477 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 15:08:44.704014 140399019657024 spec.py:349] Evaluating on the test split.
I0217 15:09:10.830007 140399019657024 submission_runner.py:408] Time since start: 36312.94s, 	Step: 41721, 	{'train/ctc_loss': Array(0.22058325, dtype=float32), 'train/wer': 0.08103187512484662, 'validation/ctc_loss': Array(0.39636406, dtype=float32), 'validation/wer': 0.11662820896531083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22631016, dtype=float32), 'test/wer': 0.07399508459772916, 'test/num_examples': 2472, 'score': 33163.71019721031, 'total_duration': 36312.93751120567, 'accumulated_submission_time': 33163.71019721031, 'accumulated_eval_time': 3146.2064123153687, 'accumulated_logging_time': 1.2387712001800537}
I0217 15:09:10.877774 140318077859584 logging_writer.py:48] [41721] accumulated_eval_time=3146.206412, accumulated_logging_time=1.238771, accumulated_submission_time=33163.710197, global_step=41721, preemption_count=0, score=33163.710197, test/ctc_loss=0.22631016373634338, test/num_examples=2472, test/wer=0.073995, total_duration=36312.937511, train/ctc_loss=0.2205832451581955, train/wer=0.081032, validation/ctc_loss=0.3963640630245209, validation/num_examples=5348, validation/wer=0.116628
I0217 15:10:12.161091 140318069466880 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.674044132232666, loss=1.090433955192566
I0217 15:11:29.175645 140318077859584 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.763886034488678, loss=1.1599116325378418
I0217 15:12:46.016737 140318069466880 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5228434801101685, loss=1.158398985862732
I0217 15:14:02.974232 140318077859584 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.5105138421058655, loss=1.1416075229644775
I0217 15:15:22.474066 140318069466880 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6983697414398193, loss=1.2291109561920166
I0217 15:16:44.065958 140318077859584 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.5306420922279358, loss=1.1433866024017334
I0217 15:18:00.967141 140318069466880 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5773101449012756, loss=1.113556981086731
I0217 15:19:18.039116 140318077859584 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.47081589698791504, loss=1.1503520011901855
I0217 15:20:35.082612 140318069466880 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.491568386554718, loss=1.1469486951828003
I0217 15:21:52.108196 140318077859584 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.5493462681770325, loss=1.0943336486816406
I0217 15:23:09.072418 140318069466880 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.5478255152702332, loss=1.0947803258895874
I0217 15:24:26.205276 140318077859584 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5707109570503235, loss=1.1055192947387695
I0217 15:25:44.747970 140318069466880 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.5529845356941223, loss=1.1858904361724854
I0217 15:27:08.427107 140318077859584 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.609251856803894, loss=1.1491063833236694
I0217 15:28:32.167081 140318069466880 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.5426759123802185, loss=1.1631765365600586
I0217 15:29:56.580085 140318077859584 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.510038435459137, loss=1.1602898836135864
I0217 15:31:13.583147 140318069466880 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5510491132736206, loss=1.1486525535583496
I0217 15:32:30.572676 140318077859584 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4910975992679596, loss=1.1367955207824707
I0217 15:33:10.972259 140399019657024 spec.py:321] Evaluating on the training split.
I0217 15:34:13.045109 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 15:35:03.903478 140399019657024 spec.py:349] Evaluating on the test split.
I0217 15:35:29.964145 140399019657024 submission_runner.py:408] Time since start: 37892.07s, 	Step: 43554, 	{'train/ctc_loss': Array(0.14703715, dtype=float32), 'train/wer': 0.0556202457259144, 'validation/ctc_loss': Array(0.39308006, dtype=float32), 'validation/wer': 0.11424350965948038, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21916676, dtype=float32), 'test/wer': 0.07255296244388926, 'test/num_examples': 2472, 'score': 34603.71418786049, 'total_duration': 37892.071621418, 'accumulated_submission_time': 34603.71418786049, 'accumulated_eval_time': 3285.1928341388702, 'accumulated_logging_time': 1.3039309978485107}
I0217 15:35:30.009485 140318077859584 logging_writer.py:48] [43554] accumulated_eval_time=3285.192834, accumulated_logging_time=1.303931, accumulated_submission_time=34603.714188, global_step=43554, preemption_count=0, score=34603.714188, test/ctc_loss=0.21916675567626953, test/num_examples=2472, test/wer=0.072553, total_duration=37892.071621, train/ctc_loss=0.14703714847564697, train/wer=0.055620, validation/ctc_loss=0.39308005571365356, validation/num_examples=5348, validation/wer=0.114244
I0217 15:36:06.032982 140318069466880 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5500157475471497, loss=1.127785325050354
I0217 15:37:22.837245 140318077859584 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6131702065467834, loss=1.1292532682418823
I0217 15:38:39.678597 140318069466880 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6706929802894592, loss=1.0968513488769531
I0217 15:39:56.674008 140318077859584 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8193868398666382, loss=1.1397836208343506
I0217 15:41:13.651828 140318069466880 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.5488550662994385, loss=1.1440513134002686
I0217 15:42:30.552467 140318077859584 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.51223224401474, loss=1.098572850227356
I0217 15:43:51.265044 140318069466880 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5724674463272095, loss=1.132821798324585
I0217 15:45:17.727435 140318077859584 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.5456631183624268, loss=1.1264797449111938
I0217 15:46:34.373914 140318069466880 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6500398516654968, loss=1.1299549341201782
I0217 15:47:51.464059 140318077859584 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5759141445159912, loss=1.1450679302215576
I0217 15:49:08.322417 140318069466880 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.5413727760314941, loss=1.1610774993896484
I0217 15:50:25.276932 140318077859584 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.5975572466850281, loss=1.1065438985824585
I0217 15:51:42.178413 140318069466880 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.5238033533096313, loss=1.1143420934677124
I0217 15:52:59.102460 140318077859584 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.5362727046012878, loss=1.1372144222259521
I0217 15:54:20.811979 140318069466880 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.51561039686203, loss=1.121846079826355
I0217 15:55:44.872807 140318077859584 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.5409032702445984, loss=1.1236467361450195
I0217 15:57:09.319709 140318069466880 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5726491212844849, loss=1.1750379800796509
I0217 15:58:32.804387 140318077859584 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5499293208122253, loss=1.0618833303451538
I0217 15:59:30.604825 140399019657024 spec.py:321] Evaluating on the training split.
I0217 16:00:25.920605 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 16:01:18.367858 140399019657024 spec.py:349] Evaluating on the test split.
I0217 16:01:44.814196 140399019657024 submission_runner.py:408] Time since start: 39466.92s, 	Step: 45371, 	{'train/ctc_loss': Array(0.12569274, dtype=float32), 'train/wer': 0.04828931963054508, 'validation/ctc_loss': Array(0.38317782, dtype=float32), 'validation/wer': 0.11275669308823387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21107632, dtype=float32), 'test/wer': 0.07021713078626125, 'test/num_examples': 2472, 'score': 36044.21834445, 'total_duration': 39466.92186617851, 'accumulated_submission_time': 36044.21834445, 'accumulated_eval_time': 3419.3971271514893, 'accumulated_logging_time': 1.3684093952178955}
I0217 16:01:44.858123 140318077859584 logging_writer.py:48] [45371] accumulated_eval_time=3419.397127, accumulated_logging_time=1.368409, accumulated_submission_time=36044.218344, global_step=45371, preemption_count=0, score=36044.218344, test/ctc_loss=0.21107631921768188, test/num_examples=2472, test/wer=0.070217, total_duration=39466.921866, train/ctc_loss=0.12569274008274078, train/wer=0.048289, validation/ctc_loss=0.38317781686782837, validation/num_examples=5348, validation/wer=0.112757
I0217 16:02:07.981055 140318069466880 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.6473234295845032, loss=1.1332390308380127
I0217 16:03:24.768338 140318077859584 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.47419941425323486, loss=1.1426228284835815
I0217 16:04:41.793132 140318069466880 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7089329361915588, loss=1.1139979362487793
I0217 16:05:59.163963 140318077859584 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.46321970224380493, loss=1.046530842781067
I0217 16:07:16.244162 140318069466880 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.7442217469215393, loss=1.1316648721694946
I0217 16:08:33.352818 140318077859584 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.643205463886261, loss=1.13332200050354
I0217 16:09:50.611738 140318069466880 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6267049908638, loss=1.1180775165557861
I0217 16:11:09.974099 140318077859584 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.5023844242095947, loss=1.1485049724578857
I0217 16:12:33.787343 140318069466880 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6584325432777405, loss=1.1048593521118164
I0217 16:13:57.681876 140318077859584 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.5820201635360718, loss=1.0996214151382446
I0217 16:15:21.654126 140318077859584 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.5398337841033936, loss=1.0729318857192993
I0217 16:16:38.470761 140318069466880 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7993523478507996, loss=1.090592861175537
I0217 16:17:55.276104 140318077859584 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.5879730582237244, loss=1.1035552024841309
I0217 16:19:12.035775 140318069466880 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.5302014946937561, loss=1.0895897150039673
I0217 16:20:28.851244 140318077859584 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6090149283409119, loss=1.075286626815796
I0217 16:21:46.174233 140318069466880 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.5596179962158203, loss=1.1250250339508057
I0217 16:23:05.859524 140318077859584 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7555884122848511, loss=1.0917783975601196
I0217 16:24:28.925515 140318069466880 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.5924971103668213, loss=1.1129238605499268
I0217 16:25:45.522849 140399019657024 spec.py:321] Evaluating on the training split.
I0217 16:26:40.731068 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 16:27:31.961149 140399019657024 spec.py:349] Evaluating on the test split.
I0217 16:27:58.211443 140399019657024 submission_runner.py:408] Time since start: 41040.32s, 	Step: 47193, 	{'train/ctc_loss': Array(0.12504135, dtype=float32), 'train/wer': 0.0475356763616586, 'validation/ctc_loss': Array(0.3714938, dtype=float32), 'validation/wer': 0.10903965166011759, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20164764, dtype=float32), 'test/wer': 0.06802348018605407, 'test/num_examples': 2472, 'score': 37484.795249700546, 'total_duration': 41040.31870007515, 'accumulated_submission_time': 37484.795249700546, 'accumulated_eval_time': 3552.0800442695618, 'accumulated_logging_time': 1.4292211532592773}
I0217 16:27:58.256047 140318077859584 logging_writer.py:48] [47193] accumulated_eval_time=3552.080044, accumulated_logging_time=1.429221, accumulated_submission_time=37484.795250, global_step=47193, preemption_count=0, score=37484.795250, test/ctc_loss=0.20164763927459717, test/num_examples=2472, test/wer=0.068023, total_duration=41040.318700, train/ctc_loss=0.12504135072231293, train/wer=0.047536, validation/ctc_loss=0.37149378657341003, validation/num_examples=5348, validation/wer=0.109040
I0217 16:28:04.466309 140318069466880 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.5076071619987488, loss=1.1234707832336426
I0217 16:29:21.148296 140318077859584 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.548875629901886, loss=1.1253694295883179
I0217 16:30:41.718531 140318077859584 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6523755788803101, loss=1.064360499382019
I0217 16:31:58.450907 140318069466880 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.5991001725196838, loss=1.1131877899169922
I0217 16:33:15.465121 140318077859584 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.5070059299468994, loss=1.0899344682693481
I0217 16:34:32.588313 140318069466880 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5865121483802795, loss=1.1024086475372314
I0217 16:35:49.618511 140318077859584 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5586413741111755, loss=1.0845800638198853
I0217 16:37:06.540632 140318069466880 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.46427950263023376, loss=1.0667842626571655
I0217 16:38:25.860602 140318077859584 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.609391450881958, loss=1.1523195505142212
I0217 16:39:48.206166 140318069466880 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.563227117061615, loss=1.14419424533844
I0217 16:41:11.459841 140318077859584 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.5634189248085022, loss=1.0820542573928833
I0217 16:42:35.426042 140318069466880 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.5808572769165039, loss=1.0810354948043823
I0217 16:43:58.244121 140318077859584 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7422613501548767, loss=1.192133903503418
I0217 16:45:19.000720 140318077859584 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.6131914258003235, loss=1.1141669750213623
I0217 16:46:36.034097 140318069466880 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.5943453907966614, loss=1.0737972259521484
I0217 16:47:52.935014 140318077859584 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6822237968444824, loss=1.1141743659973145
I0217 16:49:09.905012 140318069466880 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7172287106513977, loss=1.0780094861984253
I0217 16:50:26.944552 140318077859584 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.532770037651062, loss=1.0915896892547607
I0217 16:51:43.988496 140318069466880 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.5200871229171753, loss=1.064622402191162
I0217 16:51:58.829914 140399019657024 spec.py:321] Evaluating on the training split.
I0217 16:52:53.707235 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 16:53:45.177115 140399019657024 spec.py:349] Evaluating on the test split.
I0217 16:54:11.229791 140399019657024 submission_runner.py:408] Time since start: 42613.33s, 	Step: 49019, 	{'train/ctc_loss': Array(0.12188379, dtype=float32), 'train/wer': 0.046178170035937736, 'validation/ctc_loss': Array(0.36238962, dtype=float32), 'validation/wer': 0.10549639398708208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.199988, dtype=float32), 'test/wer': 0.06729226331931834, 'test/num_examples': 2472, 'score': 38925.28243923187, 'total_duration': 42613.33491516113, 'accumulated_submission_time': 38925.28243923187, 'accumulated_eval_time': 3684.4721508026123, 'accumulated_logging_time': 1.4894332885742188}
I0217 16:54:11.272038 140318077859584 logging_writer.py:48] [49019] accumulated_eval_time=3684.472151, accumulated_logging_time=1.489433, accumulated_submission_time=38925.282439, global_step=49019, preemption_count=0, score=38925.282439, test/ctc_loss=0.19998799264431, test/num_examples=2472, test/wer=0.067292, total_duration=42613.334915, train/ctc_loss=0.12188378721475601, train/wer=0.046178, validation/ctc_loss=0.36238962411880493, validation/num_examples=5348, validation/wer=0.105496
I0217 16:55:14.021932 140318069466880 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.525115430355072, loss=1.0604205131530762
I0217 16:56:30.991704 140318077859584 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.5258090496063232, loss=1.1239598989486694
I0217 16:57:48.332304 140318069466880 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.593796968460083, loss=1.1285052299499512
I0217 16:59:05.335494 140318077859584 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.5829853415489197, loss=1.0888038873672485
I0217 17:00:25.817393 140318077859584 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5823800563812256, loss=1.080311894416809
I0217 17:01:42.904939 140318069466880 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.5345232486724854, loss=1.0870537757873535
I0217 17:02:59.890675 140318077859584 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.6188616752624512, loss=1.0428950786590576
I0217 17:04:16.681276 140318069466880 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.6251307129859924, loss=1.0920110940933228
I0217 17:05:33.548691 140318077859584 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.5919926762580872, loss=1.1037166118621826
I0217 17:06:50.855343 140318069466880 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.5689636468887329, loss=1.064588189125061
I0217 17:08:12.405003 140318077859584 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.5292048454284668, loss=1.0976232290267944
I0217 17:09:36.411889 140318069466880 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.5678138732910156, loss=1.0585527420043945
I0217 17:10:59.458339 140318077859584 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6256013512611389, loss=1.1146084070205688
I0217 17:12:23.169852 140318069466880 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.48562657833099365, loss=1.03669273853302
I0217 17:13:49.045034 140318077859584 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.6167998909950256, loss=1.0785233974456787
I0217 17:15:05.712469 140318069466880 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6338757276535034, loss=1.096336007118225
I0217 17:16:22.598975 140318077859584 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.5406407117843628, loss=1.0721306800842285
I0217 17:17:39.545595 140318069466880 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.5095424056053162, loss=1.0298855304718018
I0217 17:18:11.556669 140399019657024 spec.py:321] Evaluating on the training split.
I0217 17:19:04.941608 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 17:19:56.074063 140399019657024 spec.py:349] Evaluating on the test split.
I0217 17:20:22.359262 140399019657024 submission_runner.py:408] Time since start: 44184.47s, 	Step: 50843, 	{'train/ctc_loss': Array(0.11072697, dtype=float32), 'train/wer': 0.04435754739135896, 'validation/ctc_loss': Array(0.36124983, dtype=float32), 'validation/wer': 0.10574741496664318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19808494, dtype=float32), 'test/wer': 0.06609388012105702, 'test/num_examples': 2472, 'score': 40365.479813575745, 'total_duration': 44184.46557068825, 'accumulated_submission_time': 40365.479813575745, 'accumulated_eval_time': 3815.268126010895, 'accumulated_logging_time': 1.547067642211914}
I0217 17:20:22.400282 140318077859584 logging_writer.py:48] [50843] accumulated_eval_time=3815.268126, accumulated_logging_time=1.547068, accumulated_submission_time=40365.479814, global_step=50843, preemption_count=0, score=40365.479814, test/ctc_loss=0.19808493554592133, test/num_examples=2472, test/wer=0.066094, total_duration=44184.465571, train/ctc_loss=0.1107269749045372, train/wer=0.044358, validation/ctc_loss=0.3612498342990875, validation/num_examples=5348, validation/wer=0.105747
I0217 17:21:06.833805 140318069466880 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.5919564962387085, loss=1.038520336151123
I0217 17:22:23.824146 140318077859584 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.5959041714668274, loss=1.1156336069107056
I0217 17:23:40.791075 140318069466880 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8726160526275635, loss=1.0529448986053467
I0217 17:24:57.888784 140318077859584 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.6630846261978149, loss=1.0997456312179565
I0217 17:26:14.825943 140318069466880 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.6431530117988586, loss=1.0863912105560303
I0217 17:27:36.110203 140318077859584 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.5014420747756958, loss=1.0726451873779297
I0217 17:29:02.770533 140318077859584 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.524911642074585, loss=1.0667414665222168
I0217 17:30:19.630842 140318069466880 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.5165097713470459, loss=1.0683449506759644
I0217 17:31:37.075908 140318077859584 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.6456076502799988, loss=1.1176952123641968
I0217 17:32:54.112104 140318069466880 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7607901692390442, loss=1.0764652490615845
I0217 17:34:10.998052 140318077859584 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9097002744674683, loss=1.0381895303726196
I0217 17:35:28.028482 140318069466880 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7210273146629333, loss=1.0923768281936646
I0217 17:36:45.226499 140318077859584 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.5396714210510254, loss=1.035233974456787
I0217 17:38:08.975354 140318069466880 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.5858747363090515, loss=1.0550118684768677
I0217 17:39:32.130476 140318077859584 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.6076275110244751, loss=1.0826923847198486
I0217 17:40:55.659553 140318069466880 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.5787061452865601, loss=1.1106170415878296
I0217 17:42:19.486653 140318077859584 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.5232681035995483, loss=1.0041987895965576
I0217 17:43:41.287744 140318077859584 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.7353773713111877, loss=1.0533268451690674
I0217 17:44:22.501758 140399019657024 spec.py:321] Evaluating on the training split.
I0217 17:45:16.982232 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 17:46:08.177365 140399019657024 spec.py:349] Evaluating on the test split.
I0217 17:46:34.199167 140399019657024 submission_runner.py:408] Time since start: 45756.31s, 	Step: 52655, 	{'train/ctc_loss': Array(0.10210627, dtype=float32), 'train/wer': 0.03982586179327278, 'validation/ctc_loss': Array(0.35422248, dtype=float32), 'validation/wer': 0.10187589908956622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19376087, dtype=float32), 'test/wer': 0.06505798955984807, 'test/num_examples': 2472, 'score': 41805.494245529175, 'total_duration': 45756.3062517643, 'accumulated_submission_time': 41805.494245529175, 'accumulated_eval_time': 3946.959716320038, 'accumulated_logging_time': 1.6031808853149414}
I0217 17:46:34.241191 140318077859584 logging_writer.py:48] [52655] accumulated_eval_time=3946.959716, accumulated_logging_time=1.603181, accumulated_submission_time=41805.494246, global_step=52655, preemption_count=0, score=41805.494246, test/ctc_loss=0.19376087188720703, test/num_examples=2472, test/wer=0.065058, total_duration=45756.306252, train/ctc_loss=0.10210626572370529, train/wer=0.039826, validation/ctc_loss=0.35422247648239136, validation/num_examples=5348, validation/wer=0.101876
I0217 17:47:09.431369 140318069466880 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.5810226798057556, loss=1.0284336805343628
I0217 17:48:26.220424 140318077859584 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.5581528544425964, loss=1.023756980895996
I0217 17:49:43.248383 140318069466880 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.5578629970550537, loss=1.0470242500305176
I0217 17:51:00.000559 140318077859584 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.5760448575019836, loss=1.0496652126312256
I0217 17:52:16.755840 140318069466880 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.6333582997322083, loss=1.0602582693099976
I0217 17:53:33.666960 140318077859584 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.6500246524810791, loss=1.0478605031967163
I0217 17:54:50.482151 140318069466880 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.5484282374382019, loss=1.0591567754745483
I0217 17:56:13.840834 140318077859584 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.7585470676422119, loss=1.0570545196533203
I0217 17:57:36.916584 140318069466880 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.5465928912162781, loss=1.0669680833816528
I0217 17:59:01.576876 140318077859584 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.6055529117584229, loss=1.0377596616744995
I0217 18:00:18.640869 140318069466880 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.6322289705276489, loss=1.0742969512939453
I0217 18:01:35.836887 140318077859584 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.5219007134437561, loss=0.9991936683654785
I0217 18:02:52.972478 140318069466880 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7053604125976562, loss=1.0470584630966187
I0217 18:04:10.079281 140318077859584 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.570630669593811, loss=1.0427777767181396
I0217 18:05:27.084084 140318069466880 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.49877727031707764, loss=1.029858946800232
I0217 18:06:45.399938 140318077859584 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5618813037872314, loss=1.0232592821121216
I0217 18:08:08.163068 140318069466880 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.7293506264686584, loss=1.00473952293396
I0217 18:09:31.827852 140318077859584 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8670421838760376, loss=1.0435858964920044
I0217 18:10:34.538192 140399019657024 spec.py:321] Evaluating on the training split.
I0217 18:11:28.395517 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 18:12:19.981561 140399019657024 spec.py:349] Evaluating on the test split.
I0217 18:12:46.150222 140399019657024 submission_runner.py:408] Time since start: 47328.26s, 	Step: 54477, 	{'train/ctc_loss': Array(0.11575904, dtype=float32), 'train/wer': 0.04398796384761528, 'validation/ctc_loss': Array(0.34668103, dtype=float32), 'validation/wer': 0.10128696525290364, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18844098, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 43245.7050819397, 'total_duration': 47328.256929636, 'accumulated_submission_time': 43245.7050819397, 'accumulated_eval_time': 4078.5655472278595, 'accumulated_logging_time': 1.6603331565856934}
I0217 18:12:46.192556 140318077859584 logging_writer.py:48] [54477] accumulated_eval_time=4078.565547, accumulated_logging_time=1.660333, accumulated_submission_time=43245.705082, global_step=54477, preemption_count=0, score=43245.705082, test/ctc_loss=0.1884409785270691, test/num_examples=2472, test/wer=0.063007, total_duration=47328.256930, train/ctc_loss=0.11575904488563538, train/wer=0.043988, validation/ctc_loss=0.34668102860450745, validation/num_examples=5348, validation/wer=0.101287
I0217 18:13:04.691290 140318069466880 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.5651450157165527, loss=0.999032735824585
I0217 18:14:25.123254 140318077859584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.6000329256057739, loss=0.9887439012527466
I0217 18:15:41.791264 140318069466880 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.5545945763587952, loss=0.9887874126434326
I0217 18:16:58.569531 140318077859584 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9061220288276672, loss=0.9870935678482056
I0217 18:18:15.331125 140318069466880 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.5984280705451965, loss=1.0064496994018555
I0217 18:19:32.109934 140318077859584 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.6007351875305176, loss=1.0028482675552368
I0217 18:20:48.940786 140318069466880 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.5887382626533508, loss=0.9693067073822021
I0217 18:22:08.849845 140318077859584 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.6241267323493958, loss=1.0290075540542603
I0217 18:23:32.922280 140318069466880 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.6057530641555786, loss=1.0312436819076538
I0217 18:24:57.666863 140318077859584 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.5698848366737366, loss=1.0174833536148071
I0217 18:26:20.834681 140318069466880 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6034244298934937, loss=1.0583972930908203
I0217 18:27:43.928350 140318077859584 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.6556078791618347, loss=1.0154305696487427
I0217 18:29:05.357929 140318077859584 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.6576982140541077, loss=1.0203639268875122
I0217 18:30:22.232812 140318069466880 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.5627240538597107, loss=1.001179814338684
I0217 18:31:39.214781 140318077859584 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.6355410814285278, loss=1.0062612295150757
I0217 18:32:56.237174 140318069466880 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.5812252163887024, loss=1.0412577390670776
I0217 18:34:13.320342 140318077859584 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6972178816795349, loss=0.9923020005226135
I0217 18:35:30.385468 140318069466880 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.5535974502563477, loss=1.0267072916030884
I0217 18:36:46.688893 140399019657024 spec.py:321] Evaluating on the training split.
I0217 18:37:42.027093 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 18:38:33.748475 140399019657024 spec.py:349] Evaluating on the test split.
I0217 18:38:59.914313 140399019657024 submission_runner.py:408] Time since start: 48902.02s, 	Step: 56295, 	{'train/ctc_loss': Array(0.10086589, dtype=float32), 'train/wer': 0.03656622642591598, 'validation/ctc_loss': Array(0.34202397, dtype=float32), 'validation/wer': 0.097714743620688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18400313, dtype=float32), 'test/wer': 0.06069099993906526, 'test/num_examples': 2472, 'score': 44686.114140987396, 'total_duration': 48902.020424842834, 'accumulated_submission_time': 44686.114140987396, 'accumulated_eval_time': 4211.784162521362, 'accumulated_logging_time': 1.719299077987671}
I0217 18:38:59.960613 140318077859584 logging_writer.py:48] [56295] accumulated_eval_time=4211.784163, accumulated_logging_time=1.719299, accumulated_submission_time=44686.114141, global_step=56295, preemption_count=0, score=44686.114141, test/ctc_loss=0.18400312960147858, test/num_examples=2472, test/wer=0.060691, total_duration=48902.020425, train/ctc_loss=0.10086589306592941, train/wer=0.036566, validation/ctc_loss=0.34202396869659424, validation/num_examples=5348, validation/wer=0.097715
I0217 18:39:04.720547 140318069466880 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6035465002059937, loss=1.020464539527893
I0217 18:40:21.579988 140318077859584 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.6139360666275024, loss=1.0033230781555176
I0217 18:41:38.688312 140318069466880 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.5742170214653015, loss=1.024256944656372
I0217 18:42:56.064840 140318077859584 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.5561309456825256, loss=1.0172052383422852
I0217 18:44:16.699807 140318077859584 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.5516706705093384, loss=1.0039952993392944
I0217 18:45:33.644390 140318069466880 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.5453015565872192, loss=1.0426454544067383
I0217 18:46:50.508483 140318077859584 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7103540301322937, loss=0.9826189875602722
I0217 18:48:07.645068 140318069466880 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.5975592136383057, loss=1.017882227897644
I0217 18:49:24.683901 140318077859584 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6803030371665955, loss=0.9892627596855164
I0217 18:50:41.702843 140318069466880 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.5980067849159241, loss=1.021762728691101
I0217 18:52:04.483310 140318077859584 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.802311360836029, loss=0.9903281331062317
I0217 18:53:27.906688 140318069466880 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.6275815367698669, loss=0.9854916334152222
I0217 18:54:50.825965 140318077859584 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.5680559873580933, loss=1.0656144618988037
I0217 18:56:15.399551 140318069466880 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.6243358254432678, loss=1.0215139389038086
I0217 18:57:41.243557 140318077859584 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.7620831727981567, loss=0.971407949924469
I0217 18:58:58.335527 140318069466880 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.6099590063095093, loss=0.9890244603157043
I0217 19:00:15.451843 140318077859584 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.5595476031303406, loss=1.0159437656402588
I0217 19:01:32.481889 140318069466880 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7270678877830505, loss=0.9858859777450562
I0217 19:02:49.542505 140318077859584 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6564833521842957, loss=0.9979721307754517
I0217 19:03:00.053499 140399019657024 spec.py:321] Evaluating on the training split.
I0217 19:03:54.773881 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 19:04:46.579307 140399019657024 spec.py:349] Evaluating on the test split.
I0217 19:05:13.337732 140399019657024 submission_runner.py:408] Time since start: 50475.45s, 	Step: 58115, 	{'train/ctc_loss': Array(0.09474793, dtype=float32), 'train/wer': 0.03643191988067334, 'validation/ctc_loss': Array(0.3363182, dtype=float32), 'validation/wer': 0.0949535128455159, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17934273, dtype=float32), 'test/wer': 0.058375479861068794, 'test/num_examples': 2472, 'score': 46126.119805812836, 'total_duration': 50475.445321798325, 'accumulated_submission_time': 46126.119805812836, 'accumulated_eval_time': 4345.063044786453, 'accumulated_logging_time': 1.7812213897705078}
I0217 19:05:13.378251 140318077859584 logging_writer.py:48] [58115] accumulated_eval_time=4345.063045, accumulated_logging_time=1.781221, accumulated_submission_time=46126.119806, global_step=58115, preemption_count=0, score=46126.119806, test/ctc_loss=0.17934273183345795, test/num_examples=2472, test/wer=0.058375, total_duration=50475.445322, train/ctc_loss=0.09474793076515198, train/wer=0.036432, validation/ctc_loss=0.3363181948661804, validation/num_examples=5348, validation/wer=0.094954
I0217 19:06:19.307828 140318069466880 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.5621927380561829, loss=1.033729910850525
I0217 19:07:36.201041 140318077859584 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.6178280711174011, loss=1.0170862674713135
I0217 19:08:52.970776 140318069466880 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.5957305431365967, loss=0.9877659678459167
I0217 19:10:09.880895 140318077859584 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.572619616985321, loss=0.961853563785553
I0217 19:11:30.835347 140318069466880 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.5578108429908752, loss=1.0246915817260742
I0217 19:12:53.887279 140318077859584 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.7931808829307556, loss=1.0039926767349243
I0217 19:14:14.539474 140318077859584 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7857698798179626, loss=0.9838043451309204
I0217 19:15:31.450454 140318069466880 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7120431065559387, loss=0.9731472134590149
I0217 19:16:48.677493 140318077859584 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.5928417444229126, loss=1.0052918195724487
I0217 19:18:05.686306 140318069466880 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6706627607345581, loss=1.0154303312301636
I0217 19:19:22.639991 140318077859584 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.7016121745109558, loss=0.972557008266449
I0217 19:20:39.480083 140318069466880 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.6478710770606995, loss=1.008499026298523
I0217 19:22:00.839650 140318077859584 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.6270322203636169, loss=1.0352036952972412
I0217 19:23:24.898423 140318069466880 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.5509292483329773, loss=1.0023244619369507
I0217 19:24:48.704128 140318077859584 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.5960100889205933, loss=1.001357913017273
I0217 19:26:11.549636 140318069466880 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.6261410117149353, loss=0.9719034433364868
I0217 19:27:34.485880 140318077859584 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7820491194725037, loss=0.9657586216926575
I0217 19:28:51.456448 140318069466880 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.6094561219215393, loss=0.9937513470649719
I0217 19:29:13.492388 140399019657024 spec.py:321] Evaluating on the training split.
I0217 19:30:07.717596 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 19:30:58.694692 140399019657024 spec.py:349] Evaluating on the test split.
I0217 19:31:24.756317 140399019657024 submission_runner.py:408] Time since start: 52046.86s, 	Step: 59930, 	{'train/ctc_loss': Array(0.07673226, dtype=float32), 'train/wer': 0.03052756372258447, 'validation/ctc_loss': Array(0.33402327, dtype=float32), 'validation/wer': 0.09432596039661315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17598493, dtype=float32), 'test/wer': 0.058375479861068794, 'test/num_examples': 2472, 'score': 47566.14169406891, 'total_duration': 52046.862721681595, 'accumulated_submission_time': 47566.14169406891, 'accumulated_eval_time': 4476.3204646110535, 'accumulated_logging_time': 1.841925859451294}
I0217 19:31:24.798798 140318077859584 logging_writer.py:48] [59930] accumulated_eval_time=4476.320465, accumulated_logging_time=1.841926, accumulated_submission_time=47566.141694, global_step=59930, preemption_count=0, score=47566.141694, test/ctc_loss=0.1759849339723587, test/num_examples=2472, test/wer=0.058375, total_duration=52046.862722, train/ctc_loss=0.07673226296901703, train/wer=0.030528, validation/ctc_loss=0.33402326703071594, validation/num_examples=5348, validation/wer=0.094326
I0217 19:32:19.103908 140318069466880 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0414574146270752, loss=1.0486756563186646
I0217 19:33:35.945753 140318077859584 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.5855896472930908, loss=0.9498625993728638
I0217 19:34:53.193391 140318069466880 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.5826396346092224, loss=0.952534019947052
I0217 19:36:10.119969 140318077859584 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.6178115010261536, loss=0.9356247782707214
I0217 19:37:27.056781 140318069466880 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.6529220342636108, loss=0.9961825013160706
I0217 19:38:43.995954 140318077859584 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.6386231780052185, loss=1.0207695960998535
I0217 19:40:06.334847 140318069466880 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.6508418917655945, loss=0.9970613121986389
I0217 19:41:29.799090 140318077859584 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7857314348220825, loss=0.9660886526107788
I0217 19:42:54.638793 140318077859584 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.5959604978561401, loss=0.9596478939056396
I0217 19:44:11.456679 140318069466880 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7864448428153992, loss=0.9687021970748901
I0217 19:45:28.394415 140318077859584 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9070234298706055, loss=0.9617460370063782
I0217 19:46:45.346215 140318069466880 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.8899332880973816, loss=0.9705555438995361
I0217 19:48:02.210105 140318077859584 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.7172123789787292, loss=0.9473094344139099
I0217 19:49:19.092916 140318069466880 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.6395877599716187, loss=0.989224910736084
I0217 19:50:38.674893 140318077859584 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.6955960988998413, loss=0.960821807384491
I0217 19:52:00.739037 140318069466880 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.5757787823677063, loss=0.9559751749038696
I0217 19:53:24.204472 140318077859584 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.5759027004241943, loss=0.9690152406692505
I0217 19:54:47.672613 140318069466880 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.6003988981246948, loss=1.0319743156433105
I0217 19:55:24.800520 140399019657024 spec.py:321] Evaluating on the training split.
I0217 19:56:19.589902 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 19:57:10.954248 140399019657024 spec.py:349] Evaluating on the test split.
I0217 19:57:37.171276 140399019657024 submission_runner.py:408] Time since start: 53619.28s, 	Step: 61747, 	{'train/ctc_loss': Array(0.07129591, dtype=float32), 'train/wer': 0.028381517879350764, 'validation/ctc_loss': Array(0.3258931, dtype=float32), 'validation/wer': 0.09269432402946601, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17221192, dtype=float32), 'test/wer': 0.05610058294233543, 'test/num_examples': 2472, 'score': 49006.05555701256, 'total_duration': 53619.27761530876, 'accumulated_submission_time': 49006.05555701256, 'accumulated_eval_time': 4608.684624910355, 'accumulated_logging_time': 1.9001836776733398}
I0217 19:57:37.218289 140318077859584 logging_writer.py:48] [61747] accumulated_eval_time=4608.684625, accumulated_logging_time=1.900184, accumulated_submission_time=49006.055557, global_step=61747, preemption_count=0, score=49006.055557, test/ctc_loss=0.1722119152545929, test/num_examples=2472, test/wer=0.056101, total_duration=53619.277615, train/ctc_loss=0.07129590958356857, train/wer=0.028382, validation/ctc_loss=0.3258931040763855, validation/num_examples=5348, validation/wer=0.092694
I0217 19:58:22.169041 140318077859584 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.686326265335083, loss=1.014028549194336
I0217 19:59:39.151114 140318069466880 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.6660907864570618, loss=0.9533692598342896
I0217 20:00:56.197008 140318077859584 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.5929047465324402, loss=0.945499062538147
I0217 20:02:13.288231 140318069466880 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.6032909750938416, loss=0.9543521404266357
I0217 20:03:30.366660 140318077859584 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.5852548480033875, loss=0.9140638113021851
I0217 20:04:47.466675 140318069466880 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9267798662185669, loss=0.9512685537338257
I0217 20:06:09.038578 140318077859584 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.6414093375205994, loss=0.9743601083755493
I0217 20:07:32.399500 140318069466880 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.6282151937484741, loss=0.9514615535736084
I0217 20:08:56.396028 140318077859584 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.6319994330406189, loss=0.9677331447601318
I0217 20:10:20.069185 140318069466880 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.7383372783660889, loss=0.9254204034805298
I0217 20:11:43.784738 140318077859584 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9409356713294983, loss=0.9451228976249695
I0217 20:13:05.587798 140318077859584 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.5981244444847107, loss=0.931429922580719
I0217 20:14:22.629461 140318069466880 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.526416003704071, loss=0.9252055883407593
I0217 20:15:39.765981 140318077859584 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.6253392696380615, loss=0.9395476579666138
I0217 20:16:56.716980 140318069466880 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.6659699082374573, loss=0.940173327922821
I0217 20:18:13.623435 140318077859584 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.6757140755653381, loss=1.0179226398468018
I0217 20:19:30.731823 140318069466880 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7183434963226318, loss=0.9870653748512268
I0217 20:20:51.459189 140318077859584 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.6263251304626465, loss=0.9272547960281372
I0217 20:21:37.879463 140399019657024 spec.py:321] Evaluating on the training split.
I0217 20:22:32.286743 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 20:23:24.062419 140399019657024 spec.py:349] Evaluating on the test split.
I0217 20:23:50.501446 140399019657024 submission_runner.py:408] Time since start: 55192.61s, 	Step: 63558, 	{'train/ctc_loss': Array(0.07846448, dtype=float32), 'train/wer': 0.030398028236006312, 'validation/ctc_loss': Array(0.3151095, dtype=float32), 'validation/wer': 0.08974000019309306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16798735, dtype=float32), 'test/wer': 0.05618182926086162, 'test/num_examples': 2472, 'score': 50446.629410505295, 'total_duration': 55192.60791397095, 'accumulated_submission_time': 50446.629410505295, 'accumulated_eval_time': 4741.300411224365, 'accumulated_logging_time': 1.9632997512817383}
I0217 20:23:50.543667 140318077859584 logging_writer.py:48] [63558] accumulated_eval_time=4741.300411, accumulated_logging_time=1.963300, accumulated_submission_time=50446.629411, global_step=63558, preemption_count=0, score=50446.629411, test/ctc_loss=0.16798734664916992, test/num_examples=2472, test/wer=0.056182, total_duration=55192.607914, train/ctc_loss=0.07846447825431824, train/wer=0.030398, validation/ctc_loss=0.3151094913482666, validation/num_examples=5348, validation/wer=0.089740
I0217 20:24:23.589014 140318069466880 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.7793585658073425, loss=0.9817419648170471
I0217 20:25:40.435400 140318077859584 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.7158949375152588, loss=0.9662746787071228
I0217 20:26:57.750068 140318069466880 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.588131308555603, loss=0.9196068644523621
I0217 20:28:18.247305 140318077859584 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.5675280094146729, loss=0.9588586091995239
I0217 20:29:35.069002 140318069466880 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.6053522825241089, loss=0.9246526956558228
I0217 20:30:52.005199 140318077859584 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.6191327571868896, loss=0.9284330010414124
I0217 20:32:08.865965 140318069466880 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.6457156538963318, loss=0.9842541813850403
I0217 20:33:25.786432 140318077859584 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.7401075959205627, loss=0.969651460647583
I0217 20:34:42.651023 140318069466880 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.8884618282318115, loss=0.9365218281745911
I0217 20:36:02.974760 140318077859584 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.5488492250442505, loss=0.9054076075553894
I0217 20:37:26.170698 140318069466880 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.685084879398346, loss=0.9052680134773254
I0217 20:38:49.848524 140318077859584 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.0202605724334717, loss=0.949067234992981
I0217 20:40:13.616728 140318069466880 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.6746017932891846, loss=0.9102386832237244
I0217 20:41:39.427764 140318077859584 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.8557003140449524, loss=0.9776600003242493
I0217 20:42:56.508311 140318069466880 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.8608033061027527, loss=0.9260360598564148
I0217 20:44:13.378517 140318077859584 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.859462559223175, loss=0.9099042415618896
I0217 20:45:31.164199 140318069466880 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.587470531463623, loss=0.9258065819740295
I0217 20:46:47.953619 140318077859584 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.5828857421875, loss=0.96393221616745
I0217 20:47:50.585519 140399019657024 spec.py:321] Evaluating on the training split.
I0217 20:48:45.595339 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 20:49:37.482075 140399019657024 spec.py:349] Evaluating on the test split.
I0217 20:50:03.749446 140399019657024 submission_runner.py:408] Time since start: 56765.86s, 	Step: 65383, 	{'train/ctc_loss': Array(0.07131476, dtype=float32), 'train/wer': 0.02782980721571492, 'validation/ctc_loss': Array(0.31282473, dtype=float32), 'validation/wer': 0.08853316856058778, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1647859, dtype=float32), 'test/wer': 0.052850730201287756, 'test/num_examples': 2472, 'score': 51886.58189582825, 'total_duration': 56765.85677433014, 'accumulated_submission_time': 51886.58189582825, 'accumulated_eval_time': 4874.458735466003, 'accumulated_logging_time': 2.022402763366699}
I0217 20:50:03.795611 140318077859584 logging_writer.py:48] [65383] accumulated_eval_time=4874.458735, accumulated_logging_time=2.022403, accumulated_submission_time=51886.581896, global_step=65383, preemption_count=0, score=51886.581896, test/ctc_loss=0.16478590667247772, test/num_examples=2472, test/wer=0.052851, total_duration=56765.856774, train/ctc_loss=0.07131475955247879, train/wer=0.027830, validation/ctc_loss=0.31282472610473633, validation/num_examples=5348, validation/wer=0.088533
I0217 20:50:17.639264 140318069466880 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.7936668395996094, loss=0.9430882334709167
I0217 20:51:34.243238 140318077859584 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.6228309869766235, loss=0.932871401309967
I0217 20:52:51.113455 140318069466880 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.7057582139968872, loss=0.9835799336433411
I0217 20:54:08.077642 140318077859584 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.6999844312667847, loss=0.9403723478317261
I0217 20:55:24.864182 140318069466880 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.6982859969139099, loss=0.965423583984375
I0217 20:56:46.651582 140318077859584 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.7309421896934509, loss=0.8948346972465515
I0217 20:58:08.211874 140318077859584 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.6423830389976501, loss=0.9248083829879761
I0217 20:59:25.259043 140318069466880 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.6615078449249268, loss=0.9278706908226013
I0217 21:00:42.590442 140318077859584 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0228674411773682, loss=0.8876490592956543
I0217 21:01:59.659189 140318069466880 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.6606255769729614, loss=0.9462549090385437
I0217 21:03:16.568391 140318077859584 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.6065448522567749, loss=0.8917698860168457
I0217 21:04:33.521874 140318069466880 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8204692006111145, loss=0.9508625268936157
I0217 21:05:55.695165 140318077859584 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.6777781248092651, loss=0.9675014615058899
I0217 21:07:18.627014 140318069466880 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.6949520707130432, loss=0.926620364189148
I0217 21:08:43.012747 140318077859584 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.7139427065849304, loss=0.9181699752807617
I0217 21:10:06.838315 140318069466880 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.7024428248405457, loss=0.9219523072242737
I0217 21:11:30.697131 140318077859584 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.5977448225021362, loss=0.9149562120437622
I0217 21:12:47.763312 140318069466880 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.7275789976119995, loss=0.9232687950134277
I0217 21:14:04.686113 140318077859584 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.7452645897865295, loss=0.9211891889572144
I0217 21:14:04.696692 140399019657024 spec.py:321] Evaluating on the training split.
I0217 21:14:59.196731 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 21:15:50.504668 140399019657024 spec.py:349] Evaluating on the test split.
I0217 21:16:16.934581 140399019657024 submission_runner.py:408] Time since start: 58339.04s, 	Step: 67201, 	{'train/ctc_loss': Array(0.06927087, dtype=float32), 'train/wer': 0.02621635247584447, 'validation/ctc_loss': Array(0.30948767, dtype=float32), 'validation/wer': 0.0870560066424013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16259553, dtype=float32), 'test/wer': 0.05258667966607763, 'test/num_examples': 2472, 'score': 53327.39374256134, 'total_duration': 58339.04134559631, 'accumulated_submission_time': 53327.39374256134, 'accumulated_eval_time': 5006.690465927124, 'accumulated_logging_time': 2.087104558944702}
I0217 21:16:16.979276 140318077859584 logging_writer.py:48] [67201] accumulated_eval_time=5006.690466, accumulated_logging_time=2.087105, accumulated_submission_time=53327.393743, global_step=67201, preemption_count=0, score=53327.393743, test/ctc_loss=0.16259552538394928, test/num_examples=2472, test/wer=0.052587, total_duration=58339.041346, train/ctc_loss=0.06927087157964706, train/wer=0.026216, validation/ctc_loss=0.3094876706600189, validation/num_examples=5348, validation/wer=0.087056
I0217 21:17:33.638462 140318069466880 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.8658562302589417, loss=0.9348495602607727
I0217 21:18:50.927909 140318077859584 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.6958084106445312, loss=0.916843831539154
I0217 21:20:07.932043 140318069466880 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.5679047703742981, loss=0.8982407450675964
I0217 21:21:24.983929 140318077859584 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.8416566848754883, loss=0.9569800496101379
I0217 21:22:42.090128 140318069466880 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.6370980739593506, loss=0.9285387396812439
I0217 21:24:04.075290 140318077859584 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.6656739711761475, loss=0.948102593421936
I0217 21:25:27.261641 140318069466880 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.5825164914131165, loss=0.9167534112930298
I0217 21:26:52.745447 140318077859584 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8070855140686035, loss=0.891667366027832
I0217 21:28:09.565388 140318069466880 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.6921426057815552, loss=0.9605808854103088
I0217 21:29:26.505728 140318077859584 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.56977778673172, loss=0.9290160536766052
I0217 21:30:43.477344 140318069466880 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.6253987550735474, loss=0.9463137984275818
I0217 21:32:00.512667 140318077859584 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.6151427626609802, loss=0.9091551899909973
I0217 21:33:17.301209 140318069466880 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.7230424284934998, loss=0.9270725846290588
I0217 21:34:34.551977 140318077859584 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.8204195499420166, loss=0.9105247259140015
I0217 21:35:56.663412 140318069466880 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.7851713299751282, loss=0.9028811454772949
I0217 21:37:19.967130 140318077859584 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.5923736691474915, loss=0.8648905158042908
I0217 21:38:44.270577 140318069466880 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.6732843518257141, loss=0.9782809019088745
I0217 21:40:07.966361 140318077859584 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.9210602641105652, loss=0.91953045129776
I0217 21:40:19.088570 140399019657024 spec.py:321] Evaluating on the training split.
I0217 21:41:14.518086 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 21:42:06.240294 140399019657024 spec.py:349] Evaluating on the test split.
I0217 21:42:32.314177 140399019657024 submission_runner.py:408] Time since start: 59914.42s, 	Step: 69011, 	{'train/ctc_loss': Array(0.05992661, dtype=float32), 'train/wer': 0.02337353454989371, 'validation/ctc_loss': Array(0.30565014, dtype=float32), 'validation/wer': 0.08574297382623555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15818636, dtype=float32), 'test/wer': 0.05134767330855321, 'test/num_examples': 2472, 'score': 54769.413890361786, 'total_duration': 59914.420617341995, 'accumulated_submission_time': 54769.413890361786, 'accumulated_eval_time': 5139.9097690582275, 'accumulated_logging_time': 2.1487863063812256}
I0217 21:42:32.359220 140318077859584 logging_writer.py:48] [69011] accumulated_eval_time=5139.909769, accumulated_logging_time=2.148786, accumulated_submission_time=54769.413890, global_step=69011, preemption_count=0, score=54769.413890, test/ctc_loss=0.15818636119365692, test/num_examples=2472, test/wer=0.051348, total_duration=59914.420617, train/ctc_loss=0.05992661416530609, train/wer=0.023374, validation/ctc_loss=0.30565014481544495, validation/num_examples=5348, validation/wer=0.085743
I0217 21:43:41.245585 140318069466880 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.6360706090927124, loss=0.8963806629180908
I0217 21:44:58.017325 140318077859584 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.6722027659416199, loss=0.9105001091957092
I0217 21:46:14.805569 140318069466880 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.9066871404647827, loss=0.9100291728973389
I0217 21:47:31.856194 140318077859584 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.699770987033844, loss=0.9446583986282349
I0217 21:48:48.886882 140318069466880 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.664913535118103, loss=0.9425089359283447
I0217 21:50:05.858860 140318077859584 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.8012079000473022, loss=0.8789243102073669
I0217 21:51:22.775829 140318069466880 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.6410399675369263, loss=0.8826424479484558
I0217 21:52:42.500512 140318077859584 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.6987327933311462, loss=0.9030289649963379
I0217 21:54:05.927761 140318069466880 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.6557690501213074, loss=0.8968713283538818
I0217 21:55:28.914594 140318077859584 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.7304220795631409, loss=0.9150647521018982
I0217 21:56:51.833181 140318077859584 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.6495111584663391, loss=0.8959816694259644
I0217 21:58:08.663237 140318069466880 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.6690696477890015, loss=0.8696831464767456
I0217 21:59:25.549074 140318077859584 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.6164831519126892, loss=0.9056204557418823
I0217 22:00:42.397539 140318069466880 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.656629741191864, loss=0.9045407176017761
I0217 22:01:59.376162 140318077859584 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.711699903011322, loss=0.8896464705467224
I0217 22:03:16.539459 140318069466880 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.8443029522895813, loss=0.9540687799453735
I0217 22:04:37.322106 140318077859584 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.5935307145118713, loss=0.8912102580070496
I0217 22:06:00.893756 140318069466880 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.7557401061058044, loss=0.9598255753517151
I0217 22:06:32.694630 140399019657024 spec.py:321] Evaluating on the training split.
I0217 22:07:28.353700 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 22:08:19.528831 140399019657024 spec.py:349] Evaluating on the test split.
I0217 22:08:45.733223 140399019657024 submission_runner.py:408] Time since start: 61487.84s, 	Step: 70839, 	{'train/ctc_loss': Array(0.05449369, dtype=float32), 'train/wer': 0.020615387821447922, 'validation/ctc_loss': Array(0.30036068, dtype=float32), 'validation/wer': 0.08332931056122499, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15762733, dtype=float32), 'test/wer': 0.050657079601080575, 'test/num_examples': 2472, 'score': 56209.6598546505, 'total_duration': 61487.83983325958, 'accumulated_submission_time': 56209.6598546505, 'accumulated_eval_time': 5272.942055225372, 'accumulated_logging_time': 2.2103707790374756}
I0217 22:08:45.781177 140318077859584 logging_writer.py:48] [70839] accumulated_eval_time=5272.942055, accumulated_logging_time=2.210371, accumulated_submission_time=56209.659855, global_step=70839, preemption_count=0, score=56209.659855, test/ctc_loss=0.15762732923030853, test/num_examples=2472, test/wer=0.050657, total_duration=61487.839833, train/ctc_loss=0.05449368804693222, train/wer=0.020615, validation/ctc_loss=0.30036067962646484, validation/num_examples=5348, validation/wer=0.083329
I0217 22:09:33.286525 140318069466880 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8885972499847412, loss=0.8780550956726074
I0217 22:10:50.328197 140318077859584 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7146205902099609, loss=0.8667870163917542
I0217 22:12:10.612597 140318077859584 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.6197609305381775, loss=0.885873556137085
I0217 22:13:27.476094 140318069466880 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.6587432026863098, loss=0.8888054490089417
I0217 22:14:44.319120 140318077859584 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.6741361618041992, loss=0.9017085433006287
I0217 22:16:00.991728 140318069466880 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.7767568230628967, loss=0.9463937878608704
I0217 22:17:17.921644 140318077859584 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.8284068703651428, loss=0.8993991613388062
I0217 22:18:34.747625 140318069466880 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.7321050763130188, loss=0.8903294801712036
I0217 22:19:56.452391 140318077859584 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.647478461265564, loss=0.9017007350921631
I0217 22:21:18.869068 140318069466880 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.7281113266944885, loss=0.9069375395774841
I0217 22:22:42.137274 140318077859584 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.6250649690628052, loss=0.8587367534637451
I0217 22:24:05.701690 140318069466880 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.6479167342185974, loss=0.9010632634162903
I0217 22:25:31.994347 140318077859584 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.6673789620399475, loss=0.8957928419113159
I0217 22:26:49.122254 140318069466880 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.6264464259147644, loss=0.8395500779151917
I0217 22:28:06.126900 140318077859584 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.5523641109466553, loss=0.8903729915618896
I0217 22:29:23.036318 140318069466880 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.6286795735359192, loss=0.886441171169281
I0217 22:30:39.968075 140318077859584 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.6383816599845886, loss=0.899570882320404
I0217 22:31:57.126479 140318069466880 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.6997315883636475, loss=0.8786492943763733
I0217 22:32:46.055143 140399019657024 spec.py:321] Evaluating on the training split.
I0217 22:33:41.421015 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 22:34:33.047734 140399019657024 spec.py:349] Evaluating on the test split.
I0217 22:34:59.154140 140399019657024 submission_runner.py:408] Time since start: 63061.26s, 	Step: 72665, 	{'train/ctc_loss': Array(0.05953811, dtype=float32), 'train/wer': 0.02181967045338246, 'validation/ctc_loss': Array(0.29563853, dtype=float32), 'validation/wer': 0.08195834982669897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15622462, dtype=float32), 'test/wer': 0.05004773221213414, 'test/num_examples': 2472, 'score': 57649.845106601715, 'total_duration': 63061.260159015656, 'accumulated_submission_time': 57649.845106601715, 'accumulated_eval_time': 5406.034177541733, 'accumulated_logging_time': 2.274899482727051}
I0217 22:34:59.197480 140318077859584 logging_writer.py:48] [72665] accumulated_eval_time=5406.034178, accumulated_logging_time=2.274899, accumulated_submission_time=57649.845107, global_step=72665, preemption_count=0, score=57649.845107, test/ctc_loss=0.15622462332248688, test/num_examples=2472, test/wer=0.050048, total_duration=63061.260159, train/ctc_loss=0.0595381073653698, train/wer=0.021820, validation/ctc_loss=0.2956385314464569, validation/num_examples=5348, validation/wer=0.081958
I0217 22:35:26.856792 140318069466880 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.8366987109184265, loss=0.902352511882782
I0217 22:36:43.608734 140318077859584 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3092241287231445, loss=0.8887501955032349
I0217 22:38:00.514817 140318069466880 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.6764562129974365, loss=0.9033045768737793
I0217 22:39:17.379861 140318077859584 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.7298853397369385, loss=0.912484347820282
I0217 22:40:34.216778 140318069466880 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.0905773639678955, loss=0.9286291003227234
I0217 22:41:54.520952 140318077859584 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1023577451705933, loss=0.925395131111145
I0217 22:43:11.330264 140318069466880 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.849457859992981, loss=0.9182629585266113
I0217 22:44:28.528577 140318077859584 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.6927220225334167, loss=0.8797542452812195
I0217 22:45:45.448245 140318069466880 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.7347339391708374, loss=0.831601619720459
I0217 22:47:02.318979 140318077859584 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.7887853384017944, loss=0.9205063581466675
I0217 22:48:19.241547 140318069466880 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.6103437542915344, loss=0.8953886032104492
I0217 22:49:38.829881 140318077859584 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.6885864734649658, loss=0.8995336890220642
I0217 22:51:01.605867 140318069466880 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.6783095598220825, loss=0.8905117511749268
I0217 22:52:24.864186 140318077859584 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.9132251739501953, loss=0.8849713802337646
I0217 22:53:48.538034 140318069466880 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.838839590549469, loss=0.8866158127784729
I0217 22:55:12.862890 140318077859584 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.8508313298225403, loss=0.9023467898368835
I0217 22:56:29.587742 140318069466880 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.646557629108429, loss=0.9113407135009766
I0217 22:57:46.513072 140318077859584 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.0544335842132568, loss=0.8942191004753113
I0217 22:58:59.206503 140399019657024 spec.py:321] Evaluating on the training split.
I0217 22:59:53.000938 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 23:00:44.729084 140399019657024 spec.py:349] Evaluating on the test split.
I0217 23:01:11.047901 140399019657024 submission_runner.py:408] Time since start: 64633.16s, 	Step: 74496, 	{'train/ctc_loss': Array(0.05710617, dtype=float32), 'train/wer': 0.021032742192350257, 'validation/ctc_loss': Array(0.29521686, dtype=float32), 'validation/wer': 0.08235419060216072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15535073, dtype=float32), 'test/wer': 0.04974305851766092, 'test/num_examples': 2472, 'score': 59089.764125585556, 'total_duration': 64633.15510225296, 'accumulated_submission_time': 59089.764125585556, 'accumulated_eval_time': 5537.869856595993, 'accumulated_logging_time': 2.33532977104187}
I0217 23:01:11.100206 140318077859584 logging_writer.py:48] [74496] accumulated_eval_time=5537.869857, accumulated_logging_time=2.335330, accumulated_submission_time=59089.764126, global_step=74496, preemption_count=0, score=59089.764126, test/ctc_loss=0.1553507298231125, test/num_examples=2472, test/wer=0.049743, total_duration=64633.155102, train/ctc_loss=0.057106174528598785, train/wer=0.021033, validation/ctc_loss=0.2952168583869934, validation/num_examples=5348, validation/wer=0.082354
I0217 23:01:15.028990 140318069466880 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.7454313039779663, loss=0.8507405519485474
I0217 23:02:31.952978 140318077859584 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.7271212935447693, loss=0.880022406578064
I0217 23:03:48.823102 140318069466880 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.103654146194458, loss=0.9085041284561157
I0217 23:05:05.750568 140318077859584 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.7079857587814331, loss=0.8127016425132751
I0217 23:06:22.658766 140318069466880 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.6405009627342224, loss=0.888396680355072
I0217 23:07:40.729150 140318077859584 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.9864248633384705, loss=0.8870176076889038
I0217 23:09:03.409454 140318069466880 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.8366405367851257, loss=0.8585004210472107
I0217 23:10:29.836304 140318077859584 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.6662285327911377, loss=0.9012227654457092
I0217 23:11:46.604414 140318069466880 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.6873726844787598, loss=0.8745430707931519
I0217 23:13:03.572572 140318077859584 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.7245534062385559, loss=0.9183667898178101
I0217 23:14:20.521172 140318069466880 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.9117221236228943, loss=0.8975149393081665
I0217 23:15:37.389285 140318077859584 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.6942061185836792, loss=0.8693886399269104
I0217 23:16:54.222088 140318069466880 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.6928495168685913, loss=0.8691184520721436
I0217 23:18:11.602591 140318077859584 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.5305626392364502, loss=0.8941158652305603
I0217 23:19:34.940476 140318069466880 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.85833740234375, loss=0.8974371552467346
I0217 23:20:58.328405 140318077859584 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.6215604543685913, loss=0.9000942707061768
I0217 23:22:21.180576 140318069466880 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.8627824783325195, loss=0.9069466590881348
I0217 23:23:44.779329 140318077859584 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.6070120334625244, loss=0.9043093323707581
I0217 23:25:05.881758 140318077859584 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.7549950480461121, loss=0.9459226131439209
I0217 23:25:11.754523 140399019657024 spec.py:321] Evaluating on the training split.
I0217 23:26:05.242716 140399019657024 spec.py:333] Evaluating on the validation split.
I0217 23:26:56.708777 140399019657024 spec.py:349] Evaluating on the test split.
I0217 23:27:22.636835 140399019657024 submission_runner.py:408] Time since start: 66204.74s, 	Step: 76309, 	{'train/ctc_loss': Array(0.05786657, dtype=float32), 'train/wer': 0.021383041405538594, 'validation/ctc_loss': Array(0.2951434, dtype=float32), 'validation/wer': 0.08191007656139877, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15429394, dtype=float32), 'test/wer': 0.04970243535839782, 'test/num_examples': 2472, 'score': 60530.329689741135, 'total_duration': 66204.74381828308, 'accumulated_submission_time': 60530.329689741135, 'accumulated_eval_time': 5668.746239900589, 'accumulated_logging_time': 2.404423713684082}
I0217 23:27:22.679934 140318077859584 logging_writer.py:48] [76309] accumulated_eval_time=5668.746240, accumulated_logging_time=2.404424, accumulated_submission_time=60530.329690, global_step=76309, preemption_count=0, score=60530.329690, test/ctc_loss=0.1542939394712448, test/num_examples=2472, test/wer=0.049702, total_duration=66204.743818, train/ctc_loss=0.057866569608449936, train/wer=0.021383, validation/ctc_loss=0.29514339566230774, validation/num_examples=5348, validation/wer=0.081910
I0217 23:28:33.164407 140318069466880 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.7104123830795288, loss=0.8940467238426208
I0217 23:29:50.014760 140318077859584 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.7023668885231018, loss=0.8832166790962219
I0217 23:31:07.002814 140318069466880 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.819071352481842, loss=0.9052643179893494
I0217 23:32:23.921059 140318077859584 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.6381760239601135, loss=0.8865453004837036
I0217 23:33:40.863328 140318069466880 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2304222583770752, loss=0.8630854487419128
I0217 23:34:57.870066 140318077859584 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.7744269371032715, loss=0.8642870783805847
I0217 23:36:17.188417 140318069466880 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.827591598033905, loss=0.9077929258346558
I0217 23:36:20.766645 140318077859584 logging_writer.py:48] [77006] global_step=77006, preemption_count=0, score=61068.353976
I0217 23:36:21.663127 140399019657024 checkpoints.py:490] Saving checkpoint at step: 77006
I0217 23:36:23.149780 140399019657024 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5/checkpoint_77006
I0217 23:36:23.184024 140399019657024 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_conformer_jax/trial_5/checkpoint_77006.
I0217 23:36:26.683600 140399019657024 submission_runner.py:583] Tuning trial 5/5
I0217 23:36:26.683836 140399019657024 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0217 23:36:26.710518 140399019657024 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.477173, dtype=float32), 'train/wer': 1.3851183971898189, 'validation/ctc_loss': Array(31.16359, dtype=float32), 'validation/wer': 1.043156299178389, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.27949, dtype=float32), 'test/wer': 1.0976986980277457, 'test/num_examples': 2472, 'score': 36.68635892868042, 'total_duration': 172.50863242149353, 'accumulated_submission_time': 36.68635892868042, 'accumulated_eval_time': 135.82221722602844, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1795, {'train/ctc_loss': Array(3.4333935, dtype=float32), 'train/wer': 0.631694270799768, 'validation/ctc_loss': Array(3.3112328, dtype=float32), 'validation/wer': 0.6119022562924201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.9762988, dtype=float32), 'test/wer': 0.5722178213799687, 'test/num_examples': 2472, 'score': 1477.175707578659, 'total_duration': 1740.0544860363007, 'accumulated_submission_time': 1477.175707578659, 'accumulated_eval_time': 262.77818775177, 'accumulated_logging_time': 0.02735280990600586, 'global_step': 1795, 'preemption_count': 0}), (3620, {'train/ctc_loss': Array(0.8712847, dtype=float32), 'train/wer': 0.27545211274931947, 'validation/ctc_loss': Array(0.91919994, dtype=float32), 'validation/wer': 0.2688917423752377, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64018565, dtype=float32), 'test/wer': 0.20916864704568075, 'test/num_examples': 2472, 'score': 2917.3443970680237, 'total_duration': 3311.0987231731415, 'accumulated_submission_time': 2917.3443970680237, 'accumulated_eval_time': 393.52325439453125, 'accumulated_logging_time': 0.0794527530670166, 'global_step': 3620, 'preemption_count': 0}), (5441, {'train/ctc_loss': Array(0.5577522, dtype=float32), 'train/wer': 0.18933290277441403, 'validation/ctc_loss': Array(0.7517841, dtype=float32), 'validation/wer': 0.224654122054124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49422714, dtype=float32), 'test/wer': 0.1648894034489062, 'test/num_examples': 2472, 'score': 4357.893091678619, 'total_duration': 4883.128641843796, 'accumulated_submission_time': 4357.893091678619, 'accumulated_eval_time': 524.8741371631622, 'accumulated_logging_time': 0.1293325424194336, 'global_step': 5441, 'preemption_count': 0}), (7243, {'train/ctc_loss': Array(0.548139, dtype=float32), 'train/wer': 0.18435576427215572, 'validation/ctc_loss': Array(0.6662565, dtype=float32), 'validation/wer': 0.20172432103652355, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4213605, dtype=float32), 'test/wer': 0.14240448479678264, 'test/num_examples': 2472, 'score': 5798.3205742836, 'total_duration': 6456.4077224731445, 'accumulated_submission_time': 5798.3205742836, 'accumulated_eval_time': 657.5992331504822, 'accumulated_logging_time': 0.17917275428771973, 'global_step': 7243, 'preemption_count': 0}), (9061, {'train/ctc_loss': Array(0.44564787, dtype=float32), 'train/wer': 0.15706012794537594, 'validation/ctc_loss': Array(0.6208973, dtype=float32), 'validation/wer': 0.1878312752831227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.383486, dtype=float32), 'test/wer': 0.13027847175674853, 'test/num_examples': 2472, 'score': 7238.6869995594025, 'total_duration': 8030.393237113953, 'accumulated_submission_time': 7238.6869995594025, 'accumulated_eval_time': 791.0809001922607, 'accumulated_logging_time': 0.23787641525268555, 'global_step': 9061, 'preemption_count': 0}), (10883, {'train/ctc_loss': Array(0.4268927, dtype=float32), 'train/wer': 0.15219139417031405, 'validation/ctc_loss': Array(0.58681077, dtype=float32), 'validation/wer': 0.17565675777440937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36785704, dtype=float32), 'test/wer': 0.12526151158775617, 'test/num_examples': 2472, 'score': 8678.63923573494, 'total_duration': 9603.92083120346, 'accumulated_submission_time': 8678.63923573494, 'accumulated_eval_time': 924.5182108879089, 'accumulated_logging_time': 0.2955894470214844, 'global_step': 10883, 'preemption_count': 0}), (12713, {'train/ctc_loss': Array(0.40124416, dtype=float32), 'train/wer': 0.13860109289617487, 'validation/ctc_loss': Array(0.5638952, dtype=float32), 'validation/wer': 0.16918814022418105, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3447162, dtype=float32), 'test/wer': 0.11577600389982329, 'test/num_examples': 2472, 'score': 10118.762867212296, 'total_duration': 11176.468502998352, 'accumulated_submission_time': 10118.762867212296, 'accumulated_eval_time': 1056.8081135749817, 'accumulated_logging_time': 0.352435827255249, 'global_step': 12713, 'preemption_count': 0}), (14529, {'train/ctc_loss': Array(0.3907849, dtype=float32), 'train/wer': 0.13760074023726432, 'validation/ctc_loss': Array(0.5494791, dtype=float32), 'validation/wer': 0.1650462940614229, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33356386, dtype=float32), 'test/wer': 0.11238397010135479, 'test/num_examples': 2472, 'score': 11559.307371377945, 'total_duration': 12746.616312265396, 'accumulated_submission_time': 11559.307371377945, 'accumulated_eval_time': 1186.2805242538452, 'accumulated_logging_time': 0.4058682918548584, 'global_step': 14529, 'preemption_count': 0}), (16345, {'train/ctc_loss': Array(0.3892411, dtype=float32), 'train/wer': 0.1341954468019204, 'validation/ctc_loss': Array(0.5266665, dtype=float32), 'validation/wer': 0.1597748534906398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31609333, dtype=float32), 'test/wer': 0.10793573416204578, 'test/num_examples': 2472, 'score': 12999.903662443161, 'total_duration': 14319.482456922531, 'accumulated_submission_time': 12999.903662443161, 'accumulated_eval_time': 1318.4176275730133, 'accumulated_logging_time': 0.46091723442077637, 'global_step': 16345, 'preemption_count': 0}), (18162, {'train/ctc_loss': Array(0.37243292, dtype=float32), 'train/wer': 0.13075699375315253, 'validation/ctc_loss': Array(0.5188964, dtype=float32), 'validation/wer': 0.15627021442984446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30815408, dtype=float32), 'test/wer': 0.10507180143399752, 'test/num_examples': 2472, 'score': 14440.013365745544, 'total_duration': 15890.79479265213, 'accumulated_submission_time': 14440.013365745544, 'accumulated_eval_time': 1449.492838382721, 'accumulated_logging_time': 0.5100352764129639, 'global_step': 18162, 'preemption_count': 0}), (19980, {'train/ctc_loss': Array(0.37521964, dtype=float32), 'train/wer': 0.13682820361485915, 'validation/ctc_loss': Array(0.50475585, dtype=float32), 'validation/wer': 0.1534027824710119, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30493766, dtype=float32), 'test/wer': 0.1033656287449475, 'test/num_examples': 2472, 'score': 15880.443354845047, 'total_duration': 17460.340955257416, 'accumulated_submission_time': 15880.443354845047, 'accumulated_eval_time': 1578.4773724079132, 'accumulated_logging_time': 0.5645277500152588, 'global_step': 19980, 'preemption_count': 0}), (21786, {'train/ctc_loss': Array(0.31174734, dtype=float32), 'train/wer': 0.11060887183355264, 'validation/ctc_loss': Array(0.49118516, dtype=float32), 'validation/wer': 0.1480830686349286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29324007, dtype=float32), 'test/wer': 0.09936424755753255, 'test/num_examples': 2472, 'score': 17320.393052101135, 'total_duration': 19030.757175445557, 'accumulated_submission_time': 17320.393052101135, 'accumulated_eval_time': 1708.8097307682037, 'accumulated_logging_time': 0.6217460632324219, 'global_step': 21786, 'preemption_count': 0}), (23592, {'train/ctc_loss': Array(0.32217106, dtype=float32), 'train/wer': 0.11466811358292639, 'validation/ctc_loss': Array(0.4742398, dtype=float32), 'validation/wer': 0.1427440454927252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2847809, dtype=float32), 'test/wer': 0.09617532955537952, 'test/num_examples': 2472, 'score': 18760.32162618637, 'total_duration': 20601.35322713852, 'accumulated_submission_time': 18760.32162618637, 'accumulated_eval_time': 1839.3516829013824, 'accumulated_logging_time': 0.6704885959625244, 'global_step': 23592, 'preemption_count': 0}), (25405, {'train/ctc_loss': Array(0.31540608, dtype=float32), 'train/wer': 0.11191176220662463, 'validation/ctc_loss': Array(0.46801168, dtype=float32), 'validation/wer': 0.1400021240236732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2735938, dtype=float32), 'test/wer': 0.09195052099201755, 'test/num_examples': 2472, 'score': 20200.92087650299, 'total_duration': 22172.902831554413, 'accumulated_submission_time': 20200.92087650299, 'accumulated_eval_time': 1970.1682348251343, 'accumulated_logging_time': 0.7266736030578613, 'global_step': 25405, 'preemption_count': 0}), (27227, {'train/ctc_loss': Array(0.3136039, dtype=float32), 'train/wer': 0.11127893054472357, 'validation/ctc_loss': Array(0.45962992, dtype=float32), 'validation/wer': 0.13821601320756538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26831707, dtype=float32), 'test/wer': 0.0913411736030711, 'test/num_examples': 2472, 'score': 21641.441780090332, 'total_duration': 23745.30660867691, 'accumulated_submission_time': 21641.441780090332, 'accumulated_eval_time': 2101.914893388748, 'accumulated_logging_time': 0.7841992378234863, 'global_step': 27227, 'preemption_count': 0}), (29041, {'train/ctc_loss': Array(0.27701193, dtype=float32), 'train/wer': 0.1009942350132865, 'validation/ctc_loss': Array(0.46328136, dtype=float32), 'validation/wer': 0.1380132654933045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2715291, dtype=float32), 'test/wer': 0.0898990514492312, 'test/num_examples': 2472, 'score': 23081.622501134872, 'total_duration': 25315.63108062744, 'accumulated_submission_time': 23081.622501134872, 'accumulated_eval_time': 2231.9260606765747, 'accumulated_logging_time': 0.8400917053222656, 'global_step': 29041, 'preemption_count': 0}), (30847, {'train/ctc_loss': Array(0.24538893, dtype=float32), 'train/wer': 0.08977366783685818, 'validation/ctc_loss': Array(0.44567695, dtype=float32), 'validation/wer': 0.13357212508568506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25223556, dtype=float32), 'test/wer': 0.08516645339508054, 'test/num_examples': 2472, 'score': 24521.818974256516, 'total_duration': 26886.398028612137, 'accumulated_submission_time': 24521.818974256516, 'accumulated_eval_time': 2362.35418009758, 'accumulated_logging_time': 0.9039266109466553, 'global_step': 30847, 'preemption_count': 0}), (32654, {'train/ctc_loss': Array(0.27409798, dtype=float32), 'train/wer': 0.09958766735105967, 'validation/ctc_loss': Array(0.43247974, dtype=float32), 'validation/wer': 0.1298164650453286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25065094, dtype=float32), 'test/wer': 0.08248532488371621, 'test/num_examples': 2472, 'score': 25961.83100414276, 'total_duration': 28457.146093845367, 'accumulated_submission_time': 25961.83100414276, 'accumulated_eval_time': 2492.959435224533, 'accumulated_logging_time': 0.9592113494873047, 'global_step': 32654, 'preemption_count': 0}), (34475, {'train/ctc_loss': Array(0.25031212, dtype=float32), 'train/wer': 0.09101841078258352, 'validation/ctc_loss': Array(0.42782575, dtype=float32), 'validation/wer': 0.12651457369879412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24289979, dtype=float32), 'test/wer': 0.08191660065403286, 'test/num_examples': 2472, 'score': 27402.239865779877, 'total_duration': 30028.323790550232, 'accumulated_submission_time': 27402.239865779877, 'accumulated_eval_time': 2623.5924849510193, 'accumulated_logging_time': 1.017035722732544, 'global_step': 34475, 'preemption_count': 0}), (36284, {'train/ctc_loss': Array(0.26957318, dtype=float32), 'train/wer': 0.09373863515562493, 'validation/ctc_loss': Array(0.42037314, dtype=float32), 'validation/wer': 0.126253898066173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24147299, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 28842.823963165283, 'total_duration': 31599.68351483345, 'accumulated_submission_time': 28842.823963165283, 'accumulated_eval_time': 2754.237830877304, 'accumulated_logging_time': 1.070730209350586, 'global_step': 36284, 'preemption_count': 0}), (38084, {'train/ctc_loss': Array(0.21194768, dtype=float32), 'train/wer': 0.07836498490351376, 'validation/ctc_loss': Array(0.41112727, dtype=float32), 'validation/wer': 0.12005561080162584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2352042, dtype=float32), 'test/wer': 0.07763085735177624, 'test/num_examples': 2472, 'score': 30283.236042499542, 'total_duration': 33171.66923260689, 'accumulated_submission_time': 30283.236042499542, 'accumulated_eval_time': 2885.6785418987274, 'accumulated_logging_time': 1.1268370151519775, 'global_step': 38084, 'preemption_count': 0}), (39891, {'train/ctc_loss': Array(0.20022973, dtype=float32), 'train/wer': 0.07365950031632573, 'validation/ctc_loss': Array(0.40568483, dtype=float32), 'validation/wer': 0.11783504059781612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22744554, dtype=float32), 'test/wer': 0.07549814149046372, 'test/num_examples': 2472, 'score': 31723.705225467682, 'total_duration': 34742.872770786285, 'accumulated_submission_time': 31723.705225467682, 'accumulated_eval_time': 3016.282743215561, 'accumulated_logging_time': 1.1808912754058838, 'global_step': 39891, 'preemption_count': 0}), (41721, {'train/ctc_loss': Array(0.22058325, dtype=float32), 'train/wer': 0.08103187512484662, 'validation/ctc_loss': Array(0.39636406, dtype=float32), 'validation/wer': 0.11662820896531083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22631016, dtype=float32), 'test/wer': 0.07399508459772916, 'test/num_examples': 2472, 'score': 33163.71019721031, 'total_duration': 36312.93751120567, 'accumulated_submission_time': 33163.71019721031, 'accumulated_eval_time': 3146.2064123153687, 'accumulated_logging_time': 1.2387712001800537, 'global_step': 41721, 'preemption_count': 0}), (43554, {'train/ctc_loss': Array(0.14703715, dtype=float32), 'train/wer': 0.0556202457259144, 'validation/ctc_loss': Array(0.39308006, dtype=float32), 'validation/wer': 0.11424350965948038, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21916676, dtype=float32), 'test/wer': 0.07255296244388926, 'test/num_examples': 2472, 'score': 34603.71418786049, 'total_duration': 37892.071621418, 'accumulated_submission_time': 34603.71418786049, 'accumulated_eval_time': 3285.1928341388702, 'accumulated_logging_time': 1.3039309978485107, 'global_step': 43554, 'preemption_count': 0}), (45371, {'train/ctc_loss': Array(0.12569274, dtype=float32), 'train/wer': 0.04828931963054508, 'validation/ctc_loss': Array(0.38317782, dtype=float32), 'validation/wer': 0.11275669308823387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21107632, dtype=float32), 'test/wer': 0.07021713078626125, 'test/num_examples': 2472, 'score': 36044.21834445, 'total_duration': 39466.92186617851, 'accumulated_submission_time': 36044.21834445, 'accumulated_eval_time': 3419.3971271514893, 'accumulated_logging_time': 1.3684093952178955, 'global_step': 45371, 'preemption_count': 0}), (47193, {'train/ctc_loss': Array(0.12504135, dtype=float32), 'train/wer': 0.0475356763616586, 'validation/ctc_loss': Array(0.3714938, dtype=float32), 'validation/wer': 0.10903965166011759, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20164764, dtype=float32), 'test/wer': 0.06802348018605407, 'test/num_examples': 2472, 'score': 37484.795249700546, 'total_duration': 41040.31870007515, 'accumulated_submission_time': 37484.795249700546, 'accumulated_eval_time': 3552.0800442695618, 'accumulated_logging_time': 1.4292211532592773, 'global_step': 47193, 'preemption_count': 0}), (49019, {'train/ctc_loss': Array(0.12188379, dtype=float32), 'train/wer': 0.046178170035937736, 'validation/ctc_loss': Array(0.36238962, dtype=float32), 'validation/wer': 0.10549639398708208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.199988, dtype=float32), 'test/wer': 0.06729226331931834, 'test/num_examples': 2472, 'score': 38925.28243923187, 'total_duration': 42613.33491516113, 'accumulated_submission_time': 38925.28243923187, 'accumulated_eval_time': 3684.4721508026123, 'accumulated_logging_time': 1.4894332885742188, 'global_step': 49019, 'preemption_count': 0}), (50843, {'train/ctc_loss': Array(0.11072697, dtype=float32), 'train/wer': 0.04435754739135896, 'validation/ctc_loss': Array(0.36124983, dtype=float32), 'validation/wer': 0.10574741496664318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19808494, dtype=float32), 'test/wer': 0.06609388012105702, 'test/num_examples': 2472, 'score': 40365.479813575745, 'total_duration': 44184.46557068825, 'accumulated_submission_time': 40365.479813575745, 'accumulated_eval_time': 3815.268126010895, 'accumulated_logging_time': 1.547067642211914, 'global_step': 50843, 'preemption_count': 0}), (52655, {'train/ctc_loss': Array(0.10210627, dtype=float32), 'train/wer': 0.03982586179327278, 'validation/ctc_loss': Array(0.35422248, dtype=float32), 'validation/wer': 0.10187589908956622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19376087, dtype=float32), 'test/wer': 0.06505798955984807, 'test/num_examples': 2472, 'score': 41805.494245529175, 'total_duration': 45756.3062517643, 'accumulated_submission_time': 41805.494245529175, 'accumulated_eval_time': 3946.959716320038, 'accumulated_logging_time': 1.6031808853149414, 'global_step': 52655, 'preemption_count': 0}), (54477, {'train/ctc_loss': Array(0.11575904, dtype=float32), 'train/wer': 0.04398796384761528, 'validation/ctc_loss': Array(0.34668103, dtype=float32), 'validation/wer': 0.10128696525290364, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18844098, dtype=float32), 'test/wer': 0.06300652001706172, 'test/num_examples': 2472, 'score': 43245.7050819397, 'total_duration': 47328.256929636, 'accumulated_submission_time': 43245.7050819397, 'accumulated_eval_time': 4078.5655472278595, 'accumulated_logging_time': 1.6603331565856934, 'global_step': 54477, 'preemption_count': 0}), (56295, {'train/ctc_loss': Array(0.10086589, dtype=float32), 'train/wer': 0.03656622642591598, 'validation/ctc_loss': Array(0.34202397, dtype=float32), 'validation/wer': 0.097714743620688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18400313, dtype=float32), 'test/wer': 0.06069099993906526, 'test/num_examples': 2472, 'score': 44686.114140987396, 'total_duration': 48902.020424842834, 'accumulated_submission_time': 44686.114140987396, 'accumulated_eval_time': 4211.784162521362, 'accumulated_logging_time': 1.719299077987671, 'global_step': 56295, 'preemption_count': 0}), (58115, {'train/ctc_loss': Array(0.09474793, dtype=float32), 'train/wer': 0.03643191988067334, 'validation/ctc_loss': Array(0.3363182, dtype=float32), 'validation/wer': 0.0949535128455159, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17934273, dtype=float32), 'test/wer': 0.058375479861068794, 'test/num_examples': 2472, 'score': 46126.119805812836, 'total_duration': 50475.445321798325, 'accumulated_submission_time': 46126.119805812836, 'accumulated_eval_time': 4345.063044786453, 'accumulated_logging_time': 1.7812213897705078, 'global_step': 58115, 'preemption_count': 0}), (59930, {'train/ctc_loss': Array(0.07673226, dtype=float32), 'train/wer': 0.03052756372258447, 'validation/ctc_loss': Array(0.33402327, dtype=float32), 'validation/wer': 0.09432596039661315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17598493, dtype=float32), 'test/wer': 0.058375479861068794, 'test/num_examples': 2472, 'score': 47566.14169406891, 'total_duration': 52046.862721681595, 'accumulated_submission_time': 47566.14169406891, 'accumulated_eval_time': 4476.3204646110535, 'accumulated_logging_time': 1.841925859451294, 'global_step': 59930, 'preemption_count': 0}), (61747, {'train/ctc_loss': Array(0.07129591, dtype=float32), 'train/wer': 0.028381517879350764, 'validation/ctc_loss': Array(0.3258931, dtype=float32), 'validation/wer': 0.09269432402946601, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17221192, dtype=float32), 'test/wer': 0.05610058294233543, 'test/num_examples': 2472, 'score': 49006.05555701256, 'total_duration': 53619.27761530876, 'accumulated_submission_time': 49006.05555701256, 'accumulated_eval_time': 4608.684624910355, 'accumulated_logging_time': 1.9001836776733398, 'global_step': 61747, 'preemption_count': 0}), (63558, {'train/ctc_loss': Array(0.07846448, dtype=float32), 'train/wer': 0.030398028236006312, 'validation/ctc_loss': Array(0.3151095, dtype=float32), 'validation/wer': 0.08974000019309306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16798735, dtype=float32), 'test/wer': 0.05618182926086162, 'test/num_examples': 2472, 'score': 50446.629410505295, 'total_duration': 55192.60791397095, 'accumulated_submission_time': 50446.629410505295, 'accumulated_eval_time': 4741.300411224365, 'accumulated_logging_time': 1.9632997512817383, 'global_step': 63558, 'preemption_count': 0}), (65383, {'train/ctc_loss': Array(0.07131476, dtype=float32), 'train/wer': 0.02782980721571492, 'validation/ctc_loss': Array(0.31282473, dtype=float32), 'validation/wer': 0.08853316856058778, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1647859, dtype=float32), 'test/wer': 0.052850730201287756, 'test/num_examples': 2472, 'score': 51886.58189582825, 'total_duration': 56765.85677433014, 'accumulated_submission_time': 51886.58189582825, 'accumulated_eval_time': 4874.458735466003, 'accumulated_logging_time': 2.022402763366699, 'global_step': 65383, 'preemption_count': 0}), (67201, {'train/ctc_loss': Array(0.06927087, dtype=float32), 'train/wer': 0.02621635247584447, 'validation/ctc_loss': Array(0.30948767, dtype=float32), 'validation/wer': 0.0870560066424013, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16259553, dtype=float32), 'test/wer': 0.05258667966607763, 'test/num_examples': 2472, 'score': 53327.39374256134, 'total_duration': 58339.04134559631, 'accumulated_submission_time': 53327.39374256134, 'accumulated_eval_time': 5006.690465927124, 'accumulated_logging_time': 2.087104558944702, 'global_step': 67201, 'preemption_count': 0}), (69011, {'train/ctc_loss': Array(0.05992661, dtype=float32), 'train/wer': 0.02337353454989371, 'validation/ctc_loss': Array(0.30565014, dtype=float32), 'validation/wer': 0.08574297382623555, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15818636, dtype=float32), 'test/wer': 0.05134767330855321, 'test/num_examples': 2472, 'score': 54769.413890361786, 'total_duration': 59914.420617341995, 'accumulated_submission_time': 54769.413890361786, 'accumulated_eval_time': 5139.9097690582275, 'accumulated_logging_time': 2.1487863063812256, 'global_step': 69011, 'preemption_count': 0}), (70839, {'train/ctc_loss': Array(0.05449369, dtype=float32), 'train/wer': 0.020615387821447922, 'validation/ctc_loss': Array(0.30036068, dtype=float32), 'validation/wer': 0.08332931056122499, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15762733, dtype=float32), 'test/wer': 0.050657079601080575, 'test/num_examples': 2472, 'score': 56209.6598546505, 'total_duration': 61487.83983325958, 'accumulated_submission_time': 56209.6598546505, 'accumulated_eval_time': 5272.942055225372, 'accumulated_logging_time': 2.2103707790374756, 'global_step': 70839, 'preemption_count': 0}), (72665, {'train/ctc_loss': Array(0.05953811, dtype=float32), 'train/wer': 0.02181967045338246, 'validation/ctc_loss': Array(0.29563853, dtype=float32), 'validation/wer': 0.08195834982669897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15622462, dtype=float32), 'test/wer': 0.05004773221213414, 'test/num_examples': 2472, 'score': 57649.845106601715, 'total_duration': 63061.260159015656, 'accumulated_submission_time': 57649.845106601715, 'accumulated_eval_time': 5406.034177541733, 'accumulated_logging_time': 2.274899482727051, 'global_step': 72665, 'preemption_count': 0}), (74496, {'train/ctc_loss': Array(0.05710617, dtype=float32), 'train/wer': 0.021032742192350257, 'validation/ctc_loss': Array(0.29521686, dtype=float32), 'validation/wer': 0.08235419060216072, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15535073, dtype=float32), 'test/wer': 0.04974305851766092, 'test/num_examples': 2472, 'score': 59089.764125585556, 'total_duration': 64633.15510225296, 'accumulated_submission_time': 59089.764125585556, 'accumulated_eval_time': 5537.869856595993, 'accumulated_logging_time': 2.33532977104187, 'global_step': 74496, 'preemption_count': 0}), (76309, {'train/ctc_loss': Array(0.05786657, dtype=float32), 'train/wer': 0.021383041405538594, 'validation/ctc_loss': Array(0.2951434, dtype=float32), 'validation/wer': 0.08191007656139877, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15429394, dtype=float32), 'test/wer': 0.04970243535839782, 'test/num_examples': 2472, 'score': 60530.329689741135, 'total_duration': 66204.74381828308, 'accumulated_submission_time': 60530.329689741135, 'accumulated_eval_time': 5668.746239900589, 'accumulated_logging_time': 2.404423713684082, 'global_step': 76309, 'preemption_count': 0})], 'global_step': 77006}
I0217 23:36:26.710706 140399019657024 submission_runner.py:586] Timing: 61068.35397648811
I0217 23:36:26.710761 140399019657024 submission_runner.py:588] Total number of evals: 43
I0217 23:36:26.710804 140399019657024 submission_runner.py:589] ====================
I0217 23:36:26.756636 140399019657024 submission_runner.py:673] Final librispeech_conformer score: 61068.183177948
