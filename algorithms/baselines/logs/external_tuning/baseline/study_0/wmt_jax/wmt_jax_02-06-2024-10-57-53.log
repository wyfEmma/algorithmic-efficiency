python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3586669017 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_jax_02-06-2024-10-57-53.log
I0206 10:58:16.786925 139785736898368 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_0/wmt_jax.
I0206 10:58:17.851709 139785736898368 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0206 10:58:17.852540 139785736898368 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0206 10:58:17.852703 139785736898368 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0206 10:58:17.853940 139785736898368 submission_runner.py:542] Using RNG seed 3586669017
I0206 10:58:19.069872 139785736898368 submission_runner.py:551] --- Tuning run 1/5 ---
I0206 10:58:19.070126 139785736898368 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1.
I0206 10:58:19.070388 139785736898368 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1/hparams.json.
I0206 10:58:19.261979 139785736898368 submission_runner.py:206] Initializing dataset.
I0206 10:58:19.276121 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 10:58:19.282936 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 10:58:19.451634 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 10:58:21.588139 139785736898368 submission_runner.py:213] Initializing model.
I0206 10:58:30.899239 139785736898368 submission_runner.py:255] Initializing optimizer.
I0206 10:58:32.052094 139785736898368 submission_runner.py:262] Initializing metrics bundle.
I0206 10:58:32.052292 139785736898368 submission_runner.py:280] Initializing checkpoint and logger.
I0206 10:58:32.053476 139785736898368 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1 with prefix checkpoint_
I0206 10:58:32.053615 139785736898368 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1/meta_data_0.json.
I0206 10:58:32.053831 139785736898368 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 10:58:32.053895 139785736898368 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 10:58:32.420416 139785736898368 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 10:58:32.743324 139785736898368 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1/flags_0.json.
I0206 10:58:32.753680 139785736898368 submission_runner.py:314] Starting training loop.
I0206 10:59:09.835445 139624568268544 logging_writer.py:48] [0] global_step=0, grad_norm=5.072338581085205, loss=11.050198554992676
I0206 10:59:09.853178 139785736898368 spec.py:321] Evaluating on the training split.
I0206 10:59:09.856768 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 10:59:09.859515 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 10:59:09.898322 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 10:59:17.559077 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:04:16.349893 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 11:04:16.354439 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:04:16.364506 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 11:04:16.409511 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:04:23.242162 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:09:10.919438 139785736898368 spec.py:349] Evaluating on the test split.
I0206 11:09:10.922412 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:09:10.925666 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 11:09:10.964960 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:09:13.828790 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:14:01.761575 139785736898368 submission_runner.py:408] Time since start: 929.01s, 	Step: 1, 	{'train/accuracy': 0.0005940888077020645, 'train/loss': 11.064360618591309, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.09946036338806, 'total_duration': 929.0078175067902, 'accumulated_submission_time': 37.09946036338806, 'accumulated_eval_time': 891.9083139896393, 'accumulated_logging_time': 0}
I0206 11:14:01.780717 139616073008896 logging_writer.py:48] [1] accumulated_eval_time=891.908314, accumulated_logging_time=0, accumulated_submission_time=37.099460, global_step=1, preemption_count=0, score=37.099460, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.041826, test/num_examples=3003, total_duration=929.007818, train/accuracy=0.000594, train/bleu=0.000000, train/loss=11.064361, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.036646, validation/num_examples=3000
I0206 11:14:36.749771 139616064616192 logging_writer.py:48] [100] global_step=100, grad_norm=0.41899770498275757, loss=8.939542770385742
I0206 11:15:11.705389 139616073008896 logging_writer.py:48] [200] global_step=200, grad_norm=0.1642497330904007, loss=8.590473175048828
I0206 11:15:46.644831 139616064616192 logging_writer.py:48] [300] global_step=300, grad_norm=0.17960835993289948, loss=8.396743774414062
I0206 11:16:21.621626 139616073008896 logging_writer.py:48] [400] global_step=400, grad_norm=0.2538672387599945, loss=7.968255996704102
I0206 11:16:56.586340 139616064616192 logging_writer.py:48] [500] global_step=500, grad_norm=0.29751938581466675, loss=7.6716628074646
I0206 11:17:31.541165 139616073008896 logging_writer.py:48] [600] global_step=600, grad_norm=0.5425823926925659, loss=7.4480743408203125
I0206 11:18:06.517739 139616064616192 logging_writer.py:48] [700] global_step=700, grad_norm=0.4807376563549042, loss=7.202721118927002
I0206 11:18:41.488383 139616073008896 logging_writer.py:48] [800] global_step=800, grad_norm=0.8087048530578613, loss=6.913716793060303
I0206 11:19:16.481261 139616064616192 logging_writer.py:48] [900] global_step=900, grad_norm=0.6763216853141785, loss=6.76100492477417
I0206 11:19:51.493115 139616073008896 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5554897785186768, loss=6.589105606079102
I0206 11:20:26.513514 139616064616192 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5135228037834167, loss=6.391354084014893
I0206 11:21:01.551769 139616073008896 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9677750468254089, loss=6.23690938949585
I0206 11:21:36.604373 139616064616192 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5139368176460266, loss=6.111748695373535
I0206 11:22:11.636460 139616073008896 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7465646266937256, loss=5.876918792724609
I0206 11:22:46.759726 139616064616192 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6132423877716064, loss=5.78518533706665
I0206 11:23:21.791793 139616073008896 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6910086870193481, loss=5.6901445388793945
I0206 11:23:56.795748 139616064616192 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7355519533157349, loss=5.580463409423828
I0206 11:24:31.806615 139616073008896 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7982794046401978, loss=5.44377326965332
I0206 11:25:06.864690 139616064616192 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1343764066696167, loss=5.342469692230225
I0206 11:25:41.881429 139616073008896 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7863844633102417, loss=5.249156475067139
I0206 11:26:16.893459 139616064616192 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7808680534362793, loss=5.124227046966553
I0206 11:26:51.910470 139616073008896 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8694171905517578, loss=5.073522090911865
I0206 11:27:26.937244 139616064616192 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7821553349494934, loss=4.9696784019470215
I0206 11:28:01.945241 139616073008896 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6810822486877441, loss=4.806544780731201
I0206 11:28:01.952254 139785736898368 spec.py:321] Evaluating on the training split.
I0206 11:28:04.689108 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:31:18.143702 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 11:31:20.853127 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:34:31.718941 139785736898368 spec.py:349] Evaluating on the test split.
I0206 11:34:34.435273 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:37:37.242075 139785736898368 submission_runner.py:408] Time since start: 2344.49s, 	Step: 2401, 	{'train/accuracy': 0.41223323345184326, 'train/loss': 3.9737305641174316, 'train/bleu': 14.315394056700264, 'validation/accuracy': 0.3997098505496979, 'validation/loss': 4.093648433685303, 'validation/bleu': 9.72599437822303, 'validation/num_examples': 3000, 'test/accuracy': 0.3828597962856293, 'test/loss': 4.295248985290527, 'test/bleu': 7.93982270373288, 'test/num_examples': 3003, 'score': 877.1772933006287, 'total_duration': 2344.4883332252502, 'accumulated_submission_time': 877.1772933006287, 'accumulated_eval_time': 1467.1980648040771, 'accumulated_logging_time': 0.030037641525268555}
I0206 11:37:37.257136 139616064616192 logging_writer.py:48] [2401] accumulated_eval_time=1467.198065, accumulated_logging_time=0.030038, accumulated_submission_time=877.177293, global_step=2401, preemption_count=0, score=877.177293, test/accuracy=0.382860, test/bleu=7.939823, test/loss=4.295249, test/num_examples=3003, total_duration=2344.488333, train/accuracy=0.412233, train/bleu=14.315394, train/loss=3.973731, validation/accuracy=0.399710, validation/bleu=9.725994, validation/loss=4.093648, validation/num_examples=3000
I0206 11:38:12.310082 139616073008896 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.9648551940917969, loss=4.712874889373779
I0206 11:38:47.321454 139616064616192 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9527726769447327, loss=4.718679904937744
I0206 11:39:22.355097 139616073008896 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8151245713233948, loss=4.613297939300537
I0206 11:39:57.394825 139616064616192 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0406384468078613, loss=4.547945022583008
I0206 11:40:32.415082 139616073008896 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.084439992904663, loss=4.537769317626953
I0206 11:41:07.440495 139616064616192 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.999860942363739, loss=4.43687105178833
I0206 11:41:42.437105 139616073008896 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.950236976146698, loss=4.265646457672119
I0206 11:42:17.414088 139616064616192 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6330411434173584, loss=4.352902889251709
I0206 11:42:52.387500 139616073008896 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6917301416397095, loss=4.275245666503906
I0206 11:43:27.373062 139616064616192 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8617948889732361, loss=4.187825679779053
I0206 11:44:02.384712 139616073008896 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7238631248474121, loss=4.13270902633667
I0206 11:44:37.361365 139616064616192 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6176629066467285, loss=4.250212669372559
I0206 11:45:12.370121 139616073008896 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7302054762840271, loss=4.074136734008789
I0206 11:45:47.402968 139616064616192 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6487040519714355, loss=4.059177398681641
I0206 11:46:22.383412 139616073008896 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7148584723472595, loss=3.9961016178131104
I0206 11:46:57.360204 139616064616192 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6282515525817871, loss=4.050450801849365
I0206 11:47:32.366489 139616073008896 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5978855490684509, loss=3.8224992752075195
I0206 11:48:07.348485 139616064616192 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6999701857566833, loss=3.9275457859039307
I0206 11:48:42.334114 139616073008896 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6100821495056152, loss=3.9754011631011963
I0206 11:49:17.323378 139616064616192 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.736687421798706, loss=3.8787758350372314
I0206 11:49:52.369062 139616073008896 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6270308494567871, loss=3.8105671405792236
I0206 11:50:27.347036 139616064616192 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6344661116600037, loss=3.8208725452423096
I0206 11:51:02.330234 139616073008896 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5360621809959412, loss=3.752976417541504
I0206 11:51:37.360465 139616064616192 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7597967386245728, loss=3.8429453372955322
I0206 11:51:37.368919 139785736898368 spec.py:321] Evaluating on the training split.
I0206 11:51:40.105684 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:54:11.423062 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 11:54:14.166513 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:56:48.678012 139785736898368 spec.py:349] Evaluating on the test split.
I0206 11:56:51.398830 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 11:59:16.011982 139785736898368 submission_runner.py:408] Time since start: 3643.26s, 	Step: 4801, 	{'train/accuracy': 0.542317271232605, 'train/loss': 2.7344980239868164, 'train/bleu': 24.2609666755636, 'validation/accuracy': 0.5440229773521423, 'validation/loss': 2.7259020805358887, 'validation/bleu': 20.122334101513726, 'validation/num_examples': 3000, 'test/accuracy': 0.5431642532348633, 'test/loss': 2.7641193866729736, 'test/bleu': 18.73440224873982, 'test/num_examples': 3003, 'score': 1717.1984317302704, 'total_duration': 3643.2582201957703, 'accumulated_submission_time': 1717.1984317302704, 'accumulated_eval_time': 1925.8410477638245, 'accumulated_logging_time': 0.056603193283081055}
I0206 11:59:16.027787 139616073008896 logging_writer.py:48] [4801] accumulated_eval_time=1925.841048, accumulated_logging_time=0.056603, accumulated_submission_time=1717.198432, global_step=4801, preemption_count=0, score=1717.198432, test/accuracy=0.543164, test/bleu=18.734402, test/loss=2.764119, test/num_examples=3003, total_duration=3643.258220, train/accuracy=0.542317, train/bleu=24.260967, train/loss=2.734498, validation/accuracy=0.544023, validation/bleu=20.122334, validation/loss=2.725902, validation/num_examples=3000
I0206 11:59:51.053856 139616064616192 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5589283108711243, loss=3.7239737510681152
I0206 12:00:26.057057 139616073008896 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6199276447296143, loss=3.7033638954162598
I0206 12:01:01.025604 139616064616192 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5873448848724365, loss=3.767878293991089
I0206 12:01:36.014835 139616073008896 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6828272342681885, loss=3.7184064388275146
I0206 12:02:11.032610 139616064616192 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.488787978887558, loss=3.711188554763794
I0206 12:02:46.012578 139616073008896 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5213741660118103, loss=3.6915783882141113
I0206 12:03:21.008811 139616064616192 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5004370808601379, loss=3.6114962100982666
I0206 12:03:55.976258 139616073008896 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5080815553665161, loss=3.6316030025482178
I0206 12:04:30.977617 139616064616192 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5261707901954651, loss=3.6762797832489014
I0206 12:05:05.949457 139616073008896 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5136988162994385, loss=3.6318037509918213
I0206 12:05:40.916967 139616064616192 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6645673513412476, loss=3.6552281379699707
I0206 12:06:15.901488 139616073008896 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4937538206577301, loss=3.612739324569702
I0206 12:06:50.929390 139616064616192 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6682127714157104, loss=3.573899507522583
I0206 12:07:25.960007 139616073008896 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5057216286659241, loss=3.585974931716919
I0206 12:08:00.910423 139616064616192 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.48659175634384155, loss=3.5285301208496094
I0206 12:08:35.863862 139616073008896 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4075908660888672, loss=3.593116283416748
I0206 12:09:10.826032 139616064616192 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6141207218170166, loss=3.572307825088501
I0206 12:09:45.780991 139616073008896 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4433992803096771, loss=3.5607523918151855
I0206 12:10:20.731155 139616064616192 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4469394087791443, loss=3.515209913253784
I0206 12:10:55.696910 139616073008896 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.47114983201026917, loss=3.5076911449432373
I0206 12:11:30.650877 139616064616192 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5072083473205566, loss=3.529171943664551
I0206 12:12:05.628039 139616073008896 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.47240665555000305, loss=3.49613881111145
I0206 12:12:40.646869 139616064616192 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6223761439323425, loss=3.4768989086151123
I0206 12:13:15.592687 139616073008896 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4463665783405304, loss=3.5371766090393066
I0206 12:13:16.013688 139785736898368 spec.py:321] Evaluating on the training split.
I0206 12:13:19.013966 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 12:15:44.783421 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 12:15:47.507399 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 12:18:09.729783 139785736898368 spec.py:349] Evaluating on the test split.
I0206 12:18:12.435615 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 12:20:22.010455 139785736898368 submission_runner.py:408] Time since start: 4909.26s, 	Step: 7203, 	{'train/accuracy': 0.5805636644363403, 'train/loss': 2.379575252532959, 'train/bleu': 27.023267640432266, 'validation/accuracy': 0.5879034399986267, 'validation/loss': 2.307112455368042, 'validation/bleu': 23.286632821120534, 'validation/num_examples': 3000, 'test/accuracy': 0.5903899073600769, 'test/loss': 2.3085203170776367, 'test/bleu': 21.953963966435513, 'test/num_examples': 3003, 'score': 2557.0957322120667, 'total_duration': 4909.256707191467, 'accumulated_submission_time': 2557.0957322120667, 'accumulated_eval_time': 2351.8377647399902, 'accumulated_logging_time': 0.08249330520629883}
I0206 12:20:22.025832 139616064616192 logging_writer.py:48] [7203] accumulated_eval_time=2351.837765, accumulated_logging_time=0.082493, accumulated_submission_time=2557.095732, global_step=7203, preemption_count=0, score=2557.095732, test/accuracy=0.590390, test/bleu=21.953964, test/loss=2.308520, test/num_examples=3003, total_duration=4909.256707, train/accuracy=0.580564, train/bleu=27.023268, train/loss=2.379575, validation/accuracy=0.587903, validation/bleu=23.286633, validation/loss=2.307112, validation/num_examples=3000
I0206 12:20:56.295637 139616073008896 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4646599292755127, loss=3.432715892791748
I0206 12:21:31.270870 139616064616192 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.39880192279815674, loss=3.459523916244507
I0206 12:22:06.228742 139616073008896 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3890102207660675, loss=3.3875484466552734
I0206 12:22:41.187161 139616064616192 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5120423436164856, loss=3.5260238647460938
I0206 12:23:16.206688 139616073008896 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4493996798992157, loss=3.351470470428467
I0206 12:23:51.189625 139616064616192 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3724571764469147, loss=3.3368778228759766
I0206 12:24:26.129798 139616073008896 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.43896016478538513, loss=3.3803534507751465
I0206 12:25:01.064353 139616064616192 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3521261513233185, loss=3.51702880859375
I0206 12:25:36.031479 139616073008896 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3629765808582306, loss=3.339857816696167
I0206 12:26:10.997875 139616064616192 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3998357951641083, loss=3.41412353515625
I0206 12:26:45.925759 139616073008896 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3847479820251465, loss=3.3943676948547363
I0206 12:27:20.874097 139616064616192 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4047991633415222, loss=3.3753459453582764
I0206 12:27:55.804992 139616073008896 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3719061613082886, loss=3.445871591567993
I0206 12:28:30.748718 139616064616192 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3482334613800049, loss=3.3641912937164307
I0206 12:29:05.690509 139616073008896 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.38051122426986694, loss=3.3740105628967285
I0206 12:29:40.633741 139616064616192 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.35241180658340454, loss=3.440443754196167
I0206 12:30:15.610146 139616073008896 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.33806318044662476, loss=3.3296337127685547
I0206 12:30:50.574188 139616064616192 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3226766586303711, loss=3.3291053771972656
I0206 12:31:25.525241 139616073008896 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.346769779920578, loss=3.380213499069214
I0206 12:32:00.445969 139616064616192 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3649369776248932, loss=3.441716194152832
I0206 12:32:35.388745 139616073008896 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.33280283212661743, loss=3.276496648788452
I0206 12:33:10.321879 139616064616192 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.32354736328125, loss=3.3960025310516357
I0206 12:33:45.267261 139616073008896 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.31061962246894836, loss=3.3475334644317627
I0206 12:34:20.214142 139616064616192 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2924667000770569, loss=3.310480833053589
I0206 12:34:22.035084 139785736898368 spec.py:321] Evaluating on the training split.
I0206 12:34:25.041060 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 12:37:10.617267 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 12:37:13.326283 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 12:39:38.454350 139785736898368 spec.py:349] Evaluating on the test split.
I0206 12:39:41.183115 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 12:41:54.578444 139785736898368 submission_runner.py:408] Time since start: 6201.82s, 	Step: 9607, 	{'train/accuracy': 0.593815803527832, 'train/loss': 2.2298953533172607, 'train/bleu': 28.305043807122836, 'validation/accuracy': 0.6075063943862915, 'validation/loss': 2.134629487991333, 'validation/bleu': 24.751819281295166, 'validation/num_examples': 3000, 'test/accuracy': 0.6115739941596985, 'test/loss': 2.1070995330810547, 'test/bleu': 23.369496351054153, 'test/num_examples': 3003, 'score': 3397.0145201683044, 'total_duration': 6201.824702739716, 'accumulated_submission_time': 3397.0145201683044, 'accumulated_eval_time': 2804.3810741901398, 'accumulated_logging_time': 0.10986804962158203}
I0206 12:41:54.594864 139616073008896 logging_writer.py:48] [9607] accumulated_eval_time=2804.381074, accumulated_logging_time=0.109868, accumulated_submission_time=3397.014520, global_step=9607, preemption_count=0, score=3397.014520, test/accuracy=0.611574, test/bleu=23.369496, test/loss=2.107100, test/num_examples=3003, total_duration=6201.824703, train/accuracy=0.593816, train/bleu=28.305044, train/loss=2.229895, validation/accuracy=0.607506, validation/bleu=24.751819, validation/loss=2.134629, validation/num_examples=3000
I0206 12:42:27.436306 139616064616192 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3694758713245392, loss=3.281102418899536
I0206 12:43:02.370752 139616073008896 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.28470128774642944, loss=3.343935012817383
I0206 12:43:37.333458 139616064616192 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.2755057215690613, loss=3.2677462100982666
I0206 12:44:12.242163 139616073008896 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.28337275981903076, loss=3.2282614707946777
I0206 12:44:47.149062 139616064616192 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.29991763830184937, loss=3.2755444049835205
I0206 12:45:22.056891 139616073008896 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.39171311259269714, loss=3.3297126293182373
I0206 12:45:56.980160 139616064616192 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.2854401469230652, loss=3.197741985321045
I0206 12:46:31.911505 139616073008896 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.274869829416275, loss=3.212561845779419
I0206 12:47:06.834225 139616064616192 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.273499071598053, loss=3.3135488033294678
I0206 12:47:41.745989 139616073008896 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.29526978731155396, loss=3.248520851135254
I0206 12:48:16.664074 139616064616192 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.27820664644241333, loss=3.2996749877929688
I0206 12:48:51.624184 139616073008896 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2808409333229065, loss=3.291980743408203
I0206 12:49:26.692595 139616064616192 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2537928819656372, loss=3.2300188541412354
I0206 12:50:01.664042 139616073008896 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.28091445565223694, loss=3.3172738552093506
I0206 12:50:36.658298 139616064616192 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2620571553707123, loss=3.239100217819214
I0206 12:51:11.596221 139616073008896 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.27539142966270447, loss=3.264739751815796
I0206 12:51:46.523510 139616064616192 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.269482284784317, loss=3.3011538982391357
I0206 12:52:21.490592 139616073008896 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2730647325515747, loss=3.177237033843994
I0206 12:52:56.461697 139616064616192 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.25330695509910583, loss=3.322143316268921
I0206 12:53:31.411808 139616073008896 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.27883532643318176, loss=3.1903109550476074
I0206 12:54:06.362206 139616064616192 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.29102492332458496, loss=3.239745855331421
I0206 12:54:41.331250 139616073008896 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.28025132417678833, loss=3.220484733581543
I0206 12:55:16.314406 139616064616192 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2642047107219696, loss=3.2520081996917725
I0206 12:55:51.247666 139616073008896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.23671254515647888, loss=3.265850067138672
I0206 12:55:54.810808 139785736898368 spec.py:321] Evaluating on the training split.
I0206 12:55:57.810760 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:00:16.317499 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 13:00:19.025879 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:04:10.714563 139785736898368 spec.py:349] Evaluating on the test split.
I0206 13:04:13.427926 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:07:05.715360 139785736898368 submission_runner.py:408] Time since start: 7712.96s, 	Step: 12012, 	{'train/accuracy': 0.6022624969482422, 'train/loss': 2.149235486984253, 'train/bleu': 28.608856006115754, 'validation/accuracy': 0.619471549987793, 'validation/loss': 2.007272720336914, 'validation/bleu': 25.498765397658293, 'validation/num_examples': 3000, 'test/accuracy': 0.6252629160881042, 'test/loss': 1.9659838676452637, 'test/bleu': 24.260364769931513, 'test/num_examples': 3003, 'score': 4237.137367010117, 'total_duration': 7712.961602926254, 'accumulated_submission_time': 4237.137367010117, 'accumulated_eval_time': 3475.285562515259, 'accumulated_logging_time': 0.1377413272857666}
I0206 13:07:05.731497 139616064616192 logging_writer.py:48] [12012] accumulated_eval_time=3475.285563, accumulated_logging_time=0.137741, accumulated_submission_time=4237.137367, global_step=12012, preemption_count=0, score=4237.137367, test/accuracy=0.625263, test/bleu=24.260365, test/loss=1.965984, test/num_examples=3003, total_duration=7712.961603, train/accuracy=0.602262, train/bleu=28.608856, train/loss=2.149235, validation/accuracy=0.619472, validation/bleu=25.498765, validation/loss=2.007273, validation/num_examples=3000
I0206 13:07:36.810841 139616073008896 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.25649744272232056, loss=3.2020556926727295
I0206 13:08:11.717865 139616064616192 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2372279316186905, loss=3.203522205352783
I0206 13:08:46.655710 139616073008896 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2349012941122055, loss=3.252232551574707
I0206 13:09:21.575911 139616064616192 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2290971726179123, loss=3.152108669281006
I0206 13:09:56.498409 139616073008896 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2457433044910431, loss=3.24324107170105
I0206 13:10:31.426568 139616064616192 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.26869216561317444, loss=3.1760101318359375
I0206 13:11:06.363720 139616073008896 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3073083162307739, loss=3.1858725547790527
I0206 13:11:41.300978 139616064616192 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.24904343485832214, loss=3.2957425117492676
I0206 13:12:16.216577 139616073008896 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.24739603698253632, loss=3.115760564804077
I0206 13:12:51.143772 139616064616192 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.24093197286128998, loss=3.2114241123199463
I0206 13:13:26.059302 139616073008896 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3085069954395294, loss=3.257329225540161
I0206 13:14:00.945105 139616064616192 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.24263037741184235, loss=3.233142137527466
I0206 13:14:35.859272 139616073008896 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2327098399400711, loss=3.2245986461639404
I0206 13:15:10.810094 139616064616192 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.27438169717788696, loss=3.2050013542175293
I0206 13:15:45.743175 139616073008896 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2506715953350067, loss=3.2429981231689453
I0206 13:16:20.677709 139616064616192 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.22551316022872925, loss=3.146441698074341
I0206 13:16:55.587936 139616073008896 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.27728599309921265, loss=3.1801376342773438
I0206 13:17:30.532949 139616064616192 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2521915137767792, loss=3.136213541030884
I0206 13:18:05.465806 139616073008896 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.24580423533916473, loss=3.1418399810791016
I0206 13:18:40.389470 139616064616192 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2681858539581299, loss=3.171025037765503
I0206 13:19:15.315090 139616073008896 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2552638053894043, loss=3.1877708435058594
I0206 13:19:50.201888 139616064616192 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.26358816027641296, loss=3.1000256538391113
I0206 13:20:25.144068 139616073008896 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.25866708159446716, loss=3.121541976928711
I0206 13:21:00.052528 139616064616192 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.35845625400543213, loss=3.141359806060791
I0206 13:21:05.722439 139785736898368 spec.py:321] Evaluating on the training split.
I0206 13:21:08.718693 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:23:45.175256 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 13:23:47.882245 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:26:16.331081 139785736898368 spec.py:349] Evaluating on the test split.
I0206 13:26:19.037754 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:28:36.263741 139785736898368 submission_runner.py:408] Time since start: 9003.51s, 	Step: 14418, 	{'train/accuracy': 0.6124773025512695, 'train/loss': 2.0575904846191406, 'train/bleu': 29.56920209281836, 'validation/accuracy': 0.6307671070098877, 'validation/loss': 1.9267817735671997, 'validation/bleu': 26.21993126585849, 'validation/num_examples': 3000, 'test/accuracy': 0.6387659311294556, 'test/loss': 1.878983736038208, 'test/bleu': 25.881130375271567, 'test/num_examples': 3003, 'score': 5077.038963317871, 'total_duration': 9003.50999879837, 'accumulated_submission_time': 5077.038963317871, 'accumulated_eval_time': 3925.826815366745, 'accumulated_logging_time': 0.1641829013824463}
I0206 13:28:36.280802 139616073008896 logging_writer.py:48] [14418] accumulated_eval_time=3925.826815, accumulated_logging_time=0.164183, accumulated_submission_time=5077.038963, global_step=14418, preemption_count=0, score=5077.038963, test/accuracy=0.638766, test/bleu=25.881130, test/loss=1.878984, test/num_examples=3003, total_duration=9003.509999, train/accuracy=0.612477, train/bleu=29.569202, train/loss=2.057590, validation/accuracy=0.630767, validation/bleu=26.219931, validation/loss=1.926782, validation/num_examples=3000
I0206 13:29:05.251581 139616064616192 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.27617979049682617, loss=3.1185312271118164
I0206 13:29:40.261404 139616073008896 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.24191102385520935, loss=3.192523241043091
I0206 13:30:15.217851 139616064616192 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2483627200126648, loss=3.1168453693389893
I0206 13:30:50.190366 139616073008896 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.31038224697113037, loss=3.1727442741394043
I0206 13:31:25.244183 139616064616192 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.24482755362987518, loss=3.143545627593994
I0206 13:32:00.200443 139616073008896 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.23128357529640198, loss=3.152824640274048
I0206 13:32:35.136190 139616064616192 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.30718377232551575, loss=3.118035078048706
I0206 13:33:10.065050 139616073008896 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.299652099609375, loss=3.1517162322998047
I0206 13:33:44.983963 139616064616192 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.24537350237369537, loss=3.1453189849853516
I0206 13:34:19.942276 139616073008896 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.318409264087677, loss=3.1400578022003174
I0206 13:34:54.849577 139616064616192 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.24935579299926758, loss=3.123098611831665
I0206 13:35:29.755538 139616073008896 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2605248987674713, loss=3.195326328277588
I0206 13:36:04.659000 139616064616192 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.26092272996902466, loss=3.177558422088623
I0206 13:36:39.621413 139616073008896 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.29306045174598694, loss=3.0766050815582275
I0206 13:37:14.530546 139616064616192 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.29577308893203735, loss=3.168686628341675
I0206 13:37:49.448561 139616073008896 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2795470654964447, loss=3.0188605785369873
I0206 13:38:24.382985 139616064616192 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2777588963508606, loss=3.150524139404297
I0206 13:38:59.326859 139616073008896 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.24471427500247955, loss=3.0484206676483154
I0206 13:39:34.292132 139616064616192 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.25360292196273804, loss=3.1192166805267334
I0206 13:40:09.235647 139616073008896 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.24830959737300873, loss=3.057439088821411
I0206 13:40:44.139803 139616064616192 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.2623538374900818, loss=3.0574381351470947
I0206 13:41:19.044116 139616073008896 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2615577280521393, loss=3.0757429599761963
I0206 13:41:53.968981 139616064616192 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.22937439382076263, loss=3.0483570098876953
I0206 13:42:28.875377 139616073008896 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.2913951873779297, loss=3.0950839519500732
I0206 13:42:36.277724 139785736898368 spec.py:321] Evaluating on the training split.
I0206 13:42:39.276355 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:45:13.971794 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 13:45:16.694032 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:47:43.448715 139785736898368 spec.py:349] Evaluating on the test split.
I0206 13:47:46.162319 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 13:50:08.254113 139785736898368 submission_runner.py:408] Time since start: 10295.50s, 	Step: 16823, 	{'train/accuracy': 0.6192818880081177, 'train/loss': 2.010906934738159, 'train/bleu': 29.80887983767533, 'validation/accuracy': 0.6372766494750977, 'validation/loss': 1.866188883781433, 'validation/bleu': 26.629292980287577, 'validation/num_examples': 3000, 'test/accuracy': 0.6472604870796204, 'test/loss': 1.8144638538360596, 'test/bleu': 26.0036035069853, 'test/num_examples': 3003, 'score': 5916.943907022476, 'total_duration': 10295.500350952148, 'accumulated_submission_time': 5916.943907022476, 'accumulated_eval_time': 4377.803135633469, 'accumulated_logging_time': 0.19261598587036133}
I0206 13:50:08.271595 139616064616192 logging_writer.py:48] [16823] accumulated_eval_time=4377.803136, accumulated_logging_time=0.192616, accumulated_submission_time=5916.943907, global_step=16823, preemption_count=0, score=5916.943907, test/accuracy=0.647260, test/bleu=26.003604, test/loss=1.814464, test/num_examples=3003, total_duration=10295.500351, train/accuracy=0.619282, train/bleu=29.808880, train/loss=2.010907, validation/accuracy=0.637277, validation/bleu=26.629293, validation/loss=1.866189, validation/num_examples=3000
I0206 13:50:35.541120 139616073008896 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2923424541950226, loss=3.17075514793396
I0206 13:51:10.479774 139616064616192 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2802334725856781, loss=3.189119815826416
I0206 13:51:45.422410 139616073008896 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3486461043357849, loss=3.138601303100586
I0206 13:52:20.381059 139616064616192 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3283105492591858, loss=3.071713447570801
I0206 13:52:55.286553 139616073008896 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2841203510761261, loss=3.0263876914978027
I0206 13:53:30.201287 139616064616192 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3257456123828888, loss=3.1174840927124023
I0206 13:54:05.131088 139616073008896 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2949060797691345, loss=3.042280435562134
I0206 13:54:40.050813 139616064616192 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2864822447299957, loss=3.091545581817627
I0206 13:55:14.945503 139616073008896 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2708861827850342, loss=3.07415509223938
I0206 13:55:49.846684 139616064616192 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2914232909679413, loss=3.1208178997039795
I0206 13:56:24.789251 139616073008896 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3003566265106201, loss=3.0118446350097656
I0206 13:56:59.686408 139616064616192 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3319794535636902, loss=3.0705363750457764
I0206 13:57:34.585032 139616073008896 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.34824028611183167, loss=3.0078721046447754
I0206 13:58:09.475066 139616064616192 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.30257949233055115, loss=3.073983669281006
I0206 13:58:44.389772 139616073008896 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.29928573966026306, loss=3.1054165363311768
I0206 13:59:19.286327 139616064616192 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.375515341758728, loss=3.0440492630004883
I0206 13:59:54.218602 139616073008896 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2811123728752136, loss=3.0890259742736816
I0206 14:00:29.159771 139616064616192 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.28805357217788696, loss=3.039308786392212
I0206 14:01:04.108856 139616073008896 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.29041802883148193, loss=3.0646698474884033
I0206 14:01:39.019248 139616064616192 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3173132538795471, loss=2.994340658187866
I0206 14:02:13.921316 139616073008896 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.31655266880989075, loss=3.066964626312256
I0206 14:02:48.821809 139616064616192 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3334845304489136, loss=2.9728198051452637
I0206 14:03:23.759228 139616073008896 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.33628198504447937, loss=3.0668065547943115
I0206 14:03:58.662247 139616064616192 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3073088228702545, loss=3.0734896659851074
I0206 14:04:08.512419 139785736898368 spec.py:321] Evaluating on the training split.
I0206 14:04:11.514226 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:07:56.480153 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 14:07:59.194754 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:11:05.234743 139785736898368 spec.py:349] Evaluating on the test split.
I0206 14:11:07.949404 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:13:55.393765 139785736898368 submission_runner.py:408] Time since start: 11722.64s, 	Step: 19230, 	{'train/accuracy': 0.636422872543335, 'train/loss': 1.8756887912750244, 'train/bleu': 30.822602520333803, 'validation/accuracy': 0.6443069577217102, 'validation/loss': 1.8201544284820557, 'validation/bleu': 27.01061448954327, 'validation/num_examples': 3000, 'test/accuracy': 0.6535820364952087, 'test/loss': 1.7662612199783325, 'test/bleu': 26.518186076268215, 'test/num_examples': 3003, 'score': 6757.095978498459, 'total_duration': 11722.64000749588, 'accumulated_submission_time': 6757.095978498459, 'accumulated_eval_time': 4964.684417009354, 'accumulated_logging_time': 0.2200927734375}
I0206 14:13:55.411990 139616073008896 logging_writer.py:48] [19230] accumulated_eval_time=4964.684417, accumulated_logging_time=0.220093, accumulated_submission_time=6757.095978, global_step=19230, preemption_count=0, score=6757.095978, test/accuracy=0.653582, test/bleu=26.518186, test/loss=1.766261, test/num_examples=3003, total_duration=11722.640007, train/accuracy=0.636423, train/bleu=30.822603, train/loss=1.875689, validation/accuracy=0.644307, validation/bleu=27.010614, validation/loss=1.820154, validation/num_examples=3000
I0206 14:14:20.198240 139616064616192 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.33170968294143677, loss=2.967519521713257
I0206 14:14:55.111124 139616073008896 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.30030930042266846, loss=3.0515499114990234
I0206 14:15:30.030482 139616064616192 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.38828444480895996, loss=3.0536906719207764
I0206 14:16:04.926007 139616073008896 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2798976004123688, loss=3.0744309425354004
I0206 14:16:39.829362 139616064616192 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2957574129104614, loss=3.055725336074829
I0206 14:17:14.716615 139616073008896 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.32213810086250305, loss=3.1095468997955322
I0206 14:17:49.627705 139616064616192 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.395486444234848, loss=3.0314674377441406
I0206 14:18:24.521869 139616073008896 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3322996199131012, loss=3.042296886444092
I0206 14:18:59.421709 139616064616192 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.31754082441329956, loss=3.0231714248657227
I0206 14:19:34.315164 139616073008896 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3145952820777893, loss=3.038071393966675
I0206 14:20:09.219878 139616064616192 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.29866817593574524, loss=3.0160701274871826
I0206 14:20:44.117748 139616073008896 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.2895587980747223, loss=3.1351425647735596
I0206 14:21:19.026459 139616064616192 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.29222649335861206, loss=3.082953691482544
I0206 14:21:53.925693 139616073008896 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.31501731276512146, loss=3.0774636268615723
I0206 14:22:28.826070 139616064616192 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3354918956756592, loss=3.0508248805999756
I0206 14:23:03.729762 139616073008896 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.4597569406032562, loss=3.0210976600646973
I0206 14:23:38.612545 139616064616192 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3424891531467438, loss=2.9823596477508545
I0206 14:24:13.505726 139616073008896 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.322331041097641, loss=3.106229782104492
I0206 14:24:48.404114 139616064616192 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4051584005355835, loss=3.0430855751037598
I0206 14:25:23.315085 139616073008896 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.31902146339416504, loss=3.054260015487671
I0206 14:25:58.199259 139616064616192 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.35790735483169556, loss=3.006885528564453
I0206 14:26:33.090238 139616073008896 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.36915910243988037, loss=3.1259114742279053
I0206 14:27:08.024284 139616064616192 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3393089771270752, loss=2.9944961071014404
I0206 14:27:42.942791 139616073008896 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3278549909591675, loss=3.0074193477630615
I0206 14:27:55.552963 139785736898368 spec.py:321] Evaluating on the training split.
I0206 14:27:58.550600 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:30:48.422133 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 14:30:51.121604 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:33:20.537551 139785736898368 spec.py:349] Evaluating on the test split.
I0206 14:33:23.241454 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:35:42.353414 139785736898368 submission_runner.py:408] Time since start: 13029.60s, 	Step: 21638, 	{'train/accuracy': 0.6261128783226013, 'train/loss': 1.9536588191986084, 'train/bleu': 30.657585430152547, 'validation/accuracy': 0.6480762958526611, 'validation/loss': 1.7900378704071045, 'validation/bleu': 27.471663978033018, 'validation/num_examples': 3000, 'test/accuracy': 0.6585091352462769, 'test/loss': 1.7311699390411377, 'test/bleu': 27.19216074685112, 'test/num_examples': 3003, 'score': 7597.150082349777, 'total_duration': 13029.59965801239, 'accumulated_submission_time': 7597.150082349777, 'accumulated_eval_time': 5431.484809875488, 'accumulated_logging_time': 0.24876832962036133}
I0206 14:35:42.370849 139616064616192 logging_writer.py:48] [21638] accumulated_eval_time=5431.484810, accumulated_logging_time=0.248768, accumulated_submission_time=7597.150082, global_step=21638, preemption_count=0, score=7597.150082, test/accuracy=0.658509, test/bleu=27.192161, test/loss=1.731170, test/num_examples=3003, total_duration=13029.599658, train/accuracy=0.626113, train/bleu=30.657585, train/loss=1.953659, validation/accuracy=0.648076, validation/bleu=27.471664, validation/loss=1.790038, validation/num_examples=3000
I0206 14:36:04.386930 139616073008896 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3196764588356018, loss=2.998749256134033
I0206 14:36:39.293882 139616064616192 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.33214882016181946, loss=3.0852503776550293
I0206 14:37:14.189478 139616073008896 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3336702585220337, loss=3.0109169483184814
I0206 14:37:49.080604 139616064616192 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3782912492752075, loss=3.021897077560425
I0206 14:38:23.981805 139616073008896 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.351950079202652, loss=3.060108184814453
I0206 14:38:58.896740 139616064616192 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.36208269000053406, loss=3.0248591899871826
I0206 14:39:33.837906 139616073008896 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3535735607147217, loss=3.0122060775756836
I0206 14:40:08.761659 139616064616192 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.2556626796722412, loss=3.2339351177215576
I0206 14:40:43.676876 139616073008896 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.40061265230178833, loss=3.0807809829711914
I0206 14:41:18.570441 139616064616192 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.323917031288147, loss=3.019169330596924
I0206 14:41:53.476354 139616073008896 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.3557114899158478, loss=3.0779669284820557
I0206 14:42:28.372663 139616064616192 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.32623806595802307, loss=2.907848596572876
I0206 14:43:03.260522 139616073008896 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.31827935576438904, loss=3.0688443183898926
I0206 14:43:38.178841 139616064616192 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.357369989156723, loss=3.0512607097625732
I0206 14:44:13.094048 139616073008896 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.35798588395118713, loss=3.0277538299560547
I0206 14:44:47.979733 139616064616192 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.40441402792930603, loss=4.951004981994629
I0206 14:45:22.807757 139616073008896 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8103988170623779, loss=4.80026912689209
I0206 14:45:57.641932 139616064616192 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.46339190006256104, loss=4.749114513397217
I0206 14:46:32.542503 139616073008896 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9162580966949463, loss=4.732560157775879
I0206 14:47:07.396200 139616064616192 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.7985848188400269, loss=4.723014831542969
I0206 14:47:42.247882 139616073008896 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.823808193206787, loss=4.523502349853516
I0206 14:48:17.122459 139616064616192 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.45383962988853455, loss=3.2167916297912598
I0206 14:48:52.052366 139616073008896 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.33084967732429504, loss=3.056257963180542
I0206 14:49:26.934507 139616064616192 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3471771478652954, loss=3.032965660095215
I0206 14:49:42.691883 139785736898368 spec.py:321] Evaluating on the training split.
I0206 14:49:45.708631 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:52:47.919495 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 14:52:50.641701 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:55:28.060974 139785736898368 spec.py:349] Evaluating on the test split.
I0206 14:55:30.770643 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 14:58:04.801706 139785736898368 submission_runner.py:408] Time since start: 14372.05s, 	Step: 24047, 	{'train/accuracy': 0.6246453523635864, 'train/loss': 1.9670435190200806, 'train/bleu': 29.88082264721335, 'validation/accuracy': 0.6471339464187622, 'validation/loss': 1.8075987100601196, 'validation/bleu': 27.139849535608672, 'validation/num_examples': 3000, 'test/accuracy': 0.6549997329711914, 'test/loss': 1.7625569105148315, 'test/bleu': 26.310299172789822, 'test/num_examples': 3003, 'score': 8437.384685277939, 'total_duration': 14372.047944068909, 'accumulated_submission_time': 8437.384685277939, 'accumulated_eval_time': 5933.594583034515, 'accumulated_logging_time': 0.2760488986968994}
I0206 14:58:04.819081 139616073008896 logging_writer.py:48] [24047] accumulated_eval_time=5933.594583, accumulated_logging_time=0.276049, accumulated_submission_time=8437.384685, global_step=24047, preemption_count=0, score=8437.384685, test/accuracy=0.655000, test/bleu=26.310299, test/loss=1.762557, test/num_examples=3003, total_duration=14372.047944, train/accuracy=0.624645, train/bleu=29.880823, train/loss=1.967044, validation/accuracy=0.647134, validation/bleu=27.139850, validation/loss=1.807599, validation/num_examples=3000
I0206 14:58:23.673923 139616064616192 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.33554503321647644, loss=3.105943202972412
I0206 14:58:58.582841 139616073008896 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3214644491672516, loss=3.0242347717285156
I0206 14:59:33.464208 139616064616192 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3160577714443207, loss=3.126899003982544
I0206 15:00:08.371430 139616073008896 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4111541509628296, loss=3.063570737838745
I0206 15:00:43.305607 139616064616192 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3287099599838257, loss=3.019026041030884
I0206 15:01:18.209595 139616073008896 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3294007480144501, loss=2.9976394176483154
I0206 15:01:53.069590 139616064616192 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.31162694096565247, loss=2.9599711894989014
I0206 15:02:27.986449 139616073008896 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.31033191084861755, loss=3.0419046878814697
I0206 15:03:02.897881 139616064616192 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.31605252623558044, loss=3.0346016883850098
I0206 15:03:37.790068 139616073008896 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.36257457733154297, loss=3.0440142154693604
I0206 15:04:12.661524 139616064616192 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3907366394996643, loss=3.0402190685272217
I0206 15:04:47.576549 139616073008896 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.375484824180603, loss=3.014984607696533
I0206 15:05:22.462727 139616064616192 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.39797788858413696, loss=3.0753250122070312
I0206 15:05:57.354991 139616073008896 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3120734393596649, loss=3.0147836208343506
I0206 15:06:32.253479 139616064616192 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3266771137714386, loss=2.930449962615967
I0206 15:07:07.152640 139616073008896 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.3194738030433655, loss=3.0399012565612793
I0206 15:07:42.036760 139616064616192 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3444063067436218, loss=3.0024752616882324
I0206 15:08:16.934315 139616073008896 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3425147831439972, loss=2.9295177459716797
I0206 15:08:51.823044 139616064616192 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3778710663318634, loss=3.0667784214019775
I0206 15:09:26.733074 139616073008896 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.35389530658721924, loss=2.902989387512207
I0206 15:10:01.618520 139616064616192 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.2919159233570099, loss=3.065884590148926
I0206 15:10:36.532004 139616073008896 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.3547888994216919, loss=3.0745837688446045
I0206 15:11:11.454606 139616064616192 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.333955854177475, loss=3.017019033432007
I0206 15:11:46.389739 139616073008896 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.34638646245002747, loss=3.1209092140197754
I0206 15:12:04.967983 139785736898368 spec.py:321] Evaluating on the training split.
I0206 15:12:07.954634 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:15:10.162913 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 15:15:12.869050 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:17:42.505221 139785736898368 spec.py:349] Evaluating on the test split.
I0206 15:17:45.211209 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:20:06.762709 139785736898368 submission_runner.py:408] Time since start: 15694.01s, 	Step: 26455, 	{'train/accuracy': 0.6341571807861328, 'train/loss': 1.8880059719085693, 'train/bleu': 30.919613536831562, 'validation/accuracy': 0.6495393514633179, 'validation/loss': 1.7730599641799927, 'validation/bleu': 27.610013018571298, 'validation/num_examples': 3000, 'test/accuracy': 0.6604846119880676, 'test/loss': 1.7240240573883057, 'test/bleu': 27.430558262691026, 'test/num_examples': 3003, 'score': 9277.445498466492, 'total_duration': 15694.008920431137, 'accumulated_submission_time': 9277.445498466492, 'accumulated_eval_time': 6415.389216184616, 'accumulated_logging_time': 0.30326366424560547}
I0206 15:20:06.784191 139616064616192 logging_writer.py:48] [26455] accumulated_eval_time=6415.389216, accumulated_logging_time=0.303264, accumulated_submission_time=9277.445498, global_step=26455, preemption_count=0, score=9277.445498, test/accuracy=0.660485, test/bleu=27.430558, test/loss=1.724024, test/num_examples=3003, total_duration=15694.008920, train/accuracy=0.634157, train/bleu=30.919614, train/loss=1.888006, validation/accuracy=0.649539, validation/bleu=27.610013, validation/loss=1.773060, validation/num_examples=3000
I0206 15:20:22.854020 139616073008896 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.381811261177063, loss=3.0662755966186523
I0206 15:20:57.827989 139616064616192 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3754972815513611, loss=3.039450168609619
I0206 15:21:32.846045 139616073008896 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3264214098453522, loss=3.015063762664795
I0206 15:22:07.736882 139616064616192 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.35898491740226746, loss=3.0587785243988037
I0206 15:22:42.615986 139616073008896 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.3792943060398102, loss=3.013042449951172
I0206 15:23:17.508233 139616064616192 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.35221946239471436, loss=2.9789254665374756
I0206 15:23:52.438754 139616073008896 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4565470218658447, loss=3.0418968200683594
I0206 15:24:27.346435 139616064616192 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.35707584023475647, loss=2.9869251251220703
I0206 15:25:02.237457 139616073008896 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3794676959514618, loss=2.9967620372772217
I0206 15:25:37.142378 139616064616192 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.31511321663856506, loss=2.946138858795166
I0206 15:26:12.016911 139616073008896 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.37863075733184814, loss=2.9651219844818115
I0206 15:26:46.886330 139616064616192 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6740385293960571, loss=3.0480287075042725
I0206 15:27:21.806543 139616073008896 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3531154990196228, loss=3.08229923248291
I0206 15:27:56.693166 139616064616192 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.3182685971260071, loss=3.0452094078063965
I0206 15:28:31.570644 139616073008896 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.4266684055328369, loss=2.9999032020568848
I0206 15:29:06.447964 139616064616192 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.3650754988193512, loss=2.995511770248413
I0206 15:29:41.309912 139616073008896 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.4341583549976349, loss=2.9604275226593018
I0206 15:30:16.234701 139616064616192 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.49441468715667725, loss=3.028822898864746
I0206 15:30:51.122867 139616073008896 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.39205458760261536, loss=2.975813627243042
I0206 15:31:26.061727 139616064616192 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.37215930223464966, loss=2.9975430965423584
I0206 15:32:00.956476 139616073008896 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5467609167098999, loss=3.0515739917755127
I0206 15:32:35.829902 139616064616192 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.3460257947444916, loss=3.036573886871338
I0206 15:33:10.690760 139616073008896 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.33977794647216797, loss=3.0010766983032227
I0206 15:33:45.612771 139616064616192 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.33848294615745544, loss=3.0464072227478027
I0206 15:34:06.951879 139785736898368 spec.py:321] Evaluating on the training split.
I0206 15:34:09.948124 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:37:07.256271 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 15:37:09.971583 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:39:55.160901 139785736898368 spec.py:349] Evaluating on the test split.
I0206 15:39:57.864208 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:42:30.192670 139785736898368 submission_runner.py:408] Time since start: 17037.44s, 	Step: 28863, 	{'train/accuracy': 0.6321583390235901, 'train/loss': 1.9047863483428955, 'train/bleu': 30.918711635523824, 'validation/accuracy': 0.6536558866500854, 'validation/loss': 1.7527354955673218, 'validation/bleu': 27.963738924004478, 'validation/num_examples': 3000, 'test/accuracy': 0.6618906855583191, 'test/loss': 1.6961952447891235, 'test/bleu': 27.259525740209053, 'test/num_examples': 3003, 'score': 10117.523353815079, 'total_duration': 17037.438900470734, 'accumulated_submission_time': 10117.523353815079, 'accumulated_eval_time': 6918.62993311882, 'accumulated_logging_time': 0.33623456954956055}
I0206 15:42:30.210841 139616073008896 logging_writer.py:48] [28863] accumulated_eval_time=6918.629933, accumulated_logging_time=0.336235, accumulated_submission_time=10117.523354, global_step=28863, preemption_count=0, score=10117.523354, test/accuracy=0.661891, test/bleu=27.259526, test/loss=1.696195, test/num_examples=3003, total_duration=17037.438900, train/accuracy=0.632158, train/bleu=30.918712, train/loss=1.904786, validation/accuracy=0.653656, validation/bleu=27.963739, validation/loss=1.752735, validation/num_examples=3000
I0206 15:42:43.472943 139616064616192 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.3664432764053345, loss=3.076702117919922
I0206 15:43:18.381341 139616073008896 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.34352198243141174, loss=2.9900710582733154
I0206 15:43:53.278920 139616064616192 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.3704381287097931, loss=3.028165817260742
I0206 15:44:28.153599 139616073008896 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.3888498544692993, loss=3.0269088745117188
I0206 15:45:03.083776 139616064616192 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3358619213104248, loss=3.0736358165740967
I0206 15:45:37.986275 139616073008896 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.45217403769493103, loss=3.0115227699279785
I0206 15:46:12.877529 139616064616192 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.35272952914237976, loss=3.037012815475464
I0206 15:46:47.768331 139616073008896 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3729418218135834, loss=2.9872398376464844
I0206 15:47:22.683032 139616064616192 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.32346275448799133, loss=3.0217502117156982
I0206 15:47:57.549013 139616073008896 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.35375842452049255, loss=2.9888103008270264
I0206 15:48:32.437391 139616064616192 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.36860230565071106, loss=3.0185277462005615
I0206 15:49:07.336794 139616073008896 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.38578662276268005, loss=3.0121729373931885
I0206 15:49:42.186184 139616064616192 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3629557192325592, loss=3.0437958240509033
I0206 15:50:17.097978 139616073008896 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.3632606267929077, loss=3.013552665710449
I0206 15:50:51.963512 139616064616192 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3864012658596039, loss=3.02364444732666
I0206 15:51:26.886127 139616073008896 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.33298003673553467, loss=2.9442219734191895
I0206 15:52:01.830306 139616064616192 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.3628321588039398, loss=3.0241751670837402
I0206 15:52:36.693243 139616073008896 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.37246954441070557, loss=2.924321413040161
I0206 15:53:11.592896 139616064616192 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3069794178009033, loss=2.996846914291382
I0206 15:53:46.433702 139616073008896 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.345809668302536, loss=2.94655442237854
I0206 15:54:21.311350 139616064616192 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3883292078971863, loss=2.9894349575042725
I0206 15:54:56.202337 139616073008896 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5217366814613342, loss=2.99995756149292
I0206 15:55:31.072955 139616064616192 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.340315580368042, loss=3.0550591945648193
I0206 15:56:05.959354 139616073008896 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.43036946654319763, loss=2.9633607864379883
I0206 15:56:30.437320 139785736898368 spec.py:321] Evaluating on the training split.
I0206 15:56:33.434768 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 15:59:19.488889 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 15:59:22.203138 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:02:24.675210 139785736898368 spec.py:349] Evaluating on the test split.
I0206 16:02:27.387674 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:05:22.784659 139785736898368 submission_runner.py:408] Time since start: 18410.03s, 	Step: 31272, 	{'train/accuracy': 0.6326280832290649, 'train/loss': 1.9133557081222534, 'train/bleu': 30.91387757741362, 'validation/accuracy': 0.6534698605537415, 'validation/loss': 1.7469457387924194, 'validation/bleu': 27.38010984222516, 'validation/num_examples': 3000, 'test/accuracy': 0.6626111268997192, 'test/loss': 1.6905726194381714, 'test/bleu': 27.10428323835168, 'test/num_examples': 3003, 'score': 10957.66311454773, 'total_duration': 18410.030876636505, 'accumulated_submission_time': 10957.66311454773, 'accumulated_eval_time': 7450.977185487747, 'accumulated_logging_time': 0.3637206554412842}
I0206 16:05:22.803308 139616064616192 logging_writer.py:48] [31272] accumulated_eval_time=7450.977185, accumulated_logging_time=0.363721, accumulated_submission_time=10957.663115, global_step=31272, preemption_count=0, score=10957.663115, test/accuracy=0.662611, test/bleu=27.104283, test/loss=1.690573, test/num_examples=3003, total_duration=18410.030877, train/accuracy=0.632628, train/bleu=30.913878, train/loss=1.913356, validation/accuracy=0.653470, validation/bleu=27.380110, validation/loss=1.746946, validation/num_examples=3000
I0206 16:05:32.941848 139616073008896 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.35279780626296997, loss=3.024115562438965
I0206 16:06:07.858952 139616064616192 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.37120378017425537, loss=3.0000407695770264
I0206 16:06:42.770716 139616073008896 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3368445634841919, loss=3.0122854709625244
I0206 16:07:17.666302 139616064616192 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.4050714373588562, loss=2.9728686809539795
I0206 16:07:52.541784 139616073008896 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.42334768176078796, loss=3.03452205657959
I0206 16:08:27.452406 139616064616192 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.360714316368103, loss=2.9203851222991943
I0206 16:09:02.346229 139616073008896 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3934120237827301, loss=3.0147297382354736
I0206 16:09:37.231507 139616064616192 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.35825398564338684, loss=2.9879231452941895
I0206 16:10:12.123764 139616073008896 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3708614110946655, loss=3.0174190998077393
I0206 16:10:46.973461 139616064616192 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.45110079646110535, loss=2.997823476791382
I0206 16:11:21.847470 139616073008896 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.36426448822021484, loss=3.029749631881714
I0206 16:11:56.730643 139616064616192 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3923480808734894, loss=3.046992063522339
I0206 16:12:31.607797 139616073008896 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.4532450735569, loss=2.979811668395996
I0206 16:13:06.503773 139616064616192 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.324660986661911, loss=2.99294114112854
I0206 16:13:41.359452 139616073008896 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3413220942020416, loss=2.9849693775177
I0206 16:14:16.220152 139616064616192 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.3589403033256531, loss=3.03318190574646
I0206 16:14:51.099776 139616073008896 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.4020184576511383, loss=2.981210947036743
I0206 16:15:25.968661 139616064616192 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.4154511094093323, loss=2.9672396183013916
I0206 16:16:00.832540 139616073008896 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.39592286944389343, loss=3.0058512687683105
I0206 16:16:35.740019 139616064616192 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.34501826763153076, loss=3.0204274654388428
I0206 16:17:10.619487 139616073008896 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.4433433711528778, loss=2.964961051940918
I0206 16:17:45.526274 139616064616192 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.46238231658935547, loss=3.01775860786438
I0206 16:18:20.425763 139616073008896 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.35901427268981934, loss=2.963122606277466
I0206 16:18:55.311853 139616064616192 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.34995555877685547, loss=3.030364751815796
I0206 16:19:22.920344 139785736898368 spec.py:321] Evaluating on the training split.
I0206 16:19:25.924764 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:22:24.911393 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 16:22:27.616856 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:24:55.514890 139785736898368 spec.py:349] Evaluating on the test split.
I0206 16:24:58.237870 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:27:23.458009 139785736898368 submission_runner.py:408] Time since start: 19730.70s, 	Step: 33681, 	{'train/accuracy': 0.63736891746521, 'train/loss': 1.8761459589004517, 'train/bleu': 30.778807132463136, 'validation/accuracy': 0.6563588976860046, 'validation/loss': 1.7285524606704712, 'validation/bleu': 27.708385993600455, 'validation/num_examples': 3000, 'test/accuracy': 0.6662018895149231, 'test/loss': 1.6690125465393066, 'test/bleu': 27.610379038748825, 'test/num_examples': 3003, 'score': 11797.693771123886, 'total_duration': 19730.70421743393, 'accumulated_submission_time': 11797.693771123886, 'accumulated_eval_time': 7931.5147523880005, 'accumulated_logging_time': 0.39255547523498535}
I0206 16:27:23.482035 139616073008896 logging_writer.py:48] [33681] accumulated_eval_time=7931.514752, accumulated_logging_time=0.392555, accumulated_submission_time=11797.693771, global_step=33681, preemption_count=0, score=11797.693771, test/accuracy=0.666202, test/bleu=27.610379, test/loss=1.669013, test/num_examples=3003, total_duration=19730.704217, train/accuracy=0.637369, train/bleu=30.778807, train/loss=1.876146, validation/accuracy=0.656359, validation/bleu=27.708386, validation/loss=1.728552, validation/num_examples=3000
I0206 16:27:30.475799 139616064616192 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.4166123569011688, loss=3.0423009395599365
I0206 16:28:05.391608 139616073008896 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.4002375602722168, loss=2.9488272666931152
I0206 16:28:40.262219 139616064616192 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.32303398847579956, loss=2.9025511741638184
I0206 16:29:15.159404 139616073008896 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.40293630957603455, loss=2.967555284500122
I0206 16:29:50.027274 139616064616192 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.3813762068748474, loss=2.975407600402832
I0206 16:30:24.928092 139616073008896 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.36010852456092834, loss=2.992645740509033
I0206 16:30:59.818616 139616064616192 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3868996500968933, loss=2.945606231689453
I0206 16:31:34.666868 139616073008896 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.34109601378440857, loss=2.949702739715576
I0206 16:32:09.551841 139616064616192 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.36510416865348816, loss=2.9095075130462646
I0206 16:32:44.418694 139616073008896 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.4225929081439972, loss=2.966452121734619
I0206 16:33:19.305070 139616064616192 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.38348332047462463, loss=3.031888723373413
I0206 16:33:54.172973 139616073008896 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.35865071415901184, loss=2.931363582611084
I0206 16:34:29.062906 139616064616192 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.4095507264137268, loss=2.9921164512634277
I0206 16:35:03.930796 139616073008896 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.42497649788856506, loss=2.938722848892212
I0206 16:35:38.794310 139616064616192 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.36146754026412964, loss=2.9752089977264404
I0206 16:36:13.654685 139616073008896 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3381258547306061, loss=2.9130406379699707
I0206 16:36:48.531821 139616064616192 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.3705492615699768, loss=2.9770967960357666
I0206 16:37:23.425113 139616073008896 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4336879551410675, loss=2.9614834785461426
I0206 16:37:58.297930 139616064616192 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.34703707695007324, loss=2.985377550125122
I0206 16:38:33.206281 139616073008896 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3449952304363251, loss=2.955800771713257
I0206 16:39:08.130653 139616064616192 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.3827356696128845, loss=3.011262893676758
I0206 16:39:43.006016 139616073008896 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.38619622588157654, loss=2.9887819290161133
I0206 16:40:17.901153 139616064616192 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.37392672896385193, loss=3.0080666542053223
I0206 16:40:52.757395 139616073008896 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.33919191360473633, loss=2.935417413711548
I0206 16:41:23.519537 139785736898368 spec.py:321] Evaluating on the training split.
I0206 16:41:26.525448 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:44:17.194495 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 16:44:19.905205 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:46:57.629335 139785736898368 spec.py:349] Evaluating on the test split.
I0206 16:47:00.354340 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 16:49:28.678672 139785736898368 submission_runner.py:408] Time since start: 21055.92s, 	Step: 36090, 	{'train/accuracy': 0.6397969126701355, 'train/loss': 1.8589222431182861, 'train/bleu': 30.72574395429576, 'validation/accuracy': 0.6573135852813721, 'validation/loss': 1.7214807271957397, 'validation/bleu': 28.437746236206387, 'validation/num_examples': 3000, 'test/accuracy': 0.6704433560371399, 'test/loss': 1.653224229812622, 'test/bleu': 28.013954515918293, 'test/num_examples': 3003, 'score': 12637.642409086227, 'total_duration': 21055.924880743027, 'accumulated_submission_time': 12637.642409086227, 'accumulated_eval_time': 8416.673792600632, 'accumulated_logging_time': 0.4278111457824707}
I0206 16:49:28.702142 139616064616192 logging_writer.py:48] [36090] accumulated_eval_time=8416.673793, accumulated_logging_time=0.427811, accumulated_submission_time=12637.642409, global_step=36090, preemption_count=0, score=12637.642409, test/accuracy=0.670443, test/bleu=28.013955, test/loss=1.653224, test/num_examples=3003, total_duration=21055.924881, train/accuracy=0.639797, train/bleu=30.725744, train/loss=1.858922, validation/accuracy=0.657314, validation/bleu=28.437746, validation/loss=1.721481, validation/num_examples=3000
I0206 16:49:32.561712 139616073008896 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.44385766983032227, loss=2.9063737392425537
I0206 16:50:07.530279 139616064616192 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3384411633014679, loss=2.98868989944458
I0206 16:50:42.401396 139616073008896 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.3321154713630676, loss=2.905590295791626
I0206 16:51:17.289528 139616064616192 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3393164873123169, loss=3.0171844959259033
I0206 16:51:52.164277 139616073008896 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.3886418342590332, loss=2.9751861095428467
I0206 16:52:27.058246 139616064616192 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.3658601939678192, loss=2.9309074878692627
I0206 16:53:01.949846 139616073008896 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.3797833025455475, loss=2.889399290084839
I0206 16:53:36.812031 139616064616192 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.39252769947052, loss=2.9308202266693115
I0206 16:54:11.665895 139616073008896 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.4416372776031494, loss=3.023002862930298
I0206 16:54:46.535622 139616064616192 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.43422242999076843, loss=2.9894773960113525
I0206 16:55:21.404849 139616073008896 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3383970558643341, loss=2.9221715927124023
I0206 16:55:56.274590 139616064616192 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.33104488253593445, loss=2.979365348815918
I0206 16:56:31.138610 139616073008896 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.38225841522216797, loss=3.016221523284912
I0206 16:57:06.047684 139616064616192 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3907412588596344, loss=3.071343183517456
I0206 16:57:40.951270 139616073008896 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.34131893515586853, loss=2.9981529712677
I0206 16:58:15.843515 139616064616192 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.3766094446182251, loss=3.0403754711151123
I0206 16:58:50.728453 139616073008896 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.369893878698349, loss=3.0335283279418945
I0206 16:59:25.587381 139616064616192 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.347764253616333, loss=2.8984251022338867
I0206 17:00:00.433348 139616073008896 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.3858010172843933, loss=2.988647222518921
I0206 17:00:35.304133 139616064616192 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.38694247603416443, loss=2.9839730262756348
I0206 17:01:10.170598 139616073008896 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.36357977986335754, loss=2.998281478881836
I0206 17:01:45.033170 139616064616192 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.4085332453250885, loss=2.946134328842163
I0206 17:02:19.890537 139616073008896 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.349875807762146, loss=2.95414400100708
I0206 17:02:54.752118 139616064616192 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.34673523902893066, loss=2.981494426727295
I0206 17:03:28.975722 139785736898368 spec.py:321] Evaluating on the training split.
I0206 17:03:31.991004 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:06:08.446196 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 17:06:11.167992 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:08:47.145165 139785736898368 spec.py:349] Evaluating on the test split.
I0206 17:08:49.886433 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:11:22.490026 139785736898368 submission_runner.py:408] Time since start: 22369.74s, 	Step: 38500, 	{'train/accuracy': 0.6449896693229675, 'train/loss': 1.8099712133407593, 'train/bleu': 31.571335776793276, 'validation/accuracy': 0.6588138937950134, 'validation/loss': 1.7067581415176392, 'validation/bleu': 28.44258842361144, 'validation/num_examples': 3000, 'test/accuracy': 0.6685724258422852, 'test/loss': 1.6480258703231812, 'test/bleu': 27.752310460703928, 'test/num_examples': 3003, 'score': 13477.828237771988, 'total_duration': 22369.73627972603, 'accumulated_submission_time': 13477.828237771988, 'accumulated_eval_time': 8890.188046693802, 'accumulated_logging_time': 0.4626927375793457}
I0206 17:11:22.510250 139616073008896 logging_writer.py:48] [38500] accumulated_eval_time=8890.188047, accumulated_logging_time=0.462693, accumulated_submission_time=13477.828238, global_step=38500, preemption_count=0, score=13477.828238, test/accuracy=0.668572, test/bleu=27.752310, test/loss=1.648026, test/num_examples=3003, total_duration=22369.736280, train/accuracy=0.644990, train/bleu=31.571336, train/loss=1.809971, validation/accuracy=0.658814, validation/bleu=28.442588, validation/loss=1.706758, validation/num_examples=3000
I0206 17:11:22.884810 139616064616192 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3683921694755554, loss=2.9878878593444824
I0206 17:11:57.761728 139616073008896 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.36799803376197815, loss=2.948951482772827
I0206 17:12:32.649701 139616064616192 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3580552935600281, loss=2.959869384765625
I0206 17:13:07.543439 139616073008896 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3486720323562622, loss=2.947730541229248
I0206 17:13:42.406666 139616064616192 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3592570126056671, loss=2.953531265258789
I0206 17:14:17.233129 139616073008896 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3783978223800659, loss=2.985764503479004
I0206 17:14:52.098772 139616064616192 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.36512812972068787, loss=2.9759817123413086
I0206 17:15:26.946941 139616073008896 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3475339114665985, loss=2.8727734088897705
I0206 17:16:01.811366 139616064616192 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3575704097747803, loss=3.002007484436035
I0206 17:16:36.667970 139616073008896 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4180753827095032, loss=2.911834716796875
I0206 17:17:11.520923 139616064616192 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.32469746470451355, loss=2.983347177505493
I0206 17:17:46.365097 139616073008896 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3718246817588806, loss=2.892241954803467
I0206 17:18:21.255042 139616064616192 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3461079001426697, loss=2.954486846923828
I0206 17:18:56.167253 139616073008896 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.39931520819664, loss=2.9466493129730225
I0206 17:19:31.077118 139616064616192 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.33079788088798523, loss=2.934316396713257
I0206 17:20:05.957434 139616073008896 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.38237887620925903, loss=2.9710240364074707
I0206 17:20:40.846605 139616064616192 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3348971903324127, loss=2.932382345199585
I0206 17:21:15.708099 139616073008896 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3720327615737915, loss=2.999048948287964
I0206 17:21:50.557883 139616064616192 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.33245500922203064, loss=2.9285881519317627
I0206 17:22:25.477813 139616073008896 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.34468069672584534, loss=2.9848568439483643
I0206 17:23:00.442230 139616064616192 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3251684308052063, loss=2.9769127368927
I0206 17:23:35.316789 139616073008896 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.37376275658607483, loss=2.8548367023468018
I0206 17:24:10.168362 139616064616192 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.41948163509368896, loss=2.96178936958313
I0206 17:24:45.072518 139616073008896 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3331063985824585, loss=2.968088388442993
I0206 17:25:19.950200 139616064616192 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.37695369124412537, loss=3.0333616733551025
I0206 17:25:22.808547 139785736898368 spec.py:321] Evaluating on the training split.
I0206 17:25:25.797847 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:28:05.514133 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 17:28:08.221915 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:30:37.770390 139785736898368 spec.py:349] Evaluating on the test split.
I0206 17:30:40.487312 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:33:10.169233 139785736898368 submission_runner.py:408] Time since start: 23677.42s, 	Step: 40910, 	{'train/accuracy': 0.6428003907203674, 'train/loss': 1.8337481021881104, 'train/bleu': 31.276876161097288, 'validation/accuracy': 0.6606861352920532, 'validation/loss': 1.705464482307434, 'validation/bleu': 28.587942244966534, 'validation/num_examples': 3000, 'test/accuracy': 0.6722212433815002, 'test/loss': 1.6383522748947144, 'test/bleu': 28.249942435103367, 'test/num_examples': 3003, 'score': 14318.037878036499, 'total_duration': 23677.41546010971, 'accumulated_submission_time': 14318.037878036499, 'accumulated_eval_time': 9357.548652887344, 'accumulated_logging_time': 0.4925673007965088}
I0206 17:33:10.193872 139616073008896 logging_writer.py:48] [40910] accumulated_eval_time=9357.548653, accumulated_logging_time=0.492567, accumulated_submission_time=14318.037878, global_step=40910, preemption_count=0, score=14318.037878, test/accuracy=0.672221, test/bleu=28.249942, test/loss=1.638352, test/num_examples=3003, total_duration=23677.415460, train/accuracy=0.642800, train/bleu=31.276876, train/loss=1.833748, validation/accuracy=0.660686, validation/bleu=28.587942, validation/loss=1.705464, validation/num_examples=3000
I0206 17:33:42.008731 139616064616192 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.4115058481693268, loss=3.0063841342926025
I0206 17:34:16.923801 139616073008896 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.3550536334514618, loss=2.991997718811035
I0206 17:34:51.861178 139616064616192 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4067472815513611, loss=2.9094815254211426
I0206 17:35:26.754563 139616073008896 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.4189925491809845, loss=3.001354694366455
I0206 17:36:01.636756 139616064616192 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.3797594904899597, loss=2.972783327102661
I0206 17:36:36.519021 139616073008896 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.35613784193992615, loss=2.926720380783081
I0206 17:37:11.403592 139616064616192 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.43319201469421387, loss=2.996976137161255
I0206 17:37:46.271090 139616073008896 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.4804249405860901, loss=2.958672523498535
I0206 17:38:21.135448 139616064616192 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3609514534473419, loss=3.006237745285034
I0206 17:38:55.974366 139616073008896 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3453037142753601, loss=2.9118292331695557
I0206 17:39:30.835408 139616064616192 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3364332616329193, loss=3.0130434036254883
I0206 17:40:05.693937 139616073008896 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.3443933427333832, loss=2.9663004875183105
I0206 17:40:40.549671 139616064616192 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4370410144329071, loss=2.93810772895813
I0206 17:41:15.407859 139616073008896 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3441159725189209, loss=2.9107882976531982
I0206 17:41:50.288777 139616064616192 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.35943853855133057, loss=2.8870413303375244
I0206 17:42:25.147601 139616073008896 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.35986924171447754, loss=2.9433021545410156
I0206 17:43:00.003861 139616064616192 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.3536546528339386, loss=2.8997113704681396
I0206 17:43:34.869691 139616073008896 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.34222012758255005, loss=2.9684853553771973
I0206 17:44:09.754132 139616064616192 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.35788559913635254, loss=2.8865814208984375
I0206 17:44:44.632626 139616073008896 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.34794020652770996, loss=2.87565279006958
I0206 17:45:19.494823 139616064616192 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3423258364200592, loss=2.959170341491699
I0206 17:45:54.389677 139616073008896 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.3640284240245819, loss=2.9702417850494385
I0206 17:46:29.260286 139616064616192 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.3817118704319, loss=2.9765145778656006
I0206 17:47:04.140281 139616073008896 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3650210499763489, loss=2.8964860439300537
I0206 17:47:10.491350 139785736898368 spec.py:321] Evaluating on the training split.
I0206 17:47:13.485533 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:50:03.152876 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 17:50:05.851823 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:52:50.199948 139785736898368 spec.py:349] Evaluating on the test split.
I0206 17:52:52.909701 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 17:55:24.586918 139785736898368 submission_runner.py:408] Time since start: 25011.83s, 	Step: 43320, 	{'train/accuracy': 0.6413927674293518, 'train/loss': 1.8486982583999634, 'train/bleu': 30.779487617140454, 'validation/accuracy': 0.6622360348701477, 'validation/loss': 1.69834566116333, 'validation/bleu': 28.52823324647621, 'validation/num_examples': 3000, 'test/accuracy': 0.6720702052116394, 'test/loss': 1.6318928003311157, 'test/bleu': 28.10957228259788, 'test/num_examples': 3003, 'score': 15158.246175765991, 'total_duration': 25011.8331720829, 'accumulated_submission_time': 15158.246175765991, 'accumulated_eval_time': 9851.644168376923, 'accumulated_logging_time': 0.5287151336669922}
I0206 17:55:24.607318 139616064616192 logging_writer.py:48] [43320] accumulated_eval_time=9851.644168, accumulated_logging_time=0.528715, accumulated_submission_time=15158.246176, global_step=43320, preemption_count=0, score=15158.246176, test/accuracy=0.672070, test/bleu=28.109572, test/loss=1.631893, test/num_examples=3003, total_duration=25011.833172, train/accuracy=0.641393, train/bleu=30.779488, train/loss=1.848698, validation/accuracy=0.662236, validation/bleu=28.528233, validation/loss=1.698346, validation/num_examples=3000
I0206 17:55:52.892224 139616073008896 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3452232778072357, loss=2.997650384902954
I0206 17:56:27.826588 139616064616192 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3531964421272278, loss=2.932987928390503
I0206 17:57:02.728385 139616073008896 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.36171069741249084, loss=2.9217123985290527
I0206 17:57:37.614174 139616064616192 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.37231236696243286, loss=2.985602617263794
I0206 17:58:12.511823 139616073008896 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3476956784725189, loss=2.9896163940429688
I0206 17:58:47.389442 139616064616192 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.4186912477016449, loss=2.9447848796844482
I0206 17:59:22.239299 139616073008896 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3742564916610718, loss=2.9576542377471924
I0206 17:59:57.079309 139616064616192 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.34962940216064453, loss=2.9131956100463867
I0206 18:00:31.940235 139616073008896 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.3419783115386963, loss=2.9652276039123535
I0206 18:01:06.860518 139616064616192 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.3880350589752197, loss=2.9335882663726807
I0206 18:01:41.743757 139616073008896 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.363202840089798, loss=2.9045419692993164
I0206 18:02:16.586586 139616064616192 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.3526122272014618, loss=2.9870128631591797
I0206 18:02:51.465525 139616073008896 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.32318544387817383, loss=2.8719403743743896
I0206 18:03:26.334121 139616064616192 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.33572065830230713, loss=2.9397904872894287
I0206 18:04:01.204031 139616073008896 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.36520013213157654, loss=2.9614500999450684
I0206 18:04:36.069726 139616064616192 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.3567045331001282, loss=2.9985454082489014
I0206 18:05:10.940628 139616073008896 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3203236758708954, loss=2.9275002479553223
I0206 18:05:45.815075 139616064616192 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.34379225969314575, loss=2.9475510120391846
I0206 18:06:20.700216 139616073008896 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3727739751338959, loss=2.9283206462860107
I0206 18:06:55.553511 139616064616192 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3895590603351593, loss=3.0191919803619385
I0206 18:07:30.460996 139616073008896 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3730519115924835, loss=2.965506076812744
I0206 18:08:05.365819 139616064616192 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.3315798044204712, loss=2.9606287479400635
I0206 18:08:40.254827 139616073008896 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3595001995563507, loss=2.952709674835205
I0206 18:09:15.103489 139616064616192 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.3797294497489929, loss=2.9132020473480225
I0206 18:09:24.931936 139785736898368 spec.py:321] Evaluating on the training split.
I0206 18:09:27.936149 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:12:16.766193 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 18:12:19.473966 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:14:55.358082 139785736898368 spec.py:349] Evaluating on the test split.
I0206 18:14:58.068038 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:17:40.349989 139785736898368 submission_runner.py:408] Time since start: 26347.60s, 	Step: 45730, 	{'train/accuracy': 0.642379641532898, 'train/loss': 1.8306187391281128, 'train/bleu': 31.599102572516422, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.6910277605056763, 'validation/bleu': 28.548053027735303, 'validation/num_examples': 3000, 'test/accuracy': 0.6729068756103516, 'test/loss': 1.6298214197158813, 'test/bleu': 27.98403783607923, 'test/num_examples': 3003, 'score': 15998.48146367073, 'total_duration': 26347.59620976448, 'accumulated_submission_time': 15998.48146367073, 'accumulated_eval_time': 10347.062133073807, 'accumulated_logging_time': 0.5609724521636963}
I0206 18:17:40.372873 139616073008896 logging_writer.py:48] [45730] accumulated_eval_time=10347.062133, accumulated_logging_time=0.560972, accumulated_submission_time=15998.481464, global_step=45730, preemption_count=0, score=15998.481464, test/accuracy=0.672907, test/bleu=27.984038, test/loss=1.629821, test/num_examples=3003, total_duration=26347.596210, train/accuracy=0.642380, train/bleu=31.599103, train/loss=1.830619, validation/accuracy=0.662062, validation/bleu=28.548053, validation/loss=1.691028, validation/num_examples=3000
I0206 18:18:05.141931 139616064616192 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.3187790811061859, loss=2.891538619995117
I0206 18:18:40.063349 139616073008896 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3532235026359558, loss=2.9464526176452637
I0206 18:19:14.988830 139616064616192 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3491516709327698, loss=2.926384687423706
I0206 18:19:49.886078 139616073008896 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.392134428024292, loss=2.983794689178467
I0206 18:20:24.755513 139616064616192 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.37804120779037476, loss=2.9770493507385254
I0206 18:20:59.631438 139616073008896 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.33689436316490173, loss=2.924832344055176
I0206 18:21:34.522136 139616064616192 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3999428153038025, loss=2.895050525665283
I0206 18:22:09.392570 139616073008896 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.40489616990089417, loss=2.9386045932769775
I0206 18:22:44.267133 139616064616192 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3593759834766388, loss=2.8725733757019043
I0206 18:23:19.137527 139616073008896 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3938975930213928, loss=3.0340986251831055
I0206 18:23:54.007230 139616064616192 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.3389529883861542, loss=2.8687644004821777
I0206 18:24:28.870926 139616073008896 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.35911303758621216, loss=2.9100630283355713
I0206 18:25:03.710043 139616064616192 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.34725552797317505, loss=2.9363796710968018
I0206 18:25:38.584211 139616073008896 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3513263761997223, loss=2.909630537033081
I0206 18:26:13.467031 139616064616192 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.32054659724235535, loss=2.867900848388672
I0206 18:26:48.327775 139616073008896 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.31157925724983215, loss=2.8788115978240967
I0206 18:27:23.207480 139616064616192 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3801024556159973, loss=2.904982566833496
I0206 18:27:58.104940 139616073008896 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.34563517570495605, loss=2.9272842407226562
I0206 18:28:32.966568 139616064616192 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.3375818729400635, loss=2.925264596939087
I0206 18:29:07.832215 139616073008896 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.38331326842308044, loss=2.9681508541107178
I0206 18:29:42.730815 139616064616192 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.34734898805618286, loss=2.8787753582000732
I0206 18:30:17.610473 139616073008896 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3791796565055847, loss=2.9338791370391846
I0206 18:30:52.468457 139616064616192 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3227011263370514, loss=2.877758026123047
I0206 18:31:27.328483 139616073008896 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3656163811683655, loss=2.962109327316284
I0206 18:31:40.648859 139785736898368 spec.py:321] Evaluating on the training split.
I0206 18:31:43.641938 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:34:35.398628 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 18:34:38.095291 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:37:13.214657 139785736898368 spec.py:349] Evaluating on the test split.
I0206 18:37:15.935438 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:39:41.853903 139785736898368 submission_runner.py:408] Time since start: 27669.10s, 	Step: 48140, 	{'train/accuracy': 0.643227219581604, 'train/loss': 1.8408466577529907, 'train/bleu': 31.4491060968431, 'validation/accuracy': 0.6614425182342529, 'validation/loss': 1.6888902187347412, 'validation/bleu': 28.538099099304283, 'validation/num_examples': 3000, 'test/accuracy': 0.674429178237915, 'test/loss': 1.6186769008636475, 'test/bleu': 28.14769900835641, 'test/num_examples': 3003, 'score': 16838.670334100723, 'total_duration': 27669.10016155243, 'accumulated_submission_time': 16838.670334100723, 'accumulated_eval_time': 10828.267130374908, 'accumulated_logging_time': 0.5938632488250732}
I0206 18:39:41.874447 139616064616192 logging_writer.py:48] [48140] accumulated_eval_time=10828.267130, accumulated_logging_time=0.593863, accumulated_submission_time=16838.670334, global_step=48140, preemption_count=0, score=16838.670334, test/accuracy=0.674429, test/bleu=28.147699, test/loss=1.618677, test/num_examples=3003, total_duration=27669.100162, train/accuracy=0.643227, train/bleu=31.449106, train/loss=1.840847, validation/accuracy=0.661443, validation/bleu=28.538099, validation/loss=1.688890, validation/num_examples=3000
I0206 18:40:03.207524 139616073008896 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3030879497528076, loss=2.9348506927490234
I0206 18:40:38.070013 139616064616192 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.36569228768348694, loss=2.961341381072998
I0206 18:41:12.947942 139616073008896 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3565238118171692, loss=2.924753427505493
I0206 18:41:47.844906 139616064616192 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3461145758628845, loss=2.8979766368865967
I0206 18:42:22.752732 139616073008896 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.33019310235977173, loss=2.9834370613098145
I0206 18:42:57.610663 139616064616192 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3494908809661865, loss=2.917473316192627
I0206 18:43:32.490780 139616073008896 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3195175230503082, loss=2.8846020698547363
I0206 18:44:07.394115 139616064616192 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.34353190660476685, loss=3.0150139331817627
I0206 18:44:42.321810 139616073008896 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.362337589263916, loss=2.943918228149414
I0206 18:45:17.235463 139616064616192 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.366837739944458, loss=2.9429867267608643
I0206 18:45:52.138556 139616073008896 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3722664713859558, loss=2.963683843612671
I0206 18:46:27.019155 139616064616192 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.44781693816185, loss=2.963872194290161
I0206 18:47:01.902087 139616073008896 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.34892937541007996, loss=2.9114291667938232
I0206 18:47:36.764596 139616064616192 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.4011228680610657, loss=2.9840846061706543
I0206 18:48:11.636616 139616073008896 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.3514690101146698, loss=2.9582836627960205
I0206 18:48:46.494294 139616064616192 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3818007707595825, loss=2.896474838256836
I0206 18:49:21.369582 139616073008896 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.3600108325481415, loss=2.9206204414367676
I0206 18:49:56.240115 139616064616192 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3883204758167267, loss=2.9918580055236816
I0206 18:50:31.110398 139616073008896 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3611927628517151, loss=2.9365158081054688
I0206 18:51:05.973017 139616064616192 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.35138073563575745, loss=2.9524667263031006
I0206 18:51:40.854935 139616073008896 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.37970834970474243, loss=3.0021982192993164
I0206 18:52:15.722969 139616064616192 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.33164823055267334, loss=2.8614144325256348
I0206 18:52:50.611786 139616073008896 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.31774982810020447, loss=2.9067225456237793
I0206 18:53:25.472299 139616064616192 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.39097118377685547, loss=2.9372713565826416
I0206 18:53:41.927669 139785736898368 spec.py:321] Evaluating on the training split.
I0206 18:53:44.923570 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 18:57:02.712716 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 18:57:05.411670 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:00:04.303711 139785736898368 spec.py:349] Evaluating on the test split.
I0206 19:00:07.008708 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:02:39.357391 139785736898368 submission_runner.py:408] Time since start: 29046.60s, 	Step: 50549, 	{'train/accuracy': 0.6617670655250549, 'train/loss': 1.7062171697616577, 'train/bleu': 32.493355001995035, 'validation/accuracy': 0.6642819046974182, 'validation/loss': 1.6819127798080444, 'validation/bleu': 28.50163118714814, 'validation/num_examples': 3000, 'test/accuracy': 0.6748009920120239, 'test/loss': 1.6188348531723022, 'test/bleu': 28.20650007439196, 'test/num_examples': 3003, 'score': 17678.634654521942, 'total_duration': 29046.603624105453, 'accumulated_submission_time': 17678.634654521942, 'accumulated_eval_time': 11365.696782827377, 'accumulated_logging_time': 0.6260786056518555}
I0206 19:02:39.379766 139616073008896 logging_writer.py:48] [50549] accumulated_eval_time=11365.696783, accumulated_logging_time=0.626079, accumulated_submission_time=17678.634655, global_step=50549, preemption_count=0, score=17678.634655, test/accuracy=0.674801, test/bleu=28.206500, test/loss=1.618835, test/num_examples=3003, total_duration=29046.603624, train/accuracy=0.661767, train/bleu=32.493355, train/loss=1.706217, validation/accuracy=0.664282, validation/bleu=28.501631, validation/loss=1.681913, validation/num_examples=3000
I0206 19:02:57.527266 139616064616192 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3642473518848419, loss=2.933346748352051
I0206 19:03:32.400746 139616073008896 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.35464608669281006, loss=3.0122649669647217
I0206 19:04:07.295241 139616064616192 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3235163986682892, loss=2.8694071769714355
I0206 19:04:42.172981 139616073008896 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.41561487317085266, loss=2.938436508178711
I0206 19:05:17.060863 139616064616192 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3233962059020996, loss=2.921085834503174
I0206 19:05:51.936552 139616073008896 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.35556623339653015, loss=2.8938636779785156
I0206 19:06:26.776056 139616064616192 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3806115686893463, loss=2.9782629013061523
I0206 19:07:01.632926 139616073008896 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.41310495138168335, loss=2.95361590385437
I0206 19:07:36.456159 139616064616192 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3699995279312134, loss=2.988454818725586
I0206 19:08:11.309366 139616073008896 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3359096944332123, loss=2.8300795555114746
I0206 19:08:46.167020 139616064616192 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.350985586643219, loss=2.9575252532958984
I0206 19:09:21.010223 139616073008896 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.49801158905029297, loss=2.91001033782959
I0206 19:09:55.875747 139616064616192 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.32030364871025085, loss=2.9817559719085693
I0206 19:10:30.732813 139616073008896 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3658076226711273, loss=2.955239772796631
I0206 19:11:05.619518 139616064616192 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3497610092163086, loss=2.950932025909424
I0206 19:11:40.458281 139616073008896 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.4175615608692169, loss=2.96901535987854
I0206 19:12:15.325407 139616064616192 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3509804606437683, loss=2.8951220512390137
I0206 19:12:50.185610 139616073008896 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3607827425003052, loss=2.9799270629882812
I0206 19:13:25.041662 139616064616192 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.3239327371120453, loss=2.940218925476074
I0206 19:13:59.922072 139616073008896 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3439393639564514, loss=2.9039957523345947
I0206 19:14:34.779485 139616064616192 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.33878177404403687, loss=2.8933680057525635
I0206 19:15:09.681647 139616073008896 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3512991666793823, loss=2.9451370239257812
I0206 19:15:44.549817 139616064616192 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3194122314453125, loss=2.9290785789489746
I0206 19:16:19.423507 139616073008896 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3327159881591797, loss=2.897948980331421
I0206 19:16:39.364564 139785736898368 spec.py:321] Evaluating on the training split.
I0206 19:16:42.364803 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:19:29.707515 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 19:19:32.410189 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:22:07.392986 139785736898368 spec.py:349] Evaluating on the test split.
I0206 19:22:10.104998 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:24:34.440955 139785736898368 submission_runner.py:408] Time since start: 30361.69s, 	Step: 52959, 	{'train/accuracy': 0.6417939066886902, 'train/loss': 1.8330038785934448, 'train/bleu': 31.521758366440302, 'validation/accuracy': 0.6663029789924622, 'validation/loss': 1.6756658554077148, 'validation/bleu': 28.710437477264808, 'validation/num_examples': 3000, 'test/accuracy': 0.6778455972671509, 'test/loss': 1.6047146320343018, 'test/bleu': 28.40794194410856, 'test/num_examples': 3003, 'score': 18518.534535884857, 'total_duration': 30361.687136888504, 'accumulated_submission_time': 18518.534535884857, 'accumulated_eval_time': 11840.773048400879, 'accumulated_logging_time': 0.6584103107452393}
I0206 19:24:34.467501 139616064616192 logging_writer.py:48] [52959] accumulated_eval_time=11840.773048, accumulated_logging_time=0.658410, accumulated_submission_time=18518.534536, global_step=52959, preemption_count=0, score=18518.534536, test/accuracy=0.677846, test/bleu=28.407942, test/loss=1.604715, test/num_examples=3003, total_duration=30361.687137, train/accuracy=0.641794, train/bleu=31.521758, train/loss=1.833004, validation/accuracy=0.666303, validation/bleu=28.710437, validation/loss=1.675666, validation/num_examples=3000
I0206 19:24:49.164345 139616073008896 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.5674176812171936, loss=2.9506311416625977
I0206 19:25:24.142902 139616064616192 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3445877134799957, loss=2.921976327896118
I0206 19:25:59.062312 139616073008896 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3996105194091797, loss=2.9077610969543457
I0206 19:26:34.012018 139616064616192 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.3155156970024109, loss=2.871450901031494
I0206 19:27:08.879439 139616073008896 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3402068614959717, loss=2.8861842155456543
I0206 19:27:43.777344 139616064616192 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3379332423210144, loss=2.9159562587738037
I0206 19:28:18.689756 139616073008896 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.35171711444854736, loss=2.9204113483428955
I0206 19:28:53.537656 139616064616192 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3337496817111969, loss=2.8397767543792725
I0206 19:29:28.410860 139616073008896 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.324980765581131, loss=2.9097208976745605
I0206 19:30:03.280147 139616064616192 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.33340370655059814, loss=2.919100761413574
I0206 19:30:38.149536 139616073008896 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3564624786376953, loss=2.925952672958374
I0206 19:31:12.988029 139616064616192 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3436327576637268, loss=2.860100030899048
I0206 19:31:47.881551 139616073008896 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.38880598545074463, loss=2.9964053630828857
I0206 19:32:22.732134 139616064616192 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.36341342329978943, loss=2.842125654220581
I0206 19:32:57.584332 139616073008896 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3832360506057739, loss=2.8705477714538574
I0206 19:33:32.443466 139616064616192 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3320970833301544, loss=2.8778584003448486
I0206 19:34:07.300586 139616073008896 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3519774079322815, loss=2.9420006275177
I0206 19:34:42.164643 139616064616192 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3480818271636963, loss=2.8826584815979004
I0206 19:35:17.024054 139616073008896 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.36797887086868286, loss=2.897157669067383
I0206 19:35:51.928948 139616064616192 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.3419225215911865, loss=2.9038405418395996
I0206 19:36:26.803563 139616073008896 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3278372287750244, loss=2.8953845500946045
I0206 19:37:01.656092 139616064616192 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.36496391892433167, loss=2.8869030475616455
I0206 19:37:36.562135 139616073008896 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.329200804233551, loss=2.856856346130371
I0206 19:38:11.429899 139616064616192 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.37262335419654846, loss=2.969935655593872
I0206 19:38:34.521319 139785736898368 spec.py:321] Evaluating on the training split.
I0206 19:38:37.513449 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:41:33.021131 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 19:41:35.730968 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:44:15.336534 139785736898368 spec.py:349] Evaluating on the test split.
I0206 19:44:18.036988 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 19:46:55.778633 139785736898368 submission_runner.py:408] Time since start: 31703.02s, 	Step: 55368, 	{'train/accuracy': 0.6438504457473755, 'train/loss': 1.8326653242111206, 'train/bleu': 31.664923167422646, 'validation/accuracy': 0.664095938205719, 'validation/loss': 1.6701427698135376, 'validation/bleu': 28.634409204856926, 'validation/num_examples': 3000, 'test/accuracy': 0.6779385209083557, 'test/loss': 1.6016871929168701, 'test/bleu': 28.4380119668229, 'test/num_examples': 3003, 'score': 19358.49755549431, 'total_duration': 31703.02485537529, 'accumulated_submission_time': 19358.49755549431, 'accumulated_eval_time': 12342.030277490616, 'accumulated_logging_time': 0.6971557140350342}
I0206 19:46:55.801165 139616073008896 logging_writer.py:48] [55368] accumulated_eval_time=12342.030277, accumulated_logging_time=0.697156, accumulated_submission_time=19358.497555, global_step=55368, preemption_count=0, score=19358.497555, test/accuracy=0.677939, test/bleu=28.438012, test/loss=1.601687, test/num_examples=3003, total_duration=31703.024855, train/accuracy=0.643850, train/bleu=31.664923, train/loss=1.832665, validation/accuracy=0.664096, validation/bleu=28.634409, validation/loss=1.670143, validation/num_examples=3000
I0206 19:47:07.330400 139616064616192 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3774454593658447, loss=2.9397640228271484
I0206 19:47:42.230943 139616073008896 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.33950144052505493, loss=2.9335169792175293
I0206 19:48:17.139386 139616064616192 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.34290921688079834, loss=2.953941822052002
I0206 19:48:52.038008 139616073008896 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3463220000267029, loss=2.9544432163238525
I0206 19:49:26.971616 139616064616192 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3497806191444397, loss=2.935682535171509
I0206 19:50:01.899752 139616073008896 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.35593873262405396, loss=2.85756254196167
I0206 19:50:36.780888 139616064616192 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.32323211431503296, loss=2.8629696369171143
I0206 19:51:11.637586 139616073008896 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3397156298160553, loss=2.8467540740966797
I0206 19:51:46.515145 139616064616192 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3330462574958801, loss=2.9357478618621826
I0206 19:52:21.413841 139616073008896 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.4088541567325592, loss=2.910015344619751
I0206 19:52:56.356531 139616064616192 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.372151643037796, loss=2.938904047012329
I0206 19:53:31.306797 139616073008896 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.35120877623558044, loss=2.88010835647583
I0206 19:54:06.175719 139616064616192 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3356640338897705, loss=2.979275703430176
I0206 19:54:41.064051 139616073008896 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.35702961683273315, loss=2.8373513221740723
I0206 19:55:15.929071 139616064616192 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.36675116419792175, loss=2.968954086303711
I0206 19:55:50.789995 139616073008896 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.35461872816085815, loss=2.881596565246582
I0206 19:56:25.672682 139616064616192 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.47986695170402527, loss=2.928173303604126
I0206 19:57:00.568609 139616073008896 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.33787664771080017, loss=2.8949975967407227
I0206 19:57:35.441053 139616064616192 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.35569581389427185, loss=2.933137893676758
I0206 19:58:10.310681 139616073008896 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.339967280626297, loss=2.9896228313446045
I0206 19:58:45.205742 139616064616192 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.44231003522872925, loss=2.909467935562134
I0206 19:59:20.101393 139616073008896 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3469782769680023, loss=2.863172769546509
I0206 19:59:54.974448 139616064616192 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.36331889033317566, loss=2.8502252101898193
I0206 20:00:29.809511 139616073008896 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3326229453086853, loss=2.9119465351104736
I0206 20:00:56.017112 139785736898368 spec.py:321] Evaluating on the training split.
I0206 20:00:59.014621 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:03:51.100395 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 20:03:53.808258 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:06:24.171008 139785736898368 spec.py:349] Evaluating on the test split.
I0206 20:06:26.873674 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:08:52.762424 139785736898368 submission_runner.py:408] Time since start: 33020.01s, 	Step: 57777, 	{'train/accuracy': 0.6529799699783325, 'train/loss': 1.7605587244033813, 'train/bleu': 32.104956986509244, 'validation/accuracy': 0.6671584844589233, 'validation/loss': 1.6584866046905518, 'validation/bleu': 28.910553589606458, 'validation/num_examples': 3000, 'test/accuracy': 0.6783917546272278, 'test/loss': 1.5919768810272217, 'test/bleu': 28.49080923108632, 'test/num_examples': 3003, 'score': 20198.622362852097, 'total_duration': 33020.00865364075, 'accumulated_submission_time': 20198.622362852097, 'accumulated_eval_time': 12818.775514364243, 'accumulated_logging_time': 0.7295718193054199}
I0206 20:08:52.785256 139616064616192 logging_writer.py:48] [57777] accumulated_eval_time=12818.775514, accumulated_logging_time=0.729572, accumulated_submission_time=20198.622363, global_step=57777, preemption_count=0, score=20198.622363, test/accuracy=0.678392, test/bleu=28.490809, test/loss=1.591977, test/num_examples=3003, total_duration=33020.008654, train/accuracy=0.652980, train/bleu=32.104957, train/loss=1.760559, validation/accuracy=0.667158, validation/bleu=28.910554, validation/loss=1.658487, validation/num_examples=3000
I0206 20:09:01.176978 139616073008896 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.3628075122833252, loss=2.8903064727783203
I0206 20:09:36.081150 139616064616192 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3524983525276184, loss=2.874506711959839
I0206 20:10:10.982640 139616073008896 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3455049395561218, loss=2.860414981842041
I0206 20:10:45.857142 139616064616192 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.33186131715774536, loss=2.826775550842285
I0206 20:11:20.738537 139616073008896 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.33075693249702454, loss=2.853670835494995
I0206 20:11:55.615679 139616064616192 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3535804748535156, loss=2.9018776416778564
I0206 20:12:30.520899 139616073008896 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.33069905638694763, loss=2.865474224090576
I0206 20:13:05.445000 139616064616192 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.3157329261302948, loss=2.864030122756958
I0206 20:13:40.297770 139616073008896 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3084372580051422, loss=2.9028866291046143
I0206 20:14:15.169471 139616064616192 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.33064576983451843, loss=2.9168126583099365
I0206 20:14:50.050990 139616073008896 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3675271272659302, loss=2.9229164123535156
I0206 20:15:24.923724 139616064616192 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3753418028354645, loss=2.923224687576294
I0206 20:15:59.772914 139616073008896 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.36780300736427307, loss=2.896339178085327
I0206 20:16:34.664685 139616064616192 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.32871559262275696, loss=2.9141130447387695
I0206 20:17:09.507953 139616073008896 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.35650578141212463, loss=2.9072835445404053
I0206 20:17:44.374697 139616064616192 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.33359333872795105, loss=2.9080545902252197
I0206 20:18:19.255084 139616073008896 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.33958572149276733, loss=2.9232985973358154
I0206 20:18:54.112973 139616064616192 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3722275495529175, loss=2.895827054977417
I0206 20:19:28.954925 139616073008896 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.34782809019088745, loss=2.880507707595825
I0206 20:20:03.828862 139616064616192 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.32622745633125305, loss=2.8501265048980713
I0206 20:20:38.735835 139616073008896 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.36149629950523376, loss=2.8890137672424316
I0206 20:21:13.598895 139616064616192 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3628600239753723, loss=2.7990944385528564
I0206 20:21:48.459139 139616073008896 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.31841611862182617, loss=2.8613526821136475
I0206 20:22:23.317460 139616064616192 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.34565508365631104, loss=2.9143831729888916
I0206 20:22:53.028166 139785736898368 spec.py:321] Evaluating on the training split.
I0206 20:22:56.036983 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:25:48.927903 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 20:25:51.656933 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:28:24.361573 139785736898368 spec.py:349] Evaluating on the test split.
I0206 20:28:27.074304 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:30:48.998251 139785736898368 submission_runner.py:408] Time since start: 34336.24s, 	Step: 60187, 	{'train/accuracy': 0.6493332982063293, 'train/loss': 1.788055658340454, 'train/bleu': 31.857854688348205, 'validation/accuracy': 0.6688447594642639, 'validation/loss': 1.6495282649993896, 'validation/bleu': 28.90816655813359, 'validation/num_examples': 3000, 'test/accuracy': 0.6815757751464844, 'test/loss': 1.579599380493164, 'test/bleu': 28.605296024995553, 'test/num_examples': 3003, 'score': 21038.775871276855, 'total_duration': 34336.24448752403, 'accumulated_submission_time': 21038.775871276855, 'accumulated_eval_time': 13294.7455265522, 'accumulated_logging_time': 0.7640244960784912}
I0206 20:30:49.021541 139616073008896 logging_writer.py:48] [60187] accumulated_eval_time=13294.745527, accumulated_logging_time=0.764024, accumulated_submission_time=21038.775871, global_step=60187, preemption_count=0, score=21038.775871, test/accuracy=0.681576, test/bleu=28.605296, test/loss=1.579599, test/num_examples=3003, total_duration=34336.244488, train/accuracy=0.649333, train/bleu=31.857855, train/loss=1.788056, validation/accuracy=0.668845, validation/bleu=28.908167, validation/loss=1.649528, validation/num_examples=3000
I0206 20:30:53.915800 139616064616192 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3219956159591675, loss=2.929220676422119
I0206 20:31:28.763900 139616073008896 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3876437544822693, loss=2.9036262035369873
I0206 20:32:03.648377 139616064616192 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.3520466089248657, loss=2.8676929473876953
I0206 20:32:38.522889 139616073008896 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.33186718821525574, loss=2.852599859237671
I0206 20:33:13.385900 139616064616192 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.35788649320602417, loss=2.9030537605285645
I0206 20:33:48.262068 139616073008896 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.33058854937553406, loss=2.8569886684417725
I0206 20:34:23.160934 139616064616192 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.353162556886673, loss=2.9466238021850586
I0206 20:34:58.020493 139616073008896 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.36056089401245117, loss=2.9877851009368896
I0206 20:35:32.899582 139616064616192 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.37561437487602234, loss=2.9181292057037354
I0206 20:36:07.764076 139616073008896 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.43908461928367615, loss=2.8786439895629883
I0206 20:36:42.626208 139616064616192 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3601777255535126, loss=2.8878111839294434
I0206 20:37:17.487294 139616073008896 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.38025569915771484, loss=2.8831875324249268
I0206 20:37:52.349385 139616064616192 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.3543418347835541, loss=2.8926477432250977
I0206 20:38:27.212935 139616073008896 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.33153125643730164, loss=2.8767247200012207
I0206 20:39:02.062671 139616064616192 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.35447466373443604, loss=2.828091621398926
I0206 20:39:36.950956 139616073008896 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3585226833820343, loss=2.8710336685180664
I0206 20:40:11.831604 139616064616192 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.36013638973236084, loss=2.921900510787964
I0206 20:40:46.671647 139616073008896 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.33814865350723267, loss=2.85018253326416
I0206 20:41:21.549116 139616064616192 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.350838303565979, loss=2.9868032932281494
I0206 20:41:56.432562 139616073008896 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.36501559615135193, loss=2.816427707672119
I0206 20:42:31.301162 139616064616192 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3381343483924866, loss=2.886176347732544
I0206 20:43:06.172409 139616073008896 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.38900431990623474, loss=2.8833322525024414
I0206 20:43:41.043804 139616064616192 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3611662983894348, loss=2.866753101348877
I0206 20:44:15.924796 139616073008896 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.35519203543663025, loss=2.9193358421325684
I0206 20:44:49.109692 139785736898368 spec.py:321] Evaluating on the training split.
I0206 20:44:52.102156 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:47:37.623423 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 20:47:40.341161 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:50:13.105313 139785736898368 spec.py:349] Evaluating on the test split.
I0206 20:50:15.842684 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 20:52:31.700932 139785736898368 submission_runner.py:408] Time since start: 35638.95s, 	Step: 62597, 	{'train/accuracy': 0.6954829692840576, 'train/loss': 1.5248199701309204, 'train/bleu': 35.48335512968942, 'validation/accuracy': 0.6700350642204285, 'validation/loss': 1.6441853046417236, 'validation/bleu': 29.231370890674135, 'validation/num_examples': 3000, 'test/accuracy': 0.6829004883766174, 'test/loss': 1.5770306587219238, 'test/bleu': 28.931010286915832, 'test/num_examples': 3003, 'score': 21878.77816271782, 'total_duration': 35638.94716525078, 'accumulated_submission_time': 21878.77816271782, 'accumulated_eval_time': 13757.336695432663, 'accumulated_logging_time': 0.7973370552062988}
I0206 20:52:31.723979 139616064616192 logging_writer.py:48] [62597] accumulated_eval_time=13757.336695, accumulated_logging_time=0.797337, accumulated_submission_time=21878.778163, global_step=62597, preemption_count=0, score=21878.778163, test/accuracy=0.682900, test/bleu=28.931010, test/loss=1.577031, test/num_examples=3003, total_duration=35638.947165, train/accuracy=0.695483, train/bleu=35.483355, train/loss=1.524820, validation/accuracy=0.670035, validation/bleu=29.231371, validation/loss=1.644185, validation/num_examples=3000
I0206 20:52:33.138606 139616073008896 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3808212876319885, loss=2.892209053039551
I0206 20:53:08.026732 139616064616192 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3427107036113739, loss=2.9100754261016846
I0206 20:53:42.929613 139616073008896 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.3468133509159088, loss=2.8487515449523926
I0206 20:54:17.816689 139616064616192 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.37804704904556274, loss=2.8720645904541016
I0206 20:54:52.687515 139616073008896 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.35299885272979736, loss=2.914147138595581
I0206 20:55:27.579872 139616064616192 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.35239794850349426, loss=2.83613920211792
I0206 20:56:02.467071 139616073008896 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.33944132924079895, loss=2.800469398498535
I0206 20:56:37.318260 139616064616192 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.359870582818985, loss=2.8736562728881836
I0206 20:57:12.201851 139616073008896 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3986419439315796, loss=2.916271924972534
I0206 20:57:47.078364 139616064616192 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3482057452201843, loss=2.930302381515503
I0206 20:58:21.962374 139616073008896 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.372527152299881, loss=2.9259560108184814
I0206 20:58:56.845621 139616064616192 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.38739505410194397, loss=2.915677785873413
I0206 20:59:31.688118 139616073008896 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3501080870628357, loss=2.911756992340088
I0206 21:00:06.574345 139616064616192 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.348766028881073, loss=2.8906850814819336
I0206 21:00:41.429670 139616073008896 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.34742245078086853, loss=2.9052116870880127
I0206 21:01:16.300517 139616064616192 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3380297124385834, loss=2.7817091941833496
I0206 21:01:51.178607 139616073008896 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.32904499769210815, loss=2.9634740352630615
I0206 21:02:26.042890 139616064616192 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.33026593923568726, loss=2.9023244380950928
I0206 21:03:00.906947 139616073008896 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.346377432346344, loss=2.9943692684173584
I0206 21:03:35.779908 139616064616192 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.35822466015815735, loss=2.929994821548462
I0206 21:04:10.635082 139616073008896 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3637443780899048, loss=2.905930519104004
I0206 21:04:45.525298 139616064616192 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3353864550590515, loss=2.8165152072906494
I0206 21:05:20.393263 139616073008896 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.34644588828086853, loss=2.9019248485565186
I0206 21:05:55.263843 139616064616192 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3370749056339264, loss=2.9036006927490234
I0206 21:06:30.150353 139616073008896 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3712036609649658, loss=2.88885498046875
I0206 21:06:31.967429 139785736898368 spec.py:321] Evaluating on the training split.
I0206 21:06:34.959400 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:09:45.131903 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 21:09:47.859380 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:12:19.706798 139785736898368 spec.py:349] Evaluating on the test split.
I0206 21:12:22.414877 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:14:40.228776 139785736898368 submission_runner.py:408] Time since start: 36967.48s, 	Step: 65007, 	{'train/accuracy': 0.6498906016349792, 'train/loss': 1.779046893119812, 'train/bleu': 32.04606986938242, 'validation/accuracy': 0.6694399118423462, 'validation/loss': 1.6440907716751099, 'validation/bleu': 28.851999438402675, 'validation/num_examples': 3000, 'test/accuracy': 0.6821103096008301, 'test/loss': 1.5699652433395386, 'test/bleu': 28.572309209482075, 'test/num_examples': 3003, 'score': 22718.93424129486, 'total_duration': 36967.4750084877, 'accumulated_submission_time': 22718.93424129486, 'accumulated_eval_time': 14245.597965955734, 'accumulated_logging_time': 0.8305079936981201}
I0206 21:14:40.256887 139616064616192 logging_writer.py:48] [65007] accumulated_eval_time=14245.597966, accumulated_logging_time=0.830508, accumulated_submission_time=22718.934241, global_step=65007, preemption_count=0, score=22718.934241, test/accuracy=0.682110, test/bleu=28.572309, test/loss=1.569965, test/num_examples=3003, total_duration=36967.475008, train/accuracy=0.649891, train/bleu=32.046070, train/loss=1.779047, validation/accuracy=0.669440, validation/bleu=28.851999, validation/loss=1.644091, validation/num_examples=3000
I0206 21:15:13.111615 139616073008896 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3825615346431732, loss=2.863361358642578
I0206 21:15:48.003729 139616064616192 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3414285182952881, loss=2.890874147415161
I0206 21:16:22.897904 139616073008896 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.356082558631897, loss=2.7689437866210938
I0206 21:16:57.774992 139616064616192 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3617512881755829, loss=2.959937810897827
I0206 21:17:32.691675 139616073008896 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3705330491065979, loss=2.9084386825561523
I0206 21:18:07.586190 139616064616192 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.39040106534957886, loss=2.9169328212738037
I0206 21:18:42.425083 139616073008896 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.4170205295085907, loss=2.8080549240112305
I0206 21:19:17.313414 139616064616192 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.33047088980674744, loss=2.8508083820343018
I0206 21:19:52.186332 139616073008896 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3386315703392029, loss=2.889853000640869
I0206 21:20:27.085847 139616064616192 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.37140321731567383, loss=2.835404872894287
I0206 21:21:02.069998 139616073008896 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3768657445907593, loss=2.8550312519073486
I0206 21:21:37.014931 139616064616192 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.342673122882843, loss=2.9031059741973877
I0206 21:22:11.863430 139616073008896 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.35469669103622437, loss=2.841550827026367
I0206 21:22:46.753066 139616064616192 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3484170138835907, loss=2.829192638397217
I0206 21:23:21.667573 139616073008896 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3628636300563812, loss=2.886105537414551
I0206 21:23:56.526808 139616064616192 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.35104408860206604, loss=2.922039747238159
I0206 21:24:31.417894 139616073008896 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3486001491546631, loss=2.8768064975738525
I0206 21:25:06.282505 139616064616192 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.38640978932380676, loss=2.9279568195343018
I0206 21:25:41.125110 139616073008896 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3450567424297333, loss=2.7992241382598877
I0206 21:26:16.076710 139616064616192 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.4040628671646118, loss=2.909543514251709
I0206 21:26:50.944112 139616073008896 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.35759803652763367, loss=2.9148504734039307
I0206 21:27:25.787347 139616064616192 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.3451252281665802, loss=2.8467485904693604
I0206 21:28:00.663244 139616073008896 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.35844531655311584, loss=2.833888053894043
I0206 21:28:35.522404 139616064616192 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.35145819187164307, loss=2.871886968612671
I0206 21:28:40.480205 139785736898368 spec.py:321] Evaluating on the training split.
I0206 21:28:43.483153 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:31:26.504086 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 21:31:29.201097 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:34:04.351369 139785736898368 spec.py:349] Evaluating on the test split.
I0206 21:34:07.054303 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:36:29.580373 139785736898368 submission_runner.py:408] Time since start: 38276.83s, 	Step: 67416, 	{'train/accuracy': 0.6513877511024475, 'train/loss': 1.778394341468811, 'train/bleu': 31.90224548614695, 'validation/accuracy': 0.6698490977287292, 'validation/loss': 1.6329776048660278, 'validation/bleu': 29.310274944733973, 'validation/num_examples': 3000, 'test/accuracy': 0.6850618720054626, 'test/loss': 1.5574126243591309, 'test/bleu': 28.808396256578643, 'test/num_examples': 3003, 'score': 23559.06813430786, 'total_duration': 38276.82657814026, 'accumulated_submission_time': 23559.06813430786, 'accumulated_eval_time': 14714.698032855988, 'accumulated_logging_time': 0.8699114322662354}
I0206 21:36:29.608747 139616073008896 logging_writer.py:48] [67416] accumulated_eval_time=14714.698033, accumulated_logging_time=0.869911, accumulated_submission_time=23559.068134, global_step=67416, preemption_count=0, score=23559.068134, test/accuracy=0.685062, test/bleu=28.808396, test/loss=1.557413, test/num_examples=3003, total_duration=38276.826578, train/accuracy=0.651388, train/bleu=31.902245, train/loss=1.778394, validation/accuracy=0.669849, validation/bleu=29.310275, validation/loss=1.632978, validation/num_examples=3000
I0206 21:36:59.294493 139616064616192 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.34527990221977234, loss=2.8366663455963135
I0206 21:37:34.194153 139616073008896 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3652927577495575, loss=2.85086989402771
I0206 21:38:09.112714 139616064616192 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.36074668169021606, loss=2.8320229053497314
I0206 21:38:44.016817 139616073008896 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3328477442264557, loss=2.871246576309204
I0206 21:39:18.909251 139616064616192 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3525093197822571, loss=2.9305503368377686
I0206 21:39:53.859521 139616073008896 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3382725715637207, loss=2.7727108001708984
I0206 21:40:28.768242 139616064616192 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.4207243025302887, loss=2.9022576808929443
I0206 21:41:03.700151 139616073008896 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.3627024292945862, loss=2.9878275394439697
I0206 21:41:38.583642 139616064616192 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.36333978176116943, loss=2.8567018508911133
I0206 21:42:13.449954 139616073008896 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.35787034034729004, loss=2.818819046020508
I0206 21:42:48.316390 139616064616192 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3193950355052948, loss=2.8596689701080322
I0206 21:43:23.209159 139616073008896 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.3501823842525482, loss=2.8695833683013916
I0206 21:43:58.081541 139616064616192 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.36385905742645264, loss=2.861637592315674
I0206 21:44:32.981834 139616073008896 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3609200119972229, loss=2.957191228866577
I0206 21:45:07.866677 139616064616192 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3382094204425812, loss=2.8464744091033936
I0206 21:45:42.735949 139616073008896 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.40124136209487915, loss=2.81691312789917
I0206 21:46:17.604469 139616064616192 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.39914068579673767, loss=2.915350914001465
I0206 21:46:52.463550 139616073008896 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.3811650574207306, loss=2.8601632118225098
I0206 21:47:27.327207 139616064616192 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.33977577090263367, loss=2.840681552886963
I0206 21:48:02.185793 139616073008896 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3534544110298157, loss=2.8690052032470703
I0206 21:48:37.051735 139616064616192 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.36619704961776733, loss=2.886444330215454
I0206 21:49:11.911651 139616073008896 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.37749582529067993, loss=2.8138513565063477
I0206 21:49:46.810374 139616064616192 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.370026558637619, loss=2.9521641731262207
I0206 21:50:21.709723 139616073008896 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.348147988319397, loss=2.8205106258392334
I0206 21:50:29.794743 139785736898368 spec.py:321] Evaluating on the training split.
I0206 21:50:32.791039 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:53:15.569994 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 21:53:18.293959 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:56:06.284062 139785736898368 spec.py:349] Evaluating on the test split.
I0206 21:56:09.021459 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 21:58:52.436923 139785736898368 submission_runner.py:408] Time since start: 39619.68s, 	Step: 69825, 	{'train/accuracy': 0.6634706258773804, 'train/loss': 1.7029547691345215, 'train/bleu': 32.673334074985306, 'validation/accuracy': 0.6726140975952148, 'validation/loss': 1.6234630346298218, 'validation/bleu': 29.223958515493404, 'validation/num_examples': 3000, 'test/accuracy': 0.6853756308555603, 'test/loss': 1.5609307289123535, 'test/bleu': 28.84675359208095, 'test/num_examples': 3003, 'score': 24399.16579413414, 'total_duration': 39619.68317270279, 'accumulated_submission_time': 24399.16579413414, 'accumulated_eval_time': 15217.340163707733, 'accumulated_logging_time': 0.9094698429107666}
I0206 21:58:52.461185 139616064616192 logging_writer.py:48] [69825] accumulated_eval_time=15217.340164, accumulated_logging_time=0.909470, accumulated_submission_time=24399.165794, global_step=69825, preemption_count=0, score=24399.165794, test/accuracy=0.685376, test/bleu=28.846754, test/loss=1.560931, test/num_examples=3003, total_duration=39619.683173, train/accuracy=0.663471, train/bleu=32.673334, train/loss=1.702955, validation/accuracy=0.672614, validation/bleu=29.223959, validation/loss=1.623463, validation/num_examples=3000
I0206 21:59:18.962842 139616073008896 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.39345496892929077, loss=2.892256259918213
I0206 21:59:53.903827 139616064616192 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3526696562767029, loss=2.7878828048706055
I0206 22:00:28.777242 139616073008896 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3374382257461548, loss=2.823995351791382
I0206 22:01:03.679887 139616064616192 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.3992449939250946, loss=2.83788800239563
I0206 22:01:38.530034 139616073008896 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.35832491517066956, loss=2.8242547512054443
I0206 22:02:13.452078 139616064616192 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.39556676149368286, loss=2.8112058639526367
I0206 22:02:48.375128 139616073008896 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3976518511772156, loss=2.924849271774292
I0206 22:03:23.299266 139616064616192 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.36739009618759155, loss=2.8151321411132812
I0206 22:03:58.172311 139616073008896 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.43280336260795593, loss=2.8282735347747803
I0206 22:04:33.045821 139616064616192 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.38487741351127625, loss=2.85737681388855
I0206 22:05:07.906504 139616073008896 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.38781705498695374, loss=2.8483197689056396
I0206 22:05:42.808800 139616064616192 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.35442274808883667, loss=2.832852840423584
I0206 22:06:17.675148 139616073008896 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.36027762293815613, loss=2.845985174179077
I0206 22:06:52.550045 139616064616192 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.357619047164917, loss=2.863271713256836
I0206 22:07:27.481799 139616073008896 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.35072559118270874, loss=2.821035861968994
I0206 22:08:02.377582 139616064616192 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3813113272190094, loss=2.873797655105591
I0206 22:08:37.235687 139616073008896 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.35894933342933655, loss=2.88712215423584
I0206 22:09:12.079099 139616064616192 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.3786461353302002, loss=2.8438937664031982
I0206 22:09:46.945576 139616073008896 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.4211295247077942, loss=2.8262887001037598
I0206 22:10:21.802820 139616064616192 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.34386420249938965, loss=2.847651958465576
I0206 22:10:56.675539 139616073008896 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3547615706920624, loss=2.7487399578094482
I0206 22:11:31.543602 139616064616192 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.35589152574539185, loss=2.8957552909851074
I0206 22:12:06.424830 139616073008896 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.35864564776420593, loss=2.836566209793091
I0206 22:12:41.289533 139616064616192 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3661038279533386, loss=2.8804829120635986
I0206 22:12:52.507462 139785736898368 spec.py:321] Evaluating on the training split.
I0206 22:12:55.501897 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 22:15:59.617338 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 22:16:02.343099 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 22:18:40.157844 139785736898368 spec.py:349] Evaluating on the test split.
I0206 22:18:42.875356 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 22:21:21.793602 139785736898368 submission_runner.py:408] Time since start: 40969.04s, 	Step: 72234, 	{'train/accuracy': 0.6572171449661255, 'train/loss': 1.744814395904541, 'train/bleu': 32.34493762341491, 'validation/accuracy': 0.6746351718902588, 'validation/loss': 1.619512677192688, 'validation/bleu': 29.395188395213626, 'validation/num_examples': 3000, 'test/accuracy': 0.6883621215820312, 'test/loss': 1.5452096462249756, 'test/bleu': 29.265421368484017, 'test/num_examples': 3003, 'score': 25239.123465538025, 'total_duration': 40969.03985142708, 'accumulated_submission_time': 25239.123465538025, 'accumulated_eval_time': 15726.62624669075, 'accumulated_logging_time': 0.9434311389923096}
I0206 22:21:21.817806 139616073008896 logging_writer.py:48] [72234] accumulated_eval_time=15726.626247, accumulated_logging_time=0.943431, accumulated_submission_time=25239.123466, global_step=72234, preemption_count=0, score=25239.123466, test/accuracy=0.688362, test/bleu=29.265421, test/loss=1.545210, test/num_examples=3003, total_duration=40969.039851, train/accuracy=0.657217, train/bleu=32.344938, train/loss=1.744814, validation/accuracy=0.674635, validation/bleu=29.395188, validation/loss=1.619513, validation/num_examples=3000
I0206 22:21:45.220605 139616064616192 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.336812824010849, loss=2.801182508468628
I0206 22:22:20.148813 139616073008896 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.4220295548439026, loss=2.833994150161743
I0206 22:22:55.040612 139616064616192 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.4190523326396942, loss=2.839289903640747
I0206 22:23:29.934585 139616073008896 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3758108615875244, loss=2.782247543334961
I0206 22:24:04.843367 139616064616192 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.35820475220680237, loss=2.8465163707733154
I0206 22:24:39.722136 139616073008896 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3585260808467865, loss=2.872096061706543
I0206 22:25:14.588361 139616064616192 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.37061038613319397, loss=2.8581652641296387
I0206 22:25:49.427781 139616073008896 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.35818174481391907, loss=2.869854688644409
I0206 22:26:24.318039 139616064616192 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.36884236335754395, loss=2.815385341644287
I0206 22:26:59.194159 139616073008896 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3368568420410156, loss=2.846829891204834
I0206 22:27:34.050353 139616064616192 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.3355799913406372, loss=2.8406600952148438
I0206 22:28:08.898380 139616073008896 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.36326882243156433, loss=2.884490728378296
I0206 22:28:43.783046 139616064616192 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3460231423377991, loss=2.798753499984741
I0206 22:29:18.625883 139616073008896 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.36721867322921753, loss=2.7881410121917725
I0206 22:29:53.480515 139616064616192 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.34152498841285706, loss=2.8663878440856934
I0206 22:30:28.364012 139616073008896 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.36540159583091736, loss=2.8836536407470703
I0206 22:31:03.227530 139616064616192 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3608614504337311, loss=2.8566672801971436
I0206 22:31:38.069196 139616073008896 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3822784125804901, loss=2.8811328411102295
I0206 22:32:12.928407 139616064616192 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3663564622402191, loss=2.841200828552246
I0206 22:32:47.794390 139616073008896 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.34356793761253357, loss=2.8414244651794434
I0206 22:33:22.693779 139616064616192 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3436419665813446, loss=2.853365659713745
I0206 22:33:57.555582 139616073008896 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3671325147151947, loss=2.829522132873535
I0206 22:34:32.439761 139616064616192 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.31843310594558716, loss=2.8353631496429443
I0206 22:35:07.312937 139616073008896 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.36899110674858093, loss=2.772214412689209
I0206 22:35:22.046008 139785736898368 spec.py:321] Evaluating on the training split.
I0206 22:35:25.062980 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 22:38:15.249049 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 22:38:17.960736 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 22:40:50.378356 139785736898368 spec.py:349] Evaluating on the test split.
I0206 22:40:53.101484 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 22:43:24.894821 139785736898368 submission_runner.py:408] Time since start: 42292.14s, 	Step: 74644, 	{'train/accuracy': 0.6548944115638733, 'train/loss': 1.7606871128082275, 'train/bleu': 32.62063170484274, 'validation/accuracy': 0.6750690937042236, 'validation/loss': 1.6131248474121094, 'validation/bleu': 29.763167120385386, 'validation/num_examples': 3000, 'test/accuracy': 0.6876532435417175, 'test/loss': 1.5402637720108032, 'test/bleu': 29.1380813702546, 'test/num_examples': 3003, 'score': 26079.265601873398, 'total_duration': 42292.14107131958, 'accumulated_submission_time': 26079.265601873398, 'accumulated_eval_time': 16209.475015640259, 'accumulated_logging_time': 0.977283239364624}
I0206 22:43:24.920574 139616064616192 logging_writer.py:48] [74644] accumulated_eval_time=16209.475016, accumulated_logging_time=0.977283, accumulated_submission_time=26079.265602, global_step=74644, preemption_count=0, score=26079.265602, test/accuracy=0.687653, test/bleu=29.138081, test/loss=1.540264, test/num_examples=3003, total_duration=42292.141071, train/accuracy=0.654894, train/bleu=32.620632, train/loss=1.760687, validation/accuracy=0.675069, validation/bleu=29.763167, validation/loss=1.613125, validation/num_examples=3000
I0206 22:43:44.798222 139616073008896 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.36600908637046814, loss=2.8499491214752197
I0206 22:44:19.702101 139616064616192 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.37721696496009827, loss=2.886861562728882
I0206 22:44:54.579063 139616073008896 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3371489346027374, loss=2.7862210273742676
I0206 22:45:29.484711 139616064616192 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3749493360519409, loss=2.875731945037842
I0206 22:46:04.381833 139616073008896 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.35299932956695557, loss=2.8188157081604004
I0206 22:46:39.265939 139616064616192 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3629058301448822, loss=2.812222957611084
I0206 22:47:14.117176 139616073008896 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.36390215158462524, loss=2.831578254699707
I0206 22:47:48.989147 139616064616192 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.35436442494392395, loss=2.807307004928589
I0206 22:48:23.871006 139616073008896 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.35421139001846313, loss=2.793719530105591
I0206 22:48:58.784704 139616064616192 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.37523192167282104, loss=2.850525140762329
I0206 22:49:33.663566 139616073008896 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3352970480918884, loss=2.7829601764678955
I0206 22:50:08.577170 139616064616192 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.35170668363571167, loss=2.8352036476135254
I0206 22:50:43.452709 139616073008896 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3771897852420807, loss=2.922260284423828
I0206 22:51:18.335788 139616064616192 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.38247227668762207, loss=2.883687734603882
I0206 22:51:53.204316 139616073008896 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3689623475074768, loss=2.7644662857055664
I0206 22:52:28.081041 139616064616192 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.3475087285041809, loss=2.829834461212158
I0206 22:53:02.992943 139616073008896 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3697448968887329, loss=2.8653948307037354
I0206 22:53:37.858148 139616064616192 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.419929563999176, loss=2.9242546558380127
I0206 22:54:12.709548 139616073008896 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.36517345905303955, loss=2.8093416690826416
I0206 22:54:47.583398 139616064616192 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.36504602432250977, loss=2.8235466480255127
I0206 22:55:22.444785 139616073008896 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3539339601993561, loss=2.802419424057007
I0206 22:55:57.329740 139616064616192 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.35594484210014343, loss=2.845331907272339
I0206 22:56:32.217044 139616073008896 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.38340598344802856, loss=2.9024481773376465
I0206 22:57:07.139663 139616064616192 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.36935359239578247, loss=2.80184006690979
I0206 22:57:25.029037 139785736898368 spec.py:321] Evaluating on the training split.
I0206 22:57:28.033407 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:01:12.602584 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 23:01:15.303362 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:03:49.009860 139785736898368 spec.py:349] Evaluating on the test split.
I0206 23:03:51.733300 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:06:20.732820 139785736898368 submission_runner.py:408] Time since start: 43667.98s, 	Step: 77053, 	{'train/accuracy': 0.6639668941497803, 'train/loss': 1.694888710975647, 'train/bleu': 32.92362728757387, 'validation/accuracy': 0.6748707294464111, 'validation/loss': 1.6068533658981323, 'validation/bleu': 29.34807239685257, 'validation/num_examples': 3000, 'test/accuracy': 0.6893498301506042, 'test/loss': 1.530578374862671, 'test/bleu': 29.18517059563849, 'test/num_examples': 3003, 'score': 26919.287441253662, 'total_duration': 43667.97906756401, 'accumulated_submission_time': 26919.287441253662, 'accumulated_eval_time': 16745.17874765396, 'accumulated_logging_time': 1.0129663944244385}
I0206 23:06:20.758460 139616073008896 logging_writer.py:48] [77053] accumulated_eval_time=16745.178748, accumulated_logging_time=1.012966, accumulated_submission_time=26919.287441, global_step=77053, preemption_count=0, score=26919.287441, test/accuracy=0.689350, test/bleu=29.185171, test/loss=1.530578, test/num_examples=3003, total_duration=43667.979068, train/accuracy=0.663967, train/bleu=32.923627, train/loss=1.694889, validation/accuracy=0.674871, validation/bleu=29.348072, validation/loss=1.606853, validation/num_examples=3000
I0206 23:06:37.512187 139616064616192 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.35105085372924805, loss=2.842609405517578
I0206 23:07:12.407752 139616073008896 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.34897032380104065, loss=2.836819887161255
I0206 23:07:47.310842 139616064616192 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.37605056166648865, loss=2.8332183361053467
I0206 23:08:22.214421 139616073008896 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.382592111825943, loss=2.8509974479675293
I0206 23:08:57.122135 139616064616192 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.3433780074119568, loss=2.800462245941162
I0206 23:09:32.015635 139616073008896 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.3817530572414398, loss=2.761065721511841
I0206 23:10:06.909455 139616064616192 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3778840899467468, loss=2.800729990005493
I0206 23:10:41.803785 139616073008896 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3890817165374756, loss=2.833573341369629
I0206 23:11:16.700390 139616064616192 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.39540985226631165, loss=2.9430055618286133
I0206 23:11:51.551406 139616073008896 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.39043015241622925, loss=2.877413749694824
I0206 23:12:26.443324 139616064616192 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.38449886441230774, loss=2.8080949783325195
I0206 23:13:01.369831 139616073008896 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.40845465660095215, loss=2.8372650146484375
I0206 23:13:36.271136 139616064616192 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3730181157588959, loss=2.853076457977295
I0206 23:14:11.137619 139616073008896 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.35760802030563354, loss=2.839683771133423
I0206 23:14:46.054249 139616064616192 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.42954638600349426, loss=2.8842451572418213
I0206 23:15:20.904054 139616073008896 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3864542543888092, loss=2.878953218460083
I0206 23:15:55.787226 139616064616192 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.37864717841148376, loss=2.8406224250793457
I0206 23:16:30.660666 139616073008896 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3567320704460144, loss=2.830105781555176
I0206 23:17:05.529982 139616064616192 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3477202355861664, loss=2.815694808959961
I0206 23:17:40.403837 139616073008896 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.39380648732185364, loss=2.829526662826538
I0206 23:18:15.271859 139616064616192 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.4224981367588043, loss=2.8217403888702393
I0206 23:18:50.134471 139616073008896 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.37463364005088806, loss=2.791663408279419
I0206 23:19:25.015788 139616064616192 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.35929498076438904, loss=2.7981741428375244
I0206 23:19:59.899261 139616073008896 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3746367394924164, loss=2.7895562648773193
I0206 23:20:20.901840 139785736898368 spec.py:321] Evaluating on the training split.
I0206 23:20:23.901588 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:23:34.409044 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 23:23:37.127186 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:26:00.318337 139785736898368 spec.py:349] Evaluating on the test split.
I0206 23:26:03.032400 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:28:24.370655 139785736898368 submission_runner.py:408] Time since start: 44991.62s, 	Step: 79462, 	{'train/accuracy': 0.6601611971855164, 'train/loss': 1.7258120775222778, 'train/bleu': 32.68691853658534, 'validation/accuracy': 0.6769289970397949, 'validation/loss': 1.5998398065567017, 'validation/bleu': 29.752262409047173, 'validation/num_examples': 3000, 'test/accuracy': 0.6904653906822205, 'test/loss': 1.5268282890319824, 'test/bleu': 29.331326809471992, 'test/num_examples': 3003, 'score': 27759.34330010414, 'total_duration': 44991.616892814636, 'accumulated_submission_time': 27759.34330010414, 'accumulated_eval_time': 17228.647493600845, 'accumulated_logging_time': 1.0498454570770264}
I0206 23:28:24.396801 139616064616192 logging_writer.py:48] [79462] accumulated_eval_time=17228.647494, accumulated_logging_time=1.049845, accumulated_submission_time=27759.343300, global_step=79462, preemption_count=0, score=27759.343300, test/accuracy=0.690465, test/bleu=29.331327, test/loss=1.526828, test/num_examples=3003, total_duration=44991.616893, train/accuracy=0.660161, train/bleu=32.686919, train/loss=1.725812, validation/accuracy=0.676929, validation/bleu=29.752262, validation/loss=1.599840, validation/num_examples=3000
I0206 23:28:38.016444 139616073008896 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.4044724702835083, loss=2.80965518951416
I0206 23:29:12.914647 139616064616192 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.578776478767395, loss=2.822322130203247
I0206 23:29:47.848987 139616073008896 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3862139880657196, loss=2.896507978439331
I0206 23:30:22.761698 139616064616192 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.38861361145973206, loss=2.8940024375915527
I0206 23:30:57.663991 139616073008896 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.36544787883758545, loss=2.8131628036499023
I0206 23:31:32.570392 139616064616192 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3806309700012207, loss=2.783596992492676
I0206 23:32:07.444460 139616073008896 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3825410306453705, loss=2.900852918624878
I0206 23:32:42.327048 139616064616192 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.37278124690055847, loss=2.803611993789673
I0206 23:33:17.204081 139616073008896 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.36835208535194397, loss=2.873239278793335
I0206 23:33:52.138077 139616064616192 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3885895907878876, loss=2.8231592178344727
I0206 23:34:27.087981 139616073008896 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3767719566822052, loss=2.83620023727417
I0206 23:35:02.036676 139616064616192 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3559899926185608, loss=2.8175008296966553
I0206 23:35:36.983049 139616073008896 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.4235728979110718, loss=2.894411325454712
I0206 23:36:11.873435 139616064616192 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.41268983483314514, loss=2.857840061187744
I0206 23:36:46.746527 139616073008896 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3750140070915222, loss=2.8168702125549316
I0206 23:37:21.609920 139616064616192 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3680974543094635, loss=2.7536847591400146
I0206 23:37:56.466377 139616073008896 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3472900390625, loss=2.807495355606079
I0206 23:38:31.329045 139616064616192 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.3695952892303467, loss=2.8394975662231445
I0206 23:39:06.230032 139616073008896 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.39713767170906067, loss=2.789522171020508
I0206 23:39:41.128160 139616064616192 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3611009418964386, loss=2.79653000831604
I0206 23:40:16.008533 139616073008896 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3503146767616272, loss=2.738250970840454
I0206 23:40:50.889976 139616064616192 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3838258385658264, loss=2.795025587081909
I0206 23:41:25.753108 139616073008896 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.34940290451049805, loss=2.8285629749298096
I0206 23:42:00.621942 139616064616192 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.37710702419281006, loss=2.826925754547119
I0206 23:42:24.387896 139785736898368 spec.py:321] Evaluating on the training split.
I0206 23:42:27.387590 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:45:30.758490 139785736898368 spec.py:333] Evaluating on the validation split.
I0206 23:45:33.466760 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:48:08.952325 139785736898368 spec.py:349] Evaluating on the test split.
I0206 23:48:11.661153 139785736898368 workload.py:181] Translating evaluation dataset.
I0206 23:50:38.385442 139785736898368 submission_runner.py:408] Time since start: 46325.63s, 	Step: 81870, 	{'train/accuracy': 0.6784107685089111, 'train/loss': 1.6096141338348389, 'train/bleu': 33.84269230391496, 'validation/accuracy': 0.6793591976165771, 'validation/loss': 1.5915672779083252, 'validation/bleu': 29.80917299937564, 'validation/num_examples': 3000, 'test/accuracy': 0.6923711895942688, 'test/loss': 1.518932819366455, 'test/bleu': 29.556238156403044, 'test/num_examples': 3003, 'score': 28599.2469124794, 'total_duration': 46325.63169527054, 'accumulated_submission_time': 28599.2469124794, 'accumulated_eval_time': 17722.64498400688, 'accumulated_logging_time': 1.086064338684082}
I0206 23:50:38.412686 139616073008896 logging_writer.py:48] [81870] accumulated_eval_time=17722.644984, accumulated_logging_time=1.086064, accumulated_submission_time=28599.246912, global_step=81870, preemption_count=0, score=28599.246912, test/accuracy=0.692371, test/bleu=29.556238, test/loss=1.518933, test/num_examples=3003, total_duration=46325.631695, train/accuracy=0.678411, train/bleu=33.842692, train/loss=1.609614, validation/accuracy=0.679359, validation/bleu=29.809173, validation/loss=1.591567, validation/num_examples=3000
I0206 23:50:49.233079 139616064616192 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.38220545649528503, loss=2.8153233528137207
I0206 23:51:24.140651 139616073008896 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3666139841079712, loss=2.7869348526000977
I0206 23:51:59.009112 139616064616192 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.4150324761867523, loss=2.869149923324585
I0206 23:52:33.885681 139616073008896 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.38494059443473816, loss=2.762993335723877
I0206 23:53:08.750007 139616064616192 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.3836512863636017, loss=2.884730577468872
I0206 23:53:43.623448 139616073008896 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.380912721157074, loss=2.8335344791412354
I0206 23:54:18.497962 139616064616192 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3694899380207062, loss=2.8561244010925293
I0206 23:54:53.433854 139616073008896 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3502618074417114, loss=2.7655160427093506
I0206 23:55:28.375031 139616064616192 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.372854083776474, loss=2.814814567565918
I0206 23:56:03.260291 139616073008896 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.35986849665641785, loss=2.8092730045318604
I0206 23:56:38.126173 139616064616192 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.384321928024292, loss=2.831127882003784
I0206 23:57:12.994633 139616073008896 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3764495551586151, loss=2.804046392440796
I0206 23:57:47.873801 139616064616192 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.39935481548309326, loss=2.7476072311401367
I0206 23:58:22.762222 139616073008896 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.40091612935066223, loss=2.7955679893493652
I0206 23:58:57.672013 139616064616192 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.361124724149704, loss=2.827899694442749
I0206 23:59:32.607110 139616073008896 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.39895787835121155, loss=2.749850273132324
I0207 00:00:07.508749 139616064616192 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3735279440879822, loss=2.80684232711792
I0207 00:00:42.375229 139616073008896 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.35811588168144226, loss=2.7960801124572754
I0207 00:01:17.253038 139616064616192 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3991173207759857, loss=2.799825429916382
I0207 00:01:52.146205 139616073008896 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3889208436012268, loss=2.859632730484009
I0207 00:02:27.013159 139616064616192 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.37395089864730835, loss=2.7705001831054688
I0207 00:03:01.895718 139616073008896 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.42624783515930176, loss=2.8737752437591553
I0207 00:03:36.786656 139616064616192 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.37532132863998413, loss=2.8010973930358887
I0207 00:04:11.694844 139616073008896 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3700551688671112, loss=2.740712881088257
I0207 00:04:38.620391 139785736898368 spec.py:321] Evaluating on the training split.
I0207 00:04:41.616742 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:07:35.878804 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 00:07:38.586661 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:10:10.972634 139785736898368 spec.py:349] Evaluating on the test split.
I0207 00:10:13.687935 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:12:46.976421 139785736898368 submission_runner.py:408] Time since start: 47654.22s, 	Step: 84279, 	{'train/accuracy': 0.6645780205726624, 'train/loss': 1.6899528503417969, 'train/bleu': 32.87321657993862, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.5834990739822388, 'validation/bleu': 29.55673975736919, 'validation/num_examples': 3000, 'test/accuracy': 0.6955435872077942, 'test/loss': 1.5063549280166626, 'test/bleu': 29.769185359462774, 'test/num_examples': 3003, 'score': 29439.365225553513, 'total_duration': 47654.22263765335, 'accumulated_submission_time': 29439.365225553513, 'accumulated_eval_time': 18211.000927448273, 'accumulated_logging_time': 1.1236786842346191}
I0207 00:12:47.002465 139616064616192 logging_writer.py:48] [84279] accumulated_eval_time=18211.000927, accumulated_logging_time=1.123679, accumulated_submission_time=29439.365226, global_step=84279, preemption_count=0, score=29439.365226, test/accuracy=0.695544, test/bleu=29.769185, test/loss=1.506355, test/num_examples=3003, total_duration=47654.222638, train/accuracy=0.664578, train/bleu=32.873217, train/loss=1.689953, validation/accuracy=0.680202, validation/bleu=29.556740, validation/loss=1.583499, validation/num_examples=3000
I0207 00:12:54.689942 139616073008896 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.39338839054107666, loss=2.8230085372924805
I0207 00:13:29.552778 139616064616192 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3927098214626312, loss=2.8221607208251953
I0207 00:14:04.456966 139616073008896 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3959888517856598, loss=2.8183577060699463
I0207 00:14:39.345516 139616064616192 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.38878336548805237, loss=2.7293872833251953
I0207 00:15:14.242897 139616073008896 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.38169822096824646, loss=2.7859721183776855
I0207 00:15:49.154829 139616064616192 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.42446914315223694, loss=2.8533143997192383
I0207 00:16:24.027044 139616073008896 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.3834684193134308, loss=2.8035378456115723
I0207 00:16:58.914914 139616064616192 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.36655935645103455, loss=2.7616922855377197
I0207 00:17:33.811691 139616073008896 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.38418668508529663, loss=2.7285571098327637
I0207 00:18:08.676947 139616064616192 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.42209452390670776, loss=2.8244128227233887
I0207 00:18:43.568360 139616073008896 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.37592223286628723, loss=2.83921217918396
I0207 00:19:18.438868 139616064616192 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3923458456993103, loss=2.775175094604492
I0207 00:19:53.359793 139616073008896 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.39085277915000916, loss=2.785320997238159
I0207 00:20:28.271956 139616064616192 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.3844825029373169, loss=2.794304370880127
I0207 00:21:03.196807 139616073008896 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.4140542447566986, loss=2.8099985122680664
I0207 00:21:38.056374 139616064616192 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.4118914306163788, loss=2.779935836791992
I0207 00:22:12.949078 139616073008896 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.3909687399864197, loss=2.8471996784210205
I0207 00:22:47.834422 139616064616192 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.380758672952652, loss=2.894137144088745
I0207 00:23:22.698097 139616073008896 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3814801275730133, loss=2.8549790382385254
I0207 00:23:57.571442 139616064616192 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4062895178794861, loss=2.81217360496521
I0207 00:24:32.458010 139616073008896 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.4255726933479309, loss=2.794654369354248
I0207 00:25:07.319675 139616064616192 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.36997920274734497, loss=2.7444169521331787
I0207 00:25:42.212467 139616073008896 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3855563700199127, loss=2.796900749206543
I0207 00:26:17.084711 139616064616192 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4479932487010956, loss=2.814605951309204
I0207 00:26:47.153622 139785736898368 spec.py:321] Evaluating on the training split.
I0207 00:26:50.174170 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:29:36.710026 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 00:29:39.439758 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:32:09.197006 139785736898368 spec.py:349] Evaluating on the test split.
I0207 00:32:11.917413 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:34:45.524227 139785736898368 submission_runner.py:408] Time since start: 48972.77s, 	Step: 86688, 	{'train/accuracy': 0.6623689532279968, 'train/loss': 1.7069313526153564, 'train/bleu': 33.333061211211565, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.5803080797195435, 'validation/bleu': 29.87338660571638, 'validation/num_examples': 3000, 'test/accuracy': 0.6947998404502869, 'test/loss': 1.4972296953201294, 'test/bleu': 30.07424609365526, 'test/num_examples': 3003, 'score': 30279.430206775665, 'total_duration': 48972.770466566086, 'accumulated_submission_time': 30279.430206775665, 'accumulated_eval_time': 18689.371471881866, 'accumulated_logging_time': 1.1595537662506104}
I0207 00:34:45.550837 139616073008896 logging_writer.py:48] [86688] accumulated_eval_time=18689.371472, accumulated_logging_time=1.159554, accumulated_submission_time=30279.430207, global_step=86688, preemption_count=0, score=30279.430207, test/accuracy=0.694800, test/bleu=30.074246, test/loss=1.497230, test/num_examples=3003, total_duration=48972.770467, train/accuracy=0.662369, train/bleu=33.333061, train/loss=1.706931, validation/accuracy=0.679880, validation/bleu=29.873387, validation/loss=1.580308, validation/num_examples=3000
I0207 00:34:50.096649 139616064616192 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.41040536761283875, loss=2.7940354347229004
I0207 00:35:24.960446 139616073008896 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4297653138637543, loss=2.764508008956909
I0207 00:35:59.888650 139616064616192 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4103507995605469, loss=2.800478935241699
I0207 00:36:34.762236 139616073008896 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.38950812816619873, loss=2.7480759620666504
I0207 00:37:09.636053 139616064616192 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.39144784212112427, loss=2.823272466659546
I0207 00:37:44.529361 139616073008896 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.37943100929260254, loss=2.7731926441192627
I0207 00:38:19.402527 139616064616192 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.38511115312576294, loss=2.7746269702911377
I0207 00:38:54.305243 139616073008896 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.402773916721344, loss=2.820279598236084
I0207 00:39:29.202935 139616064616192 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4036034941673279, loss=2.7870874404907227
I0207 00:40:04.076826 139616073008896 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.399148166179657, loss=2.814371347427368
I0207 00:40:38.949491 139616064616192 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.4047416150569916, loss=2.8159501552581787
I0207 00:41:13.833613 139616073008896 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.38058385252952576, loss=2.788454532623291
I0207 00:41:48.685066 139616064616192 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.41203591227531433, loss=2.8588881492614746
I0207 00:42:23.564056 139616073008896 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.4184163808822632, loss=2.751399278640747
I0207 00:42:58.461160 139616064616192 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3839806616306305, loss=2.801809787750244
I0207 00:43:33.337971 139616073008896 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3939005732536316, loss=2.7995147705078125
I0207 00:44:08.188713 139616064616192 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.40287473797798157, loss=2.723285436630249
I0207 00:44:43.059428 139616073008896 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.40826496481895447, loss=2.847705841064453
I0207 00:45:17.983431 139616064616192 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.4503899812698364, loss=2.7585256099700928
I0207 00:45:52.904474 139616073008896 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.40949785709381104, loss=2.7957494258880615
I0207 00:46:27.795212 139616064616192 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.3766259551048279, loss=2.818112373352051
I0207 00:47:02.713540 139616073008896 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.40747568011283875, loss=2.8367323875427246
I0207 00:47:37.601146 139616064616192 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4047258496284485, loss=2.67928147315979
I0207 00:48:12.446411 139616073008896 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3987441658973694, loss=2.744499683380127
I0207 00:48:45.667585 139785736898368 spec.py:321] Evaluating on the training split.
I0207 00:48:48.663909 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:51:29.125365 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 00:51:31.838330 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:54:11.911159 139785736898368 spec.py:349] Evaluating on the test split.
I0207 00:54:14.620950 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 00:56:49.091636 139785736898368 submission_runner.py:408] Time since start: 50296.34s, 	Step: 89097, 	{'train/accuracy': 0.6741820573806763, 'train/loss': 1.636649250984192, 'train/bleu': 33.7021119458403, 'validation/accuracy': 0.6820126175880432, 'validation/loss': 1.5686326026916504, 'validation/bleu': 29.77797063662827, 'validation/num_examples': 3000, 'test/accuracy': 0.6962872743606567, 'test/loss': 1.4902687072753906, 'test/bleu': 29.97291099023428, 'test/num_examples': 3003, 'score': 31119.458785533905, 'total_duration': 50296.33786916733, 'accumulated_submission_time': 31119.458785533905, 'accumulated_eval_time': 19172.79546570778, 'accumulated_logging_time': 1.1961100101470947}
I0207 00:56:49.118226 139616064616192 logging_writer.py:48] [89097] accumulated_eval_time=19172.795466, accumulated_logging_time=1.196110, accumulated_submission_time=31119.458786, global_step=89097, preemption_count=0, score=31119.458786, test/accuracy=0.696287, test/bleu=29.972911, test/loss=1.490269, test/num_examples=3003, total_duration=50296.337869, train/accuracy=0.674182, train/bleu=33.702112, train/loss=1.636649, validation/accuracy=0.682013, validation/bleu=29.777971, validation/loss=1.568633, validation/num_examples=3000
I0207 00:56:50.539627 139616073008896 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.38250818848609924, loss=2.8229188919067383
I0207 00:57:25.423141 139616064616192 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4011492133140564, loss=2.8280019760131836
I0207 00:58:00.299658 139616073008896 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.3974532186985016, loss=2.780752420425415
I0207 00:58:35.172907 139616064616192 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.4092346131801605, loss=2.694157361984253
I0207 00:59:10.063648 139616073008896 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4406985342502594, loss=2.7586681842803955
I0207 00:59:44.968020 139616064616192 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.40838852524757385, loss=2.785111427307129
I0207 01:00:19.875394 139616073008896 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3923266530036926, loss=2.732546806335449
I0207 01:00:54.772975 139616064616192 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.42763352394104004, loss=2.7687485218048096
I0207 01:01:29.660657 139616073008896 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.4194350838661194, loss=2.744037628173828
I0207 01:02:04.554593 139616064616192 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4376979172229767, loss=2.7999889850616455
I0207 01:02:39.480817 139616073008896 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3936336040496826, loss=2.818429946899414
I0207 01:03:14.390791 139616064616192 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.4125034809112549, loss=2.792928695678711
I0207 01:03:49.294591 139616073008896 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3921886384487152, loss=2.768900156021118
I0207 01:04:24.191092 139616064616192 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3919900953769684, loss=2.8174400329589844
I0207 01:04:59.085473 139616073008896 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.39132073521614075, loss=2.776890754699707
I0207 01:05:33.959690 139616064616192 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.41580718755722046, loss=2.7965736389160156
I0207 01:06:08.838578 139616073008896 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.41524314880371094, loss=2.712815761566162
I0207 01:06:43.730421 139616064616192 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4170444905757904, loss=2.768343210220337
I0207 01:07:18.599003 139616073008896 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4024598002433777, loss=2.7788808345794678
I0207 01:07:53.458253 139616064616192 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4269271790981293, loss=2.8181912899017334
I0207 01:08:28.313874 139616073008896 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.4192564785480499, loss=2.763551712036133
I0207 01:09:03.203063 139616064616192 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.4205290973186493, loss=2.7673540115356445
I0207 01:09:38.083753 139616073008896 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.44957661628723145, loss=2.788942337036133
I0207 01:10:12.927427 139616064616192 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.43157047033309937, loss=2.727644443511963
I0207 01:10:47.824830 139616073008896 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.4048125445842743, loss=2.7661139965057373
I0207 01:10:49.290564 139785736898368 spec.py:321] Evaluating on the training split.
I0207 01:10:52.281605 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:13:52.279460 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 01:13:54.999903 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:16:28.579300 139785736898368 spec.py:349] Evaluating on the test split.
I0207 01:16:31.293485 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:18:57.885462 139785736898368 submission_runner.py:408] Time since start: 51625.13s, 	Step: 91506, 	{'train/accuracy': 0.667885959148407, 'train/loss': 1.6756535768508911, 'train/bleu': 33.10366408069063, 'validation/accuracy': 0.6826573610305786, 'validation/loss': 1.5624314546585083, 'validation/bleu': 30.156964677978845, 'validation/num_examples': 3000, 'test/accuracy': 0.6995874643325806, 'test/loss': 1.481274127960205, 'test/bleu': 30.325125321300153, 'test/num_examples': 3003, 'score': 31959.54501605034, 'total_duration': 51625.13171696663, 'accumulated_submission_time': 31959.54501605034, 'accumulated_eval_time': 19661.390310049057, 'accumulated_logging_time': 1.2334024906158447}
I0207 01:18:57.929540 139616064616192 logging_writer.py:48] [91506] accumulated_eval_time=19661.390310, accumulated_logging_time=1.233402, accumulated_submission_time=31959.545016, global_step=91506, preemption_count=0, score=31959.545016, test/accuracy=0.699587, test/bleu=30.325125, test/loss=1.481274, test/num_examples=3003, total_duration=51625.131717, train/accuracy=0.667886, train/bleu=33.103664, train/loss=1.675654, validation/accuracy=0.682657, validation/bleu=30.156965, validation/loss=1.562431, validation/num_examples=3000
I0207 01:19:31.128100 139616073008896 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.40931785106658936, loss=2.705523729324341
I0207 01:20:06.044111 139616064616192 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.4086039066314697, loss=2.7927019596099854
I0207 01:20:40.963237 139616073008896 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.44019362330436707, loss=2.725573778152466
I0207 01:21:15.842700 139616064616192 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4132510721683502, loss=2.736941337585449
I0207 01:21:50.724406 139616073008896 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4462760090827942, loss=2.8257088661193848
I0207 01:22:25.597218 139616064616192 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.4414141774177551, loss=2.815011978149414
I0207 01:23:00.456532 139616073008896 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.42429283261299133, loss=2.7562167644500732
I0207 01:23:35.330609 139616064616192 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.42343538999557495, loss=2.7304561138153076
I0207 01:24:10.264013 139616073008896 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3943582773208618, loss=2.7248165607452393
I0207 01:24:45.145223 139616064616192 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.4511508643627167, loss=2.8170042037963867
I0207 01:25:19.998734 139616073008896 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.43262240290641785, loss=2.838361978530884
I0207 01:25:54.903977 139616064616192 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.440807968378067, loss=2.789547920227051
I0207 01:26:29.779887 139616073008896 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.4298642873764038, loss=2.752866268157959
I0207 01:27:04.658619 139616064616192 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4274677634239197, loss=2.7272539138793945
I0207 01:27:39.540721 139616073008896 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.42177635431289673, loss=2.7903544902801514
I0207 01:28:14.405383 139616064616192 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.421641081571579, loss=2.7750957012176514
I0207 01:28:49.301323 139616073008896 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.42996084690093994, loss=2.8181872367858887
I0207 01:29:24.205616 139616064616192 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.42985671758651733, loss=2.755532741546631
I0207 01:29:59.106896 139616073008896 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4310079514980316, loss=2.750253438949585
I0207 01:30:34.011023 139616064616192 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.40236225724220276, loss=2.7468087673187256
I0207 01:31:08.877015 139616073008896 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.44938749074935913, loss=2.792999744415283
I0207 01:31:43.758350 139616064616192 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.4376637935638428, loss=2.761122941970825
I0207 01:32:18.625412 139616073008896 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.42794269323349, loss=2.6785104274749756
I0207 01:32:53.479012 139616064616192 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.42035776376724243, loss=2.757018566131592
I0207 01:32:58.084164 139785736898368 spec.py:321] Evaluating on the training split.
I0207 01:33:01.074787 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:35:52.673989 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 01:35:55.379126 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:38:23.576770 139785736898368 spec.py:349] Evaluating on the test split.
I0207 01:38:26.297138 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:40:49.083572 139785736898368 submission_runner.py:408] Time since start: 52936.33s, 	Step: 93915, 	{'train/accuracy': 0.7043598890304565, 'train/loss': 1.4692833423614502, 'train/bleu': 36.35120024978537, 'validation/accuracy': 0.6835501194000244, 'validation/loss': 1.558236002922058, 'validation/bleu': 30.0273109673757, 'validation/num_examples': 3000, 'test/accuracy': 0.7001568675041199, 'test/loss': 1.4738965034484863, 'test/bleu': 30.082391755845606, 'test/num_examples': 3003, 'score': 32799.61296272278, 'total_duration': 52936.32979607582, 'accumulated_submission_time': 32799.61296272278, 'accumulated_eval_time': 20132.38963317871, 'accumulated_logging_time': 1.2875926494598389}
I0207 01:40:49.110894 139616073008896 logging_writer.py:48] [93915] accumulated_eval_time=20132.389633, accumulated_logging_time=1.287593, accumulated_submission_time=32799.612963, global_step=93915, preemption_count=0, score=32799.612963, test/accuracy=0.700157, test/bleu=30.082392, test/loss=1.473897, test/num_examples=3003, total_duration=52936.329796, train/accuracy=0.704360, train/bleu=36.351200, train/loss=1.469283, validation/accuracy=0.683550, validation/bleu=30.027311, validation/loss=1.558236, validation/num_examples=3000
I0207 01:41:19.129460 139616064616192 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4314616620540619, loss=2.7562105655670166
I0207 01:41:54.067154 139616073008896 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.41626620292663574, loss=2.747152805328369
I0207 01:42:28.961663 139616064616192 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.47243985533714294, loss=2.823265552520752
I0207 01:43:03.830957 139616073008896 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.43766069412231445, loss=2.7825443744659424
I0207 01:43:38.710100 139616064616192 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.41807493567466736, loss=2.827580213546753
I0207 01:44:13.667277 139616073008896 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4181716740131378, loss=2.768148899078369
I0207 01:44:48.578097 139616064616192 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.42694252729415894, loss=2.7511515617370605
I0207 01:45:23.439736 139616073008896 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.4277082085609436, loss=2.728809356689453
I0207 01:45:58.358834 139616064616192 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4385462999343872, loss=2.6887121200561523
I0207 01:46:33.223638 139616073008896 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4410582482814789, loss=2.735013246536255
I0207 01:47:08.149437 139616064616192 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.42901721596717834, loss=2.796165943145752
I0207 01:47:43.029280 139616073008896 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.44165217876434326, loss=2.753026247024536
I0207 01:48:17.894840 139616064616192 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.4308653771877289, loss=2.715984344482422
I0207 01:48:52.795972 139616073008896 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.4368952810764313, loss=2.7457540035247803
I0207 01:49:27.664850 139616064616192 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.46227702498435974, loss=2.686091661453247
I0207 01:50:02.528483 139616073008896 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.44836071133613586, loss=2.751339912414551
I0207 01:50:37.429360 139616064616192 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.4205785393714905, loss=2.7734649181365967
I0207 01:51:12.292456 139616073008896 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4469105899333954, loss=2.837318181991577
I0207 01:51:47.186954 139616064616192 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.4531351923942566, loss=2.792391777038574
I0207 01:52:22.083877 139616073008896 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.46591100096702576, loss=2.8199665546417236
I0207 01:52:56.980699 139616064616192 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.43562081456184387, loss=2.706176996231079
I0207 01:53:31.876463 139616073008896 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.4667541980743408, loss=2.8086371421813965
I0207 01:54:06.790798 139616064616192 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.4645967185497284, loss=2.7308642864227295
I0207 01:54:41.666057 139616073008896 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.4650155305862427, loss=2.7728500366210938
I0207 01:54:49.425507 139785736898368 spec.py:321] Evaluating on the training split.
I0207 01:54:52.421713 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 01:58:17.464668 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 01:58:20.174690 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:00:53.410395 139785736898368 spec.py:349] Evaluating on the test split.
I0207 02:00:56.133215 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:03:44.747878 139785736898368 submission_runner.py:408] Time since start: 54311.99s, 	Step: 96324, 	{'train/accuracy': 0.6754699945449829, 'train/loss': 1.6206367015838623, 'train/bleu': 33.64124946940375, 'validation/accuracy': 0.6841948628425598, 'validation/loss': 1.5504695177078247, 'validation/bleu': 30.027689307621454, 'validation/num_examples': 3000, 'test/accuracy': 0.7004590034484863, 'test/loss': 1.4679011106491089, 'test/bleu': 30.379588836077428, 'test/num_examples': 3003, 'score': 33639.83898591995, 'total_duration': 54311.994121551514, 'accumulated_submission_time': 33639.83898591995, 'accumulated_eval_time': 20667.71193766594, 'accumulated_logging_time': 1.3252015113830566}
I0207 02:03:44.774823 139616064616192 logging_writer.py:48] [96324] accumulated_eval_time=20667.711938, accumulated_logging_time=1.325202, accumulated_submission_time=33639.838986, global_step=96324, preemption_count=0, score=33639.838986, test/accuracy=0.700459, test/bleu=30.379589, test/loss=1.467901, test/num_examples=3003, total_duration=54311.994122, train/accuracy=0.675470, train/bleu=33.641249, train/loss=1.620637, validation/accuracy=0.684195, validation/bleu=30.027689, validation/loss=1.550470, validation/num_examples=3000
I0207 02:04:11.630916 139616073008896 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.443768709897995, loss=2.820406913757324
I0207 02:04:46.483243 139616064616192 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.44469374418258667, loss=2.744640827178955
I0207 02:05:21.372677 139616073008896 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.43394842743873596, loss=2.7269787788391113
I0207 02:05:56.244464 139616064616192 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.4450346827507019, loss=2.7330055236816406
I0207 02:06:31.109756 139616073008896 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.47664114832878113, loss=2.7346842288970947
I0207 02:07:06.000749 139616064616192 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.4192759096622467, loss=2.7386796474456787
I0207 02:07:40.862475 139616073008896 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.4826864004135132, loss=2.774503707885742
I0207 02:08:15.727506 139616064616192 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.4636737108230591, loss=2.662086009979248
I0207 02:08:50.644979 139616073008896 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.46015048027038574, loss=2.7667322158813477
I0207 02:09:25.501469 139616064616192 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.44607529044151306, loss=2.7724978923797607
I0207 02:10:00.399888 139616073008896 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.4576496183872223, loss=2.76208758354187
I0207 02:10:35.307958 139616064616192 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.4433043599128723, loss=2.715609550476074
I0207 02:11:10.183503 139616073008896 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.4418506324291229, loss=2.69758677482605
I0207 02:11:45.033496 139616064616192 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.4412420690059662, loss=2.732623338699341
I0207 02:12:19.885308 139616073008896 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.4741179347038269, loss=2.684433698654175
I0207 02:12:54.759385 139616064616192 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.47524571418762207, loss=2.7356293201446533
I0207 02:13:29.617338 139616073008896 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.45430320501327515, loss=2.6819801330566406
I0207 02:14:04.500275 139616064616192 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.45116323232650757, loss=2.6566717624664307
I0207 02:14:39.386856 139616073008896 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.4467947483062744, loss=2.7033443450927734
I0207 02:15:14.311332 139616064616192 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.48308265209198, loss=2.7852818965911865
I0207 02:15:49.165723 139616073008896 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.4415007531642914, loss=2.663613796234131
I0207 02:16:24.063338 139616064616192 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.46019238233566284, loss=2.719999074935913
I0207 02:16:58.960677 139616073008896 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.4654555022716522, loss=2.7254652976989746
I0207 02:17:33.857283 139616064616192 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.46047160029411316, loss=2.743972063064575
I0207 02:17:45.095134 139785736898368 spec.py:321] Evaluating on the training split.
I0207 02:17:48.085150 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:21:08.871192 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 02:21:11.580758 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:23:50.280905 139785736898368 spec.py:349] Evaluating on the test split.
I0207 02:23:52.981576 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:26:30.331706 139785736898368 submission_runner.py:408] Time since start: 55677.58s, 	Step: 98734, 	{'train/accuracy': 0.6730888485908508, 'train/loss': 1.640097737312317, 'train/bleu': 34.10313197148955, 'validation/accuracy': 0.6855587363243103, 'validation/loss': 1.5443347692489624, 'validation/bleu': 30.445706302183336, 'validation/num_examples': 3000, 'test/accuracy': 0.700714647769928, 'test/loss': 1.4622043371200562, 'test/bleu': 30.23568171788698, 'test/num_examples': 3003, 'score': 34480.07180213928, 'total_duration': 55677.57796263695, 'accumulated_submission_time': 34480.07180213928, 'accumulated_eval_time': 21192.948457717896, 'accumulated_logging_time': 1.3619556427001953}
I0207 02:26:30.359645 139616073008896 logging_writer.py:48] [98734] accumulated_eval_time=21192.948458, accumulated_logging_time=1.361956, accumulated_submission_time=34480.071802, global_step=98734, preemption_count=0, score=34480.071802, test/accuracy=0.700715, test/bleu=30.235682, test/loss=1.462204, test/num_examples=3003, total_duration=55677.577963, train/accuracy=0.673089, train/bleu=34.103132, train/loss=1.640098, validation/accuracy=0.685559, validation/bleu=30.445706, validation/loss=1.544335, validation/num_examples=3000
I0207 02:26:53.744340 139616064616192 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.4934125244617462, loss=2.725271224975586
I0207 02:27:28.647670 139616073008896 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.469536691904068, loss=2.728402853012085
I0207 02:28:03.538488 139616064616192 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.46202149987220764, loss=2.7751877307891846
I0207 02:28:38.447637 139616073008896 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.461382120847702, loss=2.6802101135253906
I0207 02:29:13.319107 139616064616192 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.4353499710559845, loss=2.6978495121002197
I0207 02:29:48.260929 139616073008896 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.46422117948532104, loss=2.756434679031372
I0207 02:30:23.158502 139616064616192 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.45699241757392883, loss=2.716071605682373
I0207 02:30:58.053055 139616073008896 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.46929651498794556, loss=2.7452821731567383
I0207 02:31:32.966569 139616064616192 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.4473223388195038, loss=2.6644670963287354
I0207 02:32:07.913398 139616073008896 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.47466161847114563, loss=2.744654655456543
I0207 02:32:42.807987 139616064616192 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.4715154767036438, loss=2.718351364135742
I0207 02:33:17.686636 139616073008896 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.4418138861656189, loss=2.720754623413086
I0207 02:33:52.562693 139616064616192 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.4766397178173065, loss=2.72834849357605
I0207 02:34:27.453728 139616073008896 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.4881664514541626, loss=2.7932047843933105
I0207 02:35:02.339859 139616064616192 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.45917102694511414, loss=2.6772193908691406
I0207 02:35:37.218402 139616073008896 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.4689488112926483, loss=2.7247986793518066
I0207 02:36:12.116841 139616064616192 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.4789802134037018, loss=2.75974178314209
I0207 02:36:47.028539 139616073008896 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.4924952983856201, loss=2.724534511566162
I0207 02:37:21.915223 139616064616192 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.484571635723114, loss=2.7112457752227783
I0207 02:37:56.764315 139616073008896 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.4819592237472534, loss=2.6199135780334473
I0207 02:38:31.683074 139616064616192 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.4840145409107208, loss=2.744715929031372
I0207 02:39:06.558805 139616073008896 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.4711149334907532, loss=2.7604236602783203
I0207 02:39:41.424871 139616064616192 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.4844221770763397, loss=2.661327600479126
I0207 02:40:16.346437 139616073008896 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.4766889214515686, loss=2.7104172706604004
I0207 02:40:30.375859 139785736898368 spec.py:321] Evaluating on the training split.
I0207 02:40:33.388339 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:43:38.672179 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 02:43:41.397912 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:46:12.998893 139785736898368 spec.py:349] Evaluating on the test split.
I0207 02:46:15.710734 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 02:48:43.072984 139785736898368 submission_runner.py:408] Time since start: 57010.32s, 	Step: 101142, 	{'train/accuracy': 0.686933696269989, 'train/loss': 1.5602630376815796, 'train/bleu': 34.618318349974814, 'validation/accuracy': 0.6874186396598816, 'validation/loss': 1.5365707874298096, 'validation/bleu': 30.188926869006934, 'validation/num_examples': 3000, 'test/accuracy': 0.702934205532074, 'test/loss': 1.454554796218872, 'test/bleu': 30.4668122978629, 'test/num_examples': 3003, 'score': 35320.00034117699, 'total_duration': 57010.319242954254, 'accumulated_submission_time': 35320.00034117699, 'accumulated_eval_time': 21685.645532608032, 'accumulated_logging_time': 1.3999087810516357}
I0207 02:48:43.101723 139616064616192 logging_writer.py:48] [101142] accumulated_eval_time=21685.645533, accumulated_logging_time=1.399909, accumulated_submission_time=35320.000341, global_step=101142, preemption_count=0, score=35320.000341, test/accuracy=0.702934, test/bleu=30.466812, test/loss=1.454555, test/num_examples=3003, total_duration=57010.319243, train/accuracy=0.686934, train/bleu=34.618318, train/loss=1.560263, validation/accuracy=0.687419, validation/bleu=30.188927, validation/loss=1.536571, validation/num_examples=3000
I0207 02:49:03.700653 139616073008896 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.4514771103858948, loss=2.6445233821868896
I0207 02:49:38.613504 139616064616192 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.4750509262084961, loss=2.6950883865356445
I0207 02:50:13.508629 139616073008896 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.46720704436302185, loss=2.6883628368377686
I0207 02:50:48.401873 139616064616192 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.48491883277893066, loss=2.752648115158081
I0207 02:51:23.286355 139616073008896 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5116786360740662, loss=2.743882417678833
I0207 02:51:58.166644 139616064616192 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.4975268244743347, loss=2.701125144958496
I0207 02:52:33.048224 139616073008896 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.49183449149131775, loss=2.686940908432007
I0207 02:53:07.910485 139616064616192 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.4816648066043854, loss=2.7458837032318115
I0207 02:53:42.786875 139616073008896 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.47073689103126526, loss=2.6976568698883057
I0207 02:54:17.655156 139616064616192 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.48088696599006653, loss=2.6721019744873047
I0207 02:54:52.535308 139616073008896 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5012947916984558, loss=2.7608935832977295
I0207 02:55:27.442800 139616064616192 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.49997031688690186, loss=2.746185302734375
I0207 02:56:02.337329 139616073008896 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.4825283885002136, loss=2.6682803630828857
I0207 02:56:37.234720 139616064616192 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.4870817959308624, loss=2.675063133239746
I0207 02:57:12.134547 139616073008896 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.4907543361186981, loss=2.6761527061462402
I0207 02:57:47.037646 139616064616192 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5006261467933655, loss=2.6988160610198975
I0207 02:58:21.931833 139616073008896 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.47860851883888245, loss=2.690969467163086
I0207 02:58:56.806596 139616064616192 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.513854444026947, loss=2.7719898223876953
I0207 02:59:31.687957 139616073008896 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.4955604672431946, loss=2.739093780517578
I0207 03:00:06.582177 139616064616192 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.48847711086273193, loss=2.7451133728027344
I0207 03:00:41.464866 139616073008896 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.487475723028183, loss=2.65781307220459
I0207 03:01:16.379786 139616064616192 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5253061056137085, loss=2.7451059818267822
I0207 03:01:51.300282 139616073008896 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5269365310668945, loss=2.775050640106201
I0207 03:02:26.231472 139616064616192 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.4917793571949005, loss=2.6265361309051514
I0207 03:02:43.397602 139785736898368 spec.py:321] Evaluating on the training split.
I0207 03:02:46.394836 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:05:39.045167 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 03:05:41.760541 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:08:13.121165 139785736898368 spec.py:349] Evaluating on the test split.
I0207 03:08:15.829470 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:10:45.756254 139785736898368 submission_runner.py:408] Time since start: 58333.00s, 	Step: 103551, 	{'train/accuracy': 0.6821432709693909, 'train/loss': 1.5882389545440674, 'train/bleu': 34.26272973822794, 'validation/accuracy': 0.688956081867218, 'validation/loss': 1.5335873365402222, 'validation/bleu': 30.360145618817835, 'validation/num_examples': 3000, 'test/accuracy': 0.7046424150466919, 'test/loss': 1.4481626749038696, 'test/bleu': 30.654866411054087, 'test/num_examples': 3003, 'score': 36160.209800720215, 'total_duration': 58333.0024998188, 'accumulated_submission_time': 36160.209800720215, 'accumulated_eval_time': 22168.00412297249, 'accumulated_logging_time': 1.4385159015655518}
I0207 03:10:45.784477 139616073008896 logging_writer.py:48] [103551] accumulated_eval_time=22168.004123, accumulated_logging_time=1.438516, accumulated_submission_time=36160.209801, global_step=103551, preemption_count=0, score=36160.209801, test/accuracy=0.704642, test/bleu=30.654866, test/loss=1.448163, test/num_examples=3003, total_duration=58333.002500, train/accuracy=0.682143, train/bleu=34.262730, train/loss=1.588239, validation/accuracy=0.688956, validation/bleu=30.360146, validation/loss=1.533587, validation/num_examples=3000
I0207 03:11:03.224106 139616064616192 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.49711325764656067, loss=2.691253185272217
I0207 03:11:38.132554 139616073008896 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.49997755885124207, loss=2.674196720123291
I0207 03:12:13.059134 139616064616192 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5284734964370728, loss=2.751516342163086
I0207 03:12:47.997230 139616073008896 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5348182916641235, loss=2.6580071449279785
I0207 03:13:22.930711 139616064616192 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.4870617389678955, loss=2.7106735706329346
I0207 03:13:57.831669 139616073008896 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.49799907207489014, loss=2.696866273880005
I0207 03:14:32.746745 139616064616192 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.5877932906150818, loss=2.800272226333618
I0207 03:15:07.631243 139616073008896 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5391618609428406, loss=2.6344494819641113
I0207 03:15:42.538146 139616064616192 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5153491497039795, loss=2.71521258354187
I0207 03:16:17.409661 139616073008896 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5093423128128052, loss=2.7051262855529785
I0207 03:16:52.299547 139616064616192 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.49752864241600037, loss=2.6672322750091553
I0207 03:17:27.171260 139616073008896 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.506847620010376, loss=2.6858088970184326
I0207 03:18:02.037066 139616064616192 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5146545767784119, loss=2.639742374420166
I0207 03:18:36.929413 139616073008896 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5238459706306458, loss=2.695659875869751
I0207 03:19:11.830354 139616064616192 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5043292045593262, loss=2.6912567615509033
I0207 03:19:46.744850 139616073008896 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5069701075553894, loss=2.7478201389312744
I0207 03:20:21.652978 139616064616192 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.5015968084335327, loss=2.665907144546509
I0207 03:20:56.551685 139616073008896 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5281693339347839, loss=2.740410566329956
I0207 03:21:31.436379 139616064616192 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5014192461967468, loss=2.6597578525543213
I0207 03:22:06.307915 139616073008896 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.533542275428772, loss=2.722623109817505
I0207 03:22:41.195061 139616064616192 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5127723217010498, loss=2.69692063331604
I0207 03:23:16.128371 139616073008896 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5385680198669434, loss=2.7010910511016846
I0207 03:23:51.020481 139616064616192 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5407788157463074, loss=2.7363831996917725
I0207 03:24:25.913852 139616073008896 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5260839462280273, loss=2.6574206352233887
I0207 03:24:45.882716 139785736898368 spec.py:321] Evaluating on the training split.
I0207 03:24:48.881814 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:28:09.311938 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 03:28:12.035392 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:30:47.433072 139785736898368 spec.py:349] Evaluating on the test split.
I0207 03:30:50.147991 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:33:17.990010 139785736898368 submission_runner.py:408] Time since start: 59685.24s, 	Step: 105959, 	{'train/accuracy': 0.6814278364181519, 'train/loss': 1.5989018678665161, 'train/bleu': 34.65526969945151, 'validation/accuracy': 0.6893280744552612, 'validation/loss': 1.5266581773757935, 'validation/bleu': 30.57867552999859, 'validation/num_examples': 3000, 'test/accuracy': 0.7055836319923401, 'test/loss': 1.4425654411315918, 'test/bleu': 30.785481276223113, 'test/num_examples': 3003, 'score': 37000.21959018707, 'total_duration': 59685.23624706268, 'accumulated_submission_time': 37000.21959018707, 'accumulated_eval_time': 22680.11134338379, 'accumulated_logging_time': 1.4766552448272705}
I0207 03:33:18.019717 139616064616192 logging_writer.py:48] [105959] accumulated_eval_time=22680.111343, accumulated_logging_time=1.476655, accumulated_submission_time=37000.219590, global_step=105959, preemption_count=0, score=37000.219590, test/accuracy=0.705584, test/bleu=30.785481, test/loss=1.442565, test/num_examples=3003, total_duration=59685.236247, train/accuracy=0.681428, train/bleu=34.655270, train/loss=1.598902, validation/accuracy=0.689328, validation/bleu=30.578676, validation/loss=1.526658, validation/num_examples=3000
I0207 03:33:32.666527 139616073008896 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5641103982925415, loss=2.713785171508789
I0207 03:34:07.570731 139616064616192 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.5274702906608582, loss=2.670255184173584
I0207 03:34:42.480498 139616073008896 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.5508392453193665, loss=2.642517328262329
I0207 03:35:17.365088 139616064616192 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5313121676445007, loss=2.6359755992889404
I0207 03:35:52.267153 139616073008896 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5324049592018127, loss=2.6306915283203125
I0207 03:36:27.137554 139616064616192 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.5153596997261047, loss=2.6049485206604004
I0207 03:37:02.045204 139616073008896 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.5458561778068542, loss=2.7072649002075195
I0207 03:37:36.953857 139616064616192 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.5262102484703064, loss=2.676788806915283
I0207 03:38:11.864118 139616073008896 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5382404327392578, loss=2.696194887161255
I0207 03:38:46.747852 139616064616192 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5661529898643494, loss=2.701514720916748
I0207 03:39:21.628400 139616073008896 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5415681004524231, loss=2.6673648357391357
I0207 03:39:56.513490 139616064616192 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.5657204985618591, loss=2.6774563789367676
I0207 03:40:31.435341 139616073008896 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.565973162651062, loss=2.7399799823760986
I0207 03:41:06.334537 139616064616192 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.5404168367385864, loss=2.622769832611084
I0207 03:41:41.241583 139616073008896 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.564759373664856, loss=2.6856765747070312
I0207 03:42:16.105993 139616064616192 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.5609065294265747, loss=2.701242446899414
I0207 03:42:50.997306 139616073008896 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.5222777128219604, loss=2.672614336013794
I0207 03:43:25.896891 139616064616192 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.528056263923645, loss=2.6340291500091553
I0207 03:44:00.791324 139616073008896 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.5653577446937561, loss=2.7239322662353516
I0207 03:44:35.665521 139616064616192 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.5650210380554199, loss=2.732741355895996
I0207 03:45:10.543894 139616073008896 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.5481368899345398, loss=2.706422805786133
I0207 03:45:45.404045 139616064616192 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.569911539554596, loss=2.7127928733825684
I0207 03:46:20.267282 139616073008896 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.5580728650093079, loss=2.620748519897461
I0207 03:46:55.185178 139616064616192 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.5491547584533691, loss=2.6315200328826904
I0207 03:47:18.284542 139785736898368 spec.py:321] Evaluating on the training split.
I0207 03:47:21.288455 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:50:29.662719 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 03:50:32.365422 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:53:07.575744 139785736898368 spec.py:349] Evaluating on the test split.
I0207 03:53:10.288609 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 03:55:38.373853 139785736898368 submission_runner.py:408] Time since start: 61025.62s, 	Step: 108368, 	{'train/accuracy': 0.6921325922012329, 'train/loss': 1.5298690795898438, 'train/bleu': 35.40294324144317, 'validation/accuracy': 0.6879394054412842, 'validation/loss': 1.5276821851730347, 'validation/bleu': 30.47344674934183, 'validation/num_examples': 3000, 'test/accuracy': 0.7045029401779175, 'test/loss': 1.443967342376709, 'test/bleu': 30.56989330909604, 'test/num_examples': 3003, 'score': 37840.398203372955, 'total_duration': 61025.62011170387, 'accumulated_submission_time': 37840.398203372955, 'accumulated_eval_time': 23180.200609207153, 'accumulated_logging_time': 1.51641845703125}
I0207 03:55:38.403771 139616073008896 logging_writer.py:48] [108368] accumulated_eval_time=23180.200609, accumulated_logging_time=1.516418, accumulated_submission_time=37840.398203, global_step=108368, preemption_count=0, score=37840.398203, test/accuracy=0.704503, test/bleu=30.569893, test/loss=1.443967, test/num_examples=3003, total_duration=61025.620112, train/accuracy=0.692133, train/bleu=35.402943, train/loss=1.529869, validation/accuracy=0.687939, validation/bleu=30.473447, validation/loss=1.527682, validation/num_examples=3000
I0207 03:55:49.952443 139616064616192 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.5829122066497803, loss=2.671086072921753
I0207 03:56:24.888011 139616073008896 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.5664862990379333, loss=2.6924548149108887
I0207 03:56:59.774349 139616064616192 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.5845139622688293, loss=2.702775478363037
I0207 03:57:34.675997 139616073008896 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.5850720405578613, loss=2.748823404312134
I0207 03:58:09.563071 139616064616192 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.5626648664474487, loss=2.6721770763397217
I0207 03:58:44.437232 139616073008896 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.568179726600647, loss=2.7204458713531494
I0207 03:59:19.341971 139616064616192 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.5687382221221924, loss=2.646578073501587
I0207 03:59:54.254208 139616073008896 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6030915379524231, loss=2.619978904724121
I0207 04:00:29.163182 139616064616192 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.5422556400299072, loss=2.6692230701446533
I0207 04:01:04.058589 139616073008896 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.572364330291748, loss=2.66139554977417
I0207 04:01:38.939770 139616064616192 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.5707765817642212, loss=2.719985008239746
I0207 04:02:13.858647 139616073008896 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.5573244094848633, loss=2.717224597930908
I0207 04:02:48.792880 139616064616192 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.5715935230255127, loss=2.645057439804077
I0207 04:03:23.665602 139616073008896 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.5352829694747925, loss=2.5994863510131836
I0207 04:03:58.544126 139616064616192 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.5796689391136169, loss=2.696554183959961
I0207 04:04:33.427436 139616073008896 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.5786694288253784, loss=2.7393791675567627
I0207 04:05:08.322550 139616064616192 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.5631526112556458, loss=2.719761610031128
I0207 04:05:43.207099 139616073008896 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.5716315507888794, loss=2.6454343795776367
I0207 04:06:18.100035 139616064616192 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.5610514283180237, loss=2.697190046310425
I0207 04:06:52.978920 139616073008896 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.5822643637657166, loss=2.649644613265991
I0207 04:07:27.856008 139616064616192 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.5972790122032166, loss=2.672239303588867
I0207 04:08:02.729527 139616073008896 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.5819122791290283, loss=2.70174503326416
I0207 04:08:37.632565 139616064616192 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6051501631736755, loss=2.682523012161255
I0207 04:09:12.534300 139616073008896 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.5859497785568237, loss=2.7011313438415527
I0207 04:09:38.432267 139785736898368 spec.py:321] Evaluating on the training split.
I0207 04:09:41.444476 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:12:41.020618 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 04:12:43.726741 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:15:18.579079 139785736898368 spec.py:349] Evaluating on the test split.
I0207 04:15:21.288024 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:17:49.302412 139785736898368 submission_runner.py:408] Time since start: 62356.55s, 	Step: 110776, 	{'train/accuracy': 0.6881821751594543, 'train/loss': 1.555271863937378, 'train/bleu': 34.94405351615512, 'validation/accuracy': 0.6900472044944763, 'validation/loss': 1.5213935375213623, 'validation/bleu': 30.51655914551938, 'validation/num_examples': 3000, 'test/accuracy': 0.705258309841156, 'test/loss': 1.4395705461502075, 'test/bleu': 30.330988152869008, 'test/num_examples': 3003, 'score': 38680.33823132515, 'total_duration': 62356.54861474037, 'accumulated_submission_time': 38680.33823132515, 'accumulated_eval_time': 23671.070686101913, 'accumulated_logging_time': 1.5580275058746338}
I0207 04:17:49.334512 139616064616192 logging_writer.py:48] [110776] accumulated_eval_time=23671.070686, accumulated_logging_time=1.558028, accumulated_submission_time=38680.338231, global_step=110776, preemption_count=0, score=38680.338231, test/accuracy=0.705258, test/bleu=30.330988, test/loss=1.439571, test/num_examples=3003, total_duration=62356.548615, train/accuracy=0.688182, train/bleu=34.944054, train/loss=1.555272, validation/accuracy=0.690047, validation/bleu=30.516559, validation/loss=1.521394, validation/num_examples=3000
I0207 04:17:58.080211 139616073008896 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.5892533659934998, loss=2.6541342735290527
I0207 04:18:33.040566 139616064616192 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.5420224070549011, loss=2.648676633834839
I0207 04:19:07.923471 139616073008896 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.5950927734375, loss=2.6806681156158447
I0207 04:19:42.801824 139616064616192 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.5924384593963623, loss=2.6412136554718018
I0207 04:20:17.686587 139616073008896 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.5956773161888123, loss=2.6446805000305176
I0207 04:20:52.590848 139616064616192 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.61667799949646, loss=2.606999158859253
I0207 04:21:27.479821 139616073008896 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.5847963094711304, loss=2.6951029300689697
I0207 04:22:02.362146 139616064616192 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.5915919542312622, loss=2.6768782138824463
I0207 04:22:37.219996 139616073008896 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.5802905559539795, loss=2.68215274810791
I0207 04:23:12.116177 139616064616192 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.6035408973693848, loss=2.5734894275665283
I0207 04:23:46.997759 139616073008896 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6437330842018127, loss=2.709636688232422
I0207 04:24:21.898890 139616064616192 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.592633843421936, loss=2.718449354171753
I0207 04:24:56.752548 139616073008896 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.5852177143096924, loss=2.673750162124634
I0207 04:25:31.673532 139616064616192 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.6082433462142944, loss=2.6646547317504883
I0207 04:26:06.604002 139616073008896 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6047239899635315, loss=2.7198116779327393
I0207 04:26:41.494378 139616064616192 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6147578954696655, loss=2.6740341186523438
I0207 04:27:16.387895 139616073008896 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.5928907990455627, loss=2.595067024230957
I0207 04:27:51.306528 139616064616192 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.620646059513092, loss=2.6989657878875732
I0207 04:28:26.224811 139616073008896 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.611825168132782, loss=2.5948710441589355
I0207 04:29:01.112972 139616064616192 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6099033355712891, loss=2.6530487537384033
I0207 04:29:36.003306 139616073008896 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.6056964993476868, loss=2.6531269550323486
I0207 04:30:10.888534 139616064616192 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6052652597427368, loss=2.638889789581299
I0207 04:30:45.761639 139616073008896 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6283312439918518, loss=2.6854310035705566
I0207 04:31:20.633764 139616064616192 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.5986332893371582, loss=2.673776388168335
I0207 04:31:49.636940 139785736898368 spec.py:321] Evaluating on the training split.
I0207 04:31:52.639371 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:34:52.696499 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 04:34:55.412964 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:37:29.392584 139785736898368 spec.py:349] Evaluating on the test split.
I0207 04:37:32.113703 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:40:06.683940 139785736898368 submission_runner.py:408] Time since start: 63693.93s, 	Step: 113185, 	{'train/accuracy': 0.7068272233009338, 'train/loss': 1.4597846269607544, 'train/bleu': 36.19503096365993, 'validation/accuracy': 0.6921550631523132, 'validation/loss': 1.5146585702896118, 'validation/bleu': 30.680754105466335, 'validation/num_examples': 3000, 'test/accuracy': 0.7078961133956909, 'test/loss': 1.430131435394287, 'test/bleu': 30.702009793502405, 'test/num_examples': 3003, 'score': 39520.55152916908, 'total_duration': 63693.93018531799, 'accumulated_submission_time': 39520.55152916908, 'accumulated_eval_time': 24168.117631196976, 'accumulated_logging_time': 1.6014611721038818}
I0207 04:40:06.714714 139616073008896 logging_writer.py:48] [113185] accumulated_eval_time=24168.117631, accumulated_logging_time=1.601461, accumulated_submission_time=39520.551529, global_step=113185, preemption_count=0, score=39520.551529, test/accuracy=0.707896, test/bleu=30.702010, test/loss=1.430131, test/num_examples=3003, total_duration=63693.930185, train/accuracy=0.706827, train/bleu=36.195031, train/loss=1.459785, validation/accuracy=0.692155, validation/bleu=30.680754, validation/loss=1.514659, validation/num_examples=3000
I0207 04:40:12.292588 139616064616192 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.6061349511146545, loss=2.6408627033233643
I0207 04:40:47.195257 139616073008896 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.6150192022323608, loss=2.6689834594726562
I0207 04:41:22.135303 139616064616192 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.6048068404197693, loss=2.7621617317199707
I0207 04:41:57.056296 139616073008896 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6389634609222412, loss=2.5937726497650146
I0207 04:42:31.958070 139616064616192 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6231894493103027, loss=2.635176181793213
I0207 04:43:06.906555 139616073008896 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.6717313528060913, loss=2.7216804027557373
I0207 04:43:41.804099 139616064616192 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.5967606902122498, loss=2.614603042602539
I0207 04:44:16.686537 139616073008896 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6393930912017822, loss=2.7414073944091797
I0207 04:44:51.584876 139616064616192 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6178932785987854, loss=2.577380418777466
I0207 04:45:26.469317 139616073008896 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.628048300743103, loss=2.6314802169799805
I0207 04:46:01.364570 139616064616192 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6268140077590942, loss=2.6706275939941406
I0207 04:46:36.256854 139616073008896 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.6253693699836731, loss=2.69053053855896
I0207 04:47:11.119930 139616064616192 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.6211071014404297, loss=2.623786687850952
I0207 04:47:46.020878 139616073008896 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6394326686859131, loss=2.6878268718719482
I0207 04:48:20.886663 139616064616192 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.626307487487793, loss=2.6271579265594482
I0207 04:48:55.768166 139616073008896 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.6353060603141785, loss=2.647463798522949
I0207 04:49:30.643686 139616064616192 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.6395152807235718, loss=2.6886727809906006
I0207 04:50:05.527298 139616073008896 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.6238986849784851, loss=2.6880903244018555
I0207 04:50:40.421878 139616064616192 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.621832549571991, loss=2.6402578353881836
I0207 04:51:15.322586 139616073008896 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.6182534098625183, loss=2.5709187984466553
I0207 04:51:50.201000 139616064616192 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.6285736560821533, loss=2.5949792861938477
I0207 04:52:25.098299 139616073008896 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.6294794678688049, loss=2.670444965362549
I0207 04:52:59.958717 139616064616192 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.6477712988853455, loss=2.5665111541748047
I0207 04:53:34.846956 139616073008896 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.6684028506278992, loss=2.6962807178497314
I0207 04:54:06.968932 139785736898368 spec.py:321] Evaluating on the training split.
I0207 04:54:09.969477 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 04:57:29.383889 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 04:57:32.092729 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:00:12.652710 139785736898368 spec.py:349] Evaluating on the test split.
I0207 05:00:15.375152 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:02:33.926651 139785736898368 submission_runner.py:408] Time since start: 65041.17s, 	Step: 115594, 	{'train/accuracy': 0.6958543062210083, 'train/loss': 1.5099217891693115, 'train/bleu': 35.537028715325654, 'validation/accuracy': 0.6910639405250549, 'validation/loss': 1.5157564878463745, 'validation/bleu': 30.634864207978154, 'validation/num_examples': 3000, 'test/accuracy': 0.7095230221748352, 'test/loss': 1.4238560199737549, 'test/bleu': 31.016669589638788, 'test/num_examples': 3003, 'score': 40360.71843266487, 'total_duration': 65041.172853946686, 'accumulated_submission_time': 40360.71843266487, 'accumulated_eval_time': 24675.07525396347, 'accumulated_logging_time': 1.6436638832092285}
I0207 05:02:33.957733 139616064616192 logging_writer.py:48] [115594] accumulated_eval_time=24675.075254, accumulated_logging_time=1.643664, accumulated_submission_time=40360.718433, global_step=115594, preemption_count=0, score=40360.718433, test/accuracy=0.709523, test/bleu=31.016670, test/loss=1.423856, test/num_examples=3003, total_duration=65041.172854, train/accuracy=0.695854, train/bleu=35.537029, train/loss=1.509922, validation/accuracy=0.691064, validation/bleu=30.634864, validation/loss=1.515756, validation/num_examples=3000
I0207 05:02:36.413866 139616073008896 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.6520417928695679, loss=2.621096134185791
I0207 05:03:11.319284 139616064616192 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.6479359865188599, loss=2.6761443614959717
I0207 05:03:46.186107 139616073008896 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.6460465788841248, loss=2.6842474937438965
I0207 05:04:21.085240 139616064616192 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.6620305180549622, loss=2.662694215774536
I0207 05:04:55.980056 139616073008896 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.6446636915206909, loss=2.6321187019348145
I0207 05:05:30.847744 139616064616192 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.6322687268257141, loss=2.636972665786743
I0207 05:06:05.776446 139616073008896 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.637921929359436, loss=2.61073637008667
I0207 05:06:40.657760 139616064616192 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.6521299481391907, loss=2.6696763038635254
I0207 05:07:15.552861 139616073008896 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.6620712876319885, loss=2.6649932861328125
I0207 05:07:50.442773 139616064616192 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6436797976493835, loss=2.652040481567383
I0207 05:08:25.325893 139616073008896 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.6648440957069397, loss=2.6247191429138184
I0207 05:09:00.215097 139616064616192 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.6590162515640259, loss=2.6204323768615723
I0207 05:09:35.091218 139616073008896 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.6418529748916626, loss=2.658013343811035
I0207 05:10:09.997191 139616064616192 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.6500734686851501, loss=2.617439031600952
I0207 05:10:44.867060 139616073008896 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.6649336814880371, loss=2.6084542274475098
I0207 05:11:19.753781 139616064616192 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.6772049069404602, loss=2.6369454860687256
I0207 05:11:54.632610 139616073008896 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.6414794325828552, loss=2.613835573196411
I0207 05:12:29.512528 139616064616192 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.6554215550422668, loss=2.5599565505981445
I0207 05:13:04.405708 139616073008896 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.6220035552978516, loss=2.6016347408294678
I0207 05:13:39.274467 139616064616192 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.6865666508674622, loss=2.711472511291504
I0207 05:14:14.133656 139616073008896 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.6342300772666931, loss=2.5766005516052246
I0207 05:14:49.011935 139616064616192 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.6288310289382935, loss=2.633012056350708
I0207 05:15:23.886915 139616073008896 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.6474961638450623, loss=2.5741219520568848
I0207 05:15:58.768002 139616064616192 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.6529128551483154, loss=2.619140386581421
I0207 05:16:33.661640 139616073008896 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.7030255794525146, loss=2.636038064956665
I0207 05:16:34.085378 139785736898368 spec.py:321] Evaluating on the training split.
I0207 05:16:37.086116 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:19:45.660390 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 05:19:48.390352 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:22:11.972937 139785736898368 spec.py:349] Evaluating on the test split.
I0207 05:22:14.711815 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:24:48.388653 139785736898368 submission_runner.py:408] Time since start: 66375.63s, 	Step: 118003, 	{'train/accuracy': 0.6981893181800842, 'train/loss': 1.5048353672027588, 'train/bleu': 35.49911535816573, 'validation/accuracy': 0.6911135315895081, 'validation/loss': 1.5123188495635986, 'validation/bleu': 30.810753206662906, 'validation/num_examples': 3000, 'test/accuracy': 0.70961594581604, 'test/loss': 1.4236197471618652, 'test/bleu': 31.12338429193865, 'test/num_examples': 3003, 'score': 41200.7603263855, 'total_duration': 66375.6348798275, 'accumulated_submission_time': 41200.7603263855, 'accumulated_eval_time': 25169.378446102142, 'accumulated_logging_time': 1.6847825050354004}
I0207 05:24:48.424890 139616064616192 logging_writer.py:48] [118003] accumulated_eval_time=25169.378446, accumulated_logging_time=1.684783, accumulated_submission_time=41200.760326, global_step=118003, preemption_count=0, score=41200.760326, test/accuracy=0.709616, test/bleu=31.123384, test/loss=1.423620, test/num_examples=3003, total_duration=66375.634880, train/accuracy=0.698189, train/bleu=35.499115, train/loss=1.504835, validation/accuracy=0.691114, validation/bleu=30.810753, validation/loss=1.512319, validation/num_examples=3000
I0207 05:25:22.654918 139616073008896 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.6805382370948792, loss=2.6644372940063477
I0207 05:25:57.588747 139616064616192 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.6755171418190002, loss=2.6113595962524414
I0207 05:26:32.477403 139616073008896 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.6633300185203552, loss=2.620492458343506
I0207 05:27:07.387533 139616064616192 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.6776876449584961, loss=2.628779411315918
I0207 05:27:42.276246 139616073008896 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.6505826115608215, loss=2.622035264968872
I0207 05:28:17.170166 139616064616192 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.6936988234519958, loss=2.6409807205200195
I0207 05:28:52.098258 139616073008896 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.6941198110580444, loss=2.702319383621216
I0207 05:29:26.976997 139616064616192 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.6696664690971375, loss=2.660604238510132
I0207 05:30:01.867835 139616073008896 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.6828272342681885, loss=2.616940975189209
I0207 05:30:36.760409 139616064616192 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.6876503825187683, loss=2.6541261672973633
I0207 05:31:11.635061 139616073008896 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.6509661078453064, loss=2.5940089225769043
I0207 05:31:46.517339 139616064616192 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.6555460095405579, loss=2.6210784912109375
I0207 05:32:21.429579 139616073008896 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.6867272853851318, loss=2.6093266010284424
I0207 05:32:56.314575 139616064616192 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.686913013458252, loss=2.6598761081695557
I0207 05:33:31.189815 139616073008896 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.6759194731712341, loss=2.624431848526001
I0207 05:34:06.053810 139616064616192 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.6595215201377869, loss=2.56036376953125
I0207 05:34:40.924123 139616073008896 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7033163905143738, loss=2.694688558578491
I0207 05:35:15.812975 139616064616192 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.6872937083244324, loss=2.6450469493865967
I0207 05:35:50.717664 139616073008896 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.6582664847373962, loss=2.59657621383667
I0207 05:36:25.588733 139616064616192 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.6728086471557617, loss=2.5687243938446045
I0207 05:37:00.474642 139616073008896 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.6847848892211914, loss=2.5976877212524414
I0207 05:37:35.350968 139616064616192 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.6660330891609192, loss=2.6247918605804443
I0207 05:38:10.241841 139616073008896 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.6787222623825073, loss=2.5922529697418213
I0207 05:38:45.126099 139616064616192 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.7078552842140198, loss=2.6265223026275635
I0207 05:38:48.682073 139785736898368 spec.py:321] Evaluating on the training split.
I0207 05:38:51.684677 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:41:50.415161 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 05:41:53.117639 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:44:31.123209 139785736898368 spec.py:349] Evaluating on the test split.
I0207 05:44:33.850322 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 05:47:07.130003 139785736898368 submission_runner.py:408] Time since start: 67714.38s, 	Step: 120412, 	{'train/accuracy': 0.7060102224349976, 'train/loss': 1.4636739492416382, 'train/bleu': 36.326105954772956, 'validation/accuracy': 0.6930354237556458, 'validation/loss': 1.5079798698425293, 'validation/bleu': 30.81666176363183, 'validation/num_examples': 3000, 'test/accuracy': 0.7101737260818481, 'test/loss': 1.4174529314041138, 'test/bleu': 31.0455792076282, 'test/num_examples': 3003, 'score': 42040.92917585373, 'total_duration': 67714.37622475624, 'accumulated_submission_time': 42040.92917585373, 'accumulated_eval_time': 25667.826288461685, 'accumulated_logging_time': 1.732635259628296}
I0207 05:47:07.161514 139616073008896 logging_writer.py:48] [120412] accumulated_eval_time=25667.826288, accumulated_logging_time=1.732635, accumulated_submission_time=42040.929176, global_step=120412, preemption_count=0, score=42040.929176, test/accuracy=0.710174, test/bleu=31.045579, test/loss=1.417453, test/num_examples=3003, total_duration=67714.376225, train/accuracy=0.706010, train/bleu=36.326106, train/loss=1.463674, validation/accuracy=0.693035, validation/bleu=30.816662, validation/loss=1.507980, validation/num_examples=3000
I0207 05:47:38.221742 139616064616192 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.6902722120285034, loss=2.559373617172241
I0207 05:48:13.182804 139616073008896 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.6778987050056458, loss=2.6348633766174316
I0207 05:48:48.142450 139616064616192 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.6770750880241394, loss=2.599898338317871
I0207 05:49:23.055307 139616073008896 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7184647917747498, loss=2.5772812366485596
I0207 05:49:57.940557 139616064616192 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7027112245559692, loss=2.59139084815979
I0207 05:50:32.839616 139616073008896 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.6736927628517151, loss=2.600470781326294
I0207 05:51:07.736753 139616064616192 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.678287148475647, loss=2.639892339706421
I0207 05:51:42.636955 139616073008896 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.6998275518417358, loss=2.5877153873443604
I0207 05:52:17.539749 139616064616192 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.6770524978637695, loss=2.6561849117279053
I0207 05:52:52.446786 139616073008896 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.6613425016403198, loss=2.5641891956329346
I0207 05:53:27.403474 139616064616192 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.6952129602432251, loss=2.588779926300049
I0207 05:54:02.275866 139616073008896 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.6699901819229126, loss=2.583925724029541
I0207 05:54:37.194446 139616064616192 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.6845749616622925, loss=2.6160218715667725
I0207 05:55:12.100470 139616073008896 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7104297280311584, loss=2.5941355228424072
I0207 05:55:47.017452 139616064616192 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7162168622016907, loss=2.6299448013305664
I0207 05:56:21.880990 139616073008896 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7041064500808716, loss=2.6376209259033203
I0207 05:56:56.756904 139616064616192 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.697638988494873, loss=2.5783426761627197
I0207 05:57:31.645288 139616073008896 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.7198913097381592, loss=2.6300876140594482
I0207 05:58:06.538620 139616064616192 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7162643074989319, loss=2.631352424621582
I0207 05:58:41.416160 139616073008896 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.6739605069160461, loss=2.5258052349090576
I0207 05:59:16.311417 139616064616192 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.700089156627655, loss=2.604539394378662
I0207 05:59:51.213040 139616073008896 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.6856222152709961, loss=2.5627808570861816
I0207 06:00:26.112417 139616064616192 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.697689950466156, loss=2.6782095432281494
I0207 06:01:01.012255 139616073008896 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7134915590286255, loss=2.5954928398132324
I0207 06:01:07.355840 139785736898368 spec.py:321] Evaluating on the training split.
I0207 06:01:10.360199 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:04:10.637491 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 06:04:13.343165 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:06:44.092544 139785736898368 spec.py:349] Evaluating on the test split.
I0207 06:06:46.810837 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:09:23.405631 139785736898368 submission_runner.py:408] Time since start: 69050.65s, 	Step: 122820, 	{'train/accuracy': 0.7075486779212952, 'train/loss': 1.4473143815994263, 'train/bleu': 36.116525022338294, 'validation/accuracy': 0.6921798586845398, 'validation/loss': 1.5096970796585083, 'validation/bleu': 30.694070873645327, 'validation/num_examples': 3000, 'test/accuracy': 0.7105455994606018, 'test/loss': 1.418480634689331, 'test/bleu': 31.13267295113105, 'test/num_examples': 3003, 'score': 42881.0361392498, 'total_duration': 69050.65186357498, 'accumulated_submission_time': 42881.0361392498, 'accumulated_eval_time': 26163.876004457474, 'accumulated_logging_time': 1.7744617462158203}
I0207 06:09:23.438228 139616064616192 logging_writer.py:48] [122820] accumulated_eval_time=26163.876004, accumulated_logging_time=1.774462, accumulated_submission_time=42881.036139, global_step=122820, preemption_count=0, score=42881.036139, test/accuracy=0.710546, test/bleu=31.132673, test/loss=1.418481, test/num_examples=3003, total_duration=69050.651864, train/accuracy=0.707549, train/bleu=36.116525, train/loss=1.447314, validation/accuracy=0.692180, validation/bleu=30.694071, validation/loss=1.509697, validation/num_examples=3000
I0207 06:09:51.735456 139616073008896 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7182446718215942, loss=2.5869455337524414
I0207 06:10:26.672482 139616064616192 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7400379180908203, loss=2.614063024520874
I0207 06:11:01.565106 139616073008896 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.6821823120117188, loss=2.596909523010254
I0207 06:11:36.620576 139616064616192 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7247994542121887, loss=2.58245849609375
I0207 06:12:11.541576 139616073008896 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.727692186832428, loss=2.617149829864502
I0207 06:12:46.445968 139616064616192 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.6945835947990417, loss=2.6283764839172363
I0207 06:13:21.346956 139616073008896 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.7093809247016907, loss=2.6344733238220215
I0207 06:13:56.239255 139616064616192 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7005948424339294, loss=2.589442491531372
I0207 06:14:31.142084 139616073008896 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.6971112489700317, loss=2.6374826431274414
I0207 06:15:06.016778 139616064616192 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.6908064484596252, loss=2.5742380619049072
I0207 06:15:40.899102 139616073008896 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.6846289038658142, loss=2.6059391498565674
I0207 06:16:15.815936 139616064616192 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.6902735233306885, loss=2.6396758556365967
I0207 06:16:50.713657 139616073008896 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.6972878575325012, loss=2.578778028488159
I0207 06:17:25.607175 139616064616192 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.7193373441696167, loss=2.590744972229004
I0207 06:18:00.490729 139616073008896 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.7194926738739014, loss=2.6649582386016846
I0207 06:18:35.384234 139616064616192 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.7148346304893494, loss=2.5860955715179443
I0207 06:19:10.291844 139616073008896 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7039516568183899, loss=2.6656830310821533
I0207 06:19:45.180607 139616064616192 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.7074257135391235, loss=2.604321002960205
I0207 06:20:20.096834 139616073008896 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.7010731101036072, loss=2.6195805072784424
I0207 06:20:55.014262 139616064616192 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.7066135406494141, loss=2.583650827407837
I0207 06:21:29.964244 139616073008896 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.7050954699516296, loss=2.6195764541625977
I0207 06:22:04.869791 139616064616192 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.6858317255973816, loss=2.5849850177764893
I0207 06:22:39.756706 139616073008896 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.7216268181800842, loss=2.638185977935791
I0207 06:23:14.641563 139616064616192 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.7985632419586182, loss=2.53532075881958
I0207 06:23:23.430629 139785736898368 spec.py:321] Evaluating on the training split.
I0207 06:23:26.434285 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:26:39.750108 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 06:26:42.469987 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:29:17.128466 139785736898368 spec.py:349] Evaluating on the test split.
I0207 06:29:19.838329 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:31:48.721150 139785736898368 submission_runner.py:408] Time since start: 70395.97s, 	Step: 125227, 	{'train/accuracy': 0.7090739607810974, 'train/loss': 1.4433223009109497, 'train/bleu': 36.57344895473294, 'validation/accuracy': 0.6934942007064819, 'validation/loss': 1.5054748058319092, 'validation/bleu': 30.817558697381532, 'validation/num_examples': 3000, 'test/accuracy': 0.7112312316894531, 'test/loss': 1.4143809080123901, 'test/bleu': 31.124645847204345, 'test/num_examples': 3003, 'score': 43720.94148516655, 'total_duration': 70395.96740603447, 'accumulated_submission_time': 43720.94148516655, 'accumulated_eval_time': 26669.166478395462, 'accumulated_logging_time': 1.816622018814087}
I0207 06:31:48.752670 139616073008896 logging_writer.py:48] [125227] accumulated_eval_time=26669.166478, accumulated_logging_time=1.816622, accumulated_submission_time=43720.941485, global_step=125227, preemption_count=0, score=43720.941485, test/accuracy=0.711231, test/bleu=31.124646, test/loss=1.414381, test/num_examples=3003, total_duration=70395.967406, train/accuracy=0.709074, train/bleu=36.573449, train/loss=1.443322, validation/accuracy=0.693494, validation/bleu=30.817559, validation/loss=1.505475, validation/num_examples=3000
I0207 06:32:14.582005 139616064616192 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.742046594619751, loss=2.699361801147461
I0207 06:32:49.505756 139616073008896 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.707375168800354, loss=2.6457903385162354
I0207 06:33:24.415789 139616064616192 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7046713829040527, loss=2.6199615001678467
I0207 06:33:59.362677 139616073008896 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.7065942883491516, loss=2.5843822956085205
I0207 06:34:34.246846 139616064616192 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.7471965551376343, loss=2.595188856124878
I0207 06:35:09.194435 139616073008896 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.697053074836731, loss=2.5817697048187256
I0207 06:35:44.127422 139616064616192 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.7131966352462769, loss=2.6145544052124023
I0207 06:36:19.012825 139616073008896 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7166062593460083, loss=2.627363681793213
I0207 06:36:53.893341 139616064616192 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.724012553691864, loss=2.603119134902954
I0207 06:37:28.771950 139616073008896 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.6831971406936646, loss=2.564711570739746
I0207 06:38:03.647016 139616064616192 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.7013909220695496, loss=2.5720157623291016
I0207 06:38:38.541480 139616073008896 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.6874844431877136, loss=2.4906816482543945
I0207 06:39:13.456512 139616064616192 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.7050424814224243, loss=2.5983448028564453
I0207 06:39:48.376242 139616073008896 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.6835113763809204, loss=2.594252824783325
I0207 06:40:23.249473 139616064616192 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.7116885781288147, loss=2.634861469268799
I0207 06:40:58.153041 139616073008896 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.7054475545883179, loss=2.6584513187408447
I0207 06:41:33.032016 139616064616192 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7164226174354553, loss=2.5494089126586914
I0207 06:42:07.923012 139616073008896 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7065557241439819, loss=2.582091808319092
I0207 06:42:42.831018 139616064616192 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.6772690415382385, loss=2.5473787784576416
I0207 06:43:17.726863 139616073008896 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.7188134789466858, loss=2.6340830326080322
I0207 06:43:52.612973 139616064616192 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.7062385082244873, loss=2.6420891284942627
I0207 06:44:27.506311 139616073008896 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.7119585871696472, loss=2.6083567142486572
I0207 06:45:02.416682 139616064616192 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.7206472754478455, loss=2.5509703159332275
I0207 06:45:37.319613 139616073008896 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.7061867117881775, loss=2.5821516513824463
I0207 06:45:48.902836 139785736898368 spec.py:321] Evaluating on the training split.
I0207 06:45:51.905475 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:48:49.381692 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 06:48:52.096899 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:51:30.457834 139785736898368 spec.py:349] Evaluating on the test split.
I0207 06:51:33.176196 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 06:53:57.764354 139785736898368 submission_runner.py:408] Time since start: 71725.01s, 	Step: 127635, 	{'train/accuracy': 0.7085887789726257, 'train/loss': 1.438305377960205, 'train/bleu': 36.61241848155233, 'validation/accuracy': 0.6935437917709351, 'validation/loss': 1.5044677257537842, 'validation/bleu': 30.862560161613207, 'validation/num_examples': 3000, 'test/accuracy': 0.7111498713493347, 'test/loss': 1.4130109548568726, 'test/bleu': 31.248859219822087, 'test/num_examples': 3003, 'score': 44561.00401854515, 'total_duration': 71725.01059532166, 'accumulated_submission_time': 44561.00401854515, 'accumulated_eval_time': 27158.02793073654, 'accumulated_logging_time': 1.858067274093628}
I0207 06:53:57.796412 139616064616192 logging_writer.py:48] [127635] accumulated_eval_time=27158.027931, accumulated_logging_time=1.858067, accumulated_submission_time=44561.004019, global_step=127635, preemption_count=0, score=44561.004019, test/accuracy=0.711150, test/bleu=31.248859, test/loss=1.413011, test/num_examples=3003, total_duration=71725.010595, train/accuracy=0.708589, train/bleu=36.612418, train/loss=1.438305, validation/accuracy=0.693544, validation/bleu=30.862560, validation/loss=1.504468, validation/num_examples=3000
I0207 06:54:20.841600 139616073008896 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.733210027217865, loss=2.5995032787323
I0207 06:54:55.769251 139616064616192 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.6994247436523438, loss=2.5777313709259033
I0207 06:55:30.671060 139616073008896 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.7208014130592346, loss=2.5715532302856445
I0207 06:56:05.558376 139616064616192 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.7194150686264038, loss=2.655865430831909
I0207 06:56:40.455932 139616073008896 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.7482934594154358, loss=2.5944628715515137
I0207 06:57:15.343710 139616064616192 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.720799446105957, loss=2.5805459022521973
I0207 06:57:50.235532 139616073008896 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.7066243886947632, loss=2.552722215652466
I0207 06:58:25.124126 139616064616192 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.7094569206237793, loss=2.5724265575408936
I0207 06:59:00.037436 139616073008896 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.7280654907226562, loss=2.528336763381958
I0207 06:59:34.914078 139616064616192 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7271764278411865, loss=2.5720953941345215
I0207 07:00:09.818136 139616073008896 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.6720436811447144, loss=2.5083982944488525
I0207 07:00:44.698223 139616064616192 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.711679995059967, loss=2.561645030975342
I0207 07:01:19.576989 139616073008896 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.7178331613540649, loss=2.5813724994659424
I0207 07:01:54.445830 139616064616192 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.7265012264251709, loss=2.627230167388916
I0207 07:02:29.355765 139616073008896 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7146930694580078, loss=2.6407310962677
I0207 07:03:04.249070 139616064616192 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.725517988204956, loss=2.5967929363250732
I0207 07:03:39.164135 139616073008896 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.7153842449188232, loss=2.6030845642089844
I0207 07:04:14.073501 139616064616192 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.70567387342453, loss=2.597928047180176
I0207 07:04:48.958874 139616073008896 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7262486815452576, loss=2.635209560394287
I0207 07:05:23.839877 139616064616192 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.7067984938621521, loss=2.585590362548828
I0207 07:05:58.699367 139616073008896 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.7099242210388184, loss=2.5590741634368896
I0207 07:06:33.597706 139616064616192 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.7181486487388611, loss=2.564250946044922
I0207 07:07:08.491275 139616073008896 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7199666500091553, loss=2.603712558746338
I0207 07:07:43.392386 139616064616192 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.7300397157669067, loss=2.5865209102630615
I0207 07:07:57.782056 139785736898368 spec.py:321] Evaluating on the training split.
I0207 07:08:00.786402 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:11:02.602441 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 07:11:05.325358 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:13:49.021236 139785736898368 spec.py:349] Evaluating on the test split.
I0207 07:13:51.768535 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:16:18.175719 139785736898368 submission_runner.py:408] Time since start: 73065.42s, 	Step: 130043, 	{'train/accuracy': 0.7091085910797119, 'train/loss': 1.4369813203811646, 'train/bleu': 36.998641531348035, 'validation/accuracy': 0.6933081746101379, 'validation/loss': 1.504925012588501, 'validation/bleu': 30.757680438251448, 'validation/num_examples': 3000, 'test/accuracy': 0.7116844058036804, 'test/loss': 1.4134386777877808, 'test/bleu': 31.10714427248701, 'test/num_examples': 3003, 'score': 45400.904284238815, 'total_duration': 73065.42194890976, 'accumulated_submission_time': 45400.904284238815, 'accumulated_eval_time': 27658.42151737213, 'accumulated_logging_time': 1.9005036354064941}
I0207 07:16:18.209084 139616073008896 logging_writer.py:48] [130043] accumulated_eval_time=27658.421517, accumulated_logging_time=1.900504, accumulated_submission_time=45400.904284, global_step=130043, preemption_count=0, score=45400.904284, test/accuracy=0.711684, test/bleu=31.107144, test/loss=1.413439, test/num_examples=3003, total_duration=73065.421949, train/accuracy=0.709109, train/bleu=36.998642, train/loss=1.436981, validation/accuracy=0.693308, validation/bleu=30.757680, validation/loss=1.504925, validation/num_examples=3000
I0207 07:16:38.474167 139616064616192 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7180601358413696, loss=2.5834219455718994
I0207 07:17:13.412269 139616073008896 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.7210519313812256, loss=2.621337413787842
I0207 07:17:48.309855 139616064616192 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.7126545310020447, loss=2.5714128017425537
I0207 07:18:23.212765 139616073008896 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.701096773147583, loss=2.4980978965759277
I0207 07:18:58.114789 139616064616192 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7056758403778076, loss=2.6066462993621826
I0207 07:19:33.023448 139616073008896 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.7166721224784851, loss=2.5936036109924316
I0207 07:20:07.940503 139616064616192 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7112079858779907, loss=2.5991744995117188
I0207 07:20:42.816679 139616073008896 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.705175518989563, loss=2.5587711334228516
I0207 07:21:17.674317 139616064616192 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.7401762008666992, loss=2.678572416305542
I0207 07:21:52.561351 139616073008896 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7101157903671265, loss=2.557382345199585
I0207 07:22:27.442545 139616064616192 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.7298032641410828, loss=2.574953079223633
I0207 07:23:02.340718 139616073008896 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.6943600177764893, loss=2.5883209705352783
I0207 07:23:37.243991 139616064616192 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.6987671256065369, loss=2.620668411254883
I0207 07:24:12.161808 139616073008896 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.7142778635025024, loss=2.5592641830444336
I0207 07:24:47.110519 139616064616192 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.6951618194580078, loss=2.5770530700683594
I0207 07:25:22.038379 139616073008896 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.7089470624923706, loss=2.610391855239868
I0207 07:25:56.951107 139616064616192 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.732933759689331, loss=2.608363628387451
I0207 07:26:31.839167 139616073008896 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7079593539237976, loss=2.6314947605133057
I0207 07:27:06.756253 139616064616192 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.7037442922592163, loss=2.570913553237915
I0207 07:27:41.641848 139616073008896 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.6890367865562439, loss=2.559889316558838
I0207 07:28:16.537747 139616064616192 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.7120935916900635, loss=2.5632410049438477
I0207 07:28:51.441990 139616073008896 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.6999528408050537, loss=2.580676317214966
I0207 07:29:26.369301 139616064616192 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.6942988038063049, loss=2.542048215866089
I0207 07:30:01.293930 139616073008896 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.7247743606567383, loss=2.5584516525268555
I0207 07:30:18.480509 139785736898368 spec.py:321] Evaluating on the training split.
I0207 07:30:21.476566 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:33:32.804758 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 07:33:35.505880 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:36:14.897954 139785736898368 spec.py:349] Evaluating on the test split.
I0207 07:36:17.615031 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:38:46.917562 139785736898368 submission_runner.py:408] Time since start: 74414.16s, 	Step: 132451, 	{'train/accuracy': 0.7096970677375793, 'train/loss': 1.4432387351989746, 'train/bleu': 36.137919749675284, 'validation/accuracy': 0.693717360496521, 'validation/loss': 1.504041075706482, 'validation/bleu': 30.865012416464875, 'validation/num_examples': 3000, 'test/accuracy': 0.7116495370864868, 'test/loss': 1.4128319025039673, 'test/bleu': 31.196985520739922, 'test/num_examples': 3003, 'score': 46241.0879445076, 'total_duration': 74414.16377663612, 'accumulated_submission_time': 46241.0879445076, 'accumulated_eval_time': 28166.858474493027, 'accumulated_logging_time': 1.944511890411377}
I0207 07:38:46.950943 139616064616192 logging_writer.py:48] [132451] accumulated_eval_time=28166.858474, accumulated_logging_time=1.944512, accumulated_submission_time=46241.087945, global_step=132451, preemption_count=0, score=46241.087945, test/accuracy=0.711650, test/bleu=31.196986, test/loss=1.412832, test/num_examples=3003, total_duration=74414.163777, train/accuracy=0.709697, train/bleu=36.137920, train/loss=1.443239, validation/accuracy=0.693717, validation/bleu=30.865012, validation/loss=1.504041, validation/num_examples=3000
I0207 07:39:04.438832 139616073008896 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.6906574368476868, loss=2.6349899768829346
I0207 07:39:39.373222 139616064616192 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.695803701877594, loss=2.6018917560577393
I0207 07:40:14.308949 139616073008896 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.7169062495231628, loss=2.587984561920166
I0207 07:40:49.235682 139616064616192 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.7098707556724548, loss=2.5573766231536865
I0207 07:41:24.126376 139616073008896 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.6771097183227539, loss=2.596595525741577
I0207 07:41:59.036870 139616064616192 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.6949294805526733, loss=2.5817160606384277
I0207 07:42:33.945086 139616073008896 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.7054750919342041, loss=2.585526466369629
I0207 07:43:08.850968 139616064616192 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.6979897618293762, loss=2.509618043899536
I0207 07:43:43.741339 139616073008896 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.7285553812980652, loss=2.6763813495635986
I0207 07:43:54.626296 139785736898368 spec.py:321] Evaluating on the training split.
I0207 07:43:57.629429 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:47:05.848088 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 07:47:08.550211 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:49:48.629936 139785736898368 spec.py:349] Evaluating on the test split.
I0207 07:49:51.362710 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:52:21.770141 139785736898368 submission_runner.py:408] Time since start: 75229.02s, 	Step: 133333, 	{'train/accuracy': 0.7093749642372131, 'train/loss': 1.4428577423095703, 'train/bleu': 36.541089722712044, 'validation/accuracy': 0.693630576133728, 'validation/loss': 1.5042879581451416, 'validation/bleu': 30.869243773906362, 'validation/num_examples': 3000, 'test/accuracy': 0.7118006348609924, 'test/loss': 1.413087010383606, 'test/bleu': 31.18247574009484, 'test/num_examples': 3003, 'score': 46548.72388243675, 'total_duration': 75229.0163321495, 'accumulated_submission_time': 46548.72388243675, 'accumulated_eval_time': 28674.002204179764, 'accumulated_logging_time': 1.9894163608551025}
I0207 07:52:21.810498 139616064616192 logging_writer.py:48] [133333] accumulated_eval_time=28674.002204, accumulated_logging_time=1.989416, accumulated_submission_time=46548.723882, global_step=133333, preemption_count=0, score=46548.723882, test/accuracy=0.711801, test/bleu=31.182476, test/loss=1.413087, test/num_examples=3003, total_duration=75229.016332, train/accuracy=0.709375, train/bleu=36.541090, train/loss=1.442858, validation/accuracy=0.693631, validation/bleu=30.869244, validation/loss=1.504288, validation/num_examples=3000
I0207 07:52:21.849394 139616073008896 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46548.723882
I0207 07:52:23.326332 139785736898368 checkpoints.py:490] Saving checkpoint at step: 133333
I0207 07:52:28.150972 139785736898368 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1/checkpoint_133333
I0207 07:52:28.155859 139785736898368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_1/checkpoint_133333.
I0207 07:52:28.204986 139785736898368 submission_runner.py:583] Tuning trial 1/5
I0207 07:52:28.205232 139785736898368 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0207 07:52:28.210939 139785736898368 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005940888077020645, 'train/loss': 11.064360618591309, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.09946036338806, 'total_duration': 929.0078175067902, 'accumulated_submission_time': 37.09946036338806, 'accumulated_eval_time': 891.9083139896393, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2401, {'train/accuracy': 0.41223323345184326, 'train/loss': 3.9737305641174316, 'train/bleu': 14.315394056700264, 'validation/accuracy': 0.3997098505496979, 'validation/loss': 4.093648433685303, 'validation/bleu': 9.72599437822303, 'validation/num_examples': 3000, 'test/accuracy': 0.3828597962856293, 'test/loss': 4.295248985290527, 'test/bleu': 7.93982270373288, 'test/num_examples': 3003, 'score': 877.1772933006287, 'total_duration': 2344.4883332252502, 'accumulated_submission_time': 877.1772933006287, 'accumulated_eval_time': 1467.1980648040771, 'accumulated_logging_time': 0.030037641525268555, 'global_step': 2401, 'preemption_count': 0}), (4801, {'train/accuracy': 0.542317271232605, 'train/loss': 2.7344980239868164, 'train/bleu': 24.2609666755636, 'validation/accuracy': 0.5440229773521423, 'validation/loss': 2.7259020805358887, 'validation/bleu': 20.122334101513726, 'validation/num_examples': 3000, 'test/accuracy': 0.5431642532348633, 'test/loss': 2.7641193866729736, 'test/bleu': 18.73440224873982, 'test/num_examples': 3003, 'score': 1717.1984317302704, 'total_duration': 3643.2582201957703, 'accumulated_submission_time': 1717.1984317302704, 'accumulated_eval_time': 1925.8410477638245, 'accumulated_logging_time': 0.056603193283081055, 'global_step': 4801, 'preemption_count': 0}), (7203, {'train/accuracy': 0.5805636644363403, 'train/loss': 2.379575252532959, 'train/bleu': 27.023267640432266, 'validation/accuracy': 0.5879034399986267, 'validation/loss': 2.307112455368042, 'validation/bleu': 23.286632821120534, 'validation/num_examples': 3000, 'test/accuracy': 0.5903899073600769, 'test/loss': 2.3085203170776367, 'test/bleu': 21.953963966435513, 'test/num_examples': 3003, 'score': 2557.0957322120667, 'total_duration': 4909.256707191467, 'accumulated_submission_time': 2557.0957322120667, 'accumulated_eval_time': 2351.8377647399902, 'accumulated_logging_time': 0.08249330520629883, 'global_step': 7203, 'preemption_count': 0}), (9607, {'train/accuracy': 0.593815803527832, 'train/loss': 2.2298953533172607, 'train/bleu': 28.305043807122836, 'validation/accuracy': 0.6075063943862915, 'validation/loss': 2.134629487991333, 'validation/bleu': 24.751819281295166, 'validation/num_examples': 3000, 'test/accuracy': 0.6115739941596985, 'test/loss': 2.1070995330810547, 'test/bleu': 23.369496351054153, 'test/num_examples': 3003, 'score': 3397.0145201683044, 'total_duration': 6201.824702739716, 'accumulated_submission_time': 3397.0145201683044, 'accumulated_eval_time': 2804.3810741901398, 'accumulated_logging_time': 0.10986804962158203, 'global_step': 9607, 'preemption_count': 0}), (12012, {'train/accuracy': 0.6022624969482422, 'train/loss': 2.149235486984253, 'train/bleu': 28.608856006115754, 'validation/accuracy': 0.619471549987793, 'validation/loss': 2.007272720336914, 'validation/bleu': 25.498765397658293, 'validation/num_examples': 3000, 'test/accuracy': 0.6252629160881042, 'test/loss': 1.9659838676452637, 'test/bleu': 24.260364769931513, 'test/num_examples': 3003, 'score': 4237.137367010117, 'total_duration': 7712.961602926254, 'accumulated_submission_time': 4237.137367010117, 'accumulated_eval_time': 3475.285562515259, 'accumulated_logging_time': 0.1377413272857666, 'global_step': 12012, 'preemption_count': 0}), (14418, {'train/accuracy': 0.6124773025512695, 'train/loss': 2.0575904846191406, 'train/bleu': 29.56920209281836, 'validation/accuracy': 0.6307671070098877, 'validation/loss': 1.9267817735671997, 'validation/bleu': 26.21993126585849, 'validation/num_examples': 3000, 'test/accuracy': 0.6387659311294556, 'test/loss': 1.878983736038208, 'test/bleu': 25.881130375271567, 'test/num_examples': 3003, 'score': 5077.038963317871, 'total_duration': 9003.50999879837, 'accumulated_submission_time': 5077.038963317871, 'accumulated_eval_time': 3925.826815366745, 'accumulated_logging_time': 0.1641829013824463, 'global_step': 14418, 'preemption_count': 0}), (16823, {'train/accuracy': 0.6192818880081177, 'train/loss': 2.010906934738159, 'train/bleu': 29.80887983767533, 'validation/accuracy': 0.6372766494750977, 'validation/loss': 1.866188883781433, 'validation/bleu': 26.629292980287577, 'validation/num_examples': 3000, 'test/accuracy': 0.6472604870796204, 'test/loss': 1.8144638538360596, 'test/bleu': 26.0036035069853, 'test/num_examples': 3003, 'score': 5916.943907022476, 'total_duration': 10295.500350952148, 'accumulated_submission_time': 5916.943907022476, 'accumulated_eval_time': 4377.803135633469, 'accumulated_logging_time': 0.19261598587036133, 'global_step': 16823, 'preemption_count': 0}), (19230, {'train/accuracy': 0.636422872543335, 'train/loss': 1.8756887912750244, 'train/bleu': 30.822602520333803, 'validation/accuracy': 0.6443069577217102, 'validation/loss': 1.8201544284820557, 'validation/bleu': 27.01061448954327, 'validation/num_examples': 3000, 'test/accuracy': 0.6535820364952087, 'test/loss': 1.7662612199783325, 'test/bleu': 26.518186076268215, 'test/num_examples': 3003, 'score': 6757.095978498459, 'total_duration': 11722.64000749588, 'accumulated_submission_time': 6757.095978498459, 'accumulated_eval_time': 4964.684417009354, 'accumulated_logging_time': 0.2200927734375, 'global_step': 19230, 'preemption_count': 0}), (21638, {'train/accuracy': 0.6261128783226013, 'train/loss': 1.9536588191986084, 'train/bleu': 30.657585430152547, 'validation/accuracy': 0.6480762958526611, 'validation/loss': 1.7900378704071045, 'validation/bleu': 27.471663978033018, 'validation/num_examples': 3000, 'test/accuracy': 0.6585091352462769, 'test/loss': 1.7311699390411377, 'test/bleu': 27.19216074685112, 'test/num_examples': 3003, 'score': 7597.150082349777, 'total_duration': 13029.59965801239, 'accumulated_submission_time': 7597.150082349777, 'accumulated_eval_time': 5431.484809875488, 'accumulated_logging_time': 0.24876832962036133, 'global_step': 21638, 'preemption_count': 0}), (24047, {'train/accuracy': 0.6246453523635864, 'train/loss': 1.9670435190200806, 'train/bleu': 29.88082264721335, 'validation/accuracy': 0.6471339464187622, 'validation/loss': 1.8075987100601196, 'validation/bleu': 27.139849535608672, 'validation/num_examples': 3000, 'test/accuracy': 0.6549997329711914, 'test/loss': 1.7625569105148315, 'test/bleu': 26.310299172789822, 'test/num_examples': 3003, 'score': 8437.384685277939, 'total_duration': 14372.047944068909, 'accumulated_submission_time': 8437.384685277939, 'accumulated_eval_time': 5933.594583034515, 'accumulated_logging_time': 0.2760488986968994, 'global_step': 24047, 'preemption_count': 0}), (26455, {'train/accuracy': 0.6341571807861328, 'train/loss': 1.8880059719085693, 'train/bleu': 30.919613536831562, 'validation/accuracy': 0.6495393514633179, 'validation/loss': 1.7730599641799927, 'validation/bleu': 27.610013018571298, 'validation/num_examples': 3000, 'test/accuracy': 0.6604846119880676, 'test/loss': 1.7240240573883057, 'test/bleu': 27.430558262691026, 'test/num_examples': 3003, 'score': 9277.445498466492, 'total_duration': 15694.008920431137, 'accumulated_submission_time': 9277.445498466492, 'accumulated_eval_time': 6415.389216184616, 'accumulated_logging_time': 0.30326366424560547, 'global_step': 26455, 'preemption_count': 0}), (28863, {'train/accuracy': 0.6321583390235901, 'train/loss': 1.9047863483428955, 'train/bleu': 30.918711635523824, 'validation/accuracy': 0.6536558866500854, 'validation/loss': 1.7527354955673218, 'validation/bleu': 27.963738924004478, 'validation/num_examples': 3000, 'test/accuracy': 0.6618906855583191, 'test/loss': 1.6961952447891235, 'test/bleu': 27.259525740209053, 'test/num_examples': 3003, 'score': 10117.523353815079, 'total_duration': 17037.438900470734, 'accumulated_submission_time': 10117.523353815079, 'accumulated_eval_time': 6918.62993311882, 'accumulated_logging_time': 0.33623456954956055, 'global_step': 28863, 'preemption_count': 0}), (31272, {'train/accuracy': 0.6326280832290649, 'train/loss': 1.9133557081222534, 'train/bleu': 30.91387757741362, 'validation/accuracy': 0.6534698605537415, 'validation/loss': 1.7469457387924194, 'validation/bleu': 27.38010984222516, 'validation/num_examples': 3000, 'test/accuracy': 0.6626111268997192, 'test/loss': 1.6905726194381714, 'test/bleu': 27.10428323835168, 'test/num_examples': 3003, 'score': 10957.66311454773, 'total_duration': 18410.030876636505, 'accumulated_submission_time': 10957.66311454773, 'accumulated_eval_time': 7450.977185487747, 'accumulated_logging_time': 0.3637206554412842, 'global_step': 31272, 'preemption_count': 0}), (33681, {'train/accuracy': 0.63736891746521, 'train/loss': 1.8761459589004517, 'train/bleu': 30.778807132463136, 'validation/accuracy': 0.6563588976860046, 'validation/loss': 1.7285524606704712, 'validation/bleu': 27.708385993600455, 'validation/num_examples': 3000, 'test/accuracy': 0.6662018895149231, 'test/loss': 1.6690125465393066, 'test/bleu': 27.610379038748825, 'test/num_examples': 3003, 'score': 11797.693771123886, 'total_duration': 19730.70421743393, 'accumulated_submission_time': 11797.693771123886, 'accumulated_eval_time': 7931.5147523880005, 'accumulated_logging_time': 0.39255547523498535, 'global_step': 33681, 'preemption_count': 0}), (36090, {'train/accuracy': 0.6397969126701355, 'train/loss': 1.8589222431182861, 'train/bleu': 30.72574395429576, 'validation/accuracy': 0.6573135852813721, 'validation/loss': 1.7214807271957397, 'validation/bleu': 28.437746236206387, 'validation/num_examples': 3000, 'test/accuracy': 0.6704433560371399, 'test/loss': 1.653224229812622, 'test/bleu': 28.013954515918293, 'test/num_examples': 3003, 'score': 12637.642409086227, 'total_duration': 21055.924880743027, 'accumulated_submission_time': 12637.642409086227, 'accumulated_eval_time': 8416.673792600632, 'accumulated_logging_time': 0.4278111457824707, 'global_step': 36090, 'preemption_count': 0}), (38500, {'train/accuracy': 0.6449896693229675, 'train/loss': 1.8099712133407593, 'train/bleu': 31.571335776793276, 'validation/accuracy': 0.6588138937950134, 'validation/loss': 1.7067581415176392, 'validation/bleu': 28.44258842361144, 'validation/num_examples': 3000, 'test/accuracy': 0.6685724258422852, 'test/loss': 1.6480258703231812, 'test/bleu': 27.752310460703928, 'test/num_examples': 3003, 'score': 13477.828237771988, 'total_duration': 22369.73627972603, 'accumulated_submission_time': 13477.828237771988, 'accumulated_eval_time': 8890.188046693802, 'accumulated_logging_time': 0.4626927375793457, 'global_step': 38500, 'preemption_count': 0}), (40910, {'train/accuracy': 0.6428003907203674, 'train/loss': 1.8337481021881104, 'train/bleu': 31.276876161097288, 'validation/accuracy': 0.6606861352920532, 'validation/loss': 1.705464482307434, 'validation/bleu': 28.587942244966534, 'validation/num_examples': 3000, 'test/accuracy': 0.6722212433815002, 'test/loss': 1.6383522748947144, 'test/bleu': 28.249942435103367, 'test/num_examples': 3003, 'score': 14318.037878036499, 'total_duration': 23677.41546010971, 'accumulated_submission_time': 14318.037878036499, 'accumulated_eval_time': 9357.548652887344, 'accumulated_logging_time': 0.4925673007965088, 'global_step': 40910, 'preemption_count': 0}), (43320, {'train/accuracy': 0.6413927674293518, 'train/loss': 1.8486982583999634, 'train/bleu': 30.779487617140454, 'validation/accuracy': 0.6622360348701477, 'validation/loss': 1.69834566116333, 'validation/bleu': 28.52823324647621, 'validation/num_examples': 3000, 'test/accuracy': 0.6720702052116394, 'test/loss': 1.6318928003311157, 'test/bleu': 28.10957228259788, 'test/num_examples': 3003, 'score': 15158.246175765991, 'total_duration': 25011.8331720829, 'accumulated_submission_time': 15158.246175765991, 'accumulated_eval_time': 9851.644168376923, 'accumulated_logging_time': 0.5287151336669922, 'global_step': 43320, 'preemption_count': 0}), (45730, {'train/accuracy': 0.642379641532898, 'train/loss': 1.8306187391281128, 'train/bleu': 31.599102572516422, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.6910277605056763, 'validation/bleu': 28.548053027735303, 'validation/num_examples': 3000, 'test/accuracy': 0.6729068756103516, 'test/loss': 1.6298214197158813, 'test/bleu': 27.98403783607923, 'test/num_examples': 3003, 'score': 15998.48146367073, 'total_duration': 26347.59620976448, 'accumulated_submission_time': 15998.48146367073, 'accumulated_eval_time': 10347.062133073807, 'accumulated_logging_time': 0.5609724521636963, 'global_step': 45730, 'preemption_count': 0}), (48140, {'train/accuracy': 0.643227219581604, 'train/loss': 1.8408466577529907, 'train/bleu': 31.4491060968431, 'validation/accuracy': 0.6614425182342529, 'validation/loss': 1.6888902187347412, 'validation/bleu': 28.538099099304283, 'validation/num_examples': 3000, 'test/accuracy': 0.674429178237915, 'test/loss': 1.6186769008636475, 'test/bleu': 28.14769900835641, 'test/num_examples': 3003, 'score': 16838.670334100723, 'total_duration': 27669.10016155243, 'accumulated_submission_time': 16838.670334100723, 'accumulated_eval_time': 10828.267130374908, 'accumulated_logging_time': 0.5938632488250732, 'global_step': 48140, 'preemption_count': 0}), (50549, {'train/accuracy': 0.6617670655250549, 'train/loss': 1.7062171697616577, 'train/bleu': 32.493355001995035, 'validation/accuracy': 0.6642819046974182, 'validation/loss': 1.6819127798080444, 'validation/bleu': 28.50163118714814, 'validation/num_examples': 3000, 'test/accuracy': 0.6748009920120239, 'test/loss': 1.6188348531723022, 'test/bleu': 28.20650007439196, 'test/num_examples': 3003, 'score': 17678.634654521942, 'total_duration': 29046.603624105453, 'accumulated_submission_time': 17678.634654521942, 'accumulated_eval_time': 11365.696782827377, 'accumulated_logging_time': 0.6260786056518555, 'global_step': 50549, 'preemption_count': 0}), (52959, {'train/accuracy': 0.6417939066886902, 'train/loss': 1.8330038785934448, 'train/bleu': 31.521758366440302, 'validation/accuracy': 0.6663029789924622, 'validation/loss': 1.6756658554077148, 'validation/bleu': 28.710437477264808, 'validation/num_examples': 3000, 'test/accuracy': 0.6778455972671509, 'test/loss': 1.6047146320343018, 'test/bleu': 28.40794194410856, 'test/num_examples': 3003, 'score': 18518.534535884857, 'total_duration': 30361.687136888504, 'accumulated_submission_time': 18518.534535884857, 'accumulated_eval_time': 11840.773048400879, 'accumulated_logging_time': 0.6584103107452393, 'global_step': 52959, 'preemption_count': 0}), (55368, {'train/accuracy': 0.6438504457473755, 'train/loss': 1.8326653242111206, 'train/bleu': 31.664923167422646, 'validation/accuracy': 0.664095938205719, 'validation/loss': 1.6701427698135376, 'validation/bleu': 28.634409204856926, 'validation/num_examples': 3000, 'test/accuracy': 0.6779385209083557, 'test/loss': 1.6016871929168701, 'test/bleu': 28.4380119668229, 'test/num_examples': 3003, 'score': 19358.49755549431, 'total_duration': 31703.02485537529, 'accumulated_submission_time': 19358.49755549431, 'accumulated_eval_time': 12342.030277490616, 'accumulated_logging_time': 0.6971557140350342, 'global_step': 55368, 'preemption_count': 0}), (57777, {'train/accuracy': 0.6529799699783325, 'train/loss': 1.7605587244033813, 'train/bleu': 32.104956986509244, 'validation/accuracy': 0.6671584844589233, 'validation/loss': 1.6584866046905518, 'validation/bleu': 28.910553589606458, 'validation/num_examples': 3000, 'test/accuracy': 0.6783917546272278, 'test/loss': 1.5919768810272217, 'test/bleu': 28.49080923108632, 'test/num_examples': 3003, 'score': 20198.622362852097, 'total_duration': 33020.00865364075, 'accumulated_submission_time': 20198.622362852097, 'accumulated_eval_time': 12818.775514364243, 'accumulated_logging_time': 0.7295718193054199, 'global_step': 57777, 'preemption_count': 0}), (60187, {'train/accuracy': 0.6493332982063293, 'train/loss': 1.788055658340454, 'train/bleu': 31.857854688348205, 'validation/accuracy': 0.6688447594642639, 'validation/loss': 1.6495282649993896, 'validation/bleu': 28.90816655813359, 'validation/num_examples': 3000, 'test/accuracy': 0.6815757751464844, 'test/loss': 1.579599380493164, 'test/bleu': 28.605296024995553, 'test/num_examples': 3003, 'score': 21038.775871276855, 'total_duration': 34336.24448752403, 'accumulated_submission_time': 21038.775871276855, 'accumulated_eval_time': 13294.7455265522, 'accumulated_logging_time': 0.7640244960784912, 'global_step': 60187, 'preemption_count': 0}), (62597, {'train/accuracy': 0.6954829692840576, 'train/loss': 1.5248199701309204, 'train/bleu': 35.48335512968942, 'validation/accuracy': 0.6700350642204285, 'validation/loss': 1.6441853046417236, 'validation/bleu': 29.231370890674135, 'validation/num_examples': 3000, 'test/accuracy': 0.6829004883766174, 'test/loss': 1.5770306587219238, 'test/bleu': 28.931010286915832, 'test/num_examples': 3003, 'score': 21878.77816271782, 'total_duration': 35638.94716525078, 'accumulated_submission_time': 21878.77816271782, 'accumulated_eval_time': 13757.336695432663, 'accumulated_logging_time': 0.7973370552062988, 'global_step': 62597, 'preemption_count': 0}), (65007, {'train/accuracy': 0.6498906016349792, 'train/loss': 1.779046893119812, 'train/bleu': 32.04606986938242, 'validation/accuracy': 0.6694399118423462, 'validation/loss': 1.6440907716751099, 'validation/bleu': 28.851999438402675, 'validation/num_examples': 3000, 'test/accuracy': 0.6821103096008301, 'test/loss': 1.5699652433395386, 'test/bleu': 28.572309209482075, 'test/num_examples': 3003, 'score': 22718.93424129486, 'total_duration': 36967.4750084877, 'accumulated_submission_time': 22718.93424129486, 'accumulated_eval_time': 14245.597965955734, 'accumulated_logging_time': 0.8305079936981201, 'global_step': 65007, 'preemption_count': 0}), (67416, {'train/accuracy': 0.6513877511024475, 'train/loss': 1.778394341468811, 'train/bleu': 31.90224548614695, 'validation/accuracy': 0.6698490977287292, 'validation/loss': 1.6329776048660278, 'validation/bleu': 29.310274944733973, 'validation/num_examples': 3000, 'test/accuracy': 0.6850618720054626, 'test/loss': 1.5574126243591309, 'test/bleu': 28.808396256578643, 'test/num_examples': 3003, 'score': 23559.06813430786, 'total_duration': 38276.82657814026, 'accumulated_submission_time': 23559.06813430786, 'accumulated_eval_time': 14714.698032855988, 'accumulated_logging_time': 0.8699114322662354, 'global_step': 67416, 'preemption_count': 0}), (69825, {'train/accuracy': 0.6634706258773804, 'train/loss': 1.7029547691345215, 'train/bleu': 32.673334074985306, 'validation/accuracy': 0.6726140975952148, 'validation/loss': 1.6234630346298218, 'validation/bleu': 29.223958515493404, 'validation/num_examples': 3000, 'test/accuracy': 0.6853756308555603, 'test/loss': 1.5609307289123535, 'test/bleu': 28.84675359208095, 'test/num_examples': 3003, 'score': 24399.16579413414, 'total_duration': 39619.68317270279, 'accumulated_submission_time': 24399.16579413414, 'accumulated_eval_time': 15217.340163707733, 'accumulated_logging_time': 0.9094698429107666, 'global_step': 69825, 'preemption_count': 0}), (72234, {'train/accuracy': 0.6572171449661255, 'train/loss': 1.744814395904541, 'train/bleu': 32.34493762341491, 'validation/accuracy': 0.6746351718902588, 'validation/loss': 1.619512677192688, 'validation/bleu': 29.395188395213626, 'validation/num_examples': 3000, 'test/accuracy': 0.6883621215820312, 'test/loss': 1.5452096462249756, 'test/bleu': 29.265421368484017, 'test/num_examples': 3003, 'score': 25239.123465538025, 'total_duration': 40969.03985142708, 'accumulated_submission_time': 25239.123465538025, 'accumulated_eval_time': 15726.62624669075, 'accumulated_logging_time': 0.9434311389923096, 'global_step': 72234, 'preemption_count': 0}), (74644, {'train/accuracy': 0.6548944115638733, 'train/loss': 1.7606871128082275, 'train/bleu': 32.62063170484274, 'validation/accuracy': 0.6750690937042236, 'validation/loss': 1.6131248474121094, 'validation/bleu': 29.763167120385386, 'validation/num_examples': 3000, 'test/accuracy': 0.6876532435417175, 'test/loss': 1.5402637720108032, 'test/bleu': 29.1380813702546, 'test/num_examples': 3003, 'score': 26079.265601873398, 'total_duration': 42292.14107131958, 'accumulated_submission_time': 26079.265601873398, 'accumulated_eval_time': 16209.475015640259, 'accumulated_logging_time': 0.977283239364624, 'global_step': 74644, 'preemption_count': 0}), (77053, {'train/accuracy': 0.6639668941497803, 'train/loss': 1.694888710975647, 'train/bleu': 32.92362728757387, 'validation/accuracy': 0.6748707294464111, 'validation/loss': 1.6068533658981323, 'validation/bleu': 29.34807239685257, 'validation/num_examples': 3000, 'test/accuracy': 0.6893498301506042, 'test/loss': 1.530578374862671, 'test/bleu': 29.18517059563849, 'test/num_examples': 3003, 'score': 26919.287441253662, 'total_duration': 43667.97906756401, 'accumulated_submission_time': 26919.287441253662, 'accumulated_eval_time': 16745.17874765396, 'accumulated_logging_time': 1.0129663944244385, 'global_step': 77053, 'preemption_count': 0}), (79462, {'train/accuracy': 0.6601611971855164, 'train/loss': 1.7258120775222778, 'train/bleu': 32.68691853658534, 'validation/accuracy': 0.6769289970397949, 'validation/loss': 1.5998398065567017, 'validation/bleu': 29.752262409047173, 'validation/num_examples': 3000, 'test/accuracy': 0.6904653906822205, 'test/loss': 1.5268282890319824, 'test/bleu': 29.331326809471992, 'test/num_examples': 3003, 'score': 27759.34330010414, 'total_duration': 44991.616892814636, 'accumulated_submission_time': 27759.34330010414, 'accumulated_eval_time': 17228.647493600845, 'accumulated_logging_time': 1.0498454570770264, 'global_step': 79462, 'preemption_count': 0}), (81870, {'train/accuracy': 0.6784107685089111, 'train/loss': 1.6096141338348389, 'train/bleu': 33.84269230391496, 'validation/accuracy': 0.6793591976165771, 'validation/loss': 1.5915672779083252, 'validation/bleu': 29.80917299937564, 'validation/num_examples': 3000, 'test/accuracy': 0.6923711895942688, 'test/loss': 1.518932819366455, 'test/bleu': 29.556238156403044, 'test/num_examples': 3003, 'score': 28599.2469124794, 'total_duration': 46325.63169527054, 'accumulated_submission_time': 28599.2469124794, 'accumulated_eval_time': 17722.64498400688, 'accumulated_logging_time': 1.086064338684082, 'global_step': 81870, 'preemption_count': 0}), (84279, {'train/accuracy': 0.6645780205726624, 'train/loss': 1.6899528503417969, 'train/bleu': 32.87321657993862, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.5834990739822388, 'validation/bleu': 29.55673975736919, 'validation/num_examples': 3000, 'test/accuracy': 0.6955435872077942, 'test/loss': 1.5063549280166626, 'test/bleu': 29.769185359462774, 'test/num_examples': 3003, 'score': 29439.365225553513, 'total_duration': 47654.22263765335, 'accumulated_submission_time': 29439.365225553513, 'accumulated_eval_time': 18211.000927448273, 'accumulated_logging_time': 1.1236786842346191, 'global_step': 84279, 'preemption_count': 0}), (86688, {'train/accuracy': 0.6623689532279968, 'train/loss': 1.7069313526153564, 'train/bleu': 33.333061211211565, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.5803080797195435, 'validation/bleu': 29.87338660571638, 'validation/num_examples': 3000, 'test/accuracy': 0.6947998404502869, 'test/loss': 1.4972296953201294, 'test/bleu': 30.07424609365526, 'test/num_examples': 3003, 'score': 30279.430206775665, 'total_duration': 48972.770466566086, 'accumulated_submission_time': 30279.430206775665, 'accumulated_eval_time': 18689.371471881866, 'accumulated_logging_time': 1.1595537662506104, 'global_step': 86688, 'preemption_count': 0}), (89097, {'train/accuracy': 0.6741820573806763, 'train/loss': 1.636649250984192, 'train/bleu': 33.7021119458403, 'validation/accuracy': 0.6820126175880432, 'validation/loss': 1.5686326026916504, 'validation/bleu': 29.77797063662827, 'validation/num_examples': 3000, 'test/accuracy': 0.6962872743606567, 'test/loss': 1.4902687072753906, 'test/bleu': 29.97291099023428, 'test/num_examples': 3003, 'score': 31119.458785533905, 'total_duration': 50296.33786916733, 'accumulated_submission_time': 31119.458785533905, 'accumulated_eval_time': 19172.79546570778, 'accumulated_logging_time': 1.1961100101470947, 'global_step': 89097, 'preemption_count': 0}), (91506, {'train/accuracy': 0.667885959148407, 'train/loss': 1.6756535768508911, 'train/bleu': 33.10366408069063, 'validation/accuracy': 0.6826573610305786, 'validation/loss': 1.5624314546585083, 'validation/bleu': 30.156964677978845, 'validation/num_examples': 3000, 'test/accuracy': 0.6995874643325806, 'test/loss': 1.481274127960205, 'test/bleu': 30.325125321300153, 'test/num_examples': 3003, 'score': 31959.54501605034, 'total_duration': 51625.13171696663, 'accumulated_submission_time': 31959.54501605034, 'accumulated_eval_time': 19661.390310049057, 'accumulated_logging_time': 1.2334024906158447, 'global_step': 91506, 'preemption_count': 0}), (93915, {'train/accuracy': 0.7043598890304565, 'train/loss': 1.4692833423614502, 'train/bleu': 36.35120024978537, 'validation/accuracy': 0.6835501194000244, 'validation/loss': 1.558236002922058, 'validation/bleu': 30.0273109673757, 'validation/num_examples': 3000, 'test/accuracy': 0.7001568675041199, 'test/loss': 1.4738965034484863, 'test/bleu': 30.082391755845606, 'test/num_examples': 3003, 'score': 32799.61296272278, 'total_duration': 52936.32979607582, 'accumulated_submission_time': 32799.61296272278, 'accumulated_eval_time': 20132.38963317871, 'accumulated_logging_time': 1.2875926494598389, 'global_step': 93915, 'preemption_count': 0}), (96324, {'train/accuracy': 0.6754699945449829, 'train/loss': 1.6206367015838623, 'train/bleu': 33.64124946940375, 'validation/accuracy': 0.6841948628425598, 'validation/loss': 1.5504695177078247, 'validation/bleu': 30.027689307621454, 'validation/num_examples': 3000, 'test/accuracy': 0.7004590034484863, 'test/loss': 1.4679011106491089, 'test/bleu': 30.379588836077428, 'test/num_examples': 3003, 'score': 33639.83898591995, 'total_duration': 54311.994121551514, 'accumulated_submission_time': 33639.83898591995, 'accumulated_eval_time': 20667.71193766594, 'accumulated_logging_time': 1.3252015113830566, 'global_step': 96324, 'preemption_count': 0}), (98734, {'train/accuracy': 0.6730888485908508, 'train/loss': 1.640097737312317, 'train/bleu': 34.10313197148955, 'validation/accuracy': 0.6855587363243103, 'validation/loss': 1.5443347692489624, 'validation/bleu': 30.445706302183336, 'validation/num_examples': 3000, 'test/accuracy': 0.700714647769928, 'test/loss': 1.4622043371200562, 'test/bleu': 30.23568171788698, 'test/num_examples': 3003, 'score': 34480.07180213928, 'total_duration': 55677.57796263695, 'accumulated_submission_time': 34480.07180213928, 'accumulated_eval_time': 21192.948457717896, 'accumulated_logging_time': 1.3619556427001953, 'global_step': 98734, 'preemption_count': 0}), (101142, {'train/accuracy': 0.686933696269989, 'train/loss': 1.5602630376815796, 'train/bleu': 34.618318349974814, 'validation/accuracy': 0.6874186396598816, 'validation/loss': 1.5365707874298096, 'validation/bleu': 30.188926869006934, 'validation/num_examples': 3000, 'test/accuracy': 0.702934205532074, 'test/loss': 1.454554796218872, 'test/bleu': 30.4668122978629, 'test/num_examples': 3003, 'score': 35320.00034117699, 'total_duration': 57010.319242954254, 'accumulated_submission_time': 35320.00034117699, 'accumulated_eval_time': 21685.645532608032, 'accumulated_logging_time': 1.3999087810516357, 'global_step': 101142, 'preemption_count': 0}), (103551, {'train/accuracy': 0.6821432709693909, 'train/loss': 1.5882389545440674, 'train/bleu': 34.26272973822794, 'validation/accuracy': 0.688956081867218, 'validation/loss': 1.5335873365402222, 'validation/bleu': 30.360145618817835, 'validation/num_examples': 3000, 'test/accuracy': 0.7046424150466919, 'test/loss': 1.4481626749038696, 'test/bleu': 30.654866411054087, 'test/num_examples': 3003, 'score': 36160.209800720215, 'total_duration': 58333.0024998188, 'accumulated_submission_time': 36160.209800720215, 'accumulated_eval_time': 22168.00412297249, 'accumulated_logging_time': 1.4385159015655518, 'global_step': 103551, 'preemption_count': 0}), (105959, {'train/accuracy': 0.6814278364181519, 'train/loss': 1.5989018678665161, 'train/bleu': 34.65526969945151, 'validation/accuracy': 0.6893280744552612, 'validation/loss': 1.5266581773757935, 'validation/bleu': 30.57867552999859, 'validation/num_examples': 3000, 'test/accuracy': 0.7055836319923401, 'test/loss': 1.4425654411315918, 'test/bleu': 30.785481276223113, 'test/num_examples': 3003, 'score': 37000.21959018707, 'total_duration': 59685.23624706268, 'accumulated_submission_time': 37000.21959018707, 'accumulated_eval_time': 22680.11134338379, 'accumulated_logging_time': 1.4766552448272705, 'global_step': 105959, 'preemption_count': 0}), (108368, {'train/accuracy': 0.6921325922012329, 'train/loss': 1.5298690795898438, 'train/bleu': 35.40294324144317, 'validation/accuracy': 0.6879394054412842, 'validation/loss': 1.5276821851730347, 'validation/bleu': 30.47344674934183, 'validation/num_examples': 3000, 'test/accuracy': 0.7045029401779175, 'test/loss': 1.443967342376709, 'test/bleu': 30.56989330909604, 'test/num_examples': 3003, 'score': 37840.398203372955, 'total_duration': 61025.62011170387, 'accumulated_submission_time': 37840.398203372955, 'accumulated_eval_time': 23180.200609207153, 'accumulated_logging_time': 1.51641845703125, 'global_step': 108368, 'preemption_count': 0}), (110776, {'train/accuracy': 0.6881821751594543, 'train/loss': 1.555271863937378, 'train/bleu': 34.94405351615512, 'validation/accuracy': 0.6900472044944763, 'validation/loss': 1.5213935375213623, 'validation/bleu': 30.51655914551938, 'validation/num_examples': 3000, 'test/accuracy': 0.705258309841156, 'test/loss': 1.4395705461502075, 'test/bleu': 30.330988152869008, 'test/num_examples': 3003, 'score': 38680.33823132515, 'total_duration': 62356.54861474037, 'accumulated_submission_time': 38680.33823132515, 'accumulated_eval_time': 23671.070686101913, 'accumulated_logging_time': 1.5580275058746338, 'global_step': 110776, 'preemption_count': 0}), (113185, {'train/accuracy': 0.7068272233009338, 'train/loss': 1.4597846269607544, 'train/bleu': 36.19503096365993, 'validation/accuracy': 0.6921550631523132, 'validation/loss': 1.5146585702896118, 'validation/bleu': 30.680754105466335, 'validation/num_examples': 3000, 'test/accuracy': 0.7078961133956909, 'test/loss': 1.430131435394287, 'test/bleu': 30.702009793502405, 'test/num_examples': 3003, 'score': 39520.55152916908, 'total_duration': 63693.93018531799, 'accumulated_submission_time': 39520.55152916908, 'accumulated_eval_time': 24168.117631196976, 'accumulated_logging_time': 1.6014611721038818, 'global_step': 113185, 'preemption_count': 0}), (115594, {'train/accuracy': 0.6958543062210083, 'train/loss': 1.5099217891693115, 'train/bleu': 35.537028715325654, 'validation/accuracy': 0.6910639405250549, 'validation/loss': 1.5157564878463745, 'validation/bleu': 30.634864207978154, 'validation/num_examples': 3000, 'test/accuracy': 0.7095230221748352, 'test/loss': 1.4238560199737549, 'test/bleu': 31.016669589638788, 'test/num_examples': 3003, 'score': 40360.71843266487, 'total_duration': 65041.172853946686, 'accumulated_submission_time': 40360.71843266487, 'accumulated_eval_time': 24675.07525396347, 'accumulated_logging_time': 1.6436638832092285, 'global_step': 115594, 'preemption_count': 0}), (118003, {'train/accuracy': 0.6981893181800842, 'train/loss': 1.5048353672027588, 'train/bleu': 35.49911535816573, 'validation/accuracy': 0.6911135315895081, 'validation/loss': 1.5123188495635986, 'validation/bleu': 30.810753206662906, 'validation/num_examples': 3000, 'test/accuracy': 0.70961594581604, 'test/loss': 1.4236197471618652, 'test/bleu': 31.12338429193865, 'test/num_examples': 3003, 'score': 41200.7603263855, 'total_duration': 66375.6348798275, 'accumulated_submission_time': 41200.7603263855, 'accumulated_eval_time': 25169.378446102142, 'accumulated_logging_time': 1.6847825050354004, 'global_step': 118003, 'preemption_count': 0}), (120412, {'train/accuracy': 0.7060102224349976, 'train/loss': 1.4636739492416382, 'train/bleu': 36.326105954772956, 'validation/accuracy': 0.6930354237556458, 'validation/loss': 1.5079798698425293, 'validation/bleu': 30.81666176363183, 'validation/num_examples': 3000, 'test/accuracy': 0.7101737260818481, 'test/loss': 1.4174529314041138, 'test/bleu': 31.0455792076282, 'test/num_examples': 3003, 'score': 42040.92917585373, 'total_duration': 67714.37622475624, 'accumulated_submission_time': 42040.92917585373, 'accumulated_eval_time': 25667.826288461685, 'accumulated_logging_time': 1.732635259628296, 'global_step': 120412, 'preemption_count': 0}), (122820, {'train/accuracy': 0.7075486779212952, 'train/loss': 1.4473143815994263, 'train/bleu': 36.116525022338294, 'validation/accuracy': 0.6921798586845398, 'validation/loss': 1.5096970796585083, 'validation/bleu': 30.694070873645327, 'validation/num_examples': 3000, 'test/accuracy': 0.7105455994606018, 'test/loss': 1.418480634689331, 'test/bleu': 31.13267295113105, 'test/num_examples': 3003, 'score': 42881.0361392498, 'total_duration': 69050.65186357498, 'accumulated_submission_time': 42881.0361392498, 'accumulated_eval_time': 26163.876004457474, 'accumulated_logging_time': 1.7744617462158203, 'global_step': 122820, 'preemption_count': 0}), (125227, {'train/accuracy': 0.7090739607810974, 'train/loss': 1.4433223009109497, 'train/bleu': 36.57344895473294, 'validation/accuracy': 0.6934942007064819, 'validation/loss': 1.5054748058319092, 'validation/bleu': 30.817558697381532, 'validation/num_examples': 3000, 'test/accuracy': 0.7112312316894531, 'test/loss': 1.4143809080123901, 'test/bleu': 31.124645847204345, 'test/num_examples': 3003, 'score': 43720.94148516655, 'total_duration': 70395.96740603447, 'accumulated_submission_time': 43720.94148516655, 'accumulated_eval_time': 26669.166478395462, 'accumulated_logging_time': 1.816622018814087, 'global_step': 125227, 'preemption_count': 0}), (127635, {'train/accuracy': 0.7085887789726257, 'train/loss': 1.438305377960205, 'train/bleu': 36.61241848155233, 'validation/accuracy': 0.6935437917709351, 'validation/loss': 1.5044677257537842, 'validation/bleu': 30.862560161613207, 'validation/num_examples': 3000, 'test/accuracy': 0.7111498713493347, 'test/loss': 1.4130109548568726, 'test/bleu': 31.248859219822087, 'test/num_examples': 3003, 'score': 44561.00401854515, 'total_duration': 71725.01059532166, 'accumulated_submission_time': 44561.00401854515, 'accumulated_eval_time': 27158.02793073654, 'accumulated_logging_time': 1.858067274093628, 'global_step': 127635, 'preemption_count': 0}), (130043, {'train/accuracy': 0.7091085910797119, 'train/loss': 1.4369813203811646, 'train/bleu': 36.998641531348035, 'validation/accuracy': 0.6933081746101379, 'validation/loss': 1.504925012588501, 'validation/bleu': 30.757680438251448, 'validation/num_examples': 3000, 'test/accuracy': 0.7116844058036804, 'test/loss': 1.4134386777877808, 'test/bleu': 31.10714427248701, 'test/num_examples': 3003, 'score': 45400.904284238815, 'total_duration': 73065.42194890976, 'accumulated_submission_time': 45400.904284238815, 'accumulated_eval_time': 27658.42151737213, 'accumulated_logging_time': 1.9005036354064941, 'global_step': 130043, 'preemption_count': 0}), (132451, {'train/accuracy': 0.7096970677375793, 'train/loss': 1.4432387351989746, 'train/bleu': 36.137919749675284, 'validation/accuracy': 0.693717360496521, 'validation/loss': 1.504041075706482, 'validation/bleu': 30.865012416464875, 'validation/num_examples': 3000, 'test/accuracy': 0.7116495370864868, 'test/loss': 1.4128319025039673, 'test/bleu': 31.196985520739922, 'test/num_examples': 3003, 'score': 46241.0879445076, 'total_duration': 74414.16377663612, 'accumulated_submission_time': 46241.0879445076, 'accumulated_eval_time': 28166.858474493027, 'accumulated_logging_time': 1.944511890411377, 'global_step': 132451, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7093749642372131, 'train/loss': 1.4428577423095703, 'train/bleu': 36.541089722712044, 'validation/accuracy': 0.693630576133728, 'validation/loss': 1.5042879581451416, 'validation/bleu': 30.869243773906362, 'validation/num_examples': 3000, 'test/accuracy': 0.7118006348609924, 'test/loss': 1.413087010383606, 'test/bleu': 31.18247574009484, 'test/num_examples': 3003, 'score': 46548.72388243675, 'total_duration': 75229.0163321495, 'accumulated_submission_time': 46548.72388243675, 'accumulated_eval_time': 28674.002204179764, 'accumulated_logging_time': 1.9894163608551025, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0207 07:52:28.211163 139785736898368 submission_runner.py:586] Timing: 46548.72388243675
I0207 07:52:28.211220 139785736898368 submission_runner.py:588] Total number of evals: 57
I0207 07:52:28.211264 139785736898368 submission_runner.py:589] ====================
I0207 07:52:28.211309 139785736898368 submission_runner.py:542] Using RNG seed 3586669017
I0207 07:52:28.212955 139785736898368 submission_runner.py:551] --- Tuning run 2/5 ---
I0207 07:52:28.213064 139785736898368 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2.
I0207 07:52:28.213514 139785736898368 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2/hparams.json.
I0207 07:52:28.214286 139785736898368 submission_runner.py:206] Initializing dataset.
I0207 07:52:28.217016 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 07:52:28.220014 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0207 07:52:28.257583 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 07:52:28.851382 139785736898368 submission_runner.py:213] Initializing model.
I0207 07:52:35.317793 139785736898368 submission_runner.py:255] Initializing optimizer.
I0207 07:52:36.130628 139785736898368 submission_runner.py:262] Initializing metrics bundle.
I0207 07:52:36.130795 139785736898368 submission_runner.py:280] Initializing checkpoint and logger.
I0207 07:52:36.131618 139785736898368 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2 with prefix checkpoint_
I0207 07:52:36.131744 139785736898368 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2/meta_data_0.json.
I0207 07:52:36.131956 139785736898368 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0207 07:52:36.132021 139785736898368 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0207 07:52:36.604094 139785736898368 logger_utils.py:220] Unable to record git information. Continuing without it.
I0207 07:52:37.051574 139785736898368 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2/flags_0.json.
I0207 07:52:37.058667 139785736898368 submission_runner.py:314] Starting training loop.
I0207 07:53:04.317960 139615972296448 logging_writer.py:48] [0] global_step=0, grad_norm=4.562565803527832, loss=11.047717094421387
I0207 07:53:04.327276 139785736898368 spec.py:321] Evaluating on the training split.
I0207 07:53:07.023450 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 07:57:54.222270 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 07:57:56.942719 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:02:43.626272 139785736898368 spec.py:349] Evaluating on the test split.
I0207 08:02:46.345831 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:07:33.141278 139785736898368 submission_runner.py:408] Time since start: 896.08s, 	Step: 1, 	{'train/accuracy': 0.0006168890395201743, 'train/loss': 11.06764030456543, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.26857042312622, 'total_duration': 896.0825242996216, 'accumulated_submission_time': 27.26857042312622, 'accumulated_eval_time': 868.8139111995697, 'accumulated_logging_time': 0}
I0207 08:07:33.150542 139615980689152 logging_writer.py:48] [1] accumulated_eval_time=868.813911, accumulated_logging_time=0, accumulated_submission_time=27.268570, global_step=1, preemption_count=0, score=27.268570, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.041826, test/num_examples=3003, total_duration=896.082524, train/accuracy=0.000617, train/bleu=0.000000, train/loss=11.067640, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.036646, validation/num_examples=3000
I0207 08:08:08.172482 139615972296448 logging_writer.py:48] [100] global_step=100, grad_norm=0.25927528738975525, loss=9.059920310974121
I0207 08:08:43.163244 139615980689152 logging_writer.py:48] [200] global_step=200, grad_norm=0.22243459522724152, loss=8.706313133239746
I0207 08:09:18.136901 139615972296448 logging_writer.py:48] [300] global_step=300, grad_norm=1.1062071323394775, loss=8.366786003112793
I0207 08:09:53.127617 139615980689152 logging_writer.py:48] [400] global_step=400, grad_norm=0.988160252571106, loss=8.043668746948242
I0207 08:10:28.127531 139615972296448 logging_writer.py:48] [500] global_step=500, grad_norm=0.719127893447876, loss=7.834822654724121
I0207 08:11:03.121895 139615980689152 logging_writer.py:48] [600] global_step=600, grad_norm=0.8250634074211121, loss=7.67850399017334
I0207 08:11:38.091987 139615972296448 logging_writer.py:48] [700] global_step=700, grad_norm=0.699671745300293, loss=7.466832637786865
I0207 08:12:13.076283 139615980689152 logging_writer.py:48] [800] global_step=800, grad_norm=0.7018195986747742, loss=7.238663673400879
I0207 08:12:48.056414 139615972296448 logging_writer.py:48] [900] global_step=900, grad_norm=0.5786747932434082, loss=7.117827415466309
I0207 08:13:23.014739 139615980689152 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.47409942746162415, loss=6.988266944885254
I0207 08:13:57.993105 139615972296448 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5683264136314392, loss=6.843958377838135
I0207 08:14:32.950422 139615980689152 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5753018260002136, loss=6.727996349334717
I0207 08:15:07.938285 139615972296448 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5221524834632874, loss=6.638073444366455
I0207 08:15:42.920112 139615980689152 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7020856738090515, loss=6.464896202087402
I0207 08:16:17.909742 139615972296448 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.624664306640625, loss=6.395724296569824
I0207 08:16:52.873804 139615980689152 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6273941397666931, loss=6.3196330070495605
I0207 08:17:27.872024 139615972296448 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6578382253646851, loss=6.2348785400390625
I0207 08:18:02.864653 139615980689152 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5571644306182861, loss=6.107324123382568
I0207 08:18:37.851688 139615972296448 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7522433400154114, loss=6.035810947418213
I0207 08:19:12.860863 139615980689152 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6864017844200134, loss=5.946208477020264
I0207 08:19:47.895328 139615972296448 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6393540501594543, loss=5.823185443878174
I0207 08:20:22.917085 139615980689152 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.619179904460907, loss=5.766654014587402
I0207 08:20:57.885653 139615972296448 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5642418265342712, loss=5.658942222595215
I0207 08:21:32.876796 139615980689152 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7018256783485413, loss=5.5513916015625
I0207 08:21:33.303376 139785736898368 spec.py:321] Evaluating on the training split.
I0207 08:21:36.312272 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:25:29.493474 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 08:25:32.204359 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:29:22.429453 139785736898368 spec.py:349] Evaluating on the test split.
I0207 08:29:25.152592 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:33:03.870177 139785736898368 submission_runner.py:408] Time since start: 2426.81s, 	Step: 2403, 	{'train/accuracy': 0.42342033982276917, 'train/loss': 3.922987222671509, 'train/bleu': 15.385226949250677, 'validation/accuracy': 0.40709972381591797, 'validation/loss': 4.05037260055542, 'validation/bleu': 10.213285356315824, 'validation/num_examples': 3000, 'test/accuracy': 0.39780375361442566, 'test/loss': 4.211139678955078, 'test/bleu': 8.912712367020626, 'test/num_examples': 3003, 'score': 867.3339140415192, 'total_duration': 2426.8114387989044, 'accumulated_submission_time': 867.3339140415192, 'accumulated_eval_time': 1559.3806607723236, 'accumulated_logging_time': 0.020984649658203125}
I0207 08:33:03.885201 139615972296448 logging_writer.py:48] [2403] accumulated_eval_time=1559.380661, accumulated_logging_time=0.020985, accumulated_submission_time=867.333914, global_step=2403, preemption_count=0, score=867.333914, test/accuracy=0.397804, test/bleu=8.912712, test/loss=4.211140, test/num_examples=3003, total_duration=2426.811439, train/accuracy=0.423420, train/bleu=15.385227, train/loss=3.922987, validation/accuracy=0.407100, validation/bleu=10.213285, validation/loss=4.050373, validation/num_examples=3000
I0207 08:33:38.226652 139615980689152 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6308470368385315, loss=5.4484758377075195
I0207 08:34:13.233607 139615972296448 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6625711917877197, loss=5.464380741119385
I0207 08:34:48.230376 139615980689152 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6824512481689453, loss=5.388047218322754
I0207 08:35:23.218500 139615972296448 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.47328174114227295, loss=5.289172172546387
I0207 08:35:58.214415 139615980689152 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.48079198598861694, loss=5.283470153808594
I0207 08:36:33.203155 139615972296448 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5490552186965942, loss=5.217790126800537
I0207 08:37:08.190860 139615980689152 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6030886769294739, loss=5.089740753173828
I0207 08:37:43.200252 139615972296448 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6455107927322388, loss=5.184981346130371
I0207 08:38:18.261610 139615980689152 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5068755149841309, loss=5.086654186248779
I0207 08:38:53.276459 139615972296448 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6008200645446777, loss=5.032498359680176
I0207 08:39:28.289317 139615980689152 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.4476433992385864, loss=4.960402965545654
I0207 08:40:03.279025 139615972296448 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.44487279653549194, loss=5.09890079498291
I0207 08:40:38.248210 139615980689152 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.43890416622161865, loss=4.930078983306885
I0207 08:41:13.224396 139615972296448 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.4971039295196533, loss=4.931765079498291
I0207 08:41:48.202582 139615980689152 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.43563660979270935, loss=4.8763837814331055
I0207 08:42:23.157276 139615972296448 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3819674551486969, loss=4.917715072631836
I0207 08:42:58.111031 139615980689152 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.39506441354751587, loss=4.738011837005615
I0207 08:43:33.067012 139615972296448 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.42104026675224304, loss=4.818382263183594
I0207 08:44:08.002908 139615980689152 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3792873024940491, loss=4.8655009269714355
I0207 08:44:42.961612 139615972296448 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3951489329338074, loss=4.778924942016602
I0207 08:45:17.931568 139615980689152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3723910450935364, loss=4.727339744567871
I0207 08:45:52.888871 139615972296448 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4490163028240204, loss=4.760716438293457
I0207 08:46:27.863079 139615980689152 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.38785770535469055, loss=4.691391468048096
I0207 08:47:02.951589 139615972296448 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.38261470198631287, loss=4.756341934204102
I0207 08:47:04.078732 139785736898368 spec.py:321] Evaluating on the training split.
I0207 08:47:07.082570 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:49:39.410251 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 08:49:42.131496 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:52:16.981663 139785736898368 spec.py:349] Evaluating on the test split.
I0207 08:52:19.687887 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 08:54:45.941372 139785736898368 submission_runner.py:408] Time since start: 3728.88s, 	Step: 4805, 	{'train/accuracy': 0.5433375835418701, 'train/loss': 2.8315465450286865, 'train/bleu': 23.489781442927157, 'validation/accuracy': 0.5458456873893738, 'validation/loss': 2.7822792530059814, 'validation/bleu': 19.935954988012053, 'validation/num_examples': 3000, 'test/accuracy': 0.5467666387557983, 'test/loss': 2.8108057975769043, 'test/bleu': 18.429889335573424, 'test/num_examples': 3003, 'score': 1707.4406251907349, 'total_duration': 3728.882633447647, 'accumulated_submission_time': 1707.4406251907349, 'accumulated_eval_time': 2021.2432608604431, 'accumulated_logging_time': 0.04601240158081055}
I0207 08:54:45.956580 139615980689152 logging_writer.py:48] [4805] accumulated_eval_time=2021.243261, accumulated_logging_time=0.046012, accumulated_submission_time=1707.440625, global_step=4805, preemption_count=0, score=1707.440625, test/accuracy=0.546767, test/bleu=18.429889, test/loss=2.810806, test/num_examples=3003, total_duration=3728.882633, train/accuracy=0.543338, train/bleu=23.489781, train/loss=2.831547, validation/accuracy=0.545846, validation/bleu=19.935955, validation/loss=2.782279, validation/num_examples=3000
I0207 08:55:19.532776 139615972296448 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3923661708831787, loss=4.670768737792969
I0207 08:55:54.489515 139615980689152 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.4161040186882019, loss=4.668042182922363
I0207 08:56:29.457641 139615972296448 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3324193060398102, loss=4.699140548706055
I0207 08:57:04.414973 139615980689152 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3320614993572235, loss=4.657759666442871
I0207 08:57:39.365752 139615972296448 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.36451295018196106, loss=4.661422252655029
I0207 08:58:14.322271 139615980689152 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.31959861516952515, loss=4.639490604400635
I0207 08:58:49.273654 139615972296448 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.2957892119884491, loss=4.57316780090332
I0207 08:59:24.229711 139615980689152 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.3433840870857239, loss=4.584693431854248
I0207 08:59:59.192089 139615972296448 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.2964695394039154, loss=4.62994384765625
I0207 09:00:34.175414 139615980689152 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.31083035469055176, loss=4.589077472686768
I0207 09:01:09.128442 139615972296448 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.33029118180274963, loss=4.616156101226807
I0207 09:01:44.115041 139615980689152 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.3440317213535309, loss=4.586411476135254
I0207 09:02:19.062324 139615972296448 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.32795166969299316, loss=4.533483505249023
I0207 09:02:54.008644 139615980689152 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.30047494173049927, loss=4.5583271980285645
I0207 09:03:28.976501 139615972296448 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.29458609223365784, loss=4.506972789764404
I0207 09:04:03.927637 139615980689152 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.25952666997909546, loss=4.555328845977783
I0207 09:04:38.873715 139615972296448 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.326961487531662, loss=4.548391819000244
I0207 09:05:13.796823 139615980689152 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2685002386569977, loss=4.53261137008667
I0207 09:05:48.772621 139615972296448 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.2664487957954407, loss=4.488704681396484
I0207 09:06:23.716474 139615980689152 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.25108253955841064, loss=4.478877067565918
I0207 09:06:58.666975 139615972296448 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.29644903540611267, loss=4.497158527374268
I0207 09:07:33.597635 139615980689152 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.2922822833061218, loss=4.474088191986084
I0207 09:08:08.536881 139615972296448 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.2642240524291992, loss=4.444921493530273
I0207 09:08:43.484313 139615980689152 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.23286175727844238, loss=4.487450122833252
I0207 09:08:46.005437 139785736898368 spec.py:321] Evaluating on the training split.
I0207 09:08:49.010320 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:11:20.808344 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 09:11:23.526500 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:14:01.465225 139785736898368 spec.py:349] Evaluating on the test split.
I0207 09:14:04.185322 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:16:31.240649 139785736898368 submission_runner.py:408] Time since start: 5034.18s, 	Step: 7209, 	{'train/accuracy': 0.5829526782035828, 'train/loss': 2.4544622898101807, 'train/bleu': 26.772562741454795, 'validation/accuracy': 0.5922927260398865, 'validation/loss': 2.3894851207733154, 'validation/bleu': 23.585131664602347, 'validation/num_examples': 3000, 'test/accuracy': 0.5943059921264648, 'test/loss': 2.385735273361206, 'test/bleu': 22.20622453217967, 'test/num_examples': 3003, 'score': 2547.405416727066, 'total_duration': 5034.181886911392, 'accumulated_submission_time': 2547.405416727066, 'accumulated_eval_time': 2486.4783968925476, 'accumulated_logging_time': 0.07095670700073242}
I0207 09:16:31.256595 139615972296448 logging_writer.py:48] [7209] accumulated_eval_time=2486.478397, accumulated_logging_time=0.070957, accumulated_submission_time=2547.405417, global_step=7209, preemption_count=0, score=2547.405417, test/accuracy=0.594306, test/bleu=22.206225, test/loss=2.385735, test/num_examples=3003, total_duration=5034.181887, train/accuracy=0.582953, train/bleu=26.772563, train/loss=2.454462, validation/accuracy=0.592293, validation/bleu=23.585132, validation/loss=2.389485, validation/num_examples=3000
I0207 09:17:03.425342 139615980689152 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2588912844657898, loss=4.4064555168151855
I0207 09:17:38.370260 139615972296448 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.23614126443862915, loss=4.4207682609558105
I0207 09:18:13.340716 139615980689152 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.2219221293926239, loss=4.357372283935547
I0207 09:18:48.280056 139615972296448 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.21402500569820404, loss=4.476132869720459
I0207 09:19:23.228874 139615980689152 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.28619956970214844, loss=4.331047534942627
I0207 09:19:58.183032 139615972296448 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.22106815874576569, loss=4.305487155914307
I0207 09:20:33.124760 139615980689152 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.2713673412799835, loss=4.347011089324951
I0207 09:21:08.072486 139615972296448 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.22682389616966248, loss=4.470519065856934
I0207 09:21:42.998680 139615980689152 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.22202152013778687, loss=4.3219709396362305
I0207 09:22:17.974809 139615972296448 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.22582006454467773, loss=4.378681182861328
I0207 09:22:52.920348 139615980689152 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.2261260449886322, loss=4.348705768585205
I0207 09:23:27.849014 139615972296448 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.20336772501468658, loss=4.325372695922852
I0207 09:24:02.807087 139615980689152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.20060332119464874, loss=4.398250579833984
I0207 09:24:37.781591 139615972296448 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.2074318379163742, loss=4.335174083709717
I0207 09:25:12.731528 139615980689152 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.20439858734607697, loss=4.335958957672119
I0207 09:25:47.673308 139615972296448 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.2291060835123062, loss=4.387130260467529
I0207 09:26:22.594487 139615980689152 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.20762187242507935, loss=4.289025783538818
I0207 09:26:57.506491 139615972296448 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1852380782365799, loss=4.2882161140441895
I0207 09:27:32.448771 139615980689152 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.1894613653421402, loss=4.331228733062744
I0207 09:28:07.387890 139615972296448 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.24029742181301117, loss=4.393800258636475
I0207 09:28:42.322262 139615980689152 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.21580249071121216, loss=4.252355575561523
I0207 09:29:17.236506 139615972296448 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.19036518037319183, loss=4.346597194671631
I0207 09:29:52.160608 139615980689152 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.18866699934005737, loss=4.309164047241211
I0207 09:30:27.058520 139615972296448 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2743057310581207, loss=4.279811859130859
I0207 09:30:31.321547 139785736898368 spec.py:321] Evaluating on the training split.
I0207 09:30:34.318042 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:33:19.993141 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 09:33:22.707756 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:36:03.322484 139785736898368 spec.py:349] Evaluating on the test split.
I0207 09:36:06.057132 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:38:32.624956 139785736898368 submission_runner.py:408] Time since start: 6355.57s, 	Step: 9614, 	{'train/accuracy': 0.6014232039451599, 'train/loss': 2.2858991622924805, 'train/bleu': 28.633667442316284, 'validation/accuracy': 0.6167809367179871, 'validation/loss': 2.163081407546997, 'validation/bleu': 24.976861417148402, 'validation/num_examples': 3000, 'test/accuracy': 0.6229039430618286, 'test/loss': 2.132373332977295, 'test/bleu': 23.902748515407488, 'test/num_examples': 3003, 'score': 3387.3830687999725, 'total_duration': 6355.566185951233, 'accumulated_submission_time': 3387.3830687999725, 'accumulated_eval_time': 2967.781727552414, 'accumulated_logging_time': 0.09715795516967773}
I0207 09:38:32.643028 139615980689152 logging_writer.py:48] [9614] accumulated_eval_time=2967.781728, accumulated_logging_time=0.097158, accumulated_submission_time=3387.383069, global_step=9614, preemption_count=0, score=3387.383069, test/accuracy=0.622904, test/bleu=23.902749, test/loss=2.132373, test/num_examples=3003, total_duration=6355.566186, train/accuracy=0.601423, train/bleu=28.633667, train/loss=2.285899, validation/accuracy=0.616781, validation/bleu=24.976861, validation/loss=2.163081, validation/num_examples=3000
I0207 09:39:03.111867 139615972296448 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1969652622938156, loss=4.247311592102051
I0207 09:39:38.070086 139615980689152 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.182892307639122, loss=4.294917106628418
I0207 09:40:13.013861 139615972296448 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.17859108746051788, loss=4.237874507904053
I0207 09:40:47.964158 139615980689152 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1781551092863083, loss=4.19659948348999
I0207 09:41:22.892956 139615972296448 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.1746508777141571, loss=4.2374982833862305
I0207 09:41:57.830279 139615980689152 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.18485461175441742, loss=4.28707218170166
I0207 09:42:32.782463 139615972296448 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.17636017501354218, loss=4.172635555267334
I0207 09:43:07.738755 139615980689152 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17890030145645142, loss=4.179076194763184
I0207 09:43:42.676189 139615972296448 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.16398940980434418, loss=4.269983768463135
I0207 09:44:17.603335 139615980689152 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.17496781051158905, loss=4.2165961265563965
I0207 09:44:52.524667 139615972296448 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.17178818583488464, loss=4.26639986038208
I0207 09:45:27.449119 139615980689152 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1761544942855835, loss=4.245900630950928
I0207 09:46:02.388073 139615972296448 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17459477484226227, loss=4.202348709106445
I0207 09:46:37.322180 139615980689152 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.1892460733652115, loss=4.2785162925720215
I0207 09:47:12.273394 139615972296448 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.17080911993980408, loss=4.202643871307373
I0207 09:47:47.215816 139615980689152 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.18174323439598083, loss=4.227766513824463
I0207 09:48:22.155048 139615972296448 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.16754010319709778, loss=4.256275177001953
I0207 09:48:57.067653 139615980689152 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.20107045769691467, loss=4.154193878173828
I0207 09:49:32.003337 139615972296448 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.169328972697258, loss=4.278940200805664
I0207 09:50:06.958475 139615980689152 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1709298938512802, loss=4.162606716156006
I0207 09:50:41.912900 139615972296448 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.15751321613788605, loss=4.196084499359131
I0207 09:51:16.842386 139615980689152 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1714145839214325, loss=4.183102130889893
I0207 09:51:51.898036 139615972296448 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.16576111316680908, loss=4.210753440856934
I0207 09:52:26.808089 139615980689152 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.16832631826400757, loss=4.228216171264648
I0207 09:52:32.820863 139785736898368 spec.py:321] Evaluating on the training split.
I0207 09:52:35.824081 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:55:24.175716 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 09:55:26.884845 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 09:58:05.414808 139785736898368 spec.py:349] Evaluating on the test split.
I0207 09:58:08.148468 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:00:43.687605 139785736898368 submission_runner.py:408] Time since start: 7686.63s, 	Step: 12019, 	{'train/accuracy': 0.6114262938499451, 'train/loss': 2.1870195865631104, 'train/bleu': 29.344313492146362, 'validation/accuracy': 0.6296511888504028, 'validation/loss': 2.044898271560669, 'validation/bleu': 25.76193795439446, 'validation/num_examples': 3000, 'test/accuracy': 0.6383940577507019, 'test/loss': 1.9973318576812744, 'test/bleu': 25.066060338305476, 'test/num_examples': 3003, 'score': 4227.473156452179, 'total_duration': 7686.628827095032, 'accumulated_submission_time': 4227.473156452179, 'accumulated_eval_time': 3458.648379802704, 'accumulated_logging_time': 0.12830424308776855}
I0207 10:00:43.706367 139615972296448 logging_writer.py:48] [12019] accumulated_eval_time=3458.648380, accumulated_logging_time=0.128304, accumulated_submission_time=4227.473156, global_step=12019, preemption_count=0, score=4227.473156, test/accuracy=0.638394, test/bleu=25.066060, test/loss=1.997332, test/num_examples=3003, total_duration=7686.628827, train/accuracy=0.611426, train/bleu=29.344313, train/loss=2.187020, validation/accuracy=0.629651, validation/bleu=25.761938, validation/loss=2.044898, validation/num_examples=3000
I0207 10:01:12.395900 139615980689152 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.1567537933588028, loss=4.171562194824219
I0207 10:01:47.330634 139615972296448 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1618434637784958, loss=4.1698994636535645
I0207 10:02:22.285722 139615980689152 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.15728634595870972, loss=4.211116790771484
I0207 10:02:57.210343 139615972296448 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.15295957028865814, loss=4.121087074279785
I0207 10:03:32.118832 139615980689152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.18942275643348694, loss=4.200960636138916
I0207 10:04:07.037316 139615972296448 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.17108836770057678, loss=4.148049354553223
I0207 10:04:41.984620 139615980689152 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1780998408794403, loss=4.160778045654297
I0207 10:05:16.917967 139615972296448 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.16765883564949036, loss=4.250334739685059
I0207 10:05:51.889343 139615980689152 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.1693296879529953, loss=4.095818996429443
I0207 10:06:26.853459 139615972296448 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.161253422498703, loss=4.173994541168213
I0207 10:07:01.800269 139615980689152 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.18094295263290405, loss=4.214157581329346
I0207 10:07:36.894463 139615972296448 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.16014540195465088, loss=4.191345691680908
I0207 10:08:11.852319 139615980689152 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.16749881207942963, loss=4.184622287750244
I0207 10:08:46.784849 139615972296448 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.16416487097740173, loss=4.165560245513916
I0207 10:09:21.709745 139615980689152 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.15714451670646667, loss=4.198823928833008
I0207 10:09:56.665254 139615972296448 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.16034744679927826, loss=4.129384994506836
I0207 10:10:31.652664 139615980689152 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.17460249364376068, loss=4.149331092834473
I0207 10:11:06.625761 139615972296448 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.16132600605487823, loss=4.112794876098633
I0207 10:11:41.666330 139615980689152 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.15324531495571136, loss=4.115442752838135
I0207 10:12:16.611445 139615972296448 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.17096783220767975, loss=4.1549763679504395
I0207 10:12:51.522288 139615980689152 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.15936273336410522, loss=4.155440330505371
I0207 10:13:26.465315 139615972296448 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.16466571390628815, loss=4.081550598144531
I0207 10:14:01.424092 139615980689152 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1653546243906021, loss=4.104343414306641
I0207 10:14:36.368567 139615972296448 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.1698712408542633, loss=4.1180806159973145
I0207 10:14:43.786214 139785736898368 spec.py:321] Evaluating on the training split.
I0207 10:14:46.787031 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:17:23.690798 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 10:17:26.400993 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:20:04.911906 139785736898368 spec.py:349] Evaluating on the test split.
I0207 10:20:07.638705 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:22:33.537780 139785736898368 submission_runner.py:408] Time since start: 8996.48s, 	Step: 14423, 	{'train/accuracy': 0.6259955763816833, 'train/loss': 2.062871217727661, 'train/bleu': 30.1025163364132, 'validation/accuracy': 0.6408227682113647, 'validation/loss': 1.9591337442398071, 'validation/bleu': 26.82538380800523, 'validation/num_examples': 3000, 'test/accuracy': 0.6490268111228943, 'test/loss': 1.909260869026184, 'test/bleu': 26.05126954299677, 'test/num_examples': 3003, 'score': 5067.463086843491, 'total_duration': 8996.47902727127, 'accumulated_submission_time': 5067.463086843491, 'accumulated_eval_time': 3928.3998835086823, 'accumulated_logging_time': 0.15821146965026855}
I0207 10:22:33.554203 139615980689152 logging_writer.py:48] [14423] accumulated_eval_time=3928.399884, accumulated_logging_time=0.158211, accumulated_submission_time=5067.463087, global_step=14423, preemption_count=0, score=5067.463087, test/accuracy=0.649027, test/bleu=26.051270, test/loss=1.909261, test/num_examples=3003, total_duration=8996.479027, train/accuracy=0.625996, train/bleu=30.102516, train/loss=2.062871, validation/accuracy=0.640823, validation/bleu=26.825384, validation/loss=1.959134, validation/num_examples=3000
I0207 10:23:00.827506 139615972296448 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1748436689376831, loss=4.103085517883301
I0207 10:23:35.801048 139615980689152 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.16159015893936157, loss=4.164847373962402
I0207 10:24:10.763221 139615972296448 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.15150035917758942, loss=4.102748870849609
I0207 10:24:45.691120 139615980689152 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.1966455727815628, loss=4.151707172393799
I0207 10:25:20.633256 139615972296448 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.16168971359729767, loss=4.123936176300049
I0207 10:25:55.547450 139615980689152 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.16177783906459808, loss=4.131224155426025
I0207 10:26:30.463106 139615972296448 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.18130870163440704, loss=4.113275527954102
I0207 10:27:05.389512 139615980689152 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.1694682240486145, loss=4.123861789703369
I0207 10:27:40.352302 139615972296448 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.16551676392555237, loss=4.132626056671143
I0207 10:28:15.327252 139615980689152 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.15798893570899963, loss=4.125590801239014
I0207 10:28:50.280964 139615972296448 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.14637623727321625, loss=4.108155250549316
I0207 10:29:25.175130 139615980689152 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.15531516075134277, loss=4.1788225173950195
I0207 10:30:00.097069 139615972296448 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.15225520730018616, loss=4.155885219573975
I0207 10:30:35.031536 139615980689152 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.14850865304470062, loss=4.065053462982178
I0207 10:31:09.991363 139615972296448 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.14268572628498077, loss=4.1422343254089355
I0207 10:31:44.933466 139615980689152 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.18748119473457336, loss=4.02097225189209
I0207 10:32:19.856966 139615972296448 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1538345366716385, loss=4.127427577972412
I0207 10:32:54.804504 139615980689152 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.14916403591632843, loss=4.041916847229004
I0207 10:33:29.773281 139615972296448 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.15769340097904205, loss=4.106678009033203
I0207 10:34:04.722231 139615980689152 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.14442333579063416, loss=4.047832012176514
I0207 10:34:39.639515 139615972296448 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.20352444052696228, loss=4.047898769378662
I0207 10:35:14.585234 139615980689152 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.16272850334644318, loss=4.068321228027344
I0207 10:35:49.538314 139615972296448 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1991795003414154, loss=4.038038730621338
I0207 10:36:24.436045 139615980689152 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.16795097291469574, loss=4.085743427276611
I0207 10:36:33.586158 139785736898368 spec.py:321] Evaluating on the training split.
I0207 10:36:36.580441 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:39:16.820650 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 10:39:19.529603 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:41:50.733417 139785736898368 spec.py:349] Evaluating on the test split.
I0207 10:41:53.453115 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 10:44:13.284783 139785736898368 submission_runner.py:408] Time since start: 10296.23s, 	Step: 16828, 	{'train/accuracy': 0.6307092905044556, 'train/loss': 2.047231912612915, 'train/bleu': 30.666321567767124, 'validation/accuracy': 0.647034764289856, 'validation/loss': 1.9078556299209595, 'validation/bleu': 27.453680639560886, 'validation/num_examples': 3000, 'test/accuracy': 0.6569984555244446, 'test/loss': 1.8533170223236084, 'test/bleu': 26.831555098397647, 'test/num_examples': 3003, 'score': 5907.40859413147, 'total_duration': 10296.226013422012, 'accumulated_submission_time': 5907.40859413147, 'accumulated_eval_time': 4388.0984263420105, 'accumulated_logging_time': 0.1860949993133545}
I0207 10:44:13.305087 139615972296448 logging_writer.py:48] [16828] accumulated_eval_time=4388.098426, accumulated_logging_time=0.186095, accumulated_submission_time=5907.408594, global_step=16828, preemption_count=0, score=5907.408594, test/accuracy=0.656998, test/bleu=26.831555, test/loss=1.853317, test/num_examples=3003, total_duration=10296.226013, train/accuracy=0.630709, train/bleu=30.666322, train/loss=2.047232, validation/accuracy=0.647035, validation/bleu=27.453681, validation/loss=1.907856, validation/num_examples=3000
I0207 10:44:38.917696 139615980689152 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.1519351303577423, loss=4.144502639770508
I0207 10:45:13.857292 139615972296448 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.16095426678657532, loss=4.164916515350342
I0207 10:45:48.788414 139615980689152 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.16437655687332153, loss=4.126972675323486
I0207 10:46:23.702375 139615972296448 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.17020678520202637, loss=4.0682525634765625
I0207 10:46:58.628123 139615980689152 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.16088712215423584, loss=4.021797180175781
I0207 10:47:33.533622 139615972296448 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.17744611203670502, loss=4.102975845336914
I0207 10:48:08.470260 139615980689152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.1519957333803177, loss=4.041461944580078
I0207 10:48:43.394164 139615972296448 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.15604443848133087, loss=4.080860137939453
I0207 10:49:18.300552 139615980689152 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1702970415353775, loss=4.065983772277832
I0207 10:49:53.254970 139615972296448 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.1584734469652176, loss=4.107894420623779
I0207 10:50:28.245620 139615980689152 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.18831780552864075, loss=4.011141777038574
I0207 10:51:03.178614 139615972296448 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.2410729080438614, loss=4.057565689086914
I0207 10:51:38.104550 139615980689152 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.20348681509494781, loss=4.0035905838012695
I0207 10:52:13.035789 139615972296448 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.16227349638938904, loss=4.066302299499512
I0207 10:52:47.968662 139615980689152 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.17701701819896698, loss=4.089574813842773
I0207 10:53:22.883680 139615972296448 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.19891636073589325, loss=4.035202980041504
I0207 10:53:57.776709 139615980689152 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.1587304174900055, loss=4.072476863861084
I0207 10:54:32.679536 139615972296448 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.16153188049793243, loss=4.036273002624512
I0207 10:55:07.619699 139615980689152 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.15856145322322845, loss=4.059092998504639
I0207 10:55:42.558478 139615972296448 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.16240504384040833, loss=3.984477996826172
I0207 10:56:17.496799 139615980689152 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.15668006241321564, loss=4.048007011413574
I0207 10:56:52.476165 139615972296448 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.1607753485441208, loss=3.9765939712524414
I0207 10:57:27.441050 139615980689152 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.18203005194664001, loss=4.0626301765441895
I0207 10:58:02.370181 139615972296448 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.15693052113056183, loss=4.057124614715576
I0207 10:58:13.619243 139785736898368 spec.py:321] Evaluating on the training split.
I0207 10:58:16.626392 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:00:57.043042 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 11:00:59.753390 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:03:29.599756 139785736898368 spec.py:349] Evaluating on the test split.
I0207 11:03:32.313111 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:05:55.104342 139785736898368 submission_runner.py:408] Time since start: 11598.05s, 	Step: 19234, 	{'train/accuracy': 0.6473884582519531, 'train/loss': 1.9363336563110352, 'train/bleu': 31.799686839622684, 'validation/accuracy': 0.6544494032859802, 'validation/loss': 1.8840500116348267, 'validation/bleu': 27.786832108362752, 'validation/num_examples': 3000, 'test/accuracy': 0.6652141213417053, 'test/loss': 1.8207148313522339, 'test/bleu': 27.350837919366988, 'test/num_examples': 3003, 'score': 6747.634348392487, 'total_duration': 11598.04559969902, 'accumulated_submission_time': 6747.634348392487, 'accumulated_eval_time': 4849.583470821381, 'accumulated_logging_time': 0.21756911277770996}
I0207 11:05:55.122259 139615980689152 logging_writer.py:48] [19234] accumulated_eval_time=4849.583471, accumulated_logging_time=0.217569, accumulated_submission_time=6747.634348, global_step=19234, preemption_count=0, score=6747.634348, test/accuracy=0.665214, test/bleu=27.350838, test/loss=1.820715, test/num_examples=3003, total_duration=11598.045600, train/accuracy=0.647388, train/bleu=31.799687, train/loss=1.936334, validation/accuracy=0.654449, validation/bleu=27.786832, validation/loss=1.884050, validation/num_examples=3000
I0207 11:06:18.579332 139615972296448 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.17116327583789825, loss=3.9682416915893555
I0207 11:06:53.545242 139615980689152 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.15184743702411652, loss=4.03486442565918
I0207 11:07:28.515911 139615972296448 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2148822695016861, loss=4.040685176849365
I0207 11:08:03.537138 139615980689152 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.16193170845508575, loss=4.064553737640381
I0207 11:08:38.545037 139615972296448 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.1580764204263687, loss=4.046797275543213
I0207 11:09:13.495651 139615980689152 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.18285878002643585, loss=4.0890278816223145
I0207 11:09:48.418179 139615972296448 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16202770173549652, loss=4.017279624938965
I0207 11:10:23.334990 139615980689152 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.1859382539987564, loss=4.027796745300293
I0207 11:10:58.247480 139615972296448 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.15991704165935516, loss=4.006575584411621
I0207 11:11:33.162138 139615980689152 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.15785716474056244, loss=4.0277099609375
I0207 11:12:08.082426 139615972296448 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.17733624577522278, loss=4.010890007019043
I0207 11:12:42.987001 139615980689152 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.15411728620529175, loss=4.111867904663086
I0207 11:13:17.892975 139615972296448 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.1636299043893814, loss=4.065464973449707
I0207 11:13:52.795169 139615980689152 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.20715203881263733, loss=4.062580108642578
I0207 11:14:27.711577 139615972296448 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.16861560940742493, loss=4.029153347015381
I0207 11:15:02.672126 139615980689152 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.18243639171123505, loss=4.009922981262207
I0207 11:15:37.585199 139615972296448 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.15972140431404114, loss=3.9761886596679688
I0207 11:16:12.501363 139615980689152 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.163052499294281, loss=4.082803249359131
I0207 11:16:47.411328 139615972296448 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.1703619360923767, loss=4.031411170959473
I0207 11:17:22.339432 139615980689152 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.20754888653755188, loss=4.038254737854004
I0207 11:17:57.252355 139615972296448 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.1763199418783188, loss=3.99383282661438
I0207 11:18:32.180820 139615980689152 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.496775358915329, loss=4.252319812774658
I0207 11:19:07.100696 139615972296448 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.16419467329978943, loss=4.008270740509033
I0207 11:19:42.065348 139615980689152 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.1814020276069641, loss=4.009422779083252
I0207 11:19:55.430262 139785736898368 spec.py:321] Evaluating on the training split.
I0207 11:19:58.447960 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:23:33.658998 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 11:23:36.379846 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:26:14.283377 139785736898368 spec.py:349] Evaluating on the test split.
I0207 11:26:16.994168 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:28:41.995377 139785736898368 submission_runner.py:408] Time since start: 12964.94s, 	Step: 21640, 	{'train/accuracy': 0.6389160752296448, 'train/loss': 1.9853614568710327, 'train/bleu': 31.149602875625366, 'validation/accuracy': 0.6562844514846802, 'validation/loss': 1.854095458984375, 'validation/bleu': 28.157027809582498, 'validation/num_examples': 3000, 'test/accuracy': 0.6675498485565186, 'test/loss': 1.796425461769104, 'test/bleu': 27.407634766447824, 'test/num_examples': 3003, 'score': 7587.8516755104065, 'total_duration': 12964.936593532562, 'accumulated_submission_time': 7587.8516755104065, 'accumulated_eval_time': 5376.148508548737, 'accumulated_logging_time': 0.24692964553833008}
I0207 11:28:42.013114 139615972296448 logging_writer.py:48] [21640] accumulated_eval_time=5376.148509, accumulated_logging_time=0.246930, accumulated_submission_time=7587.851676, global_step=21640, preemption_count=0, score=7587.851676, test/accuracy=0.667550, test/bleu=27.407635, test/loss=1.796425, test/num_examples=3003, total_duration=12964.936594, train/accuracy=0.638916, train/bleu=31.149603, train/loss=1.985361, validation/accuracy=0.656284, validation/bleu=28.157028, validation/loss=1.854095, validation/num_examples=3000
I0207 11:29:03.321622 139615980689152 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.15896029770374298, loss=3.9889206886291504
I0207 11:29:38.228750 139615972296448 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.1594209223985672, loss=4.067012786865234
I0207 11:30:13.163845 139615980689152 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.15856420993804932, loss=3.992616891860962
I0207 11:30:48.079694 139615972296448 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.16095995903015137, loss=4.0029826164245605
I0207 11:31:22.970127 139615980689152 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1726805567741394, loss=4.036375522613525
I0207 11:31:57.855397 139615972296448 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.2068796008825302, loss=4.040163993835449
I0207 11:32:32.762332 139615980689152 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.21420888602733612, loss=3.9943015575408936
I0207 11:33:07.641625 139615972296448 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.16824565827846527, loss=4.072121620178223
I0207 11:33:42.543656 139615980689152 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.160849466919899, loss=4.028164863586426
I0207 11:34:17.441696 139615972296448 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.1805540919303894, loss=3.9952290058135986
I0207 11:34:52.378508 139615980689152 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1648445576429367, loss=4.050967693328857
I0207 11:35:27.286879 139615972296448 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.16452662646770477, loss=3.903944492340088
I0207 11:36:02.230876 139615980689152 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.17353416979312897, loss=4.048613548278809
I0207 11:36:37.131993 139615972296448 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.16805551946163177, loss=4.030982494354248
I0207 11:37:12.032089 139615980689152 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.17279838025569916, loss=4.019340515136719
I0207 11:37:46.949119 139615972296448 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.18392594158649445, loss=4.010129928588867
I0207 11:38:21.891003 139615980689152 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.15749047696590424, loss=3.964552879333496
I0207 11:38:56.873715 139615972296448 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.17202478647232056, loss=4.012308120727539
I0207 11:39:31.806807 139615980689152 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.18615125119686127, loss=3.9968113899230957
I0207 11:40:06.723939 139615972296448 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2457757592201233, loss=4.058278560638428
I0207 11:40:41.630089 139615980689152 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.16165848076343536, loss=3.991313934326172
I0207 11:41:16.534466 139615972296448 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.17336876690387726, loss=4.049497604370117
I0207 11:41:51.454393 139615980689152 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.15837335586547852, loss=3.992253541946411
I0207 11:42:26.489458 139615972296448 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.17280788719654083, loss=3.9902455806732178
I0207 11:42:42.278913 139785736898368 spec.py:321] Evaluating on the training split.
I0207 11:42:45.283340 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:46:03.250337 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 11:46:05.960630 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:48:45.869384 139785736898368 spec.py:349] Evaluating on the test split.
I0207 11:48:48.582975 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 11:51:18.659247 139785736898368 submission_runner.py:408] Time since start: 14321.60s, 	Step: 24047, 	{'train/accuracy': 0.6381778717041016, 'train/loss': 1.9780657291412354, 'train/bleu': 31.645160669568956, 'validation/accuracy': 0.6574996113777161, 'validation/loss': 1.8356502056121826, 'validation/bleu': 27.9298678608305, 'validation/num_examples': 3000, 'test/accuracy': 0.668502688407898, 'test/loss': 1.7730231285095215, 'test/bleu': 27.702566928617625, 'test/num_examples': 3003, 'score': 8428.0314412117, 'total_duration': 14321.60050368309, 'accumulated_submission_time': 8428.0314412117, 'accumulated_eval_time': 5892.5287890434265, 'accumulated_logging_time': 0.274691104888916}
I0207 11:51:18.679128 139615980689152 logging_writer.py:48] [24047] accumulated_eval_time=5892.528789, accumulated_logging_time=0.274691, accumulated_submission_time=8428.031441, global_step=24047, preemption_count=0, score=8428.031441, test/accuracy=0.668503, test/bleu=27.702567, test/loss=1.773023, test/num_examples=3003, total_duration=14321.600504, train/accuracy=0.638178, train/bleu=31.645161, train/loss=1.978066, validation/accuracy=0.657500, validation/bleu=27.929868, validation/loss=1.835650, validation/num_examples=3000
I0207 11:51:37.522655 139615972296448 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.19020333886146545, loss=4.056404113769531
I0207 11:52:12.455200 139615980689152 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.1813967227935791, loss=3.995224714279175
I0207 11:52:47.381692 139615972296448 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.17399071156978607, loss=4.0952229499816895
I0207 11:53:22.310864 139615980689152 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.17137111723423004, loss=4.035724639892578
I0207 11:53:57.248010 139615972296448 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.18914780020713806, loss=3.998288869857788
I0207 11:54:32.166245 139615980689152 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.22107034921646118, loss=3.97253155708313
I0207 11:55:07.092090 139615972296448 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.1866842359304428, loss=3.934349536895752
I0207 11:55:41.972916 139615980689152 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.21357162296772003, loss=4.013838291168213
I0207 11:56:16.902123 139615972296448 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.24736207723617554, loss=4.001290321350098
I0207 11:56:51.922572 139615980689152 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.20529985427856445, loss=4.010097026824951
I0207 11:57:26.840376 139615972296448 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.17206282913684845, loss=4.003896236419678
I0207 11:58:01.781807 139615980689152 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.18236136436462402, loss=3.9816062450408936
I0207 11:58:36.700641 139615972296448 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.18954996764659882, loss=4.026837348937988
I0207 11:59:11.615173 139615980689152 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.1788317710161209, loss=3.982316493988037
I0207 11:59:46.539235 139615972296448 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.19909381866455078, loss=3.906014919281006
I0207 12:00:21.455034 139615980689152 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.17288188636302948, loss=3.996664524078369
I0207 12:00:56.362006 139615972296448 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.17792996764183044, loss=3.973773241043091
I0207 12:01:31.276458 139615980689152 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.17047175765037537, loss=3.903759717941284
I0207 12:02:06.161545 139615972296448 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.17793084681034088, loss=4.020659446716309
I0207 12:02:41.112656 139615980689152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.17994770407676697, loss=3.8765015602111816
I0207 12:03:16.035238 139615972296448 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.18524301052093506, loss=4.029201507568359
I0207 12:03:50.953352 139615980689152 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.1836581528186798, loss=4.028508186340332
I0207 12:04:25.877461 139615972296448 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.18287375569343567, loss=3.982189416885376
I0207 12:05:00.792447 139615980689152 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2324398010969162, loss=4.077703952789307
I0207 12:05:18.681473 139785736898368 spec.py:321] Evaluating on the training split.
I0207 12:05:21.682483 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:08:08.548507 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 12:08:11.250574 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:10:42.033982 139785736898368 spec.py:349] Evaluating on the test split.
I0207 12:10:44.751634 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:13:12.787028 139785736898368 submission_runner.py:408] Time since start: 15635.73s, 	Step: 26453, 	{'train/accuracy': 0.650377094745636, 'train/loss': 1.8855642080307007, 'train/bleu': 31.99286060995921, 'validation/accuracy': 0.6624096632003784, 'validation/loss': 1.792500615119934, 'validation/bleu': 28.47187284902378, 'validation/num_examples': 3000, 'test/accuracy': 0.6716518402099609, 'test/loss': 1.732503056526184, 'test/bleu': 28.19627620748597, 'test/num_examples': 3003, 'score': 9267.948320865631, 'total_duration': 15635.728284358978, 'accumulated_submission_time': 9267.948320865631, 'accumulated_eval_time': 6366.634291410446, 'accumulated_logging_time': 0.30600905418395996}
I0207 12:13:12.805573 139615972296448 logging_writer.py:48] [26453] accumulated_eval_time=6366.634291, accumulated_logging_time=0.306009, accumulated_submission_time=9267.948321, global_step=26453, preemption_count=0, score=9267.948321, test/accuracy=0.671652, test/bleu=28.196276, test/loss=1.732503, test/num_examples=3003, total_duration=15635.728284, train/accuracy=0.650377, train/bleu=31.992861, train/loss=1.885564, validation/accuracy=0.662410, validation/bleu=28.471873, validation/loss=1.792501, validation/num_examples=3000
I0207 12:13:29.551057 139615980689152 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.19907787442207336, loss=4.017410755157471
I0207 12:14:04.470293 139615972296448 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.20273831486701965, loss=3.992637872695923
I0207 12:14:39.479970 139615980689152 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2491377741098404, loss=3.9737420082092285
I0207 12:15:14.411792 139615972296448 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.21284490823745728, loss=4.019381999969482
I0207 12:15:49.316748 139615980689152 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.18576081097126007, loss=3.9667651653289795
I0207 12:16:24.228046 139615972296448 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.1839570552110672, loss=3.937528610229492
I0207 12:16:59.166275 139615980689152 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2027031034231186, loss=3.993989944458008
I0207 12:17:34.073849 139615972296448 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.19186685979366302, loss=3.951094627380371
I0207 12:18:08.909986 139615980689152 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.25244084000587463, loss=5.811736106872559
I0207 12:18:43.764206 139615972296448 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2121860235929489, loss=5.595996856689453
I0207 12:19:18.640983 139615980689152 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2929145395755768, loss=5.579542636871338
I0207 12:19:53.507663 139615972296448 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.4288581609725952, loss=5.570262432098389
I0207 12:20:28.361157 139615980689152 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.2473885715007782, loss=5.552463054656982
I0207 12:21:03.211148 139615972296448 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.29103171825408936, loss=5.507767200469971
I0207 12:21:38.100063 139615980689152 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.497569739818573, loss=5.484076976776123
I0207 12:22:12.995789 139615972296448 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.36160576343536377, loss=4.279430866241455
I0207 12:22:47.906087 139615980689152 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.23334455490112305, loss=3.987358808517456
I0207 12:23:22.824687 139615972296448 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.21840700507164001, loss=3.9943056106567383
I0207 12:23:57.726917 139615980689152 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.27601832151412964, loss=3.9544858932495117
I0207 12:24:32.611340 139615972296448 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.210441455245018, loss=3.968604803085327
I0207 12:25:07.536003 139615980689152 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.21394260227680206, loss=4.006290435791016
I0207 12:25:42.428204 139615972296448 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.17206114530563354, loss=3.991511821746826
I0207 12:26:17.345263 139615980689152 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.17746895551681519, loss=3.9598228931427
I0207 12:26:52.301383 139615972296448 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1818367838859558, loss=3.997276782989502
I0207 12:27:12.983031 139785736898368 spec.py:321] Evaluating on the training split.
I0207 12:27:15.986495 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:29:59.195010 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 12:30:01.925774 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:32:43.418161 139785736898368 spec.py:349] Evaluating on the test split.
I0207 12:32:46.155237 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:35:22.409828 139785736898368 submission_runner.py:408] Time since start: 16965.35s, 	Step: 28861, 	{'train/accuracy': 0.6490204334259033, 'train/loss': 1.8970963954925537, 'train/bleu': 31.745865182709256, 'validation/accuracy': 0.6626080274581909, 'validation/loss': 1.7790693044662476, 'validation/bleu': 28.336580553316416, 'validation/num_examples': 3000, 'test/accuracy': 0.6757306456565857, 'test/loss': 1.7089323997497559, 'test/bleu': 28.2438093623282, 'test/num_examples': 3003, 'score': 10108.041011333466, 'total_duration': 16965.351059913635, 'accumulated_submission_time': 10108.041011333466, 'accumulated_eval_time': 6856.061012983322, 'accumulated_logging_time': 0.33478879928588867}
I0207 12:35:22.432204 139615980689152 logging_writer.py:48] [28861] accumulated_eval_time=6856.061013, accumulated_logging_time=0.334789, accumulated_submission_time=10108.041011, global_step=28861, preemption_count=0, score=10108.041011, test/accuracy=0.675731, test/bleu=28.243809, test/loss=1.708932, test/num_examples=3003, total_duration=16965.351060, train/accuracy=0.649020, train/bleu=31.745865, train/loss=1.897096, validation/accuracy=0.662608, validation/bleu=28.336581, validation/loss=1.779069, validation/num_examples=3000
I0207 12:35:36.423128 139615972296448 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.2102271020412445, loss=4.022375106811523
I0207 12:36:11.352059 139615980689152 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.19489529728889465, loss=3.9479782581329346
I0207 12:36:46.253542 139615972296448 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.21144314110279083, loss=3.9873905181884766
I0207 12:37:21.158083 139615980689152 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.1806972324848175, loss=3.9835915565490723
I0207 12:37:56.057553 139615972296448 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.18367233872413635, loss=4.026176929473877
I0207 12:38:30.969460 139615980689152 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.1996697038412094, loss=3.968590497970581
I0207 12:39:05.864932 139615972296448 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2075229436159134, loss=3.9859752655029297
I0207 12:39:40.761931 139615980689152 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.25398755073547363, loss=3.948204517364502
I0207 12:40:15.688077 139615972296448 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.1828281134366989, loss=3.9786975383758545
I0207 12:40:50.598303 139615980689152 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.21593531966209412, loss=3.9503390789031982
I0207 12:41:25.501639 139615972296448 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.18956084549427032, loss=3.9740023612976074
I0207 12:42:00.428093 139615980689152 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.20273742079734802, loss=3.9707462787628174
I0207 12:42:35.374935 139615972296448 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.2584223747253418, loss=3.989898920059204
I0207 12:43:10.304802 139615980689152 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.20177097618579865, loss=3.9625601768493652
I0207 12:43:45.258897 139615972296448 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.1961670219898224, loss=3.9797708988189697
I0207 12:44:20.214197 139615980689152 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.2086440473794937, loss=3.920469284057617
I0207 12:44:55.125452 139615972296448 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.2041826695203781, loss=3.982743978500366
I0207 12:45:30.071582 139615980689152 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.19061298668384552, loss=3.9017746448516846
I0207 12:46:05.016698 139615972296448 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.22471913695335388, loss=3.954338312149048
I0207 12:46:39.954118 139615980689152 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.23361527919769287, loss=3.910397529602051
I0207 12:47:14.855258 139615972296448 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.22200927138328552, loss=3.9513449668884277
I0207 12:47:49.770765 139615980689152 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.2951420545578003, loss=3.9680869579315186
I0207 12:48:24.687731 139615972296448 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.1996312141418457, loss=4.0061469078063965
I0207 12:48:59.608356 139615980689152 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.18830657005310059, loss=3.9234840869903564
I0207 12:49:22.723304 139785736898368 spec.py:321] Evaluating on the training split.
I0207 12:49:25.719137 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:53:52.109649 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 12:53:54.819952 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 12:57:35.604257 139785736898368 spec.py:349] Evaluating on the test split.
I0207 12:57:38.316878 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:00:48.995737 139785736898368 submission_runner.py:408] Time since start: 18491.94s, 	Step: 31268, 	{'train/accuracy': 0.6462864875793457, 'train/loss': 1.938828706741333, 'train/bleu': 32.04996210677026, 'validation/accuracy': 0.6645174622535706, 'validation/loss': 1.804949164390564, 'validation/bleu': 28.524153144156863, 'validation/num_examples': 3000, 'test/accuracy': 0.6761257648468018, 'test/loss': 1.7377357482910156, 'test/bleu': 27.99109156876941, 'test/num_examples': 3003, 'score': 10948.245263814926, 'total_duration': 18491.936994314194, 'accumulated_submission_time': 10948.245263814926, 'accumulated_eval_time': 7542.333392858505, 'accumulated_logging_time': 0.3696925640106201}
I0207 13:00:49.015024 139615972296448 logging_writer.py:48] [31268] accumulated_eval_time=7542.333393, accumulated_logging_time=0.369693, accumulated_submission_time=10948.245264, global_step=31268, preemption_count=0, score=10948.245264, test/accuracy=0.676126, test/bleu=27.991092, test/loss=1.737736, test/num_examples=3003, total_duration=18491.936994, train/accuracy=0.646286, train/bleu=32.049962, train/loss=1.938829, validation/accuracy=0.664517, validation/bleu=28.524153, validation/loss=1.804949, validation/num_examples=3000
I0207 13:01:00.539500 139615980689152 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.20821110904216766, loss=3.9840939044952393
I0207 13:01:35.449741 139615972296448 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.18766292929649353, loss=3.962629795074463
I0207 13:02:10.369263 139615980689152 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.22170299291610718, loss=3.967010974884033
I0207 13:02:45.277545 139615972296448 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.21675953269004822, loss=3.9302468299865723
I0207 13:03:20.227718 139615980689152 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.19729509949684143, loss=3.982452392578125
I0207 13:03:55.133525 139615972296448 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.32350030541419983, loss=3.8898916244506836
I0207 13:04:30.080411 139615980689152 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.2097427099943161, loss=3.9758310317993164
I0207 13:05:05.003297 139615972296448 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2529183030128479, loss=3.9532079696655273
I0207 13:05:39.937302 139615980689152 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2114867866039276, loss=3.967651128768921
I0207 13:06:14.870775 139615972296448 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.22618000209331512, loss=3.9616024494171143
I0207 13:06:49.791169 139615980689152 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.20850951969623566, loss=3.9819061756134033
I0207 13:07:24.721823 139615972296448 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2588088810443878, loss=4.004624366760254
I0207 13:07:59.668735 139615980689152 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.2772093415260315, loss=3.936591863632202
I0207 13:08:34.657905 139615972296448 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.2405458241701126, loss=3.951719284057617
I0207 13:09:09.627382 139615980689152 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.18390323221683502, loss=3.9449446201324463
I0207 13:09:44.557293 139615972296448 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.1947958916425705, loss=3.987139940261841
I0207 13:10:19.505645 139615980689152 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.19293531775474548, loss=3.93758225440979
I0207 13:10:54.440949 139615972296448 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2118445634841919, loss=3.924598217010498
I0207 13:11:29.381421 139615980689152 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.20491120219230652, loss=3.9552271366119385
I0207 13:12:04.330431 139615972296448 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.20801669359207153, loss=3.9760940074920654
I0207 13:12:39.253648 139615980689152 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.2368960678577423, loss=3.9209516048431396
I0207 13:13:14.161445 139615972296448 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.22596204280853271, loss=3.968907117843628
I0207 13:13:49.078251 139615980689152 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.21032103896141052, loss=3.9350228309631348
I0207 13:14:24.021291 139615972296448 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2233961969614029, loss=3.999816417694092
I0207 13:14:49.213195 139785736898368 spec.py:321] Evaluating on the training split.
I0207 13:14:52.216941 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:18:15.810517 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 13:18:18.542197 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:20:53.133662 139785736898368 spec.py:349] Evaluating on the test split.
I0207 13:20:55.850206 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:23:24.898435 139785736898368 submission_runner.py:408] Time since start: 19847.84s, 	Step: 33674, 	{'train/accuracy': 0.6534045934677124, 'train/loss': 1.8654236793518066, 'train/bleu': 31.9049431373965, 'validation/accuracy': 0.667592465877533, 'validation/loss': 1.7680360078811646, 'validation/bleu': 28.721269757279806, 'validation/num_examples': 3000, 'test/accuracy': 0.6791703104972839, 'test/loss': 1.6936718225479126, 'test/bleu': 28.643860573925735, 'test/num_examples': 3003, 'score': 11788.358795166016, 'total_duration': 19847.839690446854, 'accumulated_submission_time': 11788.358795166016, 'accumulated_eval_time': 8058.018579006195, 'accumulated_logging_time': 0.39875078201293945}
I0207 13:23:24.918552 139615980689152 logging_writer.py:48] [33674] accumulated_eval_time=8058.018579, accumulated_logging_time=0.398751, accumulated_submission_time=11788.358795, global_step=33674, preemption_count=0, score=11788.358795, test/accuracy=0.679170, test/bleu=28.643861, test/loss=1.693672, test/num_examples=3003, total_duration=19847.839690, train/accuracy=0.653405, train/bleu=31.904943, train/loss=1.865424, validation/accuracy=0.667592, validation/bleu=28.721270, validation/loss=1.768036, validation/num_examples=3000
I0207 13:23:34.370813 139615972296448 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2919563353061676, loss=3.9979264736175537
I0207 13:24:09.353502 139615980689152 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.22128959000110626, loss=3.9246814250946045
I0207 13:24:44.295955 139615972296448 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.7214326858520508, loss=4.848241806030273
I0207 13:25:19.228030 139615980689152 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.23644499480724335, loss=4.039655685424805
I0207 13:25:54.154632 139615972296448 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.23174485564231873, loss=3.9941813945770264
I0207 13:26:29.083815 139615980689152 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.19310367107391357, loss=3.9860970973968506
I0207 13:27:04.017446 139615972296448 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2052968293428421, loss=3.9311363697052
I0207 13:27:38.966513 139615980689152 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.2058730274438858, loss=3.9219892024993896
I0207 13:28:13.888780 139615972296448 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1771383136510849, loss=3.8867948055267334
I0207 13:28:48.846785 139615980689152 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.242165207862854, loss=3.9265904426574707
I0207 13:29:23.815068 139615972296448 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.1993003785610199, loss=3.990140914916992
I0207 13:29:58.729062 139615980689152 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.1911717802286148, loss=3.8923118114471436
I0207 13:30:33.640522 139615972296448 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.17785845696926117, loss=3.944772243499756
I0207 13:31:08.573395 139615980689152 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.26203349232673645, loss=3.898587226867676
I0207 13:31:43.512106 139615972296448 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.20802944898605347, loss=3.920258045196533
I0207 13:32:18.461976 139615980689152 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.23314133286476135, loss=3.8715476989746094
I0207 13:32:53.374286 139615972296448 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.20596127212047577, loss=3.931624174118042
I0207 13:33:28.322136 139615980689152 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.244778573513031, loss=3.9174771308898926
I0207 13:34:03.249999 139615972296448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2554451525211334, loss=3.9431209564208984
I0207 13:34:38.161606 139615980689152 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.2168402522802353, loss=3.9073002338409424
I0207 13:35:13.060251 139615972296448 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.22630050778388977, loss=3.9523122310638428
I0207 13:35:47.986773 139615980689152 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.211346834897995, loss=3.9403557777404785
I0207 13:36:22.904515 139615972296448 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2623748183250427, loss=3.9555392265319824
I0207 13:36:57.838211 139615980689152 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.19300955533981323, loss=3.9011199474334717
I0207 13:37:25.137760 139785736898368 spec.py:321] Evaluating on the training split.
I0207 13:37:28.150333 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:40:25.694474 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 13:40:28.406512 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:42:59.759424 139785736898368 spec.py:349] Evaluating on the test split.
I0207 13:43:02.499191 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 13:45:22.395069 139785736898368 submission_runner.py:408] Time since start: 21165.34s, 	Step: 36080, 	{'train/accuracy': 0.6488655209541321, 'train/loss': 1.889739751815796, 'train/bleu': 32.563431170556136, 'validation/accuracy': 0.6692167520523071, 'validation/loss': 1.754157543182373, 'validation/bleu': 29.021798663119707, 'validation/num_examples': 3000, 'test/accuracy': 0.6801928877830505, 'test/loss': 1.6816045045852661, 'test/bleu': 28.412542674194228, 'test/num_examples': 3003, 'score': 12628.493074417114, 'total_duration': 21165.33632516861, 'accumulated_submission_time': 12628.493074417114, 'accumulated_eval_time': 8535.275834321976, 'accumulated_logging_time': 0.4287540912628174}
I0207 13:45:22.414884 139615972296448 logging_writer.py:48] [36080] accumulated_eval_time=8535.275834, accumulated_logging_time=0.428754, accumulated_submission_time=12628.493074, global_step=36080, preemption_count=0, score=12628.493074, test/accuracy=0.680193, test/bleu=28.412543, test/loss=1.681605, test/num_examples=3003, total_duration=21165.336325, train/accuracy=0.648866, train/bleu=32.563431, train/loss=1.889740, validation/accuracy=0.669217, validation/bleu=29.021799, validation/loss=1.754158, validation/num_examples=3000
I0207 13:45:29.761135 139615980689152 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.22193677723407745, loss=3.868377923965454
I0207 13:46:04.690819 139615972296448 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.20064440369606018, loss=3.9459657669067383
I0207 13:46:39.652043 139615980689152 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.23036149144172668, loss=3.8687825202941895
I0207 13:47:14.588969 139615972296448 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.21492618322372437, loss=3.974822998046875
I0207 13:47:49.532858 139615980689152 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.26100659370422363, loss=3.930992841720581
I0207 13:48:24.455887 139615972296448 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.23278217017650604, loss=3.8931727409362793
I0207 13:48:59.351814 139615980689152 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.23148538172245026, loss=3.865053415298462
I0207 13:49:34.297590 139615972296448 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.28765901923179626, loss=3.8901407718658447
I0207 13:50:09.238831 139615980689152 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2095392942428589, loss=3.9784111976623535
I0207 13:50:44.175050 139615972296448 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2199181616306305, loss=3.9368374347686768
I0207 13:51:19.077588 139615980689152 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.20215432345867157, loss=3.888939380645752
I0207 13:51:53.989485 139615972296448 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.19679413735866547, loss=3.932142734527588
I0207 13:52:28.893106 139615980689152 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.22541728615760803, loss=3.9660072326660156
I0207 13:53:03.807338 139615972296448 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.2893132269382477, loss=4.018876552581787
I0207 13:53:38.713285 139615980689152 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.21217823028564453, loss=3.947711706161499
I0207 13:54:13.627575 139615972296448 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2079562544822693, loss=3.992511034011841
I0207 13:54:48.575729 139615980689152 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.2372610718011856, loss=3.9759702682495117
I0207 13:55:23.488336 139615972296448 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.20839689671993256, loss=3.862398386001587
I0207 13:55:58.407299 139615980689152 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.22784677147865295, loss=3.943073272705078
I0207 13:56:33.316682 139615972296448 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.21295887231826782, loss=3.932274341583252
I0207 13:57:08.219223 139615980689152 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2500438392162323, loss=3.9485974311828613
I0207 13:57:43.129746 139615972296448 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.220971018075943, loss=3.9053351879119873
I0207 13:58:18.034285 139615980689152 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.22139431536197662, loss=3.9142680168151855
I0207 13:58:52.959428 139615972296448 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.22431053221225739, loss=3.941516399383545
I0207 13:59:22.724718 139785736898368 spec.py:321] Evaluating on the training split.
I0207 13:59:25.716954 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:02:16.777017 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 14:02:19.495786 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:04:54.264043 139785736898368 spec.py:349] Evaluating on the test split.
I0207 14:04:56.981113 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:07:14.640627 139785736898368 submission_runner.py:408] Time since start: 22477.58s, 	Step: 38487, 	{'train/accuracy': 0.6632543206214905, 'train/loss': 1.8003400564193726, 'train/bleu': 32.92000144955914, 'validation/accuracy': 0.66898113489151, 'validation/loss': 1.75272798538208, 'validation/bleu': 28.953533798359764, 'validation/num_examples': 3000, 'test/accuracy': 0.6821103096008301, 'test/loss': 1.679569959640503, 'test/bleu': 28.864725008764605, 'test/num_examples': 3003, 'score': 13468.718967199326, 'total_duration': 22477.581879138947, 'accumulated_submission_time': 13468.718967199326, 'accumulated_eval_time': 9007.191690206528, 'accumulated_logging_time': 0.4582366943359375}
I0207 14:07:14.660588 139615980689152 logging_writer.py:48] [38487] accumulated_eval_time=9007.191690, accumulated_logging_time=0.458237, accumulated_submission_time=13468.718967, global_step=38487, preemption_count=0, score=13468.718967, test/accuracy=0.682110, test/bleu=28.864725, test/loss=1.679570, test/num_examples=3003, total_duration=22477.581879, train/accuracy=0.663254, train/bleu=32.920001, train/loss=1.800340, validation/accuracy=0.668981, validation/bleu=28.953534, validation/loss=1.752728, validation/num_examples=3000
I0207 14:07:19.551581 139615972296448 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2252478003501892, loss=3.940502166748047
I0207 14:07:54.462810 139615980689152 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.22189414501190186, loss=3.9067156314849854
I0207 14:08:29.408298 139615972296448 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.26716896891593933, loss=3.913757085800171
I0207 14:09:04.340193 139615980689152 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.21162573993206024, loss=3.9091756343841553
I0207 14:09:39.256040 139615972296448 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.21467462182044983, loss=3.9070961475372314
I0207 14:10:14.195186 139615980689152 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.22308801114559174, loss=3.938059091567993
I0207 14:10:49.110968 139615972296448 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.21211129426956177, loss=3.9310081005096436
I0207 14:11:24.006052 139615980689152 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.24428045749664307, loss=3.833702325820923
I0207 14:11:58.950876 139615972296448 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.29673314094543457, loss=3.9471702575683594
I0207 14:12:33.868989 139615980689152 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.24824747443199158, loss=3.8756723403930664
I0207 14:13:08.796918 139615972296448 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.20873376727104187, loss=3.934049367904663
I0207 14:13:43.743609 139615980689152 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.25839126110076904, loss=3.8557400703430176
I0207 14:14:18.659946 139615972296448 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.24739009141921997, loss=3.9065091609954834
I0207 14:14:53.605655 139615980689152 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.25395357608795166, loss=3.9121131896972656
I0207 14:15:28.566040 139615972296448 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.31925299763679504, loss=3.90205454826355
I0207 14:16:03.527992 139615980689152 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.2198880910873413, loss=3.9257094860076904
I0207 14:16:38.437746 139615972296448 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.19713187217712402, loss=3.8955633640289307
I0207 14:17:13.336513 139615980689152 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.23206260800361633, loss=3.968752861022949
I0207 14:17:48.251163 139615972296448 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.20745901763439178, loss=3.9068448543548584
I0207 14:18:23.179016 139615980689152 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.22109031677246094, loss=3.9529099464416504
I0207 14:18:58.061333 139615972296448 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.24832096695899963, loss=3.9449827671051025
I0207 14:19:32.982074 139615980689152 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2097732573747635, loss=3.8283896446228027
I0207 14:20:07.899488 139615972296448 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3028027415275574, loss=3.9236032962799072
I0207 14:20:42.840965 139615980689152 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.2242027372121811, loss=3.930506706237793
I0207 14:21:14.714151 139785736898368 spec.py:321] Evaluating on the training split.
I0207 14:21:17.733931 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:24:20.555422 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 14:24:23.260748 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:26:55.690230 139785736898368 spec.py:349] Evaluating on the test split.
I0207 14:26:58.402234 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:29:16.326475 139785736898368 submission_runner.py:408] Time since start: 23799.27s, 	Step: 40893, 	{'train/accuracy': 0.6533471941947937, 'train/loss': 1.8556861877441406, 'train/bleu': 32.56679466777, 'validation/accuracy': 0.6716221570968628, 'validation/loss': 1.7270926237106323, 'validation/bleu': 29.09871189610873, 'validation/num_examples': 3000, 'test/accuracy': 0.6849921941757202, 'test/loss': 1.6443243026733398, 'test/bleu': 29.026990271202685, 'test/num_examples': 3003, 'score': 14308.6868288517, 'total_duration': 23799.267735242844, 'accumulated_submission_time': 14308.6868288517, 'accumulated_eval_time': 9488.803978919983, 'accumulated_logging_time': 0.48833346366882324}
I0207 14:29:16.347024 139615972296448 logging_writer.py:48] [40893] accumulated_eval_time=9488.803979, accumulated_logging_time=0.488333, accumulated_submission_time=14308.686829, global_step=40893, preemption_count=0, score=14308.686829, test/accuracy=0.684992, test/bleu=29.026990, test/loss=1.644324, test/num_examples=3003, total_duration=23799.267735, train/accuracy=0.653347, train/bleu=32.566795, train/loss=1.855686, validation/accuracy=0.671622, validation/bleu=29.098712, validation/loss=1.727093, validation/num_examples=3000
I0207 14:29:19.154455 139615980689152 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.2638474404811859, loss=3.982595205307007
I0207 14:29:54.098062 139615972296448 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.27198725938796997, loss=3.9498074054718018
I0207 14:30:29.021063 139615980689152 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.22707733511924744, loss=3.9438230991363525
I0207 14:31:03.914865 139615972296448 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3380170166492462, loss=3.8748271465301514
I0207 14:31:38.809556 139615980689152 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.22925694286823273, loss=3.9325506687164307
I0207 14:32:13.723252 139615972296448 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2171134501695633, loss=3.9204087257385254
I0207 14:32:48.654993 139615980689152 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.21031498908996582, loss=3.888012170791626
I0207 14:33:23.566093 139615972296448 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.25053513050079346, loss=3.940966844558716
I0207 14:33:58.488395 139615980689152 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2541382312774658, loss=3.9138996601104736
I0207 14:34:33.396768 139615972296448 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.2205580472946167, loss=3.95112943649292
I0207 14:35:08.320985 139615980689152 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.26705870032310486, loss=3.87774920463562
I0207 14:35:43.236399 139615972296448 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2967754602432251, loss=3.956190586090088
I0207 14:36:18.133459 139615980689152 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.23603147268295288, loss=3.9178478717803955
I0207 14:36:53.019787 139615972296448 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.2523126006126404, loss=3.892845630645752
I0207 14:37:27.930797 139615980689152 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.24093526601791382, loss=3.8692574501037598
I0207 14:38:02.838570 139615972296448 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.245378315448761, loss=3.8524937629699707
I0207 14:38:37.745497 139615980689152 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.29005977511405945, loss=3.891723394393921
I0207 14:39:12.672065 139615972296448 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.20792941749095917, loss=3.861182928085327
I0207 14:39:47.591615 139615980689152 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.23466020822525024, loss=3.901912212371826
I0207 14:40:22.520170 139615972296448 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.21915112435817719, loss=3.84491229057312
I0207 14:40:57.406889 139615980689152 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.20904065668582916, loss=3.837994337081909
I0207 14:41:32.293083 139615972296448 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24832606315612793, loss=3.902212381362915
I0207 14:42:07.182360 139615980689152 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.30194175243377686, loss=3.9193058013916016
I0207 14:42:42.078631 139615972296448 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.25152233242988586, loss=3.9119865894317627
I0207 14:43:16.345672 139785736898368 spec.py:321] Evaluating on the training split.
I0207 14:43:19.357630 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:46:59.939162 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 14:47:02.683865 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:50:34.138890 139785736898368 spec.py:349] Evaluating on the test split.
I0207 14:50:36.866435 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 14:53:20.085487 139785736898368 submission_runner.py:408] Time since start: 25243.03s, 	Step: 43300, 	{'train/accuracy': 0.6563020348548889, 'train/loss': 1.8518691062927246, 'train/bleu': 32.51621448980282, 'validation/accuracy': 0.6724033355712891, 'validation/loss': 1.733350157737732, 'validation/bleu': 29.32289596719666, 'validation/num_examples': 3000, 'test/accuracy': 0.685631275177002, 'test/loss': 1.6498874425888062, 'test/bleu': 28.75444159865201, 'test/num_examples': 3003, 'score': 15148.599982261658, 'total_duration': 25243.02674794197, 'accumulated_submission_time': 15148.599982261658, 'accumulated_eval_time': 10092.543762683868, 'accumulated_logging_time': 0.5201573371887207}
I0207 14:53:20.106282 139615980689152 logging_writer.py:48] [43300] accumulated_eval_time=10092.543763, accumulated_logging_time=0.520157, accumulated_submission_time=15148.599982, global_step=43300, preemption_count=0, score=15148.599982, test/accuracy=0.685631, test/bleu=28.754442, test/loss=1.649887, test/num_examples=3003, total_duration=25243.026748, train/accuracy=0.656302, train/bleu=32.516214, train/loss=1.851869, validation/accuracy=0.672403, validation/bleu=29.322896, validation/loss=1.733350, validation/num_examples=3000
I0207 14:53:20.484307 139615972296448 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.22220714390277863, loss=3.85178279876709
I0207 14:53:55.429660 139615980689152 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.2983648478984833, loss=3.9334628582000732
I0207 14:54:30.394648 139615972296448 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.22195811569690704, loss=3.882143974304199
I0207 14:55:05.298777 139615980689152 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.25599658489227295, loss=3.8761003017425537
I0207 14:55:40.240233 139615972296448 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.23234517872333527, loss=3.9257373809814453
I0207 14:56:15.176643 139615980689152 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.22093884646892548, loss=3.924856662750244
I0207 14:56:50.125394 139615972296448 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.22506530582904816, loss=3.898193597793579
I0207 14:57:25.029612 139615980689152 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2886850833892822, loss=3.901973009109497
I0207 14:57:59.951619 139615972296448 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.22651967406272888, loss=3.8644518852233887
I0207 14:58:34.876339 139615980689152 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.23829132318496704, loss=3.9029078483581543
I0207 14:59:09.796820 139615972296448 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.30920693278312683, loss=3.8836400508880615
I0207 14:59:44.729526 139615980689152 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2430419772863388, loss=3.86076283454895
I0207 15:00:19.664736 139615972296448 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2362232357263565, loss=3.929365873336792
I0207 15:00:54.579581 139615980689152 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2136642336845398, loss=3.826894521713257
I0207 15:01:29.499073 139615972296448 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2623054087162018, loss=3.8919999599456787
I0207 15:02:04.392416 139615980689152 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.28383874893188477, loss=3.907012462615967
I0207 15:02:39.312419 139615972296448 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2516200840473175, loss=3.9398937225341797
I0207 15:03:14.228695 139615980689152 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.26120099425315857, loss=3.881725788116455
I0207 15:03:49.135453 139615972296448 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.24114589393138885, loss=3.894237995147705
I0207 15:04:24.051177 139615980689152 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.28639042377471924, loss=3.8823461532592773
I0207 15:04:59.016158 139615972296448 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.23535721004009247, loss=3.9600512981414795
I0207 15:05:33.944020 139615980689152 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.22589074075222015, loss=3.9051711559295654
I0207 15:06:08.861457 139615972296448 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.23055219650268555, loss=3.906752109527588
I0207 15:06:43.787953 139615980689152 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.28053879737854004, loss=3.8946080207824707
I0207 15:07:18.736951 139615972296448 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.23818901181221008, loss=3.8615705966949463
I0207 15:07:20.215752 139785736898368 spec.py:321] Evaluating on the training split.
I0207 15:07:23.225960 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:10:21.028838 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 15:10:23.764423 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:13:11.012342 139785736898368 spec.py:349] Evaluating on the test split.
I0207 15:13:13.743096 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:15:45.070363 139785736898368 submission_runner.py:408] Time since start: 26588.01s, 	Step: 45706, 	{'train/accuracy': 0.6638458371162415, 'train/loss': 1.7846899032592773, 'train/bleu': 33.13194819782453, 'validation/accuracy': 0.6739159822463989, 'validation/loss': 1.7038495540618896, 'validation/bleu': 29.235713821138607, 'validation/num_examples': 3000, 'test/accuracy': 0.6875951886177063, 'test/loss': 1.6285982131958008, 'test/bleu': 29.441668743185758, 'test/num_examples': 3003, 'score': 15988.6215569973, 'total_duration': 26588.011610507965, 'accumulated_submission_time': 15988.6215569973, 'accumulated_eval_time': 10597.398320198059, 'accumulated_logging_time': 0.5518801212310791}
I0207 15:15:45.091498 139615980689152 logging_writer.py:48] [45706] accumulated_eval_time=10597.398320, accumulated_logging_time=0.551880, accumulated_submission_time=15988.621557, global_step=45706, preemption_count=0, score=15988.621557, test/accuracy=0.687595, test/bleu=29.441669, test/loss=1.628598, test/num_examples=3003, total_duration=26588.011611, train/accuracy=0.663846, train/bleu=33.131948, train/loss=1.784690, validation/accuracy=0.673916, validation/bleu=29.235714, validation/loss=1.703850, validation/num_examples=3000
I0207 15:16:18.265344 139615972296448 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.23441053926944733, loss=3.851818799972534
I0207 15:16:53.192832 139615980689152 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2362888604402542, loss=3.8947722911834717
I0207 15:17:28.112151 139615972296448 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.21664586663246155, loss=3.882497787475586
I0207 15:18:03.046439 139615980689152 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.22688041627407074, loss=3.930795669555664
I0207 15:18:37.969155 139615972296448 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2570589482784271, loss=3.923410654067993
I0207 15:19:12.893492 139615980689152 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.24781785905361176, loss=3.8804378509521484
I0207 15:19:47.798275 139615972296448 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.2641993761062622, loss=3.857545852661133
I0207 15:20:22.773689 139615980689152 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.22876837849617004, loss=3.881300210952759
I0207 15:20:57.686833 139615972296448 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2508566677570343, loss=3.840014934539795
I0207 15:21:32.638622 139615980689152 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.22661924362182617, loss=3.9722230434417725
I0207 15:22:07.562908 139615972296448 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2694956660270691, loss=3.83721661567688
I0207 15:22:42.462988 139615980689152 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.26642343401908875, loss=3.86968731880188
I0207 15:23:17.378027 139615972296448 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.24128419160842896, loss=3.891434907913208
I0207 15:23:52.308482 139615980689152 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.24585658311843872, loss=3.8620991706848145
I0207 15:24:27.225760 139615972296448 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2084934413433075, loss=3.830704927444458
I0207 15:25:02.127087 139615980689152 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.22275444865226746, loss=3.842067003250122
I0207 15:25:37.039163 139615972296448 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.27036455273628235, loss=3.8574235439300537
I0207 15:26:11.980051 139615980689152 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.23119397461414337, loss=3.8766400814056396
I0207 15:26:46.910070 139615972296448 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.25543057918548584, loss=3.876410484313965
I0207 15:27:21.835847 139615980689152 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.29209068417549133, loss=3.9118363857269287
I0207 15:27:56.745882 139615972296448 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.24716106057167053, loss=3.8341503143310547
I0207 15:28:31.650970 139615980689152 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.24052377045154572, loss=3.867769956588745
I0207 15:29:06.587791 139615972296448 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.21834373474121094, loss=3.8296396732330322
I0207 15:29:41.512581 139615980689152 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.272042453289032, loss=3.9037110805511475
I0207 15:29:45.087775 139785736898368 spec.py:321] Evaluating on the training split.
I0207 15:29:48.115949 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:32:42.958147 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 15:32:45.673846 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:35:24.013101 139785736898368 spec.py:349] Evaluating on the test split.
I0207 15:35:26.737690 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:37:51.947865 139785736898368 submission_runner.py:408] Time since start: 27914.89s, 	Step: 48112, 	{'train/accuracy': 0.6603615880012512, 'train/loss': 1.8153201341629028, 'train/bleu': 32.8939815446997, 'validation/accuracy': 0.6750814914703369, 'validation/loss': 1.6979299783706665, 'validation/bleu': 29.456297594517306, 'validation/num_examples': 3000, 'test/accuracy': 0.6886642575263977, 'test/loss': 1.616563081741333, 'test/bleu': 29.52146011305364, 'test/num_examples': 3003, 'score': 16828.530921697617, 'total_duration': 27914.889127254486, 'accumulated_submission_time': 16828.530921697617, 'accumulated_eval_time': 11084.258370399475, 'accumulated_logging_time': 0.5843021869659424}
I0207 15:37:51.969219 139615972296448 logging_writer.py:48] [48112] accumulated_eval_time=11084.258370, accumulated_logging_time=0.584302, accumulated_submission_time=16828.530922, global_step=48112, preemption_count=0, score=16828.530922, test/accuracy=0.688664, test/bleu=29.521460, test/loss=1.616563, test/num_examples=3003, total_duration=27914.889127, train/accuracy=0.660362, train/bleu=32.893982, train/loss=1.815320, validation/accuracy=0.675081, validation/bleu=29.456298, validation/loss=1.697930, validation/num_examples=3000
I0207 15:38:23.057081 139615980689152 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.2605966627597809, loss=3.8782958984375
I0207 15:38:57.986467 139615972296448 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2477857619524002, loss=3.9050121307373047
I0207 15:39:32.960790 139615980689152 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.22343511879444122, loss=3.873094081878662
I0207 15:40:07.886619 139615972296448 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2199728786945343, loss=3.8492183685302734
I0207 15:40:42.842862 139615980689152 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.26741474866867065, loss=3.920247793197632
I0207 15:41:17.748474 139615972296448 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.23399841785430908, loss=3.8739187717437744
I0207 15:41:52.631897 139615980689152 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.25047245621681213, loss=3.8348822593688965
I0207 15:42:27.625537 139615972296448 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.24466019868850708, loss=3.948564052581787
I0207 15:43:02.633784 139615980689152 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.29819831252098083, loss=3.8806700706481934
I0207 15:43:37.582914 139615972296448 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.23724734783172607, loss=3.8828530311584473
I0207 15:44:12.491175 139615980689152 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.23880870640277863, loss=3.9037253856658936
I0207 15:44:47.400776 139615972296448 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3237527906894684, loss=3.9050557613372803
I0207 15:45:22.314037 139615980689152 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.27006956934928894, loss=3.853942394256592
I0207 15:45:57.237752 139615972296448 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.23423756659030914, loss=3.9146652221679688
I0207 15:46:32.161870 139615980689152 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.24268090724945068, loss=3.894871234893799
I0207 15:47:07.081284 139615972296448 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.24781443178653717, loss=3.8502614498138428
I0207 15:47:41.983748 139615980689152 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.26560065150260925, loss=3.8699610233306885
I0207 15:48:16.935802 139615972296448 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.25413283705711365, loss=3.924252986907959
I0207 15:48:51.786244 139615980689152 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7293765544891357, loss=5.572413921356201
I0207 15:49:26.711275 139615972296448 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.2727161645889282, loss=3.985426902770996
I0207 15:50:01.633442 139615980689152 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.26247262954711914, loss=3.976699113845825
I0207 15:50:36.537374 139615972296448 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.22657866775989532, loss=3.8379745483398438
I0207 15:51:11.447302 139615980689152 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.22599592804908752, loss=3.8607821464538574
I0207 15:51:46.358011 139615972296448 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.27965018153190613, loss=3.892822027206421
I0207 15:51:52.026296 139785736898368 spec.py:321] Evaluating on the training split.
I0207 15:51:55.029706 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:54:37.984917 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 15:54:40.713837 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:57:09.824520 139785736898368 spec.py:349] Evaluating on the test split.
I0207 15:57:12.556837 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 15:59:35.017920 139785736898368 submission_runner.py:408] Time since start: 29217.96s, 	Step: 50518, 	{'train/accuracy': 0.669950544834137, 'train/loss': 1.7502673864364624, 'train/bleu': 33.24540770074166, 'validation/accuracy': 0.6771769523620605, 'validation/loss': 1.6892192363739014, 'validation/bleu': 29.521489562011492, 'validation/num_examples': 3000, 'test/accuracy': 0.6907094717025757, 'test/loss': 1.609735131263733, 'test/bleu': 29.501761063552795, 'test/num_examples': 3003, 'score': 17668.503177165985, 'total_duration': 29217.95916581154, 'accumulated_submission_time': 17668.503177165985, 'accumulated_eval_time': 11547.249927043915, 'accumulated_logging_time': 0.6154048442840576}
I0207 15:59:35.039720 139615980689152 logging_writer.py:48] [50518] accumulated_eval_time=11547.249927, accumulated_logging_time=0.615405, accumulated_submission_time=17668.503177, global_step=50518, preemption_count=0, score=17668.503177, test/accuracy=0.690709, test/bleu=29.501761, test/loss=1.609735, test/num_examples=3003, total_duration=29217.959166, train/accuracy=0.669951, train/bleu=33.245408, train/loss=1.750267, validation/accuracy=0.677177, validation/bleu=29.521490, validation/loss=1.689219, validation/num_examples=3000
I0207 16:00:04.078093 139615972296448 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.24415260553359985, loss=3.8687682151794434
I0207 16:00:39.060736 139615980689152 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.24662956595420837, loss=3.9425907135009766
I0207 16:01:13.981145 139615972296448 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2546529471874237, loss=3.8143608570098877
I0207 16:01:48.917191 139615980689152 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.24586176872253418, loss=3.8771750926971436
I0207 16:02:23.867613 139615972296448 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.24049128592014313, loss=3.860863208770752
I0207 16:02:58.799421 139615980689152 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23833680152893066, loss=3.8394582271575928
I0207 16:03:33.726197 139615972296448 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.221505269408226, loss=3.9058475494384766
I0207 16:04:08.631268 139615980689152 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.24252791702747345, loss=3.8940117359161377
I0207 16:04:43.557158 139615972296448 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.2334609180688858, loss=3.9120819568634033
I0207 16:05:18.480875 139615980689152 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2220003455877304, loss=3.785356044769287
I0207 16:05:53.392998 139615972296448 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.22959420084953308, loss=3.8952550888061523
I0207 16:06:28.350205 139615980689152 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.23350320756435394, loss=3.8533666133880615
I0207 16:07:03.272109 139615972296448 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.23742495477199554, loss=3.9176483154296875
I0207 16:07:38.167319 139615980689152 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.27043387293815613, loss=3.896738290786743
I0207 16:08:13.091248 139615972296448 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2354576140642166, loss=3.8821213245391846
I0207 16:08:48.018338 139615980689152 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.25079697370529175, loss=3.8897511959075928
I0207 16:09:22.956864 139615972296448 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.24787309765815735, loss=3.8460514545440674
I0207 16:09:57.903196 139615980689152 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.24690333008766174, loss=3.915281295776367
I0207 16:10:32.860475 139615972296448 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.23176269233226776, loss=3.8805551528930664
I0207 16:11:07.791129 139615980689152 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.31502580642700195, loss=3.850322961807251
I0207 16:11:42.717639 139615972296448 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2495207041501999, loss=3.846902847290039
I0207 16:12:17.661597 139615980689152 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2724817097187042, loss=3.8907880783081055
I0207 16:12:52.636075 139615972296448 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.24730046093463898, loss=3.8783276081085205
I0207 16:13:27.565193 139615980689152 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.23585091531276703, loss=3.853566884994507
I0207 16:13:35.307401 139785736898368 spec.py:321] Evaluating on the training split.
I0207 16:13:38.302563 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 16:17:46.740482 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 16:17:49.444623 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 16:20:31.460194 139785736898368 spec.py:349] Evaluating on the test split.
I0207 16:20:34.174011 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 16:23:18.213459 139785736898368 submission_runner.py:408] Time since start: 30641.15s, 	Step: 52924, 	{'train/accuracy': 0.6591640114784241, 'train/loss': 1.8146765232086182, 'train/bleu': 32.61795057033321, 'validation/accuracy': 0.6771769523620605, 'validation/loss': 1.7013094425201416, 'validation/bleu': 29.5259645039707, 'validation/num_examples': 3000, 'test/accuracy': 0.6914182901382446, 'test/loss': 1.6207830905914307, 'test/bleu': 29.616646797154754, 'test/num_examples': 3003, 'score': 18508.683936357498, 'total_duration': 30641.154708862305, 'accumulated_submission_time': 18508.683936357498, 'accumulated_eval_time': 12130.155924797058, 'accumulated_logging_time': 0.6474461555480957}
I0207 16:23:18.234575 139615972296448 logging_writer.py:48] [52924] accumulated_eval_time=12130.155925, accumulated_logging_time=0.647446, accumulated_submission_time=18508.683936, global_step=52924, preemption_count=0, score=18508.683936, test/accuracy=0.691418, test/bleu=29.616647, test/loss=1.620783, test/num_examples=3003, total_duration=30641.154709, train/accuracy=0.659164, train/bleu=32.617951, train/loss=1.814677, validation/accuracy=0.677177, validation/bleu=29.525965, validation/loss=1.701309, validation/num_examples=3000
I0207 16:23:45.104450 139615980689152 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.27626997232437134, loss=3.896418333053589
I0207 16:24:20.023718 139615972296448 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.25663524866104126, loss=3.873004198074341
I0207 16:24:54.969294 139615980689152 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.31086739897727966, loss=3.8556599617004395
I0207 16:25:29.922966 139615972296448 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2937358319759369, loss=3.828367233276367
I0207 16:26:04.853660 139615980689152 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.22518722712993622, loss=3.841372489929199
I0207 16:26:39.760711 139615972296448 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.24252115190029144, loss=3.872272253036499
I0207 16:27:14.667884 139615980689152 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2371179610490799, loss=3.8696091175079346
I0207 16:27:49.566572 139615972296448 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.252991646528244, loss=3.8003811836242676
I0207 16:28:24.544637 139615980689152 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2829902768135071, loss=3.8573739528656006
I0207 16:28:59.454588 139615972296448 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.25034457445144653, loss=3.865598201751709
I0207 16:29:34.381186 139615980689152 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.24927610158920288, loss=3.872950315475464
I0207 16:30:09.288731 139615972296448 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.23771901428699493, loss=3.813647985458374
I0207 16:30:44.232677 139615980689152 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.2794877290725708, loss=3.9287006855010986
I0207 16:31:19.179674 139615972296448 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.26249679923057556, loss=3.8075244426727295
I0207 16:31:54.140085 139615980689152 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.24056366086006165, loss=3.8245620727539062
I0207 16:32:29.040695 139615972296448 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.26411378383636475, loss=3.8288819789886475
I0207 16:33:03.948753 139615980689152 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.24050694704055786, loss=3.8836052417755127
I0207 16:33:38.846906 139615972296448 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.24726277589797974, loss=3.8254549503326416
I0207 16:34:13.753395 139615980689152 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2600548565387726, loss=3.841824769973755
I0207 16:34:48.667285 139615972296448 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.237661674618721, loss=3.843834638595581
I0207 16:35:23.663103 139615980689152 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.29051777720451355, loss=3.8488857746124268
I0207 16:35:58.627008 139615972296448 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.24730393290519714, loss=3.834202289581299
I0207 16:36:33.548412 139615980689152 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.25083568692207336, loss=3.8104562759399414
I0207 16:37:08.449843 139615972296448 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.22682024538516998, loss=3.901057481765747
I0207 16:37:18.290771 139785736898368 spec.py:321] Evaluating on the training split.
I0207 16:37:21.313480 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 16:41:34.700566 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 16:41:37.421477 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 16:44:38.630409 139785736898368 spec.py:349] Evaluating on the test split.
I0207 16:44:41.349787 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 16:47:43.259323 139785736898368 submission_runner.py:408] Time since start: 32106.20s, 	Step: 55330, 	{'train/accuracy': 0.6601080894470215, 'train/loss': 1.815306544303894, 'train/bleu': 32.35156533613473, 'validation/accuracy': 0.677424967288971, 'validation/loss': 1.687734603881836, 'validation/bleu': 29.76204785307309, 'validation/num_examples': 3000, 'test/accuracy': 0.6917552947998047, 'test/loss': 1.6076879501342773, 'test/bleu': 29.35782497609175, 'test/num_examples': 3003, 'score': 19348.650020122528, 'total_duration': 32106.200585126877, 'accumulated_submission_time': 19348.650020122528, 'accumulated_eval_time': 12755.12444281578, 'accumulated_logging_time': 0.679734468460083}
I0207 16:47:43.281794 139615980689152 logging_writer.py:48] [55330] accumulated_eval_time=12755.124443, accumulated_logging_time=0.679734, accumulated_submission_time=19348.650020, global_step=55330, preemption_count=0, score=19348.650020, test/accuracy=0.691755, test/bleu=29.357825, test/loss=1.607688, test/num_examples=3003, total_duration=32106.200585, train/accuracy=0.660108, train/bleu=32.351565, train/loss=1.815307, validation/accuracy=0.677425, validation/bleu=29.762048, validation/loss=1.687735, validation/num_examples=3000
I0207 16:48:08.008350 139615972296448 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.2613387703895569, loss=3.872624635696411
I0207 16:48:42.851269 139615980689152 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.24660535156726837, loss=3.8704638481140137
I0207 16:49:17.735214 139615972296448 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2521599531173706, loss=3.88989520072937
I0207 16:49:52.655918 139615980689152 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.25284233689308167, loss=3.886713743209839
I0207 16:50:27.598569 139615972296448 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.24655123054981232, loss=3.8748679161071777
I0207 16:51:02.485937 139615980689152 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2468997985124588, loss=3.807725191116333
I0207 16:51:37.427351 139615972296448 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2894071042537689, loss=3.8162176609039307
I0207 16:52:12.330466 139615980689152 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2474847435951233, loss=3.7975661754608154
I0207 16:52:47.218728 139615972296448 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2603347897529602, loss=3.8927531242370605
I0207 16:53:22.140717 139615980689152 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.24936236441135406, loss=3.862910509109497
I0207 16:53:57.031193 139615972296448 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3449493944644928, loss=3.90012526512146
I0207 16:54:31.929864 139615980689152 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2608813941478729, loss=3.8313395977020264
I0207 16:55:06.853527 139615972296448 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2660704255104065, loss=3.9245545864105225
I0207 16:55:41.789473 139615980689152 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.25121819972991943, loss=3.80264949798584
I0207 16:56:16.710239 139615972296448 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.25239086151123047, loss=3.909086227416992
I0207 16:56:51.617598 139615980689152 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2413366138935089, loss=3.836118221282959
I0207 16:57:26.531476 139615972296448 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2720623314380646, loss=3.867586612701416
I0207 16:58:01.435785 139615980689152 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.24629837274551392, loss=3.8400604724884033
I0207 16:58:36.346010 139615972296448 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2569846510887146, loss=3.883521318435669
I0207 16:59:11.243771 139615980689152 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.25731518864631653, loss=3.9236130714416504
I0207 16:59:46.189594 139615972296448 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.24334940314292908, loss=3.855668306350708
I0207 17:00:21.156101 139615980689152 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.23191815614700317, loss=3.8183271884918213
I0207 17:00:56.102619 139615972296448 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.259473979473114, loss=3.8006598949432373
I0207 17:01:30.986215 139615980689152 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.26762503385543823, loss=3.855095148086548
I0207 17:01:43.277733 139785736898368 spec.py:321] Evaluating on the training split.
I0207 17:01:46.275488 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:04:49.740360 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 17:04:52.453865 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:07:28.131518 139785736898368 spec.py:349] Evaluating on the test split.
I0207 17:07:30.853051 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:10:04.068616 139785736898368 submission_runner.py:408] Time since start: 33447.01s, 	Step: 57737, 	{'train/accuracy': 0.6684832572937012, 'train/loss': 1.7588273286819458, 'train/bleu': 33.19071769118694, 'validation/accuracy': 0.6785284876823425, 'validation/loss': 1.6828522682189941, 'validation/bleu': 29.483006700913737, 'validation/num_examples': 3000, 'test/accuracy': 0.6926733255386353, 'test/loss': 1.5996352434158325, 'test/bleu': 29.221216444179152, 'test/num_examples': 3003, 'score': 20188.56041240692, 'total_duration': 33447.00986433029, 'accumulated_submission_time': 20188.56041240692, 'accumulated_eval_time': 13255.915263652802, 'accumulated_logging_time': 0.7129116058349609}
I0207 17:10:04.091486 139615972296448 logging_writer.py:48] [57737] accumulated_eval_time=13255.915264, accumulated_logging_time=0.712912, accumulated_submission_time=20188.560412, global_step=57737, preemption_count=0, score=20188.560412, test/accuracy=0.692673, test/bleu=29.221216, test/loss=1.599635, test/num_examples=3003, total_duration=33447.009864, train/accuracy=0.668483, train/bleu=33.190718, train/loss=1.758827, validation/accuracy=0.678528, validation/bleu=29.483007, validation/loss=1.682852, validation/num_examples=3000
I0207 17:10:26.437444 139615980689152 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2901458144187927, loss=3.8344202041625977
I0207 17:11:01.368637 139615972296448 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.25374987721443176, loss=3.821359157562256
I0207 17:11:36.257696 139615980689152 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.24498958885669708, loss=3.80979323387146
I0207 17:12:11.163861 139615972296448 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.25875434279441833, loss=3.7911274433135986
I0207 17:12:46.175967 139615980689152 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.2549421489238739, loss=3.802546262741089
I0207 17:13:21.112777 139615972296448 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2826080620288849, loss=3.835688591003418
I0207 17:13:56.009056 139615980689152 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.24946531653404236, loss=3.809576988220215
I0207 17:14:30.922955 139615972296448 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.24314279854297638, loss=3.817922353744507
I0207 17:15:05.818295 139615980689152 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.23520617187023163, loss=3.8461313247680664
I0207 17:15:40.727996 139615972296448 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.265243798494339, loss=3.8632476329803467
I0207 17:16:15.636293 139615980689152 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2512393295764923, loss=3.867367744445801
I0207 17:16:50.589831 139615972296448 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.2623598873615265, loss=3.8725833892822266
I0207 17:17:25.483320 139615980689152 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.254426509141922, loss=3.8388540744781494
I0207 17:18:00.381583 139615972296448 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.2812792956829071, loss=3.8610243797302246
I0207 17:18:35.287787 139615980689152 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.29753267765045166, loss=3.841944932937622
I0207 17:19:10.199396 139615972296448 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2446872889995575, loss=3.854769229888916
I0207 17:19:45.134806 139615980689152 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2627514600753784, loss=3.862992286682129
I0207 17:20:20.060058 139615972296448 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2656375467777252, loss=3.8357481956481934
I0207 17:20:54.965206 139615980689152 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2971303164958954, loss=3.8290798664093018
I0207 17:21:29.888561 139615972296448 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3381491005420685, loss=3.802794933319092
I0207 17:22:04.791068 139615980689152 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.23739750683307648, loss=3.8331189155578613
I0207 17:22:39.706541 139615972296448 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.23283100128173828, loss=3.7554290294647217
I0207 17:23:14.607521 139615980689152 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.23836034536361694, loss=3.813654661178589
I0207 17:23:49.507119 139615972296448 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.24336346983909607, loss=3.856920003890991
I0207 17:24:04.244965 139785736898368 spec.py:321] Evaluating on the training split.
I0207 17:24:07.245484 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:27:50.736439 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 17:27:53.444732 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:30:28.763062 139785736898368 spec.py:349] Evaluating on the test split.
I0207 17:30:31.468081 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:33:05.430363 139785736898368 submission_runner.py:408] Time since start: 34828.37s, 	Step: 60144, 	{'train/accuracy': 0.6650095582008362, 'train/loss': 1.7817625999450684, 'train/bleu': 33.05104338028556, 'validation/accuracy': 0.6779953241348267, 'validation/loss': 1.6812210083007812, 'validation/bleu': 29.612110187777798, 'validation/num_examples': 3000, 'test/accuracy': 0.6945558190345764, 'test/loss': 1.593664526939392, 'test/bleu': 29.650212730852033, 'test/num_examples': 3003, 'score': 21028.63009619713, 'total_duration': 34828.37162208557, 'accumulated_submission_time': 21028.63009619713, 'accumulated_eval_time': 13797.100610494614, 'accumulated_logging_time': 0.7455697059631348}
I0207 17:33:05.453467 139615980689152 logging_writer.py:48] [60144] accumulated_eval_time=13797.100610, accumulated_logging_time=0.745570, accumulated_submission_time=21028.630096, global_step=60144, preemption_count=0, score=21028.630096, test/accuracy=0.694556, test/bleu=29.650213, test/loss=1.593665, test/num_examples=3003, total_duration=34828.371622, train/accuracy=0.665010, train/bleu=33.051043, train/loss=1.781763, validation/accuracy=0.677995, validation/bleu=29.612110, validation/loss=1.681221, validation/num_examples=3000
I0207 17:33:25.433957 139615972296448 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.2420247346162796, loss=3.8635830879211426
I0207 17:34:00.397790 139615980689152 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.2418665885925293, loss=3.853623867034912
I0207 17:34:35.408832 139615972296448 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.369790256023407, loss=3.8133656978607178
I0207 17:35:10.292573 139615980689152 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.24324174225330353, loss=3.799875259399414
I0207 17:35:45.203355 139615972296448 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.25895529985427856, loss=3.845763683319092
I0207 17:36:20.119415 139615980689152 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.25585484504699707, loss=3.800814390182495
I0207 17:36:55.018939 139615972296448 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.30018365383148193, loss=3.8834710121154785
I0207 17:37:29.918205 139615980689152 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.24045787751674652, loss=3.9138171672821045
I0207 17:38:04.833461 139615972296448 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2702050507068634, loss=3.8583717346191406
I0207 17:38:39.786337 139615980689152 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.2960899770259857, loss=3.8304924964904785
I0207 17:39:14.754941 139615972296448 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2566203474998474, loss=3.827322483062744
I0207 17:39:49.658232 139615980689152 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.26962724328041077, loss=3.8239665031433105
I0207 17:40:24.600122 139615972296448 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2523898482322693, loss=3.830817937850952
I0207 17:40:59.501634 139615980689152 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2528475224971771, loss=3.820085048675537
I0207 17:41:34.422324 139615972296448 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.28942692279815674, loss=3.7874739170074463
I0207 17:42:09.322503 139615980689152 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.27203693985939026, loss=3.81866455078125
I0207 17:42:44.246086 139615972296448 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.29075151681900024, loss=3.8548057079315186
I0207 17:43:19.188693 139615980689152 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2377094328403473, loss=3.793985605239868
I0207 17:43:54.155432 139615972296448 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.28635063767433167, loss=3.911472797393799
I0207 17:44:29.065640 139615980689152 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.28232190012931824, loss=3.7727580070495605
I0207 17:45:03.967457 139615972296448 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.27434012293815613, loss=3.829266309738159
I0207 17:45:38.905596 139615980689152 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.28458279371261597, loss=3.830786943435669
I0207 17:46:13.833065 139615972296448 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.25835534930229187, loss=3.8182411193847656
I0207 17:46:48.751626 139615980689152 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.29028254747390747, loss=3.87036395072937
I0207 17:47:05.571449 139785736898368 spec.py:321] Evaluating on the training split.
I0207 17:47:08.573319 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:51:40.380067 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 17:51:43.096523 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:55:00.515515 139785736898368 spec.py:349] Evaluating on the test split.
I0207 17:55:03.225109 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 17:58:02.523284 139785736898368 submission_runner.py:408] Time since start: 36325.46s, 	Step: 62550, 	{'train/accuracy': 0.7026515007019043, 'train/loss': 1.5845561027526855, 'train/bleu': 35.26644664948576, 'validation/accuracy': 0.6804379224777222, 'validation/loss': 1.6678342819213867, 'validation/bleu': 30.021677303349723, 'validation/num_examples': 3000, 'test/accuracy': 0.6963453888893127, 'test/loss': 1.5847262144088745, 'test/bleu': 29.76351245408125, 'test/num_examples': 3003, 'score': 21868.660029172897, 'total_duration': 36325.464529037476, 'accumulated_submission_time': 21868.660029172897, 'accumulated_eval_time': 14454.052380561829, 'accumulated_logging_time': 0.7788140773773193}
I0207 17:58:02.546601 139615972296448 logging_writer.py:48] [62550] accumulated_eval_time=14454.052381, accumulated_logging_time=0.778814, accumulated_submission_time=21868.660029, global_step=62550, preemption_count=0, score=21868.660029, test/accuracy=0.696345, test/bleu=29.763512, test/loss=1.584726, test/num_examples=3003, total_duration=36325.464529, train/accuracy=0.702652, train/bleu=35.266447, train/loss=1.584556, validation/accuracy=0.680438, validation/bleu=30.021677, validation/loss=1.667834, validation/num_examples=3000
I0207 17:58:20.328356 139615980689152 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.27047333121299744, loss=3.831681251525879
I0207 17:58:55.200463 139615972296448 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.267734169960022, loss=3.858358144760132
I0207 17:59:30.097917 139615980689152 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2733978033065796, loss=3.8028953075408936
I0207 18:00:05.022058 139615972296448 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.28779634833335876, loss=3.8223609924316406
I0207 18:00:39.908024 139615980689152 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.26259538531303406, loss=3.8522462844848633
I0207 18:01:14.864286 139615972296448 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.29693180322647095, loss=3.7974932193756104
I0207 18:01:49.853607 139615980689152 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.24027810990810394, loss=3.7594151496887207
I0207 18:02:24.896539 139615972296448 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.24456261098384857, loss=3.817542314529419
I0207 18:02:59.841920 139615980689152 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.25351840257644653, loss=3.8549933433532715
I0207 18:03:34.798704 139615972296448 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.28936347365379333, loss=3.865726947784424
I0207 18:04:09.727747 139615980689152 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2752567529678345, loss=3.8659684658050537
I0207 18:04:44.682823 139615972296448 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.271509051322937, loss=3.8545033931732178
I0207 18:05:19.636494 139615980689152 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.2706257700920105, loss=3.8573861122131348
I0207 18:05:54.559410 139615972296448 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.25069668889045715, loss=3.8327460289001465
I0207 18:06:29.470148 139615980689152 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.24992570281028748, loss=3.8375327587127686
I0207 18:07:04.419685 139615972296448 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.266739159822464, loss=3.7498936653137207
I0207 18:07:39.325105 139615980689152 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.25520816445350647, loss=3.8934173583984375
I0207 18:08:14.282513 139615972296448 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.25786736607551575, loss=3.8377113342285156
I0207 18:08:49.223393 139615980689152 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.253422349691391, loss=3.9233193397521973
I0207 18:09:24.140214 139615972296448 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2630874514579773, loss=3.8648717403411865
I0207 18:09:59.095876 139615980689152 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2706223726272583, loss=3.841458559036255
I0207 18:10:34.067180 139615972296448 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.2952214181423187, loss=3.765805244445801
I0207 18:11:08.965827 139615980689152 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.25960609316825867, loss=3.848048210144043
I0207 18:11:43.888562 139615972296448 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2560068368911743, loss=3.847843647003174
I0207 18:12:02.787270 139785736898368 spec.py:321] Evaluating on the training split.
I0207 18:12:05.792204 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:14:48.052495 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 18:14:50.750043 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:17:24.208700 139785736898368 spec.py:349] Evaluating on the test split.
I0207 18:17:26.922417 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:19:49.901495 139785736898368 submission_runner.py:408] Time since start: 37632.84s, 	Step: 64956, 	{'train/accuracy': 0.6722168922424316, 'train/loss': 1.7237827777862549, 'train/bleu': 33.19875507863805, 'validation/accuracy': 0.6810950636863708, 'validation/loss': 1.657042384147644, 'validation/bleu': 29.83164370579329, 'validation/num_examples': 3000, 'test/accuracy': 0.6964964270591736, 'test/loss': 1.5666615962982178, 'test/bleu': 30.08841979180686, 'test/num_examples': 3003, 'score': 22708.81247138977, 'total_duration': 37632.84275865555, 'accumulated_submission_time': 22708.81247138977, 'accumulated_eval_time': 14921.166560411453, 'accumulated_logging_time': 0.8124041557312012}
I0207 18:19:49.924972 139615980689152 logging_writer.py:48] [64956] accumulated_eval_time=14921.166560, accumulated_logging_time=0.812404, accumulated_submission_time=22708.812471, global_step=64956, preemption_count=0, score=22708.812471, test/accuracy=0.696496, test/bleu=30.088420, test/loss=1.566662, test/num_examples=3003, total_duration=37632.842759, train/accuracy=0.672217, train/bleu=33.198755, train/loss=1.723783, validation/accuracy=0.681095, validation/bleu=29.831644, validation/loss=1.657042, validation/num_examples=3000
I0207 18:20:05.645372 139615972296448 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.26546621322631836, loss=3.83526349067688
I0207 18:20:40.567175 139615980689152 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2667582035064697, loss=3.80682373046875
I0207 18:21:15.514742 139615972296448 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.2719164490699768, loss=3.8334975242614746
I0207 18:21:50.440281 139615980689152 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2624676525592804, loss=3.734910011291504
I0207 18:22:25.394940 139615972296448 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.27101531624794006, loss=3.8894970417022705
I0207 18:23:00.348355 139615980689152 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2668757140636444, loss=3.849829912185669
I0207 18:23:35.255246 139615972296448 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2823338210582733, loss=3.855196952819824
I0207 18:24:10.170081 139615980689152 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.27891284227371216, loss=3.770193099975586
I0207 18:24:45.070090 139615972296448 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.25082430243492126, loss=3.7960548400878906
I0207 18:25:19.996742 139615980689152 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2556661367416382, loss=3.828016757965088
I0207 18:25:54.926930 139615972296448 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2610190510749817, loss=3.7776107788085938
I0207 18:26:29.876756 139615980689152 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.24893222749233246, loss=3.8003618717193604
I0207 18:27:04.783843 139615972296448 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.25774329900741577, loss=3.841287612915039
I0207 18:27:39.687421 139615980689152 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.277366578578949, loss=3.7890827655792236
I0207 18:28:14.594903 139615972296448 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2821092903614044, loss=3.779770851135254
I0207 18:28:49.519451 139615980689152 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.29558494687080383, loss=3.8315625190734863
I0207 18:29:24.430524 139615972296448 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2588207721710205, loss=3.859351873397827
I0207 18:29:59.455499 139615980689152 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2736075222492218, loss=3.815089225769043
I0207 18:30:34.373032 139615972296448 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.25282564759254456, loss=3.8638880252838135
I0207 18:31:09.385180 139615980689152 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2653430700302124, loss=3.758620500564575
I0207 18:31:44.324182 139615972296448 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2902868986129761, loss=3.849975347518921
I0207 18:32:19.216575 139615980689152 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3286820352077484, loss=3.8472793102264404
I0207 18:32:54.143767 139615972296448 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2605781555175781, loss=3.7887983322143555
I0207 18:33:29.071719 139615980689152 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.27425339818000793, loss=3.7856285572052
I0207 18:33:50.115973 139785736898368 spec.py:321] Evaluating on the training split.
I0207 18:33:53.115932 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:37:14.446855 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 18:37:17.181112 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:39:45.405838 139785736898368 spec.py:349] Evaluating on the test split.
I0207 18:39:48.162045 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:42:15.388227 139785736898368 submission_runner.py:408] Time since start: 38978.33s, 	Step: 67362, 	{'train/accuracy': 0.6712957620620728, 'train/loss': 1.734437108039856, 'train/bleu': 33.597483300602605, 'validation/accuracy': 0.6810950636863708, 'validation/loss': 1.6491377353668213, 'validation/bleu': 29.94199898103257, 'validation/num_examples': 3000, 'test/accuracy': 0.6967636942863464, 'test/loss': 1.5596245527267456, 'test/bleu': 29.930429323320606, 'test/num_examples': 3003, 'score': 23548.917605161667, 'total_duration': 38978.3294506073, 'accumulated_submission_time': 23548.917605161667, 'accumulated_eval_time': 15426.43873333931, 'accumulated_logging_time': 0.8456981182098389}
I0207 18:42:15.418748 139615972296448 logging_writer.py:48] [67362] accumulated_eval_time=15426.438733, accumulated_logging_time=0.845698, accumulated_submission_time=23548.917605, global_step=67362, preemption_count=0, score=23548.917605, test/accuracy=0.696764, test/bleu=29.930429, test/loss=1.559625, test/num_examples=3003, total_duration=38978.329451, train/accuracy=0.671296, train/bleu=33.597483, train/loss=1.734437, validation/accuracy=0.681095, validation/bleu=29.941999, validation/loss=1.649138, validation/num_examples=3000
I0207 18:42:29.042907 139615980689152 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.28496813774108887, loss=3.8126773834228516
I0207 18:43:04.006596 139615972296448 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.25236472487449646, loss=3.785181999206543
I0207 18:43:38.926115 139615980689152 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2780861556529999, loss=3.794879674911499
I0207 18:44:13.852922 139615972296448 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.238653764128685, loss=3.780714988708496
I0207 18:44:48.781287 139615980689152 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2846306264400482, loss=3.821444511413574
I0207 18:45:23.713438 139615972296448 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.32629820704460144, loss=3.8681094646453857
I0207 18:45:58.653321 139615980689152 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.25226423144340515, loss=3.7324018478393555
I0207 18:46:33.574292 139615972296448 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.28723689913749695, loss=3.84002947807312
I0207 18:47:08.485828 139615980689152 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.284511923789978, loss=3.9078235626220703
I0207 18:47:43.418636 139615972296448 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.28205764293670654, loss=3.799592971801758
I0207 18:48:18.314269 139615980689152 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2646200954914093, loss=3.7686381340026855
I0207 18:48:53.207502 139615972296448 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.260938823223114, loss=3.8016488552093506
I0207 18:49:28.104402 139615980689152 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2614549696445465, loss=3.8176915645599365
I0207 18:50:03.025899 139615972296448 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.27408260107040405, loss=3.8084287643432617
I0207 18:50:37.929178 139615980689152 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2665331959724426, loss=3.8860061168670654
I0207 18:51:12.846874 139615972296448 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.24427631497383118, loss=3.79274845123291
I0207 18:51:47.735983 139615980689152 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.31023910641670227, loss=3.7654478549957275
I0207 18:52:22.650072 139615972296448 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2798865735530853, loss=3.851062297821045
I0207 18:52:57.557526 139615980689152 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2990473210811615, loss=3.8027560710906982
I0207 18:53:32.481770 139615972296448 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.2419423758983612, loss=3.783829689025879
I0207 18:54:07.398649 139615980689152 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3092409372329712, loss=3.8139638900756836
I0207 18:54:42.295796 139615972296448 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.26182830333709717, loss=3.8295705318450928
I0207 18:55:17.224352 139615980689152 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2383677065372467, loss=3.7659859657287598
I0207 18:55:52.156198 139615972296448 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2907862365245819, loss=3.8763492107391357
I0207 18:56:15.621507 139785736898368 spec.py:321] Evaluating on the training split.
I0207 18:56:18.627156 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 18:59:19.432307 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 18:59:22.153600 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:02:05.020399 139785736898368 spec.py:349] Evaluating on the test split.
I0207 19:02:07.733638 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:04:50.247273 139785736898368 submission_runner.py:408] Time since start: 40333.19s, 	Step: 69769, 	{'train/accuracy': 0.6853699684143066, 'train/loss': 1.6466609239578247, 'train/bleu': 34.29055829084901, 'validation/accuracy': 0.6822109818458557, 'validation/loss': 1.6531620025634766, 'validation/bleu': 29.901509568131214, 'validation/num_examples': 3000, 'test/accuracy': 0.6988670229911804, 'test/loss': 1.5575608015060425, 'test/bleu': 30.00534103427137, 'test/num_examples': 3003, 'score': 24389.033936738968, 'total_duration': 40333.188530921936, 'accumulated_submission_time': 24389.033936738968, 'accumulated_eval_time': 15941.064447641373, 'accumulated_logging_time': 0.8869020938873291}
I0207 19:04:50.273529 139615980689152 logging_writer.py:48] [69769] accumulated_eval_time=15941.064448, accumulated_logging_time=0.886902, accumulated_submission_time=24389.033937, global_step=69769, preemption_count=0, score=24389.033937, test/accuracy=0.698867, test/bleu=30.005341, test/loss=1.557561, test/num_examples=3003, total_duration=40333.188531, train/accuracy=0.685370, train/bleu=34.290558, train/loss=1.646661, validation/accuracy=0.682211, validation/bleu=29.901510, validation/loss=1.653162, validation/num_examples=3000
I0207 19:05:01.457374 139615972296448 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2724911570549011, loss=3.7671401500701904
I0207 19:05:36.344481 139615980689152 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.29946082830429077, loss=3.8206329345703125
I0207 19:06:11.252580 139615972296448 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.2564586102962494, loss=3.745558500289917
I0207 19:06:46.205495 139615980689152 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.26287925243377686, loss=3.774871587753296
I0207 19:07:21.135997 139615972296448 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.27334925532341003, loss=3.782228946685791
I0207 19:07:56.049905 139615980689152 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2912144362926483, loss=3.771623134613037
I0207 19:08:30.990838 139615972296448 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3114573061466217, loss=3.7562267780303955
I0207 19:09:05.884367 139615980689152 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2721695005893707, loss=3.8527488708496094
I0207 19:09:40.801634 139615972296448 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.27812060713768005, loss=3.7630133628845215
I0207 19:10:15.702566 139615980689152 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.26089245080947876, loss=3.774284601211548
I0207 19:10:50.620356 139615972296448 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.33772289752960205, loss=3.8018438816070557
I0207 19:11:25.550857 139615980689152 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.2542673349380493, loss=3.7885727882385254
I0207 19:12:00.451486 139615972296448 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.27678439021110535, loss=3.7768735885620117
I0207 19:12:35.427545 139615980689152 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.27345433831214905, loss=3.791705369949341
I0207 19:13:10.342068 139615972296448 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.2704779803752899, loss=3.8054423332214355
I0207 19:13:45.253885 139615980689152 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2651466727256775, loss=3.7620391845703125
I0207 19:14:20.182376 139615972296448 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3245142102241516, loss=3.8211512565612793
I0207 19:14:55.105675 139615980689152 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.28130042552948, loss=3.8234341144561768
I0207 19:15:30.013300 139615972296448 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.26516321301460266, loss=3.7914042472839355
I0207 19:16:04.921999 139615980689152 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.29009464383125305, loss=3.7652013301849365
I0207 19:16:39.882583 139615972296448 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.29480719566345215, loss=3.7979581356048584
I0207 19:17:14.788684 139615980689152 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2983751595020294, loss=3.7050483226776123
I0207 19:17:49.746063 139615972296448 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.28570011258125305, loss=3.8348636627197266
I0207 19:18:24.694628 139615980689152 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.26516786217689514, loss=3.784271717071533
I0207 19:18:50.251712 139785736898368 spec.py:321] Evaluating on the training split.
I0207 19:18:53.254533 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:21:35.100815 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 19:21:37.807444 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:23:59.703262 139785736898368 spec.py:349] Evaluating on the test split.
I0207 19:24:02.431643 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:26:15.380036 139785736898368 submission_runner.py:408] Time since start: 41618.32s, 	Step: 72175, 	{'train/accuracy': 0.6768722534179688, 'train/loss': 1.701812744140625, 'train/bleu': 33.83223992527047, 'validation/accuracy': 0.6833640933036804, 'validation/loss': 1.6468572616577148, 'validation/bleu': 29.873267425150825, 'validation/num_examples': 3000, 'test/accuracy': 0.6993783116340637, 'test/loss': 1.5530868768692017, 'test/bleu': 29.956049731472202, 'test/num_examples': 3003, 'score': 25228.926471233368, 'total_duration': 41618.32129120827, 'accumulated_submission_time': 25228.926471233368, 'accumulated_eval_time': 16386.192722558975, 'accumulated_logging_time': 0.9233071804046631}
I0207 19:26:15.405497 139615972296448 logging_writer.py:48] [72175] accumulated_eval_time=16386.192723, accumulated_logging_time=0.923307, accumulated_submission_time=25228.926471, global_step=72175, preemption_count=0, score=25228.926471, test/accuracy=0.699378, test/bleu=29.956050, test/loss=1.553087, test/num_examples=3003, total_duration=41618.321291, train/accuracy=0.676872, train/bleu=33.832240, train/loss=1.701813, validation/accuracy=0.683364, validation/bleu=29.873267, validation/loss=1.646857, validation/num_examples=3000
I0207 19:26:24.484791 139615980689152 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.27156707644462585, loss=3.8193860054016113
I0207 19:26:59.401895 139615972296448 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.29160240292549133, loss=3.754328966140747
I0207 19:27:34.307143 139615980689152 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.29143399000167847, loss=3.777815103530884
I0207 19:28:09.196794 139615972296448 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2775171101093292, loss=3.7764196395874023
I0207 19:28:44.102963 139615980689152 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.31929630041122437, loss=3.7360031604766846
I0207 19:29:19.035043 139615972296448 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.298066645860672, loss=3.788552761077881
I0207 19:29:53.951903 139615980689152 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.260711669921875, loss=3.808452844619751
I0207 19:30:28.902035 139615972296448 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2876482605934143, loss=3.805297613143921
I0207 19:31:03.833921 139615980689152 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.33271604776382446, loss=3.8138694763183594
I0207 19:31:38.765167 139615972296448 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.29827624559402466, loss=3.7658393383026123
I0207 19:32:13.657768 139615980689152 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.2574426829814911, loss=3.7816162109375
I0207 19:32:48.568816 139615972296448 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2842903733253479, loss=3.7777867317199707
I0207 19:33:23.485939 139615980689152 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2786080837249756, loss=3.8238964080810547
I0207 19:33:58.363906 139615972296448 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2647038400173187, loss=3.7437069416046143
I0207 19:34:33.291653 139615980689152 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.26075443625450134, loss=3.749356985092163
I0207 19:35:08.224889 139615972296448 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2562659978866577, loss=3.7948687076568604
I0207 19:35:43.154963 139615980689152 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.26449882984161377, loss=3.8112428188323975
I0207 19:36:18.118299 139615972296448 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.28384026885032654, loss=3.796344518661499
I0207 19:36:53.106725 139615980689152 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3068733215332031, loss=3.8163204193115234
I0207 19:37:28.057834 139615972296448 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.2648290693759918, loss=3.7785379886627197
I0207 19:38:02.975816 139615980689152 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.2710834741592407, loss=3.7864766120910645
I0207 19:38:37.884455 139615972296448 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.2662293314933777, loss=3.794802188873291
I0207 19:39:12.843695 139615980689152 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.29115039110183716, loss=3.7791895866394043
I0207 19:39:47.734992 139615972296448 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.2593267560005188, loss=3.7846412658691406
I0207 19:40:15.729521 139785736898368 spec.py:321] Evaluating on the training split.
I0207 19:40:18.729552 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:43:37.606355 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 19:43:40.303763 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:46:12.105531 139785736898368 spec.py:349] Evaluating on the test split.
I0207 19:46:14.820953 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 19:48:47.664049 139785736898368 submission_runner.py:408] Time since start: 42970.61s, 	Step: 74582, 	{'train/accuracy': 0.6750902533531189, 'train/loss': 1.7135508060455322, 'train/bleu': 33.31560347389563, 'validation/accuracy': 0.6835005283355713, 'validation/loss': 1.6415091753005981, 'validation/bleu': 30.202027501872564, 'validation/num_examples': 3000, 'test/accuracy': 0.6996455788612366, 'test/loss': 1.5487751960754395, 'test/bleu': 29.979354940034053, 'test/num_examples': 3003, 'score': 26069.164647579193, 'total_duration': 42970.60527634621, 'accumulated_submission_time': 26069.164647579193, 'accumulated_eval_time': 16898.12716984749, 'accumulated_logging_time': 0.9586780071258545}
I0207 19:48:47.695141 139615980689152 logging_writer.py:48] [74582] accumulated_eval_time=16898.127170, accumulated_logging_time=0.958678, accumulated_submission_time=26069.164648, global_step=74582, preemption_count=0, score=26069.164648, test/accuracy=0.699646, test/bleu=29.979355, test/loss=1.548775, test/num_examples=3003, total_duration=42970.605276, train/accuracy=0.675090, train/bleu=33.315603, train/loss=1.713551, validation/accuracy=0.683501, validation/bleu=30.202028, validation/loss=1.641509, validation/num_examples=3000
I0207 19:48:54.340168 139615972296448 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.27392032742500305, loss=3.726388692855835
I0207 19:49:29.262024 139615980689152 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.26443159580230713, loss=3.7916924953460693
I0207 19:50:04.198300 139615972296448 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2782386541366577, loss=3.8218343257904053
I0207 19:50:39.109889 139615980689152 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.2544647753238678, loss=3.736380100250244
I0207 19:51:14.056036 139615972296448 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.28579476475715637, loss=3.8208606243133545
I0207 19:51:48.965829 139615980689152 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.26980486512184143, loss=3.765605926513672
I0207 19:52:23.888527 139615972296448 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.270820677280426, loss=3.7583935260772705
I0207 19:52:58.807912 139615980689152 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.2797347903251648, loss=3.7789697647094727
I0207 19:53:33.713508 139615972296448 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.27618151903152466, loss=3.754765748977661
I0207 19:54:08.612421 139615980689152 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.2616107165813446, loss=3.7360780239105225
I0207 19:54:43.519254 139615972296448 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2800145149230957, loss=3.791004180908203
I0207 19:55:18.437497 139615980689152 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.298592746257782, loss=3.7386839389801025
I0207 19:55:53.354916 139615972296448 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2948661744594574, loss=3.7807679176330566
I0207 19:56:28.251274 139615980689152 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3019905090332031, loss=3.8469035625457764
I0207 19:57:03.195507 139615972296448 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2890551686286926, loss=3.82476806640625
I0207 19:57:38.090370 139615980689152 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.28591346740722656, loss=3.715470552444458
I0207 19:58:12.993613 139615972296448 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.28487205505371094, loss=3.7751998901367188
I0207 19:58:47.906289 139615980689152 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2815924286842346, loss=3.8100028038024902
I0207 19:59:22.853916 139615972296448 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.27720147371292114, loss=3.848001718521118
I0207 19:59:57.797970 139615980689152 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2833850383758545, loss=3.753779888153076
I0207 20:00:32.725415 139615972296448 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2804757058620453, loss=3.7703499794006348
I0207 20:01:07.670332 139615980689152 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.27681562304496765, loss=3.7514443397521973
I0207 20:01:42.591733 139615972296448 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.28072190284729004, loss=3.7904112339019775
I0207 20:02:17.481554 139615980689152 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.2866697907447815, loss=3.832470178604126
I0207 20:02:47.906699 139785736898368 spec.py:321] Evaluating on the training split.
I0207 20:02:50.917325 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:06:02.063352 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 20:06:04.775838 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:08:32.234215 139785736898368 spec.py:349] Evaluating on the test split.
I0207 20:08:34.942209 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:10:53.775468 139785736898368 submission_runner.py:408] Time since start: 44296.72s, 	Step: 76989, 	{'train/accuracy': 0.6826446056365967, 'train/loss': 1.6596330404281616, 'train/bleu': 34.03202156257172, 'validation/accuracy': 0.6867862939834595, 'validation/loss': 1.6362948417663574, 'validation/bleu': 30.34294338222907, 'validation/num_examples': 3000, 'test/accuracy': 0.7015281319618225, 'test/loss': 1.541305422782898, 'test/bleu': 30.308356401150277, 'test/num_examples': 3003, 'score': 26909.29024219513, 'total_duration': 44296.71672439575, 'accumulated_submission_time': 26909.29024219513, 'accumulated_eval_time': 17383.99588394165, 'accumulated_logging_time': 1.000993251800537}
I0207 20:10:53.802211 139615972296448 logging_writer.py:48] [76989] accumulated_eval_time=17383.995884, accumulated_logging_time=1.000993, accumulated_submission_time=26909.290242, global_step=76989, preemption_count=0, score=26909.290242, test/accuracy=0.701528, test/bleu=30.308356, test/loss=1.541305, test/num_examples=3003, total_duration=44296.716724, train/accuracy=0.682645, train/bleu=34.032022, train/loss=1.659633, validation/accuracy=0.686786, validation/bleu=30.342943, validation/loss=1.636295, validation/num_examples=3000
I0207 20:10:58.007011 139615980689152 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.28571420907974243, loss=3.7510643005371094
I0207 20:11:32.931447 139615972296448 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.27175289392471313, loss=3.783369541168213
I0207 20:12:07.867059 139615980689152 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.27388089895248413, loss=3.7811214923858643
I0207 20:12:42.809584 139615972296448 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2896415889263153, loss=3.7750279903411865
I0207 20:13:17.730055 139615980689152 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.28284645080566406, loss=3.7954277992248535
I0207 20:13:52.653730 139615972296448 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2622372806072235, loss=3.755424976348877
I0207 20:14:27.578510 139615980689152 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.2887808084487915, loss=3.7162015438079834
I0207 20:15:02.530311 139615972296448 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.28537270426750183, loss=3.7510266304016113
I0207 20:15:37.449925 139615980689152 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3017837405204773, loss=3.7805511951446533
I0207 20:16:12.387217 139615972296448 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3009319305419922, loss=3.8736507892608643
I0207 20:16:47.299014 139615980689152 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.27246934175491333, loss=3.812394857406616
I0207 20:17:22.219639 139615972296448 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2716886103153229, loss=3.757193088531494
I0207 20:17:57.186051 139615980689152 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3457238972187042, loss=3.7864463329315186
I0207 20:18:32.110746 139615972296448 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.29590150713920593, loss=3.799450159072876
I0207 20:19:07.025193 139615980689152 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.27921542525291443, loss=3.7846593856811523
I0207 20:19:41.965634 139615972296448 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2859758734703064, loss=3.813646078109741
I0207 20:20:16.892741 139615980689152 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.29269304871559143, loss=3.8186004161834717
I0207 20:20:51.823875 139615972296448 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.27465081214904785, loss=3.779477596282959
I0207 20:21:26.744646 139615980689152 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2836253345012665, loss=3.768853187561035
I0207 20:22:01.652295 139615972296448 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.28134405612945557, loss=3.7721059322357178
I0207 20:22:36.543276 139615980689152 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2955467402935028, loss=3.763153314590454
I0207 20:23:11.488537 139615972296448 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.27136027812957764, loss=3.770164966583252
I0207 20:23:46.475471 139615980689152 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.29680588841438293, loss=3.7404754161834717
I0207 20:24:21.436286 139615972296448 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2937767803668976, loss=3.743710517883301
I0207 20:24:54.104349 139785736898368 spec.py:321] Evaluating on the training split.
I0207 20:24:57.114460 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:27:39.263807 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 20:27:41.973658 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:30:12.420411 139785736898368 spec.py:349] Evaluating on the test split.
I0207 20:30:15.136963 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:32:39.784832 139785736898368 submission_runner.py:408] Time since start: 45602.73s, 	Step: 79395, 	{'train/accuracy': 0.6748082637786865, 'train/loss': 1.7070127725601196, 'train/bleu': 34.39932828355682, 'validation/accuracy': 0.6856703758239746, 'validation/loss': 1.6362732648849487, 'validation/bleu': 30.385912673364096, 'validation/num_examples': 3000, 'test/accuracy': 0.7026785612106323, 'test/loss': 1.537244200706482, 'test/bleu': 30.357957762145276, 'test/num_examples': 3003, 'score': 27749.502287864685, 'total_duration': 45602.726059913635, 'accumulated_submission_time': 27749.502287864685, 'accumulated_eval_time': 17849.676304340363, 'accumulated_logging_time': 1.0389277935028076}
I0207 20:32:39.815241 139615980689152 logging_writer.py:48] [79395] accumulated_eval_time=17849.676304, accumulated_logging_time=1.038928, accumulated_submission_time=27749.502288, global_step=79395, preemption_count=0, score=27749.502288, test/accuracy=0.702679, test/bleu=30.357958, test/loss=1.537244, test/num_examples=3003, total_duration=45602.726060, train/accuracy=0.674808, train/bleu=34.399328, train/loss=1.707013, validation/accuracy=0.685670, validation/bleu=30.385913, validation/loss=1.636273, validation/num_examples=3000
I0207 20:32:41.918771 139615972296448 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.29912659525871277, loss=3.742048978805542
I0207 20:33:16.852932 139615980689152 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.287468820810318, loss=3.7557713985443115
I0207 20:33:51.788288 139615972296448 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.30885666608810425, loss=3.7561612129211426
I0207 20:34:26.706352 139615980689152 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2979104220867157, loss=3.81736159324646
I0207 20:35:01.622649 139615972296448 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2775675356388092, loss=3.816756248474121
I0207 20:35:36.537712 139615980689152 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.28583353757858276, loss=3.754957914352417
I0207 20:36:11.461184 139615972296448 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.29149940609931946, loss=3.7283639907836914
I0207 20:36:46.397227 139615980689152 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2939665913581848, loss=3.837362051010132
I0207 20:37:21.334575 139615972296448 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.2748703956604004, loss=3.7517080307006836
I0207 20:37:56.278196 139615980689152 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2904365658760071, loss=3.8087847232818604
I0207 20:38:31.247983 139615972296448 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2887655794620514, loss=3.765357255935669
I0207 20:39:06.151283 139615980689152 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.2945195436477661, loss=3.7777698040008545
I0207 20:39:41.065103 139615972296448 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.29259204864501953, loss=3.761775016784668
I0207 20:40:16.003861 139615980689152 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.28429296612739563, loss=3.82572603225708
I0207 20:40:50.909414 139615972296448 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.29990142583847046, loss=3.7952582836151123
I0207 20:41:25.830060 139615980689152 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.27932944893836975, loss=3.762192964553833
I0207 20:42:00.714055 139615972296448 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.27509188652038574, loss=3.7192790508270264
I0207 20:42:35.642546 139615980689152 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.27493956685066223, loss=3.7500181198120117
I0207 20:43:10.561463 139615972296448 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.30381181836128235, loss=3.7737479209899902
I0207 20:43:45.472921 139615980689152 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.27656373381614685, loss=3.7426626682281494
I0207 20:44:20.390619 139615972296448 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.26941221952438354, loss=3.746819496154785
I0207 20:44:55.291178 139615980689152 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.31907394528388977, loss=3.6998839378356934
I0207 20:45:30.210436 139615972296448 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.2883647084236145, loss=3.7503700256347656
I0207 20:46:05.115789 139615980689152 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.27842289209365845, loss=3.7755117416381836
I0207 20:46:40.026779 139615972296448 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3082561790943146, loss=3.7775232791900635
I0207 20:46:40.033434 139785736898368 spec.py:321] Evaluating on the training split.
I0207 20:46:42.744718 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:49:57.614463 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 20:50:00.322274 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:52:31.371625 139785736898368 spec.py:349] Evaluating on the test split.
I0207 20:52:34.084866 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 20:54:59.862004 139785736898368 submission_runner.py:408] Time since start: 46942.80s, 	Step: 81801, 	{'train/accuracy': 0.7017195820808411, 'train/loss': 1.5646322965621948, 'train/bleu': 35.8828094371977, 'validation/accuracy': 0.685732364654541, 'validation/loss': 1.6307544708251953, 'validation/bleu': 30.018876279781775, 'validation/num_examples': 3000, 'test/accuracy': 0.7022950649261475, 'test/loss': 1.538099765777588, 'test/bleu': 30.294979237991356, 'test/num_examples': 3003, 'score': 28589.634435892105, 'total_duration': 46942.80326747894, 'accumulated_submission_time': 28589.634435892105, 'accumulated_eval_time': 18349.50480556488, 'accumulated_logging_time': 1.0803887844085693}
I0207 20:54:59.888499 139615980689152 logging_writer.py:48] [81801] accumulated_eval_time=18349.504806, accumulated_logging_time=1.080389, accumulated_submission_time=28589.634436, global_step=81801, preemption_count=0, score=28589.634436, test/accuracy=0.702295, test/bleu=30.294979, test/loss=1.538100, test/num_examples=3003, total_duration=46942.803267, train/accuracy=0.701720, train/bleu=35.882809, train/loss=1.564632, validation/accuracy=0.685732, validation/bleu=30.018876, validation/loss=1.630754, validation/num_examples=3000
I0207 20:55:34.807729 139615972296448 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.27636396884918213, loss=3.759873390197754
I0207 20:56:09.753830 139615980689152 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.27124685049057007, loss=3.740426778793335
I0207 20:56:44.739411 139615972296448 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3005026876926422, loss=3.8033711910247803
I0207 20:57:19.719858 139615980689152 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.27696627378463745, loss=3.7181754112243652
I0207 20:57:54.630740 139615972296448 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.27770426869392395, loss=3.823214530944824
I0207 20:58:29.550337 139615980689152 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.28366756439208984, loss=3.7770235538482666
I0207 20:59:04.472944 139615972296448 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2819195091724396, loss=3.7979438304901123
I0207 20:59:39.400832 139615980689152 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.27300605177879333, loss=3.7182986736297607
I0207 21:00:14.368908 139615972296448 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3016309142112732, loss=3.770137310028076
I0207 21:00:49.286765 139615980689152 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.2705824077129364, loss=3.7547447681427
I0207 21:01:24.190109 139615972296448 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.27814602851867676, loss=3.7751667499542236
I0207 21:01:59.102402 139615980689152 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.27410343289375305, loss=3.7528693675994873
I0207 21:02:34.002261 139615972296448 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3115982413291931, loss=3.703254461288452
I0207 21:03:08.919550 139615980689152 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.2715317904949188, loss=3.7388217449188232
I0207 21:03:43.854153 139615972296448 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.29844382405281067, loss=3.7748703956604004
I0207 21:04:18.768676 139615980689152 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.2827789783477783, loss=3.7060811519622803
I0207 21:04:53.668983 139615972296448 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3190781772136688, loss=3.7506752014160156
I0207 21:05:28.586494 139615980689152 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.29202643036842346, loss=3.7434134483337402
I0207 21:06:03.507327 139615972296448 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.29874712228775024, loss=3.7504055500030518
I0207 21:06:38.437066 139615980689152 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2862907946109772, loss=3.7919631004333496
I0207 21:07:13.357867 139615972296448 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.28124237060546875, loss=3.721503257751465
I0207 21:07:48.250537 139615980689152 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.2811432480812073, loss=3.803718090057373
I0207 21:08:23.153516 139615972296448 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.2901515066623688, loss=3.7508127689361572
I0207 21:08:58.064571 139615980689152 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.31118932366371155, loss=3.7056868076324463
I0207 21:08:59.881754 139785736898368 spec.py:321] Evaluating on the training split.
I0207 21:09:02.892908 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:12:07.435851 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 21:12:10.147250 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:14:38.898484 139785736898368 spec.py:349] Evaluating on the test split.
I0207 21:14:41.613728 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:17:05.951068 139785736898368 submission_runner.py:408] Time since start: 48268.89s, 	Step: 84207, 	{'train/accuracy': 0.683625340461731, 'train/loss': 1.652172327041626, 'train/bleu': 34.681021300800836, 'validation/accuracy': 0.6857695579528809, 'validation/loss': 1.6258001327514648, 'validation/bleu': 30.571937519327353, 'validation/num_examples': 3000, 'test/accuracy': 0.7048166990280151, 'test/loss': 1.5285508632659912, 'test/bleu': 30.50286774566312, 'test/num_examples': 3003, 'score': 29429.539747953415, 'total_duration': 48268.892318964005, 'accumulated_submission_time': 29429.539747953415, 'accumulated_eval_time': 18835.574059724808, 'accumulated_logging_time': 1.1179816722869873}
I0207 21:17:05.979516 139615972296448 logging_writer.py:48] [84207] accumulated_eval_time=18835.574060, accumulated_logging_time=1.117982, accumulated_submission_time=29429.539748, global_step=84207, preemption_count=0, score=29429.539748, test/accuracy=0.704817, test/bleu=30.502868, test/loss=1.528551, test/num_examples=3003, total_duration=48268.892319, train/accuracy=0.683625, train/bleu=34.681021, train/loss=1.652172, validation/accuracy=0.685770, validation/bleu=30.571938, validation/loss=1.625800, validation/num_examples=3000
I0207 21:17:38.803811 139615980689152 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.31084108352661133, loss=3.765871047973633
I0207 21:18:13.837440 139615972296448 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.322192519903183, loss=3.769502878189087
I0207 21:18:48.747741 139615980689152 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3010600209236145, loss=3.7659695148468018
I0207 21:19:23.665226 139615972296448 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3088117241859436, loss=3.6854770183563232
I0207 21:19:58.614879 139615980689152 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3156253397464752, loss=3.7304811477661133
I0207 21:20:33.520379 139615972296448 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.29903656244277954, loss=3.7929351329803467
I0207 21:21:08.461592 139615980689152 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2864069938659668, loss=3.744645595550537
I0207 21:21:43.363883 139615972296448 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2824842035770416, loss=3.7189576625823975
I0207 21:22:18.272013 139615980689152 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2904238998889923, loss=3.6833178997039795
I0207 21:22:53.169281 139615972296448 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.31137531995773315, loss=3.764831304550171
I0207 21:23:28.114882 139615980689152 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.29888200759887695, loss=3.785426378250122
I0207 21:24:03.051024 139615972296448 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3230125308036804, loss=3.728513479232788
I0207 21:24:37.988248 139615980689152 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.3219754695892334, loss=3.7271158695220947
I0207 21:25:12.874138 139615972296448 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.28861919045448303, loss=3.7366907596588135
I0207 21:25:47.756027 139615980689152 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.312204509973526, loss=3.749445915222168
I0207 21:26:22.687953 139615972296448 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.3058028221130371, loss=3.723423957824707
I0207 21:26:57.583085 139615980689152 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.29040294885635376, loss=3.7887134552001953
I0207 21:27:32.504189 139615972296448 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3002486824989319, loss=3.8284547328948975
I0207 21:28:07.455061 139615980689152 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.30620455741882324, loss=3.7964065074920654
I0207 21:28:42.349704 139615972296448 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.31761395931243896, loss=3.754545211791992
I0207 21:29:17.310567 139615980689152 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.2904599905014038, loss=3.744849443435669
I0207 21:29:52.322650 139615972296448 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.2848236858844757, loss=3.695220947265625
I0207 21:30:27.282454 139615980689152 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2984234094619751, loss=3.7463603019714355
I0207 21:31:02.164442 139615972296448 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.30650073289871216, loss=3.754612445831299
I0207 21:31:06.068850 139785736898368 spec.py:321] Evaluating on the training split.
I0207 21:31:09.068282 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:35:35.159064 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 21:35:37.866376 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:38:17.193799 139785736898368 spec.py:349] Evaluating on the test split.
I0207 21:38:19.914453 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:40:48.643342 139785736898368 submission_runner.py:408] Time since start: 49691.58s, 	Step: 86613, 	{'train/accuracy': 0.6862362027168274, 'train/loss': 1.647615671157837, 'train/bleu': 34.51882434228991, 'validation/accuracy': 0.6874062418937683, 'validation/loss': 1.6228716373443604, 'validation/bleu': 30.232125626879338, 'validation/num_examples': 3000, 'test/accuracy': 0.705142080783844, 'test/loss': 1.5264335870742798, 'test/bleu': 30.389439631208806, 'test/num_examples': 3003, 'score': 30269.543409347534, 'total_duration': 49691.58460474014, 'accumulated_submission_time': 30269.543409347534, 'accumulated_eval_time': 19418.148502588272, 'accumulated_logging_time': 1.1565618515014648}
I0207 21:40:48.670474 139615980689152 logging_writer.py:48] [86613] accumulated_eval_time=19418.148503, accumulated_logging_time=1.156562, accumulated_submission_time=30269.543409, global_step=86613, preemption_count=0, score=30269.543409, test/accuracy=0.705142, test/bleu=30.389440, test/loss=1.526434, test/num_examples=3003, total_duration=49691.584605, train/accuracy=0.686236, train/bleu=34.518824, train/loss=1.647616, validation/accuracy=0.687406, validation/bleu=30.232126, validation/loss=1.622872, validation/num_examples=3000
I0207 21:41:19.378956 139615972296448 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3506125211715698, loss=3.73531174659729
I0207 21:41:54.294355 139615980689152 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.292999804019928, loss=3.705716371536255
I0207 21:42:29.210936 139615972296448 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.33585113286972046, loss=3.7467048168182373
I0207 21:43:04.123334 139615980689152 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2929145395755768, loss=3.7038745880126953
I0207 21:43:39.022515 139615972296448 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.3020398020744324, loss=3.7683568000793457
I0207 21:44:13.991702 139615980689152 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2946164309978485, loss=3.7258739471435547
I0207 21:44:48.957004 139615972296448 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.2942590117454529, loss=3.7233974933624268
I0207 21:45:23.905213 139615980689152 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.30048373341560364, loss=3.7643542289733887
I0207 21:45:58.789706 139615972296448 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2898746430873871, loss=3.7335433959960938
I0207 21:46:33.714750 139615980689152 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.29398345947265625, loss=3.759685516357422
I0207 21:47:08.635270 139615972296448 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.29948920011520386, loss=3.7601120471954346
I0207 21:47:43.565074 139615980689152 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.2786043584346771, loss=3.7432656288146973
I0207 21:48:18.472948 139615972296448 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.32545140385627747, loss=3.7960939407348633
I0207 21:48:53.399787 139615980689152 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.31127187609672546, loss=3.714782476425171
I0207 21:49:28.331397 139615972296448 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3098936676979065, loss=3.7457125186920166
I0207 21:50:03.265363 139615980689152 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.32406938076019287, loss=3.7446019649505615
I0207 21:50:38.231033 139615972296448 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.30744776129722595, loss=3.6871702671051025
I0207 21:51:13.129732 139615980689152 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.31210756301879883, loss=3.7912003993988037
I0207 21:51:48.034160 139615972296448 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3264457881450653, loss=3.7159645557403564
I0207 21:52:22.952687 139615980689152 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3184858560562134, loss=3.7455010414123535
I0207 21:52:57.863991 139615972296448 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.29022476077079773, loss=3.75943660736084
I0207 21:53:32.811246 139615980689152 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.32745349407196045, loss=3.780848979949951
I0207 21:54:07.759994 139615972296448 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2820051908493042, loss=3.647082567214966
I0207 21:54:42.683083 139615980689152 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.30041688680648804, loss=3.7074248790740967
I0207 21:54:48.691686 139785736898368 spec.py:321] Evaluating on the training split.
I0207 21:54:51.690508 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 21:57:48.297833 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 21:57:51.011633 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:00:21.556398 139785736898368 spec.py:349] Evaluating on the test split.
I0207 22:00:24.269329 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:03:08.592317 139785736898368 submission_runner.py:408] Time since start: 51031.53s, 	Step: 89019, 	{'train/accuracy': 0.6950206756591797, 'train/loss': 1.5976887941360474, 'train/bleu': 35.72473038291398, 'validation/accuracy': 0.6871954202651978, 'validation/loss': 1.6184560060501099, 'validation/bleu': 30.638430798079032, 'validation/num_examples': 3000, 'test/accuracy': 0.7048166990280151, 'test/loss': 1.5245695114135742, 'test/bleu': 30.56950362536727, 'test/num_examples': 3003, 'score': 31109.477601766586, 'total_duration': 51031.5335791111, 'accumulated_submission_time': 31109.477601766586, 'accumulated_eval_time': 19918.049089193344, 'accumulated_logging_time': 1.1962296962738037}
I0207 22:03:08.620440 139615972296448 logging_writer.py:48] [89019] accumulated_eval_time=19918.049089, accumulated_logging_time=1.196230, accumulated_submission_time=31109.477602, global_step=89019, preemption_count=0, score=31109.477602, test/accuracy=0.704817, test/bleu=30.569504, test/loss=1.524570, test/num_examples=3003, total_duration=51031.533579, train/accuracy=0.695021, train/bleu=35.724730, train/loss=1.597689, validation/accuracy=0.687195, validation/bleu=30.638431, validation/loss=1.618456, validation/num_examples=3000
I0207 22:03:37.272303 139615980689152 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.3161444664001465, loss=3.762340545654297
I0207 22:04:12.196135 139615972296448 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3122178912162781, loss=3.7742769718170166
I0207 22:04:47.110522 139615980689152 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.28766509890556335, loss=3.7279646396636963
I0207 22:05:22.037073 139615972296448 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.30616575479507446, loss=3.6570534706115723
I0207 22:05:56.964011 139615980689152 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.30636394023895264, loss=3.7094998359680176
I0207 22:06:31.860096 139615972296448 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.30327722430229187, loss=3.72876238822937
I0207 22:07:06.756063 139615980689152 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.30243661999702454, loss=3.6937808990478516
I0207 22:07:41.672688 139615972296448 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.3016274869441986, loss=3.724292516708374
I0207 22:08:16.629100 139615980689152 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.30099883675575256, loss=3.6954803466796875
I0207 22:08:51.579719 139615972296448 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.2932104170322418, loss=3.7456047534942627
I0207 22:09:26.524764 139615980689152 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.30977824330329895, loss=3.7622199058532715
I0207 22:10:01.488080 139615972296448 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.30330783128738403, loss=3.7400825023651123
I0207 22:10:36.428703 139615980689152 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.29862016439437866, loss=3.722106695175171
I0207 22:11:11.375988 139615972296448 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3278065621852875, loss=3.7618308067321777
I0207 22:11:46.287008 139615980689152 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.31793829798698425, loss=3.728667974472046
I0207 22:12:21.229874 139615972296448 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.2936873733997345, loss=3.7475929260253906
I0207 22:12:56.158088 139615980689152 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.28876253962516785, loss=3.6697661876678467
I0207 22:13:31.054116 139615972296448 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.34686970710754395, loss=3.7221519947052
I0207 22:14:05.977562 139615980689152 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.3305293023586273, loss=3.7258312702178955
I0207 22:14:40.893030 139615972296448 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3127759099006653, loss=3.76471209526062
I0207 22:15:15.826745 139615980689152 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.315352201461792, loss=3.70985746383667
I0207 22:15:50.731334 139615972296448 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3137970566749573, loss=3.719289779663086
I0207 22:16:25.634650 139615980689152 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.30828672647476196, loss=3.730332374572754
I0207 22:17:00.522501 139615972296448 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3099629282951355, loss=3.6814382076263428
I0207 22:17:08.627640 139785736898368 spec.py:321] Evaluating on the training split.
I0207 22:17:11.626207 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:20:17.800170 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 22:20:20.509255 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:22:51.080870 139785736898368 spec.py:349] Evaluating on the test split.
I0207 22:22:53.800899 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:25:06.150380 139785736898368 submission_runner.py:408] Time since start: 52349.09s, 	Step: 91425, 	{'train/accuracy': 0.693752646446228, 'train/loss': 1.6055411100387573, 'train/bleu': 34.55984270647691, 'validation/accuracy': 0.688559353351593, 'validation/loss': 1.6157660484313965, 'validation/bleu': 30.754953917388068, 'validation/num_examples': 3000, 'test/accuracy': 0.7056185007095337, 'test/loss': 1.5198183059692383, 'test/bleu': 30.65265721547933, 'test/num_examples': 3003, 'score': 31949.395862579346, 'total_duration': 52349.091624975204, 'accumulated_submission_time': 31949.395862579346, 'accumulated_eval_time': 20395.571761369705, 'accumulated_logging_time': 1.2364046573638916}
I0207 22:25:06.178636 139615980689152 logging_writer.py:48] [91425] accumulated_eval_time=20395.571761, accumulated_logging_time=1.236405, accumulated_submission_time=31949.395863, global_step=91425, preemption_count=0, score=31949.395863, test/accuracy=0.705619, test/bleu=30.652657, test/loss=1.519818, test/num_examples=3003, total_duration=52349.091625, train/accuracy=0.693753, train/bleu=34.559843, train/loss=1.605541, validation/accuracy=0.688559, validation/bleu=30.754954, validation/loss=1.615766, validation/num_examples=3000
I0207 22:25:32.730058 139615972296448 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.31770607829093933, loss=3.7184906005859375
I0207 22:26:07.646243 139615980689152 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.28383082151412964, loss=3.6687850952148438
I0207 22:26:42.577784 139615972296448 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.2983188331127167, loss=3.737931251525879
I0207 22:27:17.534710 139615980689152 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3403681516647339, loss=3.6877710819244385
I0207 22:27:52.437941 139615972296448 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2914884388446808, loss=3.695013999938965
I0207 22:28:27.377574 139615980689152 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.33094295859336853, loss=3.7607288360595703
I0207 22:29:02.291887 139615972296448 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3115362226963043, loss=3.7536325454711914
I0207 22:29:37.200210 139615980689152 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3131057024002075, loss=3.7052857875823975
I0207 22:30:12.165911 139615972296448 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3189311921596527, loss=3.689018964767456
I0207 22:30:47.102591 139615980689152 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3007541000843048, loss=3.683922529220581
I0207 22:31:21.987939 139615972296448 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3318184018135071, loss=3.765266180038452
I0207 22:31:56.925082 139615980689152 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.33761006593704224, loss=3.781346321105957
I0207 22:32:31.847753 139615972296448 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.31845301389694214, loss=3.7425785064697266
I0207 22:33:06.815705 139615980689152 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.31580808758735657, loss=3.707547187805176
I0207 22:33:41.746287 139615972296448 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2918356955051422, loss=3.6843271255493164
I0207 22:34:16.665192 139615980689152 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3027549684047699, loss=3.7409698963165283
I0207 22:34:51.590768 139615972296448 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.31547823548316956, loss=3.7203071117401123
I0207 22:35:26.522813 139615980689152 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.31406649947166443, loss=3.760495662689209
I0207 22:36:01.475284 139615972296448 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3177247941493988, loss=3.7074010372161865
I0207 22:36:36.404713 139615980689152 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3103881776332855, loss=3.702415704727173
I0207 22:37:11.337947 139615972296448 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.31225472688674927, loss=3.700143814086914
I0207 22:37:46.262716 139615980689152 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.31559956073760986, loss=3.737745761871338
I0207 22:38:21.193587 139615972296448 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.3014023005962372, loss=3.7118773460388184
I0207 22:38:56.127420 139615980689152 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3058963119983673, loss=3.6445107460021973
I0207 22:39:06.328993 139785736898368 spec.py:321] Evaluating on the training split.
I0207 22:39:09.335582 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:42:29.507438 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 22:42:32.215381 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:45:17.446644 139785736898368 spec.py:349] Evaluating on the test split.
I0207 22:45:20.154548 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 22:48:05.476219 139785736898368 submission_runner.py:408] Time since start: 53728.42s, 	Step: 93831, 	{'train/accuracy': 0.7316763997077942, 'train/loss': 1.4330400228500366, 'train/bleu': 38.144238126448364, 'validation/accuracy': 0.6875426173210144, 'validation/loss': 1.6164175271987915, 'validation/bleu': 30.552952501016037, 'validation/num_examples': 3000, 'test/accuracy': 0.7061066031455994, 'test/loss': 1.518967866897583, 'test/bleu': 30.630823150616674, 'test/num_examples': 3003, 'score': 32789.459950208664, 'total_duration': 53728.417481184006, 'accumulated_submission_time': 32789.459950208664, 'accumulated_eval_time': 20934.718940973282, 'accumulated_logging_time': 1.274902582168579}
I0207 22:48:05.503811 139615972296448 logging_writer.py:48] [93831] accumulated_eval_time=20934.718941, accumulated_logging_time=1.274903, accumulated_submission_time=32789.459950, global_step=93831, preemption_count=0, score=32789.459950, test/accuracy=0.706107, test/bleu=30.630823, test/loss=1.518968, test/num_examples=3003, total_duration=53728.417481, train/accuracy=0.731676, train/bleu=38.144238, train/loss=1.433040, validation/accuracy=0.687543, validation/bleu=30.552953, validation/loss=1.616418, validation/num_examples=3000
I0207 22:48:29.958497 139615980689152 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3138993978500366, loss=3.718029260635376
I0207 22:49:04.843281 139615972296448 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3004865050315857, loss=3.7076644897460938
I0207 22:49:39.769426 139615980689152 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2959929406642914, loss=3.705603837966919
I0207 22:50:14.705734 139615972296448 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.32897838950157166, loss=3.7612152099609375
I0207 22:50:49.654172 139615980689152 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.30509328842163086, loss=3.7323734760284424
I0207 22:51:24.569191 139615972296448 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3397359251976013, loss=3.7656705379486084
I0207 22:51:59.606457 139615980689152 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.31863880157470703, loss=3.719491958618164
I0207 22:52:34.572916 139615972296448 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.32924577593803406, loss=3.706181049346924
I0207 22:53:09.527773 139615980689152 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.33257120847702026, loss=3.6893603801727295
I0207 22:53:44.481934 139615972296448 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3177695572376251, loss=3.65757417678833
I0207 22:54:19.390220 139615980689152 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.30263715982437134, loss=3.6932759284973145
I0207 22:54:54.301261 139615972296448 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3369639217853546, loss=3.745439291000366
I0207 22:55:29.197837 139615980689152 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.33536794781684875, loss=3.7066051959991455
I0207 22:56:04.112359 139615972296448 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.32445886731147766, loss=3.67348051071167
I0207 22:56:39.005575 139615980689152 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.32580098509788513, loss=3.6969151496887207
I0207 22:57:13.916475 139615972296448 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2969030737876892, loss=3.6476755142211914
I0207 22:57:48.843122 139615980689152 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3428784906864166, loss=3.7068819999694824
I0207 22:58:23.729435 139615972296448 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3203706741333008, loss=3.7290079593658447
I0207 22:58:58.616083 139615980689152 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.34544607996940613, loss=3.77286696434021
I0207 22:59:33.506000 139615972296448 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.32821226119995117, loss=3.7414305210113525
I0207 23:00:08.454680 139615980689152 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.33052921295166016, loss=3.759922504425049
I0207 23:00:43.422827 139615972296448 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.31895485520362854, loss=3.665583848953247
I0207 23:01:18.375344 139615980689152 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.33872362971305847, loss=3.751622200012207
I0207 23:01:53.358242 139615972296448 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.3295949399471283, loss=3.683884859085083
I0207 23:02:05.666089 139785736898368 spec.py:321] Evaluating on the training split.
I0207 23:02:08.669273 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:05:31.232231 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 23:05:33.937833 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:08:05.650772 139785736898368 spec.py:349] Evaluating on the test split.
I0207 23:08:08.368853 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:10:32.369645 139785736898368 submission_runner.py:408] Time since start: 55075.31s, 	Step: 96237, 	{'train/accuracy': 0.7016276121139526, 'train/loss': 1.558693528175354, 'train/bleu': 35.49628524369535, 'validation/accuracy': 0.6899976134300232, 'validation/loss': 1.6058191061019897, 'validation/bleu': 30.619367228391614, 'validation/num_examples': 3000, 'test/accuracy': 0.7069665193557739, 'test/loss': 1.5066982507705688, 'test/bleu': 30.65861045851249, 'test/num_examples': 3003, 'score': 33629.5340692997, 'total_duration': 55075.31088614464, 'accumulated_submission_time': 33629.5340692997, 'accumulated_eval_time': 21441.422430038452, 'accumulated_logging_time': 1.31276273727417}
I0207 23:10:32.398699 139615980689152 logging_writer.py:48] [96237] accumulated_eval_time=21441.422430, accumulated_logging_time=1.312763, accumulated_submission_time=33629.534069, global_step=96237, preemption_count=0, score=33629.534069, test/accuracy=0.706967, test/bleu=30.658610, test/loss=1.506698, test/num_examples=3003, total_duration=55075.310886, train/accuracy=0.701628, train/bleu=35.496285, train/loss=1.558694, validation/accuracy=0.689998, validation/bleu=30.619367, validation/loss=1.605819, validation/num_examples=3000
I0207 23:10:54.753054 139615972296448 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.34877365827560425, loss=3.7225048542022705
I0207 23:11:29.689009 139615980689152 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.33004146814346313, loss=3.7618215084075928
I0207 23:12:04.619761 139615972296448 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.32628926634788513, loss=3.704810857772827
I0207 23:12:39.520049 139615980689152 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.31837818026542664, loss=3.683682441711426
I0207 23:13:14.430292 139615972296448 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.32059407234191895, loss=3.689784049987793
I0207 23:13:49.311570 139615980689152 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.32446470856666565, loss=3.6920056343078613
I0207 23:14:24.212826 139615972296448 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3077940344810486, loss=3.697148084640503
I0207 23:14:59.142524 139615980689152 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.32783395051956177, loss=3.7221262454986572
I0207 23:15:34.064617 139615972296448 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3489435315132141, loss=3.6342194080352783
I0207 23:16:08.959490 139615980689152 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3225390315055847, loss=3.7199766635894775
I0207 23:16:43.840216 139615972296448 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3170388340950012, loss=3.7234256267547607
I0207 23:17:18.760621 139615980689152 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3285374343395233, loss=3.716038703918457
I0207 23:17:53.756537 139615972296448 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.32371360063552856, loss=3.676560640335083
I0207 23:18:28.697675 139615980689152 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3045884072780609, loss=3.6597888469696045
I0207 23:19:03.612061 139615972296448 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.30875131487846375, loss=3.6932718753814697
I0207 23:19:38.517565 139615980689152 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3205640912055969, loss=3.6443662643432617
I0207 23:20:13.452337 139615972296448 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3259109854698181, loss=3.682398796081543
I0207 23:20:48.375454 139615980689152 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.33992764353752136, loss=3.6506407260894775
I0207 23:21:23.279186 139615972296448 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.31518319249153137, loss=3.6224706172943115
I0207 23:21:58.172472 139615980689152 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.31547027826309204, loss=3.6733455657958984
I0207 23:22:33.067298 139615972296448 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3587903082370758, loss=3.7376482486724854
I0207 23:23:07.963710 139615980689152 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3093341588973999, loss=3.6359267234802246
I0207 23:23:42.876157 139615972296448 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.33041855692863464, loss=3.6832499504089355
I0207 23:24:17.801530 139615980689152 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.32404130697250366, loss=3.686511993408203
I0207 23:24:32.510407 139785736898368 spec.py:321] Evaluating on the training split.
I0207 23:24:35.505904 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:27:56.156307 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 23:27:58.867990 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:30:30.278660 139785736898368 spec.py:349] Evaluating on the test split.
I0207 23:30:32.991216 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:33:11.994620 139785736898368 submission_runner.py:408] Time since start: 56434.94s, 	Step: 98644, 	{'train/accuracy': 0.6927933096885681, 'train/loss': 1.601349115371704, 'train/bleu': 35.66692699405113, 'validation/accuracy': 0.6894148588180542, 'validation/loss': 1.6130400896072388, 'validation/bleu': 30.435671431315626, 'validation/num_examples': 3000, 'test/accuracy': 0.7068967819213867, 'test/loss': 1.5110329389572144, 'test/bleu': 30.476182677665605, 'test/num_examples': 3003, 'score': 34469.55902791023, 'total_duration': 56434.93587017059, 'accumulated_submission_time': 34469.55902791023, 'accumulated_eval_time': 21960.90658211708, 'accumulated_logging_time': 1.353524923324585}
I0207 23:33:12.022826 139615972296448 logging_writer.py:48] [98644] accumulated_eval_time=21960.906582, accumulated_logging_time=1.353525, accumulated_submission_time=34469.559028, global_step=98644, preemption_count=0, score=34469.559028, test/accuracy=0.706897, test/bleu=30.476183, test/loss=1.511033, test/num_examples=3003, total_duration=56434.935870, train/accuracy=0.692793, train/bleu=35.666927, train/loss=1.601349, validation/accuracy=0.689415, validation/bleu=30.435671, validation/loss=1.613040, validation/num_examples=3000
I0207 23:33:31.923127 139615980689152 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.3323139250278473, loss=3.6971797943115234
I0207 23:34:06.817723 139615972296448 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3327067494392395, loss=3.683636426925659
I0207 23:34:41.721738 139615980689152 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.31394538283348083, loss=3.68200945854187
I0207 23:35:16.624043 139615972296448 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.333030641078949, loss=3.724752426147461
I0207 23:35:51.537306 139615980689152 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3174848258495331, loss=3.644010066986084
I0207 23:36:26.448310 139615972296448 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.30321452021598816, loss=3.668704032897949
I0207 23:37:01.332818 139615980689152 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3233226537704468, loss=3.711378574371338
I0207 23:37:36.255259 139615972296448 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3237501382827759, loss=3.6836183071136475
I0207 23:38:11.251790 139615980689152 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.33393943309783936, loss=3.7057385444641113
I0207 23:38:46.157505 139615972296448 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.31837713718414307, loss=3.632714033126831
I0207 23:39:21.057165 139615980689152 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.33074766397476196, loss=3.6972029209136963
I0207 23:39:56.006611 139615972296448 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.3217834234237671, loss=3.680849552154541
I0207 23:40:30.921551 139615980689152 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3550094962120056, loss=3.6803972721099854
I0207 23:41:05.845491 139615972296448 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.33527615666389465, loss=3.6880650520324707
I0207 23:41:40.757144 139615980689152 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.34002283215522766, loss=3.7458369731903076
I0207 23:42:15.646127 139615972296448 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.332866907119751, loss=3.6437363624572754
I0207 23:42:50.577653 139615980689152 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.33344629406929016, loss=3.68570876121521
I0207 23:43:25.463462 139615972296448 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.35210704803466797, loss=3.7129697799682617
I0207 23:44:00.395853 139615980689152 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3382416069507599, loss=3.6934597492218018
I0207 23:44:35.349178 139615972296448 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.33955177664756775, loss=3.6749427318573
I0207 23:45:10.251310 139615980689152 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.33653518557548523, loss=3.597391128540039
I0207 23:45:45.167088 139615972296448 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.34233036637306213, loss=3.704289436340332
I0207 23:46:20.115177 139615980689152 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.32661423087120056, loss=3.723102569580078
I0207 23:46:55.092745 139615972296448 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3349302411079407, loss=3.628441572189331
I0207 23:47:12.289143 139785736898368 spec.py:321] Evaluating on the training split.
I0207 23:47:15.302439 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:49:56.796733 139785736898368 spec.py:333] Evaluating on the validation split.
I0207 23:49:59.507758 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:52:30.441596 139785736898368 spec.py:349] Evaluating on the test split.
I0207 23:52:33.163611 139785736898368 workload.py:181] Translating evaluation dataset.
I0207 23:54:45.912360 139785736898368 submission_runner.py:408] Time since start: 57728.85s, 	Step: 101051, 	{'train/accuracy': 0.7072663307189941, 'train/loss': 1.5225698947906494, 'train/bleu': 36.582741766440975, 'validation/accuracy': 0.6892164945602417, 'validation/loss': 1.6107611656188965, 'validation/bleu': 30.412669470445955, 'validation/num_examples': 3000, 'test/accuracy': 0.7070245742797852, 'test/loss': 1.507551908493042, 'test/bleu': 30.72464167955525, 'test/num_examples': 3003, 'score': 35309.74000072479, 'total_duration': 57728.85360980034, 'accumulated_submission_time': 35309.74000072479, 'accumulated_eval_time': 22414.529752969742, 'accumulated_logging_time': 1.3917343616485596}
I0207 23:54:45.942278 139615980689152 logging_writer.py:48] [101051] accumulated_eval_time=22414.529753, accumulated_logging_time=1.391734, accumulated_submission_time=35309.740001, global_step=101051, preemption_count=0, score=35309.740001, test/accuracy=0.707025, test/bleu=30.724642, test/loss=1.507552, test/num_examples=3003, total_duration=57728.853610, train/accuracy=0.707266, train/bleu=36.582742, train/loss=1.522570, validation/accuracy=0.689216, validation/bleu=30.412669, validation/loss=1.610761, validation/num_examples=3000
I0207 23:55:03.405433 139615972296448 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.34039315581321716, loss=3.6712887287139893
I0207 23:55:38.316844 139615980689152 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.3275414705276489, loss=3.6171014308929443
I0207 23:56:13.221662 139615972296448 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.31438782811164856, loss=3.664409637451172
I0207 23:56:48.137174 139615980689152 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3314044177532196, loss=3.659250020980835
I0207 23:57:23.045996 139615972296448 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.34251248836517334, loss=3.7061941623687744
I0207 23:57:57.994498 139615980689152 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.37488681077957153, loss=3.6970081329345703
I0207 23:58:32.912777 139615972296448 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.328069806098938, loss=3.662249803543091
I0207 23:59:07.834376 139615980689152 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.32141590118408203, loss=3.651817798614502
I0207 23:59:42.727823 139615972296448 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3202400207519531, loss=3.706441879272461
I0208 00:00:17.651680 139615980689152 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3149493336677551, loss=3.6659820079803467
I0208 00:00:52.574954 139615972296448 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3310721516609192, loss=3.6391546726226807
I0208 00:01:27.489754 139615980689152 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.36840155720710754, loss=3.7189948558807373
I0208 00:02:02.413918 139615972296448 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.34085986018180847, loss=3.7100400924682617
I0208 00:02:37.328643 139615980689152 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.32369622588157654, loss=3.6446776390075684
I0208 00:03:12.242995 139615972296448 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3222940266132355, loss=3.6524362564086914
I0208 00:03:47.159482 139615980689152 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.33773690462112427, loss=3.644456386566162
I0208 00:04:22.079635 139615972296448 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.338320791721344, loss=3.659322500228882
I0208 00:04:56.965914 139615980689152 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3285435736179352, loss=3.6560778617858887
I0208 00:05:31.870628 139615972296448 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.34524446725845337, loss=3.719316244125366
I0208 00:06:06.775284 139615980689152 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3465259373188019, loss=3.697486162185669
I0208 00:06:41.679120 139615972296448 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.33560776710510254, loss=3.703618049621582
I0208 00:07:16.584225 139615980689152 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.33188188076019287, loss=3.63075852394104
I0208 00:07:51.486364 139615972296448 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.34582605957984924, loss=3.7018306255340576
I0208 00:08:26.458139 139615980689152 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.3582867980003357, loss=3.724954605102539
I0208 00:08:46.100525 139785736898368 spec.py:321] Evaluating on the training split.
I0208 00:08:49.091624 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:12:19.720326 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 00:12:22.433789 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:15:02.080348 139785736898368 spec.py:349] Evaluating on the test split.
I0208 00:15:04.816402 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:17:35.324062 139785736898368 submission_runner.py:408] Time since start: 59098.27s, 	Step: 103458, 	{'train/accuracy': 0.6998023390769958, 'train/loss': 1.5615475177764893, 'train/bleu': 35.472857329654225, 'validation/accuracy': 0.6898860335350037, 'validation/loss': 1.608350396156311, 'validation/bleu': 30.398652314834955, 'validation/num_examples': 3000, 'test/accuracy': 0.707466185092926, 'test/loss': 1.5031170845031738, 'test/bleu': 30.460744589687334, 'test/num_examples': 3003, 'score': 36149.81416296959, 'total_duration': 59098.26532077789, 'accumulated_submission_time': 36149.81416296959, 'accumulated_eval_time': 22943.753257989883, 'accumulated_logging_time': 1.4315705299377441}
I0208 00:17:35.355205 139615972296448 logging_writer.py:48] [103458] accumulated_eval_time=22943.753258, accumulated_logging_time=1.431571, accumulated_submission_time=36149.814163, global_step=103458, preemption_count=0, score=36149.814163, test/accuracy=0.707466, test/bleu=30.460745, test/loss=1.503117, test/num_examples=3003, total_duration=59098.265321, train/accuracy=0.699802, train/bleu=35.472857, train/loss=1.561548, validation/accuracy=0.689886, validation/bleu=30.398652, validation/loss=1.608350, validation/num_examples=3000
I0208 00:17:50.369717 139615980689152 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3366870582103729, loss=3.607112407684326
I0208 00:18:25.295952 139615972296448 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3268996477127075, loss=3.6608119010925293
I0208 00:19:00.222176 139615980689152 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.33490675687789917, loss=3.6468749046325684
I0208 00:19:35.121493 139615972296448 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.35980159044265747, loss=3.715852737426758
I0208 00:20:10.056472 139615980689152 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.32854902744293213, loss=3.6265218257904053
I0208 00:20:44.963871 139615972296448 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3427671194076538, loss=3.670386552810669
I0208 00:21:19.882896 139615980689152 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.3343593180179596, loss=3.6639480590820312
I0208 00:21:54.776172 139615972296448 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3759618401527405, loss=3.744182586669922
I0208 00:22:29.703762 139615980689152 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3443731367588043, loss=3.6041882038116455
I0208 00:23:04.636018 139615972296448 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.3390551209449768, loss=3.6809587478637695
I0208 00:23:39.557520 139615980689152 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3362143635749817, loss=3.6726510524749756
I0208 00:24:14.481542 139615972296448 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3364793360233307, loss=3.6450648307800293
I0208 00:24:49.395792 139615980689152 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.349892258644104, loss=3.6552927494049072
I0208 00:25:24.312063 139615972296448 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.34981420636177063, loss=3.6235013008117676
I0208 00:25:59.191671 139615980689152 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.33950352668762207, loss=3.6575229167938232
I0208 00:26:34.120576 139615972296448 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.35784754157066345, loss=3.668015480041504
I0208 00:27:09.035970 139615980689152 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.35348159074783325, loss=3.70784592628479
I0208 00:27:43.968883 139615972296448 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3482627868652344, loss=3.6439297199249268
I0208 00:28:18.917658 139615980689152 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.34163251519203186, loss=3.7013065814971924
I0208 00:28:53.892188 139615972296448 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3333468735218048, loss=3.633984088897705
I0208 00:29:28.846826 139615980689152 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.374161034822464, loss=3.687528133392334
I0208 00:30:03.783993 139615972296448 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3571891486644745, loss=3.6690568923950195
I0208 00:30:38.704908 139615980689152 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3602416515350342, loss=3.6681861877441406
I0208 00:31:13.637578 139615972296448 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.36842504143714905, loss=3.695204734802246
I0208 00:31:35.365903 139785736898368 spec.py:321] Evaluating on the training split.
I0208 00:31:38.375436 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:35:01.327148 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 00:35:04.046808 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:37:39.130159 139785736898368 spec.py:349] Evaluating on the test split.
I0208 00:37:41.846144 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:40:03.693356 139785736898368 submission_runner.py:408] Time since start: 60446.63s, 	Step: 105864, 	{'train/accuracy': 0.7032337188720703, 'train/loss': 1.5497827529907227, 'train/bleu': 36.129084059359286, 'validation/accuracy': 0.6897744536399841, 'validation/loss': 1.6095569133758545, 'validation/bleu': 30.705780856663885, 'validation/num_examples': 3000, 'test/accuracy': 0.707524299621582, 'test/loss': 1.5049904584884644, 'test/bleu': 30.838508552387886, 'test/num_examples': 3003, 'score': 36989.739221572876, 'total_duration': 60446.634615659714, 'accumulated_submission_time': 36989.739221572876, 'accumulated_eval_time': 23452.08066368103, 'accumulated_logging_time': 1.4725525379180908}
I0208 00:40:03.723130 139615980689152 logging_writer.py:48] [105864] accumulated_eval_time=23452.080664, accumulated_logging_time=1.472553, accumulated_submission_time=36989.739222, global_step=105864, preemption_count=0, score=36989.739222, test/accuracy=0.707524, test/bleu=30.838509, test/loss=1.504990, test/num_examples=3003, total_duration=60446.634616, train/accuracy=0.703234, train/bleu=36.129084, train/loss=1.549783, validation/accuracy=0.689774, validation/bleu=30.705781, validation/loss=1.609557, validation/num_examples=3000
I0208 00:40:16.656785 139615972296448 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.3415873348712921, loss=3.634617567062378
I0208 00:40:51.595653 139615980689152 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.35855191946029663, loss=3.676884174346924
I0208 00:41:26.531678 139615972296448 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3467901945114136, loss=3.635896682739258
I0208 00:42:01.477625 139615980689152 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3309670090675354, loss=3.612602710723877
I0208 00:42:36.408382 139615972296448 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3611098527908325, loss=3.610530138015747
I0208 00:43:11.390343 139615980689152 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3361686170101166, loss=3.6135029792785645
I0208 00:43:46.324250 139615972296448 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.34729671478271484, loss=3.588932514190674
I0208 00:44:21.259714 139615980689152 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.36491814255714417, loss=3.6804330348968506
I0208 00:44:56.170047 139615972296448 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.36001312732696533, loss=3.6492903232574463
I0208 00:45:31.069711 139615980689152 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.36085402965545654, loss=3.660580635070801
I0208 00:46:06.009903 139615972296448 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3712309002876282, loss=3.6715002059936523
I0208 00:46:40.908776 139615980689152 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3560004234313965, loss=3.6359596252441406
I0208 00:47:15.856403 139615972296448 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.34948423504829407, loss=3.6496236324310303
I0208 00:47:50.784386 139615980689152 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.368597149848938, loss=3.698657751083374
I0208 00:48:25.815575 139615972296448 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.32284626364707947, loss=3.6034092903137207
I0208 00:49:00.788373 139615980689152 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.35185855627059937, loss=3.6560096740722656
I0208 00:49:35.708111 139615972296448 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.33962470293045044, loss=3.6696128845214844
I0208 00:50:10.664767 139615980689152 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3379404544830322, loss=3.6420626640319824
I0208 00:50:45.599259 139615972296448 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.33571699261665344, loss=3.6155381202697754
I0208 00:51:20.547636 139615980689152 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.37574341893196106, loss=3.6830742359161377
I0208 00:51:55.494539 139615972296448 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.36188429594039917, loss=3.6946842670440674
I0208 00:52:30.440365 139615980689152 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.36026322841644287, loss=3.6767663955688477
I0208 00:53:05.354547 139615972296448 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3581123352050781, loss=3.6853654384613037
I0208 00:53:40.264899 139615980689152 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3629773259162903, loss=3.6004443168640137
I0208 00:54:03.768075 139785736898368 spec.py:321] Evaluating on the training split.
I0208 00:54:06.768600 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:57:04.275398 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 00:57:06.981088 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 00:59:35.708165 139785736898368 spec.py:349] Evaluating on the test split.
I0208 00:59:38.420174 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:02:13.092080 139785736898368 submission_runner.py:408] Time since start: 61776.03s, 	Step: 108269, 	{'train/accuracy': 0.7132049202919006, 'train/loss': 1.4947891235351562, 'train/bleu': 36.41010826418029, 'validation/accuracy': 0.6916218996047974, 'validation/loss': 1.6021170616149902, 'validation/bleu': 30.63931633701646, 'validation/num_examples': 3000, 'test/accuracy': 0.7091279029846191, 'test/loss': 1.499886393547058, 'test/bleu': 30.73570104529433, 'test/num_examples': 3003, 'score': 37829.69793653488, 'total_duration': 61776.03334188461, 'accumulated_submission_time': 37829.69793653488, 'accumulated_eval_time': 23941.40462422371, 'accumulated_logging_time': 1.5124402046203613}
I0208 01:02:13.121880 139615972296448 logging_writer.py:48] [108269] accumulated_eval_time=23941.404624, accumulated_logging_time=1.512440, accumulated_submission_time=37829.697937, global_step=108269, preemption_count=0, score=37829.697937, test/accuracy=0.709128, test/bleu=30.735701, test/loss=1.499886, test/num_examples=3003, total_duration=61776.033342, train/accuracy=0.713205, train/bleu=36.410108, train/loss=1.494789, validation/accuracy=0.691622, validation/bleu=30.639316, validation/loss=1.602117, validation/num_examples=3000
I0208 01:02:24.276478 139615980689152 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.34279319643974304, loss=3.609860897064209
I0208 01:02:59.208705 139615972296448 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3544690012931824, loss=3.6439967155456543
I0208 01:03:34.191427 139615980689152 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.36179327964782715, loss=3.6580779552459717
I0208 01:04:09.241601 139615972296448 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3694818317890167, loss=3.675198554992676
I0208 01:04:44.193640 139615980689152 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3619021475315094, loss=3.710162878036499
I0208 01:05:19.142899 139615972296448 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3642655611038208, loss=3.647766351699829
I0208 01:05:54.042123 139615980689152 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.3610093593597412, loss=3.6847071647644043
I0208 01:06:28.970412 139615972296448 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.35931074619293213, loss=3.631749153137207
I0208 01:07:03.902335 139615980689152 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.35788753628730774, loss=3.6010396480560303
I0208 01:07:38.824850 139615972296448 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3534276485443115, loss=3.6446971893310547
I0208 01:08:13.755991 139615980689152 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3463537395000458, loss=3.6399056911468506
I0208 01:08:48.718893 139615972296448 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.367360919713974, loss=3.6896228790283203
I0208 01:09:23.657207 139615980689152 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.35012346506118774, loss=3.6783056259155273
I0208 01:09:58.627881 139615972296448 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.38003310561180115, loss=3.627572774887085
I0208 01:10:33.539760 139615980689152 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.33018919825553894, loss=3.5901148319244385
I0208 01:11:08.469075 139615972296448 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.36269205808639526, loss=3.665846586227417
I0208 01:11:43.377968 139615980689152 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3543514907360077, loss=3.703209400177002
I0208 01:12:18.283969 139615972296448 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.37096357345581055, loss=3.688882827758789
I0208 01:12:53.201293 139615980689152 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.34112828969955444, loss=3.630403518676758
I0208 01:13:28.127526 139615972296448 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3588387966156006, loss=3.6723568439483643
I0208 01:14:03.044472 139615980689152 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.38792961835861206, loss=3.6309447288513184
I0208 01:14:37.980134 139615972296448 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.37309402227401733, loss=3.650031566619873
I0208 01:15:12.932353 139615980689152 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3710726499557495, loss=3.6722521781921387
I0208 01:15:47.956787 139615972296448 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.36994966864585876, loss=3.655527353286743
I0208 01:16:13.200950 139785736898368 spec.py:321] Evaluating on the training split.
I0208 01:16:16.221017 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:19:02.796297 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 01:19:05.507365 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:21:35.452755 139785736898368 spec.py:349] Evaluating on the test split.
I0208 01:21:38.172895 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:23:54.289660 139785736898368 submission_runner.py:408] Time since start: 63077.23s, 	Step: 110674, 	{'train/accuracy': 0.7082228064537048, 'train/loss': 1.514228343963623, 'train/bleu': 36.62626379524225, 'validation/accuracy': 0.6904811859130859, 'validation/loss': 1.604583740234375, 'validation/bleu': 30.574834332474047, 'validation/num_examples': 3000, 'test/accuracy': 0.7097786664962769, 'test/loss': 1.4970381259918213, 'test/bleu': 30.640785565828086, 'test/num_examples': 3003, 'score': 38669.68802213669, 'total_duration': 63077.23090171814, 'accumulated_submission_time': 38669.68802213669, 'accumulated_eval_time': 24402.493275880814, 'accumulated_logging_time': 1.5521199703216553}
I0208 01:23:54.321136 139615980689152 logging_writer.py:48] [110674] accumulated_eval_time=24402.493276, accumulated_logging_time=1.552120, accumulated_submission_time=38669.688022, global_step=110674, preemption_count=0, score=38669.688022, test/accuracy=0.709779, test/bleu=30.640786, test/loss=1.497038, test/num_examples=3003, total_duration=63077.230902, train/accuracy=0.708223, train/bleu=36.626264, train/loss=1.514228, validation/accuracy=0.690481, validation/bleu=30.574834, validation/loss=1.604584, validation/num_examples=3000
I0208 01:24:03.770315 139615972296448 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.3697109520435333, loss=3.66778564453125
I0208 01:24:38.688312 139615980689152 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3532409369945526, loss=3.637477159500122
I0208 01:25:13.617590 139615972296448 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3484386205673218, loss=3.631721019744873
I0208 01:25:48.605321 139615980689152 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.39528030157089233, loss=3.650618553161621
I0208 01:26:23.550905 139615972296448 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.35939329862594604, loss=3.6236000061035156
I0208 01:26:58.469859 139615980689152 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3499109745025635, loss=3.6252682209014893
I0208 01:27:33.402570 139615972296448 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.37810376286506653, loss=3.6005046367645264
I0208 01:28:08.296959 139615980689152 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.37190359830856323, loss=3.661468505859375
I0208 01:28:43.213217 139615972296448 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.36408281326293945, loss=3.6587624549865723
I0208 01:29:18.150520 139615980689152 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3635675609111786, loss=3.6669163703918457
I0208 01:29:53.102088 139615972296448 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3861183524131775, loss=3.5635197162628174
I0208 01:30:27.986842 139615980689152 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.37825319170951843, loss=3.6808278560638428
I0208 01:31:02.892894 139615972296448 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3800952136516571, loss=3.688492774963379
I0208 01:31:37.795837 139615980689152 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.35792362689971924, loss=3.6505391597747803
I0208 01:32:12.739448 139615972296448 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3579671382904053, loss=3.6374940872192383
I0208 01:32:47.737102 139615980689152 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.380208283662796, loss=3.6944727897644043
I0208 01:33:22.671745 139615972296448 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.3519487679004669, loss=3.6487836837768555
I0208 01:33:57.587291 139615980689152 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3635491728782654, loss=3.5785021781921387
I0208 01:34:32.533396 139615972296448 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3692140579223633, loss=3.6843066215515137
I0208 01:35:07.462571 139615980689152 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3621949553489685, loss=3.587291717529297
I0208 01:35:42.380960 139615972296448 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3853197991847992, loss=3.6425399780273438
I0208 01:36:17.279139 139615980689152 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3478999435901642, loss=3.6420464515686035
I0208 01:36:52.188577 139615972296448 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.379167765378952, loss=3.629760265350342
I0208 01:37:27.078878 139615980689152 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.36824899911880493, loss=3.6610219478607178
I0208 01:37:54.367872 139785736898368 spec.py:321] Evaluating on the training split.
I0208 01:37:57.367254 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:40:52.246793 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 01:40:54.974262 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:43:21.864270 139785736898368 spec.py:349] Evaluating on the test split.
I0208 01:43:24.591540 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 01:45:39.920433 139785736898368 submission_runner.py:408] Time since start: 64382.86s, 	Step: 113080, 	{'train/accuracy': 0.7224063277244568, 'train/loss': 1.443524718284607, 'train/bleu': 37.28813238914175, 'validation/accuracy': 0.6914855241775513, 'validation/loss': 1.6032272577285767, 'validation/bleu': 30.60543726425647, 'validation/num_examples': 3000, 'test/accuracy': 0.7103829383850098, 'test/loss': 1.4962009191513062, 'test/bleu': 30.95519761628077, 'test/num_examples': 3003, 'score': 39509.64947485924, 'total_duration': 64382.861696243286, 'accumulated_submission_time': 39509.64947485924, 'accumulated_eval_time': 24868.045795202255, 'accumulated_logging_time': 1.5935924053192139}
I0208 01:45:39.950955 139615972296448 logging_writer.py:48] [113080] accumulated_eval_time=24868.045795, accumulated_logging_time=1.593592, accumulated_submission_time=39509.649475, global_step=113080, preemption_count=0, score=39509.649475, test/accuracy=0.710383, test/bleu=30.955198, test/loss=1.496201, test/num_examples=3003, total_duration=64382.861696, train/accuracy=0.722406, train/bleu=37.288132, train/loss=1.443525, validation/accuracy=0.691486, validation/bleu=30.605437, validation/loss=1.603227, validation/num_examples=3000
I0208 01:45:47.291741 139615980689152 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3839629590511322, loss=3.652946710586548
I0208 01:46:22.240000 139615972296448 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.365546852350235, loss=3.6214334964752197
I0208 01:46:57.140671 139615980689152 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.376257061958313, loss=3.651603937149048
I0208 01:47:32.039218 139615972296448 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3825322985649109, loss=3.7240138053894043
I0208 01:48:06.931121 139615980689152 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.37604641914367676, loss=3.584516763687134
I0208 01:48:41.831145 139615972296448 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.3685515820980072, loss=3.6265530586242676
I0208 01:49:16.753707 139615980689152 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.37642332911491394, loss=3.703404664993286
I0208 01:49:51.669495 139615972296448 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.35416218638420105, loss=3.603726863861084
I0208 01:50:26.617840 139615980689152 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.3746153712272644, loss=3.707555055618286
I0208 01:51:01.534490 139615972296448 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.36464723944664, loss=3.5717413425445557
I0208 01:51:36.447506 139615980689152 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.36404186487197876, loss=3.6151020526885986
I0208 01:52:11.355659 139615972296448 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.36692243814468384, loss=3.651310682296753
I0208 01:52:46.295640 139615980689152 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.37055736780166626, loss=3.6702992916107178
I0208 01:53:21.282192 139615972296448 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.37617775797843933, loss=3.6133315563201904
I0208 01:53:56.190235 139615980689152 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.38127997517585754, loss=3.6661312580108643
I0208 01:54:31.089304 139615972296448 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3573229908943176, loss=3.6151437759399414
I0208 01:55:06.007226 139615980689152 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3745093047618866, loss=3.6354660987854004
I0208 01:55:40.918693 139615972296448 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3800196945667267, loss=3.6714634895324707
I0208 01:56:15.838816 139615980689152 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.3822174072265625, loss=3.669999122619629
I0208 01:56:50.760858 139615972296448 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.36412912607192993, loss=3.62770414352417
I0208 01:57:25.698967 139615980689152 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.3616785407066345, loss=3.572204828262329
I0208 01:58:00.613186 139615972296448 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3843664228916168, loss=3.596078395843506
I0208 01:58:35.517628 139615980689152 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.3697621524333954, loss=3.666306495666504
I0208 01:59:10.437047 139615972296448 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3648770749568939, loss=3.56669545173645
I0208 01:59:40.187288 139785736898368 spec.py:321] Evaluating on the training split.
I0208 01:59:43.193139 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:02:41.794060 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 02:02:44.505004 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:05:13.056583 139785736898368 spec.py:349] Evaluating on the test split.
I0208 02:05:15.775211 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:07:36.035582 139785736898368 submission_runner.py:408] Time since start: 65698.98s, 	Step: 115487, 	{'train/accuracy': 0.713723361492157, 'train/loss': 1.487293004989624, 'train/bleu': 36.435484973088045, 'validation/accuracy': 0.6912871599197388, 'validation/loss': 1.6010518074035645, 'validation/bleu': 30.65375561094501, 'validation/num_examples': 3000, 'test/accuracy': 0.7099064588546753, 'test/loss': 1.4967427253723145, 'test/bleu': 30.68944762531302, 'test/num_examples': 3003, 'score': 40349.80061197281, 'total_duration': 65698.97678422928, 'accumulated_submission_time': 40349.80061197281, 'accumulated_eval_time': 25343.893981456757, 'accumulated_logging_time': 1.635310173034668}
I0208 02:07:36.072806 139615980689152 logging_writer.py:48] [115487] accumulated_eval_time=25343.893981, accumulated_logging_time=1.635310, accumulated_submission_time=40349.800612, global_step=115487, preemption_count=0, score=40349.800612, test/accuracy=0.709906, test/bleu=30.689448, test/loss=1.496743, test/num_examples=3003, total_duration=65698.976784, train/accuracy=0.713723, train/bleu=36.435485, train/loss=1.487293, validation/accuracy=0.691287, validation/bleu=30.653756, validation/loss=1.601052, validation/num_examples=3000
I0208 02:07:40.985199 139615972296448 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.3908820152282715, loss=3.6714589595794678
I0208 02:08:15.976070 139615980689152 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3881909251213074, loss=3.6155221462249756
I0208 02:08:50.901931 139615972296448 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3754522502422333, loss=3.660940170288086
I0208 02:09:25.808913 139615980689152 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3886975646018982, loss=3.6642978191375732
I0208 02:10:00.727493 139615972296448 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3782057762145996, loss=3.642577648162842
I0208 02:10:35.662930 139615980689152 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3820084035396576, loss=3.6147844791412354
I0208 02:11:10.566015 139615972296448 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.37316566705703735, loss=3.6371593475341797
I0208 02:11:45.488250 139615980689152 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.36008381843566895, loss=3.6023190021514893
I0208 02:12:20.391868 139615972296448 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.38108426332473755, loss=3.6544411182403564
I0208 02:12:55.312118 139615980689152 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.4138863980770111, loss=3.6461589336395264
I0208 02:13:30.225586 139615972296448 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.37196359038352966, loss=3.640284299850464
I0208 02:14:05.130306 139615980689152 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.39182350039482117, loss=3.6158206462860107
I0208 02:14:40.025120 139615972296448 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.37510958313941956, loss=3.6180179119110107
I0208 02:15:14.924275 139615980689152 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.40280306339263916, loss=3.6480515003204346
I0208 02:15:49.837632 139615972296448 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.3875313103199005, loss=3.6157758235931396
I0208 02:16:24.806117 139615980689152 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3817255198955536, loss=3.605210781097412
I0208 02:16:59.773961 139615972296448 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.38169682025909424, loss=3.6277925968170166
I0208 02:17:34.669619 139615980689152 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.370784193277359, loss=3.606142520904541
I0208 02:18:09.570098 139615972296448 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.3956229090690613, loss=3.5614778995513916
I0208 02:18:44.476689 139615980689152 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3693765103816986, loss=3.6015279293060303
I0208 02:19:19.375726 139615972296448 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3854791522026062, loss=3.6849193572998047
I0208 02:19:54.286406 139615980689152 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.36585935950279236, loss=3.5787477493286133
I0208 02:20:29.221229 139615972296448 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3749696910381317, loss=3.6299924850463867
I0208 02:21:04.134666 139615980689152 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3664737641811371, loss=3.5792644023895264
I0208 02:21:36.291081 139785736898368 spec.py:321] Evaluating on the training split.
I0208 02:21:39.288298 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:24:25.389226 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 02:24:28.099296 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:26:59.624298 139785736898368 spec.py:349] Evaluating on the test split.
I0208 02:27:02.355626 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:29:14.794956 139785736898368 submission_runner.py:408] Time since start: 66997.74s, 	Step: 117894, 	{'train/accuracy': 0.7144909501075745, 'train/loss': 1.4906255006790161, 'train/bleu': 37.108529161669374, 'validation/accuracy': 0.6910887360572815, 'validation/loss': 1.6030627489089966, 'validation/bleu': 30.71050729467754, 'validation/num_examples': 3000, 'test/accuracy': 0.7090465426445007, 'test/loss': 1.4972790479660034, 'test/bleu': 30.936780229896527, 'test/num_examples': 3003, 'score': 41189.930918216705, 'total_duration': 66997.73618650436, 'accumulated_submission_time': 41189.930918216705, 'accumulated_eval_time': 25802.397783994675, 'accumulated_logging_time': 1.6849846839904785}
I0208 02:29:14.832284 139615972296448 logging_writer.py:48] [117894] accumulated_eval_time=25802.397784, accumulated_logging_time=1.684985, accumulated_submission_time=41189.930918, global_step=117894, preemption_count=0, score=41189.930918, test/accuracy=0.709047, test/bleu=30.936780, test/loss=1.497279, test/num_examples=3003, total_duration=66997.736187, train/accuracy=0.714491, train/bleu=37.108529, train/loss=1.490626, validation/accuracy=0.691089, validation/bleu=30.710507, validation/loss=1.603063, validation/num_examples=3000
I0208 02:29:17.295578 139615980689152 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.3763900697231293, loss=3.6143546104431152
I0208 02:29:52.221099 139615972296448 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.39059507846832275, loss=3.6272852420806885
I0208 02:30:27.129299 139615980689152 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.37589770555496216, loss=3.6506476402282715
I0208 02:31:02.032160 139615972296448 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.3656121492385864, loss=3.603306531906128
I0208 02:31:36.956253 139615980689152 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3820158839225769, loss=3.6192636489868164
I0208 02:32:11.901639 139615972296448 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3805443048477173, loss=3.618354082107544
I0208 02:32:46.877980 139615980689152 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3879925012588501, loss=3.6207094192504883
I0208 02:33:21.800326 139615972296448 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3927347660064697, loss=3.6280274391174316
I0208 02:33:56.724690 139615980689152 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.4037606418132782, loss=3.6851818561553955
I0208 02:34:31.644116 139615972296448 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.3682829439640045, loss=3.650270462036133
I0208 02:35:06.558794 139615980689152 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.4098219573497772, loss=3.6147119998931885
I0208 02:35:41.483078 139615972296448 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3919796645641327, loss=3.6418228149414062
I0208 02:36:16.421792 139615980689152 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.37461066246032715, loss=3.5980384349823
I0208 02:36:51.355685 139615972296448 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.38885512948036194, loss=3.626770257949829
I0208 02:37:26.285911 139615980689152 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.38169336318969727, loss=3.6173064708709717
I0208 02:38:01.248396 139615972296448 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3978971540927887, loss=3.652817726135254
I0208 02:38:36.185169 139615980689152 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.3786478638648987, loss=3.625425338745117
I0208 02:39:11.121096 139615972296448 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.363910436630249, loss=3.568073272705078
I0208 02:39:46.070020 139615980689152 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.40485426783561707, loss=3.682830333709717
I0208 02:40:21.003483 139615972296448 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.3951282501220703, loss=3.6474039554595947
I0208 02:40:55.933851 139615980689152 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.37426331639289856, loss=3.603182554244995
I0208 02:41:30.838310 139615972296448 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.41340821981430054, loss=3.577686309814453
I0208 02:42:05.742927 139615980689152 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3816712200641632, loss=3.5968997478485107
I0208 02:42:40.742179 139615972296448 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.37235352396965027, loss=3.6249215602874756
I0208 02:43:15.060435 139785736898368 spec.py:321] Evaluating on the training split.
I0208 02:43:18.080687 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:46:08.290518 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 02:46:11.014832 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:48:44.126085 139785736898368 spec.py:349] Evaluating on the test split.
I0208 02:48:46.847855 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 02:51:04.373765 139785736898368 submission_runner.py:408] Time since start: 68307.32s, 	Step: 120300, 	{'train/accuracy': 0.7231228351593018, 'train/loss': 1.442205548286438, 'train/bleu': 37.805040018899184, 'validation/accuracy': 0.6926014423370361, 'validation/loss': 1.6032183170318604, 'validation/bleu': 30.683219104917654, 'validation/num_examples': 3000, 'test/accuracy': 0.7098715901374817, 'test/loss': 1.4950560331344604, 'test/bleu': 30.98069747621216, 'test/num_examples': 3003, 'score': 42030.07278776169, 'total_duration': 68307.31502318382, 'accumulated_submission_time': 42030.07278776169, 'accumulated_eval_time': 26271.71107816696, 'accumulated_logging_time': 1.7338509559631348}
I0208 02:51:04.407778 139615980689152 logging_writer.py:48] [120300] accumulated_eval_time=26271.711078, accumulated_logging_time=1.733851, accumulated_submission_time=42030.072788, global_step=120300, preemption_count=0, score=42030.072788, test/accuracy=0.709872, test/bleu=30.980697, test/loss=1.495056, test/num_examples=3003, total_duration=68307.315023, train/accuracy=0.723123, train/bleu=37.805040, train/loss=1.442206, validation/accuracy=0.692601, validation/bleu=30.683219, validation/loss=1.603218, validation/num_examples=3000
I0208 02:51:04.778540 139615972296448 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3742760419845581, loss=3.5984296798706055
I0208 02:51:39.644888 139615980689152 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.3866680860519409, loss=3.6268813610076904
I0208 02:52:14.510038 139615972296448 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.36842256784439087, loss=3.5664479732513428
I0208 02:52:49.408324 139615980689152 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3779975175857544, loss=3.631664991378784
I0208 02:53:24.267175 139615972296448 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.3957432508468628, loss=3.6070101261138916
I0208 02:53:59.182646 139615980689152 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.37224820256233215, loss=3.5912649631500244
I0208 02:54:34.070765 139615972296448 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3809893727302551, loss=3.599933624267578
I0208 02:55:09.014082 139615980689152 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.38493582606315613, loss=3.599933385848999
I0208 02:55:43.874875 139615972296448 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3836095929145813, loss=3.6457486152648926
I0208 02:56:18.765955 139615980689152 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3991328775882721, loss=3.590275287628174
I0208 02:56:53.604108 139615972296448 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.3845520615577698, loss=3.6505849361419678
I0208 02:57:28.457505 139615980689152 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3578016459941864, loss=3.572573184967041
I0208 02:58:03.316373 139615972296448 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3807809054851532, loss=3.595094919204712
I0208 02:58:38.212027 139615980689152 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.3625324070453644, loss=3.590402364730835
I0208 02:59:13.059800 139615972296448 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.38644668459892273, loss=3.6171646118164062
I0208 02:59:47.925081 139615980689152 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.38959088921546936, loss=3.6061010360717773
I0208 03:00:22.801131 139615972296448 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.37701216340065, loss=3.626432180404663
I0208 03:00:57.659725 139615980689152 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.37580054998397827, loss=3.632290840148926
I0208 03:01:32.518697 139615972296448 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.3738078773021698, loss=3.5849955081939697
I0208 03:02:07.370920 139615980689152 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.38059958815574646, loss=3.632014513015747
I0208 03:02:42.214667 139615972296448 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.4141184687614441, loss=3.63362717628479
I0208 03:03:17.091898 139615980689152 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.35891133546829224, loss=3.546619176864624
I0208 03:03:51.949305 139615972296448 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.38404375314712524, loss=3.6142680644989014
I0208 03:04:26.796316 139615980689152 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.3698635697364807, loss=3.576422929763794
I0208 03:05:01.682695 139615972296448 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.38906311988830566, loss=3.6723825931549072
I0208 03:05:04.549280 139785736898368 spec.py:321] Evaluating on the training split.
I0208 03:05:07.553502 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:08:05.016108 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 03:08:07.724523 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:10:32.306873 139785736898368 spec.py:349] Evaluating on the test split.
I0208 03:10:35.017047 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:12:51.220919 139785736898368 submission_runner.py:408] Time since start: 69614.16s, 	Step: 122710, 	{'train/accuracy': 0.7189242243766785, 'train/loss': 1.4655243158340454, 'train/bleu': 37.58180576067513, 'validation/accuracy': 0.6922046542167664, 'validation/loss': 1.6004964113235474, 'validation/bleu': 30.80034796220132, 'validation/num_examples': 3000, 'test/accuracy': 0.7106037139892578, 'test/loss': 1.4928041696548462, 'test/bleu': 30.8694776006975, 'test/num_examples': 3003, 'score': 42870.128358602524, 'total_duration': 69614.16217589378, 'accumulated_submission_time': 42870.128358602524, 'accumulated_eval_time': 26738.382657289505, 'accumulated_logging_time': 1.7781829833984375}
I0208 03:12:51.253021 139615980689152 logging_writer.py:48] [122710] accumulated_eval_time=26738.382657, accumulated_logging_time=1.778183, accumulated_submission_time=42870.128359, global_step=122710, preemption_count=0, score=42870.128359, test/accuracy=0.710604, test/bleu=30.869478, test/loss=1.492804, test/num_examples=3003, total_duration=69614.162176, train/accuracy=0.718924, train/bleu=37.581806, train/loss=1.465524, validation/accuracy=0.692205, validation/bleu=30.800348, validation/loss=1.600496, validation/num_examples=3000
I0208 03:13:23.030909 139615972296448 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.39033353328704834, loss=3.605027675628662
I0208 03:13:57.936560 139615980689152 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.39804351329803467, loss=3.5972163677215576
I0208 03:14:32.832733 139615972296448 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.38881510496139526, loss=3.6180763244628906
I0208 03:15:07.756450 139615980689152 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3838006854057312, loss=3.6056504249572754
I0208 03:15:42.668396 139615972296448 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3967258036136627, loss=3.591264247894287
I0208 03:16:17.567385 139615980689152 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.39255812764167786, loss=3.623241901397705
I0208 03:16:52.477105 139615972296448 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3704969584941864, loss=3.6319663524627686
I0208 03:17:27.410322 139615980689152 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.4083252549171448, loss=3.6343941688537598
I0208 03:18:02.326987 139615972296448 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.37055641412734985, loss=3.599128007888794
I0208 03:18:37.269617 139615980689152 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.37194570899009705, loss=3.638270616531372
I0208 03:19:12.181329 139615972296448 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3790285289287567, loss=3.588653326034546
I0208 03:19:47.128777 139615980689152 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3751288652420044, loss=3.6135411262512207
I0208 03:20:22.052736 139615972296448 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.39346277713775635, loss=3.642280340194702
I0208 03:20:56.997199 139615980689152 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.3768010139465332, loss=3.592026710510254
I0208 03:21:31.907914 139615972296448 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.40255993604660034, loss=3.5998265743255615
I0208 03:22:06.822058 139615980689152 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.39213618636131287, loss=3.6614911556243896
I0208 03:22:41.742619 139615972296448 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.38954123854637146, loss=3.600144386291504
I0208 03:23:16.670665 139615980689152 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.4035890996456146, loss=3.666602373123169
I0208 03:23:51.591989 139615972296448 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3825826346874237, loss=3.6092164516448975
I0208 03:24:26.515789 139615980689152 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.3908337354660034, loss=3.6220765113830566
I0208 03:25:01.456673 139615972296448 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.38459837436676025, loss=3.598306894302368
I0208 03:25:36.350398 139615980689152 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.37175965309143066, loss=3.6253721714019775
I0208 03:26:11.304607 139615972296448 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.37472227215766907, loss=3.6072070598602295
I0208 03:26:46.290111 139615980689152 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.40120118856430054, loss=3.64516544342041
I0208 03:26:51.259404 139785736898368 spec.py:321] Evaluating on the training split.
I0208 03:26:54.271816 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:29:50.519582 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 03:29:53.227846 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:32:21.862107 139785736898368 spec.py:349] Evaluating on the test split.
I0208 03:32:24.571835 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:34:35.661023 139785736898368 submission_runner.py:408] Time since start: 70918.60s, 	Step: 125116, 	{'train/accuracy': 0.7213461399078369, 'train/loss': 1.4562888145446777, 'train/bleu': 37.47862548500293, 'validation/accuracy': 0.6918947100639343, 'validation/loss': 1.6007949113845825, 'validation/bleu': 30.825696629345693, 'validation/num_examples': 3000, 'test/accuracy': 0.7102667093276978, 'test/loss': 1.4937689304351807, 'test/bleu': 30.91072129641423, 'test/num_examples': 3003, 'score': 43710.050102472305, 'total_duration': 70918.60228586197, 'accumulated_submission_time': 43710.050102472305, 'accumulated_eval_time': 27202.784227132797, 'accumulated_logging_time': 1.8205416202545166}
I0208 03:34:35.693297 139615972296448 logging_writer.py:48] [125116] accumulated_eval_time=27202.784227, accumulated_logging_time=1.820542, accumulated_submission_time=43710.050102, global_step=125116, preemption_count=0, score=43710.050102, test/accuracy=0.710267, test/bleu=30.910721, test/loss=1.493769, test/num_examples=3003, total_duration=70918.602286, train/accuracy=0.721346, train/bleu=37.478625, train/loss=1.456289, validation/accuracy=0.691895, validation/bleu=30.825697, validation/loss=1.600795, validation/num_examples=3000
I0208 03:35:05.354514 139615980689152 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.39092588424682617, loss=3.5590710639953613
I0208 03:35:40.260946 139615972296448 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.40230679512023926, loss=3.7009775638580322
I0208 03:36:15.156203 139615980689152 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.40092071890830994, loss=3.6509850025177
I0208 03:36:50.057035 139615972296448 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.38229429721832275, loss=3.626512289047241
I0208 03:37:25.009368 139615980689152 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3801288306713104, loss=3.5900027751922607
I0208 03:37:59.966232 139615972296448 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.3955160081386566, loss=3.6065573692321777
I0208 03:38:34.885962 139615980689152 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.37381651997566223, loss=3.6003177165985107
I0208 03:39:09.803072 139615972296448 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.3887266516685486, loss=3.6255788803100586
I0208 03:39:44.709211 139615980689152 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.40096011757850647, loss=3.632128953933716
I0208 03:40:19.625484 139615972296448 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.37575653195381165, loss=3.616673231124878
I0208 03:40:54.518787 139615980689152 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.3914085328578949, loss=3.585907459259033
I0208 03:41:29.426665 139615972296448 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.37479692697525024, loss=3.5848474502563477
I0208 03:42:04.339588 139615980689152 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.37871652841567993, loss=3.5191118717193604
I0208 03:42:39.226242 139615972296448 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.3848613202571869, loss=3.614231824874878
I0208 03:43:14.152317 139615980689152 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.37839797139167786, loss=3.6047446727752686
I0208 03:43:49.140031 139615972296448 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.39977696537971497, loss=3.639756202697754
I0208 03:44:24.176487 139615980689152 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.3886398673057556, loss=3.655290365219116
I0208 03:44:59.186483 139615972296448 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.37503212690353394, loss=3.5647032260894775
I0208 03:45:34.141904 139615980689152 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.38647082448005676, loss=3.5992579460144043
I0208 03:46:09.043540 139615972296448 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.37166157364845276, loss=3.572913408279419
I0208 03:46:43.990336 139615980689152 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.38905611634254456, loss=3.6404800415039062
I0208 03:47:18.937587 139615972296448 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.40068745613098145, loss=3.654508590698242
I0208 03:47:53.842528 139615980689152 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.3877849280834198, loss=3.629352331161499
I0208 03:48:28.740484 139615972296448 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3890850841999054, loss=3.5728464126586914
I0208 03:48:35.791808 139785736898368 spec.py:321] Evaluating on the training split.
I0208 03:48:38.794609 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:51:25.212658 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 03:51:27.925410 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:53:59.934030 139785736898368 spec.py:349] Evaluating on the test split.
I0208 03:54:02.667252 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 03:56:22.641686 139785736898368 submission_runner.py:408] Time since start: 72225.58s, 	Step: 127522, 	{'train/accuracy': 0.7226781845092773, 'train/loss': 1.445623517036438, 'train/bleu': 37.238805367993805, 'validation/accuracy': 0.6923658847808838, 'validation/loss': 1.6010756492614746, 'validation/bleu': 30.862977758300378, 'validation/num_examples': 3000, 'test/accuracy': 0.710719883441925, 'test/loss': 1.493880033493042, 'test/bleu': 30.998903514021368, 'test/num_examples': 3003, 'score': 44550.06398534775, 'total_duration': 72225.58293819427, 'accumulated_submission_time': 44550.06398534775, 'accumulated_eval_time': 27669.634046316147, 'accumulated_logging_time': 1.8627715110778809}
I0208 03:56:22.674788 139615980689152 logging_writer.py:48] [127522] accumulated_eval_time=27669.634046, accumulated_logging_time=1.862772, accumulated_submission_time=44550.063985, global_step=127522, preemption_count=0, score=44550.063985, test/accuracy=0.710720, test/bleu=30.998904, test/loss=1.493880, test/num_examples=3003, total_duration=72225.582938, train/accuracy=0.722678, train/bleu=37.238805, train/loss=1.445624, validation/accuracy=0.692366, validation/bleu=30.862978, validation/loss=1.601076, validation/num_examples=3000
I0208 03:56:50.255425 139615972296448 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.38168904185295105, loss=3.596804618835449
I0208 03:57:25.180108 139615980689152 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.39516234397888184, loss=3.6141724586486816
I0208 03:58:00.126577 139615972296448 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.37393587827682495, loss=3.5977234840393066
I0208 03:58:35.019129 139615980689152 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.3865731954574585, loss=3.5888659954071045
I0208 03:59:09.926363 139615972296448 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.38198697566986084, loss=3.661423921585083
I0208 03:59:44.884814 139615980689152 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.41343921422958374, loss=3.6124796867370605
I0208 04:00:19.827311 139615972296448 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.3719502389431, loss=3.6022725105285645
I0208 04:00:54.803630 139615980689152 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.3837636709213257, loss=3.575899362564087
I0208 04:01:29.743187 139615972296448 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.3890875279903412, loss=3.5959784984588623
I0208 04:02:04.651356 139615980689152 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.3662014305591583, loss=3.5535366535186768
I0208 04:02:39.560386 139615972296448 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.3729242980480194, loss=3.595149040222168
I0208 04:03:14.449244 139615980689152 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3618724048137665, loss=3.538205623626709
I0208 04:03:49.353869 139615972296448 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.3877851963043213, loss=3.5832102298736572
I0208 04:04:24.290302 139615980689152 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.3766970932483673, loss=3.6031620502471924
I0208 04:04:59.193600 139615972296448 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.4093981087207794, loss=3.639338493347168
I0208 04:05:34.102325 139615980689152 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.3822908103466034, loss=3.6429123878479004
I0208 04:06:09.001615 139615972296448 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.39039450883865356, loss=3.6091713905334473
I0208 04:06:43.915668 139615980689152 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.38740894198417664, loss=3.613182544708252
I0208 04:07:18.847936 139615972296448 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.3854156732559204, loss=3.6190247535705566
I0208 04:07:53.756434 139615980689152 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.39461636543273926, loss=3.648585796356201
I0208 04:08:28.658357 139615972296448 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3807113766670227, loss=3.605832815170288
I0208 04:09:03.568321 139615980689152 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.388515830039978, loss=3.5739057064056396
I0208 04:09:38.556868 139615972296448 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.37684425711631775, loss=3.5838894844055176
I0208 04:10:13.518605 139615980689152 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.3867897689342499, loss=3.62156081199646
I0208 04:10:22.655853 139785736898368 spec.py:321] Evaluating on the training split.
I0208 04:10:25.662175 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:13:18.253684 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 04:13:20.964323 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:15:51.332548 139785736898368 spec.py:349] Evaluating on the test split.
I0208 04:15:54.075184 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:18:16.590990 139785736898368 submission_runner.py:408] Time since start: 73539.53s, 	Step: 129928, 	{'train/accuracy': 0.7224118113517761, 'train/loss': 1.4469929933547974, 'train/bleu': 37.6780744346327, 'validation/accuracy': 0.6922914981842041, 'validation/loss': 1.6008161306381226, 'validation/bleu': 30.877284823212637, 'validation/num_examples': 3000, 'test/accuracy': 0.7106850147247314, 'test/loss': 1.4928480386734009, 'test/bleu': 30.92520062066843, 'test/num_examples': 3003, 'score': 45389.959735155106, 'total_duration': 73539.53221654892, 'accumulated_submission_time': 45389.959735155106, 'accumulated_eval_time': 28143.56909942627, 'accumulated_logging_time': 1.9056484699249268}
I0208 04:18:16.632118 139615972296448 logging_writer.py:48] [129928] accumulated_eval_time=28143.569099, accumulated_logging_time=1.905648, accumulated_submission_time=45389.959735, global_step=129928, preemption_count=0, score=45389.959735, test/accuracy=0.710685, test/bleu=30.925201, test/loss=1.492848, test/num_examples=3003, total_duration=73539.532217, train/accuracy=0.722412, train/bleu=37.678074, train/loss=1.446993, validation/accuracy=0.692291, validation/bleu=30.877285, validation/loss=1.600816, validation/num_examples=3000
I0208 04:18:42.117205 139615980689152 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.375092089176178, loss=3.603476047515869
I0208 04:19:17.013754 139615972296448 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.39110124111175537, loss=3.6055023670196533
I0208 04:19:51.940096 139615980689152 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.39388903975486755, loss=3.6419174671173096
I0208 04:20:26.848412 139615972296448 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.385816752910614, loss=3.5937039852142334
I0208 04:21:01.764427 139615980689152 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.35755836963653564, loss=3.526076555252075
I0208 04:21:36.766418 139615972296448 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.37965619564056396, loss=3.6262433528900146
I0208 04:22:11.696685 139615980689152 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.4016844928264618, loss=3.611506700515747
I0208 04:22:46.616110 139615972296448 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.381055623292923, loss=3.616821527481079
I0208 04:23:21.538248 139615980689152 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.3743501603603363, loss=3.585160493850708
I0208 04:23:56.459091 139615972296448 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.39958828687667847, loss=3.6828343868255615
I0208 04:24:31.353564 139615980689152 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.37136438488960266, loss=3.5812957286834717
I0208 04:25:06.266711 139615972296448 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.3898926079273224, loss=3.5943822860717773
I0208 04:25:41.194231 139615980689152 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.3578629493713379, loss=3.6084585189819336
I0208 04:26:16.158122 139615972296448 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.39588290452957153, loss=3.6447947025299072
I0208 04:26:51.064670 139615980689152 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.38705864548683167, loss=3.5803635120391846
I0208 04:27:25.979722 139615972296448 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.3811170160770416, loss=3.6037111282348633
I0208 04:28:00.881235 139615980689152 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.38514840602874756, loss=3.62524676322937
I0208 04:28:35.802660 139615972296448 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.40145227313041687, loss=3.622036933898926
I0208 04:29:10.721518 139615980689152 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.3733862340450287, loss=3.649181604385376
I0208 04:29:45.644083 139615972296448 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.3847704827785492, loss=3.597177267074585
I0208 04:30:20.571174 139615980689152 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.3895786702632904, loss=3.585157871246338
I0208 04:30:55.462624 139615972296448 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.38251346349716187, loss=3.581050395965576
I0208 04:31:30.380423 139615980689152 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.3769524097442627, loss=3.6014480590820312
I0208 04:32:05.285816 139615972296448 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.3718850612640381, loss=3.57297945022583
I0208 04:32:16.865729 139785736898368 spec.py:321] Evaluating on the training split.
I0208 04:32:19.871930 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:35:09.042721 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 04:35:11.782875 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:37:42.115045 139785736898368 spec.py:349] Evaluating on the test split.
I0208 04:37:44.832131 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:40:06.540735 139785736898368 submission_runner.py:408] Time since start: 74849.48s, 	Step: 132335, 	{'train/accuracy': 0.7235233783721924, 'train/loss': 1.4436975717544556, 'train/bleu': 37.51305828525132, 'validation/accuracy': 0.6922170519828796, 'validation/loss': 1.6004061698913574, 'validation/bleu': 30.95629465203926, 'validation/num_examples': 3000, 'test/accuracy': 0.7106037139892578, 'test/loss': 1.492241621017456, 'test/bleu': 30.889636565732552, 'test/num_examples': 3003, 'score': 46230.10717344284, 'total_duration': 74849.48199796677, 'accumulated_submission_time': 46230.10717344284, 'accumulated_eval_time': 28613.24405527115, 'accumulated_logging_time': 1.9580817222595215}
I0208 04:40:06.575135 139615980689152 logging_writer.py:48] [132335] accumulated_eval_time=28613.244055, accumulated_logging_time=1.958082, accumulated_submission_time=46230.107173, global_step=132335, preemption_count=0, score=46230.107173, test/accuracy=0.710604, test/bleu=30.889637, test/loss=1.492242, test/num_examples=3003, total_duration=74849.481998, train/accuracy=0.723523, train/bleu=37.513058, train/loss=1.443698, validation/accuracy=0.692217, validation/bleu=30.956295, validation/loss=1.600406, validation/num_examples=3000
I0208 04:40:29.636504 139615972296448 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.3875928819179535, loss=3.579350709915161
I0208 04:41:04.571701 139615980689152 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.38513246178627014, loss=3.6459944248199463
I0208 04:41:39.509523 139615972296448 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.38133224844932556, loss=3.621987819671631
I0208 04:42:14.472912 139615980689152 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.431043803691864, loss=3.6139721870422363
I0208 04:42:49.493717 139615972296448 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.38431888818740845, loss=3.582289457321167
I0208 04:43:24.437078 139615980689152 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.37623992562294006, loss=3.6199769973754883
I0208 04:43:59.360059 139615972296448 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.3763449192047119, loss=3.6015076637268066
I0208 04:44:34.288131 139615980689152 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.37727266550064087, loss=3.6012465953826904
I0208 04:45:09.178803 139615972296448 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.3557405471801758, loss=3.5478522777557373
I0208 04:45:44.094105 139615980689152 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.405333548784256, loss=3.6818671226501465
I0208 04:45:54.986631 139785736898368 spec.py:321] Evaluating on the training split.
I0208 04:45:57.984176 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:48:50.088905 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 04:48:52.798451 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:51:22.328676 139785736898368 spec.py:349] Evaluating on the test split.
I0208 04:51:25.062285 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:53:47.325662 139785736898368 submission_runner.py:408] Time since start: 75670.27s, 	Step: 133333, 	{'train/accuracy': 0.721368134021759, 'train/loss': 1.4508073329925537, 'train/bleu': 37.40015536663024, 'validation/accuracy': 0.6922046542167664, 'validation/loss': 1.6004267930984497, 'validation/bleu': 30.959032282956052, 'validation/num_examples': 3000, 'test/accuracy': 0.7105804681777954, 'test/loss': 1.4921976327896118, 'test/bleu': 30.862555894325826, 'test/num_examples': 3003, 'score': 46578.47412419319, 'total_duration': 75670.26692295074, 'accumulated_submission_time': 46578.47412419319, 'accumulated_eval_time': 29085.583038330078, 'accumulated_logging_time': 2.0043785572052}
I0208 04:53:47.358497 139615972296448 logging_writer.py:48] [133333] accumulated_eval_time=29085.583038, accumulated_logging_time=2.004379, accumulated_submission_time=46578.474124, global_step=133333, preemption_count=0, score=46578.474124, test/accuracy=0.710580, test/bleu=30.862556, test/loss=1.492198, test/num_examples=3003, total_duration=75670.266923, train/accuracy=0.721368, train/bleu=37.400155, train/loss=1.450807, validation/accuracy=0.692205, validation/bleu=30.959032, validation/loss=1.600427, validation/num_examples=3000
I0208 04:53:47.393380 139615980689152 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46578.474124
I0208 04:53:48.489051 139785736898368 checkpoints.py:490] Saving checkpoint at step: 133333
I0208 04:53:52.408125 139785736898368 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2/checkpoint_133333
I0208 04:53:52.413225 139785736898368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_2/checkpoint_133333.
I0208 04:53:52.461575 139785736898368 submission_runner.py:583] Tuning trial 2/5
I0208 04:53:52.461727 139785736898368 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0208 04:53:52.465526 139785736898368 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006168890395201743, 'train/loss': 11.06764030456543, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.26857042312622, 'total_duration': 896.0825242996216, 'accumulated_submission_time': 27.26857042312622, 'accumulated_eval_time': 868.8139111995697, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2403, {'train/accuracy': 0.42342033982276917, 'train/loss': 3.922987222671509, 'train/bleu': 15.385226949250677, 'validation/accuracy': 0.40709972381591797, 'validation/loss': 4.05037260055542, 'validation/bleu': 10.213285356315824, 'validation/num_examples': 3000, 'test/accuracy': 0.39780375361442566, 'test/loss': 4.211139678955078, 'test/bleu': 8.912712367020626, 'test/num_examples': 3003, 'score': 867.3339140415192, 'total_duration': 2426.8114387989044, 'accumulated_submission_time': 867.3339140415192, 'accumulated_eval_time': 1559.3806607723236, 'accumulated_logging_time': 0.020984649658203125, 'global_step': 2403, 'preemption_count': 0}), (4805, {'train/accuracy': 0.5433375835418701, 'train/loss': 2.8315465450286865, 'train/bleu': 23.489781442927157, 'validation/accuracy': 0.5458456873893738, 'validation/loss': 2.7822792530059814, 'validation/bleu': 19.935954988012053, 'validation/num_examples': 3000, 'test/accuracy': 0.5467666387557983, 'test/loss': 2.8108057975769043, 'test/bleu': 18.429889335573424, 'test/num_examples': 3003, 'score': 1707.4406251907349, 'total_duration': 3728.882633447647, 'accumulated_submission_time': 1707.4406251907349, 'accumulated_eval_time': 2021.2432608604431, 'accumulated_logging_time': 0.04601240158081055, 'global_step': 4805, 'preemption_count': 0}), (7209, {'train/accuracy': 0.5829526782035828, 'train/loss': 2.4544622898101807, 'train/bleu': 26.772562741454795, 'validation/accuracy': 0.5922927260398865, 'validation/loss': 2.3894851207733154, 'validation/bleu': 23.585131664602347, 'validation/num_examples': 3000, 'test/accuracy': 0.5943059921264648, 'test/loss': 2.385735273361206, 'test/bleu': 22.20622453217967, 'test/num_examples': 3003, 'score': 2547.405416727066, 'total_duration': 5034.181886911392, 'accumulated_submission_time': 2547.405416727066, 'accumulated_eval_time': 2486.4783968925476, 'accumulated_logging_time': 0.07095670700073242, 'global_step': 7209, 'preemption_count': 0}), (9614, {'train/accuracy': 0.6014232039451599, 'train/loss': 2.2858991622924805, 'train/bleu': 28.633667442316284, 'validation/accuracy': 0.6167809367179871, 'validation/loss': 2.163081407546997, 'validation/bleu': 24.976861417148402, 'validation/num_examples': 3000, 'test/accuracy': 0.6229039430618286, 'test/loss': 2.132373332977295, 'test/bleu': 23.902748515407488, 'test/num_examples': 3003, 'score': 3387.3830687999725, 'total_duration': 6355.566185951233, 'accumulated_submission_time': 3387.3830687999725, 'accumulated_eval_time': 2967.781727552414, 'accumulated_logging_time': 0.09715795516967773, 'global_step': 9614, 'preemption_count': 0}), (12019, {'train/accuracy': 0.6114262938499451, 'train/loss': 2.1870195865631104, 'train/bleu': 29.344313492146362, 'validation/accuracy': 0.6296511888504028, 'validation/loss': 2.044898271560669, 'validation/bleu': 25.76193795439446, 'validation/num_examples': 3000, 'test/accuracy': 0.6383940577507019, 'test/loss': 1.9973318576812744, 'test/bleu': 25.066060338305476, 'test/num_examples': 3003, 'score': 4227.473156452179, 'total_duration': 7686.628827095032, 'accumulated_submission_time': 4227.473156452179, 'accumulated_eval_time': 3458.648379802704, 'accumulated_logging_time': 0.12830424308776855, 'global_step': 12019, 'preemption_count': 0}), (14423, {'train/accuracy': 0.6259955763816833, 'train/loss': 2.062871217727661, 'train/bleu': 30.1025163364132, 'validation/accuracy': 0.6408227682113647, 'validation/loss': 1.9591337442398071, 'validation/bleu': 26.82538380800523, 'validation/num_examples': 3000, 'test/accuracy': 0.6490268111228943, 'test/loss': 1.909260869026184, 'test/bleu': 26.05126954299677, 'test/num_examples': 3003, 'score': 5067.463086843491, 'total_duration': 8996.47902727127, 'accumulated_submission_time': 5067.463086843491, 'accumulated_eval_time': 3928.3998835086823, 'accumulated_logging_time': 0.15821146965026855, 'global_step': 14423, 'preemption_count': 0}), (16828, {'train/accuracy': 0.6307092905044556, 'train/loss': 2.047231912612915, 'train/bleu': 30.666321567767124, 'validation/accuracy': 0.647034764289856, 'validation/loss': 1.9078556299209595, 'validation/bleu': 27.453680639560886, 'validation/num_examples': 3000, 'test/accuracy': 0.6569984555244446, 'test/loss': 1.8533170223236084, 'test/bleu': 26.831555098397647, 'test/num_examples': 3003, 'score': 5907.40859413147, 'total_duration': 10296.226013422012, 'accumulated_submission_time': 5907.40859413147, 'accumulated_eval_time': 4388.0984263420105, 'accumulated_logging_time': 0.1860949993133545, 'global_step': 16828, 'preemption_count': 0}), (19234, {'train/accuracy': 0.6473884582519531, 'train/loss': 1.9363336563110352, 'train/bleu': 31.799686839622684, 'validation/accuracy': 0.6544494032859802, 'validation/loss': 1.8840500116348267, 'validation/bleu': 27.786832108362752, 'validation/num_examples': 3000, 'test/accuracy': 0.6652141213417053, 'test/loss': 1.8207148313522339, 'test/bleu': 27.350837919366988, 'test/num_examples': 3003, 'score': 6747.634348392487, 'total_duration': 11598.04559969902, 'accumulated_submission_time': 6747.634348392487, 'accumulated_eval_time': 4849.583470821381, 'accumulated_logging_time': 0.21756911277770996, 'global_step': 19234, 'preemption_count': 0}), (21640, {'train/accuracy': 0.6389160752296448, 'train/loss': 1.9853614568710327, 'train/bleu': 31.149602875625366, 'validation/accuracy': 0.6562844514846802, 'validation/loss': 1.854095458984375, 'validation/bleu': 28.157027809582498, 'validation/num_examples': 3000, 'test/accuracy': 0.6675498485565186, 'test/loss': 1.796425461769104, 'test/bleu': 27.407634766447824, 'test/num_examples': 3003, 'score': 7587.8516755104065, 'total_duration': 12964.936593532562, 'accumulated_submission_time': 7587.8516755104065, 'accumulated_eval_time': 5376.148508548737, 'accumulated_logging_time': 0.24692964553833008, 'global_step': 21640, 'preemption_count': 0}), (24047, {'train/accuracy': 0.6381778717041016, 'train/loss': 1.9780657291412354, 'train/bleu': 31.645160669568956, 'validation/accuracy': 0.6574996113777161, 'validation/loss': 1.8356502056121826, 'validation/bleu': 27.9298678608305, 'validation/num_examples': 3000, 'test/accuracy': 0.668502688407898, 'test/loss': 1.7730231285095215, 'test/bleu': 27.702566928617625, 'test/num_examples': 3003, 'score': 8428.0314412117, 'total_duration': 14321.60050368309, 'accumulated_submission_time': 8428.0314412117, 'accumulated_eval_time': 5892.5287890434265, 'accumulated_logging_time': 0.274691104888916, 'global_step': 24047, 'preemption_count': 0}), (26453, {'train/accuracy': 0.650377094745636, 'train/loss': 1.8855642080307007, 'train/bleu': 31.99286060995921, 'validation/accuracy': 0.6624096632003784, 'validation/loss': 1.792500615119934, 'validation/bleu': 28.47187284902378, 'validation/num_examples': 3000, 'test/accuracy': 0.6716518402099609, 'test/loss': 1.732503056526184, 'test/bleu': 28.19627620748597, 'test/num_examples': 3003, 'score': 9267.948320865631, 'total_duration': 15635.728284358978, 'accumulated_submission_time': 9267.948320865631, 'accumulated_eval_time': 6366.634291410446, 'accumulated_logging_time': 0.30600905418395996, 'global_step': 26453, 'preemption_count': 0}), (28861, {'train/accuracy': 0.6490204334259033, 'train/loss': 1.8970963954925537, 'train/bleu': 31.745865182709256, 'validation/accuracy': 0.6626080274581909, 'validation/loss': 1.7790693044662476, 'validation/bleu': 28.336580553316416, 'validation/num_examples': 3000, 'test/accuracy': 0.6757306456565857, 'test/loss': 1.7089323997497559, 'test/bleu': 28.2438093623282, 'test/num_examples': 3003, 'score': 10108.041011333466, 'total_duration': 16965.351059913635, 'accumulated_submission_time': 10108.041011333466, 'accumulated_eval_time': 6856.061012983322, 'accumulated_logging_time': 0.33478879928588867, 'global_step': 28861, 'preemption_count': 0}), (31268, {'train/accuracy': 0.6462864875793457, 'train/loss': 1.938828706741333, 'train/bleu': 32.04996210677026, 'validation/accuracy': 0.6645174622535706, 'validation/loss': 1.804949164390564, 'validation/bleu': 28.524153144156863, 'validation/num_examples': 3000, 'test/accuracy': 0.6761257648468018, 'test/loss': 1.7377357482910156, 'test/bleu': 27.99109156876941, 'test/num_examples': 3003, 'score': 10948.245263814926, 'total_duration': 18491.936994314194, 'accumulated_submission_time': 10948.245263814926, 'accumulated_eval_time': 7542.333392858505, 'accumulated_logging_time': 0.3696925640106201, 'global_step': 31268, 'preemption_count': 0}), (33674, {'train/accuracy': 0.6534045934677124, 'train/loss': 1.8654236793518066, 'train/bleu': 31.9049431373965, 'validation/accuracy': 0.667592465877533, 'validation/loss': 1.7680360078811646, 'validation/bleu': 28.721269757279806, 'validation/num_examples': 3000, 'test/accuracy': 0.6791703104972839, 'test/loss': 1.6936718225479126, 'test/bleu': 28.643860573925735, 'test/num_examples': 3003, 'score': 11788.358795166016, 'total_duration': 19847.839690446854, 'accumulated_submission_time': 11788.358795166016, 'accumulated_eval_time': 8058.018579006195, 'accumulated_logging_time': 0.39875078201293945, 'global_step': 33674, 'preemption_count': 0}), (36080, {'train/accuracy': 0.6488655209541321, 'train/loss': 1.889739751815796, 'train/bleu': 32.563431170556136, 'validation/accuracy': 0.6692167520523071, 'validation/loss': 1.754157543182373, 'validation/bleu': 29.021798663119707, 'validation/num_examples': 3000, 'test/accuracy': 0.6801928877830505, 'test/loss': 1.6816045045852661, 'test/bleu': 28.412542674194228, 'test/num_examples': 3003, 'score': 12628.493074417114, 'total_duration': 21165.33632516861, 'accumulated_submission_time': 12628.493074417114, 'accumulated_eval_time': 8535.275834321976, 'accumulated_logging_time': 0.4287540912628174, 'global_step': 36080, 'preemption_count': 0}), (38487, {'train/accuracy': 0.6632543206214905, 'train/loss': 1.8003400564193726, 'train/bleu': 32.92000144955914, 'validation/accuracy': 0.66898113489151, 'validation/loss': 1.75272798538208, 'validation/bleu': 28.953533798359764, 'validation/num_examples': 3000, 'test/accuracy': 0.6821103096008301, 'test/loss': 1.679569959640503, 'test/bleu': 28.864725008764605, 'test/num_examples': 3003, 'score': 13468.718967199326, 'total_duration': 22477.581879138947, 'accumulated_submission_time': 13468.718967199326, 'accumulated_eval_time': 9007.191690206528, 'accumulated_logging_time': 0.4582366943359375, 'global_step': 38487, 'preemption_count': 0}), (40893, {'train/accuracy': 0.6533471941947937, 'train/loss': 1.8556861877441406, 'train/bleu': 32.56679466777, 'validation/accuracy': 0.6716221570968628, 'validation/loss': 1.7270926237106323, 'validation/bleu': 29.09871189610873, 'validation/num_examples': 3000, 'test/accuracy': 0.6849921941757202, 'test/loss': 1.6443243026733398, 'test/bleu': 29.026990271202685, 'test/num_examples': 3003, 'score': 14308.6868288517, 'total_duration': 23799.267735242844, 'accumulated_submission_time': 14308.6868288517, 'accumulated_eval_time': 9488.803978919983, 'accumulated_logging_time': 0.48833346366882324, 'global_step': 40893, 'preemption_count': 0}), (43300, {'train/accuracy': 0.6563020348548889, 'train/loss': 1.8518691062927246, 'train/bleu': 32.51621448980282, 'validation/accuracy': 0.6724033355712891, 'validation/loss': 1.733350157737732, 'validation/bleu': 29.32289596719666, 'validation/num_examples': 3000, 'test/accuracy': 0.685631275177002, 'test/loss': 1.6498874425888062, 'test/bleu': 28.75444159865201, 'test/num_examples': 3003, 'score': 15148.599982261658, 'total_duration': 25243.02674794197, 'accumulated_submission_time': 15148.599982261658, 'accumulated_eval_time': 10092.543762683868, 'accumulated_logging_time': 0.5201573371887207, 'global_step': 43300, 'preemption_count': 0}), (45706, {'train/accuracy': 0.6638458371162415, 'train/loss': 1.7846899032592773, 'train/bleu': 33.13194819782453, 'validation/accuracy': 0.6739159822463989, 'validation/loss': 1.7038495540618896, 'validation/bleu': 29.235713821138607, 'validation/num_examples': 3000, 'test/accuracy': 0.6875951886177063, 'test/loss': 1.6285982131958008, 'test/bleu': 29.441668743185758, 'test/num_examples': 3003, 'score': 15988.6215569973, 'total_duration': 26588.011610507965, 'accumulated_submission_time': 15988.6215569973, 'accumulated_eval_time': 10597.398320198059, 'accumulated_logging_time': 0.5518801212310791, 'global_step': 45706, 'preemption_count': 0}), (48112, {'train/accuracy': 0.6603615880012512, 'train/loss': 1.8153201341629028, 'train/bleu': 32.8939815446997, 'validation/accuracy': 0.6750814914703369, 'validation/loss': 1.6979299783706665, 'validation/bleu': 29.456297594517306, 'validation/num_examples': 3000, 'test/accuracy': 0.6886642575263977, 'test/loss': 1.616563081741333, 'test/bleu': 29.52146011305364, 'test/num_examples': 3003, 'score': 16828.530921697617, 'total_duration': 27914.889127254486, 'accumulated_submission_time': 16828.530921697617, 'accumulated_eval_time': 11084.258370399475, 'accumulated_logging_time': 0.5843021869659424, 'global_step': 48112, 'preemption_count': 0}), (50518, {'train/accuracy': 0.669950544834137, 'train/loss': 1.7502673864364624, 'train/bleu': 33.24540770074166, 'validation/accuracy': 0.6771769523620605, 'validation/loss': 1.6892192363739014, 'validation/bleu': 29.521489562011492, 'validation/num_examples': 3000, 'test/accuracy': 0.6907094717025757, 'test/loss': 1.609735131263733, 'test/bleu': 29.501761063552795, 'test/num_examples': 3003, 'score': 17668.503177165985, 'total_duration': 29217.95916581154, 'accumulated_submission_time': 17668.503177165985, 'accumulated_eval_time': 11547.249927043915, 'accumulated_logging_time': 0.6154048442840576, 'global_step': 50518, 'preemption_count': 0}), (52924, {'train/accuracy': 0.6591640114784241, 'train/loss': 1.8146765232086182, 'train/bleu': 32.61795057033321, 'validation/accuracy': 0.6771769523620605, 'validation/loss': 1.7013094425201416, 'validation/bleu': 29.5259645039707, 'validation/num_examples': 3000, 'test/accuracy': 0.6914182901382446, 'test/loss': 1.6207830905914307, 'test/bleu': 29.616646797154754, 'test/num_examples': 3003, 'score': 18508.683936357498, 'total_duration': 30641.154708862305, 'accumulated_submission_time': 18508.683936357498, 'accumulated_eval_time': 12130.155924797058, 'accumulated_logging_time': 0.6474461555480957, 'global_step': 52924, 'preemption_count': 0}), (55330, {'train/accuracy': 0.6601080894470215, 'train/loss': 1.815306544303894, 'train/bleu': 32.35156533613473, 'validation/accuracy': 0.677424967288971, 'validation/loss': 1.687734603881836, 'validation/bleu': 29.76204785307309, 'validation/num_examples': 3000, 'test/accuracy': 0.6917552947998047, 'test/loss': 1.6076879501342773, 'test/bleu': 29.35782497609175, 'test/num_examples': 3003, 'score': 19348.650020122528, 'total_duration': 32106.200585126877, 'accumulated_submission_time': 19348.650020122528, 'accumulated_eval_time': 12755.12444281578, 'accumulated_logging_time': 0.679734468460083, 'global_step': 55330, 'preemption_count': 0}), (57737, {'train/accuracy': 0.6684832572937012, 'train/loss': 1.7588273286819458, 'train/bleu': 33.19071769118694, 'validation/accuracy': 0.6785284876823425, 'validation/loss': 1.6828522682189941, 'validation/bleu': 29.483006700913737, 'validation/num_examples': 3000, 'test/accuracy': 0.6926733255386353, 'test/loss': 1.5996352434158325, 'test/bleu': 29.221216444179152, 'test/num_examples': 3003, 'score': 20188.56041240692, 'total_duration': 33447.00986433029, 'accumulated_submission_time': 20188.56041240692, 'accumulated_eval_time': 13255.915263652802, 'accumulated_logging_time': 0.7129116058349609, 'global_step': 57737, 'preemption_count': 0}), (60144, {'train/accuracy': 0.6650095582008362, 'train/loss': 1.7817625999450684, 'train/bleu': 33.05104338028556, 'validation/accuracy': 0.6779953241348267, 'validation/loss': 1.6812210083007812, 'validation/bleu': 29.612110187777798, 'validation/num_examples': 3000, 'test/accuracy': 0.6945558190345764, 'test/loss': 1.593664526939392, 'test/bleu': 29.650212730852033, 'test/num_examples': 3003, 'score': 21028.63009619713, 'total_duration': 34828.37162208557, 'accumulated_submission_time': 21028.63009619713, 'accumulated_eval_time': 13797.100610494614, 'accumulated_logging_time': 0.7455697059631348, 'global_step': 60144, 'preemption_count': 0}), (62550, {'train/accuracy': 0.7026515007019043, 'train/loss': 1.5845561027526855, 'train/bleu': 35.26644664948576, 'validation/accuracy': 0.6804379224777222, 'validation/loss': 1.6678342819213867, 'validation/bleu': 30.021677303349723, 'validation/num_examples': 3000, 'test/accuracy': 0.6963453888893127, 'test/loss': 1.5847262144088745, 'test/bleu': 29.76351245408125, 'test/num_examples': 3003, 'score': 21868.660029172897, 'total_duration': 36325.464529037476, 'accumulated_submission_time': 21868.660029172897, 'accumulated_eval_time': 14454.052380561829, 'accumulated_logging_time': 0.7788140773773193, 'global_step': 62550, 'preemption_count': 0}), (64956, {'train/accuracy': 0.6722168922424316, 'train/loss': 1.7237827777862549, 'train/bleu': 33.19875507863805, 'validation/accuracy': 0.6810950636863708, 'validation/loss': 1.657042384147644, 'validation/bleu': 29.83164370579329, 'validation/num_examples': 3000, 'test/accuracy': 0.6964964270591736, 'test/loss': 1.5666615962982178, 'test/bleu': 30.08841979180686, 'test/num_examples': 3003, 'score': 22708.81247138977, 'total_duration': 37632.84275865555, 'accumulated_submission_time': 22708.81247138977, 'accumulated_eval_time': 14921.166560411453, 'accumulated_logging_time': 0.8124041557312012, 'global_step': 64956, 'preemption_count': 0}), (67362, {'train/accuracy': 0.6712957620620728, 'train/loss': 1.734437108039856, 'train/bleu': 33.597483300602605, 'validation/accuracy': 0.6810950636863708, 'validation/loss': 1.6491377353668213, 'validation/bleu': 29.94199898103257, 'validation/num_examples': 3000, 'test/accuracy': 0.6967636942863464, 'test/loss': 1.5596245527267456, 'test/bleu': 29.930429323320606, 'test/num_examples': 3003, 'score': 23548.917605161667, 'total_duration': 38978.3294506073, 'accumulated_submission_time': 23548.917605161667, 'accumulated_eval_time': 15426.43873333931, 'accumulated_logging_time': 0.8456981182098389, 'global_step': 67362, 'preemption_count': 0}), (69769, {'train/accuracy': 0.6853699684143066, 'train/loss': 1.6466609239578247, 'train/bleu': 34.29055829084901, 'validation/accuracy': 0.6822109818458557, 'validation/loss': 1.6531620025634766, 'validation/bleu': 29.901509568131214, 'validation/num_examples': 3000, 'test/accuracy': 0.6988670229911804, 'test/loss': 1.5575608015060425, 'test/bleu': 30.00534103427137, 'test/num_examples': 3003, 'score': 24389.033936738968, 'total_duration': 40333.188530921936, 'accumulated_submission_time': 24389.033936738968, 'accumulated_eval_time': 15941.064447641373, 'accumulated_logging_time': 0.8869020938873291, 'global_step': 69769, 'preemption_count': 0}), (72175, {'train/accuracy': 0.6768722534179688, 'train/loss': 1.701812744140625, 'train/bleu': 33.83223992527047, 'validation/accuracy': 0.6833640933036804, 'validation/loss': 1.6468572616577148, 'validation/bleu': 29.873267425150825, 'validation/num_examples': 3000, 'test/accuracy': 0.6993783116340637, 'test/loss': 1.5530868768692017, 'test/bleu': 29.956049731472202, 'test/num_examples': 3003, 'score': 25228.926471233368, 'total_duration': 41618.32129120827, 'accumulated_submission_time': 25228.926471233368, 'accumulated_eval_time': 16386.192722558975, 'accumulated_logging_time': 0.9233071804046631, 'global_step': 72175, 'preemption_count': 0}), (74582, {'train/accuracy': 0.6750902533531189, 'train/loss': 1.7135508060455322, 'train/bleu': 33.31560347389563, 'validation/accuracy': 0.6835005283355713, 'validation/loss': 1.6415091753005981, 'validation/bleu': 30.202027501872564, 'validation/num_examples': 3000, 'test/accuracy': 0.6996455788612366, 'test/loss': 1.5487751960754395, 'test/bleu': 29.979354940034053, 'test/num_examples': 3003, 'score': 26069.164647579193, 'total_duration': 42970.60527634621, 'accumulated_submission_time': 26069.164647579193, 'accumulated_eval_time': 16898.12716984749, 'accumulated_logging_time': 0.9586780071258545, 'global_step': 74582, 'preemption_count': 0}), (76989, {'train/accuracy': 0.6826446056365967, 'train/loss': 1.6596330404281616, 'train/bleu': 34.03202156257172, 'validation/accuracy': 0.6867862939834595, 'validation/loss': 1.6362948417663574, 'validation/bleu': 30.34294338222907, 'validation/num_examples': 3000, 'test/accuracy': 0.7015281319618225, 'test/loss': 1.541305422782898, 'test/bleu': 30.308356401150277, 'test/num_examples': 3003, 'score': 26909.29024219513, 'total_duration': 44296.71672439575, 'accumulated_submission_time': 26909.29024219513, 'accumulated_eval_time': 17383.99588394165, 'accumulated_logging_time': 1.000993251800537, 'global_step': 76989, 'preemption_count': 0}), (79395, {'train/accuracy': 0.6748082637786865, 'train/loss': 1.7070127725601196, 'train/bleu': 34.39932828355682, 'validation/accuracy': 0.6856703758239746, 'validation/loss': 1.6362732648849487, 'validation/bleu': 30.385912673364096, 'validation/num_examples': 3000, 'test/accuracy': 0.7026785612106323, 'test/loss': 1.537244200706482, 'test/bleu': 30.357957762145276, 'test/num_examples': 3003, 'score': 27749.502287864685, 'total_duration': 45602.726059913635, 'accumulated_submission_time': 27749.502287864685, 'accumulated_eval_time': 17849.676304340363, 'accumulated_logging_time': 1.0389277935028076, 'global_step': 79395, 'preemption_count': 0}), (81801, {'train/accuracy': 0.7017195820808411, 'train/loss': 1.5646322965621948, 'train/bleu': 35.8828094371977, 'validation/accuracy': 0.685732364654541, 'validation/loss': 1.6307544708251953, 'validation/bleu': 30.018876279781775, 'validation/num_examples': 3000, 'test/accuracy': 0.7022950649261475, 'test/loss': 1.538099765777588, 'test/bleu': 30.294979237991356, 'test/num_examples': 3003, 'score': 28589.634435892105, 'total_duration': 46942.80326747894, 'accumulated_submission_time': 28589.634435892105, 'accumulated_eval_time': 18349.50480556488, 'accumulated_logging_time': 1.0803887844085693, 'global_step': 81801, 'preemption_count': 0}), (84207, {'train/accuracy': 0.683625340461731, 'train/loss': 1.652172327041626, 'train/bleu': 34.681021300800836, 'validation/accuracy': 0.6857695579528809, 'validation/loss': 1.6258001327514648, 'validation/bleu': 30.571937519327353, 'validation/num_examples': 3000, 'test/accuracy': 0.7048166990280151, 'test/loss': 1.5285508632659912, 'test/bleu': 30.50286774566312, 'test/num_examples': 3003, 'score': 29429.539747953415, 'total_duration': 48268.892318964005, 'accumulated_submission_time': 29429.539747953415, 'accumulated_eval_time': 18835.574059724808, 'accumulated_logging_time': 1.1179816722869873, 'global_step': 84207, 'preemption_count': 0}), (86613, {'train/accuracy': 0.6862362027168274, 'train/loss': 1.647615671157837, 'train/bleu': 34.51882434228991, 'validation/accuracy': 0.6874062418937683, 'validation/loss': 1.6228716373443604, 'validation/bleu': 30.232125626879338, 'validation/num_examples': 3000, 'test/accuracy': 0.705142080783844, 'test/loss': 1.5264335870742798, 'test/bleu': 30.389439631208806, 'test/num_examples': 3003, 'score': 30269.543409347534, 'total_duration': 49691.58460474014, 'accumulated_submission_time': 30269.543409347534, 'accumulated_eval_time': 19418.148502588272, 'accumulated_logging_time': 1.1565618515014648, 'global_step': 86613, 'preemption_count': 0}), (89019, {'train/accuracy': 0.6950206756591797, 'train/loss': 1.5976887941360474, 'train/bleu': 35.72473038291398, 'validation/accuracy': 0.6871954202651978, 'validation/loss': 1.6184560060501099, 'validation/bleu': 30.638430798079032, 'validation/num_examples': 3000, 'test/accuracy': 0.7048166990280151, 'test/loss': 1.5245695114135742, 'test/bleu': 30.56950362536727, 'test/num_examples': 3003, 'score': 31109.477601766586, 'total_duration': 51031.5335791111, 'accumulated_submission_time': 31109.477601766586, 'accumulated_eval_time': 19918.049089193344, 'accumulated_logging_time': 1.1962296962738037, 'global_step': 89019, 'preemption_count': 0}), (91425, {'train/accuracy': 0.693752646446228, 'train/loss': 1.6055411100387573, 'train/bleu': 34.55984270647691, 'validation/accuracy': 0.688559353351593, 'validation/loss': 1.6157660484313965, 'validation/bleu': 30.754953917388068, 'validation/num_examples': 3000, 'test/accuracy': 0.7056185007095337, 'test/loss': 1.5198183059692383, 'test/bleu': 30.65265721547933, 'test/num_examples': 3003, 'score': 31949.395862579346, 'total_duration': 52349.091624975204, 'accumulated_submission_time': 31949.395862579346, 'accumulated_eval_time': 20395.571761369705, 'accumulated_logging_time': 1.2364046573638916, 'global_step': 91425, 'preemption_count': 0}), (93831, {'train/accuracy': 0.7316763997077942, 'train/loss': 1.4330400228500366, 'train/bleu': 38.144238126448364, 'validation/accuracy': 0.6875426173210144, 'validation/loss': 1.6164175271987915, 'validation/bleu': 30.552952501016037, 'validation/num_examples': 3000, 'test/accuracy': 0.7061066031455994, 'test/loss': 1.518967866897583, 'test/bleu': 30.630823150616674, 'test/num_examples': 3003, 'score': 32789.459950208664, 'total_duration': 53728.417481184006, 'accumulated_submission_time': 32789.459950208664, 'accumulated_eval_time': 20934.718940973282, 'accumulated_logging_time': 1.274902582168579, 'global_step': 93831, 'preemption_count': 0}), (96237, {'train/accuracy': 0.7016276121139526, 'train/loss': 1.558693528175354, 'train/bleu': 35.49628524369535, 'validation/accuracy': 0.6899976134300232, 'validation/loss': 1.6058191061019897, 'validation/bleu': 30.619367228391614, 'validation/num_examples': 3000, 'test/accuracy': 0.7069665193557739, 'test/loss': 1.5066982507705688, 'test/bleu': 30.65861045851249, 'test/num_examples': 3003, 'score': 33629.5340692997, 'total_duration': 55075.31088614464, 'accumulated_submission_time': 33629.5340692997, 'accumulated_eval_time': 21441.422430038452, 'accumulated_logging_time': 1.31276273727417, 'global_step': 96237, 'preemption_count': 0}), (98644, {'train/accuracy': 0.6927933096885681, 'train/loss': 1.601349115371704, 'train/bleu': 35.66692699405113, 'validation/accuracy': 0.6894148588180542, 'validation/loss': 1.6130400896072388, 'validation/bleu': 30.435671431315626, 'validation/num_examples': 3000, 'test/accuracy': 0.7068967819213867, 'test/loss': 1.5110329389572144, 'test/bleu': 30.476182677665605, 'test/num_examples': 3003, 'score': 34469.55902791023, 'total_duration': 56434.93587017059, 'accumulated_submission_time': 34469.55902791023, 'accumulated_eval_time': 21960.90658211708, 'accumulated_logging_time': 1.353524923324585, 'global_step': 98644, 'preemption_count': 0}), (101051, {'train/accuracy': 0.7072663307189941, 'train/loss': 1.5225698947906494, 'train/bleu': 36.582741766440975, 'validation/accuracy': 0.6892164945602417, 'validation/loss': 1.6107611656188965, 'validation/bleu': 30.412669470445955, 'validation/num_examples': 3000, 'test/accuracy': 0.7070245742797852, 'test/loss': 1.507551908493042, 'test/bleu': 30.72464167955525, 'test/num_examples': 3003, 'score': 35309.74000072479, 'total_duration': 57728.85360980034, 'accumulated_submission_time': 35309.74000072479, 'accumulated_eval_time': 22414.529752969742, 'accumulated_logging_time': 1.3917343616485596, 'global_step': 101051, 'preemption_count': 0}), (103458, {'train/accuracy': 0.6998023390769958, 'train/loss': 1.5615475177764893, 'train/bleu': 35.472857329654225, 'validation/accuracy': 0.6898860335350037, 'validation/loss': 1.608350396156311, 'validation/bleu': 30.398652314834955, 'validation/num_examples': 3000, 'test/accuracy': 0.707466185092926, 'test/loss': 1.5031170845031738, 'test/bleu': 30.460744589687334, 'test/num_examples': 3003, 'score': 36149.81416296959, 'total_duration': 59098.26532077789, 'accumulated_submission_time': 36149.81416296959, 'accumulated_eval_time': 22943.753257989883, 'accumulated_logging_time': 1.4315705299377441, 'global_step': 103458, 'preemption_count': 0}), (105864, {'train/accuracy': 0.7032337188720703, 'train/loss': 1.5497827529907227, 'train/bleu': 36.129084059359286, 'validation/accuracy': 0.6897744536399841, 'validation/loss': 1.6095569133758545, 'validation/bleu': 30.705780856663885, 'validation/num_examples': 3000, 'test/accuracy': 0.707524299621582, 'test/loss': 1.5049904584884644, 'test/bleu': 30.838508552387886, 'test/num_examples': 3003, 'score': 36989.739221572876, 'total_duration': 60446.634615659714, 'accumulated_submission_time': 36989.739221572876, 'accumulated_eval_time': 23452.08066368103, 'accumulated_logging_time': 1.4725525379180908, 'global_step': 105864, 'preemption_count': 0}), (108269, {'train/accuracy': 0.7132049202919006, 'train/loss': 1.4947891235351562, 'train/bleu': 36.41010826418029, 'validation/accuracy': 0.6916218996047974, 'validation/loss': 1.6021170616149902, 'validation/bleu': 30.63931633701646, 'validation/num_examples': 3000, 'test/accuracy': 0.7091279029846191, 'test/loss': 1.499886393547058, 'test/bleu': 30.73570104529433, 'test/num_examples': 3003, 'score': 37829.69793653488, 'total_duration': 61776.03334188461, 'accumulated_submission_time': 37829.69793653488, 'accumulated_eval_time': 23941.40462422371, 'accumulated_logging_time': 1.5124402046203613, 'global_step': 108269, 'preemption_count': 0}), (110674, {'train/accuracy': 0.7082228064537048, 'train/loss': 1.514228343963623, 'train/bleu': 36.62626379524225, 'validation/accuracy': 0.6904811859130859, 'validation/loss': 1.604583740234375, 'validation/bleu': 30.574834332474047, 'validation/num_examples': 3000, 'test/accuracy': 0.7097786664962769, 'test/loss': 1.4970381259918213, 'test/bleu': 30.640785565828086, 'test/num_examples': 3003, 'score': 38669.68802213669, 'total_duration': 63077.23090171814, 'accumulated_submission_time': 38669.68802213669, 'accumulated_eval_time': 24402.493275880814, 'accumulated_logging_time': 1.5521199703216553, 'global_step': 110674, 'preemption_count': 0}), (113080, {'train/accuracy': 0.7224063277244568, 'train/loss': 1.443524718284607, 'train/bleu': 37.28813238914175, 'validation/accuracy': 0.6914855241775513, 'validation/loss': 1.6032272577285767, 'validation/bleu': 30.60543726425647, 'validation/num_examples': 3000, 'test/accuracy': 0.7103829383850098, 'test/loss': 1.4962009191513062, 'test/bleu': 30.95519761628077, 'test/num_examples': 3003, 'score': 39509.64947485924, 'total_duration': 64382.861696243286, 'accumulated_submission_time': 39509.64947485924, 'accumulated_eval_time': 24868.045795202255, 'accumulated_logging_time': 1.5935924053192139, 'global_step': 113080, 'preemption_count': 0}), (115487, {'train/accuracy': 0.713723361492157, 'train/loss': 1.487293004989624, 'train/bleu': 36.435484973088045, 'validation/accuracy': 0.6912871599197388, 'validation/loss': 1.6010518074035645, 'validation/bleu': 30.65375561094501, 'validation/num_examples': 3000, 'test/accuracy': 0.7099064588546753, 'test/loss': 1.4967427253723145, 'test/bleu': 30.68944762531302, 'test/num_examples': 3003, 'score': 40349.80061197281, 'total_duration': 65698.97678422928, 'accumulated_submission_time': 40349.80061197281, 'accumulated_eval_time': 25343.893981456757, 'accumulated_logging_time': 1.635310173034668, 'global_step': 115487, 'preemption_count': 0}), (117894, {'train/accuracy': 0.7144909501075745, 'train/loss': 1.4906255006790161, 'train/bleu': 37.108529161669374, 'validation/accuracy': 0.6910887360572815, 'validation/loss': 1.6030627489089966, 'validation/bleu': 30.71050729467754, 'validation/num_examples': 3000, 'test/accuracy': 0.7090465426445007, 'test/loss': 1.4972790479660034, 'test/bleu': 30.936780229896527, 'test/num_examples': 3003, 'score': 41189.930918216705, 'total_duration': 66997.73618650436, 'accumulated_submission_time': 41189.930918216705, 'accumulated_eval_time': 25802.397783994675, 'accumulated_logging_time': 1.6849846839904785, 'global_step': 117894, 'preemption_count': 0}), (120300, {'train/accuracy': 0.7231228351593018, 'train/loss': 1.442205548286438, 'train/bleu': 37.805040018899184, 'validation/accuracy': 0.6926014423370361, 'validation/loss': 1.6032183170318604, 'validation/bleu': 30.683219104917654, 'validation/num_examples': 3000, 'test/accuracy': 0.7098715901374817, 'test/loss': 1.4950560331344604, 'test/bleu': 30.98069747621216, 'test/num_examples': 3003, 'score': 42030.07278776169, 'total_duration': 68307.31502318382, 'accumulated_submission_time': 42030.07278776169, 'accumulated_eval_time': 26271.71107816696, 'accumulated_logging_time': 1.7338509559631348, 'global_step': 120300, 'preemption_count': 0}), (122710, {'train/accuracy': 0.7189242243766785, 'train/loss': 1.4655243158340454, 'train/bleu': 37.58180576067513, 'validation/accuracy': 0.6922046542167664, 'validation/loss': 1.6004964113235474, 'validation/bleu': 30.80034796220132, 'validation/num_examples': 3000, 'test/accuracy': 0.7106037139892578, 'test/loss': 1.4928041696548462, 'test/bleu': 30.8694776006975, 'test/num_examples': 3003, 'score': 42870.128358602524, 'total_duration': 69614.16217589378, 'accumulated_submission_time': 42870.128358602524, 'accumulated_eval_time': 26738.382657289505, 'accumulated_logging_time': 1.7781829833984375, 'global_step': 122710, 'preemption_count': 0}), (125116, {'train/accuracy': 0.7213461399078369, 'train/loss': 1.4562888145446777, 'train/bleu': 37.47862548500293, 'validation/accuracy': 0.6918947100639343, 'validation/loss': 1.6007949113845825, 'validation/bleu': 30.825696629345693, 'validation/num_examples': 3000, 'test/accuracy': 0.7102667093276978, 'test/loss': 1.4937689304351807, 'test/bleu': 30.91072129641423, 'test/num_examples': 3003, 'score': 43710.050102472305, 'total_duration': 70918.60228586197, 'accumulated_submission_time': 43710.050102472305, 'accumulated_eval_time': 27202.784227132797, 'accumulated_logging_time': 1.8205416202545166, 'global_step': 125116, 'preemption_count': 0}), (127522, {'train/accuracy': 0.7226781845092773, 'train/loss': 1.445623517036438, 'train/bleu': 37.238805367993805, 'validation/accuracy': 0.6923658847808838, 'validation/loss': 1.6010756492614746, 'validation/bleu': 30.862977758300378, 'validation/num_examples': 3000, 'test/accuracy': 0.710719883441925, 'test/loss': 1.493880033493042, 'test/bleu': 30.998903514021368, 'test/num_examples': 3003, 'score': 44550.06398534775, 'total_duration': 72225.58293819427, 'accumulated_submission_time': 44550.06398534775, 'accumulated_eval_time': 27669.634046316147, 'accumulated_logging_time': 1.8627715110778809, 'global_step': 127522, 'preemption_count': 0}), (129928, {'train/accuracy': 0.7224118113517761, 'train/loss': 1.4469929933547974, 'train/bleu': 37.6780744346327, 'validation/accuracy': 0.6922914981842041, 'validation/loss': 1.6008161306381226, 'validation/bleu': 30.877284823212637, 'validation/num_examples': 3000, 'test/accuracy': 0.7106850147247314, 'test/loss': 1.4928480386734009, 'test/bleu': 30.92520062066843, 'test/num_examples': 3003, 'score': 45389.959735155106, 'total_duration': 73539.53221654892, 'accumulated_submission_time': 45389.959735155106, 'accumulated_eval_time': 28143.56909942627, 'accumulated_logging_time': 1.9056484699249268, 'global_step': 129928, 'preemption_count': 0}), (132335, {'train/accuracy': 0.7235233783721924, 'train/loss': 1.4436975717544556, 'train/bleu': 37.51305828525132, 'validation/accuracy': 0.6922170519828796, 'validation/loss': 1.6004061698913574, 'validation/bleu': 30.95629465203926, 'validation/num_examples': 3000, 'test/accuracy': 0.7106037139892578, 'test/loss': 1.492241621017456, 'test/bleu': 30.889636565732552, 'test/num_examples': 3003, 'score': 46230.10717344284, 'total_duration': 74849.48199796677, 'accumulated_submission_time': 46230.10717344284, 'accumulated_eval_time': 28613.24405527115, 'accumulated_logging_time': 1.9580817222595215, 'global_step': 132335, 'preemption_count': 0}), (133333, {'train/accuracy': 0.721368134021759, 'train/loss': 1.4508073329925537, 'train/bleu': 37.40015536663024, 'validation/accuracy': 0.6922046542167664, 'validation/loss': 1.6004267930984497, 'validation/bleu': 30.959032282956052, 'validation/num_examples': 3000, 'test/accuracy': 0.7105804681777954, 'test/loss': 1.4921976327896118, 'test/bleu': 30.862555894325826, 'test/num_examples': 3003, 'score': 46578.47412419319, 'total_duration': 75670.26692295074, 'accumulated_submission_time': 46578.47412419319, 'accumulated_eval_time': 29085.583038330078, 'accumulated_logging_time': 2.0043785572052, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0208 04:53:52.465747 139785736898368 submission_runner.py:586] Timing: 46578.47412419319
I0208 04:53:52.465798 139785736898368 submission_runner.py:588] Total number of evals: 57
I0208 04:53:52.465838 139785736898368 submission_runner.py:589] ====================
I0208 04:53:52.465880 139785736898368 submission_runner.py:542] Using RNG seed 3586669017
I0208 04:53:52.467486 139785736898368 submission_runner.py:551] --- Tuning run 3/5 ---
I0208 04:53:52.467663 139785736898368 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3.
I0208 04:53:52.468112 139785736898368 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3/hparams.json.
I0208 04:53:52.468944 139785736898368 submission_runner.py:206] Initializing dataset.
I0208 04:53:52.471656 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 04:53:52.474827 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0208 04:53:52.513761 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 04:53:53.149971 139785736898368 submission_runner.py:213] Initializing model.
I0208 04:53:59.927799 139785736898368 submission_runner.py:255] Initializing optimizer.
I0208 04:54:00.747754 139785736898368 submission_runner.py:262] Initializing metrics bundle.
I0208 04:54:00.747919 139785736898368 submission_runner.py:280] Initializing checkpoint and logger.
I0208 04:54:00.748751 139785736898368 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3 with prefix checkpoint_
I0208 04:54:00.748874 139785736898368 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3/meta_data_0.json.
I0208 04:54:00.749105 139785736898368 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0208 04:54:00.749171 139785736898368 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0208 04:54:01.235348 139785736898368 logger_utils.py:220] Unable to record git information. Continuing without it.
I0208 04:54:01.698128 139785736898368 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3/flags_0.json.
I0208 04:54:01.701780 139785736898368 submission_runner.py:314] Starting training loop.
I0208 04:54:27.509462 139615954999040 logging_writer.py:48] [0] global_step=0, grad_norm=5.592713832855225, loss=11.052679061889648
I0208 04:54:27.520858 139785736898368 spec.py:321] Evaluating on the training split.
I0208 04:54:30.208422 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 04:59:17.241509 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 04:59:19.963151 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:04:06.946117 139785736898368 spec.py:349] Evaluating on the test split.
I0208 05:04:09.683723 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:08:58.329147 139785736898368 submission_runner.py:408] Time since start: 896.63s, 	Step: 1, 	{'train/accuracy': 0.000595776888076216, 'train/loss': 11.064934730529785, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.81904888153076, 'total_duration': 896.6272666454315, 'accumulated_submission_time': 25.81904888153076, 'accumulated_eval_time': 870.8081820011139, 'accumulated_logging_time': 0}
I0208 05:08:58.340233 139615963391744 logging_writer.py:48] [1] accumulated_eval_time=870.808182, accumulated_logging_time=0, accumulated_submission_time=25.819049, global_step=1, preemption_count=0, score=25.819049, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.041826, test/num_examples=3003, total_duration=896.627267, train/accuracy=0.000596, train/bleu=0.000000, train/loss=11.064935, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.036646, validation/num_examples=3000
I0208 05:09:33.336560 139615954999040 logging_writer.py:48] [100] global_step=100, grad_norm=0.46818679571151733, loss=8.697461128234863
I0208 05:10:08.298724 139615963391744 logging_writer.py:48] [200] global_step=200, grad_norm=0.18712346255779266, loss=8.311151504516602
I0208 05:10:43.283457 139615954999040 logging_writer.py:48] [300] global_step=300, grad_norm=0.2074119746685028, loss=8.086751937866211
I0208 05:11:18.276047 139615963391744 logging_writer.py:48] [400] global_step=400, grad_norm=0.2908298075199127, loss=7.601255416870117
I0208 05:11:53.227149 139615954999040 logging_writer.py:48] [500] global_step=500, grad_norm=0.3411262035369873, loss=7.267909049987793
I0208 05:12:28.196772 139615963391744 logging_writer.py:48] [600] global_step=600, grad_norm=0.6625121831893921, loss=7.015126705169678
I0208 05:13:03.174745 139615954999040 logging_writer.py:48] [700] global_step=700, grad_norm=0.5365655422210693, loss=6.734341144561768
I0208 05:13:38.138211 139615963391744 logging_writer.py:48] [800] global_step=800, grad_norm=1.0281254053115845, loss=6.405247688293457
I0208 05:14:13.107756 139615954999040 logging_writer.py:48] [900] global_step=900, grad_norm=0.8255210518836975, loss=6.227250099182129
I0208 05:14:48.099185 139615963391744 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.613947331905365, loss=6.0266923904418945
I0208 05:15:23.083894 139615954999040 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6189681887626648, loss=5.79749059677124
I0208 05:15:58.053947 139615963391744 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0285046100616455, loss=5.610708713531494
I0208 05:16:33.042208 139615954999040 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.5139392614364624, loss=5.487386226654053
I0208 05:17:08.031215 139615963391744 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7474997639656067, loss=5.183102607727051
I0208 05:17:43.024545 139615954999040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8098886013031006, loss=5.076828956604004
I0208 05:18:18.025434 139615963391744 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8217278718948364, loss=4.958217144012451
I0208 05:18:52.996958 139615954999040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9257667064666748, loss=4.82428503036499
I0208 05:19:27.984602 139615963391744 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8851213455200195, loss=4.658599853515625
I0208 05:20:02.986701 139615954999040 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.446067214012146, loss=4.537846565246582
I0208 05:20:37.967996 139615963391744 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8387926816940308, loss=4.4199628829956055
I0208 05:21:12.996497 139615954999040 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.029552698135376, loss=4.275583744049072
I0208 05:21:47.989307 139615963391744 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0105150938034058, loss=4.210229873657227
I0208 05:22:22.993354 139615954999040 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9296191930770874, loss=4.093546390533447
I0208 05:22:57.993996 139615963391744 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7584256529808044, loss=3.8946235179901123
I0208 05:22:58.420539 139785736898368 spec.py:321] Evaluating on the training split.
I0208 05:23:01.436383 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:26:26.887617 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 05:26:29.600943 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:29:36.657914 139785736898368 spec.py:349] Evaluating on the test split.
I0208 05:29:39.375253 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:32:37.845091 139785736898368 submission_runner.py:408] Time since start: 2316.14s, 	Step: 2403, 	{'train/accuracy': 0.41331639885902405, 'train/loss': 3.916454315185547, 'train/bleu': 14.633949505522779, 'validation/accuracy': 0.40114814043045044, 'validation/loss': 4.038556098937988, 'validation/bleu': 9.957431113516414, 'validation/num_examples': 3000, 'test/accuracy': 0.38661321997642517, 'test/loss': 4.231210708618164, 'test/bleu': 8.388442673207205, 'test/num_examples': 3003, 'score': 865.8087675571442, 'total_duration': 2316.1432354450226, 'accumulated_submission_time': 865.8087675571442, 'accumulated_eval_time': 1450.2326774597168, 'accumulated_logging_time': 0.023147106170654297}
I0208 05:32:37.860916 139615954999040 logging_writer.py:48] [2403] accumulated_eval_time=1450.232677, accumulated_logging_time=0.023147, accumulated_submission_time=865.808768, global_step=2403, preemption_count=0, score=865.808768, test/accuracy=0.386613, test/bleu=8.388443, test/loss=4.231211, test/num_examples=3003, total_duration=2316.143235, train/accuracy=0.413316, train/bleu=14.633950, train/loss=3.916454, validation/accuracy=0.401148, validation/bleu=9.957431, validation/loss=4.038556, validation/num_examples=3000
I0208 05:33:12.147484 139615963391744 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1828837394714355, loss=3.7850265502929688
I0208 05:33:47.185742 139615954999040 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9653971791267395, loss=3.785151481628418
I0208 05:34:22.246238 139615963391744 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9185186624526978, loss=3.663008213043213
I0208 05:34:57.293173 139615954999040 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8346633911132812, loss=3.5733559131622314
I0208 05:35:32.291714 139615963391744 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.962742805480957, loss=3.5654683113098145
I0208 05:36:07.286584 139615954999040 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2038975954055786, loss=3.455463409423828
I0208 05:36:42.296056 139615963391744 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0925718545913696, loss=3.253842830657959
I0208 05:37:17.291131 139615954999040 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7789166569709778, loss=3.351926326751709
I0208 05:37:52.262367 139615963391744 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8416723012924194, loss=3.26021146774292
I0208 05:38:27.284978 139615954999040 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.826793372631073, loss=3.1497435569763184
I0208 05:39:02.306314 139615963391744 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.796826958656311, loss=3.0963447093963623
I0208 05:39:37.328507 139615954999040 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7099209427833557, loss=3.2270143032073975
I0208 05:40:12.342779 139615963391744 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7600441575050354, loss=3.018070936203003
I0208 05:40:47.318102 139615954999040 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9095374345779419, loss=3.0068790912628174
I0208 05:41:22.325086 139615963391744 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7197831273078918, loss=2.9263241291046143
I0208 05:41:57.302618 139615954999040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9771157503128052, loss=3.0014097690582275
I0208 05:42:32.278910 139615963391744 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7758073210716248, loss=2.7327747344970703
I0208 05:43:07.255766 139615954999040 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.0361369848251343, loss=2.8563520908355713
I0208 05:43:42.204561 139615963391744 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7312067151069641, loss=2.9084134101867676
I0208 05:44:17.162704 139615954999040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6235975027084351, loss=2.777655601501465
I0208 05:44:52.158908 139615963391744 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6100671887397766, loss=2.7083077430725098
I0208 05:45:27.127154 139615954999040 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6857010722160339, loss=2.7222297191619873
I0208 05:46:02.115025 139615963391744 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6011422872543335, loss=2.639906406402588
I0208 05:46:37.079165 139615954999040 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6279148459434509, loss=2.7368931770324707
I0208 05:46:37.851297 139785736898368 spec.py:321] Evaluating on the training split.
I0208 05:46:40.854123 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:49:19.426577 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 05:49:22.140193 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:52:05.591711 139785736898368 spec.py:349] Evaluating on the test split.
I0208 05:52:08.307408 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 05:54:35.439769 139785736898368 submission_runner.py:408] Time since start: 3633.74s, 	Step: 4804, 	{'train/accuracy': 0.5378835201263428, 'train/loss': 2.7001044750213623, 'train/bleu': 24.378417805692475, 'validation/accuracy': 0.5419771671295166, 'validation/loss': 2.6425657272338867, 'validation/bleu': 20.41463598087644, 'validation/num_examples': 3000, 'test/accuracy': 0.543048083782196, 'test/loss': 2.6712963581085205, 'test/bleu': 18.875308714605914, 'test/num_examples': 3003, 'score': 1705.7114703655243, 'total_duration': 3633.7379219532013, 'accumulated_submission_time': 1705.7114703655243, 'accumulated_eval_time': 1927.821095943451, 'accumulated_logging_time': 0.049123287200927734}
I0208 05:54:35.454794 139615963391744 logging_writer.py:48] [4804] accumulated_eval_time=1927.821096, accumulated_logging_time=0.049123, accumulated_submission_time=1705.711470, global_step=4804, preemption_count=0, score=1705.711470, test/accuracy=0.543048, test/bleu=18.875309, test/loss=2.671296, test/num_examples=3003, total_duration=3633.737922, train/accuracy=0.537884, train/bleu=24.378418, train/loss=2.700104, validation/accuracy=0.541977, validation/bleu=20.414636, validation/loss=2.642566, validation/num_examples=3000
I0208 05:55:09.377508 139615954999040 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7012808918952942, loss=2.6127758026123047
I0208 05:55:44.349004 139615963391744 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6077699661254883, loss=2.5820624828338623
I0208 05:56:19.317279 139615954999040 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6481693387031555, loss=2.6604483127593994
I0208 05:56:54.287028 139615963391744 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.662217378616333, loss=2.59485125541687
I0208 05:57:29.280297 139615954999040 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5674034953117371, loss=2.594111442565918
I0208 05:58:04.291167 139615963391744 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5617917776107788, loss=2.5683701038360596
I0208 05:58:39.277410 139615954999040 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6534299850463867, loss=2.4806337356567383
I0208 05:59:14.264366 139615963391744 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6340234279632568, loss=2.5058021545410156
I0208 05:59:49.283112 139615954999040 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7897588014602661, loss=2.573136329650879
I0208 06:00:24.263690 139615963391744 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7426127791404724, loss=2.5169553756713867
I0208 06:00:59.212525 139615954999040 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6952587962150574, loss=2.530850648880005
I0208 06:01:34.175219 139615963391744 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5335245728492737, loss=2.4805169105529785
I0208 06:02:09.146894 139615954999040 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.695978045463562, loss=2.4298393726348877
I0208 06:02:44.130019 139615963391744 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6550031900405884, loss=2.4585273265838623
I0208 06:03:19.099192 139615954999040 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.6755177974700928, loss=2.3830435276031494
I0208 06:03:54.057659 139615963391744 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5734745860099792, loss=2.4704227447509766
I0208 06:04:29.006596 139615954999040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4916764497756958, loss=2.4335832595825195
I0208 06:05:03.988509 139615963391744 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5382071137428284, loss=2.4270949363708496
I0208 06:05:39.090092 139615954999040 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5506985187530518, loss=2.3694000244140625
I0208 06:06:14.078033 139615963391744 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5553699135780334, loss=2.364955425262451
I0208 06:06:49.024345 139615954999040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6243810653686523, loss=2.385627031326294
I0208 06:07:23.981229 139615963391744 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.47748976945877075, loss=2.3449442386627197
I0208 06:07:58.917455 139615954999040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6143442392349243, loss=2.329169988632202
I0208 06:08:33.873800 139615963391744 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.48428502678871155, loss=2.4006285667419434
I0208 06:08:35.697952 139785736898368 spec.py:321] Evaluating on the training split.
I0208 06:08:38.699916 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:11:18.793036 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 06:11:21.502213 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:13:56.646836 139785736898368 spec.py:349] Evaluating on the test split.
I0208 06:13:59.364911 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:16:32.450803 139785736898368 submission_runner.py:408] Time since start: 4950.75s, 	Step: 7207, 	{'train/accuracy': 0.580255389213562, 'train/loss': 2.2710001468658447, 'train/bleu': 27.101228921682285, 'validation/accuracy': 0.5857831835746765, 'validation/loss': 2.226724624633789, 'validation/bleu': 23.166204664087257, 'validation/num_examples': 3000, 'test/accuracy': 0.5880890488624573, 'test/loss': 2.2239651679992676, 'test/bleu': 22.14029524164679, 'test/num_examples': 3003, 'score': 2545.865830183029, 'total_duration': 4950.748953819275, 'accumulated_submission_time': 2545.865830183029, 'accumulated_eval_time': 2404.573896408081, 'accumulated_logging_time': 0.07438302040100098}
I0208 06:16:32.466463 139615954999040 logging_writer.py:48] [7207] accumulated_eval_time=2404.573896, accumulated_logging_time=0.074383, accumulated_submission_time=2545.865830, global_step=7207, preemption_count=0, score=2545.865830, test/accuracy=0.588089, test/bleu=22.140295, test/loss=2.223965, test/num_examples=3003, total_duration=4950.748954, train/accuracy=0.580255, train/bleu=27.101229, train/loss=2.271000, validation/accuracy=0.585783, validation/bleu=23.166205, validation/loss=2.226725, validation/num_examples=3000
I0208 06:17:05.318094 139615963391744 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4711456000804901, loss=2.2699315547943115
I0208 06:17:40.278106 139615954999040 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5212294459342957, loss=2.3093605041503906
I0208 06:18:15.229220 139615963391744 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.45637473464012146, loss=2.2322285175323486
I0208 06:18:50.174650 139615954999040 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5067203640937805, loss=2.3793282508850098
I0208 06:19:25.119165 139615963391744 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.476444810628891, loss=2.1862571239471436
I0208 06:20:00.108970 139615954999040 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5018724799156189, loss=2.1767404079437256
I0208 06:20:35.084790 139615963391744 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.47311508655548096, loss=2.206144332885742
I0208 06:21:10.067139 139615954999040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4492985010147095, loss=2.3729562759399414
I0208 06:21:45.068271 139615963391744 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.49520277976989746, loss=2.187257766723633
I0208 06:22:20.019790 139615954999040 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4463452100753784, loss=2.2611308097839355
I0208 06:22:54.974832 139615963391744 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.45652878284454346, loss=2.232825994491577
I0208 06:23:29.928637 139615954999040 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.44349759817123413, loss=2.204235553741455
I0208 06:24:04.872476 139615963391744 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.42221391201019287, loss=2.2914106845855713
I0208 06:24:39.823678 139615954999040 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3861559331417084, loss=2.2125158309936523
I0208 06:25:14.764243 139615963391744 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4244108498096466, loss=2.2129526138305664
I0208 06:25:49.736906 139615954999040 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4105914831161499, loss=2.283236503601074
I0208 06:26:24.689836 139615963391744 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.36835888028144836, loss=2.161247491836548
I0208 06:26:59.612913 139615954999040 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3605785667896271, loss=2.1592628955841064
I0208 06:27:34.541817 139615963391744 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.37699830532073975, loss=2.226762533187866
I0208 06:28:09.528427 139615954999040 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4221794605255127, loss=2.296227216720581
I0208 06:28:44.449796 139615963391744 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3536243736743927, loss=2.104729413986206
I0208 06:29:19.373227 139615954999040 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4103417694568634, loss=2.243635892868042
I0208 06:29:54.350212 139615963391744 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.35773810744285583, loss=2.1864404678344727
I0208 06:30:29.279699 139615954999040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3744964003562927, loss=2.144099712371826
I0208 06:30:32.502450 139785736898368 spec.py:321] Evaluating on the training split.
I0208 06:30:35.508337 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:33:09.424703 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 06:33:12.145664 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:35:35.561446 139785736898368 spec.py:349] Evaluating on the test split.
I0208 06:35:38.291613 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:37:50.706083 139785736898368 submission_runner.py:408] Time since start: 6229.00s, 	Step: 9611, 	{'train/accuracy': 0.592166006565094, 'train/loss': 2.1426703929901123, 'train/bleu': 28.45783172233359, 'validation/accuracy': 0.606316089630127, 'validation/loss': 2.0390303134918213, 'validation/bleu': 24.673418436166767, 'validation/num_examples': 3000, 'test/accuracy': 0.6116321086883545, 'test/loss': 2.003997325897217, 'test/bleu': 23.503151632503048, 'test/num_examples': 3003, 'score': 3385.8169617652893, 'total_duration': 6229.004224777222, 'accumulated_submission_time': 3385.8169617652893, 'accumulated_eval_time': 2842.7774851322174, 'accumulated_logging_time': 0.10021090507507324}
I0208 06:37:50.721665 139615963391744 logging_writer.py:48] [9611] accumulated_eval_time=2842.777485, accumulated_logging_time=0.100211, accumulated_submission_time=3385.816962, global_step=9611, preemption_count=0, score=3385.816962, test/accuracy=0.611632, test/bleu=23.503152, test/loss=2.003997, test/num_examples=3003, total_duration=6229.004225, train/accuracy=0.592166, train/bleu=28.457832, train/loss=2.142670, validation/accuracy=0.606316, validation/bleu=24.673418, validation/loss=2.039030, validation/num_examples=3000
I0208 06:38:22.164238 139615954999040 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3867478668689728, loss=2.1063694953918457
I0208 06:38:57.199861 139615963391744 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.32187631726264954, loss=2.178074598312378
I0208 06:39:32.197179 139615954999040 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.343192458152771, loss=2.1008729934692383
I0208 06:40:07.153949 139615963391744 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.33959710597991943, loss=2.045860767364502
I0208 06:40:42.077155 139615954999040 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.29706910252571106, loss=2.0983588695526123
I0208 06:41:17.022982 139615963391744 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3518908619880676, loss=2.1584067344665527
I0208 06:41:51.964270 139615954999040 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.39690470695495605, loss=2.0232956409454346
I0208 06:42:26.916877 139615963391744 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3312680721282959, loss=2.031402587890625
I0208 06:43:01.895202 139615954999040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.30649369955062866, loss=2.142927885055542
I0208 06:43:36.874726 139615963391744 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.325594037771225, loss=2.0767414569854736
I0208 06:44:11.822729 139615954999040 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3140237331390381, loss=2.1355135440826416
I0208 06:44:46.861617 139615963391744 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3058915138244629, loss=2.118008852005005
I0208 06:45:21.835289 139615954999040 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.3022349774837494, loss=2.0735785961151123
I0208 06:45:56.784071 139615963391744 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3185493052005768, loss=2.162059783935547
I0208 06:46:31.727489 139615954999040 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3012218773365021, loss=2.067060947418213
I0208 06:47:06.685098 139615963391744 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.32274606823921204, loss=2.0991933345794678
I0208 06:47:41.630364 139615954999040 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3170115053653717, loss=2.148691415786743
I0208 06:48:16.571509 139615963391744 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.30108609795570374, loss=2.007143974304199
I0208 06:48:51.553647 139615954999040 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.294795960187912, loss=2.160313129425049
I0208 06:49:26.443401 139615963391744 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.33048680424690247, loss=2.01485013961792
I0208 06:50:01.386867 139615954999040 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.291412889957428, loss=2.064497232437134
I0208 06:50:36.300658 139615963391744 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.29909321665763855, loss=2.0487749576568604
I0208 06:51:11.422025 139615954999040 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.28286051750183105, loss=2.0746610164642334
I0208 06:51:46.348354 139615963391744 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.31855618953704834, loss=2.1083853244781494
I0208 06:51:50.964891 139785736898368 spec.py:321] Evaluating on the training split.
I0208 06:51:53.969730 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:55:42.381910 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 06:55:45.117083 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 06:59:18.734635 139785736898368 spec.py:349] Evaluating on the test split.
I0208 06:59:21.443965 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:02:03.034977 139785736898368 submission_runner.py:408] Time since start: 7681.33s, 	Step: 12015, 	{'train/accuracy': 0.6005766987800598, 'train/loss': 2.0656661987304688, 'train/bleu': 28.810404280423196, 'validation/accuracy': 0.6186903715133667, 'validation/loss': 1.9162800312042236, 'validation/bleu': 25.631044209587834, 'validation/num_examples': 3000, 'test/accuracy': 0.6232293248176575, 'test/loss': 1.8754557371139526, 'test/bleu': 24.426011741968633, 'test/num_examples': 3003, 'score': 4225.967286348343, 'total_duration': 7681.333123445511, 'accumulated_submission_time': 4225.967286348343, 'accumulated_eval_time': 3454.847516775131, 'accumulated_logging_time': 0.12733793258666992}
I0208 07:02:03.052402 139615954999040 logging_writer.py:48] [12015] accumulated_eval_time=3454.847517, accumulated_logging_time=0.127338, accumulated_submission_time=4225.967286, global_step=12015, preemption_count=0, score=4225.967286, test/accuracy=0.623229, test/bleu=24.426012, test/loss=1.875456, test/num_examples=3003, total_duration=7681.333123, train/accuracy=0.600577, train/bleu=28.810404, train/loss=2.065666, validation/accuracy=0.618690, validation/bleu=25.631044, validation/loss=1.916280, validation/num_examples=3000
I0208 07:02:33.047153 139615963391744 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.28101876378059387, loss=2.0261433124542236
I0208 07:03:07.951277 139615954999040 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3030158281326294, loss=2.033987045288086
I0208 07:03:42.875436 139615963391744 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.27047669887542725, loss=2.0841903686523438
I0208 07:04:17.776301 139615954999040 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2512841820716858, loss=1.973501205444336
I0208 07:04:52.661841 139615963391744 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3301401436328888, loss=2.0755975246429443
I0208 07:05:27.583232 139615954999040 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3455936908721924, loss=2.009761095046997
I0208 07:06:02.503239 139615963391744 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.33199265599250793, loss=2.010173797607422
I0208 07:06:37.402268 139615954999040 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.28427854180336, loss=2.143531560897827
I0208 07:07:12.324443 139615963391744 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.26898327469825745, loss=1.9314380884170532
I0208 07:07:47.237283 139615954999040 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2985672652721405, loss=2.0331528186798096
I0208 07:08:22.195614 139615963391744 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3403249680995941, loss=2.094724655151367
I0208 07:08:57.097913 139615954999040 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2920345067977905, loss=2.0711166858673096
I0208 07:09:32.051145 139615963391744 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.27446258068084717, loss=2.0580554008483887
I0208 07:10:07.084707 139615954999040 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.32284143567085266, loss=2.032742500305176
I0208 07:10:42.032457 139615963391744 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2730274200439453, loss=2.084717273712158
I0208 07:11:16.936723 139615954999040 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2691987156867981, loss=1.9754105806350708
I0208 07:11:51.858127 139615963391744 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.30579766631126404, loss=2.0126240253448486
I0208 07:12:26.833273 139615954999040 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.25018173456192017, loss=1.959903359413147
I0208 07:13:01.798377 139615963391744 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2726137042045593, loss=1.9638745784759521
I0208 07:13:36.730222 139615954999040 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2945840656757355, loss=2.0022928714752197
I0208 07:14:11.728426 139615963391744 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3118595778942108, loss=2.0188920497894287
I0208 07:14:46.717764 139615954999040 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.26932209730148315, loss=1.9155158996582031
I0208 07:15:21.653902 139615963391744 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.28756555914878845, loss=1.9401166439056396
I0208 07:15:56.578623 139615954999040 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3156663775444031, loss=1.956183910369873
I0208 07:16:03.286128 139785736898368 spec.py:321] Evaluating on the training split.
I0208 07:16:06.290126 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:18:41.955195 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 07:18:44.674979 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:21:07.371216 139785736898368 spec.py:349] Evaluating on the test split.
I0208 07:21:10.099743 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:23:20.714034 139785736898368 submission_runner.py:408] Time since start: 8959.01s, 	Step: 14421, 	{'train/accuracy': 0.6168133616447449, 'train/loss': 1.9349526166915894, 'train/bleu': 29.425459383473545, 'validation/accuracy': 0.630469560623169, 'validation/loss': 1.8252501487731934, 'validation/bleu': 26.62938777326784, 'validation/num_examples': 3000, 'test/accuracy': 0.6382430195808411, 'test/loss': 1.775894045829773, 'test/bleu': 25.50415595748084, 'test/num_examples': 3003, 'score': 5066.110929250717, 'total_duration': 8959.01217341423, 'accumulated_submission_time': 5066.110929250717, 'accumulated_eval_time': 3892.2753579616547, 'accumulated_logging_time': 0.1567516326904297}
I0208 07:23:20.731257 139615963391744 logging_writer.py:48] [14421] accumulated_eval_time=3892.275358, accumulated_logging_time=0.156752, accumulated_submission_time=5066.110929, global_step=14421, preemption_count=0, score=5066.110929, test/accuracy=0.638243, test/bleu=25.504156, test/loss=1.775894, test/num_examples=3003, total_duration=8959.012173, train/accuracy=0.616813, train/bleu=29.425459, train/loss=1.934953, validation/accuracy=0.630470, validation/bleu=26.629388, validation/loss=1.825250, validation/num_examples=3000
I0208 07:23:48.632161 139615954999040 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2915397584438324, loss=1.9467005729675293
I0208 07:24:23.568679 139615963391744 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2638953924179077, loss=2.030566453933716
I0208 07:24:58.521803 139615954999040 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.28107762336730957, loss=1.9379860162734985
I0208 07:25:33.449265 139615963391744 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.37920910120010376, loss=2.0086896419525146
I0208 07:26:08.362360 139615954999040 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3142278492450714, loss=1.970697045326233
I0208 07:26:43.245833 139615963391744 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2940196394920349, loss=1.9797343015670776
I0208 07:27:18.157760 139615954999040 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.31947842240333557, loss=1.9434890747070312
I0208 07:27:53.071342 139615963391744 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3379170894622803, loss=1.976668119430542
I0208 07:28:28.006780 139615954999040 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3076496720314026, loss=1.9781438112258911
I0208 07:29:02.918894 139615963391744 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3373255133628845, loss=1.9706547260284424
I0208 07:29:37.892554 139615954999040 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.2850465178489685, loss=1.9488202333450317
I0208 07:30:12.822733 139615963391744 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2721712291240692, loss=2.0258073806762695
I0208 07:30:47.740859 139615954999040 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.28672200441360474, loss=2.008356809616089
I0208 07:31:22.709424 139615963391744 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.30644580721855164, loss=1.8950356245040894
I0208 07:31:57.656674 139615954999040 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.29276034235954285, loss=1.9923228025436401
I0208 07:32:32.627765 139615963391744 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.4415878355503082, loss=1.8409417867660522
I0208 07:33:07.539341 139615954999040 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3036794066429138, loss=1.9753590822219849
I0208 07:33:42.461698 139615963391744 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2762105464935303, loss=1.8685460090637207
I0208 07:34:17.387197 139615954999040 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3113365173339844, loss=1.9533941745758057
I0208 07:34:52.351345 139615963391744 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.28820183873176575, loss=1.8725260496139526
I0208 07:35:27.313600 139615954999040 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.33226755261421204, loss=1.873664379119873
I0208 07:36:02.269465 139615963391744 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3205421566963196, loss=1.8965060710906982
I0208 07:36:37.188097 139615954999040 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3517542779445648, loss=1.862766146659851
I0208 07:37:12.072446 139615963391744 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.31447482109069824, loss=1.9258661270141602
I0208 07:37:20.878270 139785736898368 spec.py:321] Evaluating on the training split.
I0208 07:37:23.881852 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:40:02.865722 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 07:40:05.577749 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:42:44.573742 139785736898368 spec.py:349] Evaluating on the test split.
I0208 07:42:47.289579 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 07:45:06.979466 139785736898368 submission_runner.py:408] Time since start: 10265.28s, 	Step: 16827, 	{'train/accuracy': 0.6208845973014832, 'train/loss': 1.901543140411377, 'train/bleu': 29.833422011187903, 'validation/accuracy': 0.6382933855056763, 'validation/loss': 1.7594693899154663, 'validation/bleu': 26.58655602133679, 'validation/num_examples': 3000, 'test/accuracy': 0.6456103920936584, 'test/loss': 1.7116901874542236, 'test/bleu': 26.099517505013495, 'test/num_examples': 3003, 'score': 5906.1692090034485, 'total_duration': 10265.277593374252, 'accumulated_submission_time': 5906.1692090034485, 'accumulated_eval_time': 4358.376479148865, 'accumulated_logging_time': 0.18583035469055176}
I0208 07:45:06.996174 139615954999040 logging_writer.py:48] [16827] accumulated_eval_time=4358.376479, accumulated_logging_time=0.185830, accumulated_submission_time=5906.169209, global_step=16827, preemption_count=0, score=5906.169209, test/accuracy=0.645610, test/bleu=26.099518, test/loss=1.711690, test/num_examples=3003, total_duration=10265.277593, train/accuracy=0.620885, train/bleu=29.833422, train/loss=1.901543, validation/accuracy=0.638293, validation/bleu=26.586556, validation/loss=1.759469, validation/num_examples=3000
I0208 07:45:32.869767 139615963391744 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.30690082907676697, loss=1.9973822832107544
I0208 07:46:07.819972 139615954999040 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.29605403542518616, loss=2.025813102722168
I0208 07:46:42.788818 139615963391744 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.36491522192955017, loss=1.9666078090667725
I0208 07:47:17.707159 139615954999040 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3053320348262787, loss=1.9000625610351562
I0208 07:47:52.614947 139615963391744 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3376062214374542, loss=1.8502197265625
I0208 07:48:27.565858 139615954999040 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.37122204899787903, loss=1.947654128074646
I0208 07:49:02.462817 139615963391744 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.30493786931037903, loss=1.8606446981430054
I0208 07:49:37.330571 139615954999040 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.36716869473457336, loss=1.916114330291748
I0208 07:50:12.261815 139615963391744 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3857840299606323, loss=1.8982528448104858
I0208 07:50:47.143120 139615954999040 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3035712242126465, loss=1.9585599899291992
I0208 07:51:22.060611 139615963391744 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.30392390489578247, loss=1.8346890211105347
I0208 07:51:57.009863 139615954999040 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.978839635848999, loss=2.038318395614624
I0208 07:52:31.906258 139615963391744 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5362923741340637, loss=1.9721509218215942
I0208 07:53:06.869314 139615954999040 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.370281845331192, loss=1.941582202911377
I0208 07:53:41.808623 139615963391744 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.337926983833313, loss=1.9409801959991455
I0208 07:54:16.745939 139615954999040 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.30252933502197266, loss=1.8533917665481567
I0208 07:54:51.642313 139615963391744 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.29934215545654297, loss=1.9112281799316406
I0208 07:55:26.527669 139615954999040 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.291291207075119, loss=1.8570129871368408
I0208 07:56:01.414578 139615963391744 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.373908132314682, loss=1.8813514709472656
I0208 07:56:36.319740 139615954999040 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.32371535897254944, loss=1.7967115640640259
I0208 07:57:11.187703 139615963391744 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3043273091316223, loss=1.881843090057373
I0208 07:57:46.115059 139615954999040 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.33829906582832336, loss=1.7786016464233398
I0208 07:58:20.998761 139615963391744 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.37218284606933594, loss=1.8896068334579468
I0208 07:58:55.910326 139615954999040 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3109968900680542, loss=1.8831684589385986
I0208 07:59:07.150416 139785736898368 spec.py:321] Evaluating on the training split.
I0208 07:59:10.152610 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:03:09.496827 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 08:03:12.196680 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:06:12.063958 139785736898368 spec.py:349] Evaluating on the test split.
I0208 08:06:14.780447 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:08:57.127671 139785736898368 submission_runner.py:408] Time since start: 11695.43s, 	Step: 19234, 	{'train/accuracy': 0.6365357041358948, 'train/loss': 1.7768452167510986, 'train/bleu': 31.482361801789065, 'validation/accuracy': 0.6452617049217224, 'validation/loss': 1.7125599384307861, 'validation/bleu': 27.290980192520678, 'validation/num_examples': 3000, 'test/accuracy': 0.6555923819541931, 'test/loss': 1.65578031539917, 'test/bleu': 26.700905226358415, 'test/num_examples': 3003, 'score': 6746.2333035469055, 'total_duration': 11695.425820350647, 'accumulated_submission_time': 6746.2333035469055, 'accumulated_eval_time': 4948.353693962097, 'accumulated_logging_time': 0.21404194831848145}
I0208 08:08:57.145827 139615963391744 logging_writer.py:48] [19234] accumulated_eval_time=4948.353694, accumulated_logging_time=0.214042, accumulated_submission_time=6746.233304, global_step=19234, preemption_count=0, score=6746.233304, test/accuracy=0.655592, test/bleu=26.700905, test/loss=1.655780, test/num_examples=3003, total_duration=11695.425820, train/accuracy=0.636536, train/bleu=31.482362, train/loss=1.776845, validation/accuracy=0.645262, validation/bleu=27.290980, validation/loss=1.712560, validation/num_examples=3000
I0208 08:09:20.541595 139615954999040 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3565066456794739, loss=1.7755110263824463
I0208 08:09:55.453197 139615963391744 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.33004483580589294, loss=1.875056266784668
I0208 08:10:30.377959 139615954999040 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4040464460849762, loss=1.8740578889846802
I0208 08:11:05.278796 139615963391744 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3815835118293762, loss=1.8973186016082764
I0208 08:11:40.190137 139615954999040 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2994539439678192, loss=1.8823388814926147
I0208 08:12:15.112258 139615963391744 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3067369759082794, loss=1.932177186012268
I0208 08:12:50.063996 139615954999040 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.38971859216690063, loss=1.8517203330993652
I0208 08:13:24.984360 139615963391744 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3503567576408386, loss=1.8659405708312988
I0208 08:13:59.873898 139615954999040 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.386080265045166, loss=1.8392984867095947
I0208 08:14:34.782190 139615963391744 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3246469497680664, loss=1.8620507717132568
I0208 08:15:09.707956 139615954999040 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.33695870637893677, loss=1.8412587642669678
I0208 08:15:44.600991 139615963391744 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.37811946868896484, loss=1.973336935043335
I0208 08:16:19.514991 139615954999040 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.33345839381217957, loss=1.913591742515564
I0208 08:16:54.435203 139615963391744 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.32635247707366943, loss=1.9074267148971558
I0208 08:17:29.380532 139615954999040 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3563217520713806, loss=1.8706045150756836
I0208 08:18:04.275482 139615963391744 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.4934771656990051, loss=1.8450593948364258
I0208 08:18:39.223405 139615954999040 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.33351537585258484, loss=1.8107160329818726
I0208 08:19:14.172411 139615963391744 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.3497889041900635, loss=1.9332534074783325
I0208 08:19:49.081093 139615954999040 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4530203938484192, loss=1.8711553812026978
I0208 08:20:23.973479 139615963391744 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.37630927562713623, loss=1.884992241859436
I0208 08:20:58.883370 139615954999040 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.38358765840530396, loss=1.825822353363037
I0208 08:21:33.814399 139615963391744 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3439539968967438, loss=1.9646896123886108
I0208 08:22:08.721143 139615954999040 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3375184237957001, loss=1.8084923028945923
I0208 08:22:43.616511 139615963391744 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.35890188813209534, loss=1.8290563821792603
I0208 08:22:57.293438 139785736898368 spec.py:321] Evaluating on the training split.
I0208 08:23:00.290810 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:27:02.552156 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 08:27:05.270390 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:29:50.481033 139785736898368 spec.py:349] Evaluating on the test split.
I0208 08:29:53.216411 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:32:48.885058 139785736898368 submission_runner.py:408] Time since start: 13127.18s, 	Step: 21641, 	{'train/accuracy': 0.6268560886383057, 'train/loss': 1.8370939493179321, 'train/bleu': 30.100959194200843, 'validation/accuracy': 0.6466503739356995, 'validation/loss': 1.69412362575531, 'validation/bleu': 27.4922055487141, 'validation/num_examples': 3000, 'test/accuracy': 0.6589041948318481, 'test/loss': 1.6343481540679932, 'test/bleu': 26.76084053464521, 'test/num_examples': 3003, 'score': 7586.293897151947, 'total_duration': 13127.183210134506, 'accumulated_submission_time': 7586.293897151947, 'accumulated_eval_time': 5539.945268630981, 'accumulated_logging_time': 0.24245548248291016}
I0208 08:32:48.903103 139615954999040 logging_writer.py:48] [21641] accumulated_eval_time=5539.945269, accumulated_logging_time=0.242455, accumulated_submission_time=7586.293897, global_step=21641, preemption_count=0, score=7586.293897, test/accuracy=0.658904, test/bleu=26.760841, test/loss=1.634348, test/num_examples=3003, total_duration=13127.183210, train/accuracy=0.626856, train/bleu=30.100959, train/loss=1.837094, validation/accuracy=0.646650, validation/bleu=27.492206, validation/loss=1.694124, validation/num_examples=3000
I0208 08:33:09.869440 139615963391744 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.4760209321975708, loss=1.8226042985916138
I0208 08:33:44.765115 139615954999040 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.37331852316856384, loss=1.9144954681396484
I0208 08:34:19.640431 139615963391744 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3501206934452057, loss=1.8340234756469727
I0208 08:34:54.542654 139615954999040 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5078604221343994, loss=1.8497869968414307
I0208 08:35:29.426037 139615963391744 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.4860530197620392, loss=1.8944320678710938
I0208 08:36:04.351726 139615954999040 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3526921570301056, loss=1.850649118423462
I0208 08:36:39.298927 139615963391744 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.36656081676483154, loss=1.8289538621902466
I0208 08:37:14.270268 139615954999040 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.37116992473602295, loss=1.9374104738235474
I0208 08:37:49.136999 139615963391744 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3588317334651947, loss=1.881020188331604
I0208 08:38:24.036418 139615954999040 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3737732768058777, loss=1.8449991941452026
I0208 08:38:58.950668 139615963391744 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.40147316455841064, loss=1.9174094200134277
I0208 08:39:33.839031 139615954999040 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3662358522415161, loss=1.7186309099197388
I0208 08:40:08.745673 139615963391744 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3793206810951233, loss=1.8983781337738037
I0208 08:40:43.672438 139615954999040 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3812476694583893, loss=1.8757978677749634
I0208 08:41:18.585392 139615963391744 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3682214617729187, loss=1.8575021028518677
I0208 08:41:53.474717 139615954999040 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.40049782395362854, loss=1.8569037914276123
I0208 08:42:28.360732 139615963391744 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3492354452610016, loss=1.7909491062164307
I0208 08:43:03.307089 139615954999040 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.40621718764305115, loss=1.8588067293167114
I0208 08:43:38.218838 139615963391744 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.41152501106262207, loss=1.838563323020935
I0208 08:44:13.138495 139615954999040 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5045751929283142, loss=1.9093283414840698
I0208 08:44:48.016349 139615963391744 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.43644729256629944, loss=1.8382364511489868
I0208 08:45:22.895704 139615954999040 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.394442081451416, loss=1.907759189605713
I0208 08:45:57.775789 139615963391744 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.36873459815979004, loss=1.833830714225769
I0208 08:46:32.698753 139615954999040 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.44139328598976135, loss=1.825913429260254
I0208 08:46:49.162634 139785736898368 spec.py:321] Evaluating on the training split.
I0208 08:46:52.157504 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:49:39.336781 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 08:49:42.042765 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:52:35.528703 139785736898368 spec.py:349] Evaluating on the test split.
I0208 08:52:38.235313 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 08:55:13.969324 139785736898368 submission_runner.py:408] Time since start: 14472.27s, 	Step: 24049, 	{'train/accuracy': 0.6285372376441956, 'train/loss': 1.8340964317321777, 'train/bleu': 30.61643964662627, 'validation/accuracy': 0.6487582325935364, 'validation/loss': 1.6795746088027954, 'validation/bleu': 27.350335835971034, 'validation/num_examples': 3000, 'test/accuracy': 0.657300591468811, 'test/loss': 1.6291598081588745, 'test/bleu': 26.492055872645757, 'test/num_examples': 3003, 'score': 8426.46519112587, 'total_duration': 14472.267474412918, 'accumulated_submission_time': 8426.46519112587, 'accumulated_eval_time': 6044.751909255981, 'accumulated_logging_time': 0.2706022262573242}
I0208 08:55:13.987676 139615963391744 logging_writer.py:48] [24049] accumulated_eval_time=6044.751909, accumulated_logging_time=0.270602, accumulated_submission_time=8426.465191, global_step=24049, preemption_count=0, score=8426.465191, test/accuracy=0.657301, test/bleu=26.492056, test/loss=1.629160, test/num_examples=3003, total_duration=14472.267474, train/accuracy=0.628537, train/bleu=30.616440, train/loss=1.834096, validation/accuracy=0.648758, validation/bleu=27.350336, validation/loss=1.679575, validation/num_examples=3000
I0208 08:55:32.131619 139615954999040 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3984370529651642, loss=1.918469786643982
I0208 08:56:07.024658 139615963391744 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.39462655782699585, loss=1.8463315963745117
I0208 08:56:41.920452 139615954999040 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.38078248500823975, loss=1.9653939008712769
I0208 08:57:16.830367 139615963391744 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.43161728978157043, loss=1.8943825960159302
I0208 08:57:51.732749 139615954999040 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4063035845756531, loss=1.8381015062332153
I0208 08:58:26.628382 139615963391744 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4000685214996338, loss=1.8168139457702637
I0208 08:59:01.536890 139615954999040 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.4027757942676544, loss=1.7675480842590332
I0208 08:59:36.466106 139615963391744 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.42499586939811707, loss=1.8657135963439941
I0208 09:00:11.401596 139615954999040 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3976532518863678, loss=1.84522545337677
I0208 09:00:46.304053 139615963391744 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.4890851378440857, loss=1.8714839220046997
I0208 09:01:21.211995 139615954999040 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.4295737147331238, loss=1.8588016033172607
I0208 09:01:56.102749 139615963391744 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4681873619556427, loss=1.8306618928909302
I0208 09:02:30.996172 139615954999040 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4988773465156555, loss=1.9026622772216797
I0208 09:03:05.891182 139615963391744 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.38184329867362976, loss=1.8305919170379639
I0208 09:03:40.776365 139615954999040 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.38474392890930176, loss=1.7333717346191406
I0208 09:04:15.684273 139615963391744 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.37573081254959106, loss=1.8568471670150757
I0208 09:04:50.597519 139615954999040 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.35838666558265686, loss=1.8084259033203125
I0208 09:05:25.502028 139615963391744 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4469638466835022, loss=1.7345161437988281
I0208 09:06:00.418242 139615954999040 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.42638787627220154, loss=1.87452232837677
I0208 09:06:35.325121 139615963391744 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.48120418190956116, loss=1.6907576322555542
I0208 09:07:10.259393 139615954999040 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3668932318687439, loss=1.887351155281067
I0208 09:07:45.160841 139615963391744 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.38518884778022766, loss=1.8823318481445312
I0208 09:08:20.064875 139615954999040 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.39889076352119446, loss=1.8201277256011963
I0208 09:08:54.965648 139615963391744 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.35600054264068604, loss=1.943145751953125
I0208 09:09:14.232533 139785736898368 spec.py:321] Evaluating on the training split.
I0208 09:09:17.233123 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:13:06.585687 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 09:13:09.299308 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:15:40.711257 139785736898368 spec.py:349] Evaluating on the test split.
I0208 09:15:43.431702 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:18:23.702617 139785736898368 submission_runner.py:408] Time since start: 15862.00s, 	Step: 26457, 	{'train/accuracy': 0.6383501887321472, 'train/loss': 1.7676606178283691, 'train/bleu': 31.067359641378342, 'validation/accuracy': 0.6521803736686707, 'validation/loss': 1.6639560461044312, 'validation/bleu': 27.703317488145554, 'validation/num_examples': 3000, 'test/accuracy': 0.6611701846122742, 'test/loss': 1.60666024684906, 'test/bleu': 27.172473220455522, 'test/num_examples': 3003, 'score': 9266.6213285923, 'total_duration': 15862.000746011734, 'accumulated_submission_time': 9266.6213285923, 'accumulated_eval_time': 6594.221925020218, 'accumulated_logging_time': 0.29915809631347656}
I0208 09:18:23.720924 139615954999040 logging_writer.py:48] [26457] accumulated_eval_time=6594.221925, accumulated_logging_time=0.299158, accumulated_submission_time=9266.621329, global_step=26457, preemption_count=0, score=9266.621329, test/accuracy=0.661170, test/bleu=27.172473, test/loss=1.606660, test/num_examples=3003, total_duration=15862.000746, train/accuracy=0.638350, train/bleu=31.067360, train/loss=1.767661, validation/accuracy=0.652180, validation/bleu=27.703317, validation/loss=1.663956, validation/num_examples=3000
I0208 09:18:39.081681 139615963391744 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.41413503885269165, loss=1.880503535270691
I0208 09:19:13.980341 139615954999040 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3676011562347412, loss=1.8496878147125244
I0208 09:19:48.936905 139615963391744 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.49009260535240173, loss=1.8263332843780518
I0208 09:20:23.872375 139615954999040 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4021606743335724, loss=1.890332818031311
I0208 09:20:58.769429 139615963391744 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.41290220618247986, loss=1.827967643737793
I0208 09:21:33.721037 139615954999040 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3805704414844513, loss=1.7834454774856567
I0208 09:22:08.635905 139615963391744 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.3955775201320648, loss=1.8536677360534668
I0208 09:22:43.661576 139615954999040 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3627808094024658, loss=1.7926889657974243
I0208 09:23:18.612766 139615963391744 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3825536072254181, loss=1.8148950338363647
I0208 09:23:53.548769 139615954999040 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3737674653530121, loss=1.751444935798645
I0208 09:24:28.444750 139615963391744 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.4117894768714905, loss=1.77065908908844
I0208 09:25:03.342055 139615954999040 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7923808097839355, loss=1.8693358898162842
I0208 09:25:38.241582 139615963391744 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4628366231918335, loss=1.9018243551254272
I0208 09:26:13.141255 139615954999040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4072631001472473, loss=1.8573602437973022
I0208 09:26:48.051708 139615963391744 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.44468793272972107, loss=1.804444670677185
I0208 09:27:22.944750 139615954999040 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.39734551310539246, loss=1.8129128217697144
I0208 09:27:57.884344 139615963391744 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.45392975211143494, loss=1.768822193145752
I0208 09:28:32.892481 139615954999040 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.38846084475517273, loss=1.8345301151275635
I0208 09:29:07.863761 139615963391744 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.41543513536453247, loss=1.779085636138916
I0208 09:29:42.760479 139615954999040 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4144631326198578, loss=1.8095364570617676
I0208 09:30:17.652995 139615963391744 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.43921762704849243, loss=1.855606198310852
I0208 09:30:52.575606 139615954999040 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.39476117491722107, loss=1.8485054969787598
I0208 09:31:27.509754 139615963391744 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.37767645716667175, loss=1.8096567392349243
I0208 09:32:02.420081 139615954999040 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.37490347027778625, loss=1.8652511835098267
I0208 09:32:23.781045 139785736898368 spec.py:321] Evaluating on the training split.
I0208 09:32:26.775096 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:35:57.498268 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 09:36:00.210721 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:38:41.120327 139785736898368 spec.py:349] Evaluating on the test split.
I0208 09:38:43.828526 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:41:39.272885 139785736898368 submission_runner.py:408] Time since start: 17257.57s, 	Step: 28863, 	{'train/accuracy': 0.6295340657234192, 'train/loss': 1.8214620351791382, 'train/bleu': 30.574595318278686, 'validation/accuracy': 0.6530855298042297, 'validation/loss': 1.650708794593811, 'validation/bleu': 27.829608372370426, 'validation/num_examples': 3000, 'test/accuracy': 0.6616930961608887, 'test/loss': 1.588875412940979, 'test/bleu': 27.073841281803126, 'test/num_examples': 3003, 'score': 10106.590117931366, 'total_duration': 17257.571030139923, 'accumulated_submission_time': 10106.590117931366, 'accumulated_eval_time': 7149.713710069656, 'accumulated_logging_time': 0.32878661155700684}
I0208 09:41:39.292108 139615963391744 logging_writer.py:48] [28863] accumulated_eval_time=7149.713710, accumulated_logging_time=0.328787, accumulated_submission_time=10106.590118, global_step=28863, preemption_count=0, score=10106.590118, test/accuracy=0.661693, test/bleu=27.073841, test/loss=1.588875, test/num_examples=3003, total_duration=17257.571030, train/accuracy=0.629534, train/bleu=30.574595, train/loss=1.821462, validation/accuracy=0.653086, validation/bleu=27.829608, validation/loss=1.650709, validation/num_examples=3000
I0208 09:41:52.545391 139615954999040 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.4354887008666992, loss=1.8953948020935059
I0208 09:42:27.515143 139615963391744 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.41814902424812317, loss=1.7944139242172241
I0208 09:43:02.451884 139615954999040 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.4307449162006378, loss=1.8502779006958008
I0208 09:43:37.461010 139615963391744 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6651598811149597, loss=1.8559846878051758
I0208 09:44:12.367823 139615954999040 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4073469638824463, loss=1.8982070684432983
I0208 09:44:47.252432 139615963391744 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.3645848333835602, loss=1.8262560367584229
I0208 09:45:22.128602 139615954999040 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.36967945098876953, loss=1.8467113971710205
I0208 09:45:57.008851 139615963391744 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.45404675602912903, loss=1.7888813018798828
I0208 09:46:31.901318 139615954999040 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.4007788300514221, loss=1.8279141187667847
I0208 09:47:06.808762 139615963391744 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.41138020157814026, loss=1.792791485786438
I0208 09:47:41.729680 139615954999040 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3761325478553772, loss=1.8285051584243774
I0208 09:48:16.637965 139615963391744 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.4242135286331177, loss=1.8333847522735596
I0208 09:48:51.532435 139615954999040 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.39868512749671936, loss=1.8691437244415283
I0208 09:49:26.452746 139615963391744 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.4070580005645752, loss=1.8235697746276855
I0208 09:50:01.383262 139615954999040 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.40144458413124084, loss=1.838484525680542
I0208 09:50:36.270241 139615963391744 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.3414148986339569, loss=1.7475721836090088
I0208 09:51:11.240345 139615954999040 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.4408474266529083, loss=1.8342987298965454
I0208 09:51:46.127765 139615963391744 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.39556705951690674, loss=1.7280312776565552
I0208 09:52:21.011798 139615954999040 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3594510555267334, loss=1.8032175302505493
I0208 09:52:55.926416 139615963391744 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.38714686036109924, loss=1.7490222454071045
I0208 09:53:30.808576 139615954999040 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.402652382850647, loss=1.7917563915252686
I0208 09:54:05.710329 139615963391744 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6266661286354065, loss=1.8109074831008911
I0208 09:54:40.598647 139615954999040 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.4331062436103821, loss=1.8737925291061401
I0208 09:55:15.505464 139615963391744 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.40640583634376526, loss=1.7662675380706787
I0208 09:55:39.296553 139785736898368 spec.py:321] Evaluating on the training split.
I0208 09:55:42.297309 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 09:59:22.769436 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 09:59:25.472421 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:01:56.717986 139785736898368 spec.py:349] Evaluating on the test split.
I0208 10:01:59.437675 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:04:27.879626 139785736898368 submission_runner.py:408] Time since start: 18626.18s, 	Step: 31270, 	{'train/accuracy': 0.6339073777198792, 'train/loss': 1.8032761812210083, 'train/bleu': 31.101627683228955, 'validation/accuracy': 0.6544494032859802, 'validation/loss': 1.6417778730392456, 'validation/bleu': 27.791217888891975, 'validation/num_examples': 3000, 'test/accuracy': 0.6649003624916077, 'test/loss': 1.5839022397994995, 'test/bleu': 27.376689492425207, 'test/num_examples': 3003, 'score': 10946.506821632385, 'total_duration': 18626.177748918533, 'accumulated_submission_time': 10946.506821632385, 'accumulated_eval_time': 7678.296708583832, 'accumulated_logging_time': 0.3578181266784668}
I0208 10:04:27.898966 139615954999040 logging_writer.py:48] [31270] accumulated_eval_time=7678.296709, accumulated_logging_time=0.357818, accumulated_submission_time=10946.506822, global_step=31270, preemption_count=0, score=10946.506822, test/accuracy=0.664900, test/bleu=27.376689, test/loss=1.583902, test/num_examples=3003, total_duration=18626.177749, train/accuracy=0.633907, train/bleu=31.101628, train/loss=1.803276, validation/accuracy=0.654449, validation/bleu=27.791218, validation/loss=1.641778, validation/num_examples=3000
I0208 10:04:38.714652 139615963391744 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.5257295966148376, loss=1.8366224765777588
I0208 10:05:13.572402 139615954999040 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.40389057993888855, loss=1.8139735460281372
I0208 10:05:48.460855 139615963391744 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4908093810081482, loss=1.8331722021102905
I0208 10:06:23.379328 139615954999040 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.8148242831230164, loss=1.8065109252929688
I0208 10:06:58.314215 139615963391744 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.478173166513443, loss=1.853334903717041
I0208 10:07:33.201900 139615954999040 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.36206626892089844, loss=1.7281670570373535
I0208 10:08:08.100194 139615963391744 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.4929441809654236, loss=1.8390095233917236
I0208 10:08:43.004393 139615954999040 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3466213047504425, loss=1.8036316633224487
I0208 10:09:17.908826 139615963391744 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3831516206264496, loss=1.8314763307571411
I0208 10:09:52.806390 139615954999040 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5579532980918884, loss=1.8214623928070068
I0208 10:10:27.726496 139615963391744 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.43069276213645935, loss=1.8460625410079956
I0208 10:11:02.653784 139615954999040 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.4186248183250427, loss=1.8664871454238892
I0208 10:11:37.516932 139615963391744 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5064470767974854, loss=1.7899609804153442
I0208 10:12:12.406872 139615954999040 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3663756847381592, loss=1.80293607711792
I0208 10:12:47.350718 139615963391744 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.39749711751937866, loss=1.7949551343917847
I0208 10:13:22.268377 139615954999040 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.46905383467674255, loss=1.8521523475646973
I0208 10:13:57.146847 139615963391744 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.4081059694290161, loss=1.7934918403625488
I0208 10:14:32.009804 139615954999040 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.44109201431274414, loss=1.776890754699707
I0208 10:15:06.858842 139615963391744 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4462567865848541, loss=1.814998745918274
I0208 10:15:41.741808 139615954999040 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.4433048367500305, loss=1.843557357788086
I0208 10:16:16.616230 139615963391744 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.45233118534088135, loss=1.7715373039245605
I0208 10:16:51.486500 139615954999040 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4157952666282654, loss=1.8419859409332275
I0208 10:17:26.358220 139615963391744 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.41816842555999756, loss=1.7697033882141113
I0208 10:18:01.241437 139615954999040 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.4368937015533447, loss=1.8544669151306152
I0208 10:18:28.158367 139785736898368 spec.py:321] Evaluating on the training split.
I0208 10:18:31.169549 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:21:49.699980 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 10:21:52.404322 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:24:30.192780 139785736898368 spec.py:349] Evaluating on the test split.
I0208 10:24:32.903783 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:26:54.946003 139785736898368 submission_runner.py:408] Time since start: 19973.24s, 	Step: 33679, 	{'train/accuracy': 0.640436053276062, 'train/loss': 1.7520774602890015, 'train/bleu': 31.079749336354755, 'validation/accuracy': 0.6569168567657471, 'validation/loss': 1.6260370016098022, 'validation/bleu': 27.985500631619285, 'validation/num_examples': 3000, 'test/accuracy': 0.6679217219352722, 'test/loss': 1.5678555965423584, 'test/bleu': 27.7718476746868, 'test/num_examples': 3003, 'score': 11786.677701950073, 'total_duration': 19973.244158029556, 'accumulated_submission_time': 11786.677701950073, 'accumulated_eval_time': 8185.084298849106, 'accumulated_logging_time': 0.3881950378417969}
I0208 10:26:54.965712 139615963391744 logging_writer.py:48] [33679] accumulated_eval_time=8185.084299, accumulated_logging_time=0.388195, accumulated_submission_time=11786.677702, global_step=33679, preemption_count=0, score=11786.677702, test/accuracy=0.667922, test/bleu=27.771848, test/loss=1.567856, test/num_examples=3003, total_duration=19973.244158, train/accuracy=0.640436, train/bleu=31.079749, train/loss=1.752077, validation/accuracy=0.656917, validation/bleu=27.985501, validation/loss=1.626037, validation/num_examples=3000
I0208 10:27:02.649117 139615954999040 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.4331853687763214, loss=1.8710294961929321
I0208 10:27:37.553555 139615963391744 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.4277743995189667, loss=1.7570332288742065
I0208 10:28:12.454101 139615954999040 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.4399366080760956, loss=1.7101914882659912
I0208 10:28:47.395584 139615963391744 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.49399465322494507, loss=1.7800604104995728
I0208 10:29:22.278519 139615954999040 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.3886740803718567, loss=1.7831050157546997
I0208 10:29:57.164383 139615963391744 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3652317523956299, loss=1.8095000982284546
I0208 10:30:32.119444 139615954999040 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.38432908058166504, loss=1.7604625225067139
I0208 10:31:06.998048 139615963391744 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.4306849539279938, loss=1.7629985809326172
I0208 10:31:41.874511 139615954999040 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4129934012889862, loss=1.7142671346664429
I0208 10:32:16.752261 139615963391744 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.4587266445159912, loss=1.785104751586914
I0208 10:32:51.636502 139615954999040 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.4532109498977661, loss=1.8507404327392578
I0208 10:33:26.525232 139615963391744 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.4267019033432007, loss=1.753278374671936
I0208 10:34:01.414905 139615954999040 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3948754370212555, loss=1.8030180931091309
I0208 10:34:36.289317 139615963391744 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.4955556094646454, loss=1.7477610111236572
I0208 10:35:11.178685 139615954999040 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.41453641653060913, loss=1.790306806564331
I0208 10:35:46.073277 139615963391744 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.44067835807800293, loss=1.72639000415802
I0208 10:36:20.942954 139615954999040 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.417222261428833, loss=1.7946165800094604
I0208 10:36:55.839075 139615963391744 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4967629611492157, loss=1.7723047733306885
I0208 10:37:30.755365 139615954999040 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.40744486451148987, loss=1.803581953048706
I0208 10:38:05.663349 139615963391744 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.4139086902141571, loss=1.7616500854492188
I0208 10:38:40.530621 139615954999040 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.4163038730621338, loss=1.8247915506362915
I0208 10:39:15.475342 139615963391744 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.4139688313007355, loss=1.7989219427108765
I0208 10:39:50.380720 139615954999040 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.37879639863967896, loss=1.8229378461837769
I0208 10:40:25.290116 139615963391744 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.39931872487068176, loss=1.7395967245101929
I0208 10:40:54.999236 139785736898368 spec.py:321] Evaluating on the training split.
I0208 10:40:57.998110 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:44:06.863370 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 10:44:09.583222 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:46:38.906235 139785736898368 spec.py:349] Evaluating on the test split.
I0208 10:46:41.628483 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 10:48:58.314777 139785736898368 submission_runner.py:408] Time since start: 21296.61s, 	Step: 36087, 	{'train/accuracy': 0.6369031071662903, 'train/loss': 1.7718901634216309, 'train/bleu': 30.80821034725975, 'validation/accuracy': 0.6572268009185791, 'validation/loss': 1.6264159679412842, 'validation/bleu': 28.234495377139638, 'validation/num_examples': 3000, 'test/accuracy': 0.6699668765068054, 'test/loss': 1.5581209659576416, 'test/bleu': 27.991591213510215, 'test/num_examples': 3003, 'score': 12626.625022888184, 'total_duration': 21296.61293053627, 'accumulated_submission_time': 12626.625022888184, 'accumulated_eval_time': 8668.399793624878, 'accumulated_logging_time': 0.41771602630615234}
I0208 10:48:58.334665 139615954999040 logging_writer.py:48] [36087] accumulated_eval_time=8668.399794, accumulated_logging_time=0.417716, accumulated_submission_time=12626.625023, global_step=36087, preemption_count=0, score=12626.625023, test/accuracy=0.669967, test/bleu=27.991591, test/loss=1.558121, test/num_examples=3003, total_duration=21296.612931, train/accuracy=0.636903, train/bleu=30.808210, train/loss=1.771890, validation/accuracy=0.657227, validation/bleu=28.234495, validation/loss=1.626416, validation/num_examples=3000
I0208 10:49:03.235408 139615963391744 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.4319515824317932, loss=1.7040739059448242
I0208 10:49:38.117255 139615954999040 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3725748360157013, loss=1.8092570304870605
I0208 10:50:12.984960 139615963391744 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.37586408853530884, loss=1.7089204788208008
I0208 10:50:47.859794 139615954999040 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3836461007595062, loss=1.8406950235366821
I0208 10:51:22.757350 139615963391744 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4247228503227234, loss=1.7935594320297241
I0208 10:51:57.634351 139615954999040 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.41799038648605347, loss=1.7371636629104614
I0208 10:52:32.570591 139615963391744 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.40459108352661133, loss=1.6851387023925781
I0208 10:53:07.503432 139615954999040 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3844304382801056, loss=1.739055871963501
I0208 10:53:42.417423 139615963391744 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.4288173019886017, loss=1.845289945602417
I0208 10:54:17.306350 139615954999040 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.4135288596153259, loss=1.803295373916626
I0208 10:54:52.205769 139615963391744 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.36736708879470825, loss=1.722550868988037
I0208 10:55:27.079489 139615954999040 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.4136684834957123, loss=1.793395757675171
I0208 10:56:02.003148 139615963391744 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4251948595046997, loss=1.832703948020935
I0208 10:56:37.031481 139615954999040 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.49144455790519714, loss=1.903950572013855
I0208 10:57:11.950598 139615963391744 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.38228654861450195, loss=1.8158336877822876
I0208 10:57:46.875427 139615954999040 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.4199206829071045, loss=1.866361379623413
I0208 10:58:21.779797 139615963391744 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.44997668266296387, loss=1.8648772239685059
I0208 10:58:56.629367 139615954999040 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.41570553183555603, loss=1.7007598876953125
I0208 10:59:31.505202 139615963391744 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.46365007758140564, loss=1.8013964891433716
I0208 11:00:06.386911 139615954999040 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.361356645822525, loss=1.7948280572891235
I0208 11:00:41.261451 139615963391744 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3964957594871521, loss=1.8092727661132812
I0208 11:01:16.149532 139615954999040 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.4217764437198639, loss=1.758219599723816
I0208 11:01:51.011271 139615963391744 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.41498276591300964, loss=1.7649109363555908
I0208 11:02:25.966048 139615954999040 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.43336451053619385, loss=1.7998385429382324
I0208 11:02:58.491732 139785736898368 spec.py:321] Evaluating on the training split.
I0208 11:03:01.495353 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:06:12.021772 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 11:06:14.733506 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:08:41.094539 139785736898368 spec.py:349] Evaluating on the test split.
I0208 11:08:43.811868 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:11:12.440841 139785736898368 submission_runner.py:408] Time since start: 22630.74s, 	Step: 38495, 	{'train/accuracy': 0.6430981159210205, 'train/loss': 1.7214125394821167, 'train/bleu': 31.545833523294696, 'validation/accuracy': 0.6587147116661072, 'validation/loss': 1.6191388368606567, 'validation/bleu': 28.382361296399072, 'validation/num_examples': 3000, 'test/accuracy': 0.6695834398269653, 'test/loss': 1.555444359779358, 'test/bleu': 27.83423761879015, 'test/num_examples': 3003, 'score': 13466.692353248596, 'total_duration': 22630.738989830017, 'accumulated_submission_time': 13466.692353248596, 'accumulated_eval_time': 9162.348851919174, 'accumulated_logging_time': 0.44786763191223145}
I0208 11:11:12.461476 139615963391744 logging_writer.py:48] [38495] accumulated_eval_time=9162.348852, accumulated_logging_time=0.447868, accumulated_submission_time=13466.692353, global_step=38495, preemption_count=0, score=13466.692353, test/accuracy=0.669583, test/bleu=27.834238, test/loss=1.555444, test/num_examples=3003, total_duration=22630.738990, train/accuracy=0.643098, train/bleu=31.545834, train/loss=1.721413, validation/accuracy=0.658715, validation/bleu=28.382361, validation/loss=1.619139, validation/num_examples=3000
I0208 11:11:14.571920 139615954999040 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.411725252866745, loss=1.8026764392852783
I0208 11:11:49.434765 139615963391744 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3968264162540436, loss=1.7588168382644653
I0208 11:12:24.287950 139615954999040 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.45504483580589294, loss=1.775733232498169
I0208 11:12:59.156515 139615963391744 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.41891002655029297, loss=1.759766697883606
I0208 11:13:34.056295 139615954999040 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.39867615699768066, loss=1.7635188102722168
I0208 11:14:08.909753 139615963391744 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.37404870986938477, loss=1.8001848459243774
I0208 11:14:43.789551 139615954999040 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.4000051021575928, loss=1.78981351852417
I0208 11:15:18.681727 139615963391744 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.38880208134651184, loss=1.6732616424560547
I0208 11:15:53.580021 139615954999040 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.39972758293151855, loss=1.8156614303588867
I0208 11:16:28.473355 139615963391744 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4268982708454132, loss=1.7161035537719727
I0208 11:17:03.360833 139615954999040 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.4641370475292206, loss=1.8006881475448608
I0208 11:17:38.252749 139615963391744 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.38557833433151245, loss=1.6975703239440918
I0208 11:18:13.165198 139615954999040 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.40225040912628174, loss=1.7693663835525513
I0208 11:18:48.049284 139615963391744 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.38231366872787476, loss=1.7519199848175049
I0208 11:19:22.942055 139615954999040 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.35911837220191956, loss=1.7499372959136963
I0208 11:19:57.853719 139615963391744 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.36239272356033325, loss=1.7753762006759644
I0208 11:20:32.729465 139615954999040 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3747684061527252, loss=1.7405736446380615
I0208 11:21:07.649245 139615963391744 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.38021528720855713, loss=1.8138928413391113
I0208 11:21:42.552133 139615954999040 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.3515119254589081, loss=1.733962893486023
I0208 11:22:17.469381 139615963391744 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.393594354391098, loss=1.8039594888687134
I0208 11:22:52.384498 139615954999040 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.37983426451683044, loss=1.7970906496047974
I0208 11:23:27.276844 139615963391744 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.39980900287628174, loss=1.659087061882019
I0208 11:24:02.132721 139615954999040 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3887295722961426, loss=1.7681668996810913
I0208 11:24:36.993436 139615963391744 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3588942587375641, loss=1.782882571220398
I0208 11:25:11.897305 139615954999040 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.425458163022995, loss=1.8587281703948975
I0208 11:25:12.671740 139785736898368 spec.py:321] Evaluating on the training split.
I0208 11:25:15.667751 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:28:21.206756 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 11:28:23.926036 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:31:09.189738 139785736898368 spec.py:349] Evaluating on the test split.
I0208 11:31:11.905106 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:33:44.765848 139785736898368 submission_runner.py:408] Time since start: 23983.06s, 	Step: 40904, 	{'train/accuracy': 0.6393962502479553, 'train/loss': 1.7517564296722412, 'train/bleu': 31.326564034977284, 'validation/accuracy': 0.6592354774475098, 'validation/loss': 1.6140036582946777, 'validation/bleu': 28.034270082296626, 'validation/num_examples': 3000, 'test/accuracy': 0.668677031993866, 'test/loss': 1.5514838695526123, 'test/bleu': 27.835884883573716, 'test/num_examples': 3003, 'score': 14306.815686225891, 'total_duration': 23983.063989400864, 'accumulated_submission_time': 14306.815686225891, 'accumulated_eval_time': 9674.442895889282, 'accumulated_logging_time': 0.47867465019226074}
I0208 11:33:44.786499 139615963391744 logging_writer.py:48] [40904] accumulated_eval_time=9674.442896, accumulated_logging_time=0.478675, accumulated_submission_time=14306.815686, global_step=40904, preemption_count=0, score=14306.815686, test/accuracy=0.668677, test/bleu=27.835885, test/loss=1.551484, test/num_examples=3003, total_duration=23983.063989, train/accuracy=0.639396, train/bleu=31.326564, train/loss=1.751756, validation/accuracy=0.659235, validation/bleu=28.034270, validation/loss=1.614004, validation/num_examples=3000
I0208 11:34:18.656913 139615954999040 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.407725989818573, loss=1.81049644947052
I0208 11:34:53.554597 139615963391744 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.38794249296188354, loss=1.809179425239563
I0208 11:35:28.468244 139615954999040 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4582444429397583, loss=1.7156959772109985
I0208 11:36:03.372101 139615963391744 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.36801618337631226, loss=1.7891465425491333
I0208 11:36:38.293382 139615954999040 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.38792386651039124, loss=1.776442527770996
I0208 11:37:13.182948 139615963391744 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3877065181732178, loss=1.7308669090270996
I0208 11:37:48.066122 139615954999040 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.3891260325908661, loss=1.8052411079406738
I0208 11:38:22.944032 139615963391744 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.4547211229801178, loss=1.7781418561935425
I0208 11:38:57.864066 139615954999040 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.44527667760849, loss=1.8320075273513794
I0208 11:39:32.780209 139615963391744 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.34218230843544006, loss=1.7180131673812866
I0208 11:40:07.685187 139615954999040 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.4047601521015167, loss=1.8430540561676025
I0208 11:40:42.578639 139615963391744 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.39174914360046387, loss=1.7754042148590088
I0208 11:41:17.468906 139615954999040 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4746735394001007, loss=1.7682820558547974
I0208 11:41:52.357751 139615963391744 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.4029828906059265, loss=1.723017692565918
I0208 11:42:27.226162 139615954999040 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.43953925371170044, loss=1.696901798248291
I0208 11:43:02.084871 139615963391744 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5264967083930969, loss=1.7608667612075806
I0208 11:43:36.965179 139615954999040 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.3675089180469513, loss=1.7103642225265503
I0208 11:44:11.846525 139615963391744 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7359580993652344, loss=1.7855650186538696
I0208 11:44:46.723950 139615954999040 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.4776102304458618, loss=1.6927512884140015
I0208 11:45:21.627753 139615963391744 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3726194202899933, loss=1.6750560998916626
I0208 11:45:56.528505 139615954999040 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.4203437566757202, loss=1.7720471620559692
I0208 11:46:31.427832 139615963391744 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.4313242733478546, loss=1.7837903499603271
I0208 11:47:06.316233 139615954999040 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.4174559414386749, loss=1.79612135887146
I0208 11:47:41.221260 139615963391744 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.41242852807044983, loss=1.7035198211669922
I0208 11:47:44.788697 139785736898368 spec.py:321] Evaluating on the training split.
I0208 11:47:47.786283 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:50:58.929260 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 11:51:01.663789 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:53:38.045199 139785736898368 spec.py:349] Evaluating on the test split.
I0208 11:53:40.763355 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 11:56:14.637623 139785736898368 submission_runner.py:408] Time since start: 25332.94s, 	Step: 43312, 	{'train/accuracy': 0.6379010081291199, 'train/loss': 1.7598146200180054, 'train/bleu': 31.306862035234403, 'validation/accuracy': 0.6621120572090149, 'validation/loss': 1.6029443740844727, 'validation/bleu': 28.58756001053182, 'validation/num_examples': 3000, 'test/accuracy': 0.6726861000061035, 'test/loss': 1.533181071281433, 'test/bleu': 28.08085070114241, 'test/num_examples': 3003, 'score': 15146.73096871376, 'total_duration': 25332.93573999405, 'accumulated_submission_time': 15146.73096871376, 'accumulated_eval_time': 10184.291736602783, 'accumulated_logging_time': 0.5092837810516357}
I0208 11:56:14.659021 139615954999040 logging_writer.py:48] [43312] accumulated_eval_time=10184.291737, accumulated_logging_time=0.509284, accumulated_submission_time=15146.730969, global_step=43312, preemption_count=0, score=15146.730969, test/accuracy=0.672686, test/bleu=28.080851, test/loss=1.533181, test/num_examples=3003, total_duration=25332.935740, train/accuracy=0.637901, train/bleu=31.306862, train/loss=1.759815, validation/accuracy=0.662112, validation/bleu=28.587560, validation/loss=1.602944, validation/num_examples=3000
I0208 11:56:45.726662 139615963391744 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.42873847484588623, loss=1.819648027420044
I0208 11:57:20.637353 139615954999040 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4586019515991211, loss=1.7439517974853516
I0208 11:57:55.549223 139615963391744 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3954682946205139, loss=1.7358523607254028
I0208 11:58:30.496459 139615954999040 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.4283023178577423, loss=1.8003849983215332
I0208 11:59:05.409832 139615963391744 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3957093060016632, loss=1.8013670444488525
I0208 11:59:40.299889 139615954999040 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.4072991907596588, loss=1.7568432092666626
I0208 12:00:15.206044 139615963391744 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.4809837341308594, loss=1.779176950454712
I0208 12:00:50.112961 139615954999040 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.38019177317619324, loss=1.7152363061904907
I0208 12:01:25.010782 139615963391744 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.3576385974884033, loss=1.7744359970092773
I0208 12:01:59.906844 139615954999040 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.49092286825180054, loss=1.7490259408950806
I0208 12:02:34.796133 139615963391744 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.39444586634635925, loss=1.7124288082122803
I0208 12:03:09.706228 139615954999040 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.3746993839740753, loss=1.8038297891616821
I0208 12:03:44.600371 139615963391744 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.39511698484420776, loss=1.6738619804382324
I0208 12:04:19.520064 139615954999040 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.3845035135746002, loss=1.7603050470352173
I0208 12:04:54.401211 139615963391744 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.4752398431301117, loss=1.7705516815185547
I0208 12:05:29.295471 139615954999040 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.3619334399700165, loss=1.8191113471984863
I0208 12:06:04.158982 139615963391744 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.37122777104377747, loss=1.7403395175933838
I0208 12:06:39.064394 139615954999040 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.3621479570865631, loss=1.7569572925567627
I0208 12:07:13.990530 139615963391744 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3640938401222229, loss=1.7362927198410034
I0208 12:07:48.880356 139615954999040 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.4351102411746979, loss=1.846147894859314
I0208 12:08:23.808108 139615963391744 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.35130929946899414, loss=1.7769370079040527
I0208 12:08:58.707620 139615954999040 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.38529863953590393, loss=1.7752158641815186
I0208 12:09:33.576454 139615963391744 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3839651942253113, loss=1.7637513875961304
I0208 12:10:08.517867 139615954999040 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.41387680172920227, loss=1.7204678058624268
I0208 12:10:14.861200 139785736898368 spec.py:321] Evaluating on the training split.
I0208 12:10:17.860541 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 12:14:38.082659 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 12:14:40.779650 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 12:18:34.972705 139785736898368 spec.py:349] Evaluating on the test split.
I0208 12:18:37.681580 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 12:21:59.693321 139785736898368 submission_runner.py:408] Time since start: 26877.99s, 	Step: 45720, 	{'train/accuracy': 0.642835795879364, 'train/loss': 1.7258001565933228, 'train/bleu': 31.514653977822757, 'validation/accuracy': 0.6610457301139832, 'validation/loss': 1.5972543954849243, 'validation/bleu': 28.238541337906106, 'validation/num_examples': 3000, 'test/accuracy': 0.6723374724388123, 'test/loss': 1.5314693450927734, 'test/bleu': 27.78312192161673, 'test/num_examples': 3003, 'score': 15986.845165491104, 'total_duration': 26877.991456270218, 'accumulated_submission_time': 15986.845165491104, 'accumulated_eval_time': 10889.123789072037, 'accumulated_logging_time': 0.5416944026947021}
I0208 12:21:59.715714 139615963391744 logging_writer.py:48] [45720] accumulated_eval_time=10889.123789, accumulated_logging_time=0.541694, accumulated_submission_time=15986.845165, global_step=45720, preemption_count=0, score=15986.845165, test/accuracy=0.672337, test/bleu=27.783122, test/loss=1.531469, test/num_examples=3003, total_duration=26877.991456, train/accuracy=0.642836, train/bleu=31.514654, train/loss=1.725800, validation/accuracy=0.661046, validation/bleu=28.238541, validation/loss=1.597254, validation/num_examples=3000
I0208 12:22:27.969800 139615954999040 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.3568107783794403, loss=1.6998404264450073
I0208 12:23:02.895710 139615963391744 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.44562768936157227, loss=1.7542946338653564
I0208 12:23:37.794959 139615954999040 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.36964765191078186, loss=1.7378761768341064
I0208 12:24:12.722284 139615963391744 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.36405453085899353, loss=1.8011335134506226
I0208 12:24:47.672235 139615954999040 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.4280741512775421, loss=1.7932394742965698
I0208 12:25:22.556685 139615963391744 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6250619888305664, loss=1.7417771816253662
I0208 12:25:57.436527 139615954999040 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4399249255657196, loss=1.6900150775909424
I0208 12:26:32.336003 139615963391744 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.4197797179222107, loss=1.7572336196899414
I0208 12:27:07.240902 139615954999040 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3713485300540924, loss=1.674025058746338
I0208 12:27:42.194699 139615963391744 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.5093223452568054, loss=1.908392310142517
I0208 12:28:17.109730 139615954999040 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.4637577533721924, loss=1.675586223602295
I0208 12:28:51.982188 139615963391744 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.40099290013313293, loss=1.7199981212615967
I0208 12:29:26.903914 139615954999040 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3632466197013855, loss=1.7511236667633057
I0208 12:30:01.844055 139615963391744 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.36983081698417664, loss=1.7188680171966553
I0208 12:30:36.716958 139615954999040 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3379993438720703, loss=1.6718556880950928
I0208 12:31:11.582827 139615963391744 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.35508841276168823, loss=1.6805917024612427
I0208 12:31:46.450721 139615954999040 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3726244866847992, loss=1.711623191833496
I0208 12:32:21.339869 139615963391744 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.35390645265579224, loss=1.7312440872192383
I0208 12:32:56.223657 139615954999040 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.36495479941368103, loss=1.7440894842147827
I0208 12:33:31.118087 139615963391744 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.39238065481185913, loss=1.7838428020477295
I0208 12:34:05.986190 139615954999040 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.37250977754592896, loss=1.6810665130615234
I0208 12:34:40.866766 139615963391744 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.37649497389793396, loss=1.7338182926177979
I0208 12:35:15.778692 139615954999040 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.39420759677886963, loss=1.6858786344528198
I0208 12:35:50.639132 139615963391744 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.4148966073989868, loss=1.7778388261795044
I0208 12:35:59.779549 139785736898368 spec.py:321] Evaluating on the training split.
I0208 12:36:02.794080 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 12:39:05.191219 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 12:39:07.897466 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 12:41:51.473296 139785736898368 spec.py:349] Evaluating on the test split.
I0208 12:41:54.196518 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 12:44:37.381273 139785736898368 submission_runner.py:408] Time since start: 28235.68s, 	Step: 48128, 	{'train/accuracy': 0.6452369689941406, 'train/loss': 1.726482629776001, 'train/bleu': 31.82425775905154, 'validation/accuracy': 0.6602894067764282, 'validation/loss': 1.590267300605774, 'validation/bleu': 28.259296946768995, 'validation/num_examples': 3000, 'test/accuracy': 0.6742199659347534, 'test/loss': 1.5215578079223633, 'test/bleu': 27.99158132715858, 'test/num_examples': 3003, 'score': 16826.8217689991, 'total_duration': 28235.679394960403, 'accumulated_submission_time': 16826.8217689991, 'accumulated_eval_time': 11406.725434064865, 'accumulated_logging_time': 0.5741453170776367}
I0208 12:44:37.407417 139615954999040 logging_writer.py:48] [48128] accumulated_eval_time=11406.725434, accumulated_logging_time=0.574145, accumulated_submission_time=16826.821769, global_step=48128, preemption_count=0, score=16826.821769, test/accuracy=0.674220, test/bleu=27.991581, test/loss=1.521558, test/num_examples=3003, total_duration=28235.679395, train/accuracy=0.645237, train/bleu=31.824258, train/loss=1.726483, validation/accuracy=0.660289, validation/bleu=28.259297, validation/loss=1.590267, validation/num_examples=3000
I0208 12:45:02.879621 139615963391744 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.36133337020874023, loss=1.7458641529083252
I0208 12:45:37.729132 139615954999040 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.4282629191875458, loss=1.7845745086669922
I0208 12:46:12.590308 139615963391744 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3552034795284271, loss=1.7343015670776367
I0208 12:46:47.446739 139615954999040 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.35638749599456787, loss=1.7033253908157349
I0208 12:47:22.345370 139615963391744 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.38591843843460083, loss=1.8011822700500488
I0208 12:47:57.244326 139615954999040 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.33675578236579895, loss=1.7246991395950317
I0208 12:48:32.140410 139615963391744 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3480137884616852, loss=1.6945363283157349
I0208 12:49:07.036140 139615954999040 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3946661651134491, loss=1.836303949356079
I0208 12:49:41.948328 139615963391744 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.40248551964759827, loss=1.7549666166305542
I0208 12:50:16.836143 139615954999040 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.34608393907546997, loss=1.7579843997955322
I0208 12:50:51.741527 139615963391744 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.4058328866958618, loss=1.785805344581604
I0208 12:51:26.634217 139615954999040 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.4195624589920044, loss=1.7732402086257935
I0208 12:52:01.516966 139615963391744 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.37764474749565125, loss=1.7117977142333984
I0208 12:52:36.442511 139615954999040 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3942219913005829, loss=1.8042489290237427
I0208 12:53:11.312253 139615963391744 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.37785109877586365, loss=1.7673720121383667
I0208 12:53:46.193941 139615954999040 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.4111030101776123, loss=1.7171854972839355
I0208 12:54:21.072528 139615963391744 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.4082131087779999, loss=1.7341911792755127
I0208 12:54:55.991104 139615954999040 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.413874089717865, loss=1.8185315132141113
I0208 12:55:30.884062 139615963391744 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3722286522388458, loss=1.7478100061416626
I0208 12:56:05.768304 139615954999040 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.42091104388237, loss=1.7685178518295288
I0208 12:56:40.640952 139615963391744 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.4483616352081299, loss=1.8249098062515259
I0208 12:57:15.540925 139615954999040 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.39792200922966003, loss=1.6637428998947144
I0208 12:57:50.403339 139615963391744 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3650595545768738, loss=1.7174263000488281
I0208 12:58:25.309879 139615954999040 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.41553255915641785, loss=1.7559096813201904
I0208 12:58:37.601338 139785736898368 spec.py:321] Evaluating on the training split.
I0208 12:58:40.603862 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:01:54.683990 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 13:01:57.392009 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:04:31.259712 139785736898368 spec.py:349] Evaluating on the test split.
I0208 13:04:33.973351 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:07:24.384343 139785736898368 submission_runner.py:408] Time since start: 29602.68s, 	Step: 50537, 	{'train/accuracy': 0.6565302610397339, 'train/loss': 1.640212893486023, 'train/bleu': 32.767179635812575, 'validation/accuracy': 0.6637363433837891, 'validation/loss': 1.5860275030136108, 'validation/bleu': 28.6552268817002, 'validation/num_examples': 3000, 'test/accuracy': 0.675358772277832, 'test/loss': 1.518868088722229, 'test/bleu': 28.05993585302966, 'test/num_examples': 3003, 'score': 17666.926292657852, 'total_duration': 29602.6824324131, 'accumulated_submission_time': 17666.926292657852, 'accumulated_eval_time': 11933.50832438469, 'accumulated_logging_time': 0.6121892929077148}
I0208 13:07:24.410416 139615963391744 logging_writer.py:48] [50537] accumulated_eval_time=11933.508324, accumulated_logging_time=0.612189, accumulated_submission_time=17666.926293, global_step=50537, preemption_count=0, score=17666.926293, test/accuracy=0.675359, test/bleu=28.059936, test/loss=1.518868, test/num_examples=3003, total_duration=29602.682432, train/accuracy=0.656530, train/bleu=32.767180, train/loss=1.640213, validation/accuracy=0.663736, validation/bleu=28.655227, validation/loss=1.586028, validation/num_examples=3000
I0208 13:07:46.831259 139615954999040 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3505025804042816, loss=1.7494410276412964
I0208 13:08:21.711391 139615963391744 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.4104343056678772, loss=1.827073335647583
I0208 13:08:56.586571 139615954999040 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.39111328125, loss=1.6686921119689941
I0208 13:09:31.474197 139615963391744 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.4004298448562622, loss=1.7514058351516724
I0208 13:10:06.380623 139615954999040 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.37661299109458923, loss=1.729190707206726
I0208 13:10:41.262685 139615963391744 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3684939444065094, loss=1.6913477182388306
I0208 13:11:16.145722 139615954999040 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.4445960521697998, loss=1.7965022325515747
I0208 13:11:51.060319 139615963391744 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.38459157943725586, loss=1.7670282125473022
I0208 13:12:25.959181 139615954999040 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3897969126701355, loss=1.8050702810287476
I0208 13:13:00.850932 139615963391744 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3604187071323395, loss=1.6338931322097778
I0208 13:13:35.751132 139615954999040 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3986049294471741, loss=1.7714998722076416
I0208 13:14:10.694247 139615963391744 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3808772563934326, loss=1.7199167013168335
I0208 13:14:45.567584 139615954999040 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3683184087276459, loss=1.8031504154205322
I0208 13:15:20.456778 139615963391744 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3951161503791809, loss=1.7779453992843628
I0208 13:15:55.309105 139615954999040 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.38399389386177063, loss=1.762468695640564
I0208 13:16:30.203125 139615963391744 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3488798439502716, loss=1.7672208547592163
I0208 13:17:05.087809 139615954999040 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3886648714542389, loss=1.7065763473510742
I0208 13:17:39.984117 139615963391744 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.4340924918651581, loss=1.7969789505004883
I0208 13:18:14.868402 139615954999040 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.37071600556373596, loss=1.75409996509552
I0208 13:18:49.771288 139615963391744 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3568783104419708, loss=1.7112535238265991
I0208 13:19:24.650209 139615954999040 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3415904641151428, loss=1.7020844221115112
I0208 13:19:59.543443 139615963391744 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3549690544605255, loss=1.7617582082748413
I0208 13:20:34.448079 139615954999040 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3945246934890747, loss=1.7425498962402344
I0208 13:21:09.307642 139615963391744 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3608894646167755, loss=1.709415316581726
I0208 13:21:24.719457 139785736898368 spec.py:321] Evaluating on the training split.
I0208 13:21:27.729562 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:25:17.959932 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 13:25:20.663507 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:27:55.311147 139785736898368 spec.py:349] Evaluating on the test split.
I0208 13:27:58.046329 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:30:37.770026 139785736898368 submission_runner.py:408] Time since start: 30996.07s, 	Step: 52946, 	{'train/accuracy': 0.6452774405479431, 'train/loss': 1.7110553979873657, 'train/bleu': 31.52326751196575, 'validation/accuracy': 0.6643067002296448, 'validation/loss': 1.580517292022705, 'validation/bleu': 28.571725114346876, 'validation/num_examples': 3000, 'test/accuracy': 0.6761606335639954, 'test/loss': 1.5175516605377197, 'test/bleu': 28.092700297437375, 'test/num_examples': 3003, 'score': 18507.14643883705, 'total_duration': 30996.068150520325, 'accumulated_submission_time': 18507.14643883705, 'accumulated_eval_time': 12486.558814764023, 'accumulated_logging_time': 0.6497421264648438}
I0208 13:30:37.796565 139615954999040 logging_writer.py:48] [52946] accumulated_eval_time=12486.558815, accumulated_logging_time=0.649742, accumulated_submission_time=18507.146439, global_step=52946, preemption_count=0, score=18507.146439, test/accuracy=0.676161, test/bleu=28.092700, test/loss=1.517552, test/num_examples=3003, total_duration=30996.068151, train/accuracy=0.645277, train/bleu=31.523268, train/loss=1.711055, validation/accuracy=0.664307, validation/bleu=28.571725, validation/loss=1.580517, validation/num_examples=3000
I0208 13:30:56.995234 139615963391744 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3985406160354614, loss=1.7612113952636719
I0208 13:31:31.915186 139615954999040 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3567223846912384, loss=1.7368535995483398
I0208 13:32:06.809760 139615963391744 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.5829809904098511, loss=1.7242653369903564
I0208 13:32:41.744237 139615954999040 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.37640297412872314, loss=1.6864408254623413
I0208 13:33:16.625136 139615963391744 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3566800057888031, loss=1.6925172805786133
I0208 13:33:51.589604 139615954999040 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3667670488357544, loss=1.736609697341919
I0208 13:34:26.548602 139615963391744 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.4152301847934723, loss=1.7332267761230469
I0208 13:35:01.446373 139615954999040 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.361774742603302, loss=1.644108772277832
I0208 13:35:36.334810 139615963391744 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.42271479964256287, loss=1.7172093391418457
I0208 13:36:11.249855 139615954999040 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.39009159803390503, loss=1.725789189338684
I0208 13:36:46.159824 139615963391744 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.34852829575538635, loss=1.7365570068359375
I0208 13:37:21.073690 139615954999040 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.379136860370636, loss=1.664462685585022
I0208 13:37:55.934795 139615963391744 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.40348368883132935, loss=1.8213750123977661
I0208 13:38:30.852952 139615954999040 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.44424837827682495, loss=1.651610016822815
I0208 13:39:05.719349 139615963391744 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.38999322056770325, loss=1.672590732574463
I0208 13:39:40.610538 139615954999040 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3595374822616577, loss=1.6866744756698608
I0208 13:40:15.507915 139615963391744 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.393415242433548, loss=1.7592536211013794
I0208 13:40:50.398991 139615954999040 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3706691265106201, loss=1.680111289024353
I0208 13:41:25.260556 139615963391744 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.37519893050193787, loss=1.7076683044433594
I0208 13:42:00.130392 139615954999040 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.3979819416999817, loss=1.7044223546981812
I0208 13:42:35.010911 139615963391744 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3820159435272217, loss=1.704786777496338
I0208 13:43:09.884340 139615954999040 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.47120770812034607, loss=1.6946167945861816
I0208 13:43:44.754274 139615963391744 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.3825887143611908, loss=1.6564279794692993
I0208 13:44:19.638156 139615954999040 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.38359183073043823, loss=1.7813040018081665
I0208 13:44:37.834790 139785736898368 spec.py:321] Evaluating on the training split.
I0208 13:44:40.839953 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:48:31.799075 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 13:48:34.519177 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:51:06.007966 139785736898368 spec.py:349] Evaluating on the test split.
I0208 13:51:08.706014 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 13:53:31.130683 139785736898368 submission_runner.py:408] Time since start: 32369.43s, 	Step: 55354, 	{'train/accuracy': 0.6445077657699585, 'train/loss': 1.712929129600525, 'train/bleu': 31.342913567592323, 'validation/accuracy': 0.663823127746582, 'validation/loss': 1.576424241065979, 'validation/bleu': 28.29432199708726, 'validation/num_examples': 3000, 'test/accuracy': 0.6759397983551025, 'test/loss': 1.5070810317993164, 'test/bleu': 28.14646888038358, 'test/num_examples': 3003, 'score': 19347.095779180527, 'total_duration': 32369.428835868835, 'accumulated_submission_time': 19347.095779180527, 'accumulated_eval_time': 13019.85465836525, 'accumulated_logging_time': 0.6876661777496338}
I0208 13:53:31.153125 139615963391744 logging_writer.py:48] [55354] accumulated_eval_time=13019.854658, accumulated_logging_time=0.687666, accumulated_submission_time=19347.095779, global_step=55354, preemption_count=0, score=19347.095779, test/accuracy=0.675940, test/bleu=28.146469, test/loss=1.507081, test/num_examples=3003, total_duration=32369.428836, train/accuracy=0.644508, train/bleu=31.342914, train/loss=1.712929, validation/accuracy=0.663823, validation/bleu=28.294322, validation/loss=1.576424, validation/num_examples=3000
I0208 13:53:47.538161 139615954999040 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3733028173446655, loss=1.7461780309677124
I0208 13:54:22.438676 139615963391744 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.4518967568874359, loss=1.7410224676132202
I0208 13:54:57.354305 139615954999040 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3990451395511627, loss=1.7769169807434082
I0208 13:55:32.252276 139615963391744 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3993853032588959, loss=1.7649468183517456
I0208 13:56:07.191540 139615954999040 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.46758216619491577, loss=1.766083002090454
I0208 13:56:42.100368 139615963391744 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.3754763603210449, loss=1.6638141870498657
I0208 13:57:17.018288 139615954999040 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.41931959986686707, loss=1.6636137962341309
I0208 13:57:51.923784 139615963391744 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.36685609817504883, loss=1.6414328813552856
I0208 13:58:26.862918 139615954999040 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3879842460155487, loss=1.7460463047027588
I0208 13:59:01.793477 139615963391744 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.4199858009815216, loss=1.7123949527740479
I0208 13:59:36.783261 139615954999040 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3968375325202942, loss=1.7508435249328613
I0208 14:00:11.787270 139615963391744 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.4417155086994171, loss=1.6746042966842651
I0208 14:00:46.697582 139615954999040 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.39896464347839355, loss=1.790853500366211
I0208 14:01:21.599908 139615963391744 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.36737510561943054, loss=1.6365671157836914
I0208 14:01:56.501452 139615954999040 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3982195556163788, loss=1.7855948209762573
I0208 14:02:31.434303 139615963391744 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.4017413854598999, loss=1.6880602836608887
I0208 14:03:06.351052 139615954999040 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.4351751208305359, loss=1.7314643859863281
I0208 14:03:41.223016 139615963391744 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.3544866144657135, loss=1.6957358121871948
I0208 14:04:16.140917 139615954999040 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3872539699077606, loss=1.739586591720581
I0208 14:04:51.030187 139615963391744 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.39979732036590576, loss=1.8089462518692017
I0208 14:05:25.880205 139615954999040 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.46848568320274353, loss=1.713808536529541
I0208 14:06:00.810364 139615963391744 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.41450998187065125, loss=1.667237639427185
I0208 14:06:35.742805 139615954999040 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3719635009765625, loss=1.6443521976470947
I0208 14:07:10.653697 139615963391744 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.408626526594162, loss=1.7269995212554932
I0208 14:07:31.322595 139785736898368 spec.py:321] Evaluating on the training split.
I0208 14:07:34.327224 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:10:59.385438 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 14:11:02.132943 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:13:52.832836 139785736898368 spec.py:349] Evaluating on the test split.
I0208 14:13:55.553359 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:16:31.416090 139785736898368 submission_runner.py:408] Time since start: 33749.71s, 	Step: 57761, 	{'train/accuracy': 0.649020254611969, 'train/loss': 1.682659387588501, 'train/bleu': 32.0713855796427, 'validation/accuracy': 0.6664641499519348, 'validation/loss': 1.5685975551605225, 'validation/bleu': 28.97768596478849, 'validation/num_examples': 3000, 'test/accuracy': 0.6782522797584534, 'test/loss': 1.4985307455062866, 'test/bleu': 28.270571580751486, 'test/num_examples': 3003, 'score': 20187.175048589706, 'total_duration': 33749.7142393589, 'accumulated_submission_time': 20187.175048589706, 'accumulated_eval_time': 13559.9481112957, 'accumulated_logging_time': 0.7214796543121338}
I0208 14:16:31.438465 139615954999040 logging_writer.py:48] [57761] accumulated_eval_time=13559.948111, accumulated_logging_time=0.721480, accumulated_submission_time=20187.175049, global_step=57761, preemption_count=0, score=20187.175049, test/accuracy=0.678252, test/bleu=28.270572, test/loss=1.498531, test/num_examples=3003, total_duration=33749.714239, train/accuracy=0.649020, train/bleu=32.071386, train/loss=1.682659, validation/accuracy=0.666464, validation/bleu=28.977686, validation/loss=1.568598, validation/num_examples=3000
I0208 14:16:45.388175 139615963391744 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.42367202043533325, loss=1.6935652494430542
I0208 14:17:20.279057 139615954999040 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.39650359749794006, loss=1.681365728378296
I0208 14:17:55.156431 139615963391744 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3548400402069092, loss=1.6562023162841797
I0208 14:18:30.028818 139615954999040 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3570204973220825, loss=1.621717929840088
I0208 14:19:04.902143 139615963391744 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.4283757507801056, loss=1.6497843265533447
I0208 14:19:39.777843 139615954999040 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.37120938301086426, loss=1.703853964805603
I0208 14:20:14.682894 139615963391744 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.4121377766132355, loss=1.6734967231750488
I0208 14:20:49.568605 139615954999040 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.3880033493041992, loss=1.6728909015655518
I0208 14:21:24.489442 139615963391744 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.4001818895339966, loss=1.713984489440918
I0208 14:21:59.407153 139615954999040 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.38455986976623535, loss=1.7272526025772095
I0208 14:22:34.297081 139615963391744 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.43466439843177795, loss=1.740795373916626
I0208 14:23:09.189910 139615954999040 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.395862340927124, loss=1.7412962913513184
I0208 14:23:44.075583 139615963391744 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.39135652780532837, loss=1.7055995464324951
I0208 14:24:18.973605 139615954999040 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.34163689613342285, loss=1.7252432107925415
I0208 14:24:53.865174 139615963391744 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.4053047299385071, loss=1.7176308631896973
I0208 14:25:28.742001 139615954999040 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3736889362335205, loss=1.7158905267715454
I0208 14:26:03.634118 139615963391744 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3720656633377075, loss=1.73117196559906
I0208 14:26:38.530822 139615954999040 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3762698769569397, loss=1.697145700454712
I0208 14:27:13.412374 139615963391744 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.418300062417984, loss=1.6866594552993774
I0208 14:27:48.315020 139615954999040 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3418199121952057, loss=1.6514939069747925
I0208 14:28:23.223740 139615963391744 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.347859650850296, loss=1.6958218812942505
I0208 14:28:58.202117 139615954999040 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.40436771512031555, loss=1.5908567905426025
I0208 14:29:33.110241 139615963391744 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.371036171913147, loss=1.6661627292633057
I0208 14:30:07.995343 139615954999040 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.38628554344177246, loss=1.7256706953048706
I0208 14:30:31.424199 139785736898368 spec.py:321] Evaluating on the training split.
I0208 14:30:34.425680 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:34:00.391622 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 14:34:03.120518 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:36:56.620698 139785736898368 spec.py:349] Evaluating on the test split.
I0208 14:36:59.366518 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:39:36.371732 139785736898368 submission_runner.py:408] Time since start: 35134.67s, 	Step: 60169, 	{'train/accuracy': 0.648051917552948, 'train/loss': 1.6906379461288452, 'train/bleu': 31.884365702800636, 'validation/accuracy': 0.6664765477180481, 'validation/loss': 1.5599160194396973, 'validation/bleu': 28.958729574332384, 'validation/num_examples': 3000, 'test/accuracy': 0.6790889501571655, 'test/loss': 1.4897910356521606, 'test/bleu': 28.51629844762099, 'test/num_examples': 3003, 'score': 21027.074191093445, 'total_duration': 35134.669848680496, 'accumulated_submission_time': 21027.074191093445, 'accumulated_eval_time': 14104.895560979843, 'accumulated_logging_time': 0.7536098957061768}
I0208 14:39:36.400007 139615963391744 logging_writer.py:48] [60169] accumulated_eval_time=14104.895561, accumulated_logging_time=0.753610, accumulated_submission_time=21027.074191, global_step=60169, preemption_count=0, score=21027.074191, test/accuracy=0.679089, test/bleu=28.516298, test/loss=1.489791, test/num_examples=3003, total_duration=35134.669849, train/accuracy=0.648052, train/bleu=31.884366, train/loss=1.690638, validation/accuracy=0.666477, validation/bleu=28.958730, validation/loss=1.559916, validation/num_examples=3000
I0208 14:39:47.580331 139615954999040 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3453919589519501, loss=1.743658185005188
I0208 14:40:22.478500 139615963391744 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.4461826682090759, loss=1.717384934425354
I0208 14:40:57.345295 139615954999040 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.39914628863334656, loss=1.6694799661636353
I0208 14:41:32.220450 139615963391744 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.36956414580345154, loss=1.6551493406295776
I0208 14:42:07.097746 139615954999040 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.4068211317062378, loss=1.7115637063980103
I0208 14:42:42.019694 139615963391744 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3586404025554657, loss=1.6572551727294922
I0208 14:43:16.925340 139615954999040 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.4148389399051666, loss=1.7602049112319946
I0208 14:43:51.815950 139615963391744 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.4065241515636444, loss=1.8035404682159424
I0208 14:44:26.688016 139615954999040 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.38122305274009705, loss=1.7318710088729858
I0208 14:45:01.571184 139615963391744 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.37052810192108154, loss=1.6794347763061523
I0208 14:45:36.414194 139615954999040 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.38275638222694397, loss=1.6835662126541138
I0208 14:46:11.330187 139615963391744 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.3853301703929901, loss=1.6861650943756104
I0208 14:46:46.243386 139615954999040 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.4203120470046997, loss=1.698946237564087
I0208 14:47:21.223694 139615963391744 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3719380795955658, loss=1.6767172813415527
I0208 14:47:56.162500 139615954999040 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.37099263072013855, loss=1.6306757926940918
I0208 14:48:31.082314 139615963391744 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.37909260392189026, loss=1.6711331605911255
I0208 14:49:06.001920 139615954999040 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.39529648423194885, loss=1.7293424606323242
I0208 14:49:40.987114 139615963391744 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3481108546257019, loss=1.6435072422027588
I0208 14:50:15.865167 139615954999040 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.37538042664527893, loss=1.8095464706420898
I0208 14:50:50.776620 139615963391744 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.34958550333976746, loss=1.6145415306091309
I0208 14:51:25.689527 139615954999040 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3938840627670288, loss=1.6934137344360352
I0208 14:52:00.577538 139615963391744 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.40828368067741394, loss=1.6961983442306519
I0208 14:52:35.457952 139615954999040 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3818664848804474, loss=1.6740981340408325
I0208 14:53:10.322142 139615963391744 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.4087273180484772, loss=1.7308803796768188
I0208 14:53:36.582594 139785736898368 spec.py:321] Evaluating on the training split.
I0208 14:53:39.588319 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:56:28.316607 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 14:56:31.022036 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 14:58:57.133657 139785736898368 spec.py:349] Evaluating on the test split.
I0208 14:58:59.858350 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:01:15.642887 139785736898368 submission_runner.py:408] Time since start: 36433.94s, 	Step: 62577, 	{'train/accuracy': 0.6952161192893982, 'train/loss': 1.432866096496582, 'train/bleu': 35.779476196254954, 'validation/accuracy': 0.6689935326576233, 'validation/loss': 1.5532121658325195, 'validation/bleu': 29.131601118820857, 'validation/num_examples': 3000, 'test/accuracy': 0.678345263004303, 'test/loss': 1.491255760192871, 'test/bleu': 28.343151127526827, 'test/num_examples': 3003, 'score': 21867.165781974792, 'total_duration': 36433.94098830223, 'accumulated_submission_time': 21867.165781974792, 'accumulated_eval_time': 14563.95576763153, 'accumulated_logging_time': 0.7935366630554199}
I0208 15:01:15.670679 139615954999040 logging_writer.py:48] [62577] accumulated_eval_time=14563.955768, accumulated_logging_time=0.793537, accumulated_submission_time=21867.165782, global_step=62577, preemption_count=0, score=21867.165782, test/accuracy=0.678345, test/bleu=28.343151, test/loss=1.491256, test/num_examples=3003, total_duration=36433.940988, train/accuracy=0.695216, train/bleu=35.779476, train/loss=1.432866, validation/accuracy=0.668994, validation/bleu=29.131601, validation/loss=1.553212, validation/num_examples=3000
I0208 15:01:24.054657 139615963391744 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3754713833332062, loss=1.694327473640442
I0208 15:01:58.980255 139615954999040 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.37443605065345764, loss=1.7149308919906616
I0208 15:02:33.861850 139615963391744 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.37250810861587524, loss=1.649643063545227
I0208 15:03:08.772730 139615954999040 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.38952648639678955, loss=1.6797034740447998
I0208 15:03:43.687045 139615963391744 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.4014762341976166, loss=1.7219784259796143
I0208 15:04:18.609776 139615954999040 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3505115807056427, loss=1.6351374387741089
I0208 15:04:53.531536 139615963391744 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.39693528413772583, loss=1.5953041315078735
I0208 15:05:28.399310 139615954999040 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.36293715238571167, loss=1.6716550588607788
I0208 15:06:03.293668 139615963391744 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3671993613243103, loss=1.7271100282669067
I0208 15:06:38.188553 139615954999040 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.39510318636894226, loss=1.7432273626327515
I0208 15:07:13.065755 139615963391744 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.38345959782600403, loss=1.7473187446594238
I0208 15:07:47.944822 139615954999040 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.38994458317756653, loss=1.7156161069869995
I0208 15:08:22.854040 139615963391744 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.37786877155303955, loss=1.727312445640564
I0208 15:08:57.750417 139615954999040 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3809717893600464, loss=1.698059320449829
I0208 15:09:32.629076 139615963391744 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3823326826095581, loss=1.7131482362747192
I0208 15:10:07.522263 139615954999040 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3854139745235443, loss=1.5794891119003296
I0208 15:10:42.405731 139615963391744 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.36713212728500366, loss=1.783980369567871
I0208 15:11:17.293479 139615954999040 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.37971335649490356, loss=1.706227421760559
I0208 15:11:52.170001 139615963391744 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3791043758392334, loss=1.8191359043121338
I0208 15:12:27.048555 139615954999040 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.4105481803417206, loss=1.7564456462860107
I0208 15:13:01.928637 139615963391744 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.4068235456943512, loss=1.7213586568832397
I0208 15:13:36.815297 139615954999040 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.41117143630981445, loss=1.6241933107376099
I0208 15:14:11.678569 139615963391744 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.3935682475566864, loss=1.7145909070968628
I0208 15:14:46.593420 139615954999040 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3642059564590454, loss=1.7168464660644531
I0208 15:15:15.973642 139785736898368 spec.py:321] Evaluating on the training split.
I0208 15:15:18.978149 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:18:15.356961 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 15:18:18.077868 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:20:46.369222 139785736898368 spec.py:349] Evaluating on the test split.
I0208 15:20:49.091654 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:23:17.895429 139785736898368 submission_runner.py:408] Time since start: 37756.19s, 	Step: 64986, 	{'train/accuracy': 0.6524246335029602, 'train/loss': 1.6702783107757568, 'train/bleu': 31.827023341578347, 'validation/accuracy': 0.6694151163101196, 'validation/loss': 1.539971113204956, 'validation/bleu': 28.90430783369693, 'validation/num_examples': 3000, 'test/accuracy': 0.6832142472267151, 'test/loss': 1.4675744771957397, 'test/bleu': 28.673106638091205, 'test/num_examples': 3003, 'score': 22707.377870559692, 'total_duration': 37756.19357728958, 'accumulated_submission_time': 22707.377870559692, 'accumulated_eval_time': 15045.877503871918, 'accumulated_logging_time': 0.8332781791687012}
I0208 15:23:17.918680 139615963391744 logging_writer.py:48] [64986] accumulated_eval_time=15045.877504, accumulated_logging_time=0.833278, accumulated_submission_time=22707.377871, global_step=64986, preemption_count=0, score=22707.377871, test/accuracy=0.683214, test/bleu=28.673107, test/loss=1.467574, test/num_examples=3003, total_duration=37756.193577, train/accuracy=0.652425, train/bleu=31.827023, train/loss=1.670278, validation/accuracy=0.669415, validation/bleu=28.904308, validation/loss=1.539971, validation/num_examples=3000
I0208 15:23:23.248724 139615954999040 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.40893423557281494, loss=1.7025279998779297
I0208 15:23:58.119847 139615963391744 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.41302356123924255, loss=1.6677237749099731
I0208 15:24:32.994720 139615954999040 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.42272600531578064, loss=1.6971039772033691
I0208 15:25:07.880289 139615963391744 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.4646589457988739, loss=1.5631874799728394
I0208 15:25:42.766200 139615954999040 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3789655864238739, loss=1.7722527980804443
I0208 15:26:17.703879 139615963391744 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.4111279249191284, loss=1.7209488153457642
I0208 15:26:52.647709 139615954999040 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.4434735178947449, loss=1.7215272188186646
I0208 15:27:27.584869 139615963391744 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.4023516774177551, loss=1.6085511445999146
I0208 15:28:02.474996 139615954999040 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.3866327404975891, loss=1.6487154960632324
I0208 15:28:37.415211 139615963391744 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3811517357826233, loss=1.6969481706619263
I0208 15:29:12.309245 139615954999040 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3838219940662384, loss=1.6221781969070435
I0208 15:29:47.237145 139615963391744 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.39373689889907837, loss=1.6549787521362305
I0208 15:30:22.150267 139615954999040 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.37412893772125244, loss=1.7184250354766846
I0208 15:30:57.031253 139615963391744 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3825032413005829, loss=1.6420725584030151
I0208 15:31:31.922742 139615954999040 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3736879825592041, loss=1.6305017471313477
I0208 15:32:06.796984 139615963391744 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.4064296782016754, loss=1.6955962181091309
I0208 15:32:41.693912 139615954999040 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.37809908390045166, loss=1.7371968030929565
I0208 15:33:16.584047 139615963391744 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3619648218154907, loss=1.6826188564300537
I0208 15:33:51.454450 139615954999040 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.4183517396450043, loss=1.7481518983840942
I0208 15:34:26.355859 139615963391744 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3714565336704254, loss=1.5979443788528442
I0208 15:35:01.264657 139615954999040 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.4319896399974823, loss=1.7250823974609375
I0208 15:35:36.138942 139615963391744 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3716006875038147, loss=1.7258707284927368
I0208 15:36:11.001456 139615954999040 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.38106974959373474, loss=1.6468802690505981
I0208 15:36:45.869303 139615963391744 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.4335106909275055, loss=1.6405737400054932
I0208 15:37:18.055583 139785736898368 spec.py:321] Evaluating on the training split.
I0208 15:37:21.064653 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:41:45.831156 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 15:41:48.552728 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:44:37.112802 139785736898368 spec.py:349] Evaluating on the test split.
I0208 15:44:39.841346 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 15:47:19.376796 139785736898368 submission_runner.py:408] Time since start: 39197.67s, 	Step: 67394, 	{'train/accuracy': 0.6514863967895508, 'train/loss': 1.6692497730255127, 'train/bleu': 31.94321390091731, 'validation/accuracy': 0.6705062389373779, 'validation/loss': 1.540156364440918, 'validation/bleu': 29.242645233820422, 'validation/num_examples': 3000, 'test/accuracy': 0.6817849278450012, 'test/loss': 1.4733388423919678, 'test/bleu': 28.59808848117703, 'test/num_examples': 3003, 'score': 23547.34636592865, 'total_duration': 39197.67491483688, 'accumulated_submission_time': 23547.34636592865, 'accumulated_eval_time': 15647.19865846634, 'accumulated_logging_time': 0.9477291107177734}
I0208 15:47:19.405003 139615954999040 logging_writer.py:48] [67394] accumulated_eval_time=15647.198658, accumulated_logging_time=0.947729, accumulated_submission_time=23547.346366, global_step=67394, preemption_count=0, score=23547.346366, test/accuracy=0.681785, test/bleu=28.598088, test/loss=1.473339, test/num_examples=3003, total_duration=39197.674915, train/accuracy=0.651486, train/bleu=31.943214, train/loss=1.669250, validation/accuracy=0.670506, validation/bleu=29.242645, validation/loss=1.540156, validation/num_examples=3000
I0208 15:47:21.863108 139615963391744 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.4308767020702362, loss=1.6784067153930664
I0208 15:47:56.734050 139615954999040 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3806898593902588, loss=1.6359223127365112
I0208 15:48:31.621338 139615963391744 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.38910242915153503, loss=1.6496144533157349
I0208 15:49:06.519544 139615954999040 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3804626166820526, loss=1.6289769411087036
I0208 15:49:41.393614 139615963391744 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3919685482978821, loss=1.6783665418624878
I0208 15:50:16.303208 139615954999040 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3936484158039093, loss=1.7468231916427612
I0208 15:50:51.279764 139615963391744 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.37470436096191406, loss=1.5697869062423706
I0208 15:51:26.176856 139615954999040 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.423099547624588, loss=1.7148942947387695
I0208 15:52:01.104669 139615963391744 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.4023796319961548, loss=1.8107637166976929
I0208 15:52:35.986080 139615954999040 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3831881284713745, loss=1.6596242189407349
I0208 15:53:10.860344 139615963391744 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.37446779012680054, loss=1.6175506114959717
I0208 15:53:45.724430 139615954999040 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3980940282344818, loss=1.6644716262817383
I0208 15:54:20.666695 139615963391744 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.37430641055107117, loss=1.6750078201293945
I0208 15:54:55.621768 139615954999040 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.4173479974269867, loss=1.6704214811325073
I0208 15:55:30.546492 139615963391744 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.4373318552970886, loss=1.7699182033538818
I0208 15:56:05.455505 139615954999040 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.40364041924476624, loss=1.6493040323257446
I0208 15:56:40.390103 139615963391744 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.4324774146080017, loss=1.610807180404663
I0208 15:57:15.265040 139615954999040 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.39589065313339233, loss=1.7243237495422363
I0208 15:57:50.179932 139615963391744 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.4185106158256531, loss=1.6655761003494263
I0208 15:58:25.056865 139615954999040 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3999335467815399, loss=1.6357369422912598
I0208 15:58:59.983293 139615963391744 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.39870724081993103, loss=1.674961805343628
I0208 15:59:34.902524 139615954999040 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.3944897949695587, loss=1.6997283697128296
I0208 16:00:09.818929 139615963391744 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3834722340106964, loss=1.6110235452651978
I0208 16:00:44.717617 139615954999040 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.4209190011024475, loss=1.7682950496673584
I0208 16:01:19.626745 139615963391744 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3612586259841919, loss=1.619472861289978
I0208 16:01:19.634890 139785736898368 spec.py:321] Evaluating on the training split.
I0208 16:01:22.354413 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:04:39.926284 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 16:04:42.645586 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:07:40.831192 139785736898368 spec.py:349] Evaluating on the test split.
I0208 16:07:43.550439 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:10:27.700961 139785736898368 submission_runner.py:408] Time since start: 40586.00s, 	Step: 69801, 	{'train/accuracy': 0.6611226201057434, 'train/loss': 1.6098320484161377, 'train/bleu': 32.63959978255694, 'validation/accuracy': 0.6719693541526794, 'validation/loss': 1.5329428911209106, 'validation/bleu': 29.068353792762746, 'validation/num_examples': 3000, 'test/accuracy': 0.684980571269989, 'test/loss': 1.4600976705551147, 'test/bleu': 29.0403300225907, 'test/num_examples': 3003, 'score': 24387.486208677292, 'total_duration': 40585.99910902977, 'accumulated_submission_time': 24387.486208677292, 'accumulated_eval_time': 16195.264653921127, 'accumulated_logging_time': 0.9868950843811035}
I0208 16:10:27.726857 139615954999040 logging_writer.py:48] [69801] accumulated_eval_time=16195.264654, accumulated_logging_time=0.986895, accumulated_submission_time=24387.486209, global_step=69801, preemption_count=0, score=24387.486209, test/accuracy=0.684981, test/bleu=29.040330, test/loss=1.460098, test/num_examples=3003, total_duration=40585.999109, train/accuracy=0.661123, train/bleu=32.639600, train/loss=1.609832, validation/accuracy=0.671969, validation/bleu=29.068354, validation/loss=1.532943, validation/num_examples=3000
I0208 16:11:02.574836 139615963391744 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.4355253577232361, loss=1.6950279474258423
I0208 16:11:37.411696 139615954999040 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.42066648602485657, loss=1.586082100868225
I0208 16:12:12.239838 139615963391744 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3553540110588074, loss=1.6299636363983154
I0208 16:12:47.067506 139615954999040 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.40089625120162964, loss=1.6377038955688477
I0208 16:13:21.901552 139615963391744 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3951646685600281, loss=1.6287472248077393
I0208 16:13:56.710605 139615954999040 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3965894281864166, loss=1.6136988401412964
I0208 16:14:31.577815 139615963391744 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.42288723587989807, loss=1.7345247268676758
I0208 16:15:06.388759 139615954999040 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3927513062953949, loss=1.6054785251617432
I0208 16:15:41.233270 139615963391744 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.40899091958999634, loss=1.6343649625778198
I0208 16:16:16.144375 139615954999040 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.45804494619369507, loss=1.6674652099609375
I0208 16:16:51.002673 139615963391744 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.362424373626709, loss=1.6538933515548706
I0208 16:17:25.851922 139615954999040 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3741424083709717, loss=1.635827660560608
I0208 16:18:00.692204 139615963391744 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.3696839511394501, loss=1.6510461568832397
I0208 16:18:35.532738 139615954999040 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.39079487323760986, loss=1.6642186641693115
I0208 16:19:10.364537 139615963391744 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.37095773220062256, loss=1.6231080293655396
I0208 16:19:45.226531 139615954999040 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.42311304807662964, loss=1.6827552318572998
I0208 16:20:20.091784 139615963391744 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3804018199443817, loss=1.6882926225662231
I0208 16:20:54.912132 139615954999040 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.3763349950313568, loss=1.648313045501709
I0208 16:21:29.717184 139615963391744 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.43483442068099976, loss=1.6258881092071533
I0208 16:22:04.593416 139615954999040 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.4009636342525482, loss=1.6511046886444092
I0208 16:22:39.450791 139615963391744 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3842964172363281, loss=1.5324528217315674
I0208 16:23:14.285619 139615954999040 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.39251938462257385, loss=1.7014728784561157
I0208 16:23:49.159402 139615963391744 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3874649703502655, loss=1.6394013166427612
I0208 16:24:24.021862 139615954999040 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3749188780784607, loss=1.687681794166565
I0208 16:24:27.945961 139785736898368 spec.py:321] Evaluating on the training split.
I0208 16:24:30.952571 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:28:05.938142 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 16:28:08.656876 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:30:37.596796 139785736898368 spec.py:349] Evaluating on the test split.
I0208 16:30:40.323936 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:32:57.338515 139785736898368 submission_runner.py:408] Time since start: 41935.64s, 	Step: 72213, 	{'train/accuracy': 0.6569823026657104, 'train/loss': 1.6434437036514282, 'train/bleu': 32.19100745654006, 'validation/accuracy': 0.6732092499732971, 'validation/loss': 1.5275042057037354, 'validation/bleu': 29.36287108991364, 'validation/num_examples': 3000, 'test/accuracy': 0.6867700815200806, 'test/loss': 1.4527868032455444, 'test/bleu': 29.133928657891733, 'test/num_examples': 3003, 'score': 25227.615775585175, 'total_duration': 41935.63665962219, 'accumulated_submission_time': 25227.615775585175, 'accumulated_eval_time': 16704.657161474228, 'accumulated_logging_time': 1.0241799354553223}
I0208 16:32:57.364257 139615963391744 logging_writer.py:48] [72213] accumulated_eval_time=16704.657161, accumulated_logging_time=1.024180, accumulated_submission_time=25227.615776, global_step=72213, preemption_count=0, score=25227.615776, test/accuracy=0.686770, test/bleu=29.133929, test/loss=1.452787, test/num_examples=3003, total_duration=41935.636660, train/accuracy=0.656982, train/bleu=32.191007, train/loss=1.643444, validation/accuracy=0.673209, validation/bleu=29.362871, validation/loss=1.527504, validation/num_examples=3000
I0208 16:33:28.077340 139615954999040 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3820115327835083, loss=1.5950372219085693
I0208 16:34:02.972618 139615963391744 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.46919679641723633, loss=1.6366671323776245
I0208 16:34:37.876710 139615954999040 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.41824591159820557, loss=1.6414791345596313
I0208 16:35:12.772614 139615963391744 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.39028993248939514, loss=1.5755125284194946
I0208 16:35:47.671329 139615954999040 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.417024165391922, loss=1.6523739099502563
I0208 16:36:22.555897 139615963391744 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3752959370613098, loss=1.6783561706542969
I0208 16:36:57.418960 139615954999040 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.41142287850379944, loss=1.6633226871490479
I0208 16:37:32.324860 139615963391744 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.38625186681747437, loss=1.6743332147598267
I0208 16:38:07.233290 139615954999040 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.4101901948451996, loss=1.6137745380401611
I0208 16:38:42.141107 139615963391744 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3751369118690491, loss=1.6473428010940552
I0208 16:39:17.024860 139615954999040 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.391167551279068, loss=1.6496018171310425
I0208 16:39:51.913653 139615963391744 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.4345740079879761, loss=1.7000072002410889
I0208 16:40:26.806557 139615954999040 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3786194622516632, loss=1.5976290702819824
I0208 16:41:01.704418 139615963391744 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.40227603912353516, loss=1.5926430225372314
I0208 16:41:36.602016 139615954999040 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.4178925156593323, loss=1.6713594198226929
I0208 16:42:11.498470 139615963391744 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3797086477279663, loss=1.688010334968567
I0208 16:42:46.379036 139615954999040 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3830333948135376, loss=1.6620094776153564
I0208 16:43:21.269915 139615963391744 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.4182969927787781, loss=1.683552861213684
I0208 16:43:56.160194 139615954999040 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3656102120876312, loss=1.644364595413208
I0208 16:44:31.028497 139615963391744 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.48215243220329285, loss=1.6495732069015503
I0208 16:45:05.936529 139615954999040 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3919386863708496, loss=1.6592103242874146
I0208 16:45:40.849004 139615963391744 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.37246912717819214, loss=1.633436679840088
I0208 16:46:15.763969 139615954999040 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.37041035294532776, loss=1.644034504890442
I0208 16:46:50.696902 139615963391744 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.38552361726760864, loss=1.5731370449066162
I0208 16:46:57.398383 139785736898368 spec.py:321] Evaluating on the training split.
I0208 16:47:00.408367 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:50:58.607625 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 16:51:01.324801 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:53:35.286275 139785736898368 spec.py:349] Evaluating on the test split.
I0208 16:53:38.003391 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 16:56:09.630710 139785736898368 submission_runner.py:408] Time since start: 43327.93s, 	Step: 74621, 	{'train/accuracy': 0.6536204218864441, 'train/loss': 1.6660853624343872, 'train/bleu': 32.52132858933652, 'validation/accuracy': 0.6733828186988831, 'validation/loss': 1.5218721628189087, 'validation/bleu': 29.318499071867286, 'validation/num_examples': 3000, 'test/accuracy': 0.6867700815200806, 'test/loss': 1.4440298080444336, 'test/bleu': 28.99523113730168, 'test/num_examples': 3003, 'score': 26067.56249308586, 'total_duration': 43327.928844451904, 'accumulated_submission_time': 26067.56249308586, 'accumulated_eval_time': 17256.88942360878, 'accumulated_logging_time': 1.0615234375}
I0208 16:56:09.656574 139615954999040 logging_writer.py:48] [74621] accumulated_eval_time=17256.889424, accumulated_logging_time=1.061523, accumulated_submission_time=26067.562493, global_step=74621, preemption_count=0, score=26067.562493, test/accuracy=0.686770, test/bleu=28.995231, test/loss=1.444030, test/num_examples=3003, total_duration=43327.928844, train/accuracy=0.653620, train/bleu=32.521329, train/loss=1.666085, validation/accuracy=0.673383, validation/bleu=29.318499, validation/loss=1.521872, validation/num_examples=3000
I0208 16:56:37.530008 139615963391744 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.38678815960884094, loss=1.652766227722168
I0208 16:57:12.417131 139615954999040 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.38056108355522156, loss=1.6933552026748657
I0208 16:57:47.273544 139615963391744 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3968656063079834, loss=1.581079125404358
I0208 16:58:22.186163 139615954999040 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.42564889788627625, loss=1.687241792678833
I0208 16:58:57.126011 139615963391744 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3965456783771515, loss=1.6152681112289429
I0208 16:59:32.015619 139615954999040 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.42580777406692505, loss=1.6094809770584106
I0208 17:00:06.943847 139615963391744 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.3865784704685211, loss=1.6291488409042358
I0208 17:00:41.823698 139615954999040 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.4205031991004944, loss=1.6023976802825928
I0208 17:01:16.700706 139615963391744 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.4053820073604584, loss=1.587201714515686
I0208 17:01:51.614805 139615954999040 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.38140711188316345, loss=1.6482658386230469
I0208 17:02:26.520568 139615963391744 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3683062493801117, loss=1.5792306661605835
I0208 17:03:01.443277 139615954999040 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.40566402673721313, loss=1.6354327201843262
I0208 17:03:36.346277 139615963391744 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.40496745705604553, loss=1.7307928800582886
I0208 17:04:11.245545 139615954999040 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.42749884724617004, loss=1.6938691139221191
I0208 17:04:46.123231 139615963391744 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.37248989939689636, loss=1.5524853467941284
I0208 17:05:21.024859 139615954999040 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.40155741572380066, loss=1.6371965408325195
I0208 17:05:55.905936 139615963391744 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.39569205045700073, loss=1.6751495599746704
I0208 17:06:30.806959 139615954999040 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.39971470832824707, loss=1.7402915954589844
I0208 17:07:05.705170 139615963391744 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.42003461718559265, loss=1.6053143739700317
I0208 17:07:40.594271 139615954999040 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3972167372703552, loss=1.6239709854125977
I0208 17:08:15.503175 139615963391744 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.400909423828125, loss=1.597256064414978
I0208 17:08:50.405666 139615954999040 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.39594829082489014, loss=1.6524453163146973
I0208 17:09:25.291486 139615963391744 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.4262550175189972, loss=1.7127797603607178
I0208 17:10:00.181900 139615954999040 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.4019474983215332, loss=1.6021126508712769
I0208 17:10:09.674141 139785736898368 spec.py:321] Evaluating on the training split.
I0208 17:10:12.673135 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:13:39.057482 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 17:13:41.773917 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:16:19.024169 139785736898368 spec.py:349] Evaluating on the test split.
I0208 17:16:21.730811 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:18:47.414103 139785736898368 submission_runner.py:408] Time since start: 44685.71s, 	Step: 77029, 	{'train/accuracy': 0.6623920798301697, 'train/loss': 1.6091350317001343, 'train/bleu': 33.60163001132954, 'validation/accuracy': 0.6767553687095642, 'validation/loss': 1.5119057893753052, 'validation/bleu': 29.60513743671253, 'validation/num_examples': 3000, 'test/accuracy': 0.6889199018478394, 'test/loss': 1.4393484592437744, 'test/bleu': 29.566770359025814, 'test/num_examples': 3003, 'score': 26907.49367928505, 'total_duration': 44685.712255477905, 'accumulated_submission_time': 26907.49367928505, 'accumulated_eval_time': 17774.62933588028, 'accumulated_logging_time': 1.0971648693084717}
I0208 17:18:47.440356 139615963391744 logging_writer.py:48] [77029] accumulated_eval_time=17774.629336, accumulated_logging_time=1.097165, accumulated_submission_time=26907.493679, global_step=77029, preemption_count=0, score=26907.493679, test/accuracy=0.688920, test/bleu=29.566770, test/loss=1.439348, test/num_examples=3003, total_duration=44685.712255, train/accuracy=0.662392, train/bleu=33.601630, train/loss=1.609135, validation/accuracy=0.676755, validation/bleu=29.605137, validation/loss=1.511906, validation/num_examples=3000
I0208 17:19:12.534793 139615954999040 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.386625736951828, loss=1.648982286453247
I0208 17:19:47.378825 139615963391744 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.426584929227829, loss=1.642979621887207
I0208 17:20:22.206176 139615954999040 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3917309641838074, loss=1.6363027095794678
I0208 17:20:57.058071 139615963391744 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.39343518018722534, loss=1.6557655334472656
I0208 17:21:31.904428 139615954999040 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.4014918804168701, loss=1.5950068235397339
I0208 17:22:06.746881 139615963391744 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.4284418523311615, loss=1.5547550916671753
I0208 17:22:41.681470 139615954999040 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.4083022177219391, loss=1.602686882019043
I0208 17:23:16.578849 139615963391744 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.41268593072891235, loss=1.62894868850708
I0208 17:23:51.466551 139615954999040 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3960719406604767, loss=1.7670761346817017
I0208 17:24:26.333526 139615963391744 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.41572731733322144, loss=1.6849606037139893
I0208 17:25:01.189055 139615954999040 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.40718957781791687, loss=1.6001085042953491
I0208 17:25:36.053447 139615963391744 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.4154931604862213, loss=1.6463325023651123
I0208 17:26:10.910271 139615954999040 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3968579173088074, loss=1.6607199907302856
I0208 17:26:45.734435 139615963391744 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.49394315481185913, loss=1.6460728645324707
I0208 17:27:20.592014 139615954999040 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.40016716718673706, loss=1.691247582435608
I0208 17:27:55.409177 139615963391744 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3848780691623688, loss=1.6866588592529297
I0208 17:28:30.261394 139615954999040 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4192497134208679, loss=1.6434507369995117
I0208 17:29:05.105099 139615963391744 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.40022972226142883, loss=1.6285085678100586
I0208 17:29:39.968585 139615954999040 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.39920133352279663, loss=1.6207544803619385
I0208 17:30:14.819465 139615963391744 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.4327804744243622, loss=1.6227874755859375
I0208 17:30:49.700726 139615954999040 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.4123009443283081, loss=1.618700623512268
I0208 17:31:24.584886 139615963391744 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.38365596532821655, loss=1.587631344795227
I0208 17:31:59.478563 139615954999040 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.4075988829135895, loss=1.5938225984573364
I0208 17:32:34.332103 139615963391744 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3841898441314697, loss=1.5935866832733154
I0208 17:32:47.654239 139785736898368 spec.py:321] Evaluating on the training split.
I0208 17:32:50.654983 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:36:09.583647 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 17:36:12.310506 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:38:46.243608 139785736898368 spec.py:349] Evaluating on the test split.
I0208 17:38:48.958238 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:41:25.551180 139785736898368 submission_runner.py:408] Time since start: 46043.85s, 	Step: 79440, 	{'train/accuracy': 0.6589401364326477, 'train/loss': 1.6313480138778687, 'train/bleu': 33.36118043719307, 'validation/accuracy': 0.6741020083427429, 'validation/loss': 1.5112824440002441, 'validation/bleu': 29.13616132874532, 'validation/num_examples': 3000, 'test/accuracy': 0.6889663934707642, 'test/loss': 1.430628776550293, 'test/bleu': 29.096602843757115, 'test/num_examples': 3003, 'score': 27747.6175262928, 'total_duration': 46043.84932875633, 'accumulated_submission_time': 27747.6175262928, 'accumulated_eval_time': 18292.526223659515, 'accumulated_logging_time': 1.133357286453247}
I0208 17:41:25.578572 139615954999040 logging_writer.py:48] [79440] accumulated_eval_time=18292.526224, accumulated_logging_time=1.133357, accumulated_submission_time=27747.617526, global_step=79440, preemption_count=0, score=27747.617526, test/accuracy=0.688966, test/bleu=29.096603, test/loss=1.430629, test/num_examples=3003, total_duration=46043.849329, train/accuracy=0.658940, train/bleu=33.361180, train/loss=1.631348, validation/accuracy=0.674102, validation/bleu=29.136161, validation/loss=1.511282, validation/num_examples=3000
I0208 17:41:46.875693 139615963391744 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.4282379448413849, loss=1.6093848943710327
I0208 17:42:21.787937 139615954999040 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.4011875092983246, loss=1.6142271757125854
I0208 17:42:56.696768 139615963391744 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.4226994812488556, loss=1.7020076513290405
I0208 17:43:31.596812 139615954999040 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.4559563100337982, loss=1.697798252105713
I0208 17:44:06.503024 139615963391744 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.4105055034160614, loss=1.614626407623291
I0208 17:44:41.374914 139615954999040 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.39528176188468933, loss=1.5757943391799927
I0208 17:45:16.301188 139615963391744 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.436001718044281, loss=1.7157344818115234
I0208 17:45:51.243645 139615954999040 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3857348561286926, loss=1.6052998304367065
I0208 17:46:26.165055 139615963391744 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.43952473998069763, loss=1.682320475578308
I0208 17:47:01.121893 139615954999040 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3983331620693207, loss=1.6248081922531128
I0208 17:47:36.009262 139615963391744 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.44028905034065247, loss=1.6325210332870483
I0208 17:48:10.903707 139615954999040 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.4082152843475342, loss=1.6239964962005615
I0208 17:48:45.830254 139615963391744 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.4480677843093872, loss=1.7016794681549072
I0208 17:49:20.722399 139615954999040 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.43566855788230896, loss=1.6652007102966309
I0208 17:49:55.607327 139615963391744 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.4205946922302246, loss=1.6176044940948486
I0208 17:50:30.512923 139615954999040 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.398799329996109, loss=1.5469250679016113
I0208 17:51:05.411106 139615963391744 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.4050320088863373, loss=1.6092250347137451
I0208 17:51:40.288063 139615954999040 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.4391607642173767, loss=1.6400351524353027
I0208 17:52:15.175299 139615963391744 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.42004984617233276, loss=1.583573341369629
I0208 17:52:50.087797 139615954999040 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3854002058506012, loss=1.5964664220809937
I0208 17:53:25.015471 139615963391744 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.4048640727996826, loss=1.5286450386047363
I0208 17:53:59.885186 139615954999040 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.38860243558883667, loss=1.598143458366394
I0208 17:54:34.787224 139615963391744 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.41071781516075134, loss=1.6330053806304932
I0208 17:55:09.708158 139615954999040 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4352942705154419, loss=1.6312743425369263
I0208 17:55:25.824144 139785736898368 spec.py:321] Evaluating on the training split.
I0208 17:55:28.825242 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 17:59:54.122124 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 17:59:56.824098 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:03:12.733314 139785736898368 spec.py:349] Evaluating on the test split.
I0208 18:03:15.441564 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:06:33.272617 139785736898368 submission_runner.py:408] Time since start: 47551.57s, 	Step: 81848, 	{'train/accuracy': 0.6777380108833313, 'train/loss': 1.5110163688659668, 'train/bleu': 33.80495966434508, 'validation/accuracy': 0.6777721047401428, 'validation/loss': 1.4969340562820435, 'validation/bleu': 29.833317296218926, 'validation/num_examples': 3000, 'test/accuracy': 0.6912091374397278, 'test/loss': 1.4248101711273193, 'test/bleu': 29.329210019172148, 'test/num_examples': 3003, 'score': 28587.77638578415, 'total_duration': 47551.57074832916, 'accumulated_submission_time': 28587.77638578415, 'accumulated_eval_time': 18959.974626541138, 'accumulated_logging_time': 1.171102523803711}
I0208 18:06:33.299663 139615963391744 logging_writer.py:48] [81848] accumulated_eval_time=18959.974627, accumulated_logging_time=1.171103, accumulated_submission_time=28587.776386, global_step=81848, preemption_count=0, score=28587.776386, test/accuracy=0.691209, test/bleu=29.329210, test/loss=1.424810, test/num_examples=3003, total_duration=47551.570748, train/accuracy=0.677738, train/bleu=33.804960, train/loss=1.511016, validation/accuracy=0.677772, validation/bleu=29.833317, validation/loss=1.496934, validation/num_examples=3000
I0208 18:06:51.754432 139615954999040 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4273742437362671, loss=1.6120022535324097
I0208 18:07:26.584174 139615963391744 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.40118372440338135, loss=1.5802099704742432
I0208 18:08:01.440577 139615954999040 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.4098259508609772, loss=1.671546459197998
I0208 18:08:36.310135 139615963391744 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.4190707802772522, loss=1.5556787252426147
I0208 18:09:11.204055 139615954999040 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.41507604718208313, loss=1.6911234855651855
I0208 18:09:46.085935 139615963391744 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4213508069515228, loss=1.6341791152954102
I0208 18:10:20.975501 139615954999040 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.42868897318840027, loss=1.6614736318588257
I0208 18:10:55.900967 139615963391744 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.39769574999809265, loss=1.560976505279541
I0208 18:11:30.773444 139615954999040 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.4078812599182129, loss=1.6210979223251343
I0208 18:12:05.688819 139615963391744 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.43244144320487976, loss=1.6134408712387085
I0208 18:12:40.550725 139615954999040 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.43107035756111145, loss=1.6378074884414673
I0208 18:13:15.424983 139615963391744 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.4041915833950043, loss=1.5990835428237915
I0208 18:13:50.293024 139615954999040 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.40984347462654114, loss=1.5358881950378418
I0208 18:14:25.218139 139615963391744 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.4508771002292633, loss=1.5971561670303345
I0208 18:15:00.100221 139615954999040 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.41757044196128845, loss=1.6418073177337646
I0208 18:15:34.990393 139615963391744 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.4205136299133301, loss=1.5454245805740356
I0208 18:16:09.917840 139615954999040 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.4053094685077667, loss=1.6069061756134033
I0208 18:16:44.788051 139615963391744 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.41282087564468384, loss=1.5948638916015625
I0208 18:17:19.724642 139615954999040 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.4510191082954407, loss=1.5972621440887451
I0208 18:17:54.648658 139615963391744 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.42431071400642395, loss=1.6682841777801514
I0208 18:18:29.555202 139615954999040 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.4205116927623749, loss=1.5701720714569092
I0208 18:19:04.438973 139615963391744 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.41279736161231995, loss=1.6751142740249634
I0208 18:19:39.311691 139615954999040 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.4216020405292511, loss=1.596013069152832
I0208 18:20:14.190664 139615963391744 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3956030309200287, loss=1.5416032075881958
I0208 18:20:33.467248 139785736898368 spec.py:321] Evaluating on the training split.
I0208 18:20:36.467973 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:23:53.298110 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 18:23:56.006635 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:26:28.253569 139785736898368 spec.py:349] Evaluating on the test split.
I0208 18:26:30.971360 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:28:53.687785 139785736898368 submission_runner.py:408] Time since start: 48891.99s, 	Step: 84257, 	{'train/accuracy': 0.6639957427978516, 'train/loss': 1.5895136594772339, 'train/bleu': 33.48077142570666, 'validation/accuracy': 0.6791732311248779, 'validation/loss': 1.4911015033721924, 'validation/bleu': 29.608461936647878, 'validation/num_examples': 3000, 'test/accuracy': 0.6941258907318115, 'test/loss': 1.4093471765518188, 'test/bleu': 29.714915994475234, 'test/num_examples': 3003, 'score': 29427.856281757355, 'total_duration': 48891.9859354496, 'accumulated_submission_time': 29427.856281757355, 'accumulated_eval_time': 19460.195112228394, 'accumulated_logging_time': 1.209423542022705}
I0208 18:28:53.714450 139615954999040 logging_writer.py:48] [84257] accumulated_eval_time=19460.195112, accumulated_logging_time=1.209424, accumulated_submission_time=29427.856282, global_step=84257, preemption_count=0, score=29427.856282, test/accuracy=0.694126, test/bleu=29.714916, test/loss=1.409347, test/num_examples=3003, total_duration=48891.985935, train/accuracy=0.663996, train/bleu=33.480771, train/loss=1.589514, validation/accuracy=0.679173, validation/bleu=29.608462, validation/loss=1.491102, validation/num_examples=3000
I0208 18:29:09.042213 139615963391744 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.4286719560623169, loss=1.6299476623535156
I0208 18:29:43.893002 139615954999040 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4132966101169586, loss=1.6232986450195312
I0208 18:30:18.831506 139615963391744 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.48365920782089233, loss=1.6209532022476196
I0208 18:30:53.722643 139615954999040 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.40737462043762207, loss=1.5145478248596191
I0208 18:31:28.631638 139615963391744 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.45509135723114014, loss=1.585068941116333
I0208 18:32:03.539474 139615954999040 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.4457857310771942, loss=1.6552448272705078
I0208 18:32:38.402911 139615963391744 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4322558045387268, loss=1.5975100994110107
I0208 18:33:13.288707 139615954999040 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3929615318775177, loss=1.5536473989486694
I0208 18:33:48.142066 139615963391744 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4325510561466217, loss=1.521686315536499
I0208 18:34:23.026624 139615954999040 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4290347695350647, loss=1.6243202686309814
I0208 18:34:57.887904 139615963391744 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.42443525791168213, loss=1.6341403722763062
I0208 18:35:32.873009 139615954999040 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.4068487882614136, loss=1.5663036108016968
I0208 18:36:07.811982 139615963391744 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.4330562651157379, loss=1.5828602313995361
I0208 18:36:42.696891 139615954999040 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.42318859696388245, loss=1.5835816860198975
I0208 18:37:17.610065 139615963391744 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.44916218519210815, loss=1.604846477508545
I0208 18:37:52.488372 139615954999040 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.47722986340522766, loss=1.5746656656265259
I0208 18:38:27.361770 139615963391744 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.4464278519153595, loss=1.6529664993286133
I0208 18:39:02.244482 139615954999040 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.5238686800003052, loss=1.7027379274368286
I0208 18:39:37.122788 139615963391744 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.4058462381362915, loss=1.6578108072280884
I0208 18:40:12.035870 139615954999040 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4406921863555908, loss=1.6110812425613403
I0208 18:40:46.917272 139615963391744 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.446350634098053, loss=1.5966976881027222
I0208 18:41:21.819014 139615954999040 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.445267915725708, loss=1.5396456718444824
I0208 18:41:56.760417 139615963391744 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.42082494497299194, loss=1.5953302383422852
I0208 18:42:31.645306 139615954999040 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.5871919989585876, loss=1.6100767850875854
I0208 18:42:54.033321 139785736898368 spec.py:321] Evaluating on the training split.
I0208 18:42:57.026636 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:46:53.785292 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 18:46:56.494201 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:49:33.163947 139785736898368 spec.py:349] Evaluating on the test split.
I0208 18:49:35.870147 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 18:52:11.627160 139785736898368 submission_runner.py:408] Time since start: 50289.93s, 	Step: 86666, 	{'train/accuracy': 0.6675595045089722, 'train/loss': 1.5793704986572266, 'train/bleu': 32.93528053933675, 'validation/accuracy': 0.6773629784584045, 'validation/loss': 1.487136960029602, 'validation/bleu': 29.583362696768006, 'validation/num_examples': 3000, 'test/accuracy': 0.693370521068573, 'test/loss': 1.4054758548736572, 'test/bleu': 29.517393962293482, 'test/num_examples': 3003, 'score': 30268.088964939117, 'total_duration': 50289.92531251907, 'accumulated_submission_time': 30268.088964939117, 'accumulated_eval_time': 20017.788903951645, 'accumulated_logging_time': 1.2460203170776367}
I0208 18:52:11.654325 139615963391744 logging_writer.py:48] [86666] accumulated_eval_time=20017.788904, accumulated_logging_time=1.246020, accumulated_submission_time=30268.088965, global_step=86666, preemption_count=0, score=30268.088965, test/accuracy=0.693371, test/bleu=29.517394, test/loss=1.405476, test/num_examples=3003, total_duration=50289.925313, train/accuracy=0.667560, train/bleu=32.935281, train/loss=1.579370, validation/accuracy=0.677363, validation/bleu=29.583363, validation/loss=1.487137, validation/num_examples=3000
I0208 18:52:23.862340 139615954999040 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.45467397570610046, loss=1.599766492843628
I0208 18:52:58.735482 139615963391744 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.45936039090156555, loss=1.5580958127975464
I0208 18:53:33.639582 139615954999040 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4554131329059601, loss=1.5992294549942017
I0208 18:54:08.517762 139615963391744 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4116479456424713, loss=1.5462629795074463
I0208 18:54:43.420274 139615954999040 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4178732931613922, loss=1.6245473623275757
I0208 18:55:18.309028 139615963391744 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.4365186393260956, loss=1.572625994682312
I0208 18:55:53.214016 139615954999040 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.42282551527023315, loss=1.5751612186431885
I0208 18:56:28.101713 139615963391744 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.44157201051712036, loss=1.6247960329055786
I0208 18:57:03.005357 139615954999040 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4500596523284912, loss=1.5818763971328735
I0208 18:57:37.943240 139615963391744 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.4342206120491028, loss=1.61393404006958
I0208 18:58:12.825232 139615954999040 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.4239273965358734, loss=1.619179368019104
I0208 18:58:47.706228 139615963391744 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.39962145686149597, loss=1.5852066278457642
I0208 18:59:22.585717 139615954999040 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4576227068901062, loss=1.6640336513519287
I0208 18:59:57.476701 139615963391744 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.44312191009521484, loss=1.5502314567565918
I0208 19:00:32.386484 139615954999040 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4376300573348999, loss=1.604280948638916
I0208 19:01:07.276038 139615963391744 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.440359890460968, loss=1.6034138202667236
I0208 19:01:42.182262 139615954999040 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.43482208251953125, loss=1.5084282159805298
I0208 19:02:17.094918 139615963391744 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4678051471710205, loss=1.654567837715149
I0208 19:02:51.980010 139615954999040 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.47932204604148865, loss=1.5557973384857178
I0208 19:03:26.855934 139615963391744 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.437153160572052, loss=1.597853660583496
I0208 19:04:01.736933 139615954999040 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.4394262731075287, loss=1.6178299188613892
I0208 19:04:36.617271 139615963391744 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4583292007446289, loss=1.6382193565368652
I0208 19:05:11.501422 139615954999040 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4198254942893982, loss=1.462428092956543
I0208 19:05:46.397134 139615963391744 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.45597684383392334, loss=1.5385839939117432
I0208 19:06:11.920647 139785736898368 spec.py:321] Evaluating on the training split.
I0208 19:06:14.917329 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:09:49.046596 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 19:09:51.749230 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:12:43.279108 139785736898368 spec.py:349] Evaluating on the test split.
I0208 19:12:45.992037 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:15:45.056873 139785736898368 submission_runner.py:408] Time since start: 51703.36s, 	Step: 89075, 	{'train/accuracy': 0.6773558855056763, 'train/loss': 1.5134402513504028, 'train/bleu': 33.889525427289215, 'validation/accuracy': 0.6815290451049805, 'validation/loss': 1.4774235486984253, 'validation/bleu': 29.90880797413811, 'validation/num_examples': 3000, 'test/accuracy': 0.696914792060852, 'test/loss': 1.3995591402053833, 'test/bleu': 29.432154247263547, 'test/num_examples': 3003, 'score': 31108.269670009613, 'total_duration': 51703.35502099991, 'accumulated_submission_time': 31108.269670009613, 'accumulated_eval_time': 20590.925078868866, 'accumulated_logging_time': 1.283271074295044}
I0208 19:15:45.083595 139615954999040 logging_writer.py:48] [89075] accumulated_eval_time=20590.925079, accumulated_logging_time=1.283271, accumulated_submission_time=31108.269670, global_step=89075, preemption_count=0, score=31108.269670, test/accuracy=0.696915, test/bleu=29.432154, test/loss=1.399559, test/num_examples=3003, total_duration=51703.355021, train/accuracy=0.677356, train/bleu=33.889525, train/loss=1.513440, validation/accuracy=0.681529, validation/bleu=29.908808, validation/loss=1.477424, validation/num_examples=3000
I0208 19:15:54.159234 139615963391744 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4524335265159607, loss=1.6241962909698486
I0208 19:16:29.017930 139615954999040 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.47041699290275574, loss=1.6380131244659424
I0208 19:17:03.891820 139615963391744 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.43460285663604736, loss=1.5814870595932007
I0208 19:17:38.778626 139615954999040 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.43067869544029236, loss=1.4748547077178955
I0208 19:18:13.669944 139615963391744 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.44503507018089294, loss=1.5547868013381958
I0208 19:18:48.590191 139615954999040 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.4547036588191986, loss=1.585332989692688
I0208 19:19:23.509182 139615963391744 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4519432783126831, loss=1.526497721672058
I0208 19:19:58.441243 139615954999040 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.4550359845161438, loss=1.5664958953857422
I0208 19:20:33.338188 139615963391744 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.45833054184913635, loss=1.5361237525939941
I0208 19:21:08.257285 139615954999040 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.46104058623313904, loss=1.5983515977859497
I0208 19:21:43.183057 139615963391744 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.46823859214782715, loss=1.6238279342651367
I0208 19:22:18.088752 139615954999040 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.47121259570121765, loss=1.590670108795166
I0208 19:22:52.979962 139615963391744 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.4577462077140808, loss=1.5626482963562012
I0208 19:23:27.865344 139615954999040 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4607281982898712, loss=1.617354154586792
I0208 19:24:02.761925 139615963391744 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4467320442199707, loss=1.5727006196975708
I0208 19:24:37.724795 139615954999040 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4310436248779297, loss=1.5949394702911377
I0208 19:25:12.649195 139615963391744 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.44302019476890564, loss=1.499936819076538
I0208 19:25:47.618838 139615954999040 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4370194971561432, loss=1.5692535638809204
I0208 19:26:22.519497 139615963391744 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4366256296634674, loss=1.5718889236450195
I0208 19:26:57.390401 139615954999040 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.43619683384895325, loss=1.6215863227844238
I0208 19:27:32.306036 139615963391744 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.45054447650909424, loss=1.5565822124481201
I0208 19:28:07.270958 139615954999040 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.45373308658599854, loss=1.565577507019043
I0208 19:28:42.213503 139615963391744 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.45984187722206116, loss=1.5855751037597656
I0208 19:29:17.091637 139615954999040 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.46333423256874084, loss=1.5218651294708252
I0208 19:29:45.075632 139785736898368 spec.py:321] Evaluating on the training split.
I0208 19:29:48.126305 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:33:49.437633 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 19:33:52.140322 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:36:30.589940 139785736898368 spec.py:349] Evaluating on the test split.
I0208 19:36:33.312817 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:39:04.275873 139785736898368 submission_runner.py:408] Time since start: 53102.57s, 	Step: 91482, 	{'train/accuracy': 0.6696231365203857, 'train/loss': 1.5658307075500488, 'train/bleu': 33.075913430571255, 'validation/accuracy': 0.6815414428710938, 'validation/loss': 1.4682722091674805, 'validation/bleu': 29.879511675630194, 'validation/num_examples': 3000, 'test/accuracy': 0.6981697678565979, 'test/loss': 1.38344144821167, 'test/bleu': 29.75321280222057, 'test/num_examples': 3003, 'score': 31948.175379037857, 'total_duration': 53102.57399082184, 'accumulated_submission_time': 31948.175379037857, 'accumulated_eval_time': 21150.12527155876, 'accumulated_logging_time': 1.319873571395874}
I0208 19:39:04.309697 139615963391744 logging_writer.py:48] [91482] accumulated_eval_time=21150.125272, accumulated_logging_time=1.319874, accumulated_submission_time=31948.175379, global_step=91482, preemption_count=0, score=31948.175379, test/accuracy=0.698170, test/bleu=29.753213, test/loss=1.383441, test/num_examples=3003, total_duration=53102.573991, train/accuracy=0.669623, train/bleu=33.075913, train/loss=1.565831, validation/accuracy=0.681541, validation/bleu=29.879512, validation/loss=1.468272, validation/num_examples=3000
I0208 19:39:10.946237 139615954999040 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.43177124857902527, loss=1.5592195987701416
I0208 19:39:45.834354 139615963391744 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4462653398513794, loss=1.495030164718628
I0208 19:40:20.758558 139615954999040 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.45903196930885315, loss=1.5902364253997803
I0208 19:40:55.657413 139615963391744 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4955778419971466, loss=1.5146076679229736
I0208 19:41:30.544689 139615954999040 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.45800504088401794, loss=1.5272455215454102
I0208 19:42:05.443613 139615963391744 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4859555959701538, loss=1.6263664960861206
I0208 19:42:40.315813 139615954999040 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.45232316851615906, loss=1.6115092039108276
I0208 19:43:15.270179 139615963391744 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.460116446018219, loss=1.551289439201355
I0208 19:43:50.155671 139615954999040 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.5268896818161011, loss=1.5293951034545898
I0208 19:44:25.067806 139615963391744 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4308937191963196, loss=1.515883207321167
I0208 19:44:59.954890 139615954999040 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.47062861919403076, loss=1.6150466203689575
I0208 19:45:34.839818 139615963391744 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.4868479371070862, loss=1.6426421403884888
I0208 19:46:09.723532 139615954999040 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4667741060256958, loss=1.5902032852172852
I0208 19:46:44.627158 139615963391744 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.46038520336151123, loss=1.552400827407837
I0208 19:47:19.517767 139615954999040 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4340631365776062, loss=1.5203710794448853
I0208 19:47:54.414433 139615963391744 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4355315566062927, loss=1.5851455926895142
I0208 19:48:29.313395 139615954999040 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.46862757205963135, loss=1.576894998550415
I0208 19:49:04.267556 139615963391744 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.49254560470581055, loss=1.616587519645691
I0208 19:49:39.160062 139615954999040 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4787578880786896, loss=1.5439984798431396
I0208 19:50:14.069737 139615963391744 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.513087809085846, loss=1.5462150573730469
I0208 19:50:48.978498 139615954999040 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.4439331591129303, loss=1.5342061519622803
I0208 19:51:23.880318 139615963391744 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.48638656735420227, loss=1.5911601781845093
I0208 19:51:58.770686 139615954999040 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.46666163206100464, loss=1.561561107635498
I0208 19:52:33.666636 139615963391744 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.48236018419265747, loss=1.4604402780532837
I0208 19:53:04.439327 139785736898368 spec.py:321] Evaluating on the training split.
I0208 19:53:07.444692 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:56:25.620123 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 19:56:28.334071 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 19:59:16.064877 139785736898368 spec.py:349] Evaluating on the test split.
I0208 19:59:18.771118 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:01:27.569306 139785736898368 submission_runner.py:408] Time since start: 54445.87s, 	Step: 93890, 	{'train/accuracy': 0.7072305679321289, 'train/loss': 1.355831503868103, 'train/bleu': 36.74364605152375, 'validation/accuracy': 0.6831037402153015, 'validation/loss': 1.466017246246338, 'validation/bleu': 30.11848958134699, 'validation/num_examples': 3000, 'test/accuracy': 0.697519063949585, 'test/loss': 1.383360743522644, 'test/bleu': 29.793031114372457, 'test/num_examples': 3003, 'score': 32788.21763634682, 'total_duration': 54445.867460012436, 'accumulated_submission_time': 32788.21763634682, 'accumulated_eval_time': 21653.255216360092, 'accumulated_logging_time': 1.3652818202972412}
I0208 20:01:27.597152 139615954999040 logging_writer.py:48] [93890] accumulated_eval_time=21653.255216, accumulated_logging_time=1.365282, accumulated_submission_time=32788.217636, global_step=93890, preemption_count=0, score=32788.217636, test/accuracy=0.697519, test/bleu=29.793031, test/loss=1.383361, test/num_examples=3003, total_duration=54445.867460, train/accuracy=0.707231, train/bleu=36.743646, train/loss=1.355832, validation/accuracy=0.683104, validation/bleu=30.118490, validation/loss=1.466017, validation/num_examples=3000
I0208 20:01:31.433784 139615963391744 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.45299032330513, loss=1.5531678199768066
I0208 20:02:06.318336 139615954999040 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4680507481098175, loss=1.5551966428756714
I0208 20:02:41.203625 139615963391744 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.47528642416000366, loss=1.5394198894500732
I0208 20:03:16.083437 139615954999040 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4905776083469391, loss=1.6172432899475098
I0208 20:03:50.975684 139615963391744 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.48377367854118347, loss=1.5822100639343262
I0208 20:04:25.860047 139615954999040 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4855624735355377, loss=1.6260124444961548
I0208 20:05:00.797772 139615963391744 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4598641097545624, loss=1.5612226724624634
I0208 20:05:35.760130 139615954999040 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.47298985719680786, loss=1.5439764261245728
I0208 20:06:10.642661 139615963391744 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.47846826910972595, loss=1.5227973461151123
I0208 20:06:45.535728 139615954999040 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.48904600739479065, loss=1.477189302444458
I0208 20:07:20.413599 139615963391744 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.46297216415405273, loss=1.5275753736495972
I0208 20:07:55.316247 139615954999040 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.4886837601661682, loss=1.5960506200790405
I0208 20:08:30.203048 139615963391744 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.4874976575374603, loss=1.548195242881775
I0208 20:09:05.070295 139615954999040 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.46217983961105347, loss=1.505886435508728
I0208 20:09:39.975231 139615963391744 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.47679033875465393, loss=1.5413163900375366
I0208 20:10:14.877224 139615954999040 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.5147644877433777, loss=1.4700431823730469
I0208 20:10:49.779966 139615963391744 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.46655210852622986, loss=1.5391085147857666
I0208 20:11:24.672849 139615954999040 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.4847710132598877, loss=1.5697611570358276
I0208 20:11:59.545495 139615963391744 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.5017067790031433, loss=1.6417216062545776
I0208 20:12:34.425447 139615954999040 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.5275569558143616, loss=1.598171591758728
I0208 20:13:09.313273 139615963391744 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.4853087067604065, loss=1.619020938873291
I0208 20:13:44.229405 139615954999040 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.48845091462135315, loss=1.485229253768921
I0208 20:14:19.116021 139615963391744 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.488423615694046, loss=1.6039400100708008
I0208 20:14:54.018706 139615954999040 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5148200988769531, loss=1.5222936868667603
I0208 20:15:27.908908 139785736898368 spec.py:321] Evaluating on the training split.
I0208 20:15:30.916250 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:19:10.172852 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 20:19:12.883003 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:21:58.195998 139785736898368 spec.py:349] Evaluating on the test split.
I0208 20:22:00.904957 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:24:48.265300 139785736898368 submission_runner.py:408] Time since start: 55846.56s, 	Step: 96299, 	{'train/accuracy': 0.6774893403053284, 'train/loss': 1.5120842456817627, 'train/bleu': 34.065012769999825, 'validation/accuracy': 0.6842816472053528, 'validation/loss': 1.452805519104004, 'validation/bleu': 30.26416928198846, 'validation/num_examples': 3000, 'test/accuracy': 0.6997618079185486, 'test/loss': 1.3693721294403076, 'test/bleu': 30.24534871633443, 'test/num_examples': 3003, 'score': 33628.44213843346, 'total_duration': 55846.56341743469, 'accumulated_submission_time': 33628.44213843346, 'accumulated_eval_time': 22213.611531734467, 'accumulated_logging_time': 1.4041471481323242}
I0208 20:24:48.293657 139615963391744 logging_writer.py:48] [96299] accumulated_eval_time=22213.611532, accumulated_logging_time=1.404147, accumulated_submission_time=33628.442138, global_step=96299, preemption_count=0, score=33628.442138, test/accuracy=0.699762, test/bleu=30.245349, test/loss=1.369372, test/num_examples=3003, total_duration=55846.563417, train/accuracy=0.677489, train/bleu=34.065013, train/loss=1.512084, validation/accuracy=0.684282, validation/bleu=30.264169, validation/loss=1.452806, validation/num_examples=3000
I0208 20:24:49.011946 139615954999040 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.4879642128944397, loss=1.5681992769241333
I0208 20:25:23.878588 139615963391744 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5050989985466003, loss=1.6200650930404663
I0208 20:25:58.749204 139615954999040 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.4888191521167755, loss=1.5396311283111572
I0208 20:26:33.633727 139615963391744 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.46636608242988586, loss=1.5146489143371582
I0208 20:27:08.533447 139615954999040 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.4999462962150574, loss=1.520174264907837
I0208 20:27:43.450329 139615963391744 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5104869604110718, loss=1.5294936895370483
I0208 20:28:18.340767 139615954999040 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.47372955083847046, loss=1.52788245677948
I0208 20:28:53.201164 139615963391744 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.504136860370636, loss=1.5716865062713623
I0208 20:29:28.062968 139615954999040 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.5019866228103638, loss=1.4435765743255615
I0208 20:30:02.960623 139615963391744 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.4980017840862274, loss=1.5662020444869995
I0208 20:30:37.908254 139615954999040 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.49146074056625366, loss=1.5594234466552734
I0208 20:31:12.817393 139615963391744 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5191200971603394, loss=1.556056022644043
I0208 20:31:47.772113 139615954999040 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.4832569360733032, loss=1.4973562955856323
I0208 20:32:22.715111 139615963391744 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.4697294533252716, loss=1.4824661016464233
I0208 20:32:57.616417 139615954999040 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.48910853266716003, loss=1.5213958024978638
I0208 20:33:32.490944 139615963391744 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5137028694152832, loss=1.4675099849700928
I0208 20:34:07.394689 139615954999040 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.4980260133743286, loss=1.5206700563430786
I0208 20:34:42.287852 139615963391744 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.48510807752609253, loss=1.4663447141647339
I0208 20:35:17.167466 139615954999040 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5057560801506042, loss=1.4405450820922852
I0208 20:35:52.051356 139615963391744 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.47369447350502014, loss=1.488263726234436
I0208 20:36:26.937170 139615954999040 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5146802663803101, loss=1.581059217453003
I0208 20:37:01.814064 139615963391744 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.49024805426597595, loss=1.4448728561401367
I0208 20:37:36.727128 139615954999040 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5330457091331482, loss=1.507541298866272
I0208 20:38:11.612337 139615963391744 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.6984753608703613, loss=1.5058366060256958
I0208 20:38:46.509033 139615954999040 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.49390730261802673, loss=1.5351742506027222
I0208 20:38:48.331648 139785736898368 spec.py:321] Evaluating on the training split.
I0208 20:38:51.335850 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:42:39.635158 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 20:42:42.338273 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:45:17.543714 139785736898368 spec.py:349] Evaluating on the test split.
I0208 20:45:20.255667 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 20:47:41.662700 139785736898368 submission_runner.py:408] Time since start: 57219.96s, 	Step: 98707, 	{'train/accuracy': 0.6742954254150391, 'train/loss': 1.5320757627487183, 'train/bleu': 34.28313749510387, 'validation/accuracy': 0.6845172047615051, 'validation/loss': 1.4518380165100098, 'validation/bleu': 30.17301385143447, 'validation/num_examples': 3000, 'test/accuracy': 0.6995061635971069, 'test/loss': 1.36750328540802, 'test/bleu': 30.188361116708307, 'test/num_examples': 3003, 'score': 34468.39393520355, 'total_duration': 57219.96085047722, 'accumulated_submission_time': 34468.39393520355, 'accumulated_eval_time': 22746.94253396988, 'accumulated_logging_time': 1.4424107074737549}
I0208 20:47:41.692477 139615963391744 logging_writer.py:48] [98707] accumulated_eval_time=22746.942534, accumulated_logging_time=1.442411, accumulated_submission_time=34468.393935, global_step=98707, preemption_count=0, score=34468.393935, test/accuracy=0.699506, test/bleu=30.188361, test/loss=1.367503, test/num_examples=3003, total_duration=57219.960850, train/accuracy=0.674295, train/bleu=34.283137, train/loss=1.532076, validation/accuracy=0.684517, validation/bleu=30.173014, validation/loss=1.451838, validation/num_examples=3000
I0208 20:48:14.473848 139615954999040 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5216345191001892, loss=1.5233979225158691
I0208 20:48:49.348754 139615963391744 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5274470448493958, loss=1.521781086921692
I0208 20:49:24.254040 139615954999040 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5039681196212769, loss=1.571658968925476
I0208 20:49:59.146886 139615963391744 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5024124383926392, loss=1.461979866027832
I0208 20:50:34.007785 139615954999040 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.49094077944755554, loss=1.4886268377304077
I0208 20:51:08.887754 139615963391744 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5108083486557007, loss=1.551487684249878
I0208 20:51:43.749644 139615954999040 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5108017325401306, loss=1.510148525238037
I0208 20:52:18.657682 139615963391744 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.536797285079956, loss=1.5400737524032593
I0208 20:52:53.551498 139615954999040 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.49493515491485596, loss=1.4480637311935425
I0208 20:53:28.466624 139615963391744 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5260759592056274, loss=1.5314459800720215
I0208 20:54:03.365921 139615954999040 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5170203447341919, loss=1.5068944692611694
I0208 20:54:38.284258 139615963391744 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5012482404708862, loss=1.519290566444397
I0208 20:55:13.199109 139615954999040 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5263907313346863, loss=1.5166889429092407
I0208 20:55:48.113766 139615963391744 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5319492816925049, loss=1.5888698101043701
I0208 20:56:23.000293 139615954999040 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5084001421928406, loss=1.4625823497772217
I0208 20:56:57.915441 139615963391744 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5095968842506409, loss=1.5177233219146729
I0208 20:57:32.814137 139615954999040 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5251903533935547, loss=1.5526518821716309
I0208 20:58:07.771237 139615963391744 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5323514342308044, loss=1.5168203115463257
I0208 20:58:42.660955 139615954999040 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5064181685447693, loss=1.4985188245773315
I0208 20:59:17.553576 139615963391744 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5130672454833984, loss=1.3977810144424438
I0208 20:59:52.473768 139615954999040 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5387400984764099, loss=1.544826626777649
I0208 21:00:27.404055 139615963391744 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5214521288871765, loss=1.5522150993347168
I0208 21:01:02.285835 139615954999040 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5186317563056946, loss=1.4356672763824463
I0208 21:01:37.170336 139615963391744 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5323899984359741, loss=1.4957895278930664
I0208 21:01:41.788352 139785736898368 spec.py:321] Evaluating on the training split.
I0208 21:01:44.796898 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:05:07.489043 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 21:05:10.205284 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:07:52.700305 139785736898368 spec.py:349] Evaluating on the test split.
I0208 21:07:55.427261 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:10:31.732724 139785736898368 submission_runner.py:408] Time since start: 58590.03s, 	Step: 101115, 	{'train/accuracy': 0.6891303658485413, 'train/loss': 1.4438997507095337, 'train/bleu': 34.77914901250355, 'validation/accuracy': 0.6863027215003967, 'validation/loss': 1.444441556930542, 'validation/bleu': 30.29630986323259, 'validation/num_examples': 3000, 'test/accuracy': 0.701225996017456, 'test/loss': 1.3644455671310425, 'test/bleu': 30.217966551902926, 'test/num_examples': 3003, 'score': 35308.40327715874, 'total_duration': 58590.03084850311, 'accumulated_submission_time': 35308.40327715874, 'accumulated_eval_time': 23276.88682627678, 'accumulated_logging_time': 1.4825043678283691}
I0208 21:10:31.768604 139615954999040 logging_writer.py:48] [101115] accumulated_eval_time=23276.886826, accumulated_logging_time=1.482504, accumulated_submission_time=35308.403277, global_step=101115, preemption_count=0, score=35308.403277, test/accuracy=0.701226, test/bleu=30.217967, test/loss=1.364446, test/num_examples=3003, total_duration=58590.030849, train/accuracy=0.689130, train/bleu=34.779149, train/loss=1.443900, validation/accuracy=0.686303, validation/bleu=30.296310, validation/loss=1.444442, validation/num_examples=3000
I0208 21:11:01.813295 139615963391744 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.49656447768211365, loss=1.426123857498169
I0208 21:11:36.729513 139615954999040 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.501950740814209, loss=1.484155297279358
I0208 21:12:11.610712 139615963391744 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5178090929985046, loss=1.478729248046875
I0208 21:12:46.522593 139615954999040 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5489637851715088, loss=1.5477882623672485
I0208 21:13:21.421339 139615963391744 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5333476662635803, loss=1.5351650714874268
I0208 21:13:56.328196 139615954999040 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5506648421287537, loss=1.4884424209594727
I0208 21:14:31.257128 139615963391744 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.531317412853241, loss=1.4697155952453613
I0208 21:15:06.216199 139615954999040 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5445062518119812, loss=1.5415668487548828
I0208 21:15:41.132320 139615963391744 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5301042795181274, loss=1.4904471635818481
I0208 21:16:16.018777 139615954999040 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5280742645263672, loss=1.4574693441390991
I0208 21:16:50.929659 139615963391744 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5703451633453369, loss=1.5603138208389282
I0208 21:17:25.827268 139615954999040 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5550031065940857, loss=1.5413247346878052
I0208 21:18:00.734415 139615963391744 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5231952667236328, loss=1.4601374864578247
I0208 21:18:35.644860 139615954999040 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5235148072242737, loss=1.458622932434082
I0208 21:19:10.523438 139615963391744 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5414095520973206, loss=1.4699018001556396
I0208 21:19:45.466527 139615954999040 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5472414493560791, loss=1.4851089715957642
I0208 21:20:20.418936 139615963391744 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5385293960571289, loss=1.4748647212982178
I0208 21:20:55.330117 139615954999040 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5705689787864685, loss=1.5669561624526978
I0208 21:21:30.243774 139615963391744 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5443553328514099, loss=1.5345357656478882
I0208 21:22:05.165086 139615954999040 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5376640558242798, loss=1.5392723083496094
I0208 21:22:40.097432 139615963391744 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5400702953338623, loss=1.4418694972991943
I0208 21:23:15.052764 139615954999040 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5762321949005127, loss=1.5347625017166138
I0208 21:23:49.987712 139615963391744 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5861596465110779, loss=1.5657482147216797
I0208 21:24:24.878011 139615954999040 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5369538068771362, loss=1.4033269882202148
I0208 21:24:31.934608 139785736898368 spec.py:321] Evaluating on the training split.
I0208 21:24:34.942006 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:28:16.354417 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 21:28:19.059724 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:30:46.671661 139785736898368 spec.py:349] Evaluating on the test split.
I0208 21:30:49.394546 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:33:20.845268 139785736898368 submission_runner.py:408] Time since start: 59959.14s, 	Step: 103522, 	{'train/accuracy': 0.6818674206733704, 'train/loss': 1.4904634952545166, 'train/bleu': 34.39548551799863, 'validation/accuracy': 0.6874682307243347, 'validation/loss': 1.439511775970459, 'validation/bleu': 30.242459944963176, 'validation/num_examples': 3000, 'test/accuracy': 0.7032827734947205, 'test/loss': 1.3556402921676636, 'test/bleu': 30.08292498174387, 'test/num_examples': 3003, 'score': 36148.475451231, 'total_duration': 59959.14341711998, 'accumulated_submission_time': 36148.475451231, 'accumulated_eval_time': 23805.797435045242, 'accumulated_logging_time': 1.5310685634613037}
I0208 21:33:20.874752 139615963391744 logging_writer.py:48] [103522] accumulated_eval_time=23805.797435, accumulated_logging_time=1.531069, accumulated_submission_time=36148.475451, global_step=103522, preemption_count=0, score=36148.475451, test/accuracy=0.703283, test/bleu=30.082925, test/loss=1.355640, test/num_examples=3003, total_duration=59959.143417, train/accuracy=0.681867, train/bleu=34.395486, train/loss=1.490463, validation/accuracy=0.687468, validation/bleu=30.242460, validation/loss=1.439512, validation/num_examples=3000
I0208 21:33:48.454983 139615954999040 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5536622405052185, loss=1.4797903299331665
I0208 21:34:23.372110 139615963391744 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5468204617500305, loss=1.4603841304779053
I0208 21:34:58.300432 139615954999040 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5440091490745544, loss=1.5513081550598145
I0208 21:35:33.189221 139615963391744 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5678917169570923, loss=1.4373503923416138
I0208 21:36:08.102993 139615954999040 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5329554677009583, loss=1.501604437828064
I0208 21:36:43.008249 139615963391744 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5441607236862183, loss=1.4807993173599243
I0208 21:37:17.904856 139615954999040 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.6087641716003418, loss=1.592126727104187
I0208 21:37:52.804817 139615963391744 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5760912895202637, loss=1.4082403182983398
I0208 21:38:27.730870 139615954999040 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5527486205101013, loss=1.5066179037094116
I0208 21:39:02.628031 139615963391744 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5634142160415649, loss=1.4931493997573853
I0208 21:39:37.519645 139615954999040 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5751923322677612, loss=1.4479273557662964
I0208 21:40:12.457581 139615963391744 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.5549719929695129, loss=1.473127841949463
I0208 21:40:47.358487 139615954999040 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.595073401927948, loss=1.428029179573059
I0208 21:41:22.268363 139615963391744 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5651464462280273, loss=1.4798356294631958
I0208 21:41:57.154566 139615954999040 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5795882940292358, loss=1.4837232828140259
I0208 21:42:32.061153 139615963391744 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5667645931243896, loss=1.5389641523361206
I0208 21:43:06.991210 139615954999040 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.5530455708503723, loss=1.448649287223816
I0208 21:43:41.904531 139615963391744 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5569980144500732, loss=1.5294321775436401
I0208 21:44:16.773868 139615954999040 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5593757033348083, loss=1.43839430809021
I0208 21:44:51.662908 139615963391744 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5915107131004333, loss=1.5097308158874512
I0208 21:45:26.572863 139615954999040 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5891068577766418, loss=1.4829586744308472
I0208 21:46:01.474197 139615963391744 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5990949273109436, loss=1.490713357925415
I0208 21:46:36.384770 139615954999040 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.6175260543823242, loss=1.5250732898712158
I0208 21:47:11.312558 139615963391744 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5621019601821899, loss=1.4414246082305908
I0208 21:47:21.154271 139785736898368 spec.py:321] Evaluating on the training split.
I0208 21:47:24.146661 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:50:52.025133 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 21:50:54.740799 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:53:28.263836 139785736898368 spec.py:349] Evaluating on the test split.
I0208 21:53:30.966743 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 21:55:59.116780 139785736898368 submission_runner.py:408] Time since start: 61317.41s, 	Step: 105930, 	{'train/accuracy': 0.6841477751731873, 'train/loss': 1.4826703071594238, 'train/bleu': 34.1736090872612, 'validation/accuracy': 0.68779057264328, 'validation/loss': 1.4393343925476074, 'validation/bleu': 30.294645014237727, 'validation/num_examples': 3000, 'test/accuracy': 0.7045843005180359, 'test/loss': 1.3529555797576904, 'test/bleu': 30.341159836823227, 'test/num_examples': 3003, 'score': 36988.668182611465, 'total_duration': 61317.41492795944, 'accumulated_submission_time': 36988.668182611465, 'accumulated_eval_time': 24323.759889364243, 'accumulated_logging_time': 1.5710361003875732}
I0208 21:55:59.149451 139615954999040 logging_writer.py:48] [105930] accumulated_eval_time=24323.759889, accumulated_logging_time=1.571036, accumulated_submission_time=36988.668183, global_step=105930, preemption_count=0, score=36988.668183, test/accuracy=0.704584, test/bleu=30.341160, test/loss=1.352956, test/num_examples=3003, total_duration=61317.414928, train/accuracy=0.684148, train/bleu=34.173609, train/loss=1.482670, validation/accuracy=0.687791, validation/bleu=30.294645, validation/loss=1.439334, validation/num_examples=3000
I0208 21:56:23.931504 139615963391744 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5901056528091431, loss=1.507234811782837
I0208 21:56:58.838348 139615954999040 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.579568088054657, loss=1.4584590196609497
I0208 21:57:33.783968 139615963391744 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.5795902609825134, loss=1.4234764575958252
I0208 21:58:08.751234 139615954999040 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5927990078926086, loss=1.4126834869384766
I0208 21:58:43.696185 139615963391744 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5931562781333923, loss=1.4050941467285156
I0208 21:59:18.631985 139615954999040 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.57085120677948, loss=1.3788673877716064
I0208 21:59:53.575522 139615963391744 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.6042445302009583, loss=1.4981549978256226
I0208 22:00:28.473226 139615954999040 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.60743248462677, loss=1.4638991355895996
I0208 22:01:03.374947 139615963391744 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5979392528533936, loss=1.4798052310943604
I0208 22:01:38.308734 139615954999040 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5866379737854004, loss=1.4972076416015625
I0208 22:02:13.220279 139615963391744 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.6128050088882446, loss=1.4524954557418823
I0208 22:02:48.122026 139615954999040 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.6055871248245239, loss=1.4597852230072021
I0208 22:03:23.042409 139615963391744 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.6041274070739746, loss=1.5296608209609985
I0208 22:03:57.954187 139615954999040 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.5725041031837463, loss=1.3968273401260376
I0208 22:04:32.829348 139615963391744 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.6259487867355347, loss=1.47389817237854
I0208 22:05:07.734705 139615954999040 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.6118694543838501, loss=1.4889707565307617
I0208 22:05:42.617253 139615963391744 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.5707148313522339, loss=1.4553532600402832
I0208 22:06:17.543101 139615954999040 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.5729144215583801, loss=1.4150701761245728
I0208 22:06:52.518780 139615963391744 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.641754150390625, loss=1.511311650276184
I0208 22:07:27.432438 139615954999040 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.6190090179443359, loss=1.5240448713302612
I0208 22:08:02.347853 139615963391744 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.5944254994392395, loss=1.4922047853469849
I0208 22:08:37.250141 139615954999040 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.5882482528686523, loss=1.5041149854660034
I0208 22:09:12.124939 139615963391744 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.6215341687202454, loss=1.3892245292663574
I0208 22:09:47.058594 139615954999040 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.5919660925865173, loss=1.4165164232254028
I0208 22:09:59.338294 139785736898368 spec.py:321] Evaluating on the training split.
I0208 22:10:02.355808 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:13:52.907118 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 22:13:55.628263 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:16:20.773496 139785736898368 spec.py:349] Evaluating on the test split.
I0208 22:16:23.500612 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:18:46.401627 139785736898368 submission_runner.py:408] Time since start: 62684.70s, 	Step: 108337, 	{'train/accuracy': 0.6883156895637512, 'train/loss': 1.451397180557251, 'train/bleu': 35.21473145739453, 'validation/accuracy': 0.6892040967941284, 'validation/loss': 1.4314101934432983, 'validation/bleu': 30.54711567710777, 'validation/num_examples': 3000, 'test/accuracy': 0.7040265202522278, 'test/loss': 1.3469500541687012, 'test/bleu': 30.276564514231993, 'test/num_examples': 3003, 'score': 37828.766984939575, 'total_duration': 62684.69976902008, 'accumulated_submission_time': 37828.766984939575, 'accumulated_eval_time': 24850.823167324066, 'accumulated_logging_time': 1.614072561264038}
I0208 22:18:46.432074 139615963391744 logging_writer.py:48] [108337] accumulated_eval_time=24850.823167, accumulated_logging_time=1.614073, accumulated_submission_time=37828.766985, global_step=108337, preemption_count=0, score=37828.766985, test/accuracy=0.704027, test/bleu=30.276565, test/loss=1.346950, test/num_examples=3003, total_duration=62684.699769, train/accuracy=0.688316, train/bleu=35.214731, train/loss=1.451397, validation/accuracy=0.689204, validation/bleu=30.547116, validation/loss=1.431410, validation/num_examples=3000
I0208 22:19:08.749570 139615954999040 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.6155515909194946, loss=1.4577585458755493
I0208 22:19:43.684074 139615963391744 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6220002174377441, loss=1.4778048992156982
I0208 22:20:18.557953 139615954999040 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.6318195462226868, loss=1.4856940507888794
I0208 22:20:53.469372 139615963391744 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.6330786943435669, loss=1.5464946031570435
I0208 22:21:28.422017 139615954999040 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.6211092472076416, loss=1.4513401985168457
I0208 22:22:03.320740 139615963391744 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6168086528778076, loss=1.5107686519622803
I0208 22:22:38.226490 139615954999040 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6223478317260742, loss=1.4326850175857544
I0208 22:23:13.107256 139615963391744 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6279810070991516, loss=1.400724172592163
I0208 22:23:48.001140 139615954999040 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.621951162815094, loss=1.4547117948532104
I0208 22:24:22.916058 139615963391744 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.6395249962806702, loss=1.4435569047927856
I0208 22:24:57.818810 139615954999040 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.6517261266708374, loss=1.5141695737838745
I0208 22:25:32.734997 139615963391744 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6229255199432373, loss=1.5035643577575684
I0208 22:26:07.643948 139615954999040 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6323245167732239, loss=1.4237425327301025
I0208 22:26:42.541913 139615963391744 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.622451663017273, loss=1.3766775131225586
I0208 22:27:17.440683 139615954999040 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.7248091697692871, loss=1.4780973196029663
I0208 22:27:52.357806 139615963391744 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6347108483314514, loss=1.529384732246399
I0208 22:28:27.253739 139615954999040 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6184910535812378, loss=1.5059139728546143
I0208 22:29:02.134127 139615963391744 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6145365834236145, loss=1.4309535026550293
I0208 22:29:37.032061 139615954999040 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6283459663391113, loss=1.4811595678329468
I0208 22:30:11.976014 139615963391744 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.636893630027771, loss=1.4291108846664429
I0208 22:30:46.897740 139615954999040 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6550468802452087, loss=1.4618288278579712
I0208 22:31:21.795854 139615963391744 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6328282952308655, loss=1.4894089698791504
I0208 22:31:56.695921 139615954999040 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6430636048316956, loss=1.4671789407730103
I0208 22:32:31.594795 139615963391744 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.613560676574707, loss=1.4793891906738281
I0208 22:32:46.672801 139785736898368 spec.py:321] Evaluating on the training split.
I0208 22:32:49.682001 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:36:12.349506 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 22:36:15.053331 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:38:39.052958 139785736898368 spec.py:349] Evaluating on the test split.
I0208 22:38:41.772601 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:41:19.300293 139785736898368 submission_runner.py:408] Time since start: 64037.60s, 	Step: 110745, 	{'train/accuracy': 0.6906625032424927, 'train/loss': 1.4407415390014648, 'train/bleu': 35.11437290724362, 'validation/accuracy': 0.6892908811569214, 'validation/loss': 1.4316939115524292, 'validation/bleu': 30.623534904601236, 'validation/num_examples': 3000, 'test/accuracy': 0.7047585844993591, 'test/loss': 1.3457623720169067, 'test/bleu': 30.64520647586331, 'test/num_examples': 3003, 'score': 38668.92079091072, 'total_duration': 64037.598447322845, 'accumulated_submission_time': 38668.92079091072, 'accumulated_eval_time': 25363.45061326027, 'accumulated_logging_time': 1.6543505191802979}
I0208 22:41:19.330796 139615954999040 logging_writer.py:48] [110745] accumulated_eval_time=25363.450613, accumulated_logging_time=1.654351, accumulated_submission_time=38668.920791, global_step=110745, preemption_count=0, score=38668.920791, test/accuracy=0.704759, test/bleu=30.645206, test/loss=1.345762, test/num_examples=3003, total_duration=64037.598447, train/accuracy=0.690663, train/bleu=35.114373, train/loss=1.440742, validation/accuracy=0.689291, validation/bleu=30.623535, validation/loss=1.431694, validation/num_examples=3000
I0208 22:41:38.860119 139615963391744 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.6540475487709045, loss=1.4410369396209717
I0208 22:42:13.757025 139615954999040 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.6193764209747314, loss=1.4281213283538818
I0208 22:42:48.679160 139615963391744 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.6607522368431091, loss=1.467879295349121
I0208 22:43:23.602574 139615954999040 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6658303141593933, loss=1.4248813390731812
I0208 22:43:58.514238 139615963391744 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6495052576065063, loss=1.4233195781707764
I0208 22:44:33.421703 139615954999040 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.6481209993362427, loss=1.382140874862671
I0208 22:45:08.344801 139615963391744 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.6403802037239075, loss=1.4769731760025024
I0208 22:45:43.246003 139615954999040 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6262645721435547, loss=1.459312915802002
I0208 22:46:18.130584 139615963391744 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6262015700340271, loss=1.4706549644470215
I0208 22:46:53.018033 139615954999040 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.6414371132850647, loss=1.3414084911346436
I0208 22:47:27.909520 139615963391744 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.7117993831634521, loss=1.5057618618011475
I0208 22:48:02.828871 139615954999040 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.6385907530784607, loss=1.5064295530319214
I0208 22:48:37.725711 139615963391744 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6619836688041687, loss=1.4618747234344482
I0208 22:49:12.625570 139615954999040 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.638888418674469, loss=1.447853922843933
I0208 22:49:47.531415 139615963391744 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6637810468673706, loss=1.5161112546920776
I0208 22:50:22.434481 139615954999040 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6700416207313538, loss=1.4555432796478271
I0208 22:50:57.345802 139615963391744 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.6409612894058228, loss=1.368260145187378
I0208 22:51:32.212404 139615954999040 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6813769340515137, loss=1.491230845451355
I0208 22:52:07.117693 139615963391744 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.6635909676551819, loss=1.3633297681808472
I0208 22:52:41.991628 139615954999040 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6670023798942566, loss=1.4334911108016968
I0208 22:53:16.900726 139615963391744 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.6752503514289856, loss=1.4343385696411133
I0208 22:53:51.922432 139615954999040 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6640102863311768, loss=1.4170615673065186
I0208 22:54:26.902163 139615963391744 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6699082851409912, loss=1.4705774784088135
I0208 22:55:01.817416 139615954999040 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.6823347210884094, loss=1.4620540142059326
I0208 22:55:19.348375 139785736898368 spec.py:321] Evaluating on the training split.
I0208 22:55:22.347854 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 22:58:54.565136 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 22:58:57.283028 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:02:18.921142 139785736898368 spec.py:349] Evaluating on the test split.
I0208 23:02:21.641523 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:05:43.671867 139785736898368 submission_runner.py:408] Time since start: 65501.97s, 	Step: 113152, 	{'train/accuracy': 0.7029312252998352, 'train/loss': 1.376530647277832, 'train/bleu': 36.05811097263953, 'validation/accuracy': 0.691299557685852, 'validation/loss': 1.4230973720550537, 'validation/bleu': 30.798797566594644, 'validation/num_examples': 3000, 'test/accuracy': 0.705909013748169, 'test/loss': 1.339033603668213, 'test/bleu': 30.242559942520206, 'test/num_examples': 3003, 'score': 39508.85071182251, 'total_duration': 65501.97001576424, 'accumulated_submission_time': 39508.85071182251, 'accumulated_eval_time': 25987.774053812027, 'accumulated_logging_time': 1.6964483261108398}
I0208 23:05:43.702920 139615963391744 logging_writer.py:48] [113152] accumulated_eval_time=25987.774054, accumulated_logging_time=1.696448, accumulated_submission_time=39508.850712, global_step=113152, preemption_count=0, score=39508.850712, test/accuracy=0.705909, test/bleu=30.242560, test/loss=1.339034, test/num_examples=3003, total_duration=65501.970016, train/accuracy=0.702931, train/bleu=36.058111, train/loss=1.376531, validation/accuracy=0.691300, validation/bleu=30.798798, validation/loss=1.423097, validation/num_examples=3000
I0208 23:06:00.818710 139615954999040 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.6579033732414246, loss=1.4168314933776855
I0208 23:06:35.702287 139615963391744 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.6814581155776978, loss=1.4500863552093506
I0208 23:07:10.564296 139615954999040 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.6681150794029236, loss=1.5527842044830322
I0208 23:07:45.443520 139615963391744 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6807624697685242, loss=1.3654955625534058
I0208 23:08:20.325504 139615954999040 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6671908497810364, loss=1.4137275218963623
I0208 23:08:55.211726 139615963391744 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.7014228105545044, loss=1.519492506980896
I0208 23:09:30.096743 139615954999040 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.689887523651123, loss=1.3970146179199219
I0208 23:10:05.036571 139615963391744 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6919000148773193, loss=1.5298256874084473
I0208 23:10:39.927639 139615954999040 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6829546093940735, loss=1.3494833707809448
I0208 23:11:14.812907 139615963391744 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6948904991149902, loss=1.4055804014205933
I0208 23:11:49.762943 139615954999040 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6746413707733154, loss=1.451472520828247
I0208 23:12:24.676596 139615963391744 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.6791145205497742, loss=1.4804311990737915
I0208 23:12:59.577394 139615954999040 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.7025365233421326, loss=1.4038838148117065
I0208 23:13:34.502826 139615963391744 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6917449235916138, loss=1.4720860719680786
I0208 23:14:09.420605 139615954999040 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.6856614351272583, loss=1.3991367816925049
I0208 23:14:44.336874 139615963391744 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.6706881523132324, loss=1.4310801029205322
I0208 23:15:19.258353 139615954999040 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.6985265612602234, loss=1.475206732749939
I0208 23:15:54.165897 139615963391744 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.6793068647384644, loss=1.4718483686447144
I0208 23:16:29.062439 139615954999040 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.714311420917511, loss=1.423097848892212
I0208 23:17:03.981700 139615963391744 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.6798837184906006, loss=1.3426077365875244
I0208 23:17:38.877268 139615954999040 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.7042779326438904, loss=1.36228346824646
I0208 23:18:13.767994 139615963391744 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.7030338644981384, loss=1.4671106338500977
I0208 23:18:48.656549 139615954999040 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.7047183513641357, loss=1.338470458984375
I0208 23:19:23.568551 139615963391744 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.7236390709877014, loss=1.472228765487671
I0208 23:19:43.868917 139785736898368 spec.py:321] Evaluating on the training split.
I0208 23:19:46.902143 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:23:23.306622 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 23:23:26.020974 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:25:53.807807 139785736898368 spec.py:349] Evaluating on the test split.
I0208 23:25:56.525221 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:28:35.611878 139785736898368 submission_runner.py:408] Time since start: 66873.91s, 	Step: 115560, 	{'train/accuracy': 0.7006024122238159, 'train/loss': 1.3854455947875977, 'train/bleu': 35.91853943560247, 'validation/accuracy': 0.6913491487503052, 'validation/loss': 1.4228339195251465, 'validation/bleu': 30.759350276539493, 'validation/num_examples': 3000, 'test/accuracy': 0.7081633806228638, 'test/loss': 1.335689663887024, 'test/bleu': 30.608363709435334, 'test/num_examples': 3003, 'score': 40348.93051624298, 'total_duration': 66873.91002559662, 'accumulated_submission_time': 40348.93051624298, 'accumulated_eval_time': 26519.516964912415, 'accumulated_logging_time': 1.7374508380889893}
I0208 23:28:35.641941 139615954999040 logging_writer.py:48] [115560] accumulated_eval_time=26519.516965, accumulated_logging_time=1.737451, accumulated_submission_time=40348.930516, global_step=115560, preemption_count=0, score=40348.930516, test/accuracy=0.708163, test/bleu=30.608364, test/loss=1.335690, test/num_examples=3003, total_duration=66873.910026, train/accuracy=0.700602, train/bleu=35.918539, train/loss=1.385446, validation/accuracy=0.691349, validation/bleu=30.759350, validation/loss=1.422834, validation/num_examples=3000
I0208 23:28:49.928774 139615963391744 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.7077523469924927, loss=1.395712971687317
I0208 23:29:24.772673 139615954999040 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.7129230499267578, loss=1.4600803852081299
I0208 23:29:59.648415 139615963391744 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.7036051154136658, loss=1.4642517566680908
I0208 23:30:34.532982 139615954999040 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.7606046199798584, loss=1.4470243453979492
I0208 23:31:09.501738 139615963391744 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.7245126366615295, loss=1.4066189527511597
I0208 23:31:44.378071 139615954999040 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.6786764860153198, loss=1.4194390773773193
I0208 23:32:19.289666 139615963391744 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.708857536315918, loss=1.388153314590454
I0208 23:32:54.223676 139615954999040 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.7292561531066895, loss=1.453795075416565
I0208 23:33:29.103047 139615963391744 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.7263980507850647, loss=1.4450790882110596
I0208 23:34:03.999919 139615954999040 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.7181903123855591, loss=1.4356348514556885
I0208 23:34:38.888158 139615963391744 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.7272332310676575, loss=1.4006625413894653
I0208 23:35:13.830732 139615954999040 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.7190924882888794, loss=1.3974087238311768
I0208 23:35:48.674413 139615963391744 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.7127640247344971, loss=1.4401479959487915
I0208 23:36:23.523084 139615954999040 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.7171890139579773, loss=1.394055724143982
I0208 23:36:58.417320 139615963391744 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.768377959728241, loss=1.3846479654312134
I0208 23:37:33.301579 139615954999040 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.7205640077590942, loss=1.4175710678100586
I0208 23:38:08.171862 139615963391744 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.6946786642074585, loss=1.3826712369918823
I0208 23:38:43.044830 139615954999040 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.7160027623176575, loss=1.32534921169281
I0208 23:39:17.960273 139615963391744 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.7022369503974915, loss=1.3762247562408447
I0208 23:39:52.801534 139615954999040 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.7397738099098206, loss=1.5015623569488525
I0208 23:40:27.704847 139615963391744 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.7209787964820862, loss=1.350925326347351
I0208 23:41:02.576695 139615954999040 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.7181796431541443, loss=1.4143974781036377
I0208 23:41:37.409760 139615963391744 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7178616523742676, loss=1.3461154699325562
I0208 23:42:12.229453 139615954999040 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.7174779176712036, loss=1.3947973251342773
I0208 23:42:35.655811 139785736898368 spec.py:321] Evaluating on the training split.
I0208 23:42:38.650608 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:46:00.148536 139785736898368 spec.py:333] Evaluating on the validation split.
I0208 23:46:02.871180 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:48:38.993791 139785736898368 spec.py:349] Evaluating on the test split.
I0208 23:48:41.723262 139785736898368 workload.py:181] Translating evaluation dataset.
I0208 23:51:11.957758 139785736898368 submission_runner.py:408] Time since start: 68230.26s, 	Step: 117969, 	{'train/accuracy': 0.6944806575775146, 'train/loss': 1.42534601688385, 'train/bleu': 35.57020902703697, 'validation/accuracy': 0.691969096660614, 'validation/loss': 1.4174968004226685, 'validation/bleu': 30.88002752465824, 'validation/num_examples': 3000, 'test/accuracy': 0.7076637148857117, 'test/loss': 1.3267302513122559, 'test/bleu': 30.73994588003593, 'test/num_examples': 3003, 'score': 41188.847472667694, 'total_duration': 68230.25590658188, 'accumulated_submission_time': 41188.847472667694, 'accumulated_eval_time': 27035.818858623505, 'accumulated_logging_time': 1.779308557510376}
I0208 23:51:11.989448 139615963391744 logging_writer.py:48] [117969] accumulated_eval_time=27035.818859, accumulated_logging_time=1.779309, accumulated_submission_time=41188.847473, global_step=117969, preemption_count=0, score=41188.847473, test/accuracy=0.707664, test/bleu=30.739946, test/loss=1.326730, test/num_examples=3003, total_duration=68230.255907, train/accuracy=0.694481, train/bleu=35.570209, train/loss=1.425346, validation/accuracy=0.691969, validation/bleu=30.880028, validation/loss=1.417497, validation/num_examples=3000
I0208 23:51:23.170278 139615954999040 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.7408736348152161, loss=1.4148184061050415
I0208 23:51:58.107113 139615963391744 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7276267409324646, loss=1.4423688650131226
I0208 23:52:33.079103 139615954999040 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.7094287872314453, loss=1.3810820579528809
I0208 23:53:07.991028 139615963391744 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.7451180219650269, loss=1.4006465673446655
I0208 23:53:42.890907 139615954999040 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.7363332509994507, loss=1.4036469459533691
I0208 23:54:17.812536 139615963391744 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.7418979406356812, loss=1.4116528034210205
I0208 23:54:52.745095 139615954999040 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.7507699728012085, loss=1.419811487197876
I0208 23:55:27.648920 139615963391744 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.7743043303489685, loss=1.4936326742172241
I0208 23:56:02.533915 139615954999040 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.7284716367721558, loss=1.4374364614486694
I0208 23:56:37.430652 139615963391744 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.7632991075515747, loss=1.3933137655258179
I0208 23:57:12.335655 139615954999040 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7406414151191711, loss=1.436677098274231
I0208 23:57:47.253249 139615963391744 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.7318323850631714, loss=1.3722490072250366
I0208 23:58:22.177856 139615954999040 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.7473117113113403, loss=1.4080381393432617
I0208 23:58:57.072744 139615963391744 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.735919177532196, loss=1.3963072299957275
I0208 23:59:31.978982 139615954999040 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.7886133790016174, loss=1.4429986476898193
I0209 00:00:06.894780 139615963391744 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7497372627258301, loss=1.3990826606750488
I0209 00:00:41.799341 139615954999040 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.7286850810050964, loss=1.332829236984253
I0209 00:01:16.718629 139615963391744 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7834914326667786, loss=1.480568766593933
I0209 00:01:51.612555 139615954999040 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.7202783823013306, loss=1.4265466928482056
I0209 00:02:26.514918 139615963391744 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.7430064678192139, loss=1.367672085762024
I0209 00:03:01.415155 139615954999040 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7377244830131531, loss=1.339375376701355
I0209 00:03:36.309102 139615963391744 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.732613205909729, loss=1.3729922771453857
I0209 00:04:11.195193 139615954999040 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.741316020488739, loss=1.408372402191162
I0209 00:04:46.083355 139615963391744 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.73100346326828, loss=1.3679903745651245
I0209 00:05:11.980476 139785736898368 spec.py:321] Evaluating on the training split.
I0209 00:05:14.976832 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:08:53.212216 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 00:08:55.945158 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:11:34.644593 139785736898368 spec.py:349] Evaluating on the test split.
I0209 00:11:37.387125 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:14:24.921813 139785736898368 submission_runner.py:408] Time since start: 69623.22s, 	Step: 120376, 	{'train/accuracy': 0.7052478790283203, 'train/loss': 1.3589560985565186, 'train/bleu': 36.04900926844258, 'validation/accuracy': 0.6927626132965088, 'validation/loss': 1.4140264987945557, 'validation/bleu': 31.06434313028884, 'validation/num_examples': 3000, 'test/accuracy': 0.7080472111701965, 'test/loss': 1.3269232511520386, 'test/bleu': 30.485523564052155, 'test/num_examples': 3003, 'score': 42028.750698804855, 'total_duration': 69623.21995973587, 'accumulated_submission_time': 42028.750698804855, 'accumulated_eval_time': 27588.76014494896, 'accumulated_logging_time': 1.8211784362792969}
I0209 00:14:24.954947 139615954999040 logging_writer.py:48] [120376] accumulated_eval_time=27588.760145, accumulated_logging_time=1.821178, accumulated_submission_time=42028.750699, global_step=120376, preemption_count=0, score=42028.750699, test/accuracy=0.708047, test/bleu=30.485524, test/loss=1.326923, test/num_examples=3003, total_duration=69623.219960, train/accuracy=0.705248, train/bleu=36.049009, train/loss=1.358956, validation/accuracy=0.692763, validation/bleu=31.064343, validation/loss=1.414026, validation/num_examples=3000
I0209 00:14:33.685543 139615963391744 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.7549505829811096, loss=1.4046077728271484
I0209 00:15:08.503937 139615954999040 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.7544587254524231, loss=1.3282322883605957
I0209 00:15:43.328145 139615963391744 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.7430172562599182, loss=1.4157365560531616
I0209 00:16:18.165533 139615954999040 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.7359557747840881, loss=1.3738276958465576
I0209 00:16:52.994035 139615963391744 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7733209133148193, loss=1.354308009147644
I0209 00:17:27.841704 139615954999040 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7502173781394958, loss=1.3694663047790527
I0209 00:18:02.679067 139615963391744 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7578606009483337, loss=1.371370553970337
I0209 00:18:37.541182 139615954999040 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7506571412086487, loss=1.4245282411575317
I0209 00:19:12.378548 139615963391744 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.7718184590339661, loss=1.3628994226455688
I0209 00:19:47.257923 139615954999040 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.7522684335708618, loss=1.4304357767105103
I0209 00:20:22.103075 139615963391744 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.7562094330787659, loss=1.3317149877548218
I0209 00:20:56.930554 139615954999040 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7483411431312561, loss=1.3629026412963867
I0209 00:21:31.777653 139615963391744 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.7282848358154297, loss=1.3488268852233887
I0209 00:22:06.609219 139615954999040 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.763812780380249, loss=1.393082857131958
I0209 00:22:41.459044 139615963391744 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7819502949714661, loss=1.369291067123413
I0209 00:23:16.295986 139615954999040 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7670919895172119, loss=1.408913016319275
I0209 00:23:51.120901 139615963391744 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7737821936607361, loss=1.4116522073745728
I0209 00:24:25.972637 139615954999040 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.7794384360313416, loss=1.3462387323379517
I0209 00:25:00.814469 139615963391744 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.7683131098747253, loss=1.4036515951156616
I0209 00:25:35.661920 139615954999040 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7732955813407898, loss=1.4107967615127563
I0209 00:26:10.501868 139615963391744 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.7660436630249023, loss=1.2925491333007812
I0209 00:26:45.331167 139615954999040 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7937967777252197, loss=1.3833329677581787
I0209 00:27:20.157638 139615963391744 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.7709822654724121, loss=1.3353713750839233
I0209 00:27:55.031314 139615954999040 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.7774963974952698, loss=1.4634329080581665
I0209 00:28:25.053600 139785736898368 spec.py:321] Evaluating on the training split.
I0209 00:28:28.049768 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:31:53.149950 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 00:31:55.870418 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:34:35.585281 139785736898368 spec.py:349] Evaluating on the test split.
I0209 00:34:38.305397 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:37:14.139995 139785736898368 submission_runner.py:408] Time since start: 70992.44s, 	Step: 122788, 	{'train/accuracy': 0.7059203386306763, 'train/loss': 1.3507002592086792, 'train/bleu': 36.08858036119556, 'validation/accuracy': 0.6928866505622864, 'validation/loss': 1.412611722946167, 'validation/bleu': 30.960635533980913, 'validation/num_examples': 3000, 'test/accuracy': 0.709337055683136, 'test/loss': 1.3233616352081299, 'test/bleu': 30.99924664179277, 'test/num_examples': 3003, 'score': 42868.7630045414, 'total_duration': 70992.43810915947, 'accumulated_submission_time': 42868.7630045414, 'accumulated_eval_time': 28117.846455574036, 'accumulated_logging_time': 1.864339828491211}
I0209 00:37:14.179300 139615963391744 logging_writer.py:48] [122788] accumulated_eval_time=28117.846456, accumulated_logging_time=1.864340, accumulated_submission_time=42868.763005, global_step=122788, preemption_count=0, score=42868.763005, test/accuracy=0.709337, test/bleu=30.999247, test/loss=1.323362, test/num_examples=3003, total_duration=70992.438109, train/accuracy=0.705920, train/bleu=36.088580, train/loss=1.350700, validation/accuracy=0.692887, validation/bleu=30.960636, validation/loss=1.412612, validation/num_examples=3000
I0209 00:37:18.730027 139615954999040 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7795088291168213, loss=1.3704912662506104
I0209 00:37:53.649633 139615963391744 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7909768223762512, loss=1.357391357421875
I0209 00:38:28.555288 139615954999040 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7504145503044128, loss=1.387545108795166
I0209 00:39:03.457242 139615963391744 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.7744022011756897, loss=1.3727604150772095
I0209 00:39:38.335024 139615954999040 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7747313976287842, loss=1.3562349081039429
I0209 00:40:13.246164 139615963391744 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.7891300320625305, loss=1.398006796836853
I0209 00:40:48.159837 139615954999040 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.7627007961273193, loss=1.4039682149887085
I0209 00:41:23.063540 139615963391744 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.789421558380127, loss=1.4145452976226807
I0209 00:41:57.976769 139615954999040 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7808117270469666, loss=1.365073800086975
I0209 00:42:32.860393 139615963391744 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.7666363716125488, loss=1.4153251647949219
I0209 00:43:07.783395 139615954999040 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.784476637840271, loss=1.3406391143798828
I0209 00:43:42.659821 139615963391744 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.7416765689849854, loss=1.3813410997390747
I0209 00:44:17.575960 139615954999040 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.7630007863044739, loss=1.4210556745529175
I0209 00:44:52.535791 139615963391744 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.7363812923431396, loss=1.3493678569793701
I0209 00:45:27.461628 139615954999040 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.8016975522041321, loss=1.363906741142273
I0209 00:46:02.380922 139615963391744 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.7782025337219238, loss=1.446590781211853
I0209 00:46:37.282125 139615954999040 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.7786554098129272, loss=1.3582960367202759
I0209 00:47:12.171381 139615963391744 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7604147791862488, loss=1.4448124170303345
I0209 00:47:47.059746 139615954999040 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.8081955909729004, loss=1.3744938373565674
I0209 00:48:21.969078 139615963391744 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.7724111080169678, loss=1.397462248802185
I0209 00:48:56.873200 139615954999040 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.7845568656921387, loss=1.3612749576568604
I0209 00:49:31.821871 139615963391744 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.7882435321807861, loss=1.3955286741256714
I0209 00:50:06.771101 139615954999040 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.7571924924850464, loss=1.3589000701904297
I0209 00:50:41.703102 139615963391744 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.7928888201713562, loss=1.41996169090271
I0209 00:51:14.217535 139785736898368 spec.py:321] Evaluating on the training split.
I0209 00:51:17.237341 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:54:53.844935 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 00:54:56.573619 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 00:57:27.523361 139785736898368 spec.py:349] Evaluating on the test split.
I0209 00:57:30.250706 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:00:05.941003 139785736898368 submission_runner.py:408] Time since start: 72364.24s, 	Step: 125195, 	{'train/accuracy': 0.7114030718803406, 'train/loss': 1.329108476638794, 'train/bleu': 37.05163518567466, 'validation/accuracy': 0.6935189962387085, 'validation/loss': 1.4125992059707642, 'validation/bleu': 30.957929462228876, 'validation/num_examples': 3000, 'test/accuracy': 0.7094649076461792, 'test/loss': 1.3227872848510742, 'test/bleu': 30.60444715357637, 'test/num_examples': 3003, 'score': 43708.71370458603, 'total_duration': 72364.23915076256, 'accumulated_submission_time': 43708.71370458603, 'accumulated_eval_time': 28649.569895029068, 'accumulated_logging_time': 1.914734125137329}
I0209 01:00:05.974174 139615954999040 logging_writer.py:48] [125195] accumulated_eval_time=28649.569895, accumulated_logging_time=1.914734, accumulated_submission_time=43708.713705, global_step=125195, preemption_count=0, score=43708.713705, test/accuracy=0.709465, test/bleu=30.604447, test/loss=1.322787, test/num_examples=3003, total_duration=72364.239151, train/accuracy=0.711403, train/bleu=37.051635, train/loss=1.329108, validation/accuracy=0.693519, validation/bleu=30.957929, validation/loss=1.412599, validation/num_examples=3000
I0209 01:00:08.085958 139615963391744 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.7691015005111694, loss=1.301408052444458
I0209 01:00:42.984044 139615954999040 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.8209549784660339, loss=1.4874546527862549
I0209 01:01:17.868430 139615963391744 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.797090470790863, loss=1.4304488897323608
I0209 01:01:52.723996 139615954999040 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7734062671661377, loss=1.3979864120483398
I0209 01:02:27.597009 139615963391744 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.7523049116134644, loss=1.3513373136520386
I0209 01:03:02.474452 139615954999040 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.7700405120849609, loss=1.3671895265579224
I0209 01:03:37.352512 139615963391744 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.7569825053215027, loss=1.354169487953186
I0209 01:04:12.226588 139615954999040 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.7681018114089966, loss=1.3879207372665405
I0209 01:04:47.105300 139615963391744 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7871218919754028, loss=1.4102141857147217
I0209 01:05:21.983238 139615954999040 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.7688159942626953, loss=1.3799742460250854
I0209 01:05:56.894299 139615963391744 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.759891152381897, loss=1.340887188911438
I0209 01:06:31.800193 139615954999040 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.773299515247345, loss=1.3486683368682861
I0209 01:07:06.731762 139615963391744 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.778354823589325, loss=1.2511906623840332
I0209 01:07:41.586986 139615954999040 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.7821581959724426, loss=1.3698416948318481
I0209 01:08:16.526580 139615963391744 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.7813585996627808, loss=1.3713706731796265
I0209 01:08:51.444879 139615954999040 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.8018239736557007, loss=1.4114214181900024
I0209 01:09:26.353548 139615963391744 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.7998298406600952, loss=1.4284003973007202
I0209 01:10:01.316084 139615954999040 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7546175718307495, loss=1.3183708190917969
I0209 01:10:36.220861 139615963391744 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7962172031402588, loss=1.3554130792617798
I0209 01:11:11.121711 139615954999040 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.7645400762557983, loss=1.3161952495574951
I0209 01:11:45.997642 139615963391744 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.7930383086204529, loss=1.413055181503296
I0209 01:12:20.883407 139615954999040 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.809739351272583, loss=1.433014154434204
I0209 01:12:55.755530 139615963391744 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.8107067346572876, loss=1.3848505020141602
I0209 01:13:30.661137 139615954999040 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.790619969367981, loss=1.323188304901123
I0209 01:14:05.542222 139615963391744 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.8043832182884216, loss=1.3542225360870361
I0209 01:14:05.967823 139785736898368 spec.py:321] Evaluating on the training split.
I0209 01:14:08.970995 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:17:31.610035 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 01:17:34.314684 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:20:03.679323 139785736898368 spec.py:349] Evaluating on the test split.
I0209 01:20:06.402479 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:22:45.106905 139785736898368 submission_runner.py:408] Time since start: 73723.41s, 	Step: 127603, 	{'train/accuracy': 0.7080180048942566, 'train/loss': 1.3448046445846558, 'train/bleu': 36.594626517179826, 'validation/accuracy': 0.6943497061729431, 'validation/loss': 1.4095133543014526, 'validation/bleu': 30.972036209681566, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.321054458618164, 'test/bleu': 30.63157748084628, 'test/num_examples': 3003, 'score': 44548.620322942734, 'total_duration': 73723.40506076813, 'accumulated_submission_time': 44548.620322942734, 'accumulated_eval_time': 29168.708927631378, 'accumulated_logging_time': 1.9580953121185303}
I0209 01:22:45.139282 139615954999040 logging_writer.py:48] [127603] accumulated_eval_time=29168.708928, accumulated_logging_time=1.958095, accumulated_submission_time=44548.620323, global_step=127603, preemption_count=0, score=44548.620323, test/accuracy=0.710127, test/bleu=30.631577, test/loss=1.321054, test/num_examples=3003, total_duration=73723.405061, train/accuracy=0.708018, train/bleu=36.594627, train/loss=1.344805, validation/accuracy=0.694350, validation/bleu=30.972036, validation/loss=1.409513, validation/num_examples=3000
I0209 01:23:19.325969 139615963391744 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.7990934252738953, loss=1.3763272762298584
I0209 01:23:54.202835 139615954999040 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.7838744521141052, loss=1.345088243484497
I0209 01:24:29.116677 139615963391744 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.7923430800437927, loss=1.3408201932907104
I0209 01:25:04.041296 139615954999040 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.7800019383430481, loss=1.436621069908142
I0209 01:25:39.001250 139615963391744 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.8362453579902649, loss=1.3680657148361206
I0209 01:26:13.904829 139615954999040 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.7787028551101685, loss=1.3536423444747925
I0209 01:26:48.810696 139615963391744 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.7897099256515503, loss=1.3222771883010864
I0209 01:27:23.700125 139615954999040 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.7682898640632629, loss=1.3472373485565186
I0209 01:27:58.652405 139615963391744 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.7860650420188904, loss=1.2943060398101807
I0209 01:28:33.541416 139615954999040 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7630680203437805, loss=1.3461003303527832
I0209 01:29:08.415657 139615963391744 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.7572906613349915, loss=1.2680268287658691
I0209 01:29:43.314618 139615954999040 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.7837223410606384, loss=1.335513710975647
I0209 01:30:18.227139 139615963391744 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.8145779371261597, loss=1.357273817062378
I0209 01:30:53.171961 139615954999040 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.804734468460083, loss=1.397731900215149
I0209 01:31:28.073926 139615963391744 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7592250108718872, loss=1.42387056350708
I0209 01:32:02.991857 139615954999040 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.80117267370224, loss=1.372111201286316
I0209 01:32:37.905127 139615963391744 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.785493016242981, loss=1.3768222332000732
I0209 01:33:12.832218 139615954999040 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.7748649716377258, loss=1.375864863395691
I0209 01:33:47.732468 139615963391744 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7990865707397461, loss=1.4096864461898804
I0209 01:34:22.603321 139615954999040 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.8044244050979614, loss=1.3568214178085327
I0209 01:34:57.506844 139615963391744 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.780316948890686, loss=1.332468867301941
I0209 01:35:32.411866 139615954999040 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.7699328064918518, loss=1.3399507999420166
I0209 01:36:07.313214 139615963391744 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7981281280517578, loss=1.3799242973327637
I0209 01:36:42.235654 139615954999040 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.7994542717933655, loss=1.3585951328277588
I0209 01:36:45.108343 139785736898368 spec.py:321] Evaluating on the training split.
I0209 01:36:48.115721 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:40:06.873572 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 01:40:09.590111 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:42:44.244608 139785736898368 spec.py:349] Evaluating on the test split.
I0209 01:42:46.969933 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 01:45:30.758872 139785736898368 submission_runner.py:408] Time since start: 75089.06s, 	Step: 130010, 	{'train/accuracy': 0.7112451791763306, 'train/loss': 1.3329684734344482, 'train/bleu': 37.12121517508366, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.4096252918243408, 'validation/bleu': 31.136288279625116, 'validation/num_examples': 3000, 'test/accuracy': 0.7105107307434082, 'test/loss': 1.320068359375, 'test/bleu': 30.66140440468964, 'test/num_examples': 3003, 'score': 45388.499920129776, 'total_duration': 75089.05698800087, 'accumulated_submission_time': 45388.499920129776, 'accumulated_eval_time': 29694.359369277954, 'accumulated_logging_time': 2.001633644104004}
I0209 01:45:30.798740 139615963391744 logging_writer.py:48] [130010] accumulated_eval_time=29694.359369, accumulated_logging_time=2.001634, accumulated_submission_time=45388.499920, global_step=130010, preemption_count=0, score=45388.499920, test/accuracy=0.710511, test/bleu=30.661404, test/loss=1.320068, test/num_examples=3003, total_duration=75089.056988, train/accuracy=0.711245, train/bleu=37.121215, train/loss=1.332968, validation/accuracy=0.694176, validation/bleu=31.136288, validation/loss=1.409625, validation/num_examples=3000
I0209 01:46:02.573731 139615954999040 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7929736375808716, loss=1.3530995845794678
I0209 01:46:37.451361 139615963391744 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.7910426259040833, loss=1.4002082347869873
I0209 01:47:12.339028 139615954999040 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.791893720626831, loss=1.3425331115722656
I0209 01:47:47.224662 139615963391744 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.7540088891983032, loss=1.256060242652893
I0209 01:48:22.153475 139615954999040 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7938953042030334, loss=1.3870179653167725
I0209 01:48:57.105062 139615963391744 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.7918180227279663, loss=1.3671905994415283
I0209 01:49:32.017811 139615954999040 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7719122171401978, loss=1.3765217065811157
I0209 01:50:06.938679 139615963391744 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.7620237469673157, loss=1.327209234237671
I0209 01:50:41.826479 139615954999040 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.814993143081665, loss=1.4670177698135376
I0209 01:51:16.751543 139615963391744 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7869729995727539, loss=1.3269256353378296
I0209 01:51:51.666090 139615954999040 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.8131983876228333, loss=1.3465654850006104
I0209 01:52:26.581783 139615963391744 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.7758405804634094, loss=1.3606677055358887
I0209 01:53:01.483225 139615954999040 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.7696080207824707, loss=1.398306131362915
I0209 01:53:36.387562 139615963391744 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.7763447761535645, loss=1.3221741914749146
I0209 01:54:11.356693 139615954999040 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.7806426882743835, loss=1.3531490564346313
I0209 01:54:46.298491 139615963391744 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.7765436172485352, loss=1.3891078233718872
I0209 01:55:21.222303 139615954999040 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.8093952536582947, loss=1.386890172958374
I0209 01:55:56.135956 139615963391744 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7841852903366089, loss=1.4152085781097412
I0209 01:56:31.060618 139615954999040 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.7802632451057434, loss=1.3389902114868164
I0209 01:57:05.993904 139615963391744 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.7563223242759705, loss=1.3283675909042358
I0209 01:57:40.907531 139615954999040 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.7933557629585266, loss=1.3310043811798096
I0209 01:58:15.806016 139615963391744 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.756637454032898, loss=1.3510359525680542
I0209 01:58:50.875181 139615954999040 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.759543240070343, loss=1.3089402914047241
I0209 01:59:25.843030 139615963391744 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.792030394077301, loss=1.3243306875228882
I0209 01:59:30.812508 139785736898368 spec.py:321] Evaluating on the training split.
I0209 01:59:33.818433 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:03:04.345103 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 02:03:07.064927 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:05:43.329352 139785736898368 spec.py:349] Evaluating on the test split.
I0209 02:05:46.059533 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:08:29.773766 139785736898368 submission_runner.py:408] Time since start: 76468.07s, 	Step: 132416, 	{'train/accuracy': 0.7100916504859924, 'train/loss': 1.3323860168457031, 'train/bleu': 37.0542430438851, 'validation/accuracy': 0.6940397620201111, 'validation/loss': 1.4101505279541016, 'validation/bleu': 31.14853330741573, 'validation/num_examples': 3000, 'test/accuracy': 0.7104874849319458, 'test/loss': 1.3203920125961304, 'test/bleu': 30.713799107089375, 'test/num_examples': 3003, 'score': 46228.42295074463, 'total_duration': 76468.07191419601, 'accumulated_submission_time': 46228.42295074463, 'accumulated_eval_time': 30233.320573568344, 'accumulated_logging_time': 2.052457571029663}
I0209 02:08:29.807491 139615954999040 logging_writer.py:48] [132416] accumulated_eval_time=30233.320574, accumulated_logging_time=2.052458, accumulated_submission_time=46228.422951, global_step=132416, preemption_count=0, score=46228.422951, test/accuracy=0.710487, test/bleu=30.713799, test/loss=1.320392, test/num_examples=3003, total_duration=76468.071914, train/accuracy=0.710092, train/bleu=37.054243, train/loss=1.332386, validation/accuracy=0.694040, validation/bleu=31.148533, validation/loss=1.410151, validation/num_examples=3000
I0209 02:08:59.460680 139615963391744 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.7616340517997742, loss=1.4078419208526611
I0209 02:09:34.354995 139615954999040 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.7986993193626404, loss=1.3827223777770996
I0209 02:10:09.233005 139615963391744 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.8035362362861633, loss=1.363526701927185
I0209 02:10:44.118216 139615954999040 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.7632330060005188, loss=1.3287084102630615
I0209 02:11:19.003774 139615963391744 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.768108069896698, loss=1.3809852600097656
I0209 02:11:53.971357 139615954999040 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.7654377222061157, loss=1.354076862335205
I0209 02:12:28.894402 139615963391744 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.7652318477630615, loss=1.3500908613204956
I0209 02:13:03.796302 139615954999040 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.7822425961494446, loss=1.2780553102493286
I0209 02:13:38.730341 139615963391744 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.8046751022338867, loss=1.4648685455322266
I0209 02:13:49.621739 139785736898368 spec.py:321] Evaluating on the training split.
I0209 02:13:52.630246 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:17:11.535565 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 02:17:14.252759 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:19:47.951366 139785736898368 spec.py:349] Evaluating on the test split.
I0209 02:19:50.670093 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:22:30.131265 139785736898368 submission_runner.py:408] Time since start: 77308.43s, 	Step: 133333, 	{'train/accuracy': 0.7102746367454529, 'train/loss': 1.3364615440368652, 'train/bleu': 37.00281637120834, 'validation/accuracy': 0.6941141486167908, 'validation/loss': 1.4101862907409668, 'validation/bleu': 31.145169924199735, 'validation/num_examples': 3000, 'test/accuracy': 0.7104526162147522, 'test/loss': 1.3205291032791138, 'test/bleu': 30.73949967608062, 'test/num_examples': 3003, 'score': 46548.19679522514, 'total_duration': 77308.42940235138, 'accumulated_submission_time': 46548.19679522514, 'accumulated_eval_time': 30753.830042362213, 'accumulated_logging_time': 2.0963242053985596}
I0209 02:22:30.165782 139615954999040 logging_writer.py:48] [133333] accumulated_eval_time=30753.830042, accumulated_logging_time=2.096324, accumulated_submission_time=46548.196795, global_step=133333, preemption_count=0, score=46548.196795, test/accuracy=0.710453, test/bleu=30.739500, test/loss=1.320529, test/num_examples=3003, total_duration=77308.429402, train/accuracy=0.710275, train/bleu=37.002816, train/loss=1.336462, validation/accuracy=0.694114, validation/bleu=31.145170, validation/loss=1.410186, validation/num_examples=3000
I0209 02:22:30.199979 139615963391744 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46548.196795
I0209 02:22:31.242368 139785736898368 checkpoints.py:490] Saving checkpoint at step: 133333
I0209 02:22:35.154294 139785736898368 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3/checkpoint_133333
I0209 02:22:35.159202 139785736898368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_3/checkpoint_133333.
I0209 02:22:35.212466 139785736898368 submission_runner.py:583] Tuning trial 3/5
I0209 02:22:35.212630 139785736898368 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0209 02:22:35.217881 139785736898368 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000595776888076216, 'train/loss': 11.064934730529785, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.81904888153076, 'total_duration': 896.6272666454315, 'accumulated_submission_time': 25.81904888153076, 'accumulated_eval_time': 870.8081820011139, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2403, {'train/accuracy': 0.41331639885902405, 'train/loss': 3.916454315185547, 'train/bleu': 14.633949505522779, 'validation/accuracy': 0.40114814043045044, 'validation/loss': 4.038556098937988, 'validation/bleu': 9.957431113516414, 'validation/num_examples': 3000, 'test/accuracy': 0.38661321997642517, 'test/loss': 4.231210708618164, 'test/bleu': 8.388442673207205, 'test/num_examples': 3003, 'score': 865.8087675571442, 'total_duration': 2316.1432354450226, 'accumulated_submission_time': 865.8087675571442, 'accumulated_eval_time': 1450.2326774597168, 'accumulated_logging_time': 0.023147106170654297, 'global_step': 2403, 'preemption_count': 0}), (4804, {'train/accuracy': 0.5378835201263428, 'train/loss': 2.7001044750213623, 'train/bleu': 24.378417805692475, 'validation/accuracy': 0.5419771671295166, 'validation/loss': 2.6425657272338867, 'validation/bleu': 20.41463598087644, 'validation/num_examples': 3000, 'test/accuracy': 0.543048083782196, 'test/loss': 2.6712963581085205, 'test/bleu': 18.875308714605914, 'test/num_examples': 3003, 'score': 1705.7114703655243, 'total_duration': 3633.7379219532013, 'accumulated_submission_time': 1705.7114703655243, 'accumulated_eval_time': 1927.821095943451, 'accumulated_logging_time': 0.049123287200927734, 'global_step': 4804, 'preemption_count': 0}), (7207, {'train/accuracy': 0.580255389213562, 'train/loss': 2.2710001468658447, 'train/bleu': 27.101228921682285, 'validation/accuracy': 0.5857831835746765, 'validation/loss': 2.226724624633789, 'validation/bleu': 23.166204664087257, 'validation/num_examples': 3000, 'test/accuracy': 0.5880890488624573, 'test/loss': 2.2239651679992676, 'test/bleu': 22.14029524164679, 'test/num_examples': 3003, 'score': 2545.865830183029, 'total_duration': 4950.748953819275, 'accumulated_submission_time': 2545.865830183029, 'accumulated_eval_time': 2404.573896408081, 'accumulated_logging_time': 0.07438302040100098, 'global_step': 7207, 'preemption_count': 0}), (9611, {'train/accuracy': 0.592166006565094, 'train/loss': 2.1426703929901123, 'train/bleu': 28.45783172233359, 'validation/accuracy': 0.606316089630127, 'validation/loss': 2.0390303134918213, 'validation/bleu': 24.673418436166767, 'validation/num_examples': 3000, 'test/accuracy': 0.6116321086883545, 'test/loss': 2.003997325897217, 'test/bleu': 23.503151632503048, 'test/num_examples': 3003, 'score': 3385.8169617652893, 'total_duration': 6229.004224777222, 'accumulated_submission_time': 3385.8169617652893, 'accumulated_eval_time': 2842.7774851322174, 'accumulated_logging_time': 0.10021090507507324, 'global_step': 9611, 'preemption_count': 0}), (12015, {'train/accuracy': 0.6005766987800598, 'train/loss': 2.0656661987304688, 'train/bleu': 28.810404280423196, 'validation/accuracy': 0.6186903715133667, 'validation/loss': 1.9162800312042236, 'validation/bleu': 25.631044209587834, 'validation/num_examples': 3000, 'test/accuracy': 0.6232293248176575, 'test/loss': 1.8754557371139526, 'test/bleu': 24.426011741968633, 'test/num_examples': 3003, 'score': 4225.967286348343, 'total_duration': 7681.333123445511, 'accumulated_submission_time': 4225.967286348343, 'accumulated_eval_time': 3454.847516775131, 'accumulated_logging_time': 0.12733793258666992, 'global_step': 12015, 'preemption_count': 0}), (14421, {'train/accuracy': 0.6168133616447449, 'train/loss': 1.9349526166915894, 'train/bleu': 29.425459383473545, 'validation/accuracy': 0.630469560623169, 'validation/loss': 1.8252501487731934, 'validation/bleu': 26.62938777326784, 'validation/num_examples': 3000, 'test/accuracy': 0.6382430195808411, 'test/loss': 1.775894045829773, 'test/bleu': 25.50415595748084, 'test/num_examples': 3003, 'score': 5066.110929250717, 'total_duration': 8959.01217341423, 'accumulated_submission_time': 5066.110929250717, 'accumulated_eval_time': 3892.2753579616547, 'accumulated_logging_time': 0.1567516326904297, 'global_step': 14421, 'preemption_count': 0}), (16827, {'train/accuracy': 0.6208845973014832, 'train/loss': 1.901543140411377, 'train/bleu': 29.833422011187903, 'validation/accuracy': 0.6382933855056763, 'validation/loss': 1.7594693899154663, 'validation/bleu': 26.58655602133679, 'validation/num_examples': 3000, 'test/accuracy': 0.6456103920936584, 'test/loss': 1.7116901874542236, 'test/bleu': 26.099517505013495, 'test/num_examples': 3003, 'score': 5906.1692090034485, 'total_duration': 10265.277593374252, 'accumulated_submission_time': 5906.1692090034485, 'accumulated_eval_time': 4358.376479148865, 'accumulated_logging_time': 0.18583035469055176, 'global_step': 16827, 'preemption_count': 0}), (19234, {'train/accuracy': 0.6365357041358948, 'train/loss': 1.7768452167510986, 'train/bleu': 31.482361801789065, 'validation/accuracy': 0.6452617049217224, 'validation/loss': 1.7125599384307861, 'validation/bleu': 27.290980192520678, 'validation/num_examples': 3000, 'test/accuracy': 0.6555923819541931, 'test/loss': 1.65578031539917, 'test/bleu': 26.700905226358415, 'test/num_examples': 3003, 'score': 6746.2333035469055, 'total_duration': 11695.425820350647, 'accumulated_submission_time': 6746.2333035469055, 'accumulated_eval_time': 4948.353693962097, 'accumulated_logging_time': 0.21404194831848145, 'global_step': 19234, 'preemption_count': 0}), (21641, {'train/accuracy': 0.6268560886383057, 'train/loss': 1.8370939493179321, 'train/bleu': 30.100959194200843, 'validation/accuracy': 0.6466503739356995, 'validation/loss': 1.69412362575531, 'validation/bleu': 27.4922055487141, 'validation/num_examples': 3000, 'test/accuracy': 0.6589041948318481, 'test/loss': 1.6343481540679932, 'test/bleu': 26.76084053464521, 'test/num_examples': 3003, 'score': 7586.293897151947, 'total_duration': 13127.183210134506, 'accumulated_submission_time': 7586.293897151947, 'accumulated_eval_time': 5539.945268630981, 'accumulated_logging_time': 0.24245548248291016, 'global_step': 21641, 'preemption_count': 0}), (24049, {'train/accuracy': 0.6285372376441956, 'train/loss': 1.8340964317321777, 'train/bleu': 30.61643964662627, 'validation/accuracy': 0.6487582325935364, 'validation/loss': 1.6795746088027954, 'validation/bleu': 27.350335835971034, 'validation/num_examples': 3000, 'test/accuracy': 0.657300591468811, 'test/loss': 1.6291598081588745, 'test/bleu': 26.492055872645757, 'test/num_examples': 3003, 'score': 8426.46519112587, 'total_duration': 14472.267474412918, 'accumulated_submission_time': 8426.46519112587, 'accumulated_eval_time': 6044.751909255981, 'accumulated_logging_time': 0.2706022262573242, 'global_step': 24049, 'preemption_count': 0}), (26457, {'train/accuracy': 0.6383501887321472, 'train/loss': 1.7676606178283691, 'train/bleu': 31.067359641378342, 'validation/accuracy': 0.6521803736686707, 'validation/loss': 1.6639560461044312, 'validation/bleu': 27.703317488145554, 'validation/num_examples': 3000, 'test/accuracy': 0.6611701846122742, 'test/loss': 1.60666024684906, 'test/bleu': 27.172473220455522, 'test/num_examples': 3003, 'score': 9266.6213285923, 'total_duration': 15862.000746011734, 'accumulated_submission_time': 9266.6213285923, 'accumulated_eval_time': 6594.221925020218, 'accumulated_logging_time': 0.29915809631347656, 'global_step': 26457, 'preemption_count': 0}), (28863, {'train/accuracy': 0.6295340657234192, 'train/loss': 1.8214620351791382, 'train/bleu': 30.574595318278686, 'validation/accuracy': 0.6530855298042297, 'validation/loss': 1.650708794593811, 'validation/bleu': 27.829608372370426, 'validation/num_examples': 3000, 'test/accuracy': 0.6616930961608887, 'test/loss': 1.588875412940979, 'test/bleu': 27.073841281803126, 'test/num_examples': 3003, 'score': 10106.590117931366, 'total_duration': 17257.571030139923, 'accumulated_submission_time': 10106.590117931366, 'accumulated_eval_time': 7149.713710069656, 'accumulated_logging_time': 0.32878661155700684, 'global_step': 28863, 'preemption_count': 0}), (31270, {'train/accuracy': 0.6339073777198792, 'train/loss': 1.8032761812210083, 'train/bleu': 31.101627683228955, 'validation/accuracy': 0.6544494032859802, 'validation/loss': 1.6417778730392456, 'validation/bleu': 27.791217888891975, 'validation/num_examples': 3000, 'test/accuracy': 0.6649003624916077, 'test/loss': 1.5839022397994995, 'test/bleu': 27.376689492425207, 'test/num_examples': 3003, 'score': 10946.506821632385, 'total_duration': 18626.177748918533, 'accumulated_submission_time': 10946.506821632385, 'accumulated_eval_time': 7678.296708583832, 'accumulated_logging_time': 0.3578181266784668, 'global_step': 31270, 'preemption_count': 0}), (33679, {'train/accuracy': 0.640436053276062, 'train/loss': 1.7520774602890015, 'train/bleu': 31.079749336354755, 'validation/accuracy': 0.6569168567657471, 'validation/loss': 1.6260370016098022, 'validation/bleu': 27.985500631619285, 'validation/num_examples': 3000, 'test/accuracy': 0.6679217219352722, 'test/loss': 1.5678555965423584, 'test/bleu': 27.7718476746868, 'test/num_examples': 3003, 'score': 11786.677701950073, 'total_duration': 19973.244158029556, 'accumulated_submission_time': 11786.677701950073, 'accumulated_eval_time': 8185.084298849106, 'accumulated_logging_time': 0.3881950378417969, 'global_step': 33679, 'preemption_count': 0}), (36087, {'train/accuracy': 0.6369031071662903, 'train/loss': 1.7718901634216309, 'train/bleu': 30.80821034725975, 'validation/accuracy': 0.6572268009185791, 'validation/loss': 1.6264159679412842, 'validation/bleu': 28.234495377139638, 'validation/num_examples': 3000, 'test/accuracy': 0.6699668765068054, 'test/loss': 1.5581209659576416, 'test/bleu': 27.991591213510215, 'test/num_examples': 3003, 'score': 12626.625022888184, 'total_duration': 21296.61293053627, 'accumulated_submission_time': 12626.625022888184, 'accumulated_eval_time': 8668.399793624878, 'accumulated_logging_time': 0.41771602630615234, 'global_step': 36087, 'preemption_count': 0}), (38495, {'train/accuracy': 0.6430981159210205, 'train/loss': 1.7214125394821167, 'train/bleu': 31.545833523294696, 'validation/accuracy': 0.6587147116661072, 'validation/loss': 1.6191388368606567, 'validation/bleu': 28.382361296399072, 'validation/num_examples': 3000, 'test/accuracy': 0.6695834398269653, 'test/loss': 1.555444359779358, 'test/bleu': 27.83423761879015, 'test/num_examples': 3003, 'score': 13466.692353248596, 'total_duration': 22630.738989830017, 'accumulated_submission_time': 13466.692353248596, 'accumulated_eval_time': 9162.348851919174, 'accumulated_logging_time': 0.44786763191223145, 'global_step': 38495, 'preemption_count': 0}), (40904, {'train/accuracy': 0.6393962502479553, 'train/loss': 1.7517564296722412, 'train/bleu': 31.326564034977284, 'validation/accuracy': 0.6592354774475098, 'validation/loss': 1.6140036582946777, 'validation/bleu': 28.034270082296626, 'validation/num_examples': 3000, 'test/accuracy': 0.668677031993866, 'test/loss': 1.5514838695526123, 'test/bleu': 27.835884883573716, 'test/num_examples': 3003, 'score': 14306.815686225891, 'total_duration': 23983.063989400864, 'accumulated_submission_time': 14306.815686225891, 'accumulated_eval_time': 9674.442895889282, 'accumulated_logging_time': 0.47867465019226074, 'global_step': 40904, 'preemption_count': 0}), (43312, {'train/accuracy': 0.6379010081291199, 'train/loss': 1.7598146200180054, 'train/bleu': 31.306862035234403, 'validation/accuracy': 0.6621120572090149, 'validation/loss': 1.6029443740844727, 'validation/bleu': 28.58756001053182, 'validation/num_examples': 3000, 'test/accuracy': 0.6726861000061035, 'test/loss': 1.533181071281433, 'test/bleu': 28.08085070114241, 'test/num_examples': 3003, 'score': 15146.73096871376, 'total_duration': 25332.93573999405, 'accumulated_submission_time': 15146.73096871376, 'accumulated_eval_time': 10184.291736602783, 'accumulated_logging_time': 0.5092837810516357, 'global_step': 43312, 'preemption_count': 0}), (45720, {'train/accuracy': 0.642835795879364, 'train/loss': 1.7258001565933228, 'train/bleu': 31.514653977822757, 'validation/accuracy': 0.6610457301139832, 'validation/loss': 1.5972543954849243, 'validation/bleu': 28.238541337906106, 'validation/num_examples': 3000, 'test/accuracy': 0.6723374724388123, 'test/loss': 1.5314693450927734, 'test/bleu': 27.78312192161673, 'test/num_examples': 3003, 'score': 15986.845165491104, 'total_duration': 26877.991456270218, 'accumulated_submission_time': 15986.845165491104, 'accumulated_eval_time': 10889.123789072037, 'accumulated_logging_time': 0.5416944026947021, 'global_step': 45720, 'preemption_count': 0}), (48128, {'train/accuracy': 0.6452369689941406, 'train/loss': 1.726482629776001, 'train/bleu': 31.82425775905154, 'validation/accuracy': 0.6602894067764282, 'validation/loss': 1.590267300605774, 'validation/bleu': 28.259296946768995, 'validation/num_examples': 3000, 'test/accuracy': 0.6742199659347534, 'test/loss': 1.5215578079223633, 'test/bleu': 27.99158132715858, 'test/num_examples': 3003, 'score': 16826.8217689991, 'total_duration': 28235.679394960403, 'accumulated_submission_time': 16826.8217689991, 'accumulated_eval_time': 11406.725434064865, 'accumulated_logging_time': 0.5741453170776367, 'global_step': 48128, 'preemption_count': 0}), (50537, {'train/accuracy': 0.6565302610397339, 'train/loss': 1.640212893486023, 'train/bleu': 32.767179635812575, 'validation/accuracy': 0.6637363433837891, 'validation/loss': 1.5860275030136108, 'validation/bleu': 28.6552268817002, 'validation/num_examples': 3000, 'test/accuracy': 0.675358772277832, 'test/loss': 1.518868088722229, 'test/bleu': 28.05993585302966, 'test/num_examples': 3003, 'score': 17666.926292657852, 'total_duration': 29602.6824324131, 'accumulated_submission_time': 17666.926292657852, 'accumulated_eval_time': 11933.50832438469, 'accumulated_logging_time': 0.6121892929077148, 'global_step': 50537, 'preemption_count': 0}), (52946, {'train/accuracy': 0.6452774405479431, 'train/loss': 1.7110553979873657, 'train/bleu': 31.52326751196575, 'validation/accuracy': 0.6643067002296448, 'validation/loss': 1.580517292022705, 'validation/bleu': 28.571725114346876, 'validation/num_examples': 3000, 'test/accuracy': 0.6761606335639954, 'test/loss': 1.5175516605377197, 'test/bleu': 28.092700297437375, 'test/num_examples': 3003, 'score': 18507.14643883705, 'total_duration': 30996.068150520325, 'accumulated_submission_time': 18507.14643883705, 'accumulated_eval_time': 12486.558814764023, 'accumulated_logging_time': 0.6497421264648438, 'global_step': 52946, 'preemption_count': 0}), (55354, {'train/accuracy': 0.6445077657699585, 'train/loss': 1.712929129600525, 'train/bleu': 31.342913567592323, 'validation/accuracy': 0.663823127746582, 'validation/loss': 1.576424241065979, 'validation/bleu': 28.29432199708726, 'validation/num_examples': 3000, 'test/accuracy': 0.6759397983551025, 'test/loss': 1.5070810317993164, 'test/bleu': 28.14646888038358, 'test/num_examples': 3003, 'score': 19347.095779180527, 'total_duration': 32369.428835868835, 'accumulated_submission_time': 19347.095779180527, 'accumulated_eval_time': 13019.85465836525, 'accumulated_logging_time': 0.6876661777496338, 'global_step': 55354, 'preemption_count': 0}), (57761, {'train/accuracy': 0.649020254611969, 'train/loss': 1.682659387588501, 'train/bleu': 32.0713855796427, 'validation/accuracy': 0.6664641499519348, 'validation/loss': 1.5685975551605225, 'validation/bleu': 28.97768596478849, 'validation/num_examples': 3000, 'test/accuracy': 0.6782522797584534, 'test/loss': 1.4985307455062866, 'test/bleu': 28.270571580751486, 'test/num_examples': 3003, 'score': 20187.175048589706, 'total_duration': 33749.7142393589, 'accumulated_submission_time': 20187.175048589706, 'accumulated_eval_time': 13559.9481112957, 'accumulated_logging_time': 0.7214796543121338, 'global_step': 57761, 'preemption_count': 0}), (60169, {'train/accuracy': 0.648051917552948, 'train/loss': 1.6906379461288452, 'train/bleu': 31.884365702800636, 'validation/accuracy': 0.6664765477180481, 'validation/loss': 1.5599160194396973, 'validation/bleu': 28.958729574332384, 'validation/num_examples': 3000, 'test/accuracy': 0.6790889501571655, 'test/loss': 1.4897910356521606, 'test/bleu': 28.51629844762099, 'test/num_examples': 3003, 'score': 21027.074191093445, 'total_duration': 35134.669848680496, 'accumulated_submission_time': 21027.074191093445, 'accumulated_eval_time': 14104.895560979843, 'accumulated_logging_time': 0.7536098957061768, 'global_step': 60169, 'preemption_count': 0}), (62577, {'train/accuracy': 0.6952161192893982, 'train/loss': 1.432866096496582, 'train/bleu': 35.779476196254954, 'validation/accuracy': 0.6689935326576233, 'validation/loss': 1.5532121658325195, 'validation/bleu': 29.131601118820857, 'validation/num_examples': 3000, 'test/accuracy': 0.678345263004303, 'test/loss': 1.491255760192871, 'test/bleu': 28.343151127526827, 'test/num_examples': 3003, 'score': 21867.165781974792, 'total_duration': 36433.94098830223, 'accumulated_submission_time': 21867.165781974792, 'accumulated_eval_time': 14563.95576763153, 'accumulated_logging_time': 0.7935366630554199, 'global_step': 62577, 'preemption_count': 0}), (64986, {'train/accuracy': 0.6524246335029602, 'train/loss': 1.6702783107757568, 'train/bleu': 31.827023341578347, 'validation/accuracy': 0.6694151163101196, 'validation/loss': 1.539971113204956, 'validation/bleu': 28.90430783369693, 'validation/num_examples': 3000, 'test/accuracy': 0.6832142472267151, 'test/loss': 1.4675744771957397, 'test/bleu': 28.673106638091205, 'test/num_examples': 3003, 'score': 22707.377870559692, 'total_duration': 37756.19357728958, 'accumulated_submission_time': 22707.377870559692, 'accumulated_eval_time': 15045.877503871918, 'accumulated_logging_time': 0.8332781791687012, 'global_step': 64986, 'preemption_count': 0}), (67394, {'train/accuracy': 0.6514863967895508, 'train/loss': 1.6692497730255127, 'train/bleu': 31.94321390091731, 'validation/accuracy': 0.6705062389373779, 'validation/loss': 1.540156364440918, 'validation/bleu': 29.242645233820422, 'validation/num_examples': 3000, 'test/accuracy': 0.6817849278450012, 'test/loss': 1.4733388423919678, 'test/bleu': 28.59808848117703, 'test/num_examples': 3003, 'score': 23547.34636592865, 'total_duration': 39197.67491483688, 'accumulated_submission_time': 23547.34636592865, 'accumulated_eval_time': 15647.19865846634, 'accumulated_logging_time': 0.9477291107177734, 'global_step': 67394, 'preemption_count': 0}), (69801, {'train/accuracy': 0.6611226201057434, 'train/loss': 1.6098320484161377, 'train/bleu': 32.63959978255694, 'validation/accuracy': 0.6719693541526794, 'validation/loss': 1.5329428911209106, 'validation/bleu': 29.068353792762746, 'validation/num_examples': 3000, 'test/accuracy': 0.684980571269989, 'test/loss': 1.4600976705551147, 'test/bleu': 29.0403300225907, 'test/num_examples': 3003, 'score': 24387.486208677292, 'total_duration': 40585.99910902977, 'accumulated_submission_time': 24387.486208677292, 'accumulated_eval_time': 16195.264653921127, 'accumulated_logging_time': 0.9868950843811035, 'global_step': 69801, 'preemption_count': 0}), (72213, {'train/accuracy': 0.6569823026657104, 'train/loss': 1.6434437036514282, 'train/bleu': 32.19100745654006, 'validation/accuracy': 0.6732092499732971, 'validation/loss': 1.5275042057037354, 'validation/bleu': 29.36287108991364, 'validation/num_examples': 3000, 'test/accuracy': 0.6867700815200806, 'test/loss': 1.4527868032455444, 'test/bleu': 29.133928657891733, 'test/num_examples': 3003, 'score': 25227.615775585175, 'total_duration': 41935.63665962219, 'accumulated_submission_time': 25227.615775585175, 'accumulated_eval_time': 16704.657161474228, 'accumulated_logging_time': 1.0241799354553223, 'global_step': 72213, 'preemption_count': 0}), (74621, {'train/accuracy': 0.6536204218864441, 'train/loss': 1.6660853624343872, 'train/bleu': 32.52132858933652, 'validation/accuracy': 0.6733828186988831, 'validation/loss': 1.5218721628189087, 'validation/bleu': 29.318499071867286, 'validation/num_examples': 3000, 'test/accuracy': 0.6867700815200806, 'test/loss': 1.4440298080444336, 'test/bleu': 28.99523113730168, 'test/num_examples': 3003, 'score': 26067.56249308586, 'total_duration': 43327.928844451904, 'accumulated_submission_time': 26067.56249308586, 'accumulated_eval_time': 17256.88942360878, 'accumulated_logging_time': 1.0615234375, 'global_step': 74621, 'preemption_count': 0}), (77029, {'train/accuracy': 0.6623920798301697, 'train/loss': 1.6091350317001343, 'train/bleu': 33.60163001132954, 'validation/accuracy': 0.6767553687095642, 'validation/loss': 1.5119057893753052, 'validation/bleu': 29.60513743671253, 'validation/num_examples': 3000, 'test/accuracy': 0.6889199018478394, 'test/loss': 1.4393484592437744, 'test/bleu': 29.566770359025814, 'test/num_examples': 3003, 'score': 26907.49367928505, 'total_duration': 44685.712255477905, 'accumulated_submission_time': 26907.49367928505, 'accumulated_eval_time': 17774.62933588028, 'accumulated_logging_time': 1.0971648693084717, 'global_step': 77029, 'preemption_count': 0}), (79440, {'train/accuracy': 0.6589401364326477, 'train/loss': 1.6313480138778687, 'train/bleu': 33.36118043719307, 'validation/accuracy': 0.6741020083427429, 'validation/loss': 1.5112824440002441, 'validation/bleu': 29.13616132874532, 'validation/num_examples': 3000, 'test/accuracy': 0.6889663934707642, 'test/loss': 1.430628776550293, 'test/bleu': 29.096602843757115, 'test/num_examples': 3003, 'score': 27747.6175262928, 'total_duration': 46043.84932875633, 'accumulated_submission_time': 27747.6175262928, 'accumulated_eval_time': 18292.526223659515, 'accumulated_logging_time': 1.133357286453247, 'global_step': 79440, 'preemption_count': 0}), (81848, {'train/accuracy': 0.6777380108833313, 'train/loss': 1.5110163688659668, 'train/bleu': 33.80495966434508, 'validation/accuracy': 0.6777721047401428, 'validation/loss': 1.4969340562820435, 'validation/bleu': 29.833317296218926, 'validation/num_examples': 3000, 'test/accuracy': 0.6912091374397278, 'test/loss': 1.4248101711273193, 'test/bleu': 29.329210019172148, 'test/num_examples': 3003, 'score': 28587.77638578415, 'total_duration': 47551.57074832916, 'accumulated_submission_time': 28587.77638578415, 'accumulated_eval_time': 18959.974626541138, 'accumulated_logging_time': 1.171102523803711, 'global_step': 81848, 'preemption_count': 0}), (84257, {'train/accuracy': 0.6639957427978516, 'train/loss': 1.5895136594772339, 'train/bleu': 33.48077142570666, 'validation/accuracy': 0.6791732311248779, 'validation/loss': 1.4911015033721924, 'validation/bleu': 29.608461936647878, 'validation/num_examples': 3000, 'test/accuracy': 0.6941258907318115, 'test/loss': 1.4093471765518188, 'test/bleu': 29.714915994475234, 'test/num_examples': 3003, 'score': 29427.856281757355, 'total_duration': 48891.9859354496, 'accumulated_submission_time': 29427.856281757355, 'accumulated_eval_time': 19460.195112228394, 'accumulated_logging_time': 1.209423542022705, 'global_step': 84257, 'preemption_count': 0}), (86666, {'train/accuracy': 0.6675595045089722, 'train/loss': 1.5793704986572266, 'train/bleu': 32.93528053933675, 'validation/accuracy': 0.6773629784584045, 'validation/loss': 1.487136960029602, 'validation/bleu': 29.583362696768006, 'validation/num_examples': 3000, 'test/accuracy': 0.693370521068573, 'test/loss': 1.4054758548736572, 'test/bleu': 29.517393962293482, 'test/num_examples': 3003, 'score': 30268.088964939117, 'total_duration': 50289.92531251907, 'accumulated_submission_time': 30268.088964939117, 'accumulated_eval_time': 20017.788903951645, 'accumulated_logging_time': 1.2460203170776367, 'global_step': 86666, 'preemption_count': 0}), (89075, {'train/accuracy': 0.6773558855056763, 'train/loss': 1.5134402513504028, 'train/bleu': 33.889525427289215, 'validation/accuracy': 0.6815290451049805, 'validation/loss': 1.4774235486984253, 'validation/bleu': 29.90880797413811, 'validation/num_examples': 3000, 'test/accuracy': 0.696914792060852, 'test/loss': 1.3995591402053833, 'test/bleu': 29.432154247263547, 'test/num_examples': 3003, 'score': 31108.269670009613, 'total_duration': 51703.35502099991, 'accumulated_submission_time': 31108.269670009613, 'accumulated_eval_time': 20590.925078868866, 'accumulated_logging_time': 1.283271074295044, 'global_step': 89075, 'preemption_count': 0}), (91482, {'train/accuracy': 0.6696231365203857, 'train/loss': 1.5658307075500488, 'train/bleu': 33.075913430571255, 'validation/accuracy': 0.6815414428710938, 'validation/loss': 1.4682722091674805, 'validation/bleu': 29.879511675630194, 'validation/num_examples': 3000, 'test/accuracy': 0.6981697678565979, 'test/loss': 1.38344144821167, 'test/bleu': 29.75321280222057, 'test/num_examples': 3003, 'score': 31948.175379037857, 'total_duration': 53102.57399082184, 'accumulated_submission_time': 31948.175379037857, 'accumulated_eval_time': 21150.12527155876, 'accumulated_logging_time': 1.319873571395874, 'global_step': 91482, 'preemption_count': 0}), (93890, {'train/accuracy': 0.7072305679321289, 'train/loss': 1.355831503868103, 'train/bleu': 36.74364605152375, 'validation/accuracy': 0.6831037402153015, 'validation/loss': 1.466017246246338, 'validation/bleu': 30.11848958134699, 'validation/num_examples': 3000, 'test/accuracy': 0.697519063949585, 'test/loss': 1.383360743522644, 'test/bleu': 29.793031114372457, 'test/num_examples': 3003, 'score': 32788.21763634682, 'total_duration': 54445.867460012436, 'accumulated_submission_time': 32788.21763634682, 'accumulated_eval_time': 21653.255216360092, 'accumulated_logging_time': 1.3652818202972412, 'global_step': 93890, 'preemption_count': 0}), (96299, {'train/accuracy': 0.6774893403053284, 'train/loss': 1.5120842456817627, 'train/bleu': 34.065012769999825, 'validation/accuracy': 0.6842816472053528, 'validation/loss': 1.452805519104004, 'validation/bleu': 30.26416928198846, 'validation/num_examples': 3000, 'test/accuracy': 0.6997618079185486, 'test/loss': 1.3693721294403076, 'test/bleu': 30.24534871633443, 'test/num_examples': 3003, 'score': 33628.44213843346, 'total_duration': 55846.56341743469, 'accumulated_submission_time': 33628.44213843346, 'accumulated_eval_time': 22213.611531734467, 'accumulated_logging_time': 1.4041471481323242, 'global_step': 96299, 'preemption_count': 0}), (98707, {'train/accuracy': 0.6742954254150391, 'train/loss': 1.5320757627487183, 'train/bleu': 34.28313749510387, 'validation/accuracy': 0.6845172047615051, 'validation/loss': 1.4518380165100098, 'validation/bleu': 30.17301385143447, 'validation/num_examples': 3000, 'test/accuracy': 0.6995061635971069, 'test/loss': 1.36750328540802, 'test/bleu': 30.188361116708307, 'test/num_examples': 3003, 'score': 34468.39393520355, 'total_duration': 57219.96085047722, 'accumulated_submission_time': 34468.39393520355, 'accumulated_eval_time': 22746.94253396988, 'accumulated_logging_time': 1.4424107074737549, 'global_step': 98707, 'preemption_count': 0}), (101115, {'train/accuracy': 0.6891303658485413, 'train/loss': 1.4438997507095337, 'train/bleu': 34.77914901250355, 'validation/accuracy': 0.6863027215003967, 'validation/loss': 1.444441556930542, 'validation/bleu': 30.29630986323259, 'validation/num_examples': 3000, 'test/accuracy': 0.701225996017456, 'test/loss': 1.3644455671310425, 'test/bleu': 30.217966551902926, 'test/num_examples': 3003, 'score': 35308.40327715874, 'total_duration': 58590.03084850311, 'accumulated_submission_time': 35308.40327715874, 'accumulated_eval_time': 23276.88682627678, 'accumulated_logging_time': 1.4825043678283691, 'global_step': 101115, 'preemption_count': 0}), (103522, {'train/accuracy': 0.6818674206733704, 'train/loss': 1.4904634952545166, 'train/bleu': 34.39548551799863, 'validation/accuracy': 0.6874682307243347, 'validation/loss': 1.439511775970459, 'validation/bleu': 30.242459944963176, 'validation/num_examples': 3000, 'test/accuracy': 0.7032827734947205, 'test/loss': 1.3556402921676636, 'test/bleu': 30.08292498174387, 'test/num_examples': 3003, 'score': 36148.475451231, 'total_duration': 59959.14341711998, 'accumulated_submission_time': 36148.475451231, 'accumulated_eval_time': 23805.797435045242, 'accumulated_logging_time': 1.5310685634613037, 'global_step': 103522, 'preemption_count': 0}), (105930, {'train/accuracy': 0.6841477751731873, 'train/loss': 1.4826703071594238, 'train/bleu': 34.1736090872612, 'validation/accuracy': 0.68779057264328, 'validation/loss': 1.4393343925476074, 'validation/bleu': 30.294645014237727, 'validation/num_examples': 3000, 'test/accuracy': 0.7045843005180359, 'test/loss': 1.3529555797576904, 'test/bleu': 30.341159836823227, 'test/num_examples': 3003, 'score': 36988.668182611465, 'total_duration': 61317.41492795944, 'accumulated_submission_time': 36988.668182611465, 'accumulated_eval_time': 24323.759889364243, 'accumulated_logging_time': 1.5710361003875732, 'global_step': 105930, 'preemption_count': 0}), (108337, {'train/accuracy': 0.6883156895637512, 'train/loss': 1.451397180557251, 'train/bleu': 35.21473145739453, 'validation/accuracy': 0.6892040967941284, 'validation/loss': 1.4314101934432983, 'validation/bleu': 30.54711567710777, 'validation/num_examples': 3000, 'test/accuracy': 0.7040265202522278, 'test/loss': 1.3469500541687012, 'test/bleu': 30.276564514231993, 'test/num_examples': 3003, 'score': 37828.766984939575, 'total_duration': 62684.69976902008, 'accumulated_submission_time': 37828.766984939575, 'accumulated_eval_time': 24850.823167324066, 'accumulated_logging_time': 1.614072561264038, 'global_step': 108337, 'preemption_count': 0}), (110745, {'train/accuracy': 0.6906625032424927, 'train/loss': 1.4407415390014648, 'train/bleu': 35.11437290724362, 'validation/accuracy': 0.6892908811569214, 'validation/loss': 1.4316939115524292, 'validation/bleu': 30.623534904601236, 'validation/num_examples': 3000, 'test/accuracy': 0.7047585844993591, 'test/loss': 1.3457623720169067, 'test/bleu': 30.64520647586331, 'test/num_examples': 3003, 'score': 38668.92079091072, 'total_duration': 64037.598447322845, 'accumulated_submission_time': 38668.92079091072, 'accumulated_eval_time': 25363.45061326027, 'accumulated_logging_time': 1.6543505191802979, 'global_step': 110745, 'preemption_count': 0}), (113152, {'train/accuracy': 0.7029312252998352, 'train/loss': 1.376530647277832, 'train/bleu': 36.05811097263953, 'validation/accuracy': 0.691299557685852, 'validation/loss': 1.4230973720550537, 'validation/bleu': 30.798797566594644, 'validation/num_examples': 3000, 'test/accuracy': 0.705909013748169, 'test/loss': 1.339033603668213, 'test/bleu': 30.242559942520206, 'test/num_examples': 3003, 'score': 39508.85071182251, 'total_duration': 65501.97001576424, 'accumulated_submission_time': 39508.85071182251, 'accumulated_eval_time': 25987.774053812027, 'accumulated_logging_time': 1.6964483261108398, 'global_step': 113152, 'preemption_count': 0}), (115560, {'train/accuracy': 0.7006024122238159, 'train/loss': 1.3854455947875977, 'train/bleu': 35.91853943560247, 'validation/accuracy': 0.6913491487503052, 'validation/loss': 1.4228339195251465, 'validation/bleu': 30.759350276539493, 'validation/num_examples': 3000, 'test/accuracy': 0.7081633806228638, 'test/loss': 1.335689663887024, 'test/bleu': 30.608363709435334, 'test/num_examples': 3003, 'score': 40348.93051624298, 'total_duration': 66873.91002559662, 'accumulated_submission_time': 40348.93051624298, 'accumulated_eval_time': 26519.516964912415, 'accumulated_logging_time': 1.7374508380889893, 'global_step': 115560, 'preemption_count': 0}), (117969, {'train/accuracy': 0.6944806575775146, 'train/loss': 1.42534601688385, 'train/bleu': 35.57020902703697, 'validation/accuracy': 0.691969096660614, 'validation/loss': 1.4174968004226685, 'validation/bleu': 30.88002752465824, 'validation/num_examples': 3000, 'test/accuracy': 0.7076637148857117, 'test/loss': 1.3267302513122559, 'test/bleu': 30.73994588003593, 'test/num_examples': 3003, 'score': 41188.847472667694, 'total_duration': 68230.25590658188, 'accumulated_submission_time': 41188.847472667694, 'accumulated_eval_time': 27035.818858623505, 'accumulated_logging_time': 1.779308557510376, 'global_step': 117969, 'preemption_count': 0}), (120376, {'train/accuracy': 0.7052478790283203, 'train/loss': 1.3589560985565186, 'train/bleu': 36.04900926844258, 'validation/accuracy': 0.6927626132965088, 'validation/loss': 1.4140264987945557, 'validation/bleu': 31.06434313028884, 'validation/num_examples': 3000, 'test/accuracy': 0.7080472111701965, 'test/loss': 1.3269232511520386, 'test/bleu': 30.485523564052155, 'test/num_examples': 3003, 'score': 42028.750698804855, 'total_duration': 69623.21995973587, 'accumulated_submission_time': 42028.750698804855, 'accumulated_eval_time': 27588.76014494896, 'accumulated_logging_time': 1.8211784362792969, 'global_step': 120376, 'preemption_count': 0}), (122788, {'train/accuracy': 0.7059203386306763, 'train/loss': 1.3507002592086792, 'train/bleu': 36.08858036119556, 'validation/accuracy': 0.6928866505622864, 'validation/loss': 1.412611722946167, 'validation/bleu': 30.960635533980913, 'validation/num_examples': 3000, 'test/accuracy': 0.709337055683136, 'test/loss': 1.3233616352081299, 'test/bleu': 30.99924664179277, 'test/num_examples': 3003, 'score': 42868.7630045414, 'total_duration': 70992.43810915947, 'accumulated_submission_time': 42868.7630045414, 'accumulated_eval_time': 28117.846455574036, 'accumulated_logging_time': 1.864339828491211, 'global_step': 122788, 'preemption_count': 0}), (125195, {'train/accuracy': 0.7114030718803406, 'train/loss': 1.329108476638794, 'train/bleu': 37.05163518567466, 'validation/accuracy': 0.6935189962387085, 'validation/loss': 1.4125992059707642, 'validation/bleu': 30.957929462228876, 'validation/num_examples': 3000, 'test/accuracy': 0.7094649076461792, 'test/loss': 1.3227872848510742, 'test/bleu': 30.60444715357637, 'test/num_examples': 3003, 'score': 43708.71370458603, 'total_duration': 72364.23915076256, 'accumulated_submission_time': 43708.71370458603, 'accumulated_eval_time': 28649.569895029068, 'accumulated_logging_time': 1.914734125137329, 'global_step': 125195, 'preemption_count': 0}), (127603, {'train/accuracy': 0.7080180048942566, 'train/loss': 1.3448046445846558, 'train/bleu': 36.594626517179826, 'validation/accuracy': 0.6943497061729431, 'validation/loss': 1.4095133543014526, 'validation/bleu': 30.972036209681566, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.321054458618164, 'test/bleu': 30.63157748084628, 'test/num_examples': 3003, 'score': 44548.620322942734, 'total_duration': 73723.40506076813, 'accumulated_submission_time': 44548.620322942734, 'accumulated_eval_time': 29168.708927631378, 'accumulated_logging_time': 1.9580953121185303, 'global_step': 127603, 'preemption_count': 0}), (130010, {'train/accuracy': 0.7112451791763306, 'train/loss': 1.3329684734344482, 'train/bleu': 37.12121517508366, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.4096252918243408, 'validation/bleu': 31.136288279625116, 'validation/num_examples': 3000, 'test/accuracy': 0.7105107307434082, 'test/loss': 1.320068359375, 'test/bleu': 30.66140440468964, 'test/num_examples': 3003, 'score': 45388.499920129776, 'total_duration': 75089.05698800087, 'accumulated_submission_time': 45388.499920129776, 'accumulated_eval_time': 29694.359369277954, 'accumulated_logging_time': 2.001633644104004, 'global_step': 130010, 'preemption_count': 0}), (132416, {'train/accuracy': 0.7100916504859924, 'train/loss': 1.3323860168457031, 'train/bleu': 37.0542430438851, 'validation/accuracy': 0.6940397620201111, 'validation/loss': 1.4101505279541016, 'validation/bleu': 31.14853330741573, 'validation/num_examples': 3000, 'test/accuracy': 0.7104874849319458, 'test/loss': 1.3203920125961304, 'test/bleu': 30.713799107089375, 'test/num_examples': 3003, 'score': 46228.42295074463, 'total_duration': 76468.07191419601, 'accumulated_submission_time': 46228.42295074463, 'accumulated_eval_time': 30233.320573568344, 'accumulated_logging_time': 2.052457571029663, 'global_step': 132416, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7102746367454529, 'train/loss': 1.3364615440368652, 'train/bleu': 37.00281637120834, 'validation/accuracy': 0.6941141486167908, 'validation/loss': 1.4101862907409668, 'validation/bleu': 31.145169924199735, 'validation/num_examples': 3000, 'test/accuracy': 0.7104526162147522, 'test/loss': 1.3205291032791138, 'test/bleu': 30.73949967608062, 'test/num_examples': 3003, 'score': 46548.19679522514, 'total_duration': 77308.42940235138, 'accumulated_submission_time': 46548.19679522514, 'accumulated_eval_time': 30753.830042362213, 'accumulated_logging_time': 2.0963242053985596, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0209 02:22:35.218101 139785736898368 submission_runner.py:586] Timing: 46548.19679522514
I0209 02:22:35.218161 139785736898368 submission_runner.py:588] Total number of evals: 57
I0209 02:22:35.218203 139785736898368 submission_runner.py:589] ====================
I0209 02:22:35.218249 139785736898368 submission_runner.py:542] Using RNG seed 3586669017
I0209 02:22:35.219836 139785736898368 submission_runner.py:551] --- Tuning run 4/5 ---
I0209 02:22:35.219956 139785736898368 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4.
I0209 02:22:35.220416 139785736898368 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4/hparams.json.
I0209 02:22:35.221298 139785736898368 submission_runner.py:206] Initializing dataset.
I0209 02:22:35.224051 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 02:22:35.226964 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0209 02:22:35.265335 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 02:22:35.908164 139785736898368 submission_runner.py:213] Initializing model.
I0209 02:22:42.349269 139785736898368 submission_runner.py:255] Initializing optimizer.
I0209 02:22:43.165932 139785736898368 submission_runner.py:262] Initializing metrics bundle.
I0209 02:22:43.166107 139785736898368 submission_runner.py:280] Initializing checkpoint and logger.
I0209 02:22:43.167064 139785736898368 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4 with prefix checkpoint_
I0209 02:22:43.167191 139785736898368 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4/meta_data_0.json.
I0209 02:22:43.167420 139785736898368 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 02:22:43.167484 139785736898368 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 02:22:43.670157 139785736898368 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 02:22:44.145617 139785736898368 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4/flags_0.json.
I0209 02:22:44.149198 139785736898368 submission_runner.py:314] Starting training loop.
I0209 02:23:11.464872 139615897859840 logging_writer.py:48] [0] global_step=0, grad_norm=5.592713832855225, loss=11.052679061889648
I0209 02:23:11.476422 139785736898368 spec.py:321] Evaluating on the training split.
I0209 02:23:14.165340 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:28:02.171198 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 02:28:04.906402 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:32:53.525820 139785736898368 spec.py:349] Evaluating on the test split.
I0209 02:32:56.261500 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:37:43.114988 139785736898368 submission_runner.py:408] Time since start: 898.97s, 	Step: 1, 	{'train/accuracy': 0.0004938441561535001, 'train/loss': 11.066301345825195, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.327144145965576, 'total_duration': 898.9657063484192, 'accumulated_submission_time': 27.327144145965576, 'accumulated_eval_time': 871.6384983062744, 'accumulated_logging_time': 0}
I0209 02:37:43.125307 139615906252544 logging_writer.py:48] [1] accumulated_eval_time=871.638498, accumulated_logging_time=0, accumulated_submission_time=27.327144, global_step=1, preemption_count=0, score=27.327144, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.041826, test/num_examples=3003, total_duration=898.965706, train/accuracy=0.000494, train/bleu=0.000000, train/loss=11.066301, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.036646, validation/num_examples=3000
I0209 02:38:18.057416 139615897859840 logging_writer.py:48] [100] global_step=100, grad_norm=0.5568857789039612, loss=7.583726406097412
I0209 02:38:53.020831 139615906252544 logging_writer.py:48] [200] global_step=200, grad_norm=0.5368911027908325, loss=6.576267242431641
I0209 02:39:27.956323 139615897859840 logging_writer.py:48] [300] global_step=300, grad_norm=0.5887362360954285, loss=5.885468006134033
I0209 02:40:02.919137 139615906252544 logging_writer.py:48] [400] global_step=400, grad_norm=0.387145459651947, loss=5.331754684448242
I0209 02:40:37.876360 139615897859840 logging_writer.py:48] [500] global_step=500, grad_norm=0.4167199730873108, loss=5.0099592208862305
I0209 02:41:12.874236 139615906252544 logging_writer.py:48] [600] global_step=600, grad_norm=0.5747637152671814, loss=4.7823896408081055
I0209 02:41:47.891172 139615897859840 logging_writer.py:48] [700] global_step=700, grad_norm=0.502460777759552, loss=4.458037853240967
I0209 02:42:22.878437 139615906252544 logging_writer.py:48] [800] global_step=800, grad_norm=0.49296826124191284, loss=4.097362518310547
I0209 02:42:57.902207 139615897859840 logging_writer.py:48] [900] global_step=900, grad_norm=0.4298290014266968, loss=3.9813647270202637
I0209 02:43:32.865253 139615906252544 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4646872282028198, loss=3.8193137645721436
I0209 02:44:07.849171 139615897859840 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4724588692188263, loss=3.6509950160980225
I0209 02:44:42.761461 139615906252544 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.38255563378334045, loss=3.58719539642334
I0209 02:45:17.682540 139615897859840 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.33420640230178833, loss=3.578428268432617
I0209 02:45:52.596970 139615906252544 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.3196429908275604, loss=3.3677515983581543
I0209 02:46:27.516356 139615897859840 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.38953033089637756, loss=3.277451992034912
I0209 02:47:02.411535 139615906252544 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.38463854789733887, loss=3.2426509857177734
I0209 02:47:37.294473 139615897859840 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2656855285167694, loss=3.133812427520752
I0209 02:48:12.258960 139615906252544 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.26104116439819336, loss=2.981555700302124
I0209 02:48:47.179896 139615897859840 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.31719985604286194, loss=2.9055721759796143
I0209 02:49:22.042630 139615906252544 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.28138959407806396, loss=2.8770253658294678
I0209 02:49:56.936298 139615897859840 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.18977995216846466, loss=2.733907699584961
I0209 02:50:31.858951 139615906252544 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.25899264216423035, loss=2.7744927406311035
I0209 02:51:06.732162 139615897859840 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.16525329649448395, loss=2.6883833408355713
I0209 02:51:41.630839 139615906252544 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.23351912200450897, loss=2.6340174674987793
I0209 02:51:43.451194 139785736898368 spec.py:321] Evaluating on the training split.
I0209 02:51:46.441215 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:54:21.664255 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 02:54:24.369588 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:56:59.332479 139785736898368 spec.py:349] Evaluating on the test split.
I0209 02:57:02.035431 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 02:59:27.138417 139785736898368 submission_runner.py:408] Time since start: 2202.99s, 	Step: 2407, 	{'train/accuracy': 0.5285830497741699, 'train/loss': 2.639150857925415, 'train/bleu': 22.527051695575885, 'validation/accuracy': 0.5278669595718384, 'validation/loss': 2.6120784282684326, 'validation/bleu': 18.724227858741123, 'validation/num_examples': 3000, 'test/accuracy': 0.5244320631027222, 'test/loss': 2.647041082382202, 'test/bleu': 17.176751782717933, 'test/num_examples': 3003, 'score': 867.5621616840363, 'total_duration': 2202.9891409873962, 'accumulated_submission_time': 867.5621616840363, 'accumulated_eval_time': 1335.3256645202637, 'accumulated_logging_time': 0.022200345993041992}
I0209 02:59:27.154168 139615897859840 logging_writer.py:48] [2407] accumulated_eval_time=1335.325665, accumulated_logging_time=0.022200, accumulated_submission_time=867.562162, global_step=2407, preemption_count=0, score=867.562162, test/accuracy=0.524432, test/bleu=17.176752, test/loss=2.647041, test/num_examples=3003, total_duration=2202.989141, train/accuracy=0.528583, train/bleu=22.527052, train/loss=2.639151, validation/accuracy=0.527867, validation/bleu=18.724228, validation/loss=2.612078, validation/num_examples=3000
I0209 02:59:59.948457 139615906252544 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.2703305184841156, loss=2.5681543350219727
I0209 03:00:34.870993 139615897859840 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.1980748474597931, loss=2.560554027557373
I0209 03:01:09.804165 139615906252544 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4404686689376831, loss=2.5488686561584473
I0209 03:01:44.686708 139615897859840 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.3491404950618744, loss=2.501619815826416
I0209 03:02:19.580048 139615906252544 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.17676813900470734, loss=2.490898609161377
I0209 03:02:54.458531 139615897859840 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.16483594477176666, loss=2.4348714351654053
I0209 03:03:29.387318 139615906252544 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2783028781414032, loss=2.300288438796997
I0209 03:04:04.379188 139615897859840 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.19533230364322662, loss=2.4195868968963623
I0209 03:04:39.284356 139615906252544 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.38513997197151184, loss=2.3897294998168945
I0209 03:05:14.186202 139615897859840 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.27487683296203613, loss=2.3095450401306152
I0209 03:05:49.071722 139615906252544 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.33105289936065674, loss=2.2602694034576416
I0209 03:06:23.956682 139615897859840 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.21038520336151123, loss=2.4380645751953125
I0209 03:06:58.865661 139615906252544 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.39016005396842957, loss=2.2759652137756348
I0209 03:07:33.753141 139615897859840 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6131134629249573, loss=2.297240734100342
I0209 03:08:08.669032 139615906252544 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.27262526750564575, loss=2.248629331588745
I0209 03:08:43.593332 139615897859840 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.507927417755127, loss=2.3426506519317627
I0209 03:09:18.525952 139615906252544 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.34731271862983704, loss=2.120701551437378
I0209 03:09:53.487136 139615897859840 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3410152792930603, loss=2.2494595050811768
I0209 03:10:28.350758 139615906252544 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.47927629947662354, loss=2.33918833732605
I0209 03:11:03.253312 139615897859840 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.35046979784965515, loss=2.239806652069092
I0209 03:11:38.180512 139615906252544 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.219865620136261, loss=2.1759543418884277
I0209 03:12:13.080090 139615897859840 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.35784369707107544, loss=2.216097116470337
I0209 03:12:47.986055 139615906252544 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.49367573857307434, loss=2.164602756500244
I0209 03:13:22.872160 139615897859840 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7004782557487488, loss=2.2772092819213867
I0209 03:13:27.482371 139785736898368 spec.py:321] Evaluating on the training split.
I0209 03:13:30.474465 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 03:17:10.397945 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 03:17:13.094979 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 03:20:15.617893 139785736898368 spec.py:349] Evaluating on the test split.
I0209 03:20:18.322919 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 03:23:05.984803 139785736898368 submission_runner.py:408] Time since start: 3621.84s, 	Step: 4815, 	{'train/accuracy': 0.5797721147537231, 'train/loss': 2.221475601196289, 'train/bleu': 27.448449614774468, 'validation/accuracy': 0.5930862426757812, 'validation/loss': 2.0885164737701416, 'validation/bleu': 23.89420124644968, 'validation/num_examples': 3000, 'test/accuracy': 0.5975945591926575, 'test/loss': 2.0635406970977783, 'test/bleu': 22.466849339434948, 'test/num_examples': 3003, 'score': 1707.8006699085236, 'total_duration': 3621.8355102539062, 'accumulated_submission_time': 1707.8006699085236, 'accumulated_eval_time': 1913.8280260562897, 'accumulated_logging_time': 0.049674034118652344}
I0209 03:23:05.999635 139615906252544 logging_writer.py:48] [4815] accumulated_eval_time=1913.828026, accumulated_logging_time=0.049674, accumulated_submission_time=1707.800670, global_step=4815, preemption_count=0, score=1707.800670, test/accuracy=0.597595, test/bleu=22.466849, test/loss=2.063541, test/num_examples=3003, total_duration=3621.835510, train/accuracy=0.579772, train/bleu=27.448450, train/loss=2.221476, validation/accuracy=0.593086, validation/bleu=23.894201, validation/loss=2.088516, validation/num_examples=3000
I0209 03:23:35.979778 139615897859840 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3508569002151489, loss=2.1731462478637695
I0209 03:24:10.830892 139615906252544 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.723351776599884, loss=2.1798532009124756
I0209 03:24:45.710086 139615897859840 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.30662986636161804, loss=2.240741491317749
I0209 03:25:20.610725 139615906252544 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4327317774295807, loss=2.2330760955810547
I0209 03:25:55.536500 139615897859840 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.2767842411994934, loss=2.224456787109375
I0209 03:26:30.494317 139615906252544 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.33051252365112305, loss=2.2132463455200195
I0209 03:27:05.395325 139615897859840 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.30799341201782227, loss=2.1421914100646973
I0209 03:27:40.278510 139615906252544 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.35895028710365295, loss=2.14703106880188
I0209 03:28:15.132096 139615897859840 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5118942260742188, loss=2.239271879196167
I0209 03:28:50.059657 139615906252544 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.35469281673431396, loss=2.201212167739868
I0209 03:29:24.947052 139615897859840 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9474306702613831, loss=2.232130527496338
I0209 03:29:59.850262 139615906252544 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.25832265615463257, loss=2.178785800933838
I0209 03:30:34.732081 139615897859840 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5609577298164368, loss=2.1623430252075195
I0209 03:31:09.607912 139615906252544 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.3018389642238617, loss=2.1818859577178955
I0209 03:31:44.506785 139615906252544 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.28615331649780273, loss=2.125969409942627
I0209 03:32:19.388075 139615914645248 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.48306959867477417, loss=2.2217721939086914
I0209 03:32:54.270230 139615906252544 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.27430543303489685, loss=2.2053608894348145
I0209 03:33:29.194098 139615914645248 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4802669584751129, loss=2.1977736949920654
I0209 03:34:04.126439 139615906252544 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.29744699597358704, loss=2.158585786819458
I0209 03:34:39.015969 139615914645248 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4979332983493805, loss=2.16726016998291
I0209 03:35:13.919359 139615906252544 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.0662822723388672, loss=2.2105660438537598
I0209 03:35:48.833732 139615914645248 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.3405684530735016, loss=2.1720657348632812
I0209 03:36:23.733524 139615906252544 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.44328784942626953, loss=2.1457090377807617
I0209 03:36:58.671219 139615914645248 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5278148651123047, loss=2.21742844581604
I0209 03:37:06.076238 139785736898368 spec.py:321] Evaluating on the training split.
I0209 03:37:09.087176 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 03:39:58.832636 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 03:40:01.565522 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 03:42:39.687823 139785736898368 spec.py:349] Evaluating on the test split.
I0209 03:42:42.403686 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 03:45:13.017584 139785736898368 submission_runner.py:408] Time since start: 4948.87s, 	Step: 7223, 	{'train/accuracy': 0.5924304723739624, 'train/loss': 2.1243929862976074, 'train/bleu': 28.306294692431962, 'validation/accuracy': 0.6062664985656738, 'validation/loss': 2.0095314979553223, 'validation/bleu': 24.714309696920097, 'validation/num_examples': 3000, 'test/accuracy': 0.611388087272644, 'test/loss': 1.9671002626419067, 'test/bleu': 23.430505913385765, 'test/num_examples': 3003, 'score': 2547.789595603943, 'total_duration': 4948.868293046951, 'accumulated_submission_time': 2547.789595603943, 'accumulated_eval_time': 2400.7693164348602, 'accumulated_logging_time': 0.0744776725769043}
I0209 03:45:13.033216 139615906252544 logging_writer.py:48] [7223] accumulated_eval_time=2400.769316, accumulated_logging_time=0.074478, accumulated_submission_time=2547.789596, global_step=7223, preemption_count=0, score=2547.789596, test/accuracy=0.611388, test/bleu=23.430506, test/loss=1.967100, test/num_examples=3003, total_duration=4948.868293, train/accuracy=0.592430, train/bleu=28.306295, train/loss=2.124393, validation/accuracy=0.606266, validation/bleu=24.714310, validation/loss=2.009531, validation/num_examples=3000
I0209 03:45:40.225885 139615914645248 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2876335680484772, loss=2.1244118213653564
I0209 03:46:15.121045 139615906252544 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.37343448400497437, loss=2.1397104263305664
I0209 03:46:50.011833 139615914645248 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.46572113037109375, loss=2.08996844291687
I0209 03:47:24.901365 139615906252544 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6492601037025452, loss=2.23390793800354
I0209 03:47:59.797106 139615914645248 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.42885008454322815, loss=2.041771650314331
I0209 03:48:34.686190 139615906252544 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3944513499736786, loss=2.053077220916748
I0209 03:49:09.584313 139615914645248 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5097304582595825, loss=2.0799436569213867
I0209 03:49:44.485098 139615906252544 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7328412532806396, loss=2.270193576812744
I0209 03:50:19.420003 139615914645248 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7688007354736328, loss=2.086252212524414
I0209 03:50:54.370809 139615906252544 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.681300938129425, loss=2.1539766788482666
I0209 03:51:29.251023 139615914645248 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.28878530859947205, loss=2.130661725997925
I0209 03:52:04.207843 139615906252544 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3042396605014801, loss=2.114415407180786
I0209 03:52:39.102129 139615914645248 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.2934480607509613, loss=2.182799816131592
I0209 03:53:13.996097 139615906252544 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3085096776485443, loss=2.115455150604248
I0209 03:53:48.890130 139615914645248 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.45174553990364075, loss=2.138164520263672
I0209 03:54:23.767453 139615906252544 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3335634469985962, loss=2.2067785263061523
I0209 03:54:58.654883 139615914645248 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.2912047803401947, loss=2.079796314239502
I0209 03:55:33.510267 139615906252544 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6684834361076355, loss=2.10113263130188
I0209 03:56:08.395401 139615914645248 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4148423969745636, loss=2.1610052585601807
I0209 03:56:43.274213 139615906252544 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.45152994990348816, loss=2.2317750453948975
I0209 03:57:18.165051 139615914645248 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6099838018417358, loss=2.069138765335083
I0209 03:57:53.046486 139615906252544 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4108288884162903, loss=2.176823616027832
I0209 03:58:27.980315 139615914645248 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.39754050970077515, loss=2.1438159942626953
I0209 03:59:02.845181 139615906252544 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8167133927345276, loss=2.097970724105835
I0209 03:59:13.030968 139785736898368 spec.py:321] Evaluating on the training split.
I0209 03:59:16.022572 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:01:53.437578 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 04:01:56.134276 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:04:24.403631 139785736898368 spec.py:349] Evaluating on the test split.
I0209 04:04:27.107545 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:06:44.962985 139785736898368 submission_runner.py:408] Time since start: 6240.81s, 	Step: 9631, 	{'train/accuracy': 0.5950818657875061, 'train/loss': 2.1058120727539062, 'train/bleu': 27.14673853695992, 'validation/accuracy': 0.6074815988540649, 'validation/loss': 1.9885951280593872, 'validation/bleu': 24.029534022545416, 'validation/num_examples': 3000, 'test/accuracy': 0.612155020236969, 'test/loss': 1.9553987979888916, 'test/bleu': 22.814568874859706, 'test/num_examples': 3003, 'score': 3387.7001433372498, 'total_duration': 6240.813716411591, 'accumulated_submission_time': 3387.7001433372498, 'accumulated_eval_time': 2852.701284646988, 'accumulated_logging_time': 0.10046052932739258}
I0209 04:06:44.978797 139615914645248 logging_writer.py:48] [9631] accumulated_eval_time=2852.701285, accumulated_logging_time=0.100461, accumulated_submission_time=3387.700143, global_step=9631, preemption_count=0, score=3387.700143, test/accuracy=0.612155, test/bleu=22.814569, test/loss=1.955399, test/num_examples=3003, total_duration=6240.813716, train/accuracy=0.595082, train/bleu=27.146739, train/loss=2.105812, validation/accuracy=0.607482, validation/bleu=24.029534, validation/loss=1.988595, validation/num_examples=3000
I0209 04:07:09.371791 139615906252544 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4489672780036926, loss=2.0774142742156982
I0209 04:07:44.261617 139615914645248 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4179985523223877, loss=2.156337261199951
I0209 04:08:19.306838 139615906252544 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.368208646774292, loss=2.0852363109588623
I0209 04:08:54.195073 139615914645248 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.37699535489082336, loss=2.0197129249572754
I0209 04:09:29.070313 139615906252544 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3482179343700409, loss=2.08418345451355
I0209 04:10:03.939516 139615914645248 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4060187041759491, loss=2.153773546218872
I0209 04:10:38.827187 139615906252544 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.2542715072631836, loss=1.9964145421981812
I0209 04:11:13.725389 139615914645248 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.28535348176956177, loss=2.035743474960327
I0209 04:11:48.639972 139615906252544 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5574662089347839, loss=2.1562647819519043
I0209 04:12:23.548635 139615914645248 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.34194549918174744, loss=2.0743322372436523
I0209 04:12:58.467047 139615906252544 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.411363810300827, loss=2.137779712677002
I0209 04:13:33.394943 139615914645248 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.35080084204673767, loss=2.123523473739624
I0209 04:14:08.308514 139615906252544 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2696324288845062, loss=2.0714988708496094
I0209 04:14:43.193511 139615914645248 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9216163754463196, loss=2.1800851821899414
I0209 04:15:18.066256 139615906252544 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5163356065750122, loss=2.071902275085449
I0209 04:15:52.945389 139615914645248 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6247187256813049, loss=2.117877960205078
I0209 04:16:27.799391 139615906252544 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.533085286617279, loss=2.1632580757141113
I0209 04:17:02.695663 139615914645248 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.42116019129753113, loss=2.025705575942993
I0209 04:17:37.613055 139615906252544 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2833814024925232, loss=2.187119722366333
I0209 04:18:12.507031 139615914645248 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3337102234363556, loss=2.042306900024414
I0209 04:18:47.355332 139615906252544 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7167447209358215, loss=2.1034839153289795
I0209 04:19:22.248098 139615914645248 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.4618702828884125, loss=2.075056791305542
I0209 04:19:57.128023 139615906252544 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.739108145236969, loss=2.1201236248016357
I0209 04:20:32.043178 139615914645248 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9606069922447205, loss=2.163815498352051
I0209 04:20:45.036248 139785736898368 spec.py:321] Evaluating on the training split.
I0209 04:20:48.020313 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:23:46.441962 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 04:23:49.141588 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:26:19.961161 139785736898368 spec.py:349] Evaluating on the test split.
I0209 04:26:22.667204 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:28:39.504025 139785736898368 submission_runner.py:408] Time since start: 7555.35s, 	Step: 12039, 	{'train/accuracy': 0.595694899559021, 'train/loss': 2.1055915355682373, 'train/bleu': 28.150822337375132, 'validation/accuracy': 0.612738847732544, 'validation/loss': 1.9446474313735962, 'validation/bleu': 24.70037868232737, 'validation/num_examples': 3000, 'test/accuracy': 0.6214398145675659, 'test/loss': 1.895939826965332, 'test/bleu': 23.912404164530532, 'test/num_examples': 3003, 'score': 4227.669346570969, 'total_duration': 7555.3547575473785, 'accumulated_submission_time': 4227.669346570969, 'accumulated_eval_time': 3327.1690158843994, 'accumulated_logging_time': 0.12771081924438477}
I0209 04:28:39.519850 139615906252544 logging_writer.py:48] [12039] accumulated_eval_time=3327.169016, accumulated_logging_time=0.127711, accumulated_submission_time=4227.669347, global_step=12039, preemption_count=0, score=4227.669347, test/accuracy=0.621440, test/bleu=23.912404, test/loss=1.895940, test/num_examples=3003, total_duration=7555.354758, train/accuracy=0.595695, train/bleu=28.150822, train/loss=2.105592, validation/accuracy=0.612739, validation/bleu=24.700379, validation/loss=1.944647, validation/num_examples=3000
I0209 04:29:01.130088 139615914645248 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3044959604740143, loss=2.065929889678955
I0209 04:29:35.993843 139615906252544 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.40788665413856506, loss=2.0761842727661133
I0209 04:30:10.878564 139615914645248 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2586880326271057, loss=2.124051809310913
I0209 04:30:45.747205 139615906252544 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4964384436607361, loss=2.0206761360168457
I0209 04:31:20.623386 139615914645248 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.963752031326294, loss=2.143326997756958
I0209 04:31:55.563662 139615906252544 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4322834014892578, loss=2.0602266788482666
I0209 04:32:30.490436 139615914645248 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.8021339774131775, loss=2.072178363800049
I0209 04:33:05.382967 139615906252544 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.607886552810669, loss=2.215120315551758
I0209 04:33:40.258646 139615914645248 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.49176108837127686, loss=2.0020668506622314
I0209 04:34:15.169176 139615906252544 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7280117273330688, loss=2.1081085205078125
I0209 04:34:50.092930 139615914645248 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6834158301353455, loss=2.156553268432617
I0209 04:35:24.972401 139615906252544 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.38202622532844543, loss=2.1440508365631104
I0209 04:35:59.844353 139615914645248 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.5684738755226135, loss=2.1424787044525146
I0209 04:36:34.746331 139615906252544 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.4623737037181854, loss=2.105820655822754
I0209 04:37:09.639430 139615914645248 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.49858564138412476, loss=2.1588404178619385
I0209 04:37:44.532188 139615906252544 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4507773518562317, loss=2.0600497722625732
I0209 04:38:19.434991 139615914645248 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5656570196151733, loss=2.1007254123687744
I0209 04:38:54.399380 139615906252544 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3220979869365692, loss=2.046785831451416
I0209 04:39:29.281120 139615914645248 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.390071839094162, loss=2.059091091156006
I0209 04:40:04.176994 139615906252544 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.290665864944458, loss=2.0963425636291504
I0209 04:40:39.108192 139615914645248 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.463011771440506, loss=2.1164326667785645
I0209 04:41:14.010980 139615906252544 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.37842121720314026, loss=2.0071628093719482
I0209 04:41:48.922739 139615914645248 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.388480544090271, loss=2.0460903644561768
I0209 04:42:23.823495 139615906252544 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.8164034485816956, loss=2.0740699768066406
I0209 04:42:39.593909 139785736898368 spec.py:321] Evaluating on the training split.
I0209 04:42:42.592780 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:46:05.485262 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 04:46:08.203443 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:48:55.622547 139785736898368 spec.py:349] Evaluating on the test split.
I0209 04:48:58.335383 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 04:51:46.435774 139785736898368 submission_runner.py:408] Time since start: 8942.29s, 	Step: 14447, 	{'train/accuracy': 0.597169041633606, 'train/loss': 2.078855276107788, 'train/bleu': 28.419443225761, 'validation/accuracy': 0.615293025970459, 'validation/loss': 1.9333375692367554, 'validation/bleu': 25.01238163048505, 'validation/num_examples': 3000, 'test/accuracy': 0.6236011981964111, 'test/loss': 1.8801435232162476, 'test/bleu': 24.321233521773724, 'test/num_examples': 3003, 'score': 5067.6564655303955, 'total_duration': 8942.286494970322, 'accumulated_submission_time': 5067.6564655303955, 'accumulated_eval_time': 3874.0108201503754, 'accumulated_logging_time': 0.15516328811645508}
I0209 04:51:46.452379 139615914645248 logging_writer.py:48] [14447] accumulated_eval_time=3874.010820, accumulated_logging_time=0.155163, accumulated_submission_time=5067.656466, global_step=14447, preemption_count=0, score=5067.656466, test/accuracy=0.623601, test/bleu=24.321234, test/loss=1.880144, test/num_examples=3003, total_duration=8942.286495, train/accuracy=0.597169, train/bleu=28.419443, train/loss=2.078855, validation/accuracy=0.615293, validation/bleu=25.012382, validation/loss=1.933338, validation/num_examples=3000
I0209 04:52:05.296106 139615906252544 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.320525586605072, loss=2.053558588027954
I0209 04:52:40.140365 139615914645248 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.31295186281204224, loss=2.1485068798065186
I0209 04:53:15.025255 139615906252544 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7076069712638855, loss=2.0521888732910156
I0209 04:53:49.910687 139615914645248 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.46874096989631653, loss=2.124840021133423
I0209 04:54:24.795267 139615906252544 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3658473789691925, loss=2.0741751194000244
I0209 04:54:59.675208 139615914645248 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5406505465507507, loss=2.101599931716919
I0209 04:55:34.617612 139615906252544 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8473068475723267, loss=2.0822978019714355
I0209 04:56:09.478378 139615914645248 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5077754855155945, loss=2.103365421295166
I0209 04:56:44.352341 139615906252544 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.385879784822464, loss=2.097959280014038
I0209 04:57:19.211536 139615914645248 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5712990164756775, loss=2.107900381088257
I0209 04:57:54.085901 139615906252544 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.759844183921814, loss=2.0748844146728516
I0209 04:58:28.982995 139615914645248 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5329566597938538, loss=2.1616053581237793
I0209 04:59:03.962280 139615906252544 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.44672891497612, loss=2.1453113555908203
I0209 04:59:38.832187 139615914645248 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.4260842800140381, loss=2.0199363231658936
I0209 05:00:13.737010 139615906252544 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.36998119950294495, loss=2.13338565826416
I0209 05:00:48.634404 139615914645248 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.4540749192237854, loss=1.9716694355010986
I0209 05:01:23.524064 139615906252544 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3037518560886383, loss=2.123800754547119
I0209 05:01:58.414307 139615914645248 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8437268733978271, loss=2.0251405239105225
I0209 05:02:33.296917 139615906252544 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.29020029306411743, loss=2.0875627994537354
I0209 05:03:08.244699 139615914645248 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.49227777123451233, loss=2.010256290435791
I0209 05:03:43.137642 139615906252544 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9220041036605835, loss=2.0317938327789307
I0209 05:04:18.025660 139615914645248 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5498446226119995, loss=2.0422723293304443
I0209 05:04:52.906299 139615906252544 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5262267589569092, loss=2.016479969024658
I0209 05:05:27.794886 139615914645248 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.47463297843933105, loss=2.0785276889801025
I0209 05:05:46.712439 139785736898368 spec.py:321] Evaluating on the training split.
I0209 05:05:49.717205 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:08:39.638111 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 05:08:42.341009 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:11:17.387675 139785736898368 spec.py:349] Evaluating on the test split.
I0209 05:11:20.103260 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:13:53.770523 139785736898368 submission_runner.py:408] Time since start: 10269.62s, 	Step: 16856, 	{'train/accuracy': 0.5945723056793213, 'train/loss': 2.086899518966675, 'train/bleu': 28.754664656766057, 'validation/accuracy': 0.6146854758262634, 'validation/loss': 1.9335582256317139, 'validation/bleu': 24.68478496541946, 'validation/num_examples': 3000, 'test/accuracy': 0.6249956488609314, 'test/loss': 1.8804055452346802, 'test/bleu': 24.498520031012532, 'test/num_examples': 3003, 'score': 5907.831691741943, 'total_duration': 10269.621250391006, 'accumulated_submission_time': 5907.831691741943, 'accumulated_eval_time': 4361.068868637085, 'accumulated_logging_time': 0.18170881271362305}
I0209 05:13:53.787357 139615906252544 logging_writer.py:48] [16856] accumulated_eval_time=4361.068869, accumulated_logging_time=0.181709, accumulated_submission_time=5907.831692, global_step=16856, preemption_count=0, score=5907.831692, test/accuracy=0.624996, test/bleu=24.498520, test/loss=1.880406, test/num_examples=3003, total_duration=10269.621250, train/accuracy=0.594572, train/bleu=28.754665, train/loss=2.086900, validation/accuracy=0.614685, validation/bleu=24.684785, validation/loss=1.933558, validation/num_examples=3000
I0209 05:14:09.498492 139615914645248 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.30445224046707153, loss=2.1418964862823486
I0209 05:14:44.387468 139615906252544 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.40227627754211426, loss=2.1858644485473633
I0209 05:15:19.258721 139615914645248 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2671964168548584, loss=2.118055820465088
I0209 05:15:54.177262 139615906252544 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.39415299892425537, loss=2.0543742179870605
I0209 05:16:29.060641 139615914645248 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2781883776187897, loss=2.0078859329223633
I0209 05:17:03.982366 139615906252544 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8876436948776245, loss=2.1201117038726807
I0209 05:17:38.921961 139615914645248 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.29516175389289856, loss=2.0300133228302
I0209 05:18:13.958995 139615906252544 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.6502405405044556, loss=2.0847949981689453
I0209 05:18:48.921003 139615914645248 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7641036510467529, loss=2.0544259548187256
I0209 05:19:23.878415 139615906252544 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.36373719573020935, loss=2.1283490657806396
I0209 05:19:58.773889 139615914645248 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.4915328025817871, loss=1.9933087825775146
I0209 05:20:33.652422 139615906252544 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.8163964748382568, loss=2.065856456756592
I0209 05:21:08.521339 139615914645248 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.39508524537086487, loss=1.9885172843933105
I0209 05:21:43.427995 139615906252544 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5312222838401794, loss=2.07228422164917
I0209 05:22:18.320660 139615914645248 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3864639401435852, loss=2.1135494709014893
I0209 05:22:53.186080 139615906252544 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.29599663615226746, loss=2.018204689025879
I0209 05:23:28.084410 139615914645248 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.30128219723701477, loss=2.092923879623413
I0209 05:24:02.986048 139615906252544 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5877526998519897, loss=2.0509955883026123
I0209 05:24:37.935150 139615914645248 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.28495898842811584, loss=2.0606393814086914
I0209 05:25:12.838363 139615906252544 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5594936013221741, loss=1.9859607219696045
I0209 05:25:47.757432 139615914645248 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3337019085884094, loss=2.0732038021087646
I0209 05:26:22.651299 139615906252544 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3141558766365051, loss=1.9661201238632202
I0209 05:26:57.542254 139615914645248 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.4113498330116272, loss=2.082636833190918
I0209 05:27:32.391926 139615906252544 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5532758831977844, loss=2.077475070953369
I0209 05:27:54.079679 139785736898368 spec.py:321] Evaluating on the training split.
I0209 05:27:57.071523 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:31:34.699438 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 05:31:37.426301 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:35:02.482801 139785736898368 spec.py:349] Evaluating on the test split.
I0209 05:35:05.194151 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:37:44.473370 139785736898368 submission_runner.py:408] Time since start: 11700.32s, 	Step: 19264, 	{'train/accuracy': 0.6116220355033875, 'train/loss': 1.968550443649292, 'train/bleu': 28.895200180970697, 'validation/accuracy': 0.6213685870170593, 'validation/loss': 1.8958373069763184, 'validation/bleu': 25.17453094857682, 'validation/num_examples': 3000, 'test/accuracy': 0.6264830827713013, 'test/loss': 1.8554015159606934, 'test/bleu': 23.868531253383175, 'test/num_examples': 3003, 'score': 6748.037788152695, 'total_duration': 11700.324100255966, 'accumulated_submission_time': 6748.037788152695, 'accumulated_eval_time': 4951.462516546249, 'accumulated_logging_time': 0.2087688446044922}
I0209 05:37:44.490639 139615914645248 logging_writer.py:48] [19264] accumulated_eval_time=4951.462517, accumulated_logging_time=0.208769, accumulated_submission_time=6748.037788, global_step=19264, preemption_count=0, score=6748.037788, test/accuracy=0.626483, test/bleu=23.868531, test/loss=1.855402, test/num_examples=3003, total_duration=11700.324100, train/accuracy=0.611622, train/bleu=28.895200, train/loss=1.968550, validation/accuracy=0.621369, validation/bleu=25.174531, validation/loss=1.895837, validation/num_examples=3000
I0209 05:37:57.406641 139615906252544 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.4666624963283539, loss=1.9568262100219727
I0209 05:38:32.248382 139615914645248 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.523310124874115, loss=2.0507137775421143
I0209 05:39:07.152817 139615906252544 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8288173079490662, loss=2.0589566230773926
I0209 05:39:42.039854 139615914645248 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3546135723590851, loss=2.0909221172332764
I0209 05:40:16.930387 139615906252544 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.47974705696105957, loss=2.077378273010254
I0209 05:40:51.788628 139615914645248 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.29827216267585754, loss=2.1088428497314453
I0209 05:41:26.674914 139615906252544 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.30317333340644836, loss=2.0369930267333984
I0209 05:42:01.558842 139615914645248 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.59085613489151, loss=2.057990550994873
I0209 05:42:36.477908 139615906252544 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.32120275497436523, loss=2.025984048843384
I0209 05:43:11.375383 139615914645248 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.34024113416671753, loss=2.0561885833740234
I0209 05:43:46.273486 139615906252544 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3962016701698303, loss=2.0209577083587646
I0209 05:44:21.144322 139615914645248 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.2896703779697418, loss=2.16452693939209
I0209 05:44:56.003704 139615906252544 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5372958779335022, loss=2.110077142715454
I0209 05:45:30.855331 139615914645248 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3420635759830475, loss=2.1149466037750244
I0209 05:46:05.739997 139615906252544 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.39085254073143005, loss=2.0590550899505615
I0209 05:46:40.686392 139615914645248 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.4473941922187805, loss=2.0197789669036865
I0209 05:47:15.580842 139615906252544 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.37025120854377747, loss=1.9920902252197266
I0209 05:47:50.463127 139615914645248 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6763721108436584, loss=2.141857385635376
I0209 05:48:25.346575 139615906252544 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.2645813524723053, loss=2.058701992034912
I0209 05:49:00.203603 139615914645248 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.31531792879104614, loss=2.07881498336792
I0209 05:49:35.106859 139615906252544 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.5380751490592957, loss=2.0147862434387207
I0209 05:50:10.017789 139615914645248 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.29259976744651794, loss=2.1398675441741943
I0209 05:50:44.888686 139615906252544 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.36395174264907837, loss=1.9881469011306763
I0209 05:51:19.790103 139615914645248 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8112549781799316, loss=2.047816276550293
I0209 05:51:44.670086 139785736898368 spec.py:321] Evaluating on the training split.
I0209 05:51:47.673263 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:54:46.646369 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 05:54:49.346122 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 05:57:19.108143 139785736898368 spec.py:349] Evaluating on the test split.
I0209 05:57:21.832642 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:00:00.661877 139785736898368 submission_runner.py:408] Time since start: 13036.51s, 	Step: 21673, 	{'train/accuracy': 0.6006941199302673, 'train/loss': 2.0575037002563477, 'train/bleu': 28.424609301688978, 'validation/accuracy': 0.6233896613121033, 'validation/loss': 1.8967119455337524, 'validation/bleu': 25.493374681381614, 'validation/num_examples': 3000, 'test/accuracy': 0.629283607006073, 'test/loss': 1.8413125276565552, 'test/bleu': 24.442632777732154, 'test/num_examples': 3003, 'score': 7588.130204200745, 'total_duration': 13036.512573957443, 'accumulated_submission_time': 7588.130204200745, 'accumulated_eval_time': 5447.454230070114, 'accumulated_logging_time': 0.23738527297973633}
I0209 06:00:00.683425 139615906252544 logging_writer.py:48] [21673] accumulated_eval_time=5447.454230, accumulated_logging_time=0.237385, accumulated_submission_time=7588.130204, global_step=21673, preemption_count=0, score=7588.130204, test/accuracy=0.629284, test/bleu=24.442633, test/loss=1.841313, test/num_examples=3003, total_duration=13036.512574, train/accuracy=0.600694, train/bleu=28.424609, train/loss=2.057504, validation/accuracy=0.623390, validation/bleu=25.493375, validation/loss=1.896712, validation/num_examples=3000
I0209 06:00:10.428078 139615914645248 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7019340395927429, loss=2.0217843055725098
I0209 06:00:45.282558 139615906252544 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.49580371379852295, loss=2.134683609008789
I0209 06:01:20.131300 139615914645248 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.42049407958984375, loss=2.02740216255188
I0209 06:01:55.003525 139615906252544 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5915884971618652, loss=2.050767660140991
I0209 06:02:29.872215 139615914645248 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7344187498092651, loss=2.10245680809021
I0209 06:03:04.735392 139615906252544 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9189737439155579, loss=2.051945686340332
I0209 06:03:39.641392 139615914645248 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.4419388175010681, loss=2.0273704528808594
I0209 06:04:14.501565 139615906252544 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.383510947227478, loss=2.1370785236358643
I0209 06:04:49.386875 139615914645248 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.40192216634750366, loss=2.0899465084075928
I0209 06:05:24.288503 139615906252544 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.45791175961494446, loss=2.0490078926086426
I0209 06:05:59.169833 139615914645248 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.4541727602481842, loss=2.113225221633911
I0209 06:06:34.048726 139615906252544 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.4892047941684723, loss=1.9066489934921265
I0209 06:07:08.949655 139615914645248 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3189846873283386, loss=2.0897834300994873
I0209 06:07:43.863619 139615906252544 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.31177058815956116, loss=2.0849380493164062
I0209 06:08:18.748093 139615914645248 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.37798967957496643, loss=2.0710794925689697
I0209 06:08:53.671866 139615906252544 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6355887055397034, loss=2.077099561691284
I0209 06:09:28.548057 139615914645248 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0608105659484863, loss=2.0072696208953857
I0209 06:10:03.445105 139615906252544 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3648723065853119, loss=2.0668838024139404
I0209 06:10:38.339440 139615914645248 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2616882622241974, loss=2.0407629013061523
I0209 06:11:13.236161 139615906252544 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.401989221572876, loss=2.129359245300293
I0209 06:11:48.113479 139615914645248 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3905332684516907, loss=2.035590887069702
I0209 06:12:23.058903 139615906252544 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5546623468399048, loss=2.1134848594665527
I0209 06:12:57.952643 139615914645248 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3328050673007965, loss=2.049360990524292
I0209 06:13:32.847616 139615906252544 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.29418423771858215, loss=2.0290708541870117
I0209 06:14:00.819185 139785736898368 spec.py:321] Evaluating on the training split.
I0209 06:14:03.819079 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:17:05.969183 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 06:17:08.669110 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:19:55.220551 139785736898368 spec.py:349] Evaluating on the test split.
I0209 06:19:57.917655 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:22:34.554090 139785736898368 submission_runner.py:408] Time since start: 14390.40s, 	Step: 24082, 	{'train/accuracy': 0.6017192602157593, 'train/loss': 2.061342716217041, 'train/bleu': 27.65494665264454, 'validation/accuracy': 0.6205998659133911, 'validation/loss': 1.8996304273605347, 'validation/bleu': 24.93630152938552, 'validation/num_examples': 3000, 'test/accuracy': 0.6273081302642822, 'test/loss': 1.8463226556777954, 'test/bleu': 23.7675562804176, 'test/num_examples': 3003, 'score': 8428.177636146545, 'total_duration': 14390.404823541641, 'accumulated_submission_time': 8428.177636146545, 'accumulated_eval_time': 5961.189096927643, 'accumulated_logging_time': 0.27000856399536133}
I0209 06:22:34.571969 139615914645248 logging_writer.py:48] [24082] accumulated_eval_time=5961.189097, accumulated_logging_time=0.270009, accumulated_submission_time=8428.177636, global_step=24082, preemption_count=0, score=8428.177636, test/accuracy=0.627308, test/bleu=23.767556, test/loss=1.846323, test/num_examples=3003, total_duration=14390.404824, train/accuracy=0.601719, train/bleu=27.654947, train/loss=2.061343, validation/accuracy=0.620600, validation/bleu=24.936302, validation/loss=1.899630, validation/num_examples=3000
I0209 06:22:41.219709 139615906252544 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.28810495138168335, loss=2.13041090965271
I0209 06:23:16.104887 139615914645248 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.41536566615104675, loss=2.0607097148895264
I0209 06:23:51.005218 139615906252544 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3352811932563782, loss=2.204350709915161
I0209 06:24:25.878761 139615914645248 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8424760103225708, loss=2.1174638271331787
I0209 06:25:00.741348 139615906252544 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5774828195571899, loss=2.0477664470672607
I0209 06:25:35.591252 139615914645248 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.2813992202281952, loss=2.0164105892181396
I0209 06:26:10.472137 139615906252544 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.41777604818344116, loss=1.9625436067581177
I0209 06:26:45.353619 139615914645248 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3629363477230072, loss=2.0692050457000732
I0209 06:27:20.250366 139615906252544 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6457748413085938, loss=2.085848808288574
I0209 06:27:55.148137 139615914645248 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7833679914474487, loss=2.0732526779174805
I0209 06:28:30.033660 139615906252544 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.4552803933620453, loss=2.0600171089172363
I0209 06:29:04.934364 139615914645248 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.49151039123535156, loss=2.0481388568878174
I0209 06:29:39.831569 139615906252544 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5670594573020935, loss=2.1048951148986816
I0209 06:30:14.703982 139615914645248 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3541080355644226, loss=2.0371479988098145
I0209 06:30:49.589313 139615906252544 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3263895511627197, loss=1.939823031425476
I0209 06:31:24.464224 139615914645248 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4288313090801239, loss=2.0613515377044678
I0209 06:31:59.374539 139615906252544 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4293743371963501, loss=2.039485454559326
I0209 06:32:34.250656 139615914645248 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6044269800186157, loss=1.9467846155166626
I0209 06:33:09.161128 139615906252544 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.44810792803764343, loss=2.0925662517547607
I0209 06:33:44.064564 139615914645248 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3811465799808502, loss=1.898659348487854
I0209 06:34:19.004777 139615906252544 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3004244565963745, loss=2.1003780364990234
I0209 06:34:53.897192 139615914645248 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5531487464904785, loss=2.106496810913086
I0209 06:35:28.832939 139615906252544 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.36457815766334534, loss=2.0378434658050537
I0209 06:36:03.705716 139615914645248 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.4597650468349457, loss=2.185971260070801
I0209 06:36:34.857524 139785736898368 spec.py:321] Evaluating on the training split.
I0209 06:36:37.866194 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:39:59.466420 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 06:40:02.182601 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:42:59.988708 139785736898368 spec.py:349] Evaluating on the test split.
I0209 06:43:02.732362 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 06:46:17.664219 139785736898368 submission_runner.py:408] Time since start: 15813.51s, 	Step: 26491, 	{'train/accuracy': 0.6068245768547058, 'train/loss': 2.0195443630218506, 'train/bleu': 28.520838515349766, 'validation/accuracy': 0.6253611445426941, 'validation/loss': 1.8684108257293701, 'validation/bleu': 25.046618347673732, 'validation/num_examples': 3000, 'test/accuracy': 0.6311893463134766, 'test/loss': 1.8257577419281006, 'test/bleu': 24.47399505397761, 'test/num_examples': 3003, 'score': 9268.377697706223, 'total_duration': 15813.514918327332, 'accumulated_submission_time': 9268.377697706223, 'accumulated_eval_time': 6543.995737552643, 'accumulated_logging_time': 0.2978191375732422}
I0209 06:46:17.685793 139615906252544 logging_writer.py:48] [26491] accumulated_eval_time=6543.995738, accumulated_logging_time=0.297819, accumulated_submission_time=9268.377698, global_step=26491, preemption_count=0, score=9268.377698, test/accuracy=0.631189, test/bleu=24.473995, test/loss=1.825758, test/num_examples=3003, total_duration=15813.514918, train/accuracy=0.606825, train/bleu=28.520839, train/loss=2.019544, validation/accuracy=0.625361, validation/bleu=25.046618, validation/loss=1.868411, validation/num_examples=3000
I0209 06:46:21.194598 139615914645248 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.44888317584991455, loss=2.0955636501312256
I0209 06:46:56.090330 139615906252544 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5221743583679199, loss=2.061591625213623
I0209 06:47:31.012226 139615914645248 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.5144491791725159, loss=2.0364158153533936
I0209 06:48:05.966748 139615906252544 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7969588041305542, loss=2.111281633377075
I0209 06:48:40.848859 139615914645248 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5174716711044312, loss=2.054410934448242
I0209 06:49:15.737536 139615906252544 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.9241154789924622, loss=2.0024750232696533
I0209 06:49:50.682649 139615914645248 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.40551769733428955, loss=2.077183961868286
I0209 06:50:25.585570 139615906252544 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.6563165783882141, loss=2.0256898403167725
I0209 06:51:00.478601 139615914645248 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6308397054672241, loss=2.044973850250244
I0209 06:51:35.351380 139615906252544 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.34400492906570435, loss=1.9638631343841553
I0209 06:52:10.234199 139615914645248 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5104184150695801, loss=1.9895353317260742
I0209 06:52:45.099099 139615906252544 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.3502386808395386, loss=2.081352472305298
I0209 06:53:20.002429 139615914645248 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.26867836713790894, loss=2.1055147647857666
I0209 06:53:54.877054 139615906252544 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5587656497955322, loss=2.083341360092163
I0209 06:54:29.741323 139615914645248 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5389687418937683, loss=2.0259881019592285
I0209 06:55:04.638877 139615906252544 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.3372659385204315, loss=2.0229547023773193
I0209 06:55:39.537631 139615914645248 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.550626814365387, loss=1.9982609748840332
I0209 06:56:14.440558 139615906252544 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.36800476908683777, loss=2.049633502960205
I0209 06:56:49.359225 139615914645248 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.3403831720352173, loss=1.9950262308120728
I0209 06:57:24.297735 139615906252544 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.34651443362236023, loss=2.0234756469726562
I0209 06:57:59.226317 139615914645248 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.32207930088043213, loss=2.0663843154907227
I0209 06:58:34.102582 139615906252544 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.28214332461357117, loss=2.0706419944763184
I0209 06:59:09.029016 139615914645248 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.30755650997161865, loss=2.023494243621826
I0209 06:59:43.970450 139615906252544 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8078568577766418, loss=2.108402729034424
I0209 07:00:17.882849 139785736898368 spec.py:321] Evaluating on the training split.
I0209 07:00:20.883052 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:03:20.126322 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 07:03:22.838297 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:06:31.098712 139785736898368 spec.py:349] Evaluating on the test split.
I0209 07:06:33.807604 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:09:33.294996 139785736898368 submission_runner.py:408] Time since start: 17209.15s, 	Step: 28899, 	{'train/accuracy': 0.6058840155601501, 'train/loss': 2.0256903171539307, 'train/bleu': 28.15505565929569, 'validation/accuracy': 0.6242700219154358, 'validation/loss': 1.872414231300354, 'validation/bleu': 25.25882325875779, 'validation/num_examples': 3000, 'test/accuracy': 0.6313404440879822, 'test/loss': 1.8199553489685059, 'test/bleu': 24.222597592597598, 'test/num_examples': 3003, 'score': 10108.486330747604, 'total_duration': 17209.145723819733, 'accumulated_submission_time': 10108.486330747604, 'accumulated_eval_time': 7099.407840251923, 'accumulated_logging_time': 0.3309357166290283}
I0209 07:09:33.314475 139615914645248 logging_writer.py:48] [28899] accumulated_eval_time=7099.407840, accumulated_logging_time=0.330936, accumulated_submission_time=10108.486331, global_step=28899, preemption_count=0, score=10108.486331, test/accuracy=0.631340, test/bleu=24.222598, test/loss=1.819955, test/num_examples=3003, total_duration=17209.145724, train/accuracy=0.605884, train/bleu=28.155056, train/loss=2.025690, validation/accuracy=0.624270, validation/bleu=25.258823, validation/loss=1.872414, validation/num_examples=3000
I0209 07:09:34.023111 139615906252544 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.45448896288871765, loss=2.1192474365234375
I0209 07:10:08.908809 139615914645248 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2884747385978699, loss=2.0049731731414795
I0209 07:10:43.771422 139615906252544 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.3151288330554962, loss=2.0712711811065674
I0209 07:11:18.667027 139615914645248 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6400789618492126, loss=2.070601224899292
I0209 07:11:53.570751 139615906252544 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.427678644657135, loss=2.1211931705474854
I0209 07:12:28.460070 139615914645248 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.4799305200576782, loss=2.0589969158172607
I0209 07:13:03.354663 139615906252544 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3094431459903717, loss=2.0737085342407227
I0209 07:13:38.230065 139615914645248 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.46490758657455444, loss=2.013509750366211
I0209 07:14:13.121626 139615906252544 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.37287893891334534, loss=2.0303220748901367
I0209 07:14:48.064560 139615914645248 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.2647968530654907, loss=2.005491018295288
I0209 07:15:22.927782 139615906252544 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.31986603140830994, loss=2.0488853454589844
I0209 07:15:57.819985 139615914645248 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.5425778031349182, loss=2.0581438541412354
I0209 07:16:32.734035 139615906252544 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.2899872362613678, loss=2.075122833251953
I0209 07:17:07.619860 139615914645248 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.40894266963005066, loss=2.040154218673706
I0209 07:17:42.513768 139615906252544 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3190997540950775, loss=2.0443451404571533
I0209 07:18:17.399279 139615914645248 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.3261343836784363, loss=1.9754571914672852
I0209 07:18:52.293205 139615906252544 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.3161139488220215, loss=2.067366600036621
I0209 07:19:27.175962 139615914645248 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.2839024066925049, loss=1.931134581565857
I0209 07:20:02.105635 139615906252544 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.30536216497421265, loss=2.036040782928467
I0209 07:20:36.963856 139615914645248 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.4507979452610016, loss=1.9670579433441162
I0209 07:21:11.849422 139615906252544 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3662199079990387, loss=2.018097162246704
I0209 07:21:46.731248 139615914645248 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.45302146673202515, loss=2.0434632301330566
I0209 07:22:21.640757 139615906252544 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.4063173234462738, loss=2.1104860305786133
I0209 07:22:56.553804 139615914645248 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.34150487184524536, loss=1.9794397354125977
I0209 07:23:31.449464 139615906252544 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3909895718097687, loss=2.0721216201782227
I0209 07:23:33.624105 139785736898368 spec.py:321] Evaluating on the training split.
I0209 07:23:36.622311 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:26:11.644073 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 07:26:14.329120 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:28:40.518133 139785736898368 spec.py:349] Evaluating on the test split.
I0209 07:28:43.226288 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:31:08.515158 139785736898368 submission_runner.py:408] Time since start: 18504.37s, 	Step: 31308, 	{'train/accuracy': 0.6748192310333252, 'train/loss': 1.52360999584198, 'train/bleu': 32.83433125238866, 'validation/accuracy': 0.6257950663566589, 'validation/loss': 1.8585841655731201, 'validation/bleu': 25.46695972247443, 'validation/num_examples': 3000, 'test/accuracy': 0.6303178071975708, 'test/loss': 1.821932077407837, 'test/bleu': 24.27951532927107, 'test/num_examples': 3003, 'score': 10948.709831953049, 'total_duration': 18504.365869522095, 'accumulated_submission_time': 10948.709831953049, 'accumulated_eval_time': 7554.298825263977, 'accumulated_logging_time': 0.36050963401794434}
I0209 07:31:08.534895 139615914645248 logging_writer.py:48] [31308] accumulated_eval_time=7554.298825, accumulated_logging_time=0.360510, accumulated_submission_time=10948.709832, global_step=31308, preemption_count=0, score=10948.709832, test/accuracy=0.630318, test/bleu=24.279515, test/loss=1.821932, test/num_examples=3003, total_duration=18504.365870, train/accuracy=0.674819, train/bleu=32.834331, train/loss=1.523610, validation/accuracy=0.625795, validation/bleu=25.466960, validation/loss=1.858584, validation/num_examples=3000
I0209 07:31:41.015961 139615906252544 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.43534159660339355, loss=2.042231798171997
I0209 07:32:15.948100 139615914645248 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3667661249637604, loss=2.0521202087402344
I0209 07:32:50.836020 139615906252544 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.5514238476753235, loss=2.0228426456451416
I0209 07:33:25.747871 139615914645248 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4608765244483948, loss=2.0782907009124756
I0209 07:34:00.624234 139615906252544 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.35112765431404114, loss=1.9517945051193237
I0209 07:34:35.495645 139615914645248 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.4506956934928894, loss=2.064349412918091
I0209 07:35:10.404660 139615906252544 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.35024145245552063, loss=2.0181996822357178
I0209 07:35:45.300785 139615914645248 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2855170667171478, loss=2.067417860031128
I0209 07:36:20.159512 139615906252544 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.30298078060150146, loss=2.036661148071289
I0209 07:36:55.022189 139615914645248 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.4246642589569092, loss=2.076221227645874
I0209 07:37:29.890552 139615906252544 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3149183392524719, loss=2.0976476669311523
I0209 07:38:04.774344 139615914645248 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.4508136212825775, loss=2.019880533218384
I0209 07:38:39.663004 139615906252544 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.27617812156677246, loss=2.0272107124328613
I0209 07:39:14.534703 139615914645248 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3045046031475067, loss=2.023521661758423
I0209 07:39:49.437807 139615906252544 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.27389705181121826, loss=2.072960376739502
I0209 07:40:24.356649 139615914645248 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.326294481754303, loss=2.023416519165039
I0209 07:40:59.277371 139615906252544 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.36361411213874817, loss=1.9996521472930908
I0209 07:41:34.302971 139615914645248 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3003295063972473, loss=2.0395243167877197
I0209 07:42:09.218363 139615906252544 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.32769525051116943, loss=2.07063627243042
I0209 07:42:44.111479 139615914645248 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3187481462955475, loss=2.0013673305511475
I0209 07:43:19.012298 139615906252544 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4014763832092285, loss=2.068129062652588
I0209 07:43:53.887994 139615914645248 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.34041494131088257, loss=1.9921232461929321
I0209 07:44:28.790224 139615906252544 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.38314151763916016, loss=2.083523750305176
I0209 07:45:03.736662 139615914645248 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.5536236763000488, loss=2.08975887298584
I0209 07:45:08.704857 139785736898368 spec.py:321] Evaluating on the training split.
I0209 07:45:11.703887 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:48:35.059155 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 07:48:37.756942 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:51:13.653370 139785736898368 spec.py:349] Evaluating on the test split.
I0209 07:51:16.372615 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 07:53:57.846881 139785736898368 submission_runner.py:408] Time since start: 19873.70s, 	Step: 33716, 	{'train/accuracy': 0.606469988822937, 'train/loss': 2.0195164680480957, 'train/bleu': 28.84137114817753, 'validation/accuracy': 0.627865731716156, 'validation/loss': 1.842737078666687, 'validation/bleu': 25.98349001394717, 'validation/num_examples': 3000, 'test/accuracy': 0.6331764459609985, 'test/loss': 1.7970689535140991, 'test/bleu': 24.776988322226952, 'test/num_examples': 3003, 'score': 11788.792954921722, 'total_duration': 19873.697608232498, 'accumulated_submission_time': 11788.792954921722, 'accumulated_eval_time': 8083.440821886063, 'accumulated_logging_time': 0.39031457901000977}
I0209 07:53:57.866439 139615906252544 logging_writer.py:48] [33716] accumulated_eval_time=8083.440822, accumulated_logging_time=0.390315, accumulated_submission_time=11788.792955, global_step=33716, preemption_count=0, score=11788.792955, test/accuracy=0.633176, test/bleu=24.776988, test/loss=1.797069, test/num_examples=3003, total_duration=19873.697608, train/accuracy=0.606470, train/bleu=28.841371, train/loss=2.019516, validation/accuracy=0.627866, validation/bleu=25.983490, validation/loss=1.842737, validation/num_examples=3000
I0209 07:54:27.503345 139615914645248 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.2945097088813782, loss=1.9748361110687256
I0209 07:55:02.442651 139615906252544 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5094960331916809, loss=1.9365953207015991
I0209 07:55:37.339497 139615914645248 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.31670770049095154, loss=1.9987059831619263
I0209 07:56:12.219189 139615906252544 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.3045841455459595, loss=2.011000633239746
I0209 07:56:47.133282 139615914645248 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3461773693561554, loss=2.031564712524414
I0209 07:57:22.035722 139615906252544 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.4647764563560486, loss=1.9726155996322632
I0209 07:57:56.916160 139615914645248 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.2830466330051422, loss=1.9895820617675781
I0209 07:58:31.790078 139615906252544 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.48806285858154297, loss=1.948722004890442
I0209 07:59:06.684027 139615914645248 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3218715786933899, loss=2.0045788288116455
I0209 07:59:41.563099 139615906252544 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3986038565635681, loss=2.090815305709839
I0209 08:00:16.443832 139615914645248 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.31929296255111694, loss=1.965877890586853
I0209 08:00:51.334999 139615906252544 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3265814781188965, loss=2.0386106967926025
I0209 08:01:26.199628 139615914645248 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5185928344726562, loss=1.9889096021652222
I0209 08:02:01.081653 139615906252544 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.3550666570663452, loss=1.9998294115066528
I0209 08:02:35.991045 139615914645248 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.4513900876045227, loss=1.9404349327087402
I0209 08:03:10.886596 139615906252544 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.5214799046516418, loss=2.0098423957824707
I0209 08:03:45.777216 139615914645248 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.2537368834018707, loss=1.990807294845581
I0209 08:04:20.647471 139615906252544 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3067735731601715, loss=2.0275323390960693
I0209 08:04:55.511282 139615914645248 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3339768052101135, loss=2.002974271774292
I0209 08:05:30.375445 139615906252544 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.37117791175842285, loss=2.051896572113037
I0209 08:06:05.265692 139615914645248 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.40737149119377136, loss=2.0282092094421387
I0209 08:06:40.136129 139615906252544 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.37244680523872375, loss=2.0577986240386963
I0209 08:07:15.066912 139615914645248 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5199048519134521, loss=1.9571890830993652
I0209 08:07:49.983981 139615906252544 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3865358829498291, loss=1.9256412982940674
I0209 08:07:58.105669 139785736898368 spec.py:321] Evaluating on the training split.
I0209 08:08:01.120207 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:10:40.219406 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 08:10:42.914699 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:13:21.960961 139785736898368 spec.py:349] Evaluating on the test split.
I0209 08:13:24.691991 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:15:59.359862 139785736898368 submission_runner.py:408] Time since start: 21195.21s, 	Step: 36125, 	{'train/accuracy': 0.6092419028282166, 'train/loss': 2.0129339694976807, 'train/bleu': 28.618722834476483, 'validation/accuracy': 0.6275929808616638, 'validation/loss': 1.8477492332458496, 'validation/bleu': 25.35805133784494, 'validation/num_examples': 3000, 'test/accuracy': 0.6359886527061462, 'test/loss': 1.7861478328704834, 'test/bleu': 24.281479120561823, 'test/num_examples': 3003, 'score': 12628.945209980011, 'total_duration': 21195.21056318283, 'accumulated_submission_time': 12628.945209980011, 'accumulated_eval_time': 8564.694946527481, 'accumulated_logging_time': 0.42111873626708984}
I0209 08:15:59.384832 139615914645248 logging_writer.py:48] [36125] accumulated_eval_time=8564.694947, accumulated_logging_time=0.421119, accumulated_submission_time=12628.945210, global_step=36125, preemption_count=0, score=12628.945210, test/accuracy=0.635989, test/bleu=24.281479, test/loss=1.786148, test/num_examples=3003, total_duration=21195.210563, train/accuracy=0.609242, train/bleu=28.618723, train/loss=2.012934, validation/accuracy=0.627593, validation/bleu=25.358051, validation/loss=1.847749, validation/num_examples=3000
I0209 08:16:25.888839 139615906252544 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.25851985812187195, loss=2.027620553970337
I0209 08:17:00.749546 139615914645248 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.3434310853481293, loss=1.9240577220916748
I0209 08:17:35.616880 139615906252544 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.31932809948921204, loss=2.077289581298828
I0209 08:18:10.489444 139615914645248 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4254281520843506, loss=2.01981782913208
I0209 08:18:45.370315 139615906252544 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.3299604058265686, loss=1.9574742317199707
I0209 08:19:20.268742 139615914645248 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.3476264476776123, loss=1.9171348810195923
I0209 08:19:55.148694 139615906252544 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.39378899335861206, loss=1.9577217102050781
I0209 08:20:30.047953 139615914645248 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.4465164840221405, loss=2.0861854553222656
I0209 08:21:04.914388 139615906252544 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.3545549213886261, loss=2.0257909297943115
I0209 08:21:39.768638 139615914645248 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.27507537603378296, loss=1.948988914489746
I0209 08:22:14.669290 139615906252544 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.43813368678092957, loss=2.0145912170410156
I0209 08:22:49.552212 139615914645248 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.3572157621383667, loss=2.073699474334717
I0209 08:23:24.416795 139615906252544 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.5610544085502625, loss=2.1355767250061035
I0209 08:23:59.288907 139615914645248 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.29208093881607056, loss=2.0555336475372314
I0209 08:24:34.171009 139615906252544 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.3161664307117462, loss=2.085312843322754
I0209 08:25:09.058388 139615914645248 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5285645723342896, loss=2.091505289077759
I0209 08:25:43.930668 139615906252544 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.25521427392959595, loss=1.9182751178741455
I0209 08:26:18.827705 139615914645248 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.3111935555934906, loss=2.0309548377990723
I0209 08:26:53.710491 139615906252544 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.48171326518058777, loss=2.013103485107422
I0209 08:27:28.595829 139615914645248 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.37627458572387695, loss=2.03564715385437
I0209 08:28:03.464624 139615906252544 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.47259521484375, loss=1.9912378787994385
I0209 08:28:38.458832 139615914645248 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.2923649251461029, loss=1.9877053499221802
I0209 08:29:13.344443 139615906252544 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.2570975124835968, loss=2.0143463611602783
I0209 08:29:48.212130 139615914645248 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3868660628795624, loss=2.0396604537963867
I0209 08:29:59.456353 139785736898368 spec.py:321] Evaluating on the training split.
I0209 08:30:02.463799 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:33:25.287628 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 08:33:27.985711 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:36:04.269851 139785736898368 spec.py:349] Evaluating on the test split.
I0209 08:36:06.974677 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:39:17.131468 139785736898368 submission_runner.py:408] Time since start: 22592.98s, 	Step: 38534, 	{'train/accuracy': 0.6119536757469177, 'train/loss': 1.9667437076568604, 'train/bleu': 28.499661182879315, 'validation/accuracy': 0.6280888915061951, 'validation/loss': 1.836946964263916, 'validation/bleu': 26.057433104129547, 'validation/num_examples': 3000, 'test/accuracy': 0.6339085698127747, 'test/loss': 1.7903285026550293, 'test/bleu': 24.38247752713862, 'test/num_examples': 3003, 'score': 13468.92739701271, 'total_duration': 22592.982152462006, 'accumulated_submission_time': 13468.92739701271, 'accumulated_eval_time': 9122.370000362396, 'accumulated_logging_time': 0.45857930183410645}
I0209 08:39:17.158361 139615906252544 logging_writer.py:48] [38534] accumulated_eval_time=9122.370000, accumulated_logging_time=0.458579, accumulated_submission_time=13468.927397, global_step=38534, preemption_count=0, score=13468.927397, test/accuracy=0.633909, test/bleu=24.382478, test/loss=1.790329, test/num_examples=3003, total_duration=22592.982152, train/accuracy=0.611954, train/bleu=28.499661, train/loss=1.966744, validation/accuracy=0.628089, validation/bleu=26.057433, validation/loss=1.836947, validation/num_examples=3000
I0209 08:39:40.565893 139615914645248 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2631038725376129, loss=1.9836868047714233
I0209 08:40:15.493609 139615906252544 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2690945565700531, loss=1.9930177927017212
I0209 08:40:50.364812 139615914645248 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3979305326938629, loss=1.9843732118606567
I0209 08:41:25.223335 139615906252544 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.5273246765136719, loss=2.0112884044647217
I0209 08:42:00.112576 139615914645248 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.29942628741264343, loss=2.0370941162109375
I0209 08:42:34.989373 139615906252544 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.2895737886428833, loss=2.023782253265381
I0209 08:43:09.859742 139615914645248 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.2445286363363266, loss=1.8898463249206543
I0209 08:43:44.759160 139615906252544 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7727439999580383, loss=2.0513904094696045
I0209 08:44:19.656604 139615914645248 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4569762647151947, loss=1.93803870677948
I0209 08:44:54.540432 139615906252544 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.35397717356681824, loss=2.0378003120422363
I0209 08:45:29.416108 139615914645248 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.34490469098091125, loss=1.9129564762115479
I0209 08:46:04.312915 139615906252544 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.38708844780921936, loss=2.000572443008423
I0209 08:46:39.254337 139615914645248 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3362185060977936, loss=1.9824668169021606
I0209 08:47:14.152338 139615906252544 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.34021395444869995, loss=1.9537283182144165
I0209 08:47:49.015599 139615914645248 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.40513959527015686, loss=2.0063064098358154
I0209 08:48:23.908639 139615906252544 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.5505053997039795, loss=1.9561761617660522
I0209 08:48:58.770237 139615914645248 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.32689639925956726, loss=2.04363751411438
I0209 08:49:33.692256 139615906252544 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2580002248287201, loss=1.9602065086364746
I0209 08:50:08.615061 139615914645248 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.4133746027946472, loss=2.0261149406433105
I0209 08:50:43.514712 139615906252544 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.40931808948516846, loss=2.0234930515289307
I0209 08:51:18.415885 139615914645248 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.41621890664100647, loss=1.8768117427825928
I0209 08:51:53.305511 139615906252544 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6378061771392822, loss=2.0037004947662354
I0209 08:52:28.183012 139615914645248 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3214346766471863, loss=2.0004708766937256
I0209 08:53:03.092835 139615906252544 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.42243722081184387, loss=2.0803685188293457
I0209 08:53:17.462900 139785736898368 spec.py:321] Evaluating on the training split.
I0209 08:53:20.452569 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:56:11.468914 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 08:56:14.173643 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 08:59:16.769277 139785736898368 spec.py:349] Evaluating on the test split.
I0209 08:59:19.489020 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:02:06.423234 139785736898368 submission_runner.py:408] Time since start: 23962.27s, 	Step: 40943, 	{'train/accuracy': 0.6094376444816589, 'train/loss': 1.9945857524871826, 'train/bleu': 29.30273771814245, 'validation/accuracy': 0.6275309324264526, 'validation/loss': 1.8460413217544556, 'validation/bleu': 25.75543886358571, 'validation/num_examples': 3000, 'test/accuracy': 0.6396607160568237, 'test/loss': 1.7795579433441162, 'test/bleu': 24.5530504118486, 'test/num_examples': 3003, 'score': 14309.143985748291, 'total_duration': 23962.273962020874, 'accumulated_submission_time': 14309.143985748291, 'accumulated_eval_time': 9651.330287218094, 'accumulated_logging_time': 0.49736833572387695}
I0209 09:02:06.444160 139615914645248 logging_writer.py:48] [40943] accumulated_eval_time=9651.330287, accumulated_logging_time=0.497368, accumulated_submission_time=14309.143986, global_step=40943, preemption_count=0, score=14309.143986, test/accuracy=0.639661, test/bleu=24.553050, test/loss=1.779558, test/num_examples=3003, total_duration=23962.273962, train/accuracy=0.609438, train/bleu=29.302738, train/loss=1.994586, validation/accuracy=0.627531, validation/bleu=25.755439, validation/loss=1.846041, validation/num_examples=3000
I0209 09:02:26.662626 139615906252544 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.4345872402191162, loss=2.041430711746216
I0209 09:03:01.575614 139615914645248 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.32287517189979553, loss=2.0337183475494385
I0209 09:03:36.492623 139615906252544 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3040728271007538, loss=1.9401906728744507
I0209 09:04:11.385824 139615914645248 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3159887194633484, loss=2.01640248298645
I0209 09:04:46.280465 139615906252544 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2678852081298828, loss=1.9856175184249878
I0209 09:05:21.230123 139615914645248 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.2870647609233856, loss=1.9702616930007935
I0209 09:05:56.115410 139615906252544 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.37810784578323364, loss=2.014145612716675
I0209 09:06:31.004628 139615914645248 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.4847446382045746, loss=2.013532876968384
I0209 09:07:05.876335 139615906252544 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.29156357049942017, loss=2.0566885471343994
I0209 09:07:40.748708 139615914645248 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.38197532296180725, loss=1.9416474103927612
I0209 09:08:15.618274 139615906252544 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5250527858734131, loss=2.0556161403656006
I0209 09:08:50.510732 139615914645248 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.5340856313705444, loss=2.0056183338165283
I0209 09:09:25.394669 139615906252544 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.33503884077072144, loss=1.9721722602844238
I0209 09:10:00.297835 139615914645248 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3082784116268158, loss=1.9396857023239136
I0209 09:10:35.183531 139615906252544 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.33220526576042175, loss=1.9138233661651611
I0209 09:11:10.066367 139615914645248 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.29600828886032104, loss=1.9699770212173462
I0209 09:11:44.977402 139615906252544 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.31599143147468567, loss=1.9241108894348145
I0209 09:12:19.835969 139615914645248 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.37551793456077576, loss=2.008295774459839
I0209 09:12:54.724605 139615906252544 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.3018577992916107, loss=1.9105807542800903
I0209 09:13:29.585308 139615914645248 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2816740572452545, loss=1.9039721488952637
I0209 09:14:04.448258 139615906252544 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.26719769835472107, loss=1.9976836442947388
I0209 09:14:39.330637 139615914645248 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2759726643562317, loss=2.014549970626831
I0209 09:15:14.200784 139615906252544 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.45857173204421997, loss=2.0123329162597656
I0209 09:15:49.077421 139615914645248 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.341991126537323, loss=1.9244847297668457
I0209 09:16:06.605635 139785736898368 spec.py:321] Evaluating on the training split.
I0209 09:16:09.604830 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:19:27.715982 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 09:19:30.429707 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:22:23.742620 139785736898368 spec.py:349] Evaluating on the test split.
I0209 09:22:26.448369 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:25:11.685165 139785736898368 submission_runner.py:408] Time since start: 25347.54s, 	Step: 43352, 	{'train/accuracy': 0.6117132902145386, 'train/loss': 1.979415774345398, 'train/bleu': 28.894642089106842, 'validation/accuracy': 0.6324781775474548, 'validation/loss': 1.8176498413085938, 'validation/bleu': 26.215222262675244, 'validation/num_examples': 3000, 'test/accuracy': 0.638893723487854, 'test/loss': 1.7688039541244507, 'test/bleu': 25.21839585129908, 'test/num_examples': 3003, 'score': 15149.22114944458, 'total_duration': 25347.535893440247, 'accumulated_submission_time': 15149.22114944458, 'accumulated_eval_time': 10196.409768819809, 'accumulated_logging_time': 0.5284297466278076}
I0209 09:25:11.705528 139615906252544 logging_writer.py:48] [43352] accumulated_eval_time=10196.409769, accumulated_logging_time=0.528430, accumulated_submission_time=15149.221149, global_step=43352, preemption_count=0, score=15149.221149, test/accuracy=0.638894, test/bleu=25.218396, test/loss=1.768804, test/num_examples=3003, total_duration=25347.535893, train/accuracy=0.611713, train/bleu=28.894642, train/loss=1.979416, validation/accuracy=0.632478, validation/bleu=26.215222, validation/loss=1.817650, validation/num_examples=3000
I0209 09:25:28.771415 139615914645248 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7546668648719788, loss=2.0478808879852295
I0209 09:26:03.643661 139615906252544 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4506661891937256, loss=1.9573521614074707
I0209 09:26:38.555647 139615914645248 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.4045790433883667, loss=1.9545098543167114
I0209 09:27:13.429560 139615906252544 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.4493265748023987, loss=2.0353260040283203
I0209 09:27:48.324573 139615914645248 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.41010722517967224, loss=2.03752064704895
I0209 09:28:23.176096 139615906252544 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.36063411831855774, loss=1.9859893321990967
I0209 09:28:58.097106 139615914645248 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.38696786761283875, loss=1.9974311590194702
I0209 09:29:32.969315 139615906252544 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.3561760485172272, loss=1.9231539964675903
I0209 09:30:07.844705 139615914645248 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.30096757411956787, loss=1.9939210414886475
I0209 09:30:42.712249 139615906252544 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.6761569380760193, loss=1.971908688545227
I0209 09:31:17.583461 139615914645248 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.5338256359100342, loss=1.9328510761260986
I0209 09:31:52.471060 139615906252544 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2927418649196625, loss=2.0265109539031982
I0209 09:32:27.357938 139615914645248 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.40656888484954834, loss=1.8962067365646362
I0209 09:33:02.233351 139615906252544 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.520067036151886, loss=1.9787445068359375
I0209 09:33:37.147676 139615914645248 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.4911207854747772, loss=1.9976356029510498
I0209 09:34:12.041425 139615906252544 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.433417946100235, loss=2.0424699783325195
I0209 09:34:46.936893 139615914645248 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3704042136669159, loss=1.9628897905349731
I0209 09:35:21.853894 139615906252544 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.4613437056541443, loss=1.9776941537857056
I0209 09:35:56.809058 139615914645248 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.46992191672325134, loss=1.960954189300537
I0209 09:36:31.661903 139615906252544 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.4860365390777588, loss=2.07157826423645
I0209 09:37:06.543770 139615914645248 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.473986953496933, loss=1.9934773445129395
I0209 09:37:41.443450 139615906252544 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.42625921964645386, loss=1.9966520071029663
I0209 09:38:16.352640 139615914645248 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.4380373954772949, loss=1.9911895990371704
I0209 09:38:51.229710 139615906252544 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.3758574426174164, loss=1.9496212005615234
I0209 09:39:11.897947 139785736898368 spec.py:321] Evaluating on the training split.
I0209 09:39:14.900950 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:43:33.929692 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 09:43:36.633891 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:46:23.761744 139785736898368 spec.py:349] Evaluating on the test split.
I0209 09:46:26.463078 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 09:49:13.354972 139785736898368 submission_runner.py:408] Time since start: 26789.21s, 	Step: 45761, 	{'train/accuracy': 0.6118826866149902, 'train/loss': 1.9624874591827393, 'train/bleu': 28.652251811486824, 'validation/accuracy': 0.630605936050415, 'validation/loss': 1.8224587440490723, 'validation/bleu': 26.06368270027007, 'validation/num_examples': 3000, 'test/accuracy': 0.6413805484771729, 'test/loss': 1.7728618383407593, 'test/bleu': 25.37917327989407, 'test/num_examples': 3003, 'score': 15989.326357841492, 'total_duration': 26789.205688476562, 'accumulated_submission_time': 15989.326357841492, 'accumulated_eval_time': 10797.866746902466, 'accumulated_logging_time': 0.5607788562774658}
I0209 09:49:13.376043 139615914645248 logging_writer.py:48] [45761] accumulated_eval_time=10797.866747, accumulated_logging_time=0.560779, accumulated_submission_time=15989.326358, global_step=45761, preemption_count=0, score=15989.326358, test/accuracy=0.641381, test/bleu=25.379173, test/loss=1.772862, test/num_examples=3003, total_duration=26789.205688, train/accuracy=0.611883, train/bleu=28.652252, train/loss=1.962487, validation/accuracy=0.630606, validation/bleu=26.063683, validation/loss=1.822459, validation/num_examples=3000
I0209 09:49:27.297771 139615906252544 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.3537130653858185, loss=1.914222002029419
I0209 09:50:02.115255 139615914645248 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3592509627342224, loss=1.9760850667953491
I0209 09:50:36.948958 139615906252544 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.5486692786216736, loss=1.9503607749938965
I0209 09:51:11.820479 139615914645248 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3084903955459595, loss=2.0222039222717285
I0209 09:51:46.669107 139615906252544 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.33559852838516235, loss=2.0256218910217285
I0209 09:52:21.517795 139615914645248 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2988027334213257, loss=1.955702543258667
I0209 09:52:56.346790 139615906252544 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4460914433002472, loss=1.918047547340393
I0209 09:53:31.205074 139615914645248 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.36559703946113586, loss=1.9494850635528564
I0209 09:54:05.997721 139615906252544 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3425939083099365, loss=1.8997548818588257
I0209 09:54:40.858554 139615914645248 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.36383330821990967, loss=2.0839157104492188
I0209 09:55:15.679515 139615906252544 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.5106582045555115, loss=1.8944803476333618
I0209 09:55:50.515309 139615914645248 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.24517333507537842, loss=1.930004596710205
I0209 09:56:25.400722 139615906252544 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.45868349075317383, loss=1.9949835538864136
I0209 09:57:00.275865 139615914645248 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.23623749613761902, loss=1.931248664855957
I0209 09:57:35.124436 139615906252544 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.38321059942245483, loss=1.8877745866775513
I0209 09:58:09.959668 139615914645248 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.24992071092128754, loss=1.8945144414901733
I0209 09:58:44.860235 139615906252544 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.31469184160232544, loss=1.9458757638931274
I0209 09:59:19.710601 139615914645248 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.4673963785171509, loss=1.95171058177948
I0209 09:59:54.534363 139615906252544 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.3288538157939911, loss=1.9599190950393677
I0209 10:00:29.394539 139615914645248 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.39167770743370056, loss=2.0217037200927734
I0209 10:01:04.268544 139615906252544 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.41949450969696045, loss=1.898361086845398
I0209 10:01:39.086157 139615914645248 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3441273272037506, loss=1.9590452909469604
I0209 10:02:13.913145 139615906252544 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.298869252204895, loss=1.9047071933746338
I0209 10:02:48.744913 139615914645248 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3801593780517578, loss=2.005687952041626
I0209 10:03:13.559308 139785736898368 spec.py:321] Evaluating on the training split.
I0209 10:03:16.554529 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:06:32.925133 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 10:06:35.633546 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:09:16.325763 139785736898368 spec.py:349] Evaluating on the test split.
I0209 10:09:19.044396 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:12:06.326904 139785736898368 submission_runner.py:408] Time since start: 28162.18s, 	Step: 48173, 	{'train/accuracy': 0.6136108636856079, 'train/loss': 1.9673712253570557, 'train/bleu': 29.17189039490762, 'validation/accuracy': 0.6328873634338379, 'validation/loss': 1.8119760751724243, 'validation/bleu': 25.978765786257235, 'validation/num_examples': 3000, 'test/accuracy': 0.6418104767799377, 'test/loss': 1.7519155740737915, 'test/bleu': 25.306392423383475, 'test/num_examples': 3003, 'score': 16829.423892498016, 'total_duration': 28162.17760157585, 'accumulated_submission_time': 16829.423892498016, 'accumulated_eval_time': 11330.634264469147, 'accumulated_logging_time': 0.5914275646209717}
I0209 10:12:06.352877 139615906252544 logging_writer.py:48] [48173] accumulated_eval_time=11330.634264, accumulated_logging_time=0.591428, accumulated_submission_time=16829.423892, global_step=48173, preemption_count=0, score=16829.423892, test/accuracy=0.641810, test/bleu=25.306392, test/loss=1.751916, test/num_examples=3003, total_duration=28162.177602, train/accuracy=0.613611, train/bleu=29.171890, train/loss=1.967371, validation/accuracy=0.632887, validation/bleu=25.978766, validation/loss=1.811976, validation/num_examples=3000
I0209 10:12:16.135210 139615914645248 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.28914332389831543, loss=1.970155119895935
I0209 10:12:51.011956 139615906252544 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.30623483657836914, loss=1.9905868768692017
I0209 10:13:25.911805 139615914645248 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.4073289632797241, loss=1.9513630867004395
I0209 10:14:00.798018 139615906252544 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.44422686100006104, loss=1.9088643789291382
I0209 10:14:35.719334 139615914645248 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2964642643928528, loss=2.017489194869995
I0209 10:15:10.586972 139615906252544 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.45830729603767395, loss=1.9471678733825684
I0209 10:15:45.468452 139615914645248 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.34008803963661194, loss=1.9059804677963257
I0209 10:16:20.370754 139615906252544 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.36596328020095825, loss=2.0577430725097656
I0209 10:16:55.264840 139615914645248 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.42493757605552673, loss=1.9717270135879517
I0209 10:17:30.153721 139615906252544 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3155776262283325, loss=1.96007239818573
I0209 10:18:05.051935 139615914645248 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.31223931908607483, loss=2.006150245666504
I0209 10:18:39.940417 139615906252544 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.34561794996261597, loss=1.9845302104949951
I0209 10:19:14.820797 139615914645248 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3893270790576935, loss=1.9397794008255005
I0209 10:19:49.683755 139615906252544 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.30508092045783997, loss=2.0403671264648438
I0209 10:20:24.589169 139615914645248 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.2711000442504883, loss=1.9830113649368286
I0209 10:20:59.581072 139615906252544 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.5402113795280457, loss=1.9147037267684937
I0209 10:21:34.467765 139615914645248 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.4590672552585602, loss=1.9456098079681396
I0209 10:22:09.353665 139615906252544 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.35755664110183716, loss=2.042567014694214
I0209 10:22:44.266234 139615914645248 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.34560859203338623, loss=1.9638046026229858
I0209 10:23:19.168987 139615906252544 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.4235565662384033, loss=1.9948266744613647
I0209 10:23:54.064398 139615914645248 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3302988111972809, loss=2.0555105209350586
I0209 10:24:28.939546 139615906252544 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.37695765495300293, loss=1.8748201131820679
I0209 10:25:03.826015 139615914645248 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.4205705523490906, loss=1.9359179735183716
I0209 10:25:38.723366 139615906252544 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.6159601211547852, loss=1.976913571357727
I0209 10:26:06.377660 139785736898368 spec.py:321] Evaluating on the training split.
I0209 10:26:09.363668 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:29:10.338235 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 10:29:13.045604 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:31:50.137023 139785736898368 spec.py:349] Evaluating on the test split.
I0209 10:31:52.857089 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:34:21.560266 139785736898368 submission_runner.py:408] Time since start: 29497.41s, 	Step: 50581, 	{'train/accuracy': 0.6235958337783813, 'train/loss': 1.867612361907959, 'train/bleu': 29.89978511352561, 'validation/accuracy': 0.6353052258491516, 'validation/loss': 1.7884082794189453, 'validation/bleu': 26.18907230106764, 'validation/num_examples': 3000, 'test/accuracy': 0.6434489488601685, 'test/loss': 1.7368030548095703, 'test/bleu': 25.11578170365946, 'test/num_examples': 3003, 'score': 17669.362218618393, 'total_duration': 29497.41099047661, 'accumulated_submission_time': 17669.362218618393, 'accumulated_eval_time': 11825.816824674606, 'accumulated_logging_time': 0.6280605792999268}
I0209 10:34:21.583414 139615914645248 logging_writer.py:48] [50581] accumulated_eval_time=11825.816825, accumulated_logging_time=0.628061, accumulated_submission_time=17669.362219, global_step=50581, preemption_count=0, score=17669.362219, test/accuracy=0.643449, test/bleu=25.115782, test/loss=1.736803, test/num_examples=3003, total_duration=29497.410990, train/accuracy=0.623596, train/bleu=29.899785, train/loss=1.867612, validation/accuracy=0.635305, validation/bleu=26.189072, validation/loss=1.788408, validation/num_examples=3000
I0209 10:34:28.568867 139615906252544 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.7072126269340515, loss=1.9637115001678467
I0209 10:35:03.516806 139615914645248 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.607887327671051, loss=2.0659494400024414
I0209 10:35:38.403011 139615906252544 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3126930892467499, loss=1.8893884420394897
I0209 10:36:13.294354 139615914645248 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.4446316361427307, loss=1.9753261804580688
I0209 10:36:48.226602 139615906252544 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2692805230617523, loss=1.95664644241333
I0209 10:37:23.133690 139615914645248 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.383573442697525, loss=1.9334180355072021
I0209 10:37:58.062948 139615906252544 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3933553695678711, loss=2.0143301486968994
I0209 10:38:32.960208 139615914645248 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.5698734521865845, loss=2.0044312477111816
I0209 10:39:07.831454 139615906252544 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.31673094630241394, loss=2.025545120239258
I0209 10:39:42.731931 139615914645248 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3127721846103668, loss=1.8353424072265625
I0209 10:40:17.633640 139615906252544 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.5758838057518005, loss=1.9964377880096436
I0209 10:40:52.548227 139615914645248 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3073182702064514, loss=1.928966999053955
I0209 10:41:27.421152 139615906252544 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2867680788040161, loss=2.0182762145996094
I0209 10:42:02.291621 139615914645248 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.31517645716667175, loss=1.9992045164108276
I0209 10:42:37.178475 139615906252544 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.4124828279018402, loss=1.9849470853805542
I0209 10:43:12.070732 139615914645248 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.4608592987060547, loss=1.980393409729004
I0209 10:43:46.959942 139615906252544 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.34697261452674866, loss=1.9120090007781982
I0209 10:44:21.856297 139615914645248 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.31168949604034424, loss=2.0165724754333496
I0209 10:44:56.728684 139615906252544 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.4665525555610657, loss=1.9623050689697266
I0209 10:45:31.620587 139615914645248 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.39313048124313354, loss=1.924479365348816
I0209 10:46:06.486335 139615906252544 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3346826434135437, loss=1.920258641242981
I0209 10:46:41.427237 139615914645248 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.25591862201690674, loss=1.9780906438827515
I0209 10:47:16.466508 139615906252544 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.470907986164093, loss=1.9626449346542358
I0209 10:47:51.340545 139615914645248 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.5867512226104736, loss=1.9097912311553955
I0209 10:48:21.740099 139785736898368 spec.py:321] Evaluating on the training split.
I0209 10:48:24.724386 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:52:01.448037 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 10:52:04.151754 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:55:01.833341 139785736898368 spec.py:349] Evaluating on the test split.
I0209 10:55:04.548159 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 10:57:52.275771 139785736898368 submission_runner.py:408] Time since start: 30908.13s, 	Step: 52989, 	{'train/accuracy': 0.6194369792938232, 'train/loss': 1.9303407669067383, 'train/bleu': 29.233347669535043, 'validation/accuracy': 0.6357763409614563, 'validation/loss': 1.7921721935272217, 'validation/bleu': 26.26460212317322, 'validation/num_examples': 3000, 'test/accuracy': 0.646528422832489, 'test/loss': 1.7195571660995483, 'test/bleu': 25.58466393499604, 'test/num_examples': 3003, 'score': 18509.431342840195, 'total_duration': 30908.126499176025, 'accumulated_submission_time': 18509.431342840195, 'accumulated_eval_time': 12396.352447509766, 'accumulated_logging_time': 0.6613750457763672}
I0209 10:57:52.297554 139615906252544 logging_writer.py:48] [52989] accumulated_eval_time=12396.352448, accumulated_logging_time=0.661375, accumulated_submission_time=18509.431343, global_step=52989, preemption_count=0, score=18509.431343, test/accuracy=0.646528, test/bleu=25.584664, test/loss=1.719557, test/num_examples=3003, total_duration=30908.126499, train/accuracy=0.619437, train/bleu=29.233348, train/loss=1.930341, validation/accuracy=0.635776, validation/bleu=26.264602, validation/loss=1.792172, validation/num_examples=3000
I0209 10:57:56.502696 139615914645248 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6089668869972229, loss=1.987820029258728
I0209 10:58:31.421303 139615906252544 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.40814390778541565, loss=1.9648312330245972
I0209 10:59:06.330515 139615914645248 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.43686652183532715, loss=1.9390616416931152
I0209 10:59:41.225236 139615906252544 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.3629441559314728, loss=1.8975430727005005
I0209 11:00:16.113568 139615914645248 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.4232913851737976, loss=1.9087086915969849
I0209 11:00:50.984366 139615906252544 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3591253161430359, loss=1.953145146369934
I0209 11:01:25.843842 139615914645248 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.3168617784976959, loss=1.945228099822998
I0209 11:02:00.708520 139615906252544 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.27736997604370117, loss=1.8553249835968018
I0209 11:02:35.642230 139615914645248 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.25195538997650146, loss=1.9256370067596436
I0209 11:03:10.559310 139615906252544 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.3728635013103485, loss=1.9271833896636963
I0209 11:03:45.439506 139615914645248 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.4424503445625305, loss=1.9575341939926147
I0209 11:04:20.350219 139615906252544 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3403119444847107, loss=1.878664255142212
I0209 11:04:55.264329 139615914645248 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5960919857025146, loss=2.0350730419158936
I0209 11:05:30.173144 139615906252544 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3697180151939392, loss=1.855674386024475
I0209 11:06:05.108816 139615914645248 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.5355311036109924, loss=1.885291576385498
I0209 11:06:40.029387 139615906252544 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2884156405925751, loss=1.8886443376541138
I0209 11:07:14.926037 139615914645248 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3748787045478821, loss=1.9728022813796997
I0209 11:07:49.844683 139615906252544 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.25914785265922546, loss=1.8816556930541992
I0209 11:08:24.899901 139615914645248 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3658377528190613, loss=1.9271461963653564
I0209 11:08:59.808880 139615906252544 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.35414937138557434, loss=1.919551134109497
I0209 11:09:34.690685 139615914645248 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3327881097793579, loss=1.904357671737671
I0209 11:10:09.599287 139615906252544 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2724580466747284, loss=1.9015408754348755
I0209 11:10:44.477007 139615914645248 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7146201133728027, loss=1.8721739053726196
I0209 11:11:19.368512 139615906252544 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.34536007046699524, loss=2.008592367172241
I0209 11:11:52.569333 139785736898368 spec.py:321] Evaluating on the training split.
I0209 11:11:55.563796 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 11:15:19.497272 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 11:15:22.203933 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 11:18:26.520255 139785736898368 spec.py:349] Evaluating on the test split.
I0209 11:18:29.236003 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 11:21:12.180139 139785736898368 submission_runner.py:408] Time since start: 32308.03s, 	Step: 55397, 	{'train/accuracy': 0.619412362575531, 'train/loss': 1.933494210243225, 'train/bleu': 29.55223406319987, 'validation/accuracy': 0.6385537385940552, 'validation/loss': 1.7798618078231812, 'validation/bleu': 26.649243759342824, 'validation/num_examples': 3000, 'test/accuracy': 0.6478763818740845, 'test/loss': 1.710266351699829, 'test/bleu': 25.675784306440917, 'test/num_examples': 3003, 'score': 19349.615591049194, 'total_duration': 32308.030866384506, 'accumulated_submission_time': 19349.615591049194, 'accumulated_eval_time': 12955.963208198547, 'accumulated_logging_time': 0.6942222118377686}
I0209 11:21:12.203003 139615914645248 logging_writer.py:48] [55397] accumulated_eval_time=12955.963208, accumulated_logging_time=0.694222, accumulated_submission_time=19349.615591, global_step=55397, preemption_count=0, score=19349.615591, test/accuracy=0.647876, test/bleu=25.675784, test/loss=1.710266, test/num_examples=3003, total_duration=32308.030866, train/accuracy=0.619412, train/bleu=29.552234, train/loss=1.933494, validation/accuracy=0.638554, validation/bleu=26.649244, validation/loss=1.779862, validation/num_examples=3000
I0209 11:21:13.616245 139615906252544 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.32373061776161194, loss=1.9666404724121094
I0209 11:21:48.537076 139615914645248 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.28625011444091797, loss=1.9518071413040161
I0209 11:22:23.443253 139615906252544 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.566168487071991, loss=1.991841435432434
I0209 11:22:58.327619 139615914645248 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.4184342622756958, loss=1.9805309772491455
I0209 11:23:33.228364 139615906252544 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.24551790952682495, loss=1.957167625427246
I0209 11:24:08.251678 139615914645248 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.29742422699928284, loss=1.8694862127304077
I0209 11:24:43.197566 139615906252544 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2901952564716339, loss=1.8742774724960327
I0209 11:25:18.101719 139615914645248 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3233797252178192, loss=1.857406735420227
I0209 11:25:52.977873 139615906252544 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2790137827396393, loss=1.9624594449996948
I0209 11:26:27.895430 139615914645248 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.6162965893745422, loss=1.9358572959899902
I0209 11:27:02.770391 139615906252544 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.31374311447143555, loss=1.9722360372543335
I0209 11:27:37.655305 139615914645248 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3281819820404053, loss=1.8828256130218506
I0209 11:28:12.532979 139615906252544 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.511447012424469, loss=2.0187532901763916
I0209 11:28:47.436789 139615914645248 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.35710328817367554, loss=1.845884919166565
I0209 11:29:22.352948 139615906252544 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.4157010614871979, loss=1.9986114501953125
I0209 11:29:57.310355 139615914645248 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.6048949360847473, loss=1.9004442691802979
I0209 11:30:32.184615 139615906252544 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.568710446357727, loss=1.9360703229904175
I0209 11:31:07.085305 139615914645248 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.43441227078437805, loss=1.914882779121399
I0209 11:31:41.969485 139615906252544 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.32592400908470154, loss=1.9571809768676758
I0209 11:32:16.866761 139615914645248 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3774401545524597, loss=2.030440092086792
I0209 11:32:51.708681 139615906252544 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.5313022136688232, loss=1.9203581809997559
I0209 11:33:26.589277 139615914645248 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2574424743652344, loss=1.8724335432052612
I0209 11:34:01.460799 139615906252544 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2823406457901001, loss=1.8465574979782104
I0209 11:34:36.399915 139615914645248 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.33098021149635315, loss=1.9400726556777954
I0209 11:35:11.264831 139615906252544 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.4747219979763031, loss=1.9078264236450195
I0209 11:35:12.390845 139785736898368 spec.py:321] Evaluating on the training split.
I0209 11:35:15.380939 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 11:38:58.193026 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 11:39:00.897324 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 11:42:31.829427 139785736898368 spec.py:349] Evaluating on the test split.
I0209 11:42:34.551721 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 11:45:37.043382 139785736898368 submission_runner.py:408] Time since start: 33772.89s, 	Step: 57805, 	{'train/accuracy': 0.6222780346870422, 'train/loss': 1.8981658220291138, 'train/bleu': 29.94380117071459, 'validation/accuracy': 0.6379957795143127, 'validation/loss': 1.7573578357696533, 'validation/bleu': 26.4487990260602, 'validation/num_examples': 3000, 'test/accuracy': 0.648561954498291, 'test/loss': 1.6985454559326172, 'test/bleu': 25.331366094109622, 'test/num_examples': 3003, 'score': 20189.717071056366, 'total_duration': 33772.894074201584, 'accumulated_submission_time': 20189.717071056366, 'accumulated_eval_time': 13580.615655899048, 'accumulated_logging_time': 0.7280442714691162}
I0209 11:45:37.070076 139615914645248 logging_writer.py:48] [57805] accumulated_eval_time=13580.615656, accumulated_logging_time=0.728044, accumulated_submission_time=20189.717071, global_step=57805, preemption_count=0, score=20189.717071, test/accuracy=0.648562, test/bleu=25.331366, test/loss=1.698545, test/num_examples=3003, total_duration=33772.894074, train/accuracy=0.622278, train/bleu=29.943801, train/loss=1.898166, validation/accuracy=0.637996, validation/bleu=26.448799, validation/loss=1.757358, validation/num_examples=3000
I0209 11:46:10.591408 139615906252544 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.5338463187217712, loss=1.89854896068573
I0209 11:46:45.663525 139615914645248 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2838437259197235, loss=1.8686823844909668
I0209 11:47:20.603980 139615906252544 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.2774997651576996, loss=1.8228126764297485
I0209 11:47:55.505023 139615914645248 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.323049932718277, loss=1.8478316068649292
I0209 11:48:30.373752 139615906252544 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.5320528149604797, loss=1.9087854623794556
I0209 11:49:05.323498 139615914645248 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.5056965351104736, loss=1.8856230974197388
I0209 11:49:40.211251 139615906252544 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.35368233919143677, loss=1.8535295724868774
I0209 11:50:15.137345 139615914645248 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2662251591682434, loss=1.9169130325317383
I0209 11:50:50.038529 139615906252544 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.340562641620636, loss=1.9385682344436646
I0209 11:51:24.976038 139615914645248 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.32054340839385986, loss=1.9503049850463867
I0209 11:51:59.854171 139615906252544 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.5761268734931946, loss=1.9509243965148926
I0209 11:52:34.779060 139615914645248 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.40598365664482117, loss=1.9124675989151
I0209 11:53:09.679745 139615906252544 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.2622215151786804, loss=1.9257175922393799
I0209 11:53:44.560771 139615914645248 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.33881545066833496, loss=1.9283208847045898
I0209 11:54:19.457983 139615906252544 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.32813355326652527, loss=1.9264516830444336
I0209 11:54:54.338841 139615914645248 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.49312910437583923, loss=1.9561758041381836
I0209 11:55:29.210571 139615906252544 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.34406206011772156, loss=1.9122035503387451
I0209 11:56:04.094278 139615914645248 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.36046797037124634, loss=1.8917332887649536
I0209 11:56:38.994379 139615906252544 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3993018567562103, loss=1.8569997549057007
I0209 11:57:13.869700 139615914645248 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.32525449991226196, loss=1.8977651596069336
I0209 11:57:48.748548 139615906252544 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.40779909491539, loss=1.795562744140625
I0209 11:58:23.647727 139615914645248 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2573263943195343, loss=1.8592175245285034
I0209 11:58:58.525314 139615906252544 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.42482125759124756, loss=1.9327741861343384
I0209 11:59:33.432394 139615914645248 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.4545968174934387, loss=1.940037488937378
I0209 11:59:37.352590 139785736898368 spec.py:321] Evaluating on the training split.
I0209 11:59:40.373367 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:02:51.672672 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 12:02:54.397208 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:05:52.436037 139785736898368 spec.py:349] Evaluating on the test split.
I0209 12:05:55.149707 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:08:41.834065 139785736898368 submission_runner.py:408] Time since start: 35157.68s, 	Step: 60213, 	{'train/accuracy': 0.6185654997825623, 'train/loss': 1.9307162761688232, 'train/bleu': 29.454532044501722, 'validation/accuracy': 0.6402896642684937, 'validation/loss': 1.7573803663253784, 'validation/bleu': 26.696946604753215, 'validation/num_examples': 3000, 'test/accuracy': 0.6507001519203186, 'test/loss': 1.6981478929519653, 'test/bleu': 26.085239423423623, 'test/num_examples': 3003, 'score': 21029.91014480591, 'total_duration': 35157.684792757034, 'accumulated_submission_time': 21029.91014480591, 'accumulated_eval_time': 14125.097086429596, 'accumulated_logging_time': 0.7669491767883301}
I0209 12:08:41.858412 139615906252544 logging_writer.py:48] [60213] accumulated_eval_time=14125.097086, accumulated_logging_time=0.766949, accumulated_submission_time=21029.910145, global_step=60213, preemption_count=0, score=21029.910145, test/accuracy=0.650700, test/bleu=26.085239, test/loss=1.698148, test/num_examples=3003, total_duration=35157.684793, train/accuracy=0.618565, train/bleu=29.454532, train/loss=1.930716, validation/accuracy=0.640290, validation/bleu=26.696947, validation/loss=1.757380, validation/num_examples=3000
I0209 12:09:12.539652 139615914645248 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.4266124963760376, loss=1.9214951992034912
I0209 12:09:47.425734 139615906252544 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.28692513704299927, loss=1.8794212341308594
I0209 12:10:22.338271 139615914645248 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.27300116419792175, loss=1.8510868549346924
I0209 12:10:57.199456 139615906252544 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.4211004972457886, loss=1.929233193397522
I0209 12:11:32.129783 139615914645248 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.35086533427238464, loss=1.8638056516647339
I0209 12:12:07.136480 139615906252544 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.5950854420661926, loss=1.970737099647522
I0209 12:12:42.116883 139615914645248 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3229375183582306, loss=2.0053396224975586
I0209 12:13:17.036311 139615906252544 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.6077521443367004, loss=1.9516059160232544
I0209 12:13:51.927767 139615914645248 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3247901499271393, loss=1.8827295303344727
I0209 12:14:26.800036 139615906252544 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2735665738582611, loss=1.8969383239746094
I0209 12:15:01.691300 139615914645248 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.538841187953949, loss=1.8868424892425537
I0209 12:15:36.571861 139615906252544 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.44324034452438354, loss=1.9073102474212646
I0209 12:16:11.453224 139615914645248 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2922850549221039, loss=1.8853929042816162
I0209 12:16:46.340752 139615906252544 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.34070608019828796, loss=1.8310528993606567
I0209 12:17:21.303236 139615914645248 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.30583909153938293, loss=1.8726425170898438
I0209 12:17:56.280508 139615906252544 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3201066553592682, loss=1.9345029592514038
I0209 12:18:31.214265 139615914645248 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2650008797645569, loss=1.8420908451080322
I0209 12:19:06.162922 139615906252544 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3163001537322998, loss=2.021228790283203
I0209 12:19:41.063277 139615914645248 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.36712315678596497, loss=1.8142249584197998
I0209 12:20:15.970627 139615906252544 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2960134744644165, loss=1.8963836431503296
I0209 12:20:50.853038 139615914645248 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.34681087732315063, loss=1.8876478672027588
I0209 12:21:25.782981 139615906252544 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.490307092666626, loss=1.8736485242843628
I0209 12:22:00.690649 139615914645248 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.27682769298553467, loss=1.9231570959091187
I0209 12:22:35.590083 139615906252544 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.32689782977104187, loss=1.915205717086792
I0209 12:22:41.950201 139785736898368 spec.py:321] Evaluating on the training split.
I0209 12:22:44.941012 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:26:11.243922 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 12:26:13.940897 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:29:15.659502 139785736898368 spec.py:349] Evaluating on the test split.
I0209 12:29:18.353241 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:31:56.493124 139785736898368 submission_runner.py:408] Time since start: 36552.34s, 	Step: 62620, 	{'train/accuracy': 0.6608275771141052, 'train/loss': 1.6121519804000854, 'train/bleu': 33.19012386447994, 'validation/accuracy': 0.6453608870506287, 'validation/loss': 1.729269027709961, 'validation/bleu': 26.712245705946223, 'validation/num_examples': 3000, 'test/accuracy': 0.6520132422447205, 'test/loss': 1.678233027458191, 'test/bleu': 25.953660552026207, 'test/num_examples': 3003, 'score': 21869.91243505478, 'total_duration': 36552.34384179115, 'accumulated_submission_time': 21869.91243505478, 'accumulated_eval_time': 14679.639946699142, 'accumulated_logging_time': 0.8013193607330322}
I0209 12:31:56.516308 139615914645248 logging_writer.py:48] [62620] accumulated_eval_time=14679.639947, accumulated_logging_time=0.801319, accumulated_submission_time=21869.912435, global_step=62620, preemption_count=0, score=21869.912435, test/accuracy=0.652013, test/bleu=25.953661, test/loss=1.678233, test/num_examples=3003, total_duration=36552.343842, train/accuracy=0.660828, train/bleu=33.190124, train/loss=1.612152, validation/accuracy=0.645361, validation/bleu=26.712246, validation/loss=1.729269, validation/num_examples=3000
I0209 12:32:24.766235 139615906252544 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2916870713233948, loss=1.9214329719543457
I0209 12:32:59.673019 139615914645248 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.30766528844833374, loss=1.8573215007781982
I0209 12:33:34.617452 139615906252544 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.29082632064819336, loss=1.890738606452942
I0209 12:34:09.544546 139615914645248 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.39156055450439453, loss=1.9278072118759155
I0209 12:34:44.449771 139615906252544 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.4082741439342499, loss=1.8576405048370361
I0209 12:35:19.341221 139615914645248 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.3231864273548126, loss=1.8025506734848022
I0209 12:35:54.320478 139615906252544 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.28364643454551697, loss=1.8832417726516724
I0209 12:36:29.198738 139615914645248 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.362198144197464, loss=1.9398648738861084
I0209 12:37:04.112169 139615906252544 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.42528364062309265, loss=1.949022889137268
I0209 12:37:38.999354 139615914645248 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3350598216056824, loss=1.9556245803833008
I0209 12:38:13.893396 139615906252544 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3550166189670563, loss=1.9260241985321045
I0209 12:38:48.770869 139615914645248 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.37072429060935974, loss=1.9344059228897095
I0209 12:39:23.665511 139615906252544 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.40306273102760315, loss=1.9048696756362915
I0209 12:39:58.613095 139615914645248 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.27844181656837463, loss=1.9174715280532837
I0209 12:40:33.524752 139615906252544 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.4045470058917999, loss=1.7772752046585083
I0209 12:41:08.440253 139615914645248 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3104243576526642, loss=1.993701457977295
I0209 12:41:43.407962 139615906252544 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2708567976951599, loss=1.901529312133789
I0209 12:42:18.305977 139615914645248 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.4931284189224243, loss=2.0332083702087402
I0209 12:42:53.242949 139615906252544 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2876257598400116, loss=1.9351567029953003
I0209 12:43:28.182694 139615914645248 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.37318873405456543, loss=1.9273868799209595
I0209 12:44:03.110404 139615906252544 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.34043100476264954, loss=1.814639925956726
I0209 12:44:38.006207 139615914645248 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.35347509384155273, loss=1.9249944686889648
I0209 12:45:12.882382 139615906252544 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3776361644268036, loss=1.9281085729599
I0209 12:45:47.784637 139615914645248 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3898029327392578, loss=1.915696620941162
I0209 12:45:56.613203 139785736898368 spec.py:321] Evaluating on the training split.
I0209 12:45:59.618660 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:49:10.747861 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 12:49:13.475844 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:52:01.970484 139785736898368 spec.py:349] Evaluating on the test split.
I0209 12:52:04.696631 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 12:54:41.759983 139785736898368 submission_runner.py:408] Time since start: 37917.61s, 	Step: 65027, 	{'train/accuracy': 0.6231135129928589, 'train/loss': 1.8940107822418213, 'train/bleu': 30.158621549991196, 'validation/accuracy': 0.6435257792472839, 'validation/loss': 1.7233736515045166, 'validation/bleu': 26.89136604491987, 'validation/num_examples': 3000, 'test/accuracy': 0.6520829796791077, 'test/loss': 1.6709034442901611, 'test/bleu': 26.038383151652898, 'test/num_examples': 3003, 'score': 22709.923098564148, 'total_duration': 37917.61070728302, 'accumulated_submission_time': 22709.923098564148, 'accumulated_eval_time': 15204.78667140007, 'accumulated_logging_time': 0.8344008922576904}
I0209 12:54:41.784410 139615906252544 logging_writer.py:48] [65027] accumulated_eval_time=15204.786671, accumulated_logging_time=0.834401, accumulated_submission_time=22709.923099, global_step=65027, preemption_count=0, score=22709.923099, test/accuracy=0.652083, test/bleu=26.038383, test/loss=1.670903, test/num_examples=3003, total_duration=37917.610707, train/accuracy=0.623114, train/bleu=30.158622, train/loss=1.894011, validation/accuracy=0.643526, validation/bleu=26.891366, validation/loss=1.723374, validation/num_examples=3000
I0209 12:55:07.662716 139615914645248 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3727876842021942, loss=1.8724788427352905
I0209 12:55:42.618328 139615906252544 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.30483895540237427, loss=1.899823784828186
I0209 12:56:17.524685 139615914645248 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2922654449939728, loss=1.7613582611083984
I0209 12:56:52.429678 139615906252544 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3315969407558441, loss=1.9864510297775269
I0209 12:57:27.318158 139615914645248 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.4064213037490845, loss=1.9291951656341553
I0209 12:58:02.240329 139615906252544 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.4728929102420807, loss=1.932347297668457
I0209 12:58:37.163962 139615914645248 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.29071635007858276, loss=1.8135559558868408
I0209 12:59:12.079436 139615906252544 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.3173639178276062, loss=1.8654717206954956
I0209 12:59:46.994269 139615914645248 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.28428417444229126, loss=1.8888013362884521
I0209 13:00:21.883162 139615906252544 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.40220868587493896, loss=1.8190011978149414
I0209 13:00:56.765191 139615914645248 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2782806158065796, loss=1.8566274642944336
I0209 13:01:31.796431 139615906252544 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.28057774901390076, loss=1.9082276821136475
I0209 13:02:06.683498 139615914645248 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.36056387424468994, loss=1.8397729396820068
I0209 13:02:41.608174 139615906252544 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3182610869407654, loss=1.836486577987671
I0209 13:03:16.494538 139615914645248 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.35419774055480957, loss=1.8975390195846558
I0209 13:03:51.383296 139615906252544 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.3867357075214386, loss=1.9521543979644775
I0209 13:04:26.290906 139615914645248 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.29908517003059387, loss=1.8897444009780884
I0209 13:05:01.168175 139615906252544 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3201252222061157, loss=1.954576849937439
I0209 13:05:36.060146 139615914645248 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.28452029824256897, loss=1.8049134016036987
I0209 13:06:10.943079 139615906252544 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3068448305130005, loss=1.9291545152664185
I0209 13:06:45.829161 139615914645248 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.2845804989337921, loss=1.9251124858856201
I0209 13:07:20.728471 139615906252544 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.31041914224624634, loss=1.8449872732162476
I0209 13:07:55.701623 139615914645248 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3515051603317261, loss=1.8251299858093262
I0209 13:08:30.589643 139615906252544 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.5723702311515808, loss=1.8816888332366943
I0209 13:08:41.835759 139785736898368 spec.py:321] Evaluating on the training split.
I0209 13:08:44.830975 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:12:41.344100 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 13:12:44.062429 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:15:42.628317 139785736898368 spec.py:349] Evaluating on the test split.
I0209 13:15:45.350517 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:18:42.401630 139785736898368 submission_runner.py:408] Time since start: 39358.25s, 	Step: 67434, 	{'train/accuracy': 0.6210484504699707, 'train/loss': 1.9104690551757812, 'train/bleu': 30.423989758952263, 'validation/accuracy': 0.6472083330154419, 'validation/loss': 1.7089643478393555, 'validation/bleu': 26.998232384874996, 'validation/num_examples': 3000, 'test/accuracy': 0.6572192311286926, 'test/loss': 1.6496104001998901, 'test/bleu': 26.566984573098747, 'test/num_examples': 3003, 'score': 23549.88878941536, 'total_duration': 39358.25233221054, 'accumulated_submission_time': 23549.88878941536, 'accumulated_eval_time': 15805.35246348381, 'accumulated_logging_time': 0.8699424266815186}
I0209 13:18:42.431064 139615914645248 logging_writer.py:48] [67434] accumulated_eval_time=15805.352463, accumulated_logging_time=0.869942, accumulated_submission_time=23549.888789, global_step=67434, preemption_count=0, score=23549.888789, test/accuracy=0.657219, test/bleu=26.566985, test/loss=1.649610, test/num_examples=3003, total_duration=39358.252332, train/accuracy=0.621048, train/bleu=30.423990, train/loss=1.910469, validation/accuracy=0.647208, validation/bleu=26.998232, validation/loss=1.708964, validation/num_examples=3000
I0209 13:19:05.829568 139615906252544 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.35827314853668213, loss=1.8354429006576538
I0209 13:19:40.737462 139615914645248 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.29089462757110596, loss=1.8431164026260376
I0209 13:20:15.663516 139615906252544 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.27609971165657043, loss=1.829033613204956
I0209 13:20:50.592700 139615914645248 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.5493252873420715, loss=1.8764541149139404
I0209 13:21:25.462743 139615906252544 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2605751156806946, loss=1.9475160837173462
I0209 13:22:00.340764 139615914645248 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3641478717327118, loss=1.767534613609314
I0209 13:22:35.214947 139615906252544 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.35705071687698364, loss=1.9103330373764038
I0209 13:23:10.147106 139615914645248 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.3488841950893402, loss=2.029034376144409
I0209 13:23:45.049087 139615906252544 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.388086199760437, loss=1.854785442352295
I0209 13:24:19.978425 139615914645248 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2778773307800293, loss=1.8123741149902344
I0209 13:24:54.874465 139615906252544 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.26181530952453613, loss=1.863638162612915
I0209 13:25:29.775688 139615914645248 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.32303398847579956, loss=1.8660707473754883
I0209 13:26:04.672450 139615906252544 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.4475077688694, loss=1.8655916452407837
I0209 13:26:39.563117 139615914645248 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.32020068168640137, loss=1.9765347242355347
I0209 13:27:14.445897 139615906252544 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3249174952507019, loss=1.844874620437622
I0209 13:27:49.339616 139615914645248 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.3972041606903076, loss=1.804797649383545
I0209 13:28:24.210560 139615906252544 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.32438939809799194, loss=1.9305729866027832
I0209 13:28:59.083324 139615914645248 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.299357533454895, loss=1.850575566291809
I0209 13:29:34.010136 139615906252544 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.49435198307037354, loss=1.8382060527801514
I0209 13:30:08.975938 139615914645248 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.4831734597682953, loss=1.8742902278900146
I0209 13:30:43.857594 139615906252544 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.27225232124328613, loss=1.8839596509933472
I0209 13:31:18.790102 139615914645248 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.39096400141716003, loss=1.7982488870620728
I0209 13:31:53.685445 139615906252544 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.31202995777130127, loss=1.9737837314605713
I0209 13:32:28.585086 139615914645248 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3160860240459442, loss=1.816540002822876
I0209 13:32:42.637371 139785736898368 spec.py:321] Evaluating on the training split.
I0209 13:32:45.644643 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:36:38.758727 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 13:36:41.482048 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:39:16.355626 139785736898368 spec.py:349] Evaluating on the test split.
I0209 13:39:19.087334 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:42:02.843486 139785736898368 submission_runner.py:408] Time since start: 40758.69s, 	Step: 69842, 	{'train/accuracy': 0.6296305060386658, 'train/loss': 1.8308836221694946, 'train/bleu': 30.28551842580955, 'validation/accuracy': 0.6483986377716064, 'validation/loss': 1.700315237045288, 'validation/bleu': 27.293684458325167, 'validation/num_examples': 3000, 'test/accuracy': 0.6600197553634644, 'test/loss': 1.6326422691345215, 'test/bleu': 26.693438091255096, 'test/num_examples': 3003, 'score': 24390.009131908417, 'total_duration': 40758.694177389145, 'accumulated_submission_time': 24390.009131908417, 'accumulated_eval_time': 16365.558494091034, 'accumulated_logging_time': 0.9100024700164795}
I0209 13:42:02.868879 139615906252544 logging_writer.py:48] [69842] accumulated_eval_time=16365.558494, accumulated_logging_time=0.910002, accumulated_submission_time=24390.009132, global_step=69842, preemption_count=0, score=24390.009132, test/accuracy=0.660020, test/bleu=26.693438, test/loss=1.632642, test/num_examples=3003, total_duration=40758.694177, train/accuracy=0.629631, train/bleu=30.285518, train/loss=1.830884, validation/accuracy=0.648399, validation/bleu=27.293684, validation/loss=1.700315, validation/num_examples=3000
I0209 13:42:23.501889 139615914645248 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2954894006252289, loss=1.904868245124817
I0209 13:42:58.439061 139615906252544 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.31152698397636414, loss=1.7724963426589966
I0209 13:43:33.356427 139615914645248 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.26933756470680237, loss=1.8237797021865845
I0209 13:44:08.291527 139615906252544 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4763350486755371, loss=1.833748459815979
I0209 13:44:43.173820 139615914645248 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3087138235569, loss=1.826053261756897
I0209 13:45:18.082723 139615906252544 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.31007760763168335, loss=1.8094969987869263
I0209 13:45:52.956224 139615914645248 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3325689136981964, loss=1.9307390451431274
I0209 13:46:27.868092 139615906252544 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.38643819093704224, loss=1.800397515296936
I0209 13:47:02.761780 139615914645248 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.42618727684020996, loss=1.8279922008514404
I0209 13:47:37.688239 139615906252544 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.4864201247692108, loss=1.86847984790802
I0209 13:48:12.597704 139615914645248 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3224043548107147, loss=1.849212408065796
I0209 13:48:47.518433 139615906252544 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3115427792072296, loss=1.8249154090881348
I0209 13:49:22.445307 139615914645248 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.28245794773101807, loss=1.8427917957305908
I0209 13:49:57.349610 139615906252544 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.27476951479911804, loss=1.8632259368896484
I0209 13:50:32.227900 139615914645248 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.32362625002861023, loss=1.811569094657898
I0209 13:51:07.151187 139615906252544 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3068443238735199, loss=1.8704811334609985
I0209 13:51:42.067498 139615914645248 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.30880990624427795, loss=1.875881314277649
I0209 13:52:16.969826 139615906252544 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.2558977007865906, loss=1.8325642347335815
I0209 13:52:51.842238 139615914645248 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3446546494960785, loss=1.8247573375701904
I0209 13:53:26.742573 139615906252544 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.2795938551425934, loss=1.8436859846115112
I0209 13:54:01.643881 139615914645248 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.289876252412796, loss=1.7294654846191406
I0209 13:54:36.530088 139615906252544 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.38157686591148376, loss=1.893219232559204
I0209 13:55:11.429168 139615914645248 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3333294093608856, loss=1.8282023668289185
I0209 13:55:46.351771 139615906252544 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.40865129232406616, loss=1.8840250968933105
I0209 13:56:03.192123 139785736898368 spec.py:321] Evaluating on the training split.
I0209 13:56:06.210536 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 13:59:22.686046 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 13:59:25.405880 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:01:58.321470 139785736898368 spec.py:349] Evaluating on the test split.
I0209 14:02:01.039476 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:04:33.144343 139785736898368 submission_runner.py:408] Time since start: 42109.00s, 	Step: 72250, 	{'train/accuracy': 0.6270090937614441, 'train/loss': 1.8659878969192505, 'train/bleu': 30.405617139420453, 'validation/accuracy': 0.6491177678108215, 'validation/loss': 1.6966098546981812, 'validation/bleu': 27.24253045501503, 'validation/num_examples': 3000, 'test/accuracy': 0.6600894927978516, 'test/loss': 1.6326830387115479, 'test/bleu': 26.730419230948563, 'test/num_examples': 3003, 'score': 25230.245144844055, 'total_duration': 42108.99506902695, 'accumulated_submission_time': 25230.245144844055, 'accumulated_eval_time': 16875.51067852974, 'accumulated_logging_time': 0.9470679759979248}
I0209 14:04:33.170162 139615914645248 logging_writer.py:48] [72250] accumulated_eval_time=16875.510679, accumulated_logging_time=0.947068, accumulated_submission_time=25230.245145, global_step=72250, preemption_count=0, score=25230.245145, test/accuracy=0.660089, test/bleu=26.730419, test/loss=1.632683, test/num_examples=3003, total_duration=42108.995069, train/accuracy=0.627009, train/bleu=30.405617, train/loss=1.865988, validation/accuracy=0.649118, validation/bleu=27.242530, validation/loss=1.696610, validation/num_examples=3000
I0209 14:04:50.978873 139615906252544 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.28642532229423523, loss=1.7988677024841309
I0209 14:05:25.886941 139615914645248 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.33379557728767395, loss=1.8198845386505127
I0209 14:06:00.807319 139615906252544 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.41106778383255005, loss=1.8227893114089966
I0209 14:06:35.724313 139615914645248 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.5408815145492554, loss=1.7667149305343628
I0209 14:07:10.636836 139615906252544 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.35024020075798035, loss=1.8417307138442993
I0209 14:07:45.527198 139615914645248 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2961903214454651, loss=1.8717608451843262
I0209 14:08:20.436539 139615906252544 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.34718841314315796, loss=1.8534845113754272
I0209 14:08:55.318478 139615914645248 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2961871922016144, loss=1.863230586051941
I0209 14:09:30.223170 139615906252544 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3038959205150604, loss=1.798647403717041
I0209 14:10:05.128759 139615914645248 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3754090964794159, loss=1.8407692909240723
I0209 14:10:40.056788 139615906252544 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.29485204815864563, loss=1.8344389200210571
I0209 14:11:14.961134 139615914645248 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3026442527770996, loss=1.8969484567642212
I0209 14:11:49.880263 139615906252544 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3468155860900879, loss=1.788602590560913
I0209 14:12:24.786406 139615914645248 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.33643028140068054, loss=1.7759796380996704
I0209 14:12:59.663510 139615906252544 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2917436361312866, loss=1.8613213300704956
I0209 14:13:34.604063 139615914645248 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.40127065777778625, loss=1.8867982625961304
I0209 14:14:09.513352 139615906252544 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.26685631275177, loss=1.8490872383117676
I0209 14:14:44.410316 139615914645248 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.34231823682785034, loss=1.8816899061203003
I0209 14:15:19.324460 139615906252544 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.29241684079170227, loss=1.8307459354400635
I0209 14:15:54.238364 139615914645248 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.32933181524276733, loss=1.8396235704421997
I0209 14:16:29.125604 139615906252544 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3435952663421631, loss=1.85368013381958
I0209 14:17:04.059071 139615914645248 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3508680462837219, loss=1.8165619373321533
I0209 14:17:38.993213 139615906252544 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.2634549140930176, loss=1.8229994773864746
I0209 14:18:13.902866 139615914645248 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.27400729060173035, loss=1.7521774768829346
I0209 14:18:33.156430 139785736898368 spec.py:321] Evaluating on the training split.
I0209 14:18:36.156883 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:22:12.041435 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 14:22:14.774830 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:24:56.246016 139785736898368 spec.py:349] Evaluating on the test split.
I0209 14:24:58.974421 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:27:37.002945 139785736898368 submission_runner.py:408] Time since start: 43492.85s, 	Step: 74657, 	{'train/accuracy': 0.6295642852783203, 'train/loss': 1.8524365425109863, 'train/bleu': 29.989682284103964, 'validation/accuracy': 0.6508660912513733, 'validation/loss': 1.6791478395462036, 'validation/bleu': 27.602717674089575, 'validation/num_examples': 3000, 'test/accuracy': 0.6641218066215515, 'test/loss': 1.6093859672546387, 'test/bleu': 26.914830269027004, 'test/num_examples': 3003, 'score': 26070.144366502762, 'total_duration': 43492.85367035866, 'accumulated_submission_time': 26070.144366502762, 'accumulated_eval_time': 17419.357144355774, 'accumulated_logging_time': 0.9834208488464355}
I0209 14:27:37.028898 139615906252544 logging_writer.py:48] [74657] accumulated_eval_time=17419.357144, accumulated_logging_time=0.983421, accumulated_submission_time=26070.144367, global_step=74657, preemption_count=0, score=26070.144367, test/accuracy=0.664122, test/bleu=26.914830, test/loss=1.609386, test/num_examples=3003, total_duration=43492.853670, train/accuracy=0.629564, train/bleu=29.989682, train/loss=1.852437, validation/accuracy=0.650866, validation/bleu=27.602718, validation/loss=1.679148, validation/num_examples=3000
I0209 14:27:52.375512 139615914645248 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3043031692504883, loss=1.8482987880706787
I0209 14:28:27.283189 139615906252544 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2847183346748352, loss=1.8828798532485962
I0209 14:29:02.156981 139615914645248 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.29661640524864197, loss=1.7657588720321655
I0209 14:29:37.064706 139615906252544 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3272143602371216, loss=1.878827452659607
I0209 14:30:11.989415 139615914645248 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.43537887930870056, loss=1.7966641187667847
I0209 14:30:46.889170 139615906252544 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.31204408407211304, loss=1.7849639654159546
I0209 14:31:21.788252 139615914645248 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.46674874424934387, loss=1.8238377571105957
I0209 14:31:56.701436 139615906252544 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.32525870203971863, loss=1.7802667617797852
I0209 14:32:31.619254 139615914645248 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.30530038475990295, loss=1.7787377834320068
I0209 14:33:06.545356 139615906252544 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2961466312408447, loss=1.8481409549713135
I0209 14:33:41.435499 139615914645248 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.326291024684906, loss=1.764323353767395
I0209 14:34:16.344060 139615906252544 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.31027594208717346, loss=1.8219050168991089
I0209 14:34:51.223356 139615914645248 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3075956106185913, loss=1.9109326601028442
I0209 14:35:26.139816 139615906252544 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3683462142944336, loss=1.8845218420028687
I0209 14:36:01.068231 139615914645248 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.283743679523468, loss=1.7283262014389038
I0209 14:36:35.925658 139615906252544 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.37877634167671204, loss=1.8052698373794556
I0209 14:37:10.864998 139615914645248 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.5678497552871704, loss=1.8659228086471558
I0209 14:37:45.755413 139615906252544 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.4272747337818146, loss=1.9285705089569092
I0209 14:38:20.646507 139615914645248 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2795819938182831, loss=1.787580966949463
I0209 14:38:55.549998 139615906252544 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3514813780784607, loss=1.809549331665039
I0209 14:39:30.469690 139615914645248 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3684081435203552, loss=1.7875981330871582
I0209 14:40:05.406960 139615906252544 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3071573078632355, loss=1.8322665691375732
I0209 14:40:40.284141 139615914645248 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.38345494866371155, loss=1.8883030414581299
I0209 14:41:15.188432 139615906252544 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.294845312833786, loss=1.7897599935531616
I0209 14:41:37.226795 139785736898368 spec.py:321] Evaluating on the training split.
I0209 14:41:40.222538 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:44:47.159605 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 14:44:49.861500 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:47:33.107276 139785736898368 spec.py:349] Evaluating on the test split.
I0209 14:47:35.821535 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 14:50:11.513952 139785736898368 submission_runner.py:408] Time since start: 44847.36s, 	Step: 77065, 	{'train/accuracy': 0.6358433365821838, 'train/loss': 1.804227352142334, 'train/bleu': 30.153082714253948, 'validation/accuracy': 0.6521183848381042, 'validation/loss': 1.6711089611053467, 'validation/bleu': 27.59902273365211, 'validation/num_examples': 3000, 'test/accuracy': 0.6620184779167175, 'test/loss': 1.6064454317092896, 'test/bleu': 26.76849545725773, 'test/num_examples': 3003, 'score': 26910.256639242172, 'total_duration': 44847.364674806595, 'accumulated_submission_time': 26910.256639242172, 'accumulated_eval_time': 17933.644248008728, 'accumulated_logging_time': 1.0192360877990723}
I0209 14:50:11.541001 139615914645248 logging_writer.py:48] [77065] accumulated_eval_time=17933.644248, accumulated_logging_time=1.019236, accumulated_submission_time=26910.256639, global_step=77065, preemption_count=0, score=26910.256639, test/accuracy=0.662018, test/bleu=26.768495, test/loss=1.606445, test/num_examples=3003, total_duration=44847.364675, train/accuracy=0.635843, train/bleu=30.153083, train/loss=1.804227, validation/accuracy=0.652118, validation/bleu=27.599023, validation/loss=1.671109, validation/num_examples=3000
I0209 14:50:24.101213 139615906252544 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.28033140301704407, loss=1.823233723640442
I0209 14:50:58.979861 139615914645248 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.30927202105522156, loss=1.8273330926895142
I0209 14:51:33.878033 139615906252544 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.42092427611351013, loss=1.8205370903015137
I0209 14:52:08.764515 139615914645248 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.277920663356781, loss=1.8329293727874756
I0209 14:52:43.695544 139615906252544 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.48291268944740295, loss=1.7819359302520752
I0209 14:53:18.580330 139615914645248 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.300929456949234, loss=1.7294411659240723
I0209 14:53:53.467966 139615906252544 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.2756001949310303, loss=1.7872512340545654
I0209 14:54:28.362257 139615914645248 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.41819626092910767, loss=1.8182467222213745
I0209 14:55:03.249365 139615906252544 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.4702052175998688, loss=1.9539035558700562
I0209 14:55:38.177077 139615914645248 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.35035911202430725, loss=1.8818219900131226
I0209 14:56:13.112094 139615906252544 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.33154523372650146, loss=1.7961193323135376
I0209 14:56:48.056017 139615914645248 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3395376205444336, loss=1.8224010467529297
I0209 14:57:22.985653 139615906252544 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2992958426475525, loss=1.8336843252182007
I0209 14:57:57.857604 139615914645248 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.363268107175827, loss=1.8295289278030396
I0209 14:58:32.739364 139615906252544 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.4216866195201874, loss=1.8678227663040161
I0209 14:59:07.622101 139615914645248 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3199653923511505, loss=1.8634088039398193
I0209 14:59:42.501272 139615906252544 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.27380093932151794, loss=1.8250699043273926
I0209 15:00:17.404582 139615914645248 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.35213592648506165, loss=1.8089038133621216
I0209 15:00:52.311657 139615906252544 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.33617958426475525, loss=1.795149564743042
I0209 15:01:27.196443 139615914645248 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3113851845264435, loss=1.8075114488601685
I0209 15:02:02.100283 139615906252544 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.2789372503757477, loss=1.794541597366333
I0209 15:02:36.972148 139615914645248 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.27789047360420227, loss=1.7558878660202026
I0209 15:03:11.865655 139615906252544 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.35544291138648987, loss=1.7694944143295288
I0209 15:03:46.810786 139615914645248 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.29290446639060974, loss=1.771438479423523
I0209 15:04:11.671571 139785736898368 spec.py:321] Evaluating on the training split.
I0209 15:04:14.676015 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:08:29.014208 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 15:08:31.716320 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:11:17.015917 139785736898368 spec.py:349] Evaluating on the test split.
I0209 15:11:19.739911 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:13:51.803806 139785736898368 submission_runner.py:408] Time since start: 46267.65s, 	Step: 79473, 	{'train/accuracy': 0.632390558719635, 'train/loss': 1.8178280591964722, 'train/bleu': 30.46417054723414, 'validation/accuracy': 0.654263436794281, 'validation/loss': 1.6578301191329956, 'validation/bleu': 27.5063002215074, 'validation/num_examples': 3000, 'test/accuracy': 0.6640869379043579, 'test/loss': 1.592431664466858, 'test/bleu': 26.96958389750291, 'test/num_examples': 3003, 'score': 27750.299865961075, 'total_duration': 46267.65453505516, 'accumulated_submission_time': 27750.299865961075, 'accumulated_eval_time': 18513.776450157166, 'accumulated_logging_time': 1.0562868118286133}
I0209 15:13:51.830135 139615906252544 logging_writer.py:48] [79473] accumulated_eval_time=18513.776450, accumulated_logging_time=1.056287, accumulated_submission_time=27750.299866, global_step=79473, preemption_count=0, score=27750.299866, test/accuracy=0.664087, test/bleu=26.969584, test/loss=1.592432, test/num_examples=3003, total_duration=46267.654535, train/accuracy=0.632391, train/bleu=30.464171, train/loss=1.817828, validation/accuracy=0.654263, validation/bleu=27.506300, validation/loss=1.657830, validation/num_examples=3000
I0209 15:14:01.633567 139615914645248 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2953084409236908, loss=1.7954288721084595
I0209 15:14:36.562543 139615906252544 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.32749930024147034, loss=1.8015599250793457
I0209 15:15:11.478386 139615914645248 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3334668278694153, loss=1.8895564079284668
I0209 15:15:46.343115 139615906252544 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.42393913865089417, loss=1.8843330144882202
I0209 15:16:21.271591 139615914645248 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.30174532532691956, loss=1.7984020709991455
I0209 15:16:56.153076 139615906252544 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.29749464988708496, loss=1.7464563846588135
I0209 15:17:31.053035 139615914645248 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.36554592847824097, loss=1.9298709630966187
I0209 15:18:05.983211 139615906252544 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.32895946502685547, loss=1.7815983295440674
I0209 15:18:40.888389 139615914645248 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.3270326554775238, loss=1.8623206615447998
I0209 15:19:15.770104 139615906252544 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3831193745136261, loss=1.807909369468689
I0209 15:19:50.683883 139615914645248 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.36608371138572693, loss=1.8167303800582886
I0209 15:20:25.592230 139615906252544 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.32564276456832886, loss=1.7980856895446777
I0209 15:21:00.511237 139615914645248 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.31317034363746643, loss=1.8883225917816162
I0209 15:21:35.380649 139615906252544 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.29864221811294556, loss=1.831694483757019
I0209 15:22:10.318861 139615914645248 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3087468147277832, loss=1.7903958559036255
I0209 15:22:45.232240 139615906252544 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.31678345799446106, loss=1.7248284816741943
I0209 15:23:20.126224 139615914645248 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.30436772108078003, loss=1.782810926437378
I0209 15:23:55.068011 139615906252544 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.3408129811286926, loss=1.80576753616333
I0209 15:24:29.982511 139615914645248 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3076658844947815, loss=1.7568364143371582
I0209 15:25:04.895196 139615906252544 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.4049488604068756, loss=1.7721900939941406
I0209 15:25:39.774192 139615914645248 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.31232142448425293, loss=1.6966018676757812
I0209 15:26:14.673182 139615906252544 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.40148818492889404, loss=1.7713710069656372
I0209 15:26:49.561191 139615914645248 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2956567704677582, loss=1.7946505546569824
I0209 15:27:24.487434 139615906252544 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.31273624300956726, loss=1.7962920665740967
I0209 15:27:52.123418 139785736898368 spec.py:321] Evaluating on the training split.
I0209 15:27:55.116310 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:31:17.537038 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 15:31:20.246814 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:34:17.333801 139785736898368 spec.py:349] Evaluating on the test split.
I0209 15:34:20.051722 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:37:18.677087 139785736898368 submission_runner.py:408] Time since start: 47674.53s, 	Step: 81881, 	{'train/accuracy': 0.6480662226676941, 'train/loss': 1.706380844116211, 'train/bleu': 31.98326599336038, 'validation/accuracy': 0.6571152210235596, 'validation/loss': 1.6383646726608276, 'validation/bleu': 27.902996214779897, 'validation/num_examples': 3000, 'test/accuracy': 0.6693974733352661, 'test/loss': 1.568246603012085, 'test/bleu': 27.44548816720142, 'test/num_examples': 3003, 'score': 28590.505031108856, 'total_duration': 47674.52781748772, 'accumulated_submission_time': 28590.505031108856, 'accumulated_eval_time': 19080.33007502556, 'accumulated_logging_time': 1.0926096439361572}
I0209 15:37:18.703791 139615914645248 logging_writer.py:48] [81881] accumulated_eval_time=19080.330075, accumulated_logging_time=1.092610, accumulated_submission_time=28590.505031, global_step=81881, preemption_count=0, score=28590.505031, test/accuracy=0.669397, test/bleu=27.445488, test/loss=1.568247, test/num_examples=3003, total_duration=47674.527817, train/accuracy=0.648066, train/bleu=31.983266, train/loss=1.706381, validation/accuracy=0.657115, validation/bleu=27.902996, validation/loss=1.638365, validation/num_examples=3000
I0209 15:37:25.693624 139615906252544 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.29078933596611023, loss=1.7795013189315796
I0209 15:38:00.601761 139615914645248 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.27607083320617676, loss=1.7580357789993286
I0209 15:38:35.589947 139615906252544 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3514257073402405, loss=1.847888469696045
I0209 15:39:10.525214 139615914645248 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3358999788761139, loss=1.7381985187530518
I0209 15:39:45.459861 139615906252544 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.3621615767478943, loss=1.8742812871932983
I0209 15:40:20.393778 139615914645248 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.28374648094177246, loss=1.8046940565109253
I0209 15:40:55.300275 139615906252544 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3749256432056427, loss=1.8323078155517578
I0209 15:41:30.215851 139615914645248 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3032260835170746, loss=1.729159951210022
I0209 15:42:05.114523 139615906252544 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3335791826248169, loss=1.7920851707458496
I0209 15:42:40.030072 139615914645248 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.33190396428108215, loss=1.781428575515747
I0209 15:43:14.909035 139615906252544 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.3389706611633301, loss=1.8151803016662598
I0209 15:43:49.789460 139615914645248 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3413856327533722, loss=1.7766313552856445
I0209 15:44:24.664875 139615906252544 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.30597689747810364, loss=1.696982502937317
I0209 15:44:59.556519 139615914645248 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.26734673976898193, loss=1.759790301322937
I0209 15:45:34.441070 139615906252544 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.4001755118370056, loss=1.8079808950424194
I0209 15:46:09.316723 139615914645248 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3393458127975464, loss=1.717178225517273
I0209 15:46:44.205526 139615906252544 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.30633988976478577, loss=1.7690935134887695
I0209 15:47:19.108867 139615914645248 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.27474287152290344, loss=1.7651362419128418
I0209 15:47:53.997103 139615906252544 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2917013466358185, loss=1.7878884077072144
I0209 15:48:28.888792 139615914645248 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.4184833765029907, loss=1.8542085886001587
I0209 15:49:03.798493 139615906252544 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.312388151884079, loss=1.7336031198501587
I0209 15:49:38.697859 139615914645248 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.34307432174682617, loss=1.846886396408081
I0209 15:50:13.597974 139615906252544 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.3286498785018921, loss=1.7769509553909302
I0209 15:50:48.488353 139615914645248 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.2843916714191437, loss=1.7135740518569946
I0209 15:51:18.919053 139785736898368 spec.py:321] Evaluating on the training split.
I0209 15:51:21.910698 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:55:29.532866 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 15:55:32.234518 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 15:58:14.115998 139785736898368 spec.py:349] Evaluating on the test split.
I0209 15:58:16.845979 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:00:42.272175 139785736898368 submission_runner.py:408] Time since start: 49078.12s, 	Step: 84289, 	{'train/accuracy': 0.6433086395263672, 'train/loss': 1.7546846866607666, 'train/bleu': 30.503595770525028, 'validation/accuracy': 0.6575739979743958, 'validation/loss': 1.631597876548767, 'validation/bleu': 27.817556353401674, 'validation/num_examples': 3000, 'test/accuracy': 0.6694555878639221, 'test/loss': 1.5594232082366943, 'test/bleu': 27.313128787818524, 'test/num_examples': 3003, 'score': 29430.63443851471, 'total_duration': 49078.12288761139, 'accumulated_submission_time': 29430.63443851471, 'accumulated_eval_time': 19643.68313574791, 'accumulated_logging_time': 1.1292719841003418}
I0209 16:00:42.300383 139615906252544 logging_writer.py:48] [84289] accumulated_eval_time=19643.683136, accumulated_logging_time=1.129272, accumulated_submission_time=29430.634439, global_step=84289, preemption_count=0, score=29430.634439, test/accuracy=0.669456, test/bleu=27.313129, test/loss=1.559423, test/num_examples=3003, total_duration=49078.122888, train/accuracy=0.643309, train/bleu=30.503596, train/loss=1.754685, validation/accuracy=0.657574, validation/bleu=27.817556, validation/loss=1.631598, validation/num_examples=3000
I0209 16:00:46.487996 139615914645248 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3081321716308594, loss=1.7931450605392456
I0209 16:01:21.409015 139615906252544 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3297179639339447, loss=1.8087855577468872
I0209 16:01:56.295642 139615914645248 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3578912317752838, loss=1.7902171611785889
I0209 16:02:31.193758 139615906252544 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3084718883037567, loss=1.6851389408111572
I0209 16:03:06.129292 139615914645248 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.2952902615070343, loss=1.7378253936767578
I0209 16:03:41.026370 139615906252544 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.31444844603538513, loss=1.8307853937149048
I0209 16:04:15.966024 139615914645248 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2934235632419586, loss=1.7779333591461182
I0209 16:04:50.904230 139615906252544 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3281446099281311, loss=1.7272847890853882
I0209 16:05:25.800429 139615914645248 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.27567434310913086, loss=1.6856751441955566
I0209 16:06:00.669214 139615906252544 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.38036078214645386, loss=1.790565848350525
I0209 16:06:35.582181 139615914645248 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.32890811562538147, loss=1.8176770210266113
I0209 16:07:10.457038 139615906252544 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3441123068332672, loss=1.745814561843872
I0209 16:07:45.349240 139615914645248 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.28389471769332886, loss=1.751940131187439
I0209 16:08:20.240039 139615906252544 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2718277871608734, loss=1.7528645992279053
I0209 16:08:55.131938 139615914645248 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.2829199433326721, loss=1.7653534412384033
I0209 16:09:29.996899 139615906252544 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.31821778416633606, loss=1.7388395071029663
I0209 16:10:04.908988 139615914645248 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.308423787355423, loss=1.8105887174606323
I0209 16:10:39.771648 139615906252544 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3037535548210144, loss=1.8728939294815063
I0209 16:11:14.727465 139615914645248 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3326472342014313, loss=1.8318973779678345
I0209 16:11:49.622300 139615906252544 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.29965734481811523, loss=1.7738412618637085
I0209 16:12:24.506190 139615914645248 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.31114616990089417, loss=1.7610193490982056
I0209 16:12:59.383343 139615906252544 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3230571448802948, loss=1.7051475048065186
I0209 16:13:34.292865 139615914645248 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.34017422795295715, loss=1.764296293258667
I0209 16:14:09.187379 139615906252544 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.42673996090888977, loss=1.7846050262451172
I0209 16:14:42.470960 139785736898368 spec.py:321] Evaluating on the training split.
I0209 16:14:45.498660 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:18:22.033420 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 16:18:24.748683 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:21:04.568369 139785736898368 spec.py:349] Evaluating on the test split.
I0209 16:21:07.280137 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:23:51.880617 139785736898368 submission_runner.py:408] Time since start: 50467.73s, 	Step: 86697, 	{'train/accuracy': 0.6414464712142944, 'train/loss': 1.7629821300506592, 'train/bleu': 30.919897634747738, 'validation/accuracy': 0.6594710350036621, 'validation/loss': 1.6154887676239014, 'validation/bleu': 27.643855522380758, 'validation/num_examples': 3000, 'test/accuracy': 0.6706408858299255, 'test/loss': 1.5513267517089844, 'test/bleu': 27.45172026614546, 'test/num_examples': 3003, 'score': 30270.717396259308, 'total_duration': 50467.73133611679, 'accumulated_submission_time': 30270.717396259308, 'accumulated_eval_time': 20193.092746973038, 'accumulated_logging_time': 1.167518138885498}
I0209 16:23:51.908342 139615914645248 logging_writer.py:48] [86697] accumulated_eval_time=20193.092747, accumulated_logging_time=1.167518, accumulated_submission_time=30270.717396, global_step=86697, preemption_count=0, score=30270.717396, test/accuracy=0.670641, test/bleu=27.451720, test/loss=1.551327, test/num_examples=3003, total_duration=50467.731336, train/accuracy=0.641446, train/bleu=30.919898, train/loss=1.762982, validation/accuracy=0.659471, validation/bleu=27.643856, validation/loss=1.615489, validation/num_examples=3000
I0209 16:23:53.321725 139615906252544 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3297160267829895, loss=1.7686847448349
I0209 16:24:28.184393 139615914645248 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4477163553237915, loss=1.727162480354309
I0209 16:25:03.068539 139615906252544 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.35418832302093506, loss=1.7665979862213135
I0209 16:25:37.960242 139615914645248 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.3210567235946655, loss=1.7021989822387695
I0209 16:26:12.835337 139615906252544 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.3427039682865143, loss=1.8056520223617554
I0209 16:26:47.729955 139615914645248 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.28932103514671326, loss=1.73697829246521
I0209 16:27:22.639480 139615906252544 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.30810433626174927, loss=1.7324647903442383
I0209 16:27:57.513647 139615914645248 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.2897912263870239, loss=1.7972795963287354
I0209 16:28:32.413282 139615906252544 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3388558030128479, loss=1.7535831928253174
I0209 16:29:07.317547 139615914645248 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.33460310101509094, loss=1.7865689992904663
I0209 16:29:42.241644 139615906252544 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.326630175113678, loss=1.789627194404602
I0209 16:30:17.171997 139615914645248 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3688458502292633, loss=1.7478148937225342
I0209 16:30:52.084427 139615906252544 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3263874650001526, loss=1.839866280555725
I0209 16:31:26.965023 139615914645248 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3141615390777588, loss=1.7108232975006104
I0209 16:32:01.851195 139615906252544 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.30999958515167236, loss=1.767685055732727
I0209 16:32:36.788347 139615914645248 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.28232190012931824, loss=1.7557767629623413
I0209 16:33:11.708921 139615906252544 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.40661099553108215, loss=1.6725904941558838
I0209 16:33:46.603624 139615914645248 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.35258880257606506, loss=1.819961428642273
I0209 16:34:21.490638 139615906252544 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.34034425020217896, loss=1.713496208190918
I0209 16:34:56.368446 139615914645248 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3591531813144684, loss=1.768716812133789
I0209 16:35:31.267626 139615906252544 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.33670419454574585, loss=1.7886552810668945
I0209 16:36:06.139234 139615914645248 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.33198463916778564, loss=1.813607931137085
I0209 16:36:41.032819 139615906252544 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3184780180454254, loss=1.6258385181427002
I0209 16:37:15.935865 139615914645248 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.27999770641326904, loss=1.7000287771224976
I0209 16:37:50.838469 139615906252544 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.35612571239471436, loss=1.7976971864700317
I0209 16:37:51.966140 139785736898368 spec.py:321] Evaluating on the training split.
I0209 16:37:54.960999 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:42:18.129889 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 16:42:20.843265 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:46:04.120759 139785736898368 spec.py:349] Evaluating on the test split.
I0209 16:46:06.839230 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 16:49:35.841437 139785736898368 submission_runner.py:408] Time since start: 52011.69s, 	Step: 89105, 	{'train/accuracy': 0.6479898691177368, 'train/loss': 1.7014189958572388, 'train/bleu': 31.870726073454623, 'validation/accuracy': 0.664678692817688, 'validation/loss': 1.591239333152771, 'validation/bleu': 27.893390280404773, 'validation/num_examples': 3000, 'test/accuracy': 0.676021158695221, 'test/loss': 1.5245511531829834, 'test/bleu': 27.724933282657265, 'test/num_examples': 3003, 'score': 31110.686940193176, 'total_duration': 52011.69216299057, 'accumulated_submission_time': 31110.686940193176, 'accumulated_eval_time': 20896.967987298965, 'accumulated_logging_time': 1.2072625160217285}
I0209 16:49:35.869965 139615914645248 logging_writer.py:48] [89105] accumulated_eval_time=20896.967987, accumulated_logging_time=1.207263, accumulated_submission_time=31110.686940, global_step=89105, preemption_count=0, score=31110.686940, test/accuracy=0.676021, test/bleu=27.724933, test/loss=1.524551, test/num_examples=3003, total_duration=52011.692163, train/accuracy=0.647990, train/bleu=31.870726, train/loss=1.701419, validation/accuracy=0.664679, validation/bleu=27.893390, validation/loss=1.591239, validation/num_examples=3000
I0209 16:50:09.370386 139615906252544 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3075985908508301, loss=1.7941516637802124
I0209 16:50:44.308521 139615914645248 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.30240997672080994, loss=1.7404494285583496
I0209 16:51:19.217105 139615906252544 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.33974897861480713, loss=1.6232916116714478
I0209 16:51:54.118087 139615914645248 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.29717257618904114, loss=1.7171939611434937
I0209 16:52:29.014234 139615906252544 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.3128229081630707, loss=1.7446632385253906
I0209 16:53:03.910356 139615914645248 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3161841034889221, loss=1.683785080909729
I0209 16:53:38.797324 139615906252544 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.30506980419158936, loss=1.726546287536621
I0209 16:54:13.742414 139615914645248 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2878481149673462, loss=1.699341058731079
I0209 16:54:48.666882 139615906252544 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.3122054934501648, loss=1.7677268981933594
I0209 16:55:23.556075 139615914645248 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.2972700595855713, loss=1.7905278205871582
I0209 16:55:58.428473 139615906252544 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2793894112110138, loss=1.765207052230835
I0209 16:56:33.321403 139615914645248 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.36107301712036133, loss=1.725164771080017
I0209 16:57:08.230091 139615906252544 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.32163316011428833, loss=1.7751604318618774
I0209 16:57:43.109318 139615914645248 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3199959099292755, loss=1.7270935773849487
I0209 16:58:18.016737 139615906252544 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.30426517128944397, loss=1.7613272666931152
I0209 16:58:52.894844 139615914645248 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.31483209133148193, loss=1.6542991399765015
I0209 16:59:27.826440 139615906252544 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3565707504749298, loss=1.7273279428482056
I0209 17:00:02.724650 139615914645248 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.3393990397453308, loss=1.7399580478668213
I0209 17:00:37.606081 139615906252544 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4102436900138855, loss=1.775341272354126
I0209 17:01:12.473075 139615914645248 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.32828933000564575, loss=1.7258715629577637
I0209 17:01:47.393867 139615906252544 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.41491463780403137, loss=1.7177315950393677
I0209 17:02:22.306653 139615914645248 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.546656608581543, loss=1.7459675073623657
I0209 17:02:57.196365 139615906252544 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.31881049275398254, loss=1.672688603401184
I0209 17:03:32.144976 139615914645248 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.29491087794303894, loss=1.724599003791809
I0209 17:03:36.073278 139785736898368 spec.py:321] Evaluating on the training split.
I0209 17:03:39.069221 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:07:18.034628 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 17:07:20.735404 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:09:52.940363 139785736898368 spec.py:349] Evaluating on the test split.
I0209 17:09:55.660626 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:12:30.365675 139785736898368 submission_runner.py:408] Time since start: 53386.22s, 	Step: 91513, 	{'train/accuracy': 0.6436875462532043, 'train/loss': 1.7378581762313843, 'train/bleu': 31.34514009640573, 'validation/accuracy': 0.6646538972854614, 'validation/loss': 1.5829776525497437, 'validation/bleu': 28.456537818777825, 'validation/num_examples': 3000, 'test/accuracy': 0.6771948337554932, 'test/loss': 1.5109649896621704, 'test/bleu': 27.948103940076336, 'test/num_examples': 3003, 'score': 31950.802713632584, 'total_duration': 53386.21637392044, 'accumulated_submission_time': 31950.802713632584, 'accumulated_eval_time': 21431.260320663452, 'accumulated_logging_time': 1.246953010559082}
I0209 17:12:30.399543 139615906252544 logging_writer.py:48] [91513] accumulated_eval_time=21431.260321, accumulated_logging_time=1.246953, accumulated_submission_time=31950.802714, global_step=91513, preemption_count=0, score=31950.802714, test/accuracy=0.677195, test/bleu=27.948104, test/loss=1.510965, test/num_examples=3003, total_duration=53386.216374, train/accuracy=0.643688, train/bleu=31.345140, train/loss=1.737858, validation/accuracy=0.664654, validation/bleu=28.456538, validation/loss=1.582978, validation/num_examples=3000
I0209 17:13:01.115553 139615914645248 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.2884364128112793, loss=1.641022801399231
I0209 17:13:36.001691 139615906252544 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.32299676537513733, loss=1.747205138206482
I0209 17:14:10.908936 139615914645248 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.30299684405326843, loss=1.6753177642822266
I0209 17:14:45.797583 139615906252544 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.32565000653266907, loss=1.669595718383789
I0209 17:15:20.690663 139615914645248 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.309948593378067, loss=1.7789419889450073
I0209 17:15:55.693382 139615906252544 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3228956162929535, loss=1.7762584686279297
I0209 17:16:30.596343 139615914645248 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.303109735250473, loss=1.7025046348571777
I0209 17:17:05.467273 139615906252544 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.33076241612434387, loss=1.6870499849319458
I0209 17:17:40.361666 139615914645248 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.2970515787601471, loss=1.6584612131118774
I0209 17:18:15.249457 139615906252544 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3166404664516449, loss=1.7833127975463867
I0209 17:18:50.133883 139615914645248 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.30914145708084106, loss=1.8079015016555786
I0209 17:19:25.014544 139615906252544 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.37600988149642944, loss=1.7509058713912964
I0209 17:19:59.895780 139615914645248 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.32020828127861023, loss=1.700539231300354
I0209 17:20:34.777513 139615906252544 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.30209845304489136, loss=1.6626578569412231
I0209 17:21:09.672549 139615914645248 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.31336140632629395, loss=1.7391185760498047
I0209 17:21:44.613314 139615906252544 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3073403239250183, loss=1.7235362529754639
I0209 17:22:19.516027 139615914645248 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.31837180256843567, loss=1.7801969051361084
I0209 17:22:54.394214 139615906252544 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.28641241788864136, loss=1.7064141035079956
I0209 17:23:29.300218 139615914645248 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3330988585948944, loss=1.6987800598144531
I0209 17:24:04.190029 139615906252544 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3130238652229309, loss=1.6801763772964478
I0209 17:24:39.073449 139615914645248 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.33426254987716675, loss=1.7488304376602173
I0209 17:25:13.947630 139615906252544 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.2883627712726593, loss=1.7095617055892944
I0209 17:25:48.814999 139615914645248 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3057231605052948, loss=1.604867935180664
I0209 17:26:23.736200 139615906252544 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3189004957675934, loss=1.699238657951355
I0209 17:26:30.458720 139785736898368 spec.py:321] Evaluating on the training split.
I0209 17:26:33.466309 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:30:15.873701 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 17:30:18.586869 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:33:09.576074 139785736898368 spec.py:349] Evaluating on the test split.
I0209 17:33:12.275982 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:35:47.599928 139785736898368 submission_runner.py:408] Time since start: 54783.45s, 	Step: 93921, 	{'train/accuracy': 0.6853712797164917, 'train/loss': 1.4687334299087524, 'train/bleu': 34.03844929654699, 'validation/accuracy': 0.6677164435386658, 'validation/loss': 1.5667170286178589, 'validation/bleu': 28.313632988196403, 'validation/num_examples': 3000, 'test/accuracy': 0.6811574101448059, 'test/loss': 1.4931126832962036, 'test/bleu': 28.463327942895543, 'test/num_examples': 3003, 'score': 32790.77405810356, 'total_duration': 54783.45065832138, 'accumulated_submission_time': 32790.77405810356, 'accumulated_eval_time': 21988.401491642, 'accumulated_logging_time': 1.2917671203613281}
I0209 17:35:47.628735 139615914645248 logging_writer.py:48] [93921] accumulated_eval_time=21988.401492, accumulated_logging_time=1.291767, accumulated_submission_time=32790.774058, global_step=93921, preemption_count=0, score=32790.774058, test/accuracy=0.681157, test/bleu=28.463328, test/loss=1.493113, test/num_examples=3003, total_duration=54783.450658, train/accuracy=0.685371, train/bleu=34.038449, train/loss=1.468733, validation/accuracy=0.667716, validation/bleu=28.313633, validation/loss=1.566717, validation/num_examples=3000
I0209 17:36:15.518902 139615906252544 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3164754807949066, loss=1.704452633857727
I0209 17:36:50.430036 139615914645248 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2929946184158325, loss=1.6891043186187744
I0209 17:37:25.349318 139615906252544 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3410714566707611, loss=1.7797781229019165
I0209 17:38:00.265262 139615914645248 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.3160337507724762, loss=1.7383127212524414
I0209 17:38:35.159324 139615906252544 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3099660873413086, loss=1.7819126844406128
I0209 17:39:10.068805 139615914645248 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.36766812205314636, loss=1.712523102760315
I0209 17:39:44.960327 139615906252544 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3461483120918274, loss=1.6990253925323486
I0209 17:40:19.871474 139615914645248 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.33728668093681335, loss=1.6782784461975098
I0209 17:40:54.755407 139615906252544 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3332480490207672, loss=1.6290736198425293
I0209 17:41:29.732029 139615914645248 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.3245709240436554, loss=1.676230788230896
I0209 17:42:04.669541 139615906252544 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3202475905418396, loss=1.7526675462722778
I0209 17:42:39.535369 139615914645248 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3137340545654297, loss=1.7012730836868286
I0209 17:43:14.469523 139615906252544 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3129390776157379, loss=1.6637766361236572
I0209 17:43:49.364824 139615914645248 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3570937216281891, loss=1.6892282962799072
I0209 17:44:24.276603 139615906252544 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3142467141151428, loss=1.6227291822433472
I0209 17:44:59.183184 139615914645248 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3184055984020233, loss=1.6955451965332031
I0209 17:45:34.067102 139615906252544 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.29378384351730347, loss=1.7211494445800781
I0209 17:46:08.924871 139615914645248 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3346055746078491, loss=1.7936203479766846
I0209 17:46:43.801962 139615906252544 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.35769742727279663, loss=1.7529244422912598
I0209 17:47:18.664527 139615914645248 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3620454967021942, loss=1.7770525217056274
I0209 17:47:53.549997 139615906252544 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.8366702795028687, loss=1.6560078859329224
I0209 17:48:28.429967 139615914645248 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3212239444255829, loss=1.7604482173919678
I0209 17:49:03.326327 139615906252544 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.3481229841709137, loss=1.6681618690490723
I0209 17:49:38.196654 139615914645248 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.7661077976226807, loss=1.7314696311950684
I0209 17:49:47.700972 139785736898368 spec.py:321] Evaluating on the training split.
I0209 17:49:50.704013 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:54:18.290859 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 17:54:20.991458 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 17:57:11.856253 139785736898368 spec.py:349] Evaluating on the test split.
I0209 17:57:14.579402 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:00:16.629650 139785736898368 submission_runner.py:408] Time since start: 56252.48s, 	Step: 96329, 	{'train/accuracy': 0.6571729779243469, 'train/loss': 1.6591682434082031, 'train/bleu': 31.706807133045615, 'validation/accuracy': 0.6698614954948425, 'validation/loss': 1.549560785293579, 'validation/bleu': 28.72916283015469, 'validation/num_examples': 3000, 'test/accuracy': 0.6832374930381775, 'test/loss': 1.4707233905792236, 'test/bleu': 28.521573655414183, 'test/num_examples': 3003, 'score': 33630.75802206993, 'total_duration': 56252.48037528992, 'accumulated_submission_time': 33630.75802206993, 'accumulated_eval_time': 22617.3301281929, 'accumulated_logging_time': 1.331843376159668}
I0209 18:00:16.658455 139615906252544 logging_writer.py:48] [96329] accumulated_eval_time=22617.330128, accumulated_logging_time=1.331843, accumulated_submission_time=33630.758022, global_step=96329, preemption_count=0, score=33630.758022, test/accuracy=0.683237, test/bleu=28.521574, test/loss=1.470723, test/num_examples=3003, total_duration=56252.480375, train/accuracy=0.657173, train/bleu=31.706807, train/loss=1.659168, validation/accuracy=0.669861, validation/bleu=28.729163, validation/loss=1.549561, validation/num_examples=3000
I0209 18:00:41.753119 139615914645248 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3390437364578247, loss=1.7740187644958496
I0209 18:01:16.620864 139615906252544 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.32073137164115906, loss=1.694990873336792
I0209 18:01:51.495919 139615914645248 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.2790844440460205, loss=1.6554642915725708
I0209 18:02:26.371922 139615906252544 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3183094263076782, loss=1.671673059463501
I0209 18:03:01.263968 139615914645248 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.305598646402359, loss=1.6765459775924683
I0209 18:03:36.149968 139615906252544 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3172582983970642, loss=1.6848039627075195
I0209 18:04:11.089167 139615914645248 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3048435151576996, loss=1.7222716808319092
I0209 18:04:45.977727 139615906252544 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.37575504183769226, loss=1.5801301002502441
I0209 18:05:20.878483 139615914645248 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.36581820249557495, loss=1.718004822731018
I0209 18:05:55.780920 139615906252544 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3496313691139221, loss=1.715150237083435
I0209 18:06:30.705919 139615914645248 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.289728045463562, loss=1.7036672830581665
I0209 18:07:05.611418 139615906252544 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3869533836841583, loss=1.650525450706482
I0209 18:07:40.571745 139615914645248 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3069487512111664, loss=1.630610704421997
I0209 18:08:15.568339 139615906252544 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.28969284892082214, loss=1.6717309951782227
I0209 18:08:50.456302 139615914645248 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.29900386929512024, loss=1.612473726272583
I0209 18:09:25.333398 139615906252544 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3130173981189728, loss=1.6604275703430176
I0209 18:10:00.208740 139615914645248 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.31163841485977173, loss=1.6146794557571411
I0209 18:10:35.082215 139615906252544 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3155660033226013, loss=1.5761678218841553
I0209 18:11:09.933438 139615914645248 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.32563135027885437, loss=1.6294798851013184
I0209 18:11:44.785595 139615906252544 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.36789199709892273, loss=1.7333219051361084
I0209 18:12:19.657319 139615914645248 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.35407519340515137, loss=1.5872422456741333
I0209 18:12:54.534836 139615906252544 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3093506991863251, loss=1.6640877723693848
I0209 18:13:29.434626 139615914645248 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.31316035985946655, loss=1.6552612781524658
I0209 18:14:04.343113 139615906252544 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.31901031732559204, loss=1.6835163831710815
I0209 18:14:16.631127 139785736898368 spec.py:321] Evaluating on the training split.
I0209 18:14:19.626656 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:17:57.503767 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 18:18:00.222031 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:20:39.621930 139785736898368 spec.py:349] Evaluating on the test split.
I0209 18:20:42.331493 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:23:07.404389 139785736898368 submission_runner.py:408] Time since start: 57623.26s, 	Step: 98737, 	{'train/accuracy': 0.6514151692390442, 'train/loss': 1.690314531326294, 'train/bleu': 31.84842778417848, 'validation/accuracy': 0.6744739413261414, 'validation/loss': 1.5323798656463623, 'validation/bleu': 29.34685369574843, 'validation/num_examples': 3000, 'test/accuracy': 0.6842019557952881, 'test/loss': 1.4612979888916016, 'test/bleu': 28.679556840476543, 'test/num_examples': 3003, 'score': 34470.64178228378, 'total_duration': 57623.255120038986, 'accumulated_submission_time': 34470.64178228378, 'accumulated_eval_time': 23148.10334300995, 'accumulated_logging_time': 1.372218370437622}
I0209 18:23:07.434519 139615914645248 logging_writer.py:48] [98737] accumulated_eval_time=23148.103343, accumulated_logging_time=1.372218, accumulated_submission_time=34470.641782, global_step=98737, preemption_count=0, score=34470.641782, test/accuracy=0.684202, test/bleu=28.679557, test/loss=1.461298, test/num_examples=3003, total_duration=57623.255120, train/accuracy=0.651415, train/bleu=31.848428, train/loss=1.690315, validation/accuracy=0.674474, validation/bleu=29.346854, validation/loss=1.532380, validation/num_examples=3000
I0209 18:23:29.769407 139615906252544 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.31073376536369324, loss=1.6592559814453125
I0209 18:24:04.661408 139615914645248 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.32821714878082275, loss=1.6649196147918701
I0209 18:24:39.590148 139615906252544 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3747412860393524, loss=1.7170917987823486
I0209 18:25:14.489114 139615914645248 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.30649104714393616, loss=1.6095744371414185
I0209 18:25:49.380488 139615906252544 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3233281970024109, loss=1.627180576324463
I0209 18:26:24.263488 139615914645248 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.33553388714790344, loss=1.7003271579742432
I0209 18:26:59.179882 139615906252544 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.30999860167503357, loss=1.6525185108184814
I0209 18:27:34.122426 139615914645248 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.31840112805366516, loss=1.681768774986267
I0209 18:28:09.073869 139615906252544 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.2950369119644165, loss=1.5821518898010254
I0209 18:28:43.967492 139615914645248 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.31155121326446533, loss=1.682409405708313
I0209 18:29:18.866177 139615906252544 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.30427154898643494, loss=1.6461012363433838
I0209 18:29:53.764979 139615914645248 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.37664908170700073, loss=1.6597012281417847
I0209 18:30:28.651862 139615906252544 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3357968330383301, loss=1.6583608388900757
I0209 18:31:03.524449 139615914645248 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.345400869846344, loss=1.7453904151916504
I0209 18:31:38.412398 139615906252544 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.34473493695259094, loss=1.6026471853256226
I0209 18:32:13.286427 139615914645248 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.30427059531211853, loss=1.6527748107910156
I0209 18:32:48.148781 139615906252544 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3072090446949005, loss=1.6967238187789917
I0209 18:33:23.030680 139615914645248 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.31533733010292053, loss=1.6610487699508667
I0209 18:33:57.928891 139615906252544 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3432008624076843, loss=1.6413145065307617
I0209 18:34:32.830236 139615914645248 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.34341567754745483, loss=1.5321345329284668
I0209 18:35:07.728787 139615906252544 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.3358575403690338, loss=1.677138090133667
I0209 18:35:42.621325 139615914645248 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.3444700241088867, loss=1.6960208415985107
I0209 18:36:17.544175 139615906252544 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.290998250246048, loss=1.5746674537658691
I0209 18:36:52.463955 139615914645248 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.32225799560546875, loss=1.646663784980774
I0209 18:37:07.539023 139785736898368 spec.py:321] Evaluating on the training split.
I0209 18:37:10.537837 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:40:42.102399 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 18:40:44.824991 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:43:30.512372 139785736898368 spec.py:349] Evaluating on the test split.
I0209 18:43:33.227797 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 18:46:17.188305 139785736898368 submission_runner.py:408] Time since start: 59013.04s, 	Step: 101145, 	{'train/accuracy': 0.6625736951828003, 'train/loss': 1.604433298110962, 'train/bleu': 32.59588528405028, 'validation/accuracy': 0.6739407777786255, 'validation/loss': 1.5239664316177368, 'validation/bleu': 29.29180447115428, 'validation/num_examples': 3000, 'test/accuracy': 0.688176155090332, 'test/loss': 1.4475120306015015, 'test/bleu': 29.02306537809828, 'test/num_examples': 3003, 'score': 35310.656386613846, 'total_duration': 59013.03903198242, 'accumulated_submission_time': 35310.656386613846, 'accumulated_eval_time': 23697.752576589584, 'accumulated_logging_time': 1.4147746562957764}
I0209 18:46:17.217299 139615906252544 logging_writer.py:48] [101145] accumulated_eval_time=23697.752577, accumulated_logging_time=1.414775, accumulated_submission_time=35310.656387, global_step=101145, preemption_count=0, score=35310.656387, test/accuracy=0.688176, test/bleu=29.023065, test/loss=1.447512, test/num_examples=3003, total_duration=59013.039032, train/accuracy=0.662574, train/bleu=32.595885, train/loss=1.604433, validation/accuracy=0.673941, validation/bleu=29.291804, validation/loss=1.523966, validation/num_examples=3000
I0209 18:46:36.770284 139615914645248 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.33311140537261963, loss=1.5563857555389404
I0209 18:47:11.657047 139615906252544 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.32733988761901855, loss=1.6201356649398804
I0209 18:47:46.607962 139615914645248 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.325368732213974, loss=1.6187398433685303
I0209 18:48:21.521514 139615906252544 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3635433614253998, loss=1.694685935974121
I0209 18:48:56.477802 139615914645248 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.32812410593032837, loss=1.6859103441238403
I0209 18:49:31.412813 139615906252544 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.32450708746910095, loss=1.627089262008667
I0209 18:50:06.324668 139615914645248 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.35377922654151917, loss=1.6059759855270386
I0209 18:50:41.213980 139615906252544 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3266051411628723, loss=1.6800436973571777
I0209 18:51:16.109313 139615914645248 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.2950971722602844, loss=1.6262246370315552
I0209 18:51:51.003235 139615906252544 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2951468229293823, loss=1.5867515802383423
I0209 18:52:25.883463 139615914645248 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3175780177116394, loss=1.708916187286377
I0209 18:53:00.774000 139615906252544 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3121682405471802, loss=1.6782782077789307
I0209 18:53:35.655337 139615914645248 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.364013135433197, loss=1.5964093208312988
I0209 18:54:10.547832 139615906252544 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.28452032804489136, loss=1.5846832990646362
I0209 18:54:45.426871 139615914645248 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.32551416754722595, loss=1.5984203815460205
I0209 18:55:20.325572 139615906252544 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.3508540391921997, loss=1.623721718788147
I0209 18:55:55.172143 139615914645248 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.32238274812698364, loss=1.6087533235549927
I0209 18:56:30.056020 139615906252544 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.33500781655311584, loss=1.7130954265594482
I0209 18:57:04.965321 139615914645248 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2976475954055786, loss=1.6605371236801147
I0209 18:57:39.848548 139615906252544 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.34547290205955505, loss=1.6689003705978394
I0209 18:58:14.782587 139615914645248 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3229946196079254, loss=1.568336009979248
I0209 18:58:49.690428 139615906252544 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.31895849108695984, loss=1.6697860956192017
I0209 18:59:24.603312 139615914645248 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.3616255223751068, loss=1.7099089622497559
I0209 18:59:59.499608 139615906252544 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3079661428928375, loss=1.5345089435577393
I0209 19:00:17.373595 139785736898368 spec.py:321] Evaluating on the training split.
I0209 19:00:20.380845 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:04:19.514966 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 19:04:22.238285 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:07:33.706984 139785736898368 spec.py:349] Evaluating on the test split.
I0209 19:07:36.418311 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:11:17.831284 139785736898368 submission_runner.py:408] Time since start: 60513.68s, 	Step: 103553, 	{'train/accuracy': 0.6571160554885864, 'train/loss': 1.6496434211730957, 'train/bleu': 32.57483756565918, 'validation/accuracy': 0.6794335842132568, 'validation/loss': 1.5055320262908936, 'validation/bleu': 29.719286400010557, 'validation/num_examples': 3000, 'test/accuracy': 0.6907559037208557, 'test/loss': 1.4269720315933228, 'test/bleu': 29.401809938841794, 'test/num_examples': 3003, 'score': 36150.72316431999, 'total_duration': 60513.682002067566, 'accumulated_submission_time': 36150.72316431999, 'accumulated_eval_time': 24358.2102060318, 'accumulated_logging_time': 1.4556384086608887}
I0209 19:11:17.860143 139615914645248 logging_writer.py:48] [103553] accumulated_eval_time=24358.210206, accumulated_logging_time=1.455638, accumulated_submission_time=36150.723164, global_step=103553, preemption_count=0, score=36150.723164, test/accuracy=0.690756, test/bleu=29.401810, test/loss=1.426972, test/num_examples=3003, total_duration=60513.682002, train/accuracy=0.657116, train/bleu=32.574838, train/loss=1.649643, validation/accuracy=0.679434, validation/bleu=29.719286, validation/loss=1.505532, validation/num_examples=3000
I0209 19:11:34.575162 139615906252544 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.31492429971694946, loss=1.6141685247421265
I0209 19:12:09.439469 139615914645248 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.31629106402397156, loss=1.596911072731018
I0209 19:12:44.323643 139615906252544 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.32318615913391113, loss=1.689489483833313
I0209 19:13:19.203702 139615914645248 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.37949442863464355, loss=1.5720100402832031
I0209 19:13:54.091113 139615906252544 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.30400824546813965, loss=1.6351722478866577
I0209 19:14:28.978812 139615914645248 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.33913204073905945, loss=1.6132994890213013
I0209 19:15:03.907375 139615906252544 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.32064855098724365, loss=1.7351394891738892
I0209 19:15:38.751707 139615914645248 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3250120282173157, loss=1.5416072607040405
I0209 19:16:13.663325 139615906252544 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.3076193928718567, loss=1.634662389755249
I0209 19:16:48.531099 139615914645248 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3185606002807617, loss=1.633323073387146
I0209 19:17:23.404923 139615906252544 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.31721216440200806, loss=1.5793964862823486
I0209 19:17:58.274586 139615914645248 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.3151167035102844, loss=1.6082465648651123
I0209 19:18:33.182586 139615906252544 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.31401321291923523, loss=1.551035761833191
I0209 19:19:08.094351 139615914645248 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.3722083568572998, loss=1.6212656497955322
I0209 19:19:43.030495 139615906252544 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.34393906593322754, loss=1.6204156875610352
I0209 19:20:17.934161 139615914645248 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.34933024644851685, loss=1.67611825466156
I0209 19:20:52.791486 139615906252544 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.31244906783103943, loss=1.582027792930603
I0209 19:21:27.674991 139615914645248 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3322620093822479, loss=1.6677706241607666
I0209 19:22:02.593727 139615906252544 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3152700960636139, loss=1.5653859376907349
I0209 19:22:37.502276 139615914645248 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.33642256259918213, loss=1.6416569948196411
I0209 19:23:12.389916 139615906252544 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.33241209387779236, loss=1.6225311756134033
I0209 19:23:47.277537 139615914645248 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.38148027658462524, loss=1.6105965375900269
I0209 19:24:22.147050 139615906252544 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.33117738366127014, loss=1.6637389659881592
I0209 19:24:57.057909 139615914645248 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.31481707096099854, loss=1.5605111122131348
I0209 19:25:18.082591 139785736898368 spec.py:321] Evaluating on the training split.
I0209 19:25:21.095971 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:29:10.074383 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 19:29:12.785871 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:32:15.546159 139785736898368 spec.py:349] Evaluating on the test split.
I0209 19:32:18.268884 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:35:09.760717 139785736898368 submission_runner.py:408] Time since start: 61945.61s, 	Step: 105962, 	{'train/accuracy': 0.6626680493354797, 'train/loss': 1.6218568086624146, 'train/bleu': 32.79830573550928, 'validation/accuracy': 0.6800907254219055, 'validation/loss': 1.4952389001846313, 'validation/bleu': 29.620485713663335, 'validation/num_examples': 3000, 'test/accuracy': 0.6929289698600769, 'test/loss': 1.4130771160125732, 'test/bleu': 29.46607487977802, 'test/num_examples': 3003, 'score': 36990.85962986946, 'total_duration': 61945.61144256592, 'accumulated_submission_time': 36990.85962986946, 'accumulated_eval_time': 24949.888291597366, 'accumulated_logging_time': 1.4942302703857422}
I0209 19:35:09.790288 139615906252544 logging_writer.py:48] [105962] accumulated_eval_time=24949.888292, accumulated_logging_time=1.494230, accumulated_submission_time=36990.859630, global_step=105962, preemption_count=0, score=36990.859630, test/accuracy=0.692929, test/bleu=29.466075, test/loss=1.413077, test/num_examples=3003, total_duration=61945.611443, train/accuracy=0.662668, train/bleu=32.798306, train/loss=1.621857, validation/accuracy=0.680091, validation/bleu=29.620486, validation/loss=1.495239, validation/num_examples=3000
I0209 19:35:23.408497 139615914645248 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.43429577350616455, loss=1.645612120628357
I0209 19:35:58.315457 139615906252544 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.36632227897644043, loss=1.5840016603469849
I0209 19:36:33.202460 139615914645248 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.312724232673645, loss=1.5416103601455688
I0209 19:37:08.129095 139615906252544 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3311218023300171, loss=1.5378888845443726
I0209 19:37:43.089341 139615914645248 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.31925711035728455, loss=1.5299493074417114
I0209 19:38:18.042271 139615906252544 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3251846432685852, loss=1.4972647428512573
I0209 19:38:52.946380 139615914645248 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.32671090960502625, loss=1.629770040512085
I0209 19:39:27.820518 139615906252544 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3391285538673401, loss=1.5941274166107178
I0209 19:40:02.707295 139615914645248 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.3319774866104126, loss=1.597934603691101
I0209 19:40:37.599601 139615906252544 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.35877156257629395, loss=1.6266179084777832
I0209 19:41:12.527098 139615914645248 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.35641199350357056, loss=1.5819085836410522
I0209 19:41:47.398259 139615906252544 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.3185972273349762, loss=1.5832773447036743
I0209 19:42:22.287517 139615914645248 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.31779026985168457, loss=1.6591596603393555
I0209 19:42:57.154826 139615906252544 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3156597912311554, loss=1.5174199342727661
I0209 19:43:32.034922 139615914645248 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3162640333175659, loss=1.5971274375915527
I0209 19:44:06.951765 139615906252544 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3219244182109833, loss=1.6135303974151611
I0209 19:44:41.964694 139615914645248 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.29593414068222046, loss=1.5792244672775269
I0209 19:45:16.899923 139615906252544 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.32799994945526123, loss=1.533704161643982
I0209 19:45:51.875829 139615914645248 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.336376816034317, loss=1.6499285697937012
I0209 19:46:26.751447 139615906252544 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3298928737640381, loss=1.6527760028839111
I0209 19:47:01.638204 139615914645248 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.34132587909698486, loss=1.6143081188201904
I0209 19:47:36.555988 139615906252544 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3052445650100708, loss=1.6230759620666504
I0209 19:48:11.439370 139615914645248 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.32897713780403137, loss=1.5100772380828857
I0209 19:48:46.320952 139615906252544 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3327175974845886, loss=1.5305241346359253
I0209 19:49:10.108975 139785736898368 spec.py:321] Evaluating on the training split.
I0209 19:49:13.117089 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:53:23.740419 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 19:53:26.449391 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:56:05.736089 139785736898368 spec.py:349] Evaluating on the test split.
I0209 19:56:08.443730 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 19:58:34.339804 139785736898368 submission_runner.py:408] Time since start: 63350.19s, 	Step: 108370, 	{'train/accuracy': 0.6737005710601807, 'train/loss': 1.5475784540176392, 'train/bleu': 33.575351495616545, 'validation/accuracy': 0.6833640933036804, 'validation/loss': 1.479899525642395, 'validation/bleu': 29.811560649958405, 'validation/num_examples': 3000, 'test/accuracy': 0.6960200071334839, 'test/loss': 1.3971692323684692, 'test/bleu': 29.64580006144558, 'test/num_examples': 3003, 'score': 37831.08743548393, 'total_duration': 63350.190529346466, 'accumulated_submission_time': 37831.08743548393, 'accumulated_eval_time': 25514.119074106216, 'accumulated_logging_time': 1.5336592197418213}
I0209 19:58:34.370770 139615914645248 logging_writer.py:48] [108370] accumulated_eval_time=25514.119074, accumulated_logging_time=1.533659, accumulated_submission_time=37831.087435, global_step=108370, preemption_count=0, score=37831.087435, test/accuracy=0.696020, test/bleu=29.645800, test/loss=1.397169, test/num_examples=3003, total_duration=63350.190529, train/accuracy=0.673701, train/bleu=33.575351, train/loss=1.547578, validation/accuracy=0.683364, validation/bleu=29.811561, validation/loss=1.479900, validation/num_examples=3000
I0209 19:58:45.148676 139615906252544 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3255405128002167, loss=1.5799119472503662
I0209 19:59:19.982618 139615914645248 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3487222194671631, loss=1.609391212463379
I0209 19:59:54.857898 139615906252544 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3271911144256592, loss=1.6156222820281982
I0209 20:00:29.657951 139615914645248 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3421449065208435, loss=1.66775381565094
I0209 20:01:04.510521 139615906252544 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3369351029396057, loss=1.5747547149658203
I0209 20:01:39.323410 139615914645248 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.33249592781066895, loss=1.6262222528457642
I0209 20:02:14.187145 139615906252544 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3528580069541931, loss=1.5570473670959473
I0209 20:02:49.089142 139615914645248 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3405502438545227, loss=1.5204428434371948
I0209 20:03:24.065355 139615906252544 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.35801419615745544, loss=1.5861313343048096
I0209 20:03:58.925643 139615914645248 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3441382646560669, loss=1.5689316987991333
I0209 20:04:33.805445 139615906252544 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3261502981185913, loss=1.625546932220459
I0209 20:05:08.665245 139615914645248 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.36525455117225647, loss=1.6281529664993286
I0209 20:05:43.634076 139615906252544 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.33467885851860046, loss=1.5338847637176514
I0209 20:06:18.483465 139615914645248 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3329031467437744, loss=1.4903802871704102
I0209 20:06:53.346717 139615906252544 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.3507525622844696, loss=1.6015419960021973
I0209 20:07:28.172485 139615914645248 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.34759870171546936, loss=1.6566535234451294
I0209 20:08:03.066372 139615906252544 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.33084824681282043, loss=1.6305205821990967
I0209 20:08:37.879040 139615914645248 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.32448068261146545, loss=1.5438450574874878
I0209 20:09:12.720275 139615906252544 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3878336250782013, loss=1.6005427837371826
I0209 20:09:47.565317 139615914645248 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.3379645049571991, loss=1.5429672002792358
I0209 20:10:22.437304 139615906252544 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3901987671852112, loss=1.58111572265625
I0209 20:10:57.257181 139615914645248 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3519662916660309, loss=1.6048601865768433
I0209 20:11:32.108143 139615906252544 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3641669750213623, loss=1.5864211320877075
I0209 20:12:06.981006 139615914645248 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.3429557979106903, loss=1.6025781631469727
I0209 20:12:34.597373 139785736898368 spec.py:321] Evaluating on the training split.
I0209 20:12:37.594599 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 20:16:56.954458 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 20:16:59.673589 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 20:19:36.272349 139785736898368 spec.py:349] Evaluating on the test split.
I0209 20:19:39.010010 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 20:22:15.048807 139785736898368 submission_runner.py:408] Time since start: 64770.90s, 	Step: 110781, 	{'train/accuracy': 0.6674370765686035, 'train/loss': 1.5858745574951172, 'train/bleu': 33.23845615206641, 'validation/accuracy': 0.6833516955375671, 'validation/loss': 1.4708929061889648, 'validation/bleu': 30.19010167590585, 'validation/num_examples': 3000, 'test/accuracy': 0.6975306868553162, 'test/loss': 1.3853771686553955, 'test/bleu': 29.50214132674396, 'test/num_examples': 3003, 'score': 38671.22283697128, 'total_duration': 64770.899523973465, 'accumulated_submission_time': 38671.22283697128, 'accumulated_eval_time': 26094.5704498291, 'accumulated_logging_time': 1.57651948928833}
I0209 20:22:15.078836 139615906252544 logging_writer.py:48] [110781] accumulated_eval_time=26094.570450, accumulated_logging_time=1.576519, accumulated_submission_time=38671.222837, global_step=110781, preemption_count=0, score=38671.222837, test/accuracy=0.697531, test/bleu=29.502141, test/loss=1.385377, test/num_examples=3003, total_duration=64770.899524, train/accuracy=0.667437, train/bleu=33.238456, train/loss=1.585875, validation/accuracy=0.683352, validation/bleu=30.190102, validation/loss=1.470893, validation/num_examples=3000
I0209 20:22:22.076785 139615914645248 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.34080207347869873, loss=1.5636695623397827
I0209 20:22:56.963146 139615906252544 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3425331711769104, loss=1.5500881671905518
I0209 20:23:31.897116 139615914645248 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.34293028712272644, loss=1.5785130262374878
I0209 20:24:06.790306 139615906252544 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.34876132011413574, loss=1.5468008518218994
I0209 20:24:41.674689 139615914645248 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.34075772762298584, loss=1.5375823974609375
I0209 20:25:16.541826 139615906252544 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.3272280693054199, loss=1.5005207061767578
I0209 20:25:51.448180 139615914645248 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.34034013748168945, loss=1.5914682149887085
I0209 20:26:26.381207 139615906252544 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.33860817551612854, loss=1.5786164999008179
I0209 20:27:01.313726 139615914645248 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.33687809109687805, loss=1.588418960571289
I0209 20:27:36.215789 139615906252544 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3310166299343109, loss=1.4541821479797363
I0209 20:28:11.091778 139615914645248 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.3442566394805908, loss=1.6252613067626953
I0209 20:28:45.993754 139615906252544 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3430054485797882, loss=1.620284080505371
I0209 20:29:20.903993 139615914645248 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.33455145359039307, loss=1.5716599225997925
I0209 20:29:55.793989 139615906252544 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.35190004110336304, loss=1.5551382303237915
I0209 20:30:30.720772 139615914645248 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.34154045581817627, loss=1.6273775100708008
I0209 20:31:05.606587 139615906252544 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.330104261636734, loss=1.5654222965240479
I0209 20:31:40.510992 139615914645248 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3388621509075165, loss=1.473821997642517
I0209 20:32:15.366049 139615906252544 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3570415675640106, loss=1.6004970073699951
I0209 20:32:50.241967 139615914645248 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3515411615371704, loss=1.4793187379837036
I0209 20:33:25.271635 139615906252544 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.34672632813453674, loss=1.5576341152191162
I0209 20:34:00.276101 139615914645248 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.349989652633667, loss=1.540412425994873
I0209 20:34:35.197487 139615906252544 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.35390034317970276, loss=1.5340107679367065
I0209 20:35:10.114894 139615914645248 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.37074217200279236, loss=1.5819967985153198
I0209 20:35:45.054940 139615906252544 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3622719645500183, loss=1.573144555091858
I0209 20:36:15.136508 139785736898368 spec.py:321] Evaluating on the training split.
I0209 20:36:18.141918 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 20:39:48.323077 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 20:39:51.029706 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 20:43:07.985083 139785736898368 spec.py:349] Evaluating on the test split.
I0209 20:43:10.698618 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 20:46:37.258721 139785736898368 submission_runner.py:408] Time since start: 66233.11s, 	Step: 113188, 	{'train/accuracy': 0.686324417591095, 'train/loss': 1.4715728759765625, 'train/bleu': 34.47618891337282, 'validation/accuracy': 0.6864266991615295, 'validation/loss': 1.4545800685882568, 'validation/bleu': 29.93864461495176, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3694384098052979, 'test/bleu': 29.955883103222526, 'test/num_examples': 3003, 'score': 39511.19406795502, 'total_duration': 66233.1094198227, 'accumulated_submission_time': 39511.19406795502, 'accumulated_eval_time': 26716.692593574524, 'accumulated_logging_time': 1.61653470993042}
I0209 20:46:37.295493 139615914645248 logging_writer.py:48] [113188] accumulated_eval_time=26716.692594, accumulated_logging_time=1.616535, accumulated_submission_time=39511.194068, global_step=113188, preemption_count=0, score=39511.194068, test/accuracy=0.700203, test/bleu=29.955883, test/loss=1.369438, test/num_examples=3003, total_duration=66233.109420, train/accuracy=0.686324, train/bleu=34.476189, train/loss=1.471573, validation/accuracy=0.686427, validation/bleu=29.938645, validation/loss=1.454580, validation/num_examples=3000
I0209 20:46:41.851669 139615906252544 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.356791228055954, loss=1.5244789123535156
I0209 20:47:16.756429 139615914645248 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.35937002301216125, loss=1.562092661857605
I0209 20:47:51.664428 139615906252544 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3475303053855896, loss=1.6628419160842896
I0209 20:48:26.554726 139615914645248 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.33855751156806946, loss=1.4686427116394043
I0209 20:49:01.416787 139615906252544 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.3943462669849396, loss=1.524553894996643
I0209 20:49:36.306120 139615914645248 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.3724152743816376, loss=1.6345255374908447
I0209 20:50:11.255738 139615906252544 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3492220640182495, loss=1.506600260734558
I0209 20:50:46.168332 139615914645248 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.34773340821266174, loss=1.647310495376587
I0209 20:51:21.059210 139615906252544 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3528876304626465, loss=1.4483401775360107
I0209 20:51:55.975994 139615914645248 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.33892956376075745, loss=1.5118008852005005
I0209 20:52:30.862464 139615906252544 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.34991928935050964, loss=1.5560712814331055
I0209 20:53:05.760544 139615914645248 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3499336242675781, loss=1.5941542387008667
I0209 20:53:40.659226 139615906252544 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3613176643848419, loss=1.5014598369598389
I0209 20:54:15.590064 139615914645248 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3638363778591156, loss=1.5796327590942383
I0209 20:54:50.489573 139615906252544 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3561849594116211, loss=1.5092494487762451
I0209 20:55:25.369810 139615914645248 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3462749719619751, loss=1.5426286458969116
I0209 20:56:00.267482 139615906252544 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3727370798587799, loss=1.5711851119995117
I0209 20:56:35.160382 139615914645248 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.345825731754303, loss=1.5817785263061523
I0209 20:57:10.027018 139615906252544 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.37022414803504944, loss=1.5246787071228027
I0209 20:57:44.954799 139615914645248 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.32291415333747864, loss=1.4437408447265625
I0209 20:58:19.841705 139615906252544 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3524129092693329, loss=1.473650574684143
I0209 20:58:54.740364 139615914645248 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.34787991642951965, loss=1.5687782764434814
I0209 20:59:29.647725 139615906252544 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3546907305717468, loss=1.4365122318267822
I0209 21:00:04.572162 139615914645248 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.3547806441783905, loss=1.586564540863037
I0209 21:00:37.426933 139785736898368 spec.py:321] Evaluating on the training split.
I0209 21:00:40.427770 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:04:20.017978 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 21:04:22.728370 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:06:55.780508 139785736898368 spec.py:349] Evaluating on the test split.
I0209 21:06:58.486826 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:09:23.713321 139785736898368 submission_runner.py:408] Time since start: 67599.56s, 	Step: 115596, 	{'train/accuracy': 0.6802150011062622, 'train/loss': 1.505571722984314, 'train/bleu': 34.051214524478446, 'validation/accuracy': 0.6871086359024048, 'validation/loss': 1.4513825178146362, 'validation/bleu': 30.445699454155303, 'validation/num_examples': 3000, 'test/accuracy': 0.7025042176246643, 'test/loss': 1.358932375907898, 'test/bleu': 30.259197723267093, 'test/num_examples': 3003, 'score': 40351.23788332939, 'total_duration': 67599.56405115128, 'accumulated_submission_time': 40351.23788332939, 'accumulated_eval_time': 27242.97893857956, 'accumulated_logging_time': 1.6643104553222656}
I0209 21:09:23.744080 139615906252544 logging_writer.py:48] [115596] accumulated_eval_time=27242.978939, accumulated_logging_time=1.664310, accumulated_submission_time=40351.237883, global_step=115596, preemption_count=0, score=40351.237883, test/accuracy=0.702504, test/bleu=30.259198, test/loss=1.358932, test/num_examples=3003, total_duration=67599.564051, train/accuracy=0.680215, train/bleu=34.051215, train/loss=1.505572, validation/accuracy=0.687109, validation/bleu=30.445699, validation/loss=1.451383, validation/num_examples=3000
I0209 21:09:25.512465 139615914645248 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.37303102016448975, loss=1.5033732652664185
I0209 21:10:00.411082 139615906252544 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3449680507183075, loss=1.561628818511963
I0209 21:10:35.293125 139615914645248 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3733726441860199, loss=1.5699005126953125
I0209 21:11:10.225367 139615906252544 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.35214486718177795, loss=1.5591835975646973
I0209 21:11:45.108935 139615914645248 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.357833594083786, loss=1.5085952281951904
I0209 21:12:19.985800 139615906252544 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.354742169380188, loss=1.5212239027023315
I0209 21:12:54.863711 139615914645248 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3410068452358246, loss=1.4906831979751587
I0209 21:13:29.738124 139615906252544 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.33706313371658325, loss=1.5446535348892212
I0209 21:14:04.626904 139615914645248 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.3913896977901459, loss=1.5570039749145508
I0209 21:14:39.502715 139615906252544 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.365975558757782, loss=1.5357195138931274
I0209 21:15:14.393353 139615914645248 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.35984763503074646, loss=1.5021804571151733
I0209 21:15:49.284308 139615906252544 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.35144463181495667, loss=1.492170810699463
I0209 21:16:24.285835 139615914645248 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3592619299888611, loss=1.5468672513961792
I0209 21:16:59.174468 139615906252544 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.36942073702812195, loss=1.4888153076171875
I0209 21:17:34.075933 139615914645248 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3728024363517761, loss=1.4856640100479126
I0209 21:18:08.934954 139615906252544 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.3855747878551483, loss=1.5156761407852173
I0209 21:18:43.825420 139615914645248 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.34688425064086914, loss=1.4792896509170532
I0209 21:19:18.668679 139615906252544 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.3559894263744354, loss=1.4229042530059814
I0209 21:19:53.571085 139615914645248 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.35046765208244324, loss=1.471193552017212
I0209 21:20:28.467847 139615906252544 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.39219895005226135, loss=1.6079611778259277
I0209 21:21:03.356012 139615914645248 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.34754225611686707, loss=1.451683521270752
I0209 21:21:38.256293 139615906252544 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.35753360390663147, loss=1.505305528640747
I0209 21:22:13.136843 139615914645248 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3614845275878906, loss=1.4392112493515015
I0209 21:22:48.025685 139615906252544 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.36456298828125, loss=1.4889895915985107
I0209 21:23:22.905051 139615914645248 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.38956931233406067, loss=1.5121209621429443
I0209 21:23:24.027204 139785736898368 spec.py:321] Evaluating on the training split.
I0209 21:23:27.035881 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:26:41.435022 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 21:26:44.149998 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:29:08.059505 139785736898368 spec.py:349] Evaluating on the test split.
I0209 21:29:10.770583 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:31:27.635408 139785736898368 submission_runner.py:408] Time since start: 68923.49s, 	Step: 118005, 	{'train/accuracy': 0.680402934551239, 'train/loss': 1.5085806846618652, 'train/bleu': 34.025486843982236, 'validation/accuracy': 0.6894644498825073, 'validation/loss': 1.437278389930725, 'validation/bleu': 30.400086035153965, 'validation/num_examples': 3000, 'test/accuracy': 0.7049096822738647, 'test/loss': 1.342985987663269, 'test/bleu': 30.587941780081074, 'test/num_examples': 3003, 'score': 41191.43535208702, 'total_duration': 68923.48609733582, 'accumulated_submission_time': 41191.43535208702, 'accumulated_eval_time': 27726.58704996109, 'accumulated_logging_time': 1.705122947692871}
I0209 21:31:27.673691 139615906252544 logging_writer.py:48] [118005] accumulated_eval_time=27726.587050, accumulated_logging_time=1.705123, accumulated_submission_time=41191.435352, global_step=118005, preemption_count=0, score=41191.435352, test/accuracy=0.704910, test/bleu=30.587942, test/loss=1.342986, test/num_examples=3003, total_duration=68923.486097, train/accuracy=0.680403, train/bleu=34.025487, train/loss=1.508581, validation/accuracy=0.689464, validation/bleu=30.400086, validation/loss=1.437278, validation/num_examples=3000
I0209 21:32:01.203528 139615914645248 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.35948318243026733, loss=1.5433402061462402
I0209 21:32:36.129277 139615906252544 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.3741988241672516, loss=1.476040005683899
I0209 21:33:11.018722 139615914645248 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.35882988572120667, loss=1.4972326755523682
I0209 21:33:45.914157 139615906252544 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.35198095440864563, loss=1.499415397644043
I0209 21:34:20.831405 139615914645248 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.36544957756996155, loss=1.503705382347107
I0209 21:34:55.714062 139615906252544 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.4053548276424408, loss=1.5199321508407593
I0209 21:35:30.651788 139615914645248 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.38367849588394165, loss=1.590459942817688
I0209 21:36:05.562210 139615906252544 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.35360556840896606, loss=1.5287402868270874
I0209 21:36:40.481051 139615914645248 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.38276660442352295, loss=1.4801970720291138
I0209 21:37:15.357223 139615906252544 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.43004611134529114, loss=1.5353261232376099
I0209 21:37:50.227961 139615914645248 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3493684232234955, loss=1.4521657228469849
I0209 21:38:25.128463 139615906252544 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3676964342594147, loss=1.50397527217865
I0209 21:39:00.100625 139615914645248 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.36765632033348083, loss=1.4866241216659546
I0209 21:39:35.034042 139615906252544 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3517257571220398, loss=1.534104347229004
I0209 21:40:09.965875 139615914645248 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.3684517443180084, loss=1.4929193258285522
I0209 21:40:44.845359 139615906252544 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3658674359321594, loss=1.4208072423934937
I0209 21:41:19.738417 139615914645248 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.38477426767349243, loss=1.5750032663345337
I0209 21:41:54.675724 139615906252544 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.37550222873687744, loss=1.5254086256027222
I0209 21:42:29.580372 139615914645248 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.3405289947986603, loss=1.4508492946624756
I0209 21:43:04.493262 139615906252544 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3857262432575226, loss=1.4316365718841553
I0209 21:43:39.379544 139615914645248 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.38901975750923157, loss=1.4501935243606567
I0209 21:44:14.291066 139615906252544 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3762371242046356, loss=1.4940770864486694
I0209 21:44:49.162428 139615914645248 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3570935130119324, loss=1.4542044401168823
I0209 21:45:24.040708 139615906252544 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.3694525957107544, loss=1.4972522258758545
I0209 21:45:27.957527 139785736898368 spec.py:321] Evaluating on the training split.
I0209 21:45:30.955310 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:49:31.777265 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 21:49:34.506562 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:52:21.227247 139785736898368 spec.py:349] Evaluating on the test split.
I0209 21:52:23.955315 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 21:55:07.235844 139785736898368 submission_runner.py:408] Time since start: 70343.09s, 	Step: 120413, 	{'train/accuracy': 0.6904399991035461, 'train/loss': 1.4429056644439697, 'train/bleu': 35.09718083113306, 'validation/accuracy': 0.6901216506958008, 'validation/loss': 1.4297069311141968, 'validation/bleu': 30.587810108397143, 'validation/num_examples': 3000, 'test/accuracy': 0.7053279876708984, 'test/loss': 1.3383585214614868, 'test/bleu': 30.600486228597944, 'test/num_examples': 3003, 'score': 42031.62937164307, 'total_duration': 70343.08654594421, 'accumulated_submission_time': 42031.62937164307, 'accumulated_eval_time': 28305.865301132202, 'accumulated_logging_time': 1.7545392513275146}
I0209 21:55:07.273925 139615914645248 logging_writer.py:48] [120413] accumulated_eval_time=28305.865301, accumulated_logging_time=1.754539, accumulated_submission_time=42031.629372, global_step=120413, preemption_count=0, score=42031.629372, test/accuracy=0.705328, test/bleu=30.600486, test/loss=1.338359, test/num_examples=3003, total_duration=70343.086546, train/accuracy=0.690440, train/bleu=35.097181, train/loss=1.442906, validation/accuracy=0.690122, validation/bleu=30.587810, validation/loss=1.429707, validation/num_examples=3000
I0209 21:55:38.012888 139615906252544 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.381188303232193, loss=1.415592074394226
I0209 21:56:12.964958 139615914645248 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.38834133744239807, loss=1.5082447528839111
I0209 21:56:47.915473 139615906252544 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.38479194045066833, loss=1.463948369026184
I0209 21:57:22.827033 139615914645248 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.3699929714202881, loss=1.4469596147537231
I0209 21:57:57.727858 139615906252544 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3658580183982849, loss=1.4581571817398071
I0209 21:58:32.653694 139615914645248 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3948019742965698, loss=1.4575307369232178
I0209 21:59:07.570136 139615906252544 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3769720494747162, loss=1.5140970945358276
I0209 21:59:42.472452 139615914645248 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3674200773239136, loss=1.4434840679168701
I0209 22:00:17.392071 139615906252544 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.4083094298839569, loss=1.5211976766586304
I0209 22:00:52.259760 139615914645248 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3653623163700104, loss=1.4217500686645508
I0209 22:01:27.212268 139615906252544 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.37615352869033813, loss=1.4471436738967896
I0209 22:02:02.115364 139615914645248 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.37488240003585815, loss=1.4393587112426758
I0209 22:02:36.983341 139615906252544 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.385111540555954, loss=1.4820276498794556
I0209 22:03:11.897929 139615914645248 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.38587284088134766, loss=1.456394910812378
I0209 22:03:46.791632 139615906252544 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.37698525190353394, loss=1.489675760269165
I0209 22:04:21.680738 139615914645248 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.40549618005752563, loss=1.4988657236099243
I0209 22:04:56.564221 139615906252544 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.3720921576023102, loss=1.4240186214447021
I0209 22:05:31.482920 139615914645248 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.38421332836151123, loss=1.4892693758010864
I0209 22:06:06.367475 139615906252544 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.38424402475357056, loss=1.502961277961731
I0209 22:06:41.347917 139615914645248 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.3525714576244354, loss=1.3747044801712036
I0209 22:07:16.255708 139615906252544 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3694283366203308, loss=1.4602874517440796
I0209 22:07:51.193720 139615914645248 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.3613351285457611, loss=1.412775993347168
I0209 22:08:26.112710 139615906252544 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.3931673765182495, loss=1.5445326566696167
I0209 22:09:01.004427 139615914645248 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3891645073890686, loss=1.4612394571304321
I0209 22:09:07.352169 139785736898368 spec.py:321] Evaluating on the training split.
I0209 22:09:10.362435 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:12:53.478726 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 22:12:56.180376 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:15:58.426255 139785736898368 spec.py:349] Evaluating on the test split.
I0209 22:16:01.155602 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:18:44.824735 139785736898368 submission_runner.py:408] Time since start: 71760.68s, 	Step: 122820, 	{'train/accuracy': 0.688624382019043, 'train/loss': 1.460661768913269, 'train/bleu': 35.01808765515029, 'validation/accuracy': 0.6914607286453247, 'validation/loss': 1.4257858991622925, 'validation/bleu': 30.7654477246006, 'validation/num_examples': 3000, 'test/accuracy': 0.7075591087341309, 'test/loss': 1.3309905529022217, 'test/bleu': 30.70963575294762, 'test/num_examples': 3003, 'score': 42871.6183693409, 'total_duration': 71760.67544698715, 'accumulated_submission_time': 42871.6183693409, 'accumulated_eval_time': 28883.33780145645, 'accumulated_logging_time': 1.80470871925354}
I0209 22:18:44.857745 139615906252544 logging_writer.py:48] [122820] accumulated_eval_time=28883.337801, accumulated_logging_time=1.804709, accumulated_submission_time=42871.618369, global_step=122820, preemption_count=0, score=42871.618369, test/accuracy=0.707559, test/bleu=30.709636, test/loss=1.330991, test/num_examples=3003, total_duration=71760.675447, train/accuracy=0.688624, train/bleu=35.018088, train/loss=1.460662, validation/accuracy=0.691461, validation/bleu=30.765448, validation/loss=1.425786, validation/num_examples=3000
I0209 22:19:13.142914 139615914645248 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.39547017216682434, loss=1.441667914390564
I0209 22:19:48.041305 139615906252544 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3818274736404419, loss=1.4732224941253662
I0209 22:20:22.948948 139615914645248 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.36149781942367554, loss=1.4502441883087158
I0209 22:20:57.837083 139615906252544 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3921341300010681, loss=1.441800832748413
I0209 22:21:32.768729 139615914645248 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.3990543782711029, loss=1.476477861404419
I0209 22:22:07.678141 139615906252544 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3760550320148468, loss=1.4917256832122803
I0209 22:22:42.549086 139615914645248 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.36923515796661377, loss=1.4913314580917358
I0209 22:23:17.485914 139615906252544 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.3756929934024811, loss=1.4340829849243164
I0209 22:23:52.358902 139615914645248 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.38406190276145935, loss=1.5015240907669067
I0209 22:24:27.260169 139615906252544 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.38396522402763367, loss=1.4249707460403442
I0209 22:25:02.145438 139615914645248 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3807451128959656, loss=1.4596061706542969
I0209 22:25:37.064392 139615906252544 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3806092441082001, loss=1.49307119846344
I0209 22:26:11.964111 139615914645248 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.37084126472473145, loss=1.429931640625
I0209 22:26:46.865133 139615906252544 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3818891942501068, loss=1.453344464302063
I0209 22:27:21.733799 139615914645248 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.39371705055236816, loss=1.525963306427002
I0209 22:27:56.626015 139615906252544 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.38189324736595154, loss=1.4360536336898804
I0209 22:28:31.512872 139615914645248 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.40975090861320496, loss=1.5290510654449463
I0209 22:29:06.421486 139615906252544 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.4038773775100708, loss=1.4528136253356934
I0209 22:29:41.384678 139615914645248 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.39646074175834656, loss=1.479618787765503
I0209 22:30:16.298144 139615906252544 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.39274275302886963, loss=1.4340534210205078
I0209 22:30:51.199475 139615914645248 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.3827972412109375, loss=1.4671128988265991
I0209 22:31:26.131489 139615906252544 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.390350878238678, loss=1.4329078197479248
I0209 22:32:01.095379 139615914645248 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.37734824419021606, loss=1.4947367906570435
I0209 22:32:35.985570 139615906252544 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.38859620690345764, loss=1.3796579837799072
I0209 22:32:45.134408 139785736898368 spec.py:321] Evaluating on the training split.
I0209 22:32:48.139613 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:36:16.601700 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 22:36:19.317981 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:39:09.158589 139785736898368 spec.py:349] Evaluating on the test split.
I0209 22:39:11.868767 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:41:52.854071 139785736898368 submission_runner.py:408] Time since start: 73148.70s, 	Step: 125228, 	{'train/accuracy': 0.699196994304657, 'train/loss': 1.396242618560791, 'train/bleu': 36.034047007172774, 'validation/accuracy': 0.6935189962387085, 'validation/loss': 1.4201627969741821, 'validation/bleu': 31.030971558360275, 'validation/num_examples': 3000, 'test/accuracy': 0.7094881534576416, 'test/loss': 1.3231710195541382, 'test/bleu': 30.738778424579746, 'test/num_examples': 3003, 'score': 43711.80949354172, 'total_duration': 73148.70480275154, 'accumulated_submission_time': 43711.80949354172, 'accumulated_eval_time': 29431.057413339615, 'accumulated_logging_time': 1.8475875854492188}
I0209 22:41:52.886551 139615914645248 logging_writer.py:48] [125228] accumulated_eval_time=29431.057413, accumulated_logging_time=1.847588, accumulated_submission_time=43711.809494, global_step=125228, preemption_count=0, score=43711.809494, test/accuracy=0.709488, test/bleu=30.738778, test/loss=1.323171, test/num_examples=3003, total_duration=73148.704803, train/accuracy=0.699197, train/bleu=36.034047, train/loss=1.396243, validation/accuracy=0.693519, validation/bleu=31.030972, validation/loss=1.420163, validation/num_examples=3000
I0209 22:42:18.362146 139615906252544 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.41787639260292053, loss=1.5675678253173828
I0209 22:42:53.291439 139615914645248 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.37381988763809204, loss=1.5037360191345215
I0209 22:43:28.183313 139615906252544 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.38617125153541565, loss=1.471412181854248
I0209 22:44:03.058656 139615914645248 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3847340941429138, loss=1.4268356561660767
I0209 22:44:37.959667 139615906252544 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.3731468915939331, loss=1.4387580156326294
I0209 22:45:12.883434 139615914645248 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.37680816650390625, loss=1.430493712425232
I0209 22:45:47.791053 139615906252544 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.39127489924430847, loss=1.4652533531188965
I0209 22:46:22.688806 139615914645248 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.3791569471359253, loss=1.4896913766860962
I0209 22:46:57.586094 139615906252544 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.3862394094467163, loss=1.4507197141647339
I0209 22:47:32.575394 139615914645248 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.39173755049705505, loss=1.409265160560608
I0209 22:48:07.471225 139615906252544 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.3841460347175598, loss=1.4146820306777954
I0209 22:48:42.352355 139615914645248 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.35892894864082336, loss=1.316064715385437
I0209 22:49:17.238819 139615906252544 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.3808278441429138, loss=1.4471980333328247
I0209 22:49:52.157032 139615914645248 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.3747290372848511, loss=1.4373722076416016
I0209 22:50:27.051898 139615906252544 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.38749662041664124, loss=1.4905976057052612
I0209 22:51:01.950330 139615914645248 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.38407859206199646, loss=1.5028491020202637
I0209 22:51:36.862278 139615906252544 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.37626713514328003, loss=1.386832356452942
I0209 22:52:11.805011 139615914645248 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.407082736492157, loss=1.4286190271377563
I0209 22:52:46.685093 139615906252544 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3727021813392639, loss=1.3791857957839966
I0209 22:53:21.596019 139615914645248 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.3773927092552185, loss=1.4788798093795776
I0209 22:53:56.532366 139615906252544 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.39316099882125854, loss=1.5110446214675903
I0209 22:54:31.433320 139615914645248 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.38429853320121765, loss=1.449493646621704
I0209 22:55:06.314257 139615906252544 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3969730734825134, loss=1.392256498336792
I0209 22:55:41.171672 139615914645248 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.3929659128189087, loss=1.4182965755462646
I0209 22:55:53.114665 139785736898368 spec.py:321] Evaluating on the training split.
I0209 22:55:56.119866 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 22:59:31.033653 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 22:59:33.745032 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:02:18.155507 139785736898368 spec.py:349] Evaluating on the test split.
I0209 23:02:20.869741 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:05:05.564949 139785736898368 submission_runner.py:408] Time since start: 74541.42s, 	Step: 127636, 	{'train/accuracy': 0.696530818939209, 'train/loss': 1.4135314226150513, 'train/bleu': 35.25999172604652, 'validation/accuracy': 0.6937545537948608, 'validation/loss': 1.4164506196975708, 'validation/bleu': 31.00998771095735, 'validation/num_examples': 3000, 'test/accuracy': 0.7097554206848145, 'test/loss': 1.319035291671753, 'test/bleu': 30.757323033480343, 'test/num_examples': 3003, 'score': 44551.95158267021, 'total_duration': 74541.41567277908, 'accumulated_submission_time': 44551.95158267021, 'accumulated_eval_time': 29983.50764322281, 'accumulated_logging_time': 1.889930009841919}
I0209 23:05:05.597245 139615906252544 logging_writer.py:48] [127636] accumulated_eval_time=29983.507643, accumulated_logging_time=1.889930, accumulated_submission_time=44551.951583, global_step=127636, preemption_count=0, score=44551.951583, test/accuracy=0.709755, test/bleu=30.757323, test/loss=1.319035, test/num_examples=3003, total_duration=74541.415673, train/accuracy=0.696531, train/bleu=35.259992, train/loss=1.413531, validation/accuracy=0.693755, validation/bleu=31.009988, validation/loss=1.416451, validation/num_examples=3000
I0209 23:05:28.294408 139615914645248 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.3896608352661133, loss=1.4245691299438477
I0209 23:06:03.215959 139615906252544 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.37899401783943176, loss=1.4194055795669556
I0209 23:06:38.092833 139615914645248 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.37685757875442505, loss=1.4148907661437988
I0209 23:07:13.025045 139615906252544 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.4018460810184479, loss=1.5063167810440063
I0209 23:07:47.969243 139615914645248 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.3886083960533142, loss=1.4351166486740112
I0209 23:08:22.910257 139615906252544 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.3846208155155182, loss=1.4159523248672485
I0209 23:08:57.926483 139615914645248 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.3834688365459442, loss=1.3887888193130493
I0209 23:09:32.866611 139615906252544 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.3872857391834259, loss=1.4127246141433716
I0209 23:10:07.784407 139615914645248 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.3662838041782379, loss=1.3551918268203735
I0209 23:10:42.718243 139615906252544 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.37539705634117126, loss=1.4074491262435913
I0209 23:11:17.626563 139615914645248 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3758912980556488, loss=1.3369632959365845
I0209 23:11:52.597154 139615906252544 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.3880715072154999, loss=1.4050922393798828
I0209 23:12:27.525101 139615914645248 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.381282776594162, loss=1.4207415580749512
I0209 23:13:02.440334 139615906252544 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.4037899076938629, loss=1.4700146913528442
I0209 23:13:37.333199 139615914645248 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.37296250462532043, loss=1.478771448135376
I0209 23:14:12.228064 139615906252544 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.4014115333557129, loss=1.441959261894226
I0209 23:14:47.159506 139615914645248 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.39668008685112, loss=1.4402140378952026
I0209 23:15:22.067181 139615906252544 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.3696036636829376, loss=1.4348359107971191
I0209 23:15:56.996906 139615914645248 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.40351060032844543, loss=1.4817698001861572
I0209 23:16:31.907290 139615906252544 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.4130842089653015, loss=1.428733229637146
I0209 23:17:06.781794 139615914645248 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.38683927059173584, loss=1.3896747827529907
I0209 23:17:41.683450 139615906252544 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.38062533736228943, loss=1.3985401391983032
I0209 23:18:16.563450 139615914645248 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.39335817098617554, loss=1.455248475074768
I0209 23:18:51.453532 139615906252544 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.39489221572875977, loss=1.4179853200912476
I0209 23:19:05.838481 139785736898368 spec.py:321] Evaluating on the training split.
I0209 23:19:08.836123 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:22:49.807231 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 23:22:52.517716 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:25:32.391319 139785736898368 spec.py:349] Evaluating on the test split.
I0209 23:25:35.124751 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:28:05.911324 139785736898368 submission_runner.py:408] Time since start: 75921.76s, 	Step: 130043, 	{'train/accuracy': 0.6964461803436279, 'train/loss': 1.4212472438812256, 'train/bleu': 35.426351940087365, 'validation/accuracy': 0.693804144859314, 'validation/loss': 1.4148198366165161, 'validation/bleu': 30.977180725630046, 'validation/num_examples': 3000, 'test/accuracy': 0.7098251581192017, 'test/loss': 1.3173716068267822, 'test/bleu': 30.876633593370595, 'test/num_examples': 3003, 'score': 45392.100281476974, 'total_duration': 75921.76205563545, 'accumulated_submission_time': 45392.100281476974, 'accumulated_eval_time': 30523.580441236496, 'accumulated_logging_time': 1.9321520328521729}
I0209 23:28:05.944032 139615914645248 logging_writer.py:48] [130043] accumulated_eval_time=30523.580441, accumulated_logging_time=1.932152, accumulated_submission_time=45392.100281, global_step=130043, preemption_count=0, score=45392.100281, test/accuracy=0.709825, test/bleu=30.876634, test/loss=1.317372, test/num_examples=3003, total_duration=75921.762056, train/accuracy=0.696446, train/bleu=35.426352, train/loss=1.421247, validation/accuracy=0.693804, validation/bleu=30.977181, validation/loss=1.414820, validation/num_examples=3000
I0209 23:28:26.180883 139615906252544 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.38261106610298157, loss=1.4150168895721436
I0209 23:29:01.056873 139615914645248 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.38738134503364563, loss=1.4680413007736206
I0209 23:29:35.941239 139615906252544 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.3773033320903778, loss=1.4053624868392944
I0209 23:30:10.883456 139615914645248 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.3729553520679474, loss=1.319347858428955
I0209 23:30:45.794281 139615906252544 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.37082237005233765, loss=1.45946204662323
I0209 23:31:20.678554 139615914645248 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.3822658956050873, loss=1.427817940711975
I0209 23:31:55.597243 139615906252544 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.3919609785079956, loss=1.4375752210617065
I0209 23:32:30.467231 139615914645248 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.38666895031929016, loss=1.3962593078613281
I0209 23:33:05.365751 139615906252544 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.41704854369163513, loss=1.5267333984375
I0209 23:33:40.247612 139615914645248 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.38555195927619934, loss=1.3860969543457031
I0209 23:34:15.142693 139615906252544 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.39095059037208557, loss=1.4141730070114136
I0209 23:34:50.040596 139615914645248 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.3680069148540497, loss=1.4260648488998413
I0209 23:35:24.914272 139615906252544 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.3776194751262665, loss=1.4608508348464966
I0209 23:35:59.808290 139615914645248 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.36604470014572144, loss=1.386838674545288
I0209 23:36:34.763900 139615906252544 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.379239559173584, loss=1.4105912446975708
I0209 23:37:09.666403 139615914645248 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.3892599046230316, loss=1.4502002000808716
I0209 23:37:44.561815 139615906252544 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.4012393653392792, loss=1.448825478553772
I0209 23:38:19.469815 139615914645248 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.3833199143409729, loss=1.4803926944732666
I0209 23:38:54.414626 139615906252544 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.38120412826538086, loss=1.402558445930481
I0209 23:39:29.344493 139615914645248 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.37638384103775024, loss=1.3858625888824463
I0209 23:40:04.268331 139615906252544 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.38533860445022583, loss=1.4004026651382446
I0209 23:40:39.123703 139615914645248 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.37016934156417847, loss=1.413346767425537
I0209 23:41:13.999294 139615906252544 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.390999972820282, loss=1.375902771949768
I0209 23:41:48.929441 139615914645248 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.38065305352211, loss=1.3850189447402954
I0209 23:42:06.122316 139785736898368 spec.py:321] Evaluating on the training split.
I0209 23:42:09.140262 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:45:41.354244 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 23:45:44.069436 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:48:23.989604 139785736898368 spec.py:349] Evaluating on the test split.
I0209 23:48:26.706018 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:51:01.491029 139785736898368 submission_runner.py:408] Time since start: 77297.34s, 	Step: 132451, 	{'train/accuracy': 0.6961711049079895, 'train/loss': 1.419246792793274, 'train/bleu': 35.54502780300605, 'validation/accuracy': 0.6945357322692871, 'validation/loss': 1.414249300956726, 'validation/bleu': 31.107725195668912, 'validation/num_examples': 3000, 'test/accuracy': 0.7100808024406433, 'test/loss': 1.3164043426513672, 'test/bleu': 30.8854601382876, 'test/num_examples': 3003, 'score': 46232.19126367569, 'total_duration': 77297.3417544365, 'accumulated_submission_time': 46232.19126367569, 'accumulated_eval_time': 31058.94911122322, 'accumulated_logging_time': 1.9750447273254395}
I0209 23:51:01.525151 139615906252544 logging_writer.py:48] [132451] accumulated_eval_time=31058.949111, accumulated_logging_time=1.975045, accumulated_submission_time=46232.191264, global_step=132451, preemption_count=0, score=46232.191264, test/accuracy=0.710081, test/bleu=30.885460, test/loss=1.316404, test/num_examples=3003, total_duration=77297.341754, train/accuracy=0.696171, train/bleu=35.545028, train/loss=1.419247, validation/accuracy=0.694536, validation/bleu=31.107725, validation/loss=1.414249, validation/num_examples=3000
I0209 23:51:18.928055 139615914645248 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.38715267181396484, loss=1.4754979610443115
I0209 23:51:53.804048 139615906252544 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.37447336316108704, loss=1.445133924484253
I0209 23:52:28.709321 139615914645248 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.39567142724990845, loss=1.4280767440795898
I0209 23:53:03.611176 139615906252544 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.37280622124671936, loss=1.3869855403900146
I0209 23:53:38.507182 139615914645248 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.3712262511253357, loss=1.4354091882705688
I0209 23:54:13.398962 139615906252544 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.38207149505615234, loss=1.4201531410217285
I0209 23:54:48.283256 139615914645248 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.3812766969203949, loss=1.414834976196289
I0209 23:55:23.170254 139615906252544 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.36862480640411377, loss=1.3346071243286133
I0209 23:55:58.088112 139615914645248 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.39591604471206665, loss=1.530308485031128
I0209 23:56:09.000838 139785736898368 spec.py:321] Evaluating on the training split.
I0209 23:56:12.017688 139785736898368 workload.py:181] Translating evaluation dataset.
I0209 23:59:36.641559 139785736898368 spec.py:333] Evaluating on the validation split.
I0209 23:59:39.360139 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:02:25.933843 139785736898368 spec.py:349] Evaluating on the test split.
I0210 00:02:28.672577 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:05:07.203209 139785736898368 submission_runner.py:408] Time since start: 78143.05s, 	Step: 133333, 	{'train/accuracy': 0.6953193545341492, 'train/loss': 1.4255940914154053, 'train/bleu': 35.78710381048599, 'validation/accuracy': 0.6945977210998535, 'validation/loss': 1.4143544435501099, 'validation/bleu': 31.089424411746926, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.316562533378601, 'test/bleu': 30.94145452506206, 'test/num_examples': 3003, 'score': 46539.62941074371, 'total_duration': 78143.05393791199, 'accumulated_submission_time': 46539.62941074371, 'accumulated_eval_time': 31597.15144467354, 'accumulated_logging_time': 2.018925905227661}
I0210 00:05:07.236636 139615906252544 logging_writer.py:48] [133333] accumulated_eval_time=31597.151445, accumulated_logging_time=2.018926, accumulated_submission_time=46539.629411, global_step=133333, preemption_count=0, score=46539.629411, test/accuracy=0.709976, test/bleu=30.941455, test/loss=1.316563, test/num_examples=3003, total_duration=78143.053938, train/accuracy=0.695319, train/bleu=35.787104, train/loss=1.425594, validation/accuracy=0.694598, validation/bleu=31.089424, validation/loss=1.414354, validation/num_examples=3000
I0210 00:05:07.271005 139615914645248 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46539.629411
I0210 00:05:08.315388 139785736898368 checkpoints.py:490] Saving checkpoint at step: 133333
I0210 00:05:12.243168 139785736898368 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4/checkpoint_133333
I0210 00:05:12.247973 139785736898368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_4/checkpoint_133333.
I0210 00:05:12.307604 139785736898368 submission_runner.py:583] Tuning trial 4/5
I0210 00:05:12.307778 139785736898368 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0210 00:05:12.312479 139785736898368 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0004938441561535001, 'train/loss': 11.066301345825195, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.327144145965576, 'total_duration': 898.9657063484192, 'accumulated_submission_time': 27.327144145965576, 'accumulated_eval_time': 871.6384983062744, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2407, {'train/accuracy': 0.5285830497741699, 'train/loss': 2.639150857925415, 'train/bleu': 22.527051695575885, 'validation/accuracy': 0.5278669595718384, 'validation/loss': 2.6120784282684326, 'validation/bleu': 18.724227858741123, 'validation/num_examples': 3000, 'test/accuracy': 0.5244320631027222, 'test/loss': 2.647041082382202, 'test/bleu': 17.176751782717933, 'test/num_examples': 3003, 'score': 867.5621616840363, 'total_duration': 2202.9891409873962, 'accumulated_submission_time': 867.5621616840363, 'accumulated_eval_time': 1335.3256645202637, 'accumulated_logging_time': 0.022200345993041992, 'global_step': 2407, 'preemption_count': 0}), (4815, {'train/accuracy': 0.5797721147537231, 'train/loss': 2.221475601196289, 'train/bleu': 27.448449614774468, 'validation/accuracy': 0.5930862426757812, 'validation/loss': 2.0885164737701416, 'validation/bleu': 23.89420124644968, 'validation/num_examples': 3000, 'test/accuracy': 0.5975945591926575, 'test/loss': 2.0635406970977783, 'test/bleu': 22.466849339434948, 'test/num_examples': 3003, 'score': 1707.8006699085236, 'total_duration': 3621.8355102539062, 'accumulated_submission_time': 1707.8006699085236, 'accumulated_eval_time': 1913.8280260562897, 'accumulated_logging_time': 0.049674034118652344, 'global_step': 4815, 'preemption_count': 0}), (7223, {'train/accuracy': 0.5924304723739624, 'train/loss': 2.1243929862976074, 'train/bleu': 28.306294692431962, 'validation/accuracy': 0.6062664985656738, 'validation/loss': 2.0095314979553223, 'validation/bleu': 24.714309696920097, 'validation/num_examples': 3000, 'test/accuracy': 0.611388087272644, 'test/loss': 1.9671002626419067, 'test/bleu': 23.430505913385765, 'test/num_examples': 3003, 'score': 2547.789595603943, 'total_duration': 4948.868293046951, 'accumulated_submission_time': 2547.789595603943, 'accumulated_eval_time': 2400.7693164348602, 'accumulated_logging_time': 0.0744776725769043, 'global_step': 7223, 'preemption_count': 0}), (9631, {'train/accuracy': 0.5950818657875061, 'train/loss': 2.1058120727539062, 'train/bleu': 27.14673853695992, 'validation/accuracy': 0.6074815988540649, 'validation/loss': 1.9885951280593872, 'validation/bleu': 24.029534022545416, 'validation/num_examples': 3000, 'test/accuracy': 0.612155020236969, 'test/loss': 1.9553987979888916, 'test/bleu': 22.814568874859706, 'test/num_examples': 3003, 'score': 3387.7001433372498, 'total_duration': 6240.813716411591, 'accumulated_submission_time': 3387.7001433372498, 'accumulated_eval_time': 2852.701284646988, 'accumulated_logging_time': 0.10046052932739258, 'global_step': 9631, 'preemption_count': 0}), (12039, {'train/accuracy': 0.595694899559021, 'train/loss': 2.1055915355682373, 'train/bleu': 28.150822337375132, 'validation/accuracy': 0.612738847732544, 'validation/loss': 1.9446474313735962, 'validation/bleu': 24.70037868232737, 'validation/num_examples': 3000, 'test/accuracy': 0.6214398145675659, 'test/loss': 1.895939826965332, 'test/bleu': 23.912404164530532, 'test/num_examples': 3003, 'score': 4227.669346570969, 'total_duration': 7555.3547575473785, 'accumulated_submission_time': 4227.669346570969, 'accumulated_eval_time': 3327.1690158843994, 'accumulated_logging_time': 0.12771081924438477, 'global_step': 12039, 'preemption_count': 0}), (14447, {'train/accuracy': 0.597169041633606, 'train/loss': 2.078855276107788, 'train/bleu': 28.419443225761, 'validation/accuracy': 0.615293025970459, 'validation/loss': 1.9333375692367554, 'validation/bleu': 25.01238163048505, 'validation/num_examples': 3000, 'test/accuracy': 0.6236011981964111, 'test/loss': 1.8801435232162476, 'test/bleu': 24.321233521773724, 'test/num_examples': 3003, 'score': 5067.6564655303955, 'total_duration': 8942.286494970322, 'accumulated_submission_time': 5067.6564655303955, 'accumulated_eval_time': 3874.0108201503754, 'accumulated_logging_time': 0.15516328811645508, 'global_step': 14447, 'preemption_count': 0}), (16856, {'train/accuracy': 0.5945723056793213, 'train/loss': 2.086899518966675, 'train/bleu': 28.754664656766057, 'validation/accuracy': 0.6146854758262634, 'validation/loss': 1.9335582256317139, 'validation/bleu': 24.68478496541946, 'validation/num_examples': 3000, 'test/accuracy': 0.6249956488609314, 'test/loss': 1.8804055452346802, 'test/bleu': 24.498520031012532, 'test/num_examples': 3003, 'score': 5907.831691741943, 'total_duration': 10269.621250391006, 'accumulated_submission_time': 5907.831691741943, 'accumulated_eval_time': 4361.068868637085, 'accumulated_logging_time': 0.18170881271362305, 'global_step': 16856, 'preemption_count': 0}), (19264, {'train/accuracy': 0.6116220355033875, 'train/loss': 1.968550443649292, 'train/bleu': 28.895200180970697, 'validation/accuracy': 0.6213685870170593, 'validation/loss': 1.8958373069763184, 'validation/bleu': 25.17453094857682, 'validation/num_examples': 3000, 'test/accuracy': 0.6264830827713013, 'test/loss': 1.8554015159606934, 'test/bleu': 23.868531253383175, 'test/num_examples': 3003, 'score': 6748.037788152695, 'total_duration': 11700.324100255966, 'accumulated_submission_time': 6748.037788152695, 'accumulated_eval_time': 4951.462516546249, 'accumulated_logging_time': 0.2087688446044922, 'global_step': 19264, 'preemption_count': 0}), (21673, {'train/accuracy': 0.6006941199302673, 'train/loss': 2.0575037002563477, 'train/bleu': 28.424609301688978, 'validation/accuracy': 0.6233896613121033, 'validation/loss': 1.8967119455337524, 'validation/bleu': 25.493374681381614, 'validation/num_examples': 3000, 'test/accuracy': 0.629283607006073, 'test/loss': 1.8413125276565552, 'test/bleu': 24.442632777732154, 'test/num_examples': 3003, 'score': 7588.130204200745, 'total_duration': 13036.512573957443, 'accumulated_submission_time': 7588.130204200745, 'accumulated_eval_time': 5447.454230070114, 'accumulated_logging_time': 0.23738527297973633, 'global_step': 21673, 'preemption_count': 0}), (24082, {'train/accuracy': 0.6017192602157593, 'train/loss': 2.061342716217041, 'train/bleu': 27.65494665264454, 'validation/accuracy': 0.6205998659133911, 'validation/loss': 1.8996304273605347, 'validation/bleu': 24.93630152938552, 'validation/num_examples': 3000, 'test/accuracy': 0.6273081302642822, 'test/loss': 1.8463226556777954, 'test/bleu': 23.7675562804176, 'test/num_examples': 3003, 'score': 8428.177636146545, 'total_duration': 14390.404823541641, 'accumulated_submission_time': 8428.177636146545, 'accumulated_eval_time': 5961.189096927643, 'accumulated_logging_time': 0.27000856399536133, 'global_step': 24082, 'preemption_count': 0}), (26491, {'train/accuracy': 0.6068245768547058, 'train/loss': 2.0195443630218506, 'train/bleu': 28.520838515349766, 'validation/accuracy': 0.6253611445426941, 'validation/loss': 1.8684108257293701, 'validation/bleu': 25.046618347673732, 'validation/num_examples': 3000, 'test/accuracy': 0.6311893463134766, 'test/loss': 1.8257577419281006, 'test/bleu': 24.47399505397761, 'test/num_examples': 3003, 'score': 9268.377697706223, 'total_duration': 15813.514918327332, 'accumulated_submission_time': 9268.377697706223, 'accumulated_eval_time': 6543.995737552643, 'accumulated_logging_time': 0.2978191375732422, 'global_step': 26491, 'preemption_count': 0}), (28899, {'train/accuracy': 0.6058840155601501, 'train/loss': 2.0256903171539307, 'train/bleu': 28.15505565929569, 'validation/accuracy': 0.6242700219154358, 'validation/loss': 1.872414231300354, 'validation/bleu': 25.25882325875779, 'validation/num_examples': 3000, 'test/accuracy': 0.6313404440879822, 'test/loss': 1.8199553489685059, 'test/bleu': 24.222597592597598, 'test/num_examples': 3003, 'score': 10108.486330747604, 'total_duration': 17209.145723819733, 'accumulated_submission_time': 10108.486330747604, 'accumulated_eval_time': 7099.407840251923, 'accumulated_logging_time': 0.3309357166290283, 'global_step': 28899, 'preemption_count': 0}), (31308, {'train/accuracy': 0.6748192310333252, 'train/loss': 1.52360999584198, 'train/bleu': 32.83433125238866, 'validation/accuracy': 0.6257950663566589, 'validation/loss': 1.8585841655731201, 'validation/bleu': 25.46695972247443, 'validation/num_examples': 3000, 'test/accuracy': 0.6303178071975708, 'test/loss': 1.821932077407837, 'test/bleu': 24.27951532927107, 'test/num_examples': 3003, 'score': 10948.709831953049, 'total_duration': 18504.365869522095, 'accumulated_submission_time': 10948.709831953049, 'accumulated_eval_time': 7554.298825263977, 'accumulated_logging_time': 0.36050963401794434, 'global_step': 31308, 'preemption_count': 0}), (33716, {'train/accuracy': 0.606469988822937, 'train/loss': 2.0195164680480957, 'train/bleu': 28.84137114817753, 'validation/accuracy': 0.627865731716156, 'validation/loss': 1.842737078666687, 'validation/bleu': 25.98349001394717, 'validation/num_examples': 3000, 'test/accuracy': 0.6331764459609985, 'test/loss': 1.7970689535140991, 'test/bleu': 24.776988322226952, 'test/num_examples': 3003, 'score': 11788.792954921722, 'total_duration': 19873.697608232498, 'accumulated_submission_time': 11788.792954921722, 'accumulated_eval_time': 8083.440821886063, 'accumulated_logging_time': 0.39031457901000977, 'global_step': 33716, 'preemption_count': 0}), (36125, {'train/accuracy': 0.6092419028282166, 'train/loss': 2.0129339694976807, 'train/bleu': 28.618722834476483, 'validation/accuracy': 0.6275929808616638, 'validation/loss': 1.8477492332458496, 'validation/bleu': 25.35805133784494, 'validation/num_examples': 3000, 'test/accuracy': 0.6359886527061462, 'test/loss': 1.7861478328704834, 'test/bleu': 24.281479120561823, 'test/num_examples': 3003, 'score': 12628.945209980011, 'total_duration': 21195.21056318283, 'accumulated_submission_time': 12628.945209980011, 'accumulated_eval_time': 8564.694946527481, 'accumulated_logging_time': 0.42111873626708984, 'global_step': 36125, 'preemption_count': 0}), (38534, {'train/accuracy': 0.6119536757469177, 'train/loss': 1.9667437076568604, 'train/bleu': 28.499661182879315, 'validation/accuracy': 0.6280888915061951, 'validation/loss': 1.836946964263916, 'validation/bleu': 26.057433104129547, 'validation/num_examples': 3000, 'test/accuracy': 0.6339085698127747, 'test/loss': 1.7903285026550293, 'test/bleu': 24.38247752713862, 'test/num_examples': 3003, 'score': 13468.92739701271, 'total_duration': 22592.982152462006, 'accumulated_submission_time': 13468.92739701271, 'accumulated_eval_time': 9122.370000362396, 'accumulated_logging_time': 0.45857930183410645, 'global_step': 38534, 'preemption_count': 0}), (40943, {'train/accuracy': 0.6094376444816589, 'train/loss': 1.9945857524871826, 'train/bleu': 29.30273771814245, 'validation/accuracy': 0.6275309324264526, 'validation/loss': 1.8460413217544556, 'validation/bleu': 25.75543886358571, 'validation/num_examples': 3000, 'test/accuracy': 0.6396607160568237, 'test/loss': 1.7795579433441162, 'test/bleu': 24.5530504118486, 'test/num_examples': 3003, 'score': 14309.143985748291, 'total_duration': 23962.273962020874, 'accumulated_submission_time': 14309.143985748291, 'accumulated_eval_time': 9651.330287218094, 'accumulated_logging_time': 0.49736833572387695, 'global_step': 40943, 'preemption_count': 0}), (43352, {'train/accuracy': 0.6117132902145386, 'train/loss': 1.979415774345398, 'train/bleu': 28.894642089106842, 'validation/accuracy': 0.6324781775474548, 'validation/loss': 1.8176498413085938, 'validation/bleu': 26.215222262675244, 'validation/num_examples': 3000, 'test/accuracy': 0.638893723487854, 'test/loss': 1.7688039541244507, 'test/bleu': 25.21839585129908, 'test/num_examples': 3003, 'score': 15149.22114944458, 'total_duration': 25347.535893440247, 'accumulated_submission_time': 15149.22114944458, 'accumulated_eval_time': 10196.409768819809, 'accumulated_logging_time': 0.5284297466278076, 'global_step': 43352, 'preemption_count': 0}), (45761, {'train/accuracy': 0.6118826866149902, 'train/loss': 1.9624874591827393, 'train/bleu': 28.652251811486824, 'validation/accuracy': 0.630605936050415, 'validation/loss': 1.8224587440490723, 'validation/bleu': 26.06368270027007, 'validation/num_examples': 3000, 'test/accuracy': 0.6413805484771729, 'test/loss': 1.7728618383407593, 'test/bleu': 25.37917327989407, 'test/num_examples': 3003, 'score': 15989.326357841492, 'total_duration': 26789.205688476562, 'accumulated_submission_time': 15989.326357841492, 'accumulated_eval_time': 10797.866746902466, 'accumulated_logging_time': 0.5607788562774658, 'global_step': 45761, 'preemption_count': 0}), (48173, {'train/accuracy': 0.6136108636856079, 'train/loss': 1.9673712253570557, 'train/bleu': 29.17189039490762, 'validation/accuracy': 0.6328873634338379, 'validation/loss': 1.8119760751724243, 'validation/bleu': 25.978765786257235, 'validation/num_examples': 3000, 'test/accuracy': 0.6418104767799377, 'test/loss': 1.7519155740737915, 'test/bleu': 25.306392423383475, 'test/num_examples': 3003, 'score': 16829.423892498016, 'total_duration': 28162.17760157585, 'accumulated_submission_time': 16829.423892498016, 'accumulated_eval_time': 11330.634264469147, 'accumulated_logging_time': 0.5914275646209717, 'global_step': 48173, 'preemption_count': 0}), (50581, {'train/accuracy': 0.6235958337783813, 'train/loss': 1.867612361907959, 'train/bleu': 29.89978511352561, 'validation/accuracy': 0.6353052258491516, 'validation/loss': 1.7884082794189453, 'validation/bleu': 26.18907230106764, 'validation/num_examples': 3000, 'test/accuracy': 0.6434489488601685, 'test/loss': 1.7368030548095703, 'test/bleu': 25.11578170365946, 'test/num_examples': 3003, 'score': 17669.362218618393, 'total_duration': 29497.41099047661, 'accumulated_submission_time': 17669.362218618393, 'accumulated_eval_time': 11825.816824674606, 'accumulated_logging_time': 0.6280605792999268, 'global_step': 50581, 'preemption_count': 0}), (52989, {'train/accuracy': 0.6194369792938232, 'train/loss': 1.9303407669067383, 'train/bleu': 29.233347669535043, 'validation/accuracy': 0.6357763409614563, 'validation/loss': 1.7921721935272217, 'validation/bleu': 26.26460212317322, 'validation/num_examples': 3000, 'test/accuracy': 0.646528422832489, 'test/loss': 1.7195571660995483, 'test/bleu': 25.58466393499604, 'test/num_examples': 3003, 'score': 18509.431342840195, 'total_duration': 30908.126499176025, 'accumulated_submission_time': 18509.431342840195, 'accumulated_eval_time': 12396.352447509766, 'accumulated_logging_time': 0.6613750457763672, 'global_step': 52989, 'preemption_count': 0}), (55397, {'train/accuracy': 0.619412362575531, 'train/loss': 1.933494210243225, 'train/bleu': 29.55223406319987, 'validation/accuracy': 0.6385537385940552, 'validation/loss': 1.7798618078231812, 'validation/bleu': 26.649243759342824, 'validation/num_examples': 3000, 'test/accuracy': 0.6478763818740845, 'test/loss': 1.710266351699829, 'test/bleu': 25.675784306440917, 'test/num_examples': 3003, 'score': 19349.615591049194, 'total_duration': 32308.030866384506, 'accumulated_submission_time': 19349.615591049194, 'accumulated_eval_time': 12955.963208198547, 'accumulated_logging_time': 0.6942222118377686, 'global_step': 55397, 'preemption_count': 0}), (57805, {'train/accuracy': 0.6222780346870422, 'train/loss': 1.8981658220291138, 'train/bleu': 29.94380117071459, 'validation/accuracy': 0.6379957795143127, 'validation/loss': 1.7573578357696533, 'validation/bleu': 26.4487990260602, 'validation/num_examples': 3000, 'test/accuracy': 0.648561954498291, 'test/loss': 1.6985454559326172, 'test/bleu': 25.331366094109622, 'test/num_examples': 3003, 'score': 20189.717071056366, 'total_duration': 33772.894074201584, 'accumulated_submission_time': 20189.717071056366, 'accumulated_eval_time': 13580.615655899048, 'accumulated_logging_time': 0.7280442714691162, 'global_step': 57805, 'preemption_count': 0}), (60213, {'train/accuracy': 0.6185654997825623, 'train/loss': 1.9307162761688232, 'train/bleu': 29.454532044501722, 'validation/accuracy': 0.6402896642684937, 'validation/loss': 1.7573803663253784, 'validation/bleu': 26.696946604753215, 'validation/num_examples': 3000, 'test/accuracy': 0.6507001519203186, 'test/loss': 1.6981478929519653, 'test/bleu': 26.085239423423623, 'test/num_examples': 3003, 'score': 21029.91014480591, 'total_duration': 35157.684792757034, 'accumulated_submission_time': 21029.91014480591, 'accumulated_eval_time': 14125.097086429596, 'accumulated_logging_time': 0.7669491767883301, 'global_step': 60213, 'preemption_count': 0}), (62620, {'train/accuracy': 0.6608275771141052, 'train/loss': 1.6121519804000854, 'train/bleu': 33.19012386447994, 'validation/accuracy': 0.6453608870506287, 'validation/loss': 1.729269027709961, 'validation/bleu': 26.712245705946223, 'validation/num_examples': 3000, 'test/accuracy': 0.6520132422447205, 'test/loss': 1.678233027458191, 'test/bleu': 25.953660552026207, 'test/num_examples': 3003, 'score': 21869.91243505478, 'total_duration': 36552.34384179115, 'accumulated_submission_time': 21869.91243505478, 'accumulated_eval_time': 14679.639946699142, 'accumulated_logging_time': 0.8013193607330322, 'global_step': 62620, 'preemption_count': 0}), (65027, {'train/accuracy': 0.6231135129928589, 'train/loss': 1.8940107822418213, 'train/bleu': 30.158621549991196, 'validation/accuracy': 0.6435257792472839, 'validation/loss': 1.7233736515045166, 'validation/bleu': 26.89136604491987, 'validation/num_examples': 3000, 'test/accuracy': 0.6520829796791077, 'test/loss': 1.6709034442901611, 'test/bleu': 26.038383151652898, 'test/num_examples': 3003, 'score': 22709.923098564148, 'total_duration': 37917.61070728302, 'accumulated_submission_time': 22709.923098564148, 'accumulated_eval_time': 15204.78667140007, 'accumulated_logging_time': 0.8344008922576904, 'global_step': 65027, 'preemption_count': 0}), (67434, {'train/accuracy': 0.6210484504699707, 'train/loss': 1.9104690551757812, 'train/bleu': 30.423989758952263, 'validation/accuracy': 0.6472083330154419, 'validation/loss': 1.7089643478393555, 'validation/bleu': 26.998232384874996, 'validation/num_examples': 3000, 'test/accuracy': 0.6572192311286926, 'test/loss': 1.6496104001998901, 'test/bleu': 26.566984573098747, 'test/num_examples': 3003, 'score': 23549.88878941536, 'total_duration': 39358.25233221054, 'accumulated_submission_time': 23549.88878941536, 'accumulated_eval_time': 15805.35246348381, 'accumulated_logging_time': 0.8699424266815186, 'global_step': 67434, 'preemption_count': 0}), (69842, {'train/accuracy': 0.6296305060386658, 'train/loss': 1.8308836221694946, 'train/bleu': 30.28551842580955, 'validation/accuracy': 0.6483986377716064, 'validation/loss': 1.700315237045288, 'validation/bleu': 27.293684458325167, 'validation/num_examples': 3000, 'test/accuracy': 0.6600197553634644, 'test/loss': 1.6326422691345215, 'test/bleu': 26.693438091255096, 'test/num_examples': 3003, 'score': 24390.009131908417, 'total_duration': 40758.694177389145, 'accumulated_submission_time': 24390.009131908417, 'accumulated_eval_time': 16365.558494091034, 'accumulated_logging_time': 0.9100024700164795, 'global_step': 69842, 'preemption_count': 0}), (72250, {'train/accuracy': 0.6270090937614441, 'train/loss': 1.8659878969192505, 'train/bleu': 30.405617139420453, 'validation/accuracy': 0.6491177678108215, 'validation/loss': 1.6966098546981812, 'validation/bleu': 27.24253045501503, 'validation/num_examples': 3000, 'test/accuracy': 0.6600894927978516, 'test/loss': 1.6326830387115479, 'test/bleu': 26.730419230948563, 'test/num_examples': 3003, 'score': 25230.245144844055, 'total_duration': 42108.99506902695, 'accumulated_submission_time': 25230.245144844055, 'accumulated_eval_time': 16875.51067852974, 'accumulated_logging_time': 0.9470679759979248, 'global_step': 72250, 'preemption_count': 0}), (74657, {'train/accuracy': 0.6295642852783203, 'train/loss': 1.8524365425109863, 'train/bleu': 29.989682284103964, 'validation/accuracy': 0.6508660912513733, 'validation/loss': 1.6791478395462036, 'validation/bleu': 27.602717674089575, 'validation/num_examples': 3000, 'test/accuracy': 0.6641218066215515, 'test/loss': 1.6093859672546387, 'test/bleu': 26.914830269027004, 'test/num_examples': 3003, 'score': 26070.144366502762, 'total_duration': 43492.85367035866, 'accumulated_submission_time': 26070.144366502762, 'accumulated_eval_time': 17419.357144355774, 'accumulated_logging_time': 0.9834208488464355, 'global_step': 74657, 'preemption_count': 0}), (77065, {'train/accuracy': 0.6358433365821838, 'train/loss': 1.804227352142334, 'train/bleu': 30.153082714253948, 'validation/accuracy': 0.6521183848381042, 'validation/loss': 1.6711089611053467, 'validation/bleu': 27.59902273365211, 'validation/num_examples': 3000, 'test/accuracy': 0.6620184779167175, 'test/loss': 1.6064454317092896, 'test/bleu': 26.76849545725773, 'test/num_examples': 3003, 'score': 26910.256639242172, 'total_duration': 44847.364674806595, 'accumulated_submission_time': 26910.256639242172, 'accumulated_eval_time': 17933.644248008728, 'accumulated_logging_time': 1.0192360877990723, 'global_step': 77065, 'preemption_count': 0}), (79473, {'train/accuracy': 0.632390558719635, 'train/loss': 1.8178280591964722, 'train/bleu': 30.46417054723414, 'validation/accuracy': 0.654263436794281, 'validation/loss': 1.6578301191329956, 'validation/bleu': 27.5063002215074, 'validation/num_examples': 3000, 'test/accuracy': 0.6640869379043579, 'test/loss': 1.592431664466858, 'test/bleu': 26.96958389750291, 'test/num_examples': 3003, 'score': 27750.299865961075, 'total_duration': 46267.65453505516, 'accumulated_submission_time': 27750.299865961075, 'accumulated_eval_time': 18513.776450157166, 'accumulated_logging_time': 1.0562868118286133, 'global_step': 79473, 'preemption_count': 0}), (81881, {'train/accuracy': 0.6480662226676941, 'train/loss': 1.706380844116211, 'train/bleu': 31.98326599336038, 'validation/accuracy': 0.6571152210235596, 'validation/loss': 1.6383646726608276, 'validation/bleu': 27.902996214779897, 'validation/num_examples': 3000, 'test/accuracy': 0.6693974733352661, 'test/loss': 1.568246603012085, 'test/bleu': 27.44548816720142, 'test/num_examples': 3003, 'score': 28590.505031108856, 'total_duration': 47674.52781748772, 'accumulated_submission_time': 28590.505031108856, 'accumulated_eval_time': 19080.33007502556, 'accumulated_logging_time': 1.0926096439361572, 'global_step': 81881, 'preemption_count': 0}), (84289, {'train/accuracy': 0.6433086395263672, 'train/loss': 1.7546846866607666, 'train/bleu': 30.503595770525028, 'validation/accuracy': 0.6575739979743958, 'validation/loss': 1.631597876548767, 'validation/bleu': 27.817556353401674, 'validation/num_examples': 3000, 'test/accuracy': 0.6694555878639221, 'test/loss': 1.5594232082366943, 'test/bleu': 27.313128787818524, 'test/num_examples': 3003, 'score': 29430.63443851471, 'total_duration': 49078.12288761139, 'accumulated_submission_time': 29430.63443851471, 'accumulated_eval_time': 19643.68313574791, 'accumulated_logging_time': 1.1292719841003418, 'global_step': 84289, 'preemption_count': 0}), (86697, {'train/accuracy': 0.6414464712142944, 'train/loss': 1.7629821300506592, 'train/bleu': 30.919897634747738, 'validation/accuracy': 0.6594710350036621, 'validation/loss': 1.6154887676239014, 'validation/bleu': 27.643855522380758, 'validation/num_examples': 3000, 'test/accuracy': 0.6706408858299255, 'test/loss': 1.5513267517089844, 'test/bleu': 27.45172026614546, 'test/num_examples': 3003, 'score': 30270.717396259308, 'total_duration': 50467.73133611679, 'accumulated_submission_time': 30270.717396259308, 'accumulated_eval_time': 20193.092746973038, 'accumulated_logging_time': 1.167518138885498, 'global_step': 86697, 'preemption_count': 0}), (89105, {'train/accuracy': 0.6479898691177368, 'train/loss': 1.7014189958572388, 'train/bleu': 31.870726073454623, 'validation/accuracy': 0.664678692817688, 'validation/loss': 1.591239333152771, 'validation/bleu': 27.893390280404773, 'validation/num_examples': 3000, 'test/accuracy': 0.676021158695221, 'test/loss': 1.5245511531829834, 'test/bleu': 27.724933282657265, 'test/num_examples': 3003, 'score': 31110.686940193176, 'total_duration': 52011.69216299057, 'accumulated_submission_time': 31110.686940193176, 'accumulated_eval_time': 20896.967987298965, 'accumulated_logging_time': 1.2072625160217285, 'global_step': 89105, 'preemption_count': 0}), (91513, {'train/accuracy': 0.6436875462532043, 'train/loss': 1.7378581762313843, 'train/bleu': 31.34514009640573, 'validation/accuracy': 0.6646538972854614, 'validation/loss': 1.5829776525497437, 'validation/bleu': 28.456537818777825, 'validation/num_examples': 3000, 'test/accuracy': 0.6771948337554932, 'test/loss': 1.5109649896621704, 'test/bleu': 27.948103940076336, 'test/num_examples': 3003, 'score': 31950.802713632584, 'total_duration': 53386.21637392044, 'accumulated_submission_time': 31950.802713632584, 'accumulated_eval_time': 21431.260320663452, 'accumulated_logging_time': 1.246953010559082, 'global_step': 91513, 'preemption_count': 0}), (93921, {'train/accuracy': 0.6853712797164917, 'train/loss': 1.4687334299087524, 'train/bleu': 34.03844929654699, 'validation/accuracy': 0.6677164435386658, 'validation/loss': 1.5667170286178589, 'validation/bleu': 28.313632988196403, 'validation/num_examples': 3000, 'test/accuracy': 0.6811574101448059, 'test/loss': 1.4931126832962036, 'test/bleu': 28.463327942895543, 'test/num_examples': 3003, 'score': 32790.77405810356, 'total_duration': 54783.45065832138, 'accumulated_submission_time': 32790.77405810356, 'accumulated_eval_time': 21988.401491642, 'accumulated_logging_time': 1.2917671203613281, 'global_step': 93921, 'preemption_count': 0}), (96329, {'train/accuracy': 0.6571729779243469, 'train/loss': 1.6591682434082031, 'train/bleu': 31.706807133045615, 'validation/accuracy': 0.6698614954948425, 'validation/loss': 1.549560785293579, 'validation/bleu': 28.72916283015469, 'validation/num_examples': 3000, 'test/accuracy': 0.6832374930381775, 'test/loss': 1.4707233905792236, 'test/bleu': 28.521573655414183, 'test/num_examples': 3003, 'score': 33630.75802206993, 'total_duration': 56252.48037528992, 'accumulated_submission_time': 33630.75802206993, 'accumulated_eval_time': 22617.3301281929, 'accumulated_logging_time': 1.331843376159668, 'global_step': 96329, 'preemption_count': 0}), (98737, {'train/accuracy': 0.6514151692390442, 'train/loss': 1.690314531326294, 'train/bleu': 31.84842778417848, 'validation/accuracy': 0.6744739413261414, 'validation/loss': 1.5323798656463623, 'validation/bleu': 29.34685369574843, 'validation/num_examples': 3000, 'test/accuracy': 0.6842019557952881, 'test/loss': 1.4612979888916016, 'test/bleu': 28.679556840476543, 'test/num_examples': 3003, 'score': 34470.64178228378, 'total_duration': 57623.255120038986, 'accumulated_submission_time': 34470.64178228378, 'accumulated_eval_time': 23148.10334300995, 'accumulated_logging_time': 1.372218370437622, 'global_step': 98737, 'preemption_count': 0}), (101145, {'train/accuracy': 0.6625736951828003, 'train/loss': 1.604433298110962, 'train/bleu': 32.59588528405028, 'validation/accuracy': 0.6739407777786255, 'validation/loss': 1.5239664316177368, 'validation/bleu': 29.29180447115428, 'validation/num_examples': 3000, 'test/accuracy': 0.688176155090332, 'test/loss': 1.4475120306015015, 'test/bleu': 29.02306537809828, 'test/num_examples': 3003, 'score': 35310.656386613846, 'total_duration': 59013.03903198242, 'accumulated_submission_time': 35310.656386613846, 'accumulated_eval_time': 23697.752576589584, 'accumulated_logging_time': 1.4147746562957764, 'global_step': 101145, 'preemption_count': 0}), (103553, {'train/accuracy': 0.6571160554885864, 'train/loss': 1.6496434211730957, 'train/bleu': 32.57483756565918, 'validation/accuracy': 0.6794335842132568, 'validation/loss': 1.5055320262908936, 'validation/bleu': 29.719286400010557, 'validation/num_examples': 3000, 'test/accuracy': 0.6907559037208557, 'test/loss': 1.4269720315933228, 'test/bleu': 29.401809938841794, 'test/num_examples': 3003, 'score': 36150.72316431999, 'total_duration': 60513.682002067566, 'accumulated_submission_time': 36150.72316431999, 'accumulated_eval_time': 24358.2102060318, 'accumulated_logging_time': 1.4556384086608887, 'global_step': 103553, 'preemption_count': 0}), (105962, {'train/accuracy': 0.6626680493354797, 'train/loss': 1.6218568086624146, 'train/bleu': 32.79830573550928, 'validation/accuracy': 0.6800907254219055, 'validation/loss': 1.4952389001846313, 'validation/bleu': 29.620485713663335, 'validation/num_examples': 3000, 'test/accuracy': 0.6929289698600769, 'test/loss': 1.4130771160125732, 'test/bleu': 29.46607487977802, 'test/num_examples': 3003, 'score': 36990.85962986946, 'total_duration': 61945.61144256592, 'accumulated_submission_time': 36990.85962986946, 'accumulated_eval_time': 24949.888291597366, 'accumulated_logging_time': 1.4942302703857422, 'global_step': 105962, 'preemption_count': 0}), (108370, {'train/accuracy': 0.6737005710601807, 'train/loss': 1.5475784540176392, 'train/bleu': 33.575351495616545, 'validation/accuracy': 0.6833640933036804, 'validation/loss': 1.479899525642395, 'validation/bleu': 29.811560649958405, 'validation/num_examples': 3000, 'test/accuracy': 0.6960200071334839, 'test/loss': 1.3971692323684692, 'test/bleu': 29.64580006144558, 'test/num_examples': 3003, 'score': 37831.08743548393, 'total_duration': 63350.190529346466, 'accumulated_submission_time': 37831.08743548393, 'accumulated_eval_time': 25514.119074106216, 'accumulated_logging_time': 1.5336592197418213, 'global_step': 108370, 'preemption_count': 0}), (110781, {'train/accuracy': 0.6674370765686035, 'train/loss': 1.5858745574951172, 'train/bleu': 33.23845615206641, 'validation/accuracy': 0.6833516955375671, 'validation/loss': 1.4708929061889648, 'validation/bleu': 30.19010167590585, 'validation/num_examples': 3000, 'test/accuracy': 0.6975306868553162, 'test/loss': 1.3853771686553955, 'test/bleu': 29.50214132674396, 'test/num_examples': 3003, 'score': 38671.22283697128, 'total_duration': 64770.899523973465, 'accumulated_submission_time': 38671.22283697128, 'accumulated_eval_time': 26094.5704498291, 'accumulated_logging_time': 1.57651948928833, 'global_step': 110781, 'preemption_count': 0}), (113188, {'train/accuracy': 0.686324417591095, 'train/loss': 1.4715728759765625, 'train/bleu': 34.47618891337282, 'validation/accuracy': 0.6864266991615295, 'validation/loss': 1.4545800685882568, 'validation/bleu': 29.93864461495176, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3694384098052979, 'test/bleu': 29.955883103222526, 'test/num_examples': 3003, 'score': 39511.19406795502, 'total_duration': 66233.1094198227, 'accumulated_submission_time': 39511.19406795502, 'accumulated_eval_time': 26716.692593574524, 'accumulated_logging_time': 1.61653470993042, 'global_step': 113188, 'preemption_count': 0}), (115596, {'train/accuracy': 0.6802150011062622, 'train/loss': 1.505571722984314, 'train/bleu': 34.051214524478446, 'validation/accuracy': 0.6871086359024048, 'validation/loss': 1.4513825178146362, 'validation/bleu': 30.445699454155303, 'validation/num_examples': 3000, 'test/accuracy': 0.7025042176246643, 'test/loss': 1.358932375907898, 'test/bleu': 30.259197723267093, 'test/num_examples': 3003, 'score': 40351.23788332939, 'total_duration': 67599.56405115128, 'accumulated_submission_time': 40351.23788332939, 'accumulated_eval_time': 27242.97893857956, 'accumulated_logging_time': 1.6643104553222656, 'global_step': 115596, 'preemption_count': 0}), (118005, {'train/accuracy': 0.680402934551239, 'train/loss': 1.5085806846618652, 'train/bleu': 34.025486843982236, 'validation/accuracy': 0.6894644498825073, 'validation/loss': 1.437278389930725, 'validation/bleu': 30.400086035153965, 'validation/num_examples': 3000, 'test/accuracy': 0.7049096822738647, 'test/loss': 1.342985987663269, 'test/bleu': 30.587941780081074, 'test/num_examples': 3003, 'score': 41191.43535208702, 'total_duration': 68923.48609733582, 'accumulated_submission_time': 41191.43535208702, 'accumulated_eval_time': 27726.58704996109, 'accumulated_logging_time': 1.705122947692871, 'global_step': 118005, 'preemption_count': 0}), (120413, {'train/accuracy': 0.6904399991035461, 'train/loss': 1.4429056644439697, 'train/bleu': 35.09718083113306, 'validation/accuracy': 0.6901216506958008, 'validation/loss': 1.4297069311141968, 'validation/bleu': 30.587810108397143, 'validation/num_examples': 3000, 'test/accuracy': 0.7053279876708984, 'test/loss': 1.3383585214614868, 'test/bleu': 30.600486228597944, 'test/num_examples': 3003, 'score': 42031.62937164307, 'total_duration': 70343.08654594421, 'accumulated_submission_time': 42031.62937164307, 'accumulated_eval_time': 28305.865301132202, 'accumulated_logging_time': 1.7545392513275146, 'global_step': 120413, 'preemption_count': 0}), (122820, {'train/accuracy': 0.688624382019043, 'train/loss': 1.460661768913269, 'train/bleu': 35.01808765515029, 'validation/accuracy': 0.6914607286453247, 'validation/loss': 1.4257858991622925, 'validation/bleu': 30.7654477246006, 'validation/num_examples': 3000, 'test/accuracy': 0.7075591087341309, 'test/loss': 1.3309905529022217, 'test/bleu': 30.70963575294762, 'test/num_examples': 3003, 'score': 42871.6183693409, 'total_duration': 71760.67544698715, 'accumulated_submission_time': 42871.6183693409, 'accumulated_eval_time': 28883.33780145645, 'accumulated_logging_time': 1.80470871925354, 'global_step': 122820, 'preemption_count': 0}), (125228, {'train/accuracy': 0.699196994304657, 'train/loss': 1.396242618560791, 'train/bleu': 36.034047007172774, 'validation/accuracy': 0.6935189962387085, 'validation/loss': 1.4201627969741821, 'validation/bleu': 31.030971558360275, 'validation/num_examples': 3000, 'test/accuracy': 0.7094881534576416, 'test/loss': 1.3231710195541382, 'test/bleu': 30.738778424579746, 'test/num_examples': 3003, 'score': 43711.80949354172, 'total_duration': 73148.70480275154, 'accumulated_submission_time': 43711.80949354172, 'accumulated_eval_time': 29431.057413339615, 'accumulated_logging_time': 1.8475875854492188, 'global_step': 125228, 'preemption_count': 0}), (127636, {'train/accuracy': 0.696530818939209, 'train/loss': 1.4135314226150513, 'train/bleu': 35.25999172604652, 'validation/accuracy': 0.6937545537948608, 'validation/loss': 1.4164506196975708, 'validation/bleu': 31.00998771095735, 'validation/num_examples': 3000, 'test/accuracy': 0.7097554206848145, 'test/loss': 1.319035291671753, 'test/bleu': 30.757323033480343, 'test/num_examples': 3003, 'score': 44551.95158267021, 'total_duration': 74541.41567277908, 'accumulated_submission_time': 44551.95158267021, 'accumulated_eval_time': 29983.50764322281, 'accumulated_logging_time': 1.889930009841919, 'global_step': 127636, 'preemption_count': 0}), (130043, {'train/accuracy': 0.6964461803436279, 'train/loss': 1.4212472438812256, 'train/bleu': 35.426351940087365, 'validation/accuracy': 0.693804144859314, 'validation/loss': 1.4148198366165161, 'validation/bleu': 30.977180725630046, 'validation/num_examples': 3000, 'test/accuracy': 0.7098251581192017, 'test/loss': 1.3173716068267822, 'test/bleu': 30.876633593370595, 'test/num_examples': 3003, 'score': 45392.100281476974, 'total_duration': 75921.76205563545, 'accumulated_submission_time': 45392.100281476974, 'accumulated_eval_time': 30523.580441236496, 'accumulated_logging_time': 1.9321520328521729, 'global_step': 130043, 'preemption_count': 0}), (132451, {'train/accuracy': 0.6961711049079895, 'train/loss': 1.419246792793274, 'train/bleu': 35.54502780300605, 'validation/accuracy': 0.6945357322692871, 'validation/loss': 1.414249300956726, 'validation/bleu': 31.107725195668912, 'validation/num_examples': 3000, 'test/accuracy': 0.7100808024406433, 'test/loss': 1.3164043426513672, 'test/bleu': 30.8854601382876, 'test/num_examples': 3003, 'score': 46232.19126367569, 'total_duration': 77297.3417544365, 'accumulated_submission_time': 46232.19126367569, 'accumulated_eval_time': 31058.94911122322, 'accumulated_logging_time': 1.9750447273254395, 'global_step': 132451, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6953193545341492, 'train/loss': 1.4255940914154053, 'train/bleu': 35.78710381048599, 'validation/accuracy': 0.6945977210998535, 'validation/loss': 1.4143544435501099, 'validation/bleu': 31.089424411746926, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.316562533378601, 'test/bleu': 30.94145452506206, 'test/num_examples': 3003, 'score': 46539.62941074371, 'total_duration': 78143.05393791199, 'accumulated_submission_time': 46539.62941074371, 'accumulated_eval_time': 31597.15144467354, 'accumulated_logging_time': 2.018925905227661, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0210 00:05:12.312686 139785736898368 submission_runner.py:586] Timing: 46539.62941074371
I0210 00:05:12.312742 139785736898368 submission_runner.py:588] Total number of evals: 57
I0210 00:05:12.312787 139785736898368 submission_runner.py:589] ====================
I0210 00:05:12.312834 139785736898368 submission_runner.py:542] Using RNG seed 3586669017
I0210 00:05:12.314579 139785736898368 submission_runner.py:551] --- Tuning run 5/5 ---
I0210 00:05:12.314701 139785736898368 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5.
I0210 00:05:12.315151 139785736898368 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5/hparams.json.
I0210 00:05:12.316030 139785736898368 submission_runner.py:206] Initializing dataset.
I0210 00:05:12.318862 139785736898368 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 00:05:12.322095 139785736898368 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 00:05:12.363893 139785736898368 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 00:05:13.074635 139785736898368 submission_runner.py:213] Initializing model.
I0210 00:05:19.954078 139785736898368 submission_runner.py:255] Initializing optimizer.
I0210 00:05:20.776103 139785736898368 submission_runner.py:262] Initializing metrics bundle.
I0210 00:05:20.776284 139785736898368 submission_runner.py:280] Initializing checkpoint and logger.
I0210 00:05:20.777468 139785736898368 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5 with prefix checkpoint_
I0210 00:05:20.777601 139785736898368 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5/meta_data_0.json.
I0210 00:05:20.777826 139785736898368 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 00:05:20.777891 139785736898368 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 00:05:21.286184 139785736898368 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 00:05:21.784222 139785736898368 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5/flags_0.json.
I0210 00:05:21.787930 139785736898368 submission_runner.py:314] Starting training loop.
I0210 00:05:51.838665 139615864289024 logging_writer.py:48] [0] global_step=0, grad_norm=4.717680931091309, loss=11.032805442810059
I0210 00:05:51.853089 139785736898368 spec.py:321] Evaluating on the training split.
I0210 00:05:54.551437 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:10:42.126460 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 00:10:44.847764 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:15:32.054405 139785736898368 spec.py:349] Evaluating on the test split.
I0210 00:15:34.773185 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:20:21.733707 139785736898368 submission_runner.py:408] Time since start: 899.95s, 	Step: 1, 	{'train/accuracy': 0.0006145372171886265, 'train/loss': 11.063870429992676, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.065114498138428, 'total_duration': 899.9457039833069, 'accumulated_submission_time': 30.065114498138428, 'accumulated_eval_time': 869.8805475234985, 'accumulated_logging_time': 0}
I0210 00:20:21.742587 139615872681728 logging_writer.py:48] [1] accumulated_eval_time=869.880548, accumulated_logging_time=0, accumulated_submission_time=30.065114, global_step=1, preemption_count=0, score=30.065114, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.041826, test/num_examples=3003, total_duration=899.945704, train/accuracy=0.000615, train/bleu=0.000000, train/loss=11.063870, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.036646, validation/num_examples=3000
I0210 00:20:57.517161 139615864289024 logging_writer.py:48] [100] global_step=100, grad_norm=0.16883128881454468, loss=8.255187034606934
I0210 00:21:33.308616 139615872681728 logging_writer.py:48] [200] global_step=200, grad_norm=0.5680549740791321, loss=7.4329071044921875
I0210 00:22:09.105562 139615864289024 logging_writer.py:48] [300] global_step=300, grad_norm=0.600547194480896, loss=6.877121448516846
I0210 00:22:44.912405 139615872681728 logging_writer.py:48] [400] global_step=400, grad_norm=0.45084455609321594, loss=6.282824993133545
I0210 00:23:20.746527 139615864289024 logging_writer.py:48] [500] global_step=500, grad_norm=0.36807557940483093, loss=5.86079216003418
I0210 00:23:56.548227 139615872681728 logging_writer.py:48] [600] global_step=600, grad_norm=0.3561907708644867, loss=5.561834812164307
I0210 00:24:32.355971 139615864289024 logging_writer.py:48] [700] global_step=700, grad_norm=0.4322507083415985, loss=5.26604700088501
I0210 00:25:08.157784 139615872681728 logging_writer.py:48] [800] global_step=800, grad_norm=0.712771475315094, loss=4.982021808624268
I0210 00:25:44.033081 139615864289024 logging_writer.py:48] [900] global_step=900, grad_norm=0.5225725769996643, loss=4.81382417678833
I0210 00:26:19.844123 139615872681728 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5303153991699219, loss=4.556842803955078
I0210 00:26:55.610379 139615864289024 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5395450592041016, loss=4.271928310394287
I0210 00:27:31.385777 139615872681728 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5537109971046448, loss=4.115427017211914
I0210 00:28:07.195391 139615864289024 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5937427878379822, loss=4.03533935546875
I0210 00:28:42.956178 139615872681728 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5307892560958862, loss=3.7172913551330566
I0210 00:29:18.768229 139615864289024 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6158642768859863, loss=3.6267144680023193
I0210 00:29:54.549396 139615872681728 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6859300136566162, loss=3.566025495529175
I0210 00:30:30.322492 139615864289024 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6303173899650574, loss=3.5168769359588623
I0210 00:31:06.095359 139615872681728 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5276762247085571, loss=3.335792064666748
I0210 00:31:41.863862 139615864289024 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4316152334213257, loss=3.2170443534851074
I0210 00:32:17.669088 139615872681728 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3901920020580292, loss=3.188032865524292
I0210 00:32:53.431362 139615864289024 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.41922494769096375, loss=3.085103988647461
I0210 00:33:29.267518 139615872681728 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3804072141647339, loss=3.083364725112915
I0210 00:34:05.052045 139615864289024 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5281038284301758, loss=3.052269458770752
I0210 00:34:21.963153 139785736898368 spec.py:321] Evaluating on the training split.
I0210 00:34:24.957670 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:37:09.742544 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 00:37:12.444735 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:39:50.714661 139785736898368 spec.py:349] Evaluating on the test split.
I0210 00:39:53.448369 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:42:23.704389 139785736898368 submission_runner.py:408] Time since start: 2221.92s, 	Step: 2349, 	{'train/accuracy': 0.5141674280166626, 'train/loss': 2.8809711933135986, 'train/bleu': 22.965880254505947, 'validation/accuracy': 0.5140295624732971, 'validation/loss': 2.860582113265991, 'validation/bleu': 18.528226390624614, 'validation/num_examples': 3000, 'test/accuracy': 0.5108012557029724, 'test/loss': 2.912444591522217, 'test/bleu': 16.717221966085745, 'test/num_examples': 3003, 'score': 870.2016100883484, 'total_duration': 2221.916384935379, 'accumulated_submission_time': 870.2016100883484, 'accumulated_eval_time': 1351.6217308044434, 'accumulated_logging_time': 0.01892399787902832}
I0210 00:42:23.719064 139615872681728 logging_writer.py:48] [2349] accumulated_eval_time=1351.621731, accumulated_logging_time=0.018924, accumulated_submission_time=870.201610, global_step=2349, preemption_count=0, score=870.201610, test/accuracy=0.510801, test/bleu=16.717222, test/loss=2.912445, test/num_examples=3003, total_duration=2221.916385, train/accuracy=0.514167, train/bleu=22.965880, train/loss=2.880971, validation/accuracy=0.514030, validation/bleu=18.528226, validation/loss=2.860582, validation/num_examples=3000
I0210 00:42:42.367815 139615864289024 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3794611096382141, loss=2.9273269176483154
I0210 00:43:18.187052 139615872681728 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3284659683704376, loss=2.8416860103607178
I0210 00:43:53.991257 139615864289024 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.3135361969470978, loss=2.8681955337524414
I0210 00:44:29.741889 139615872681728 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3783254623413086, loss=2.8418571949005127
I0210 00:45:05.492241 139615864289024 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.35523056983947754, loss=2.7884206771850586
I0210 00:45:41.275855 139615872681728 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.30884793400764465, loss=2.7859244346618652
I0210 00:46:17.030788 139615864289024 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.31046393513679504, loss=2.7306082248687744
I0210 00:46:52.819299 139615872681728 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2697618305683136, loss=2.545262575149536
I0210 00:47:28.585410 139615864289024 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.228819340467453, loss=2.681906223297119
I0210 00:48:04.341804 139615872681728 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.24443203210830688, loss=2.6144859790802
I0210 00:48:40.072803 139615864289024 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2518607974052429, loss=2.546579360961914
I0210 00:49:15.857739 139615872681728 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.22732208669185638, loss=2.4908924102783203
I0210 00:49:51.646565 139615864289024 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.2210875153541565, loss=2.6612794399261475
I0210 00:50:27.386863 139615872681728 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2058108001947403, loss=2.4807932376861572
I0210 00:51:03.157468 139615864289024 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.18980441987514496, loss=2.474533796310425
I0210 00:51:38.921225 139615872681728 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.23309794068336487, loss=2.4362804889678955
I0210 00:52:14.671496 139615864289024 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.2044444978237152, loss=2.5289933681488037
I0210 00:52:50.417737 139615872681728 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.18440906703472137, loss=2.280914783477783
I0210 00:53:26.154891 139615864289024 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.2084263563156128, loss=2.3985421657562256
I0210 00:54:01.929836 139615872681728 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.18668155372142792, loss=2.481980085372925
I0210 00:54:37.686287 139615864289024 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.1824769675731659, loss=2.379645347595215
I0210 00:55:13.438423 139615872681728 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.1879587024450302, loss=2.3139469623565674
I0210 00:55:49.180900 139615864289024 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.19456416368484497, loss=2.3426694869995117
I0210 00:56:23.955317 139785736898368 spec.py:321] Evaluating on the training split.
I0210 00:56:26.961196 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 00:59:13.508739 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 00:59:16.246865 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:01:52.482569 139785736898368 spec.py:349] Evaluating on the test split.
I0210 01:01:55.227405 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:04:14.157208 139785736898368 submission_runner.py:408] Time since start: 3532.37s, 	Step: 4699, 	{'train/accuracy': 0.5758695602416992, 'train/loss': 2.2680907249450684, 'train/bleu': 27.232874276111815, 'validation/accuracy': 0.589490532875061, 'validation/loss': 2.1476547718048096, 'validation/bleu': 23.513794603427275, 'validation/num_examples': 3000, 'test/accuracy': 0.5915867686271667, 'test/loss': 2.1269924640655518, 'test/bleu': 22.13067104185569, 'test/num_examples': 3003, 'score': 1710.351322889328, 'total_duration': 3532.369195461273, 'accumulated_submission_time': 1710.351322889328, 'accumulated_eval_time': 1821.823566198349, 'accumulated_logging_time': 0.04367566108703613}
I0210 01:04:14.172277 139615872681728 logging_writer.py:48] [4699] accumulated_eval_time=1821.823566, accumulated_logging_time=0.043676, accumulated_submission_time=1710.351323, global_step=4699, preemption_count=0, score=1710.351323, test/accuracy=0.591587, test/bleu=22.130671, test/loss=2.126992, test/num_examples=3003, total_duration=3532.369195, train/accuracy=0.575870, train/bleu=27.232874, train/loss=2.268091, validation/accuracy=0.589491, validation/bleu=23.513795, validation/loss=2.147655, validation/num_examples=3000
I0210 01:04:14.910344 139615864289024 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1769009828567505, loss=2.27691650390625
I0210 01:04:50.671898 139615872681728 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1897880733013153, loss=2.3698232173919678
I0210 01:05:26.423465 139615864289024 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.15938332676887512, loss=2.270026445388794
I0210 01:06:02.198993 139615872681728 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1598382294178009, loss=2.256950855255127
I0210 01:06:37.945286 139615864289024 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.15338541567325592, loss=2.317497968673706
I0210 01:07:13.702164 139615872681728 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.20335571467876434, loss=2.285845994949341
I0210 01:07:49.452146 139615864289024 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.16538292169570923, loss=2.2856945991516113
I0210 01:08:25.209237 139615872681728 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.15247993171215057, loss=2.280139446258545
I0210 01:09:01.020676 139615864289024 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.15306250751018524, loss=2.198920726776123
I0210 01:09:36.808538 139615872681728 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.1611771285533905, loss=2.2097294330596924
I0210 01:10:12.611840 139615864289024 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.1690148264169693, loss=2.285224676132202
I0210 01:10:48.390797 139615872681728 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.14989954233169556, loss=2.233389377593994
I0210 01:11:24.118970 139615864289024 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.1457691341638565, loss=2.254727363586426
I0210 01:11:59.873692 139615872681728 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.1807582527399063, loss=2.222018003463745
I0210 01:12:35.615322 139615864289024 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.16176840662956238, loss=2.1810245513916016
I0210 01:13:11.360465 139615872681728 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.16350960731506348, loss=2.2109506130218506
I0210 01:13:47.111448 139615864289024 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.18813221156597137, loss=2.1436004638671875
I0210 01:14:22.832695 139615872681728 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.19042523205280304, loss=2.238436698913574
I0210 01:14:58.620528 139615864289024 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1661834716796875, loss=2.2059881687164307
I0210 01:15:34.383720 139615872681728 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.18049897253513336, loss=2.204408884048462
I0210 01:16:10.190199 139615864289024 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.15139970183372498, loss=2.1499581336975098
I0210 01:16:45.948029 139615872681728 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1569814831018448, loss=2.1518640518188477
I0210 01:17:21.726123 139615864289024 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.20707538723945618, loss=2.1808645725250244
I0210 01:17:57.473283 139615872681728 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.148636132478714, loss=2.148963689804077
I0210 01:18:14.360672 139785736898368 spec.py:321] Evaluating on the training split.
I0210 01:18:17.355813 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:20:54.667708 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 01:20:57.395601 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:23:34.549049 139785736898368 spec.py:349] Evaluating on the test split.
I0210 01:23:37.295003 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:26:00.202611 139785736898368 submission_runner.py:408] Time since start: 4838.41s, 	Step: 7049, 	{'train/accuracy': 0.6065484881401062, 'train/loss': 1.9995412826538086, 'train/bleu': 29.012708543686976, 'validation/accuracy': 0.6178224682807922, 'validation/loss': 1.9245233535766602, 'validation/bleu': 25.410653443752153, 'validation/num_examples': 3000, 'test/accuracy': 0.620010495185852, 'test/loss': 1.8880869150161743, 'test/bleu': 23.77728489354792, 'test/num_examples': 3003, 'score': 2550.4531705379486, 'total_duration': 4838.414577007294, 'accumulated_submission_time': 2550.4531705379486, 'accumulated_eval_time': 2287.665424346924, 'accumulated_logging_time': 0.06898212432861328}
I0210 01:26:00.221198 139615864289024 logging_writer.py:48] [7049] accumulated_eval_time=2287.665424, accumulated_logging_time=0.068982, accumulated_submission_time=2550.453171, global_step=7049, preemption_count=0, score=2550.453171, test/accuracy=0.620010, test/bleu=23.777285, test/loss=1.888087, test/num_examples=3003, total_duration=4838.414577, train/accuracy=0.606548, train/bleu=29.012709, train/loss=1.999541, validation/accuracy=0.617822, validation/bleu=25.410653, validation/loss=1.924523, validation/num_examples=3000
I0210 01:26:18.834921 139615872681728 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.16762614250183105, loss=2.121291399002075
I0210 01:26:54.570030 139615864289024 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.2055281400680542, loss=2.1868488788604736
I0210 01:27:30.347670 139615872681728 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.17960071563720703, loss=2.0853819847106934
I0210 01:28:06.079255 139615864289024 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.19595901668071747, loss=2.117363452911377
I0210 01:28:41.866289 139615872681728 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.152887225151062, loss=2.0508036613464355
I0210 01:29:17.586459 139615864289024 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.15790677070617676, loss=2.1878767013549805
I0210 01:29:53.370032 139615872681728 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.2434738576412201, loss=2.00032639503479
I0210 01:30:29.104847 139615864289024 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.15961457788944244, loss=2.01170015335083
I0210 01:31:04.861164 139615872681728 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.19171732664108276, loss=2.0401923656463623
I0210 01:31:40.611391 139615864289024 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.16592665016651154, loss=2.205232620239258
I0210 01:32:16.343754 139615872681728 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.2284380942583084, loss=2.0254273414611816
I0210 01:32:52.065607 139615864289024 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17745167016983032, loss=2.0843284130096436
I0210 01:33:27.811765 139615872681728 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.1854744255542755, loss=2.0671637058258057
I0210 01:34:03.606947 139615864289024 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.16416819393634796, loss=2.047502040863037
I0210 01:34:39.574621 139615872681728 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.19956304132938385, loss=2.1391713619232178
I0210 01:35:15.336128 139615864289024 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.17019250988960266, loss=2.054396390914917
I0210 01:35:51.085868 139615872681728 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.1822863072156906, loss=2.0576322078704834
I0210 01:36:26.885362 139615864289024 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1709715873003006, loss=2.1256515979766846
I0210 01:37:02.632104 139615872681728 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.21023358404636383, loss=2.0194785594940186
I0210 01:37:38.360060 139615864289024 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15616872906684875, loss=2.011051893234253
I0210 01:38:14.145523 139615872681728 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.18001842498779297, loss=2.0787854194641113
I0210 01:38:49.917158 139615864289024 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.2297849953174591, loss=2.1427135467529297
I0210 01:39:25.655255 139615872681728 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.15068119764328003, loss=1.970709204673767
I0210 01:40:00.404266 139785736898368 spec.py:321] Evaluating on the training split.
I0210 01:40:03.404174 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:43:34.025320 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 01:43:36.741344 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:46:17.773251 139785736898368 spec.py:349] Evaluating on the test split.
I0210 01:46:20.505946 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 01:48:29.053503 139785736898368 submission_runner.py:408] Time since start: 6187.27s, 	Step: 9399, 	{'train/accuracy': 0.6141012907028198, 'train/loss': 1.939665675163269, 'train/bleu': 29.74698626680871, 'validation/accuracy': 0.6299859881401062, 'validation/loss': 1.8144594430923462, 'validation/bleu': 26.071460310919417, 'validation/num_examples': 3000, 'test/accuracy': 0.6367672085762024, 'test/loss': 1.7608153820037842, 'test/bleu': 25.14278859816317, 'test/num_examples': 3003, 'score': 3390.548141002655, 'total_duration': 6187.265499591827, 'accumulated_submission_time': 3390.548141002655, 'accumulated_eval_time': 2796.3146154880524, 'accumulated_logging_time': 0.09975481033325195}
I0210 01:48:29.069436 139615864289024 logging_writer.py:48] [9399] accumulated_eval_time=2796.314615, accumulated_logging_time=0.099755, accumulated_submission_time=3390.548141, global_step=9399, preemption_count=0, score=3390.548141, test/accuracy=0.636767, test/bleu=25.142789, test/loss=1.760815, test/num_examples=3003, total_duration=6187.265500, train/accuracy=0.614101, train/bleu=29.746986, train/loss=1.939666, validation/accuracy=0.629986, validation/bleu=26.071460, validation/loss=1.814459, validation/num_examples=3000
I0210 01:48:29.801582 139615872681728 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.1760852038860321, loss=2.099191188812256
I0210 01:49:05.547595 139615864289024 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.216653972864151, loss=2.0452005863189697
I0210 01:49:41.307756 139615872681728 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.1745612472295761, loss=2.0004851818084717
I0210 01:50:17.070545 139615864289024 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1790839433670044, loss=1.9775465726852417
I0210 01:50:52.827325 139615872681728 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.18613918125629425, loss=2.061217784881592
I0210 01:51:28.552716 139615864289024 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.19143348932266235, loss=1.9897801876068115
I0210 01:52:04.332261 139615872681728 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.18226204812526703, loss=1.9316810369491577
I0210 01:52:40.069011 139615864289024 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.14328986406326294, loss=1.9763997793197632
I0210 01:53:15.801067 139615872681728 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.17797046899795532, loss=2.043987989425659
I0210 01:53:51.534009 139615864289024 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.17336826026439667, loss=1.8982981443405151
I0210 01:54:27.310084 139615872681728 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.1788327991962433, loss=1.9239736795425415
I0210 01:55:03.100223 139615864289024 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.1886075735092163, loss=2.0306215286254883
I0210 01:55:38.863281 139615872681728 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.18730995059013367, loss=1.9563181400299072
I0210 01:56:14.657618 139615864289024 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.18015778064727783, loss=2.0236637592315674
I0210 01:56:50.407265 139615872681728 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1753416508436203, loss=2.009138345718384
I0210 01:57:26.185440 139615864289024 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.16605235636234283, loss=1.9624511003494263
I0210 01:58:01.987110 139615872681728 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.2378409504890442, loss=2.0648863315582275
I0210 01:58:37.732841 139615864289024 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.22569890320301056, loss=1.9571771621704102
I0210 01:59:13.457953 139615872681728 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.33621665835380554, loss=1.9792907238006592
I0210 01:59:49.268662 139615864289024 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.18194955587387085, loss=2.038353681564331
I0210 02:00:25.063009 139615872681728 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2336759716272354, loss=1.9089858531951904
I0210 02:01:00.855909 139615864289024 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1786067932844162, loss=2.063594341278076
I0210 02:01:36.627031 139615872681728 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.26570940017700195, loss=1.9212557077407837
I0210 02:02:12.428803 139615864289024 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.21869109570980072, loss=1.9688576459884644
I0210 02:02:29.311385 139785736898368 spec.py:321] Evaluating on the training split.
I0210 02:02:32.308849 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:05:51.426930 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 02:05:54.127256 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:08:18.382755 139785736898368 spec.py:349] Evaluating on the test split.
I0210 02:08:21.091475 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:10:38.604068 139785736898368 submission_runner.py:408] Time since start: 7516.82s, 	Step: 11749, 	{'train/accuracy': 0.6164451241493225, 'train/loss': 1.9157977104187012, 'train/bleu': 29.745159669492597, 'validation/accuracy': 0.6390249133110046, 'validation/loss': 1.7488797903060913, 'validation/bleu': 26.594450639942533, 'validation/num_examples': 3000, 'test/accuracy': 0.6456801295280457, 'test/loss': 1.6872280836105347, 'test/bleu': 25.485975409278893, 'test/num_examples': 3003, 'score': 4230.704621553421, 'total_duration': 7516.816064357758, 'accumulated_submission_time': 4230.704621553421, 'accumulated_eval_time': 3285.607243537903, 'accumulated_logging_time': 0.12725400924682617}
I0210 02:10:38.620565 139615872681728 logging_writer.py:48] [11749] accumulated_eval_time=3285.607244, accumulated_logging_time=0.127254, accumulated_submission_time=4230.704622, global_step=11749, preemption_count=0, score=4230.704622, test/accuracy=0.645680, test/bleu=25.485975, test/loss=1.687228, test/num_examples=3003, total_duration=7516.816064, train/accuracy=0.616445, train/bleu=29.745160, train/loss=1.915798, validation/accuracy=0.639025, validation/bleu=26.594451, validation/loss=1.748880, validation/num_examples=3000
I0210 02:10:57.227866 139615864289024 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2944546639919281, loss=1.949368953704834
I0210 02:11:32.968976 139615872681728 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.18599113821983337, loss=2.000021457672119
I0210 02:12:08.745009 139615864289024 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.19508680701255798, loss=2.0239038467407227
I0210 02:12:44.476574 139615872681728 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.19868645071983337, loss=1.9359846115112305
I0210 02:13:20.218775 139615864289024 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.16393478214740753, loss=1.933384895324707
I0210 02:13:55.961150 139615872681728 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1696675717830658, loss=1.9956740140914917
I0210 02:14:31.689842 139615864289024 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.15793073177337646, loss=1.885757327079773
I0210 02:15:07.421768 139615872681728 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.18337637186050415, loss=1.9930219650268555
I0210 02:15:43.176248 139615864289024 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.16205036640167236, loss=1.9104163646697998
I0210 02:16:18.920241 139615872681728 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.23368246853351593, loss=1.9410429000854492
I0210 02:16:54.680400 139615864289024 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.19445033371448517, loss=2.0532174110412598
I0210 02:17:30.456892 139615872681728 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.23594719171524048, loss=1.8515568971633911
I0210 02:18:06.183809 139615864289024 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.16364626586437225, loss=1.9598010778427124
I0210 02:18:41.917266 139615872681728 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.27796366810798645, loss=2.0105936527252197
I0210 02:19:17.666173 139615864289024 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.1776924878358841, loss=1.989572286605835
I0210 02:19:53.444186 139615872681728 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.1861177384853363, loss=1.9887897968292236
I0210 02:20:29.280264 139615864289024 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.22202186286449432, loss=1.9505747556686401
I0210 02:21:05.072986 139615872681728 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.18226595222949982, loss=2.0082263946533203
I0210 02:21:40.807079 139615864289024 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.22805634140968323, loss=1.9068260192871094
I0210 02:22:16.537939 139615872681728 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.18683618307113647, loss=1.9420698881149292
I0210 02:22:52.293079 139615864289024 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.16409529745578766, loss=1.8955073356628418
I0210 02:23:28.093926 139615872681728 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.19414964318275452, loss=1.9006668329238892
I0210 02:24:03.853842 139615864289024 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.18945495784282684, loss=1.941402792930603
I0210 02:24:38.610534 139785736898368 spec.py:321] Evaluating on the training split.
I0210 02:24:41.622073 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:28:18.983291 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 02:28:21.701115 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:30:56.421777 139785736898368 spec.py:349] Evaluating on the test split.
I0210 02:30:59.115386 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:33:15.457828 139785736898368 submission_runner.py:408] Time since start: 8873.67s, 	Step: 14099, 	{'train/accuracy': 0.6286578178405762, 'train/loss': 1.8235194683074951, 'train/bleu': 30.895011292840326, 'validation/accuracy': 0.6437737941741943, 'validation/loss': 1.7144992351531982, 'validation/bleu': 26.906824146885256, 'validation/num_examples': 3000, 'test/accuracy': 0.651943564414978, 'test/loss': 1.6426197290420532, 'test/bleu': 26.12009908804103, 'test/num_examples': 3003, 'score': 5070.610538482666, 'total_duration': 8873.669828891754, 'accumulated_submission_time': 5070.610538482666, 'accumulated_eval_time': 3802.4545085430145, 'accumulated_logging_time': 0.15345525741577148}
I0210 02:33:15.474005 139615872681728 logging_writer.py:48] [14099] accumulated_eval_time=3802.454509, accumulated_logging_time=0.153455, accumulated_submission_time=5070.610538, global_step=14099, preemption_count=0, score=5070.610538, test/accuracy=0.651944, test/bleu=26.120099, test/loss=1.642620, test/num_examples=3003, total_duration=8873.669829, train/accuracy=0.628658, train/bleu=30.895011, train/loss=1.823519, validation/accuracy=0.643774, validation/bleu=26.906824, validation/loss=1.714499, validation/num_examples=3000
I0210 02:33:16.213972 139615864289024 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.21434961259365082, loss=1.9534329175949097
I0210 02:33:51.950243 139615872681728 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.38837650418281555, loss=1.8810343742370605
I0210 02:34:27.696119 139615864289024 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1963026523590088, loss=1.8915743827819824
I0210 02:35:03.448529 139615872681728 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.1931719034910202, loss=1.9055912494659424
I0210 02:35:39.235314 139615864289024 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2195052057504654, loss=1.8909852504730225
I0210 02:36:14.955075 139615872681728 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.22070735692977905, loss=1.9734928607940674
I0210 02:36:50.701371 139615864289024 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17968882620334625, loss=1.887303113937378
I0210 02:37:26.497293 139615872681728 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.24976220726966858, loss=1.9511297941207886
I0210 02:38:02.235260 139615864289024 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.18213969469070435, loss=1.9221469163894653
I0210 02:38:38.020094 139615872681728 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18868693709373474, loss=1.936326503753662
I0210 02:39:13.800265 139615864289024 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.26639294624328613, loss=1.9065184593200684
I0210 02:39:49.581001 139615872681728 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.1937120109796524, loss=1.9326380491256714
I0210 02:40:25.319863 139615864289024 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.18517464399337769, loss=1.939832091331482
I0210 02:41:01.085779 139615872681728 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1914072334766388, loss=1.927573323249817
I0210 02:41:36.808267 139615864289024 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.17339245975017548, loss=1.9030691385269165
I0210 02:42:12.575919 139615872681728 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.21705202758312225, loss=1.9917482137680054
I0210 02:42:48.346084 139615864289024 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1973988115787506, loss=1.9690998792648315
I0210 02:43:24.074176 139615872681728 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.17160235345363617, loss=1.8507490158081055
I0210 02:43:59.848618 139615864289024 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2325967699289322, loss=1.9604748487472534
I0210 02:44:35.613303 139615872681728 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2477150410413742, loss=1.7979985475540161
I0210 02:45:11.350392 139615864289024 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.19544333219528198, loss=1.937362551689148
I0210 02:45:47.124517 139615872681728 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.16985827684402466, loss=1.8333431482315063
I0210 02:46:22.886804 139615864289024 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1746603101491928, loss=1.9258562326431274
I0210 02:46:58.676621 139615872681728 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.18332774937152863, loss=1.8401302099227905
I0210 02:47:15.552812 139785736898368 spec.py:321] Evaluating on the training split.
I0210 02:47:18.541410 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:50:38.446826 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 02:50:41.157113 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:53:13.798219 139785736898368 spec.py:349] Evaluating on the test split.
I0210 02:53:16.519387 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 02:55:34.173735 139785736898368 submission_runner.py:408] Time since start: 10212.39s, 	Step: 16449, 	{'train/accuracy': 0.6269233226776123, 'train/loss': 1.8308695554733276, 'train/bleu': 30.456122777081536, 'validation/accuracy': 0.6486342549324036, 'validation/loss': 1.6799792051315308, 'validation/bleu': 27.54467483066716, 'validation/num_examples': 3000, 'test/accuracy': 0.6589855551719666, 'test/loss': 1.607773780822754, 'test/bleu': 26.91923126334531, 'test/num_examples': 3003, 'score': 5910.604150533676, 'total_duration': 10212.38568687439, 'accumulated_submission_time': 5910.604150533676, 'accumulated_eval_time': 4301.075335741043, 'accumulated_logging_time': 0.17943310737609863}
I0210 02:55:34.194673 139615864289024 logging_writer.py:48] [16449] accumulated_eval_time=4301.075336, accumulated_logging_time=0.179433, accumulated_submission_time=5910.604151, global_step=16449, preemption_count=0, score=5910.604151, test/accuracy=0.658986, test/bleu=26.919231, test/loss=1.607774, test/num_examples=3003, total_duration=10212.385687, train/accuracy=0.626923, train/bleu=30.456123, train/loss=1.830870, validation/accuracy=0.648634, validation/bleu=27.544675, validation/loss=1.679979, validation/num_examples=3000
I0210 02:55:52.787549 139615872681728 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.18593384325504303, loss=1.8366308212280273
I0210 02:56:28.487693 139615864289024 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.20780982077121735, loss=1.8753178119659424
I0210 02:57:04.244576 139615872681728 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.30227863788604736, loss=1.8349051475524902
I0210 02:57:39.962882 139615864289024 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.19539476931095123, loss=1.8961427211761475
I0210 02:58:15.725462 139615872681728 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.17088915407657623, loss=1.9729177951812744
I0210 02:58:51.483067 139615864289024 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.18585720658302307, loss=2.009775161743164
I0210 02:59:27.224456 139615872681728 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2644844949245453, loss=1.9437159299850464
I0210 03:00:02.981302 139615864289024 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.16997718811035156, loss=1.8745908737182617
I0210 03:00:38.784489 139615872681728 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.19074223935604095, loss=1.817818284034729
I0210 03:01:14.538010 139615864289024 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.25440943241119385, loss=1.929518461227417
I0210 03:01:50.273529 139615872681728 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.16106534004211426, loss=1.838913083076477
I0210 03:02:26.016532 139615864289024 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.1700705885887146, loss=1.897902011871338
I0210 03:03:01.748081 139615872681728 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.19825497269630432, loss=1.8850210905075073
I0210 03:03:37.475515 139615864289024 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2122824490070343, loss=1.9345372915267944
I0210 03:04:13.205173 139615872681728 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.21797804534435272, loss=1.8087685108184814
I0210 03:04:48.996373 139615864289024 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.17365680634975433, loss=1.8764922618865967
I0210 03:05:24.769968 139615872681728 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.1742771714925766, loss=1.7957881689071655
I0210 03:06:00.561361 139615864289024 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.19392257928848267, loss=1.885054349899292
I0210 03:06:36.292099 139615872681728 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.23456145823001862, loss=1.9174623489379883
I0210 03:07:12.055389 139615864289024 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.25195127725601196, loss=1.8344753980636597
I0210 03:07:47.776974 139615872681728 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.19221338629722595, loss=1.8993244171142578
I0210 03:08:23.514520 139615864289024 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.2068706601858139, loss=1.8492401838302612
I0210 03:08:59.278304 139615872681728 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.1949128955602646, loss=1.8736811876296997
I0210 03:09:34.405681 139785736898368 spec.py:321] Evaluating on the training split.
I0210 03:09:37.399152 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:12:42.778020 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 03:12:45.496833 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:15:06.782301 139785736898368 spec.py:349] Evaluating on the test split.
I0210 03:15:09.505274 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:17:15.521267 139785736898368 submission_runner.py:408] Time since start: 11513.73s, 	Step: 18800, 	{'train/accuracy': 0.671360194683075, 'train/loss': 1.5273321866989136, 'train/bleu': 34.06164879315195, 'validation/accuracy': 0.6512752175331116, 'validation/loss': 1.659209132194519, 'validation/bleu': 27.372778668807793, 'validation/num_examples': 3000, 'test/accuracy': 0.6605542898178101, 'test/loss': 1.596093773841858, 'test/bleu': 26.86495937979053, 'test/num_examples': 3003, 'score': 6750.731124639511, 'total_duration': 11513.733264446259, 'accumulated_submission_time': 6750.731124639511, 'accumulated_eval_time': 4762.190878391266, 'accumulated_logging_time': 0.2115025520324707}
I0210 03:17:15.538787 139615864289024 logging_writer.py:48] [18800] accumulated_eval_time=4762.190878, accumulated_logging_time=0.211503, accumulated_submission_time=6750.731125, global_step=18800, preemption_count=0, score=6750.731125, test/accuracy=0.660554, test/bleu=26.864959, test/loss=1.596094, test/num_examples=3003, total_duration=11513.733264, train/accuracy=0.671360, train/bleu=34.061649, train/loss=1.527332, validation/accuracy=0.651275, validation/bleu=27.372779, validation/loss=1.659209, validation/num_examples=3000
I0210 03:17:15.916391 139615872681728 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.2262691706418991, loss=1.7856091260910034
I0210 03:17:51.651084 139615864289024 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.18307864665985107, loss=1.8828182220458984
I0210 03:18:27.399338 139615872681728 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.1891585737466812, loss=1.776045322418213
I0210 03:19:03.191147 139615864289024 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2186644971370697, loss=1.885530948638916
I0210 03:19:38.965691 139615872681728 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.1959950029850006, loss=1.8879048824310303
I0210 03:20:14.699319 139615864289024 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.21808341145515442, loss=1.767167091369629
I0210 03:20:50.447231 139615872681728 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1843409389257431, loss=1.860273003578186
I0210 03:21:26.193899 139615864289024 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.29287493228912354, loss=1.8646130561828613
I0210 03:22:01.940048 139615872681728 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17739631235599518, loss=1.8861584663391113
I0210 03:22:37.706967 139615864289024 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.18324774503707886, loss=1.8638426065444946
I0210 03:23:13.481129 139615872681728 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.22857266664505005, loss=1.924822449684143
I0210 03:23:49.226820 139615864289024 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.18820829689502716, loss=1.8315011262893677
I0210 03:24:24.980547 139615872681728 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.20436127483844757, loss=1.8474575281143188
I0210 03:25:00.731329 139615864289024 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.17489834129810333, loss=1.8257009983062744
I0210 03:25:36.500106 139615872681728 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.17162641882896423, loss=1.8631759881973267
I0210 03:26:12.250079 139615864289024 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.1813434213399887, loss=1.8230754137039185
I0210 03:26:47.993918 139615872681728 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.27384841442108154, loss=1.970763921737671
I0210 03:27:23.712928 139615864289024 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.28263944387435913, loss=1.9065275192260742
I0210 03:27:59.515690 139615872681728 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.25623780488967896, loss=1.9041564464569092
I0210 03:28:35.280482 139615864289024 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.18314935266971588, loss=1.8620606660842896
I0210 03:29:10.994517 139615872681728 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.28973886370658875, loss=1.828934907913208
I0210 03:29:46.760033 139615864289024 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.20075581967830658, loss=1.7967051267623901
I0210 03:30:22.542907 139615872681728 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.17422133684158325, loss=1.9206714630126953
I0210 03:30:58.359372 139615864289024 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.231918603181839, loss=1.8524541854858398
I0210 03:31:15.579402 139785736898368 spec.py:321] Evaluating on the training split.
I0210 03:31:18.566113 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:34:36.594224 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 03:34:39.305702 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:37:11.958266 139785736898368 spec.py:349] Evaluating on the test split.
I0210 03:37:14.660193 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:39:49.298751 139785736898368 submission_runner.py:408] Time since start: 12867.51s, 	Step: 21150, 	{'train/accuracy': 0.6342859268188477, 'train/loss': 1.768483281135559, 'train/bleu': 30.72447289999782, 'validation/accuracy': 0.6521803736686707, 'validation/loss': 1.6406331062316895, 'validation/bleu': 27.661841579977537, 'validation/num_examples': 3000, 'test/accuracy': 0.6623671054840088, 'test/loss': 1.5717984437942505, 'test/bleu': 26.760107849685753, 'test/num_examples': 3003, 'score': 7590.68705701828, 'total_duration': 12867.510741472244, 'accumulated_submission_time': 7590.68705701828, 'accumulated_eval_time': 5275.910169124603, 'accumulated_logging_time': 0.24033284187316895}
I0210 03:39:49.317390 139615872681728 logging_writer.py:48] [21150] accumulated_eval_time=5275.910169, accumulated_logging_time=0.240333, accumulated_submission_time=7590.687057, global_step=21150, preemption_count=0, score=7590.687057, test/accuracy=0.662367, test/bleu=26.760108, test/loss=1.571798, test/num_examples=3003, total_duration=12867.510741, train/accuracy=0.634286, train/bleu=30.724473, train/loss=1.768483, validation/accuracy=0.652180, validation/bleu=27.661842, validation/loss=1.640633, validation/num_examples=3000
I0210 03:40:07.545109 139615864289024 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.18982525169849396, loss=1.8701483011245728
I0210 03:40:43.318177 139615872681728 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2104135900735855, loss=1.8175368309020996
I0210 03:41:19.051341 139615864289024 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.1724717915058136, loss=1.952144980430603
I0210 03:41:54.866324 139615872681728 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.19744917750358582, loss=1.8013728857040405
I0210 03:42:30.610250 139615864289024 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.1792483925819397, loss=1.818908929824829
I0210 03:43:06.413466 139615872681728 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.25908544659614563, loss=1.8483951091766357
I0210 03:43:42.150732 139615864289024 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.18495985865592957, loss=1.921546459197998
I0210 03:44:17.924458 139615872681728 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.22239506244659424, loss=1.8232173919677734
I0210 03:44:53.758983 139615864289024 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.18912959098815918, loss=1.8398462533950806
I0210 03:45:29.511778 139615872681728 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2410498559474945, loss=1.8830773830413818
I0210 03:46:05.239323 139615864289024 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.20729419589042664, loss=1.8422774076461792
I0210 03:46:41.000422 139615872681728 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.22540487349033356, loss=1.8281006813049316
I0210 03:47:16.777793 139615864289024 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.179294615983963, loss=1.9276657104492188
I0210 03:47:52.535392 139615872681728 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.2809927463531494, loss=1.878194808959961
I0210 03:48:28.274944 139615864289024 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.21286624670028687, loss=1.8381394147872925
I0210 03:49:04.060166 139615872681728 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.18972446024417877, loss=1.9023032188415527
I0210 03:49:39.780998 139615864289024 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.22907017171382904, loss=1.7068692445755005
I0210 03:50:15.534873 139615872681728 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.20929674804210663, loss=1.9008687734603882
I0210 03:50:51.303289 139615864289024 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.18499767780303955, loss=1.8742748498916626
I0210 03:51:27.048092 139615872681728 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.17740695178508759, loss=1.858494758605957
I0210 03:52:02.845824 139615864289024 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2017672061920166, loss=1.857424020767212
I0210 03:52:38.602546 139615872681728 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.18439580500125885, loss=1.7924402952194214
I0210 03:53:14.384110 139615864289024 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.17212294042110443, loss=1.8620537519454956
I0210 03:53:49.501056 139785736898368 spec.py:321] Evaluating on the training split.
I0210 03:53:52.500792 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 03:57:56.128089 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 03:57:58.841697 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:00:42.202867 139785736898368 spec.py:349] Evaluating on the test split.
I0210 04:00:44.928728 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:03:03.614702 139785736898368 submission_runner.py:408] Time since start: 14261.83s, 	Step: 23500, 	{'train/accuracy': 0.6302686333656311, 'train/loss': 1.8009165525436401, 'train/bleu': 30.477764659911852, 'validation/accuracy': 0.6556645035743713, 'validation/loss': 1.6268913745880127, 'validation/bleu': 27.54684052250174, 'validation/num_examples': 3000, 'test/accuracy': 0.667851984500885, 'test/loss': 1.5515425205230713, 'test/bleu': 27.18575754374719, 'test/num_examples': 3003, 'score': 8430.784281015396, 'total_duration': 14261.82667016983, 'accumulated_submission_time': 8430.784281015396, 'accumulated_eval_time': 5830.0237646102905, 'accumulated_logging_time': 0.27062082290649414}
I0210 04:03:03.636250 139615872681728 logging_writer.py:48] [23500] accumulated_eval_time=5830.023765, accumulated_logging_time=0.270621, accumulated_submission_time=8430.784281, global_step=23500, preemption_count=0, score=8430.784281, test/accuracy=0.667852, test/bleu=27.185758, test/loss=1.551543, test/num_examples=3003, total_duration=14261.826670, train/accuracy=0.630269, train/bleu=30.477765, train/loss=1.800917, validation/accuracy=0.655665, validation/bleu=27.546841, validation/loss=1.626891, validation/num_examples=3000
I0210 04:03:04.019944 139615864289024 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.18746791779994965, loss=1.8346879482269287
I0210 04:03:39.759634 139615872681728 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2669251561164856, loss=1.9071943759918213
I0210 04:04:15.508473 139615864289024 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.20272651314735413, loss=1.8367865085601807
I0210 04:04:51.263697 139615872681728 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.19880279898643494, loss=1.8973366022109985
I0210 04:05:27.004689 139615864289024 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.19735901057720184, loss=1.8299697637557983
I0210 04:06:02.733361 139615872681728 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2111409306526184, loss=1.8245216608047485
I0210 04:06:38.468579 139615864289024 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.18998713791370392, loss=1.926497459411621
I0210 04:07:14.206448 139615872681728 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.17090743780136108, loss=1.839091420173645
I0210 04:07:49.952091 139615864289024 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.19288411736488342, loss=1.969805121421814
I0210 04:08:25.702992 139615872681728 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.20683854818344116, loss=1.8900213241577148
I0210 04:09:01.456362 139615864289024 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.18908101320266724, loss=1.843176007270813
I0210 04:09:37.223923 139615872681728 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.18981552124023438, loss=1.822879672050476
I0210 04:10:12.989145 139615864289024 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.1901063174009323, loss=1.757769227027893
I0210 04:10:48.723866 139615872681728 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.18551906943321228, loss=1.8601529598236084
I0210 04:11:24.456489 139615864289024 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.18705160915851593, loss=1.8491116762161255
I0210 04:12:00.200048 139615872681728 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2632312476634979, loss=1.8646882772445679
I0210 04:12:35.941910 139615864289024 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.22618529200553894, loss=1.8517571687698364
I0210 04:13:11.690717 139615872681728 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1816737800836563, loss=1.8210150003433228
I0210 04:13:47.407093 139615864289024 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.25438278913497925, loss=1.8897383213043213
I0210 04:14:23.158371 139615872681728 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.16498181223869324, loss=1.823154091835022
I0210 04:14:58.960078 139615864289024 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.17101743817329407, loss=1.7273809909820557
I0210 04:15:34.768185 139615872681728 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2190854400396347, loss=1.851607322692871
I0210 04:16:10.510914 139615864289024 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.18336042761802673, loss=1.807185173034668
I0210 04:16:46.257363 139615872681728 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.19987145066261292, loss=1.7324227094650269
I0210 04:17:03.849112 139785736898368 spec.py:321] Evaluating on the training split.
I0210 04:17:06.839694 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:20:22.142431 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 04:20:24.853201 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:23:02.642292 139785736898368 spec.py:349] Evaluating on the test split.
I0210 04:23:05.354460 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:25:36.253753 139785736898368 submission_runner.py:408] Time since start: 15614.47s, 	Step: 25851, 	{'train/accuracy': 0.6471864581108093, 'train/loss': 1.6817272901535034, 'train/bleu': 31.524659902627377, 'validation/accuracy': 0.6565448641777039, 'validation/loss': 1.6173309087753296, 'validation/bleu': 27.799491595963826, 'validation/num_examples': 3000, 'test/accuracy': 0.6690372824668884, 'test/loss': 1.541406273841858, 'test/bleu': 27.224962151330093, 'test/num_examples': 3003, 'score': 9270.913045167923, 'total_duration': 15614.465742111206, 'accumulated_submission_time': 9270.913045167923, 'accumulated_eval_time': 6342.428351640701, 'accumulated_logging_time': 0.3031198978424072}
I0210 04:25:36.271834 139615864289024 logging_writer.py:48] [25851] accumulated_eval_time=6342.428352, accumulated_logging_time=0.303120, accumulated_submission_time=9270.913045, global_step=25851, preemption_count=0, score=9270.913045, test/accuracy=0.669037, test/bleu=27.224962, test/loss=1.541406, test/num_examples=3003, total_duration=15614.465742, train/accuracy=0.647186, train/bleu=31.524660, train/loss=1.681727, validation/accuracy=0.656545, validation/bleu=27.799492, validation/loss=1.617331, validation/num_examples=3000
I0210 04:25:54.164488 139615872681728 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.20029473304748535, loss=1.8779277801513672
I0210 04:26:29.919712 139615864289024 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.19068534672260284, loss=1.6984246969223022
I0210 04:27:05.673902 139615872681728 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.2483929544687271, loss=1.9084539413452148
I0210 04:27:41.417727 139615864289024 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.26426514983177185, loss=1.9015661478042603
I0210 04:28:17.170755 139615872681728 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.17784777283668518, loss=1.8272415399551392
I0210 04:28:52.925628 139615864289024 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2424696534872055, loss=1.947174072265625
I0210 04:29:28.696855 139615872681728 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.20054872334003448, loss=1.8796377182006836
I0210 04:30:04.476969 139615864289024 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19107170403003693, loss=1.8501499891281128
I0210 04:30:40.211432 139615872681728 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.20227161049842834, loss=1.8216959238052368
I0210 04:31:15.934442 139615864289024 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.22051861882209778, loss=1.8845375776290894
I0210 04:31:51.712938 139615872681728 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1993744671344757, loss=1.824133038520813
I0210 04:32:27.531102 139615864289024 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.20620739459991455, loss=1.7907390594482422
I0210 04:33:03.277735 139615872681728 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.21696259081363678, loss=1.8642725944519043
I0210 04:33:39.035511 139615864289024 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.19195233285427094, loss=1.802038311958313
I0210 04:34:14.796707 139615872681728 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.24671323597431183, loss=1.8143118619918823
I0210 04:34:50.596899 139615864289024 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.19530971348285675, loss=1.7569706439971924
I0210 04:35:26.356604 139615872681728 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19417710602283478, loss=1.7781931161880493
I0210 04:36:02.103902 139615864289024 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.18513606488704681, loss=1.87386155128479
I0210 04:36:37.864436 139615872681728 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.23928289115428925, loss=1.8947415351867676
I0210 04:37:13.584611 139615864289024 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.22598624229431152, loss=1.8594753742218018
I0210 04:37:49.345515 139615872681728 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.20171615481376648, loss=1.8172372579574585
I0210 04:38:25.119365 139615864289024 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.23989985883235931, loss=1.8235375881195068
I0210 04:39:00.851085 139615872681728 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.18355217576026917, loss=1.7720530033111572
I0210 04:39:36.599710 139615864289024 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.18828926980495453, loss=1.8267688751220703
I0210 04:39:36.606697 139785736898368 spec.py:321] Evaluating on the training split.
I0210 04:39:39.328716 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:43:37.706499 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 04:43:40.408681 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:47:12.694157 139785736898368 spec.py:349] Evaluating on the test split.
I0210 04:47:15.393994 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 04:51:10.914808 139785736898368 submission_runner.py:408] Time since start: 17149.13s, 	Step: 28201, 	{'train/accuracy': 0.6392074823379517, 'train/loss': 1.7369545698165894, 'train/bleu': 31.261923161305248, 'validation/accuracy': 0.6590991020202637, 'validation/loss': 1.6041685342788696, 'validation/bleu': 28.174983894107854, 'validation/num_examples': 3000, 'test/accuracy': 0.6693510413169861, 'test/loss': 1.5309597253799438, 'test/bleu': 27.50862562138666, 'test/num_examples': 3003, 'score': 10111.160809516907, 'total_duration': 17149.126772880554, 'accumulated_submission_time': 10111.160809516907, 'accumulated_eval_time': 7036.736354827881, 'accumulated_logging_time': 0.33391356468200684}
I0210 04:51:10.932890 139615872681728 logging_writer.py:48] [28201] accumulated_eval_time=7036.736355, accumulated_logging_time=0.333914, accumulated_submission_time=10111.160810, global_step=28201, preemption_count=0, score=10111.160810, test/accuracy=0.669351, test/bleu=27.508626, test/loss=1.530960, test/num_examples=3003, total_duration=17149.126773, train/accuracy=0.639207, train/bleu=31.261923, train/loss=1.736955, validation/accuracy=0.659099, validation/bleu=28.174984, validation/loss=1.604169, validation/num_examples=3000
I0210 04:51:46.631134 139615864289024 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1705818921327591, loss=1.783178448677063
I0210 04:52:22.410366 139615872681728 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2206452488899231, loss=1.8038066625595093
I0210 04:52:58.207513 139615864289024 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.2576900124549866, loss=1.8639352321624756
I0210 04:53:33.985430 139615872681728 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18587496876716614, loss=1.8505991697311401
I0210 04:54:09.756110 139615864289024 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.17510353028774261, loss=1.8124864101409912
I0210 04:54:45.477795 139615872681728 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.20846988260746002, loss=1.8628160953521729
I0210 04:55:21.218791 139615864289024 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.29323825240135193, loss=1.9078781604766846
I0210 04:55:56.953394 139615872681728 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.24357527494430542, loss=1.8071391582489014
I0210 04:56:32.701725 139615864289024 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.18800732493400574, loss=1.8520691394805908
I0210 04:57:08.470820 139615872681728 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.2305728644132614, loss=1.8508145809173584
I0210 04:57:44.208005 139615864289024 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1831195205450058, loss=1.8974566459655762
I0210 04:58:19.981471 139615872681728 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.19884833693504333, loss=1.8236461877822876
I0210 04:58:55.695290 139615864289024 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.20911039412021637, loss=1.8676600456237793
I0210 04:59:31.436713 139615872681728 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19767117500305176, loss=1.7983901500701904
I0210 05:00:07.224536 139615864289024 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.21064811944961548, loss=1.8355954885482788
I0210 05:00:42.958417 139615872681728 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1972273588180542, loss=1.8011761903762817
I0210 05:01:18.683997 139615864289024 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.19913822412490845, loss=1.8412346839904785
I0210 05:01:54.407365 139615872681728 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2020643949508667, loss=1.83061945438385
I0210 05:02:30.133430 139615864289024 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.22876939177513123, loss=1.8597394227981567
I0210 05:03:05.912644 139615872681728 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.23784269392490387, loss=1.8132307529449463
I0210 05:03:41.657965 139615864289024 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.21472063660621643, loss=1.8402726650238037
I0210 05:04:17.389132 139615872681728 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.18757200241088867, loss=1.757250428199768
I0210 05:04:53.159756 139615864289024 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.24529197812080383, loss=1.8497774600982666
I0210 05:05:11.110647 139785736898368 spec.py:321] Evaluating on the training split.
I0210 05:05:14.104912 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:08:53.593188 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 05:08:56.303800 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:11:46.767021 139785736898368 spec.py:349] Evaluating on the test split.
I0210 05:11:49.479379 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:14:37.934408 139785736898368 submission_runner.py:408] Time since start: 18556.15s, 	Step: 30552, 	{'train/accuracy': 0.6390373110771179, 'train/loss': 1.7507790327072144, 'train/bleu': 31.27872196488971, 'validation/accuracy': 0.6580327749252319, 'validation/loss': 1.595872402191162, 'validation/bleu': 27.93156543775106, 'validation/num_examples': 3000, 'test/accuracy': 0.6702225208282471, 'test/loss': 1.522372841835022, 'test/bleu': 27.486084020074678, 'test/num_examples': 3003, 'score': 10951.253396511078, 'total_duration': 18556.146406650543, 'accumulated_submission_time': 10951.253396511078, 'accumulated_eval_time': 7603.5600752830505, 'accumulated_logging_time': 0.36229705810546875}
I0210 05:14:37.952929 139615872681728 logging_writer.py:48] [30552] accumulated_eval_time=7603.560075, accumulated_logging_time=0.362297, accumulated_submission_time=10951.253397, global_step=30552, preemption_count=0, score=10951.253397, test/accuracy=0.670223, test/bleu=27.486084, test/loss=1.522373, test/num_examples=3003, total_duration=18556.146407, train/accuracy=0.639037, train/bleu=31.278722, train/loss=1.750779, validation/accuracy=0.658033, validation/bleu=27.931565, validation/loss=1.595872, validation/num_examples=3000
I0210 05:14:55.483671 139615864289024 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.19968564808368683, loss=1.7308703660964966
I0210 05:15:31.247852 139615872681728 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.17697903513908386, loss=1.809934139251709
I0210 05:16:06.986101 139615864289024 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.20203520357608795, loss=1.75383460521698
I0210 05:16:42.713477 139615872681728 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3013932406902313, loss=1.7967256307601929
I0210 05:17:18.493544 139615864289024 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.3575426936149597, loss=1.8227589130401611
I0210 05:17:54.225580 139615872681728 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.20151910185813904, loss=1.8737090826034546
I0210 05:18:29.959229 139615864289024 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.18772625923156738, loss=1.7630693912506104
I0210 05:19:05.714994 139615872681728 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.20374859869480133, loss=1.8418469429016113
I0210 05:19:41.516809 139615864289024 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.18846207857131958, loss=1.822042465209961
I0210 05:20:17.255927 139615872681728 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.23933914303779602, loss=1.826657772064209
I0210 05:20:53.005864 139615864289024 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1856769323348999, loss=1.7811148166656494
I0210 05:21:28.749470 139615872681728 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.19718492031097412, loss=1.8504639863967896
I0210 05:22:04.513260 139615864289024 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.17356336116790771, loss=1.7257596254348755
I0210 05:22:40.260672 139615872681728 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1833140254020691, loss=1.8369262218475342
I0210 05:23:15.990774 139615864289024 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.16876006126403809, loss=1.81341552734375
I0210 05:23:51.736397 139615872681728 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.18994936347007751, loss=1.8336102962493896
I0210 05:24:27.490100 139615864289024 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.21713809669017792, loss=1.8241686820983887
I0210 05:25:03.265920 139615872681728 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.19677934050559998, loss=1.8585929870605469
I0210 05:25:39.002799 139615864289024 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.20831242203712463, loss=1.8633280992507935
I0210 05:26:14.755917 139615872681728 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.20788505673408508, loss=1.7877246141433716
I0210 05:26:50.527135 139615864289024 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.23449864983558655, loss=1.8063727617263794
I0210 05:27:26.283791 139615872681728 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.20620229840278625, loss=1.8080976009368896
I0210 05:28:02.021437 139615864289024 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2088792324066162, loss=1.8567203283309937
I0210 05:28:37.774719 139615872681728 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.18995141983032227, loss=1.8011243343353271
I0210 05:28:38.215761 139785736898368 spec.py:321] Evaluating on the training split.
I0210 05:28:41.212501 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:32:49.018104 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 05:32:51.735817 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:35:20.609072 139785736898368 spec.py:349] Evaluating on the test split.
I0210 05:35:23.344625 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:37:29.347410 139785736898368 submission_runner.py:408] Time since start: 19927.56s, 	Step: 32903, 	{'train/accuracy': 0.6409906148910522, 'train/loss': 1.7161387205123901, 'train/bleu': 31.478213278866903, 'validation/accuracy': 0.6610209345817566, 'validation/loss': 1.5890172719955444, 'validation/bleu': 28.024704001168377, 'validation/num_examples': 3000, 'test/accuracy': 0.6717913150787354, 'test/loss': 1.5121753215789795, 'test/bleu': 27.377931603736805, 'test/num_examples': 3003, 'score': 11791.432320356369, 'total_duration': 19927.55940771103, 'accumulated_submission_time': 11791.432320356369, 'accumulated_eval_time': 8134.691674232483, 'accumulated_logging_time': 0.39051246643066406}
I0210 05:37:29.366486 139615864289024 logging_writer.py:48] [32903] accumulated_eval_time=8134.691674, accumulated_logging_time=0.390512, accumulated_submission_time=11791.432320, global_step=32903, preemption_count=0, score=11791.432320, test/accuracy=0.671791, test/bleu=27.377932, test/loss=1.512175, test/num_examples=3003, total_duration=19927.559408, train/accuracy=0.640991, train/bleu=31.478213, train/loss=1.716139, validation/accuracy=0.661021, validation/bleu=28.024704, validation/loss=1.589017, validation/num_examples=3000
I0210 05:38:04.380391 139615872681728 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2645006477832794, loss=1.7795300483703613
I0210 05:38:40.124214 139615864289024 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.19919130206108093, loss=1.8194955587387085
I0210 05:39:15.869497 139615872681728 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.19151882827281952, loss=1.8481650352478027
I0210 05:39:51.606922 139615864289024 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.19291095435619354, loss=1.7778226137161255
I0210 05:40:27.347377 139615872681728 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.18947570025920868, loss=1.8440696001052856
I0210 05:41:03.093135 139615864289024 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.20899726450443268, loss=1.770459771156311
I0210 05:41:38.834791 139615872681728 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.19672125577926636, loss=1.8533347845077515
I0210 05:42:14.596430 139615864289024 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.22295837104320526, loss=1.8684085607528687
I0210 05:42:50.393253 139615872681728 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.21040025353431702, loss=1.7621673345565796
I0210 05:43:26.194972 139615864289024 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.22861601412296295, loss=1.7322098016738892
I0210 05:44:01.947642 139615872681728 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.22078870236873627, loss=1.7766824960708618
I0210 05:44:37.716521 139615864289024 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.19523517787456512, loss=1.7951983213424683
I0210 05:45:13.442137 139615872681728 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.17874006927013397, loss=1.8058277368545532
I0210 05:45:49.243068 139615864289024 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.1838604360818863, loss=1.7599180936813354
I0210 05:46:25.041513 139615872681728 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.2124515175819397, loss=1.757894515991211
I0210 05:47:00.836589 139615864289024 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.19356124103069305, loss=1.7150639295578003
I0210 05:47:36.627608 139615872681728 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2265782654285431, loss=1.7929258346557617
I0210 05:48:12.376571 139615864289024 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.19990484416484833, loss=1.8583271503448486
I0210 05:48:48.095963 139615872681728 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.24039208889007568, loss=1.740299940109253
I0210 05:49:23.853294 139615864289024 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.17409676313400269, loss=1.816584825515747
I0210 05:49:59.645259 139615872681728 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.23494525253772736, loss=1.7580240964889526
I0210 05:50:35.503371 139615864289024 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2319709062576294, loss=1.8028088808059692
I0210 05:51:11.286423 139615872681728 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.20146815478801727, loss=1.7196335792541504
I0210 05:51:29.589417 139785736898368 spec.py:321] Evaluating on the training split.
I0210 05:51:32.580623 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:55:04.214278 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 05:55:06.916727 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 05:57:54.398825 139785736898368 spec.py:349] Evaluating on the test split.
I0210 05:57:57.120177 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:00:44.910231 139785736898368 submission_runner.py:408] Time since start: 21323.12s, 	Step: 35253, 	{'train/accuracy': 0.6431419849395752, 'train/loss': 1.7123115062713623, 'train/bleu': 31.608488104923683, 'validation/accuracy': 0.6615169048309326, 'validation/loss': 1.5796931982040405, 'validation/bleu': 28.002704988103364, 'validation/num_examples': 3000, 'test/accuracy': 0.6747545599937439, 'test/loss': 1.4998594522476196, 'test/bleu': 28.059762524152184, 'test/num_examples': 3003, 'score': 12631.56686782837, 'total_duration': 21323.122226953506, 'accumulated_submission_time': 12631.56686782837, 'accumulated_eval_time': 8690.012436389923, 'accumulated_logging_time': 0.4196438789367676}
I0210 06:00:44.930237 139615864289024 logging_writer.py:48] [35253] accumulated_eval_time=8690.012436, accumulated_logging_time=0.419644, accumulated_submission_time=12631.566868, global_step=35253, preemption_count=0, score=12631.566868, test/accuracy=0.674755, test/bleu=28.059763, test/loss=1.499859, test/num_examples=3003, total_duration=21323.122227, train/accuracy=0.643142, train/bleu=31.608488, train/loss=1.712312, validation/accuracy=0.661517, validation/bleu=28.002705, validation/loss=1.579693, validation/num_examples=3000
I0210 06:01:02.095197 139615872681728 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.1876639723777771, loss=1.7979052066802979
I0210 06:01:37.829566 139615864289024 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.23890897631645203, loss=1.7664082050323486
I0210 06:02:13.573875 139615872681728 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2592792510986328, loss=1.8091728687286377
I0210 06:02:49.325455 139615864289024 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.21030406653881073, loss=1.7677795886993408
I0210 06:03:25.064915 139615872681728 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.24618858098983765, loss=1.829712152481079
I0210 06:04:00.830405 139615864289024 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.2146293669939041, loss=1.804280161857605
I0210 06:04:36.633573 139615872681728 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.19957342743873596, loss=1.8214538097381592
I0210 06:05:12.492651 139615864289024 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.18457283079624176, loss=1.7409732341766357
I0210 06:05:48.256561 139615872681728 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.21710708737373352, loss=1.7137248516082764
I0210 06:06:24.023515 139615864289024 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.23549668490886688, loss=1.828653335571289
I0210 06:06:59.788211 139615872681728 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.19311179220676422, loss=1.7164959907531738
I0210 06:07:35.593034 139615864289024 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.21068693697452545, loss=1.8497514724731445
I0210 06:08:11.377108 139615872681728 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.19284048676490784, loss=1.786863923072815
I0210 06:08:47.189250 139615864289024 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.20272193849086761, loss=1.7301088571548462
I0210 06:09:22.931110 139615872681728 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.20258274674415588, loss=1.6955052614212036
I0210 06:09:58.643159 139615864289024 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.25023552775382996, loss=1.7434390783309937
I0210 06:10:34.444391 139615872681728 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2226068377494812, loss=1.8532190322875977
I0210 06:11:10.214602 139615864289024 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2362576574087143, loss=1.7979488372802734
I0210 06:11:45.995887 139615872681728 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.18205943703651428, loss=1.7377461194992065
I0210 06:12:21.739760 139615864289024 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.19147241115570068, loss=1.7905330657958984
I0210 06:12:57.473732 139615872681728 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.19579753279685974, loss=1.8334482908248901
I0210 06:13:33.266537 139615864289024 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3310474157333374, loss=1.9147071838378906
I0210 06:14:09.059390 139615872681728 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.21894405782222748, loss=1.8156663179397583
I0210 06:14:44.794646 139615864289024 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2040041834115982, loss=1.8779739141464233
I0210 06:14:45.236233 139785736898368 spec.py:321] Evaluating on the training split.
I0210 06:14:48.232686 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:18:53.970781 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 06:18:56.669742 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:21:32.287669 139785736898368 spec.py:349] Evaluating on the test split.
I0210 06:21:34.995372 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:24:00.823976 139785736898368 submission_runner.py:408] Time since start: 22719.04s, 	Step: 37603, 	{'train/accuracy': 0.6761975288391113, 'train/loss': 1.4833768606185913, 'train/bleu': 34.013655184488385, 'validation/accuracy': 0.6622856259346008, 'validation/loss': 1.571945309638977, 'validation/bleu': 28.19304704726278, 'validation/num_examples': 3000, 'test/accuracy': 0.6743013262748718, 'test/loss': 1.4964731931686401, 'test/bleu': 27.469852057344102, 'test/num_examples': 3003, 'score': 13471.785254716873, 'total_duration': 22719.035972356796, 'accumulated_submission_time': 13471.785254716873, 'accumulated_eval_time': 9245.60012793541, 'accumulated_logging_time': 0.45088791847229004}
I0210 06:24:00.844906 139615872681728 logging_writer.py:48] [37603] accumulated_eval_time=9245.600128, accumulated_logging_time=0.450888, accumulated_submission_time=13471.785255, global_step=37603, preemption_count=0, score=13471.785255, test/accuracy=0.674301, test/bleu=27.469852, test/loss=1.496473, test/num_examples=3003, total_duration=22719.035972, train/accuracy=0.676198, train/bleu=34.013655, train/loss=1.483377, validation/accuracy=0.662286, validation/bleu=28.193047, validation/loss=1.571945, validation/num_examples=3000
I0210 06:24:35.895009 139615864289024 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.2216815948486328, loss=1.8619276285171509
I0210 06:25:11.651124 139615872681728 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.18570271134376526, loss=1.7066227197647095
I0210 06:25:47.401893 139615864289024 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2060653567314148, loss=1.8023357391357422
I0210 06:26:23.145892 139615872681728 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.23495671153068542, loss=1.792374610900879
I0210 06:26:58.899873 139615864289024 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.19356344640254974, loss=1.8161187171936035
I0210 06:27:34.634138 139615872681728 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.20126193761825562, loss=1.7577905654907227
I0210 06:28:10.386874 139615864289024 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.20451828837394714, loss=1.770516037940979
I0210 06:28:46.172031 139615872681728 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.20279937982559204, loss=1.8025097846984863
I0210 06:29:21.925884 139615864289024 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.18873004615306854, loss=1.8083181381225586
I0210 06:29:57.700361 139615872681728 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.19031018018722534, loss=1.766595721244812
I0210 06:30:33.443383 139615864289024 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.21394233405590057, loss=1.774701714515686
I0210 06:31:09.229411 139615872681728 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.20178477466106415, loss=1.768180251121521
I0210 06:31:44.993048 139615864289024 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.20569781959056854, loss=1.7750768661499023
I0210 06:32:20.723794 139615872681728 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.20455379784107208, loss=1.8095850944519043
I0210 06:32:56.471401 139615864289024 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.1876872330904007, loss=1.7912352085113525
I0210 06:33:32.236220 139615872681728 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.17197920382022858, loss=1.6775472164154053
I0210 06:34:08.008501 139615864289024 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.198222354054451, loss=1.8213374614715576
I0210 06:34:43.798454 139615872681728 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.23799113929271698, loss=1.7280266284942627
I0210 06:35:19.570454 139615864289024 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.18425732851028442, loss=1.8022669553756714
I0210 06:35:55.330733 139615872681728 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.20618295669555664, loss=1.7023193836212158
I0210 06:36:31.068938 139615864289024 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.20366644859313965, loss=1.77775239944458
I0210 06:37:06.808056 139615872681728 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.21230024099349976, loss=1.7586206197738647
I0210 06:37:42.565993 139615864289024 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.20840032398700714, loss=1.7504158020019531
I0210 06:38:00.882670 139785736898368 spec.py:321] Evaluating on the training split.
I0210 06:38:03.885733 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:41:23.541440 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 06:41:26.242301 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:44:08.375674 139785736898368 spec.py:349] Evaluating on the test split.
I0210 06:44:11.082493 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 06:46:43.004134 139785736898368 submission_runner.py:408] Time since start: 24081.22s, 	Step: 39953, 	{'train/accuracy': 0.6511838436126709, 'train/loss': 1.662556767463684, 'train/bleu': 31.470296217893207, 'validation/accuracy': 0.665162205696106, 'validation/loss': 1.560990333557129, 'validation/bleu': 28.531168082604847, 'validation/num_examples': 3000, 'test/accuracy': 0.6754401326179504, 'test/loss': 1.4862949848175049, 'test/bleu': 28.240845223490155, 'test/num_examples': 3003, 'score': 14311.73891043663, 'total_duration': 24081.216133594513, 'accumulated_submission_time': 14311.73891043663, 'accumulated_eval_time': 9767.721544027328, 'accumulated_logging_time': 0.4817836284637451}
I0210 06:46:43.023929 139615872681728 logging_writer.py:48] [39953] accumulated_eval_time=9767.721544, accumulated_logging_time=0.481784, accumulated_submission_time=14311.738910, global_step=39953, preemption_count=0, score=14311.738910, test/accuracy=0.675440, test/bleu=28.240845, test/loss=1.486295, test/num_examples=3003, total_duration=24081.216134, train/accuracy=0.651184, train/bleu=31.470296, train/loss=1.662557, validation/accuracy=0.665162, validation/bleu=28.531168, validation/loss=1.560990, validation/num_examples=3000
I0210 06:47:00.197726 139615864289024 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.20863159000873566, loss=1.7808398008346558
I0210 06:47:35.969265 139615872681728 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.23186181485652924, loss=1.7490466833114624
I0210 06:48:11.731906 139615864289024 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2094232439994812, loss=1.8253036737442017
I0210 06:48:47.485029 139615872681728 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.1977839171886444, loss=1.7443716526031494
I0210 06:49:23.262993 139615864289024 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.19824470579624176, loss=1.8069429397583008
I0210 06:49:59.016333 139615872681728 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.19980885088443756, loss=1.8016886711120605
I0210 06:50:34.821571 139615864289024 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20110906660556793, loss=1.649659514427185
I0210 06:51:10.602043 139615872681728 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.20428411662578583, loss=1.7832951545715332
I0210 06:51:46.357348 139615864289024 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.20179982483386993, loss=1.7921093702316284
I0210 06:52:22.161693 139615872681728 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.22801977396011353, loss=1.8589808940887451
I0210 06:52:58.033587 139615864289024 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.1982751190662384, loss=1.8087399005889893
I0210 06:53:33.880988 139615872681728 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1915915161371231, loss=1.8089083433151245
I0210 06:54:09.676076 139615864289024 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.23671987652778625, loss=1.713700294494629
I0210 06:54:45.458717 139615872681728 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.19425973296165466, loss=1.8030825853347778
I0210 06:55:21.196498 139615864289024 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2057482749223709, loss=1.7740366458892822
I0210 06:55:56.940524 139615872681728 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.24575985968112946, loss=1.742553472518921
I0210 06:56:32.713985 139615864289024 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2289290875196457, loss=1.8013477325439453
I0210 06:57:08.488340 139615872681728 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.24175547063350677, loss=1.7809447050094604
I0210 06:57:44.280712 139615864289024 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.20545794069766998, loss=1.8325387239456177
I0210 06:58:20.021831 139615872681728 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.19074881076812744, loss=1.7237262725830078
I0210 06:58:55.771884 139615864289024 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2217061072587967, loss=1.8344800472259521
I0210 06:59:31.533685 139615872681728 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.20250065624713898, loss=1.7828421592712402
I0210 07:00:07.359051 139615864289024 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.2779347598552704, loss=1.7532446384429932
I0210 07:00:43.099085 139615872681728 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1961658000946045, loss=1.7202061414718628
I0210 07:00:43.106127 139785736898368 spec.py:321] Evaluating on the training split.
I0210 07:00:45.823980 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:04:21.485821 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 07:04:24.180183 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:06:45.357057 139785736898368 spec.py:349] Evaluating on the test split.
I0210 07:06:48.069720 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:09:15.919526 139785736898368 submission_runner.py:408] Time since start: 25434.13s, 	Step: 42301, 	{'train/accuracy': 0.6459149718284607, 'train/loss': 1.6967586278915405, 'train/bleu': 31.73541057923894, 'validation/accuracy': 0.6646910905838013, 'validation/loss': 1.5630953311920166, 'validation/bleu': 28.590118849023177, 'validation/num_examples': 3000, 'test/accuracy': 0.6777874827384949, 'test/loss': 1.4830689430236816, 'test/bleu': 28.08367316038388, 'test/num_examples': 3003, 'score': 15151.733073234558, 'total_duration': 25434.131522655487, 'accumulated_submission_time': 15151.733073234558, 'accumulated_eval_time': 10280.534869909286, 'accumulated_logging_time': 0.5133066177368164}
I0210 07:09:15.939876 139615864289024 logging_writer.py:48] [42301] accumulated_eval_time=10280.534870, accumulated_logging_time=0.513307, accumulated_submission_time=15151.733073, global_step=42301, preemption_count=0, score=15151.733073, test/accuracy=0.677787, test/bleu=28.083673, test/loss=1.483069, test/num_examples=3003, total_duration=25434.131523, train/accuracy=0.645915, train/bleu=31.735411, train/loss=1.696759, validation/accuracy=0.664691, validation/bleu=28.590119, validation/loss=1.563095, validation/num_examples=3000
I0210 07:09:51.730192 139615872681728 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.18144068121910095, loss=1.6976038217544556
I0210 07:10:27.446422 139615864289024 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.19310197234153748, loss=1.7564280033111572
I0210 07:11:03.266936 139615872681728 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.18877486884593964, loss=1.7050107717514038
I0210 07:11:39.005858 139615864289024 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.1923210769891739, loss=1.7753163576126099
I0210 07:12:14.741767 139615872681728 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.21307814121246338, loss=1.691490650177002
I0210 07:12:50.523914 139615864289024 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.22246766090393066, loss=1.677465796470642
I0210 07:13:26.299880 139615872681728 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.20518536865711212, loss=1.7719005346298218
I0210 07:14:02.112985 139615864289024 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.87269127368927, loss=1.7887839078903198
I0210 07:14:37.897455 139615872681728 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.2105160653591156, loss=1.7913410663604736
I0210 07:15:13.722027 139615864289024 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.19264094531536102, loss=1.7098280191421509
I0210 07:15:49.608642 139615872681728 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.20604254305362701, loss=1.8184903860092163
I0210 07:16:25.399004 139615864289024 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.19366540014743805, loss=1.7374482154846191
I0210 07:17:01.154565 139615872681728 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.1836399883031845, loss=1.7352094650268555
I0210 07:17:36.891932 139615864289024 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.22516286373138428, loss=1.8084505796432495
I0210 07:18:12.653550 139615872681728 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.17765358090400696, loss=1.7983559370040894
I0210 07:18:48.401175 139615864289024 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.1964396983385086, loss=1.7652333974838257
I0210 07:19:24.136837 139615872681728 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.19250281155109406, loss=1.7701133489608765
I0210 07:19:59.883815 139615864289024 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1879122406244278, loss=1.720958948135376
I0210 07:20:35.639745 139615872681728 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1896257847547531, loss=1.7802320718765259
I0210 07:21:11.368743 139615864289024 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.25956273078918457, loss=1.74413001537323
I0210 07:21:47.114904 139615872681728 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.20536868274211884, loss=1.7105984687805176
I0210 07:22:22.937620 139615864289024 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.18418565392494202, loss=1.7953623533248901
I0210 07:22:58.735979 139615872681728 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.18698926270008087, loss=1.6714129447937012
I0210 07:23:15.972299 139785736898368 spec.py:321] Evaluating on the training split.
I0210 07:23:18.962976 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:27:12.219307 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 07:27:14.929521 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:29:34.923243 139785736898368 spec.py:349] Evaluating on the test split.
I0210 07:29:37.627719 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:31:55.212839 139785736898368 submission_runner.py:408] Time since start: 26793.42s, 	Step: 44650, 	{'train/accuracy': 0.6544402837753296, 'train/loss': 1.6286474466323853, 'train/bleu': 32.2549238963339, 'validation/accuracy': 0.666154146194458, 'validation/loss': 1.5522340536117554, 'validation/bleu': 28.460654397908776, 'validation/num_examples': 3000, 'test/accuracy': 0.6779733896255493, 'test/loss': 1.475687861442566, 'test/bleu': 27.80825715299147, 'test/num_examples': 3003, 'score': 15991.678433418274, 'total_duration': 26793.424834012985, 'accumulated_submission_time': 15991.678433418274, 'accumulated_eval_time': 10799.775356054306, 'accumulated_logging_time': 0.5443167686462402}
I0210 07:31:55.233927 139615864289024 logging_writer.py:48] [44650] accumulated_eval_time=10799.775356, accumulated_logging_time=0.544317, accumulated_submission_time=15991.678433, global_step=44650, preemption_count=0, score=15991.678433, test/accuracy=0.677973, test/bleu=27.808257, test/loss=1.475688, test/num_examples=3003, total_duration=26793.424834, train/accuracy=0.654440, train/bleu=32.254924, train/loss=1.628647, validation/accuracy=0.666154, validation/bleu=28.460654, validation/loss=1.552234, validation/num_examples=3000
I0210 07:32:13.478705 139615872681728 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1986439824104309, loss=1.7627087831497192
I0210 07:32:49.338512 139615864289024 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.20389647781848907, loss=1.7741740942001343
I0210 07:33:25.119846 139615872681728 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.20306730270385742, loss=1.8186535835266113
I0210 07:34:00.921862 139615864289024 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.20108862221240997, loss=1.7338045835494995
I0210 07:34:36.673625 139615872681728 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.20296655595302582, loss=1.766326904296875
I0210 07:35:12.431959 139615864289024 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.25487348437309265, loss=1.7600151300430298
I0210 07:35:48.160387 139615872681728 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.21257224678993225, loss=1.8469048738479614
I0210 07:36:23.952704 139615864289024 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.18005011975765228, loss=1.7787525653839111
I0210 07:36:59.718685 139615872681728 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.21654728055000305, loss=1.7689990997314453
I0210 07:37:35.465481 139615864289024 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.21374395489692688, loss=1.7636829614639282
I0210 07:38:11.238502 139615872681728 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.1989094465970993, loss=1.716706395149231
I0210 07:38:47.011590 139615864289024 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.18844038248062134, loss=1.7005776166915894
I0210 07:39:22.751931 139615872681728 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.20458556711673737, loss=1.7650229930877686
I0210 07:39:58.570034 139615864289024 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.19444304704666138, loss=1.7323075532913208
I0210 07:40:34.317573 139615872681728 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.21590737998485565, loss=1.80154550075531
I0210 07:41:10.075749 139615864289024 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2177705466747284, loss=1.788613200187683
I0210 07:41:45.871947 139615872681728 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19922752678394318, loss=1.7349767684936523
I0210 07:42:21.605505 139615864289024 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.18710631132125854, loss=1.6976618766784668
I0210 07:42:57.353215 139615872681728 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.21079756319522858, loss=1.7379324436187744
I0210 07:43:33.156067 139615864289024 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.19988830387592316, loss=1.6793915033340454
I0210 07:44:08.910319 139615872681728 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.21465511620044708, loss=1.8790562152862549
I0210 07:44:44.663413 139615864289024 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.18803992867469788, loss=1.6796594858169556
I0210 07:45:20.464305 139615872681728 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2096131294965744, loss=1.7166541814804077
I0210 07:45:55.246865 139785736898368 spec.py:321] Evaluating on the training split.
I0210 07:45:58.247588 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:50:10.132487 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 07:50:12.834010 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:53:17.997909 139785736898368 spec.py:349] Evaluating on the test split.
I0210 07:53:20.722917 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 07:56:27.947739 139785736898368 submission_runner.py:408] Time since start: 28266.16s, 	Step: 46999, 	{'train/accuracy': 0.6529893279075623, 'train/loss': 1.6469743251800537, 'train/bleu': 31.74572989276444, 'validation/accuracy': 0.6668609380722046, 'validation/loss': 1.5455957651138306, 'validation/bleu': 28.479802138785814, 'validation/num_examples': 3000, 'test/accuracy': 0.6801813244819641, 'test/loss': 1.458451747894287, 'test/bleu': 28.38958892681242, 'test/num_examples': 3003, 'score': 16831.602340459824, 'total_duration': 28266.15973854065, 'accumulated_submission_time': 16831.602340459824, 'accumulated_eval_time': 11432.476187705994, 'accumulated_logging_time': 0.577672004699707}
I0210 07:56:27.968803 139615864289024 logging_writer.py:48] [46999] accumulated_eval_time=11432.476188, accumulated_logging_time=0.577672, accumulated_submission_time=16831.602340, global_step=46999, preemption_count=0, score=16831.602340, test/accuracy=0.680181, test/bleu=28.389589, test/loss=1.458452, test/num_examples=3003, total_duration=28266.159739, train/accuracy=0.652989, train/bleu=31.745730, train/loss=1.646974, validation/accuracy=0.666861, validation/bleu=28.479802, validation/loss=1.545596, validation/num_examples=3000
I0210 07:56:28.695831 139615872681728 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.17635703086853027, loss=1.7638527154922485
I0210 07:57:04.460169 139615864289024 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.18175795674324036, loss=1.7145479917526245
I0210 07:57:40.245110 139615872681728 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.17915506660938263, loss=1.6710528135299683
I0210 07:58:16.073051 139615864289024 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.1892559826374054, loss=1.6921627521514893
I0210 07:58:51.846918 139615872681728 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.21795080602169037, loss=1.7238273620605469
I0210 07:59:27.587577 139615864289024 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.19645065069198608, loss=1.738834261894226
I0210 08:00:03.334775 139615872681728 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2076767235994339, loss=1.7437909841537476
I0210 08:00:39.144703 139615864289024 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19877435266971588, loss=1.7829030752182007
I0210 08:01:14.865271 139615872681728 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.19411826133728027, loss=1.6906338930130005
I0210 08:01:50.624888 139615864289024 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2033315896987915, loss=1.7432316541671753
I0210 08:02:26.404198 139615872681728 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.22155459225177765, loss=1.6878156661987305
I0210 08:03:02.182553 139615864289024 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.23885118961334229, loss=1.7761479616165161
I0210 08:03:37.933227 139615872681728 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.210388645529747, loss=1.744333267211914
I0210 08:04:13.659868 139615864289024 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.22141702473163605, loss=1.7872263193130493
I0210 08:04:49.422431 139615872681728 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.1774960607290268, loss=1.7339953184127808
I0210 08:05:25.162057 139615864289024 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.19022762775421143, loss=1.704806923866272
I0210 08:06:00.931689 139615872681728 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.20868614315986633, loss=1.8053350448608398
I0210 08:06:36.671184 139615864289024 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.22417816519737244, loss=1.7370725870132446
I0210 08:07:12.451044 139615872681728 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19189633429050446, loss=1.6992335319519043
I0210 08:07:48.223156 139615864289024 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.20263411104679108, loss=1.8390809297561646
I0210 08:08:23.960723 139615872681728 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.22183439135551453, loss=1.7481439113616943
I0210 08:08:59.714480 139615864289024 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.2087443768978119, loss=1.7541234493255615
I0210 08:09:35.467054 139615872681728 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.20762848854064941, loss=1.7910988330841064
I0210 08:10:11.254832 139615864289024 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.232427716255188, loss=1.7782429456710815
I0210 08:10:28.139125 139785736898368 spec.py:321] Evaluating on the training split.
I0210 08:10:31.133622 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 08:13:51.553596 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 08:13:54.280928 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 08:16:25.142324 139785736898368 spec.py:349] Evaluating on the test split.
I0210 08:16:27.863574 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 08:18:51.302404 139785736898368 submission_runner.py:408] Time since start: 29609.51s, 	Step: 49349, 	{'train/accuracy': 0.64932781457901, 'train/loss': 1.6753994226455688, 'train/bleu': 31.92990226128372, 'validation/accuracy': 0.6689067482948303, 'validation/loss': 1.5383224487304688, 'validation/bleu': 28.933472139073533, 'validation/num_examples': 3000, 'test/accuracy': 0.6800883412361145, 'test/loss': 1.4577808380126953, 'test/bleu': 28.21943189274275, 'test/num_examples': 3003, 'score': 17671.68727684021, 'total_duration': 29609.51440000534, 'accumulated_submission_time': 17671.68727684021, 'accumulated_eval_time': 11935.63941526413, 'accumulated_logging_time': 0.6097137928009033}
I0210 08:18:51.326091 139615872681728 logging_writer.py:48] [49349] accumulated_eval_time=11935.639415, accumulated_logging_time=0.609714, accumulated_submission_time=17671.687277, global_step=49349, preemption_count=0, score=17671.687277, test/accuracy=0.680088, test/bleu=28.219432, test/loss=1.457781, test/num_examples=3003, total_duration=29609.514400, train/accuracy=0.649328, train/bleu=31.929902, train/loss=1.675399, validation/accuracy=0.668907, validation/bleu=28.933472, validation/loss=1.538322, validation/num_examples=3000
I0210 08:19:09.974101 139615864289024 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1808578073978424, loss=1.7167671918869019
I0210 08:19:45.745509 139615872681728 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.20044328272342682, loss=1.8031094074249268
I0210 08:20:21.509536 139615864289024 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.25763407349586487, loss=1.7675294876098633
I0210 08:20:57.274225 139615872681728 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.19542346894741058, loss=1.7093613147735596
I0210 08:21:33.028067 139615864289024 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.20431680977344513, loss=1.7323740720748901
I0210 08:22:08.814984 139615872681728 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.20768077671527863, loss=1.8137587308883667
I0210 08:22:44.604907 139615864289024 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2018948793411255, loss=1.7446436882019043
I0210 08:23:20.365234 139615872681728 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.19779522716999054, loss=1.7720342874526978
I0210 08:23:56.124233 139615864289024 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2647715210914612, loss=1.8204975128173828
I0210 08:24:31.868171 139615872681728 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.19880199432373047, loss=1.6662218570709229
I0210 08:25:07.622234 139615864289024 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.20249666273593903, loss=1.7180792093276978
I0210 08:25:43.384209 139615872681728 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.17829041182994843, loss=1.7586098909378052
I0210 08:26:19.117672 139615864289024 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.28535977005958557, loss=1.7458524703979492
I0210 08:26:54.860321 139615872681728 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.20334134995937347, loss=1.8373582363128662
I0210 08:27:30.608660 139615864289024 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.19881147146224976, loss=1.6686322689056396
I0210 08:28:06.327543 139615872681728 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.21722577512264252, loss=1.755185842514038
I0210 08:28:42.119417 139615864289024 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.22301247715950012, loss=1.7246662378311157
I0210 08:29:17.925752 139615872681728 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.19157719612121582, loss=1.6886483430862427
I0210 08:29:53.703857 139615864289024 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.19796456396579742, loss=1.792861819267273
I0210 08:30:29.427052 139615872681728 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.22861763834953308, loss=1.7698276042938232
I0210 08:31:05.190749 139615864289024 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.22331300377845764, loss=1.8060462474822998
I0210 08:31:40.920008 139615872681728 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1991075724363327, loss=1.6348682641983032
I0210 08:32:16.648012 139615864289024 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20652885735034943, loss=1.7774220705032349
I0210 08:32:51.392808 139785736898368 spec.py:321] Evaluating on the training split.
I0210 08:32:54.387657 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 08:36:53.903043 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 08:36:56.599909 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 08:40:38.322461 139785736898368 spec.py:349] Evaluating on the test split.
I0210 08:40:41.020533 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 08:44:15.949239 139785736898368 submission_runner.py:408] Time since start: 31134.16s, 	Step: 51699, 	{'train/accuracy': 0.6542662978172302, 'train/loss': 1.6272282600402832, 'train/bleu': 32.459203013070265, 'validation/accuracy': 0.6693159341812134, 'validation/loss': 1.5343632698059082, 'validation/bleu': 28.620420565903288, 'validation/num_examples': 3000, 'test/accuracy': 0.6813085079193115, 'test/loss': 1.4497711658477783, 'test/bleu': 28.43325483477639, 'test/num_examples': 3003, 'score': 18511.66797375679, 'total_duration': 31134.161219596863, 'accumulated_submission_time': 18511.66797375679, 'accumulated_eval_time': 12620.1957821846, 'accumulated_logging_time': 0.643932580947876}
I0210 08:44:15.971194 139615872681728 logging_writer.py:48] [51699] accumulated_eval_time=12620.195782, accumulated_logging_time=0.643933, accumulated_submission_time=18511.667974, global_step=51699, preemption_count=0, score=18511.667974, test/accuracy=0.681309, test/bleu=28.433255, test/loss=1.449771, test/num_examples=3003, total_duration=31134.161220, train/accuracy=0.654266, train/bleu=32.459203, train/loss=1.627228, validation/accuracy=0.669316, validation/bleu=28.620421, validation/loss=1.534363, validation/num_examples=3000
I0210 08:44:16.704542 139615864289024 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2141922414302826, loss=1.7222886085510254
I0210 08:44:52.403141 139615872681728 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.23320429027080536, loss=1.8013300895690918
I0210 08:45:28.320914 139615864289024 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.20587176084518433, loss=1.7784267663955688
I0210 08:46:04.077083 139615872681728 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.21089185774326324, loss=1.7624212503433228
I0210 08:46:39.920182 139615864289024 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.1942245066165924, loss=1.774290680885315
I0210 08:47:15.754736 139615872681728 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.19871735572814941, loss=1.701250433921814
I0210 08:47:51.548130 139615864289024 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.23053103685379028, loss=1.7983171939849854
I0210 08:48:27.323435 139615872681728 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.18785026669502258, loss=1.7483177185058594
I0210 08:49:03.069364 139615864289024 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19635197520256042, loss=1.7135980129241943
I0210 08:49:38.818605 139615872681728 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.28461241722106934, loss=1.7018938064575195
I0210 08:50:14.588584 139615864289024 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.20312675833702087, loss=1.7655853033065796
I0210 08:50:50.311608 139615872681728 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.23834657669067383, loss=1.7465782165527344
I0210 08:51:26.125553 139615864289024 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18367931246757507, loss=1.7030521631240845
I0210 08:52:01.986100 139615872681728 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.20707859098911285, loss=1.775280237197876
I0210 08:52:37.729341 139615864289024 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.25567999482154846, loss=1.740189790725708
I0210 08:53:13.456887 139615872681728 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.23904217779636383, loss=1.7077150344848633
I0210 08:53:49.198399 139615864289024 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2040836662054062, loss=1.68337881565094
I0210 08:54:24.948108 139615872681728 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.29614871740341187, loss=1.6947510242462158
I0210 08:55:00.770472 139615864289024 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.19586287438869476, loss=1.7297372817993164
I0210 08:55:36.527398 139615872681728 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.20891816914081573, loss=1.7353779077529907
I0210 08:56:12.284467 139615864289024 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2263597548007965, loss=1.6552841663360596
I0210 08:56:48.037332 139615872681728 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2057250440120697, loss=1.7059829235076904
I0210 08:57:23.757886 139615864289024 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.20280878245830536, loss=1.7191838026046753
I0210 08:57:59.550619 139615872681728 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2204095870256424, loss=1.7447696924209595
I0210 08:58:16.062075 139785736898368 spec.py:321] Evaluating on the training split.
I0210 08:58:19.055338 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:02:03.589560 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 09:02:06.303579 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:05:33.065439 139785736898368 spec.py:349] Evaluating on the test split.
I0210 09:05:35.792034 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:09:18.645787 139785736898368 submission_runner.py:408] Time since start: 32636.86s, 	Step: 54048, 	{'train/accuracy': 0.6529883146286011, 'train/loss': 1.654189944267273, 'train/bleu': 32.510313488174106, 'validation/accuracy': 0.6688695549964905, 'validation/loss': 1.5301557779312134, 'validation/bleu': 28.57319480788474, 'validation/num_examples': 3000, 'test/accuracy': 0.68177330493927, 'test/loss': 1.448590636253357, 'test/bleu': 28.328816470806622, 'test/num_examples': 3003, 'score': 19351.668765306473, 'total_duration': 32636.85775065422, 'accumulated_submission_time': 19351.668765306473, 'accumulated_eval_time': 13282.779415607452, 'accumulated_logging_time': 0.6774072647094727}
I0210 09:09:18.672586 139615864289024 logging_writer.py:48] [54048] accumulated_eval_time=13282.779416, accumulated_logging_time=0.677407, accumulated_submission_time=19351.668765, global_step=54048, preemption_count=0, score=19351.668765, test/accuracy=0.681773, test/bleu=28.328816, test/loss=1.448591, test/num_examples=3003, total_duration=32636.857751, train/accuracy=0.652988, train/bleu=32.510313, train/loss=1.654190, validation/accuracy=0.668870, validation/bleu=28.573195, validation/loss=1.530156, validation/num_examples=3000
I0210 09:09:37.644629 139615872681728 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.19426223635673523, loss=1.6688472032546997
I0210 09:10:13.475594 139615864289024 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.22753556072711945, loss=1.811234951019287
I0210 09:10:49.237054 139615872681728 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2032582312822342, loss=1.655380129814148
I0210 09:11:24.961367 139615864289024 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.18804819881916046, loss=1.6817002296447754
I0210 09:12:00.705120 139615872681728 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.20454660058021545, loss=1.6719428300857544
I0210 09:12:36.537537 139615864289024 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.21403123438358307, loss=1.7566386461257935
I0210 09:13:12.366650 139615872681728 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.18915267288684845, loss=1.676947832107544
I0210 09:13:48.184880 139615864289024 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.188629612326622, loss=1.7028597593307495
I0210 09:14:23.962306 139615872681728 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.19512751698493958, loss=1.7054580450057983
I0210 09:14:59.751032 139615864289024 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.18079964816570282, loss=1.7077304124832153
I0210 09:15:35.520411 139615872681728 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.22148200869560242, loss=1.6984809637069702
I0210 09:16:11.314949 139615864289024 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.253157377243042, loss=1.6632328033447266
I0210 09:16:47.047498 139615872681728 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.2094210684299469, loss=1.7844531536102295
I0210 09:17:22.790608 139615864289024 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.18722973763942719, loss=1.753609538078308
I0210 09:17:58.531307 139615872681728 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.18939344584941864, loss=1.7449157238006592
I0210 09:18:34.288341 139615864289024 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.42540639638900757, loss=1.8338212966918945
I0210 09:19:10.147821 139615872681728 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19041593372821808, loss=1.7631951570510864
I0210 09:19:45.957404 139615864289024 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2267237901687622, loss=1.7488917112350464
I0210 09:20:21.756414 139615872681728 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.22290948033332825, loss=1.6651618480682373
I0210 09:20:57.503743 139615864289024 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2178511619567871, loss=1.674087643623352
I0210 09:21:33.268660 139615872681728 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.19318902492523193, loss=1.6497387886047363
I0210 09:22:09.065635 139615864289024 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19538743793964386, loss=1.7374402284622192
I0210 09:22:44.830940 139615872681728 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.22052015364170074, loss=1.71116304397583
I0210 09:23:18.853546 139785736898368 spec.py:321] Evaluating on the training split.
I0210 09:23:21.848785 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:27:37.047192 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 09:27:39.752096 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:30:36.964639 139785736898368 spec.py:349] Evaluating on the test split.
I0210 09:30:39.663833 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:33:20.591377 139785736898368 submission_runner.py:408] Time since start: 34078.80s, 	Step: 56397, 	{'train/accuracy': 0.68346107006073, 'train/loss': 1.446035623550415, 'train/bleu': 34.018130430164184, 'validation/accuracy': 0.6704070568084717, 'validation/loss': 1.5175199508666992, 'validation/bleu': 29.086346659127816, 'validation/num_examples': 3000, 'test/accuracy': 0.682877242565155, 'test/loss': 1.4382997751235962, 'test/bleu': 28.169233789147054, 'test/num_examples': 3003, 'score': 20191.756650686264, 'total_duration': 34078.80337572098, 'accumulated_submission_time': 20191.756650686264, 'accumulated_eval_time': 13884.517220497131, 'accumulated_logging_time': 0.7161824703216553}
I0210 09:33:20.614356 139615864289024 logging_writer.py:48] [56397] accumulated_eval_time=13884.517220, accumulated_logging_time=0.716182, accumulated_submission_time=20191.756651, global_step=56397, preemption_count=0, score=20191.756651, test/accuracy=0.682877, test/bleu=28.169234, test/loss=1.438300, test/num_examples=3003, total_duration=34078.803376, train/accuracy=0.683461, train/bleu=34.018130, train/loss=1.446036, validation/accuracy=0.670407, validation/bleu=29.086347, validation/loss=1.517520, validation/num_examples=3000
I0210 09:33:22.055072 139615872681728 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19754531979560852, loss=1.748450756072998
I0210 09:33:57.796448 139615864289024 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2040484994649887, loss=1.6796635389328003
I0210 09:34:33.616523 139615872681728 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2102118879556656, loss=1.7974803447723389
I0210 09:35:09.430030 139615864289024 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.19133540987968445, loss=1.6406317949295044
I0210 09:35:45.221830 139615872681728 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.19656580686569214, loss=1.7867449522018433
I0210 09:36:20.968711 139615864289024 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.19784826040267944, loss=1.6948115825653076
I0210 09:36:56.690701 139615872681728 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.19562482833862305, loss=1.7364014387130737
I0210 09:37:32.429840 139615864289024 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19936645030975342, loss=1.702974557876587
I0210 09:38:08.172002 139615872681728 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1903490126132965, loss=1.7388969659805298
I0210 09:38:43.941975 139615864289024 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20488445460796356, loss=1.818350076675415
I0210 09:39:19.691780 139615872681728 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2731601595878601, loss=1.718499779701233
I0210 09:39:55.473654 139615864289024 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2076343446969986, loss=1.6644278764724731
I0210 09:40:31.248054 139615872681728 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.18529242277145386, loss=1.6520861387252808
I0210 09:41:07.061504 139615864289024 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2127801775932312, loss=1.7228434085845947
I0210 09:41:42.786262 139615872681728 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.200836181640625, loss=1.6941086053848267
I0210 09:42:18.555146 139615864289024 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2079063355922699, loss=1.6776618957519531
I0210 09:42:54.352786 139615872681728 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.1824599951505661, loss=1.662257194519043
I0210 09:43:30.111027 139615864289024 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.18831248581409454, loss=1.6279937028884888
I0210 09:44:05.842051 139615872681728 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19081203639507294, loss=1.6574833393096924
I0210 09:44:41.577933 139615864289024 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.20026381313800812, loss=1.7026619911193848
I0210 09:45:17.327775 139615872681728 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.21637730300426483, loss=1.670209527015686
I0210 09:45:53.069960 139615864289024 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.22050443291664124, loss=1.660380482673645
I0210 09:46:28.867673 139615872681728 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.18915045261383057, loss=1.7154849767684937
I0210 09:47:04.679005 139615864289024 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2023410201072693, loss=1.730640172958374
I0210 09:47:20.826988 139785736898368 spec.py:321] Evaluating on the training split.
I0210 09:47:23.824482 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:51:30.241111 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 09:51:32.951162 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:54:24.474891 139785736898368 spec.py:349] Evaluating on the test split.
I0210 09:54:27.189185 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 09:57:09.942441 139785736898368 submission_runner.py:408] Time since start: 35508.15s, 	Step: 58747, 	{'train/accuracy': 0.6575668454170227, 'train/loss': 1.6168078184127808, 'train/bleu': 32.5052435486537, 'validation/accuracy': 0.6729984879493713, 'validation/loss': 1.5107536315917969, 'validation/bleu': 29.125088035511126, 'validation/num_examples': 3000, 'test/accuracy': 0.6855267286300659, 'test/loss': 1.4299596548080444, 'test/bleu': 28.621032891839445, 'test/num_examples': 3003, 'score': 21031.882925748825, 'total_duration': 35508.15443897247, 'accumulated_submission_time': 21031.882925748825, 'accumulated_eval_time': 14473.632624387741, 'accumulated_logging_time': 0.7506313323974609}
I0210 09:57:09.964798 139615872681728 logging_writer.py:48] [58747] accumulated_eval_time=14473.632624, accumulated_logging_time=0.750631, accumulated_submission_time=21031.882926, global_step=58747, preemption_count=0, score=21031.882926, test/accuracy=0.685527, test/bleu=28.621033, test/loss=1.429960, test/num_examples=3003, total_duration=35508.154439, train/accuracy=0.657567, train/bleu=32.505244, train/loss=1.616808, validation/accuracy=0.672998, validation/bleu=29.125088, validation/loss=1.510754, validation/num_examples=3000
I0210 09:57:29.272943 139615864289024 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2078254520893097, loss=1.742294192314148
I0210 09:58:05.018905 139615872681728 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.21974629163742065, loss=1.7380425930023193
I0210 09:58:40.767539 139615864289024 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.19497543573379517, loss=1.6986474990844727
I0210 09:59:16.526194 139615872681728 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.1898851990699768, loss=1.722253680229187
I0210 09:59:52.290027 139615864289024 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.20491057634353638, loss=1.7096993923187256
I0210 10:00:28.065856 139615872681728 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2243787795305252, loss=1.723603367805481
I0210 10:01:03.868834 139615864289024 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.20774544775485992, loss=1.7353278398513794
I0210 10:01:39.622372 139615872681728 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.22055071592330933, loss=1.701985239982605
I0210 10:02:15.391909 139615864289024 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2047969102859497, loss=1.6891334056854248
I0210 10:02:51.167553 139615872681728 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.19681701064109802, loss=1.6579221487045288
I0210 10:03:26.943655 139615864289024 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.1909020096063614, loss=1.6979129314422607
I0210 10:04:02.766249 139615872681728 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.29633966088294983, loss=1.5890337228775024
I0210 10:04:38.540350 139615864289024 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.17927691340446472, loss=1.6728923320770264
I0210 10:05:14.340517 139615872681728 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.18615998327732086, loss=1.718516230583191
I0210 10:05:50.101233 139615864289024 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.19388149678707123, loss=1.744767665863037
I0210 10:06:25.856114 139615872681728 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.22705751657485962, loss=1.7182621955871582
I0210 10:07:01.678715 139615864289024 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.22630959749221802, loss=1.6703206300735474
I0210 10:07:37.433743 139615872681728 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2311335951089859, loss=1.64584219455719
I0210 10:08:13.174317 139615864289024 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1932329684495926, loss=1.7229163646697998
I0210 10:08:48.934231 139615872681728 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.20895808935165405, loss=1.6539274454116821
I0210 10:09:24.729276 139615864289024 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.20622019469738007, loss=1.7613451480865479
I0210 10:10:00.491382 139615872681728 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.21168780326843262, loss=1.8127555847167969
I0210 10:10:36.258555 139615864289024 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2548553943634033, loss=1.7386642694473267
I0210 10:11:09.943680 139785736898368 spec.py:321] Evaluating on the training split.
I0210 10:11:12.935606 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 10:15:03.838565 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 10:15:06.547721 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 10:17:35.531577 139785736898368 spec.py:349] Evaluating on the test split.
I0210 10:17:38.242671 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 10:19:55.249398 139785736898368 submission_runner.py:408] Time since start: 36873.46s, 	Step: 61096, 	{'train/accuracy': 0.6561620235443115, 'train/loss': 1.6366279125213623, 'train/bleu': 32.63350373980873, 'validation/accuracy': 0.6729736924171448, 'validation/loss': 1.512277603149414, 'validation/bleu': 29.140785546387807, 'validation/num_examples': 3000, 'test/accuracy': 0.6879902482032776, 'test/loss': 1.4211199283599854, 'test/bleu': 29.141972118259318, 'test/num_examples': 3003, 'score': 21871.777238607407, 'total_duration': 36873.4613969326, 'accumulated_submission_time': 21871.777238607407, 'accumulated_eval_time': 14998.93830871582, 'accumulated_logging_time': 0.7828867435455322}
I0210 10:19:55.272048 139615872681728 logging_writer.py:48] [61096] accumulated_eval_time=14998.938309, accumulated_logging_time=0.782887, accumulated_submission_time=21871.777239, global_step=61096, preemption_count=0, score=21871.777239, test/accuracy=0.687990, test/bleu=29.141972, test/loss=1.421120, test/num_examples=3003, total_duration=36873.461397, train/accuracy=0.656162, train/bleu=32.633504, train/loss=1.636628, validation/accuracy=0.672974, validation/bleu=29.140786, validation/loss=1.512278, validation/num_examples=3000
I0210 10:19:57.069759 139615864289024 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.23802918195724487, loss=1.679694652557373
I0210 10:20:32.806393 139615872681728 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.18805770576000214, loss=1.6871490478515625
I0210 10:21:08.553839 139615864289024 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.19649098813533783, loss=1.6848196983337402
I0210 10:21:44.377746 139615872681728 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.1963234841823578, loss=1.6946333646774292
I0210 10:22:20.124692 139615864289024 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.21276436746120453, loss=1.6783344745635986
I0210 10:22:55.861990 139615872681728 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.20013658702373505, loss=1.6272320747375488
I0210 10:23:31.602180 139615864289024 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.186784029006958, loss=1.6763314008712769
I0210 10:24:07.402150 139615872681728 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.2206464260816574, loss=1.7331430912017822
I0210 10:24:43.174888 139615864289024 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.18389612436294556, loss=1.6362173557281494
I0210 10:25:18.970872 139615872681728 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.2014155387878418, loss=1.8105155229568481
I0210 10:25:54.731574 139615864289024 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.1962914913892746, loss=1.6170510053634644
I0210 10:26:30.489385 139615872681728 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19758063554763794, loss=1.688348650932312
I0210 10:27:06.236050 139615864289024 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.2150116115808487, loss=1.684258222579956
I0210 10:27:42.044578 139615872681728 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.20250175893306732, loss=1.6777074337005615
I0210 10:28:17.920149 139615864289024 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20995692908763885, loss=1.7286429405212402
I0210 10:28:53.738449 139615872681728 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.20896339416503906, loss=1.6934617757797241
I0210 10:29:29.533064 139615864289024 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.1990254670381546, loss=1.7268555164337158
I0210 10:30:05.337418 139615872681728 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.19504818320274353, loss=1.6488131284713745
I0210 10:30:41.172535 139615864289024 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.1941736340522766, loss=1.675767183303833
I0210 10:31:16.992074 139615872681728 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.24333424866199493, loss=1.7279809713363647
I0210 10:31:52.792483 139615864289024 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.1954907327890396, loss=1.6345592737197876
I0210 10:32:28.579345 139615872681728 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.18754272162914276, loss=1.5975557565689087
I0210 10:33:04.390571 139615864289024 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.1799897700548172, loss=1.6727783679962158
I0210 10:33:40.188321 139615872681728 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.18954788148403168, loss=1.7262852191925049
I0210 10:33:55.298246 139785736898368 spec.py:321] Evaluating on the training split.
I0210 10:33:58.301464 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 10:37:48.968861 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 10:37:51.687299 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 10:40:29.965672 139785736898368 spec.py:349] Evaluating on the test split.
I0210 10:40:32.678696 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 10:43:14.932283 139785736898368 submission_runner.py:408] Time since start: 38273.14s, 	Step: 63444, 	{'train/accuracy': 0.6674286723136902, 'train/loss': 1.547412633895874, 'train/bleu': 33.49294815381641, 'validation/accuracy': 0.6748335361480713, 'validation/loss': 1.4961423873901367, 'validation/bleu': 29.477780759940647, 'validation/num_examples': 3000, 'test/accuracy': 0.6889896392822266, 'test/loss': 1.410127878189087, 'test/bleu': 28.9947816834239, 'test/num_examples': 3003, 'score': 22711.711676359177, 'total_duration': 38273.14426493645, 'accumulated_submission_time': 22711.711676359177, 'accumulated_eval_time': 15558.572283506393, 'accumulated_logging_time': 0.8171112537384033}
I0210 10:43:14.955629 139615864289024 logging_writer.py:48] [63444] accumulated_eval_time=15558.572284, accumulated_logging_time=0.817111, accumulated_submission_time=22711.711676, global_step=63444, preemption_count=0, score=22711.711676, test/accuracy=0.688990, test/bleu=28.994782, test/loss=1.410128, test/num_examples=3003, total_duration=38273.144265, train/accuracy=0.667429, train/bleu=33.492948, train/loss=1.547413, validation/accuracy=0.674834, validation/bleu=29.477781, validation/loss=1.496142, validation/num_examples=3000
I0210 10:43:35.375849 139615872681728 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.4337325394153595, loss=1.7421684265136719
I0210 10:44:11.145049 139615864289024 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2081948220729828, loss=1.7455781698226929
I0210 10:44:46.906895 139615872681728 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20497241616249084, loss=1.7233538627624512
I0210 10:45:22.675980 139615864289024 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.1928822100162506, loss=1.729828953742981
I0210 10:45:58.435779 139615872681728 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.1903780996799469, loss=1.6963493824005127
I0210 10:46:34.246021 139615864289024 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.19215752184391022, loss=1.7063570022583008
I0210 10:47:09.989821 139615872681728 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.220358744263649, loss=1.5769565105438232
I0210 10:47:45.697156 139615864289024 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.19089898467063904, loss=1.7821598052978516
I0210 10:48:21.463227 139615872681728 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19995605945587158, loss=1.6932847499847412
I0210 10:48:57.272119 139615864289024 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2094637155532837, loss=1.8169047832489014
I0210 10:49:33.008660 139615872681728 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2059057056903839, loss=1.738703727722168
I0210 10:50:08.845636 139615864289024 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.1981080174446106, loss=1.716823697090149
I0210 10:50:44.660782 139615872681728 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19443312287330627, loss=1.6121470928192139
I0210 10:51:20.444190 139615864289024 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.189759761095047, loss=1.7204715013504028
I0210 10:51:56.196853 139615872681728 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.19521956145763397, loss=1.7222658395767212
I0210 10:52:31.947946 139615864289024 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19735746085643768, loss=1.706786870956421
I0210 10:53:07.705390 139615872681728 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.19277934730052948, loss=1.6568032503128052
I0210 10:53:43.422548 139615864289024 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.23221080005168915, loss=1.7081605195999146
I0210 10:54:19.176500 139615872681728 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.21478290855884552, loss=1.579322099685669
I0210 10:54:54.902694 139615864289024 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.20555183291435242, loss=1.7757591009140015
I0210 10:55:30.671032 139615872681728 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.19105692207813263, loss=1.7188793420791626
I0210 10:56:06.410675 139615864289024 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2280011624097824, loss=1.7274234294891357
I0210 10:56:42.171416 139615872681728 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20967496931552887, loss=1.6081373691558838
I0210 10:57:15.179317 139785736898368 spec.py:321] Evaluating on the training split.
I0210 10:57:18.188929 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:01:21.577442 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 11:01:24.298242 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:03:57.912544 139785736898368 spec.py:349] Evaluating on the test split.
I0210 11:04:00.641426 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:06:36.506393 139785736898368 submission_runner.py:408] Time since start: 39674.72s, 	Step: 65794, 	{'train/accuracy': 0.6565250754356384, 'train/loss': 1.6203813552856445, 'train/bleu': 32.66615467451501, 'validation/accuracy': 0.676073431968689, 'validation/loss': 1.4927388429641724, 'validation/bleu': 29.329756231686886, 'validation/num_examples': 3000, 'test/accuracy': 0.6903724670410156, 'test/loss': 1.4074007272720337, 'test/bleu': 29.564089044554855, 'test/num_examples': 3003, 'score': 23551.849817037582, 'total_duration': 39674.71835613251, 'accumulated_submission_time': 23551.849817037582, 'accumulated_eval_time': 16119.899282455444, 'accumulated_logging_time': 0.8502511978149414}
I0210 11:06:36.534192 139615864289024 logging_writer.py:48] [65794] accumulated_eval_time=16119.899282, accumulated_logging_time=0.850251, accumulated_submission_time=23551.849817, global_step=65794, preemption_count=0, score=23551.849817, test/accuracy=0.690372, test/bleu=29.564089, test/loss=1.407401, test/num_examples=3003, total_duration=39674.718356, train/accuracy=0.656525, train/bleu=32.666155, train/loss=1.620381, validation/accuracy=0.676073, validation/bleu=29.329756, validation/loss=1.492739, validation/num_examples=3000
I0210 11:06:39.063593 139615872681728 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.19220885634422302, loss=1.6557155847549438
I0210 11:07:14.845940 139615864289024 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.1919163316488266, loss=1.691976547241211
I0210 11:07:50.688895 139615872681728 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19443294405937195, loss=1.6218818426132202
I0210 11:08:26.478971 139615864289024 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2031095325946808, loss=1.6579712629318237
I0210 11:09:02.291890 139615872681728 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.18840302526950836, loss=1.7178856134414673
I0210 11:09:38.024867 139615864289024 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2000720500946045, loss=1.647422194480896
I0210 11:10:13.784756 139615872681728 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.1971510499715805, loss=1.6361703872680664
I0210 11:10:49.557104 139615864289024 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2025710642337799, loss=1.6995539665222168
I0210 11:11:25.311430 139615872681728 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.18943703174591064, loss=1.7364214658737183
I0210 11:12:01.086512 139615864289024 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.25425946712493896, loss=1.684572696685791
I0210 11:12:36.891455 139615872681728 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2056281715631485, loss=1.747020959854126
I0210 11:13:12.630933 139615864289024 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.18951725959777832, loss=1.602757215499878
I0210 11:13:48.361529 139615872681728 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2247152477502823, loss=1.7268801927566528
I0210 11:14:24.078345 139615864289024 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.19121479988098145, loss=1.7153311967849731
I0210 11:14:59.818449 139615872681728 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.19908007979393005, loss=1.6443367004394531
I0210 11:15:35.583964 139615864289024 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.1810045689344406, loss=1.6273114681243896
I0210 11:16:11.328613 139615872681728 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.22082284092903137, loss=1.6746532917022705
I0210 11:16:47.063445 139615864289024 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.19636978209018707, loss=1.6473512649536133
I0210 11:17:22.808805 139615872681728 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.198643758893013, loss=1.648674488067627
I0210 11:17:58.604210 139615864289024 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.19535021483898163, loss=1.6393266916275024
I0210 11:18:34.382867 139615872681728 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.21022938191890717, loss=1.6765433549880981
I0210 11:19:10.139397 139615864289024 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2092105597257614, loss=1.7457642555236816
I0210 11:19:45.885175 139615872681728 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.22000767290592194, loss=1.5847561359405518
I0210 11:20:21.686261 139615864289024 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.25087201595306396, loss=1.712784767150879
I0210 11:20:36.765093 139785736898368 spec.py:321] Evaluating on the training split.
I0210 11:20:39.769122 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:24:43.163058 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 11:24:45.886088 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:27:28.886393 139785736898368 spec.py:349] Evaluating on the test split.
I0210 11:27:31.601184 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:29:59.884998 139785736898368 submission_runner.py:408] Time since start: 41078.10s, 	Step: 68144, 	{'train/accuracy': 0.6547269821166992, 'train/loss': 1.634839653968811, 'train/bleu': 32.47219353442573, 'validation/accuracy': 0.677561342716217, 'validation/loss': 1.4846450090408325, 'validation/bleu': 29.4492775140218, 'validation/num_examples': 3000, 'test/accuracy': 0.6897798180580139, 'test/loss': 1.4021321535110474, 'test/bleu': 29.068497099009715, 'test/num_examples': 3003, 'score': 24391.992929935455, 'total_duration': 41078.09699392319, 'accumulated_submission_time': 24391.992929935455, 'accumulated_eval_time': 16683.019134521484, 'accumulated_logging_time': 0.8892166614532471}
I0210 11:29:59.909725 139615872681728 logging_writer.py:48] [68144] accumulated_eval_time=16683.019135, accumulated_logging_time=0.889217, accumulated_submission_time=24391.992930, global_step=68144, preemption_count=0, score=24391.992930, test/accuracy=0.689780, test/bleu=29.068497, test/loss=1.402132, test/num_examples=3003, total_duration=41078.096994, train/accuracy=0.654727, train/bleu=32.472194, train/loss=1.634840, validation/accuracy=0.677561, validation/bleu=29.449278, validation/loss=1.484645, validation/num_examples=3000
I0210 11:30:20.281696 139615864289024 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.198776975274086, loss=1.8113999366760254
I0210 11:30:56.020275 139615872681728 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.20284543931484222, loss=1.6644339561462402
I0210 11:31:31.765935 139615864289024 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.18301646411418915, loss=1.6205942630767822
I0210 11:32:07.531309 139615872681728 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.23153991997241974, loss=1.661453366279602
I0210 11:32:43.301734 139615864289024 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.18592722713947296, loss=1.6774747371673584
I0210 11:33:19.081150 139615872681728 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.19385814666748047, loss=1.6755634546279907
I0210 11:33:54.903517 139615864289024 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.21024173498153687, loss=1.7588247060775757
I0210 11:34:30.628189 139615872681728 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.18594881892204285, loss=1.6488173007965088
I0210 11:35:06.380439 139615864289024 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.21937212347984314, loss=1.6063838005065918
I0210 11:35:42.116440 139615872681728 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.19742637872695923, loss=1.7324721813201904
I0210 11:36:17.844022 139615864289024 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.20969633758068085, loss=1.6580439805984497
I0210 11:36:53.597525 139615872681728 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.19592531025409698, loss=1.6366316080093384
I0210 11:37:29.344387 139615864289024 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.19987235963344574, loss=1.6790275573730469
I0210 11:38:05.075814 139615872681728 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.20668281614780426, loss=1.696779727935791
I0210 11:38:40.806378 139615864289024 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19080621004104614, loss=1.6046321392059326
I0210 11:39:16.577861 139615872681728 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.20396047830581665, loss=1.764259934425354
I0210 11:39:52.325288 139615864289024 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.19156798720359802, loss=1.6210484504699707
I0210 11:40:28.222215 139615872681728 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.21805019676685333, loss=1.6911216974258423
I0210 11:41:04.010482 139615864289024 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.18809331953525543, loss=1.5810438394546509
I0210 11:41:39.768455 139615872681728 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.18547993898391724, loss=1.6346098184585571
I0210 11:42:15.554359 139615864289024 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.21236301958560944, loss=1.6459424495697021
I0210 11:42:51.317741 139615872681728 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.19537845253944397, loss=1.6287935972213745
I0210 11:43:27.100706 139615864289024 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2267419695854187, loss=1.6086294651031494
I0210 11:44:00.040190 139785736898368 spec.py:321] Evaluating on the training split.
I0210 11:44:03.044620 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:47:00.572747 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 11:47:03.283627 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:49:43.466544 139785736898368 spec.py:349] Evaluating on the test split.
I0210 11:49:46.222640 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 11:52:33.889426 139785736898368 submission_runner.py:408] Time since start: 42432.10s, 	Step: 70494, 	{'train/accuracy': 0.6648585796356201, 'train/loss': 1.5671465396881104, 'train/bleu': 32.83949660411032, 'validation/accuracy': 0.6763090491294861, 'validation/loss': 1.4787070751190186, 'validation/bleu': 29.078669687795948, 'validation/num_examples': 3000, 'test/accuracy': 0.6899541020393372, 'test/loss': 1.3922793865203857, 'test/bleu': 29.032294326235682, 'test/num_examples': 3003, 'score': 25232.03781723976, 'total_duration': 42432.101380348206, 'accumulated_submission_time': 25232.03781723976, 'accumulated_eval_time': 17196.86828827858, 'accumulated_logging_time': 0.9239933490753174}
I0210 11:52:33.920112 139615872681728 logging_writer.py:48] [70494] accumulated_eval_time=17196.868288, accumulated_logging_time=0.923993, accumulated_submission_time=25232.037817, global_step=70494, preemption_count=0, score=25232.037817, test/accuracy=0.689954, test/bleu=29.032294, test/loss=1.392279, test/num_examples=3003, total_duration=42432.101380, train/accuracy=0.664859, train/bleu=32.839497, train/loss=1.567147, validation/accuracy=0.676309, validation/bleu=29.078670, validation/loss=1.478707, validation/num_examples=3000
I0210 11:52:36.448353 139615864289024 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3252352476119995, loss=1.7359919548034668
I0210 11:53:12.247539 139615872681728 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.20284385979175568, loss=1.6029231548309326
I0210 11:53:48.052678 139615864289024 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.1885261833667755, loss=1.6308315992355347
I0210 11:54:23.914999 139615872681728 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.21110472083091736, loss=1.6676539182662964
I0210 11:54:59.771808 139615864289024 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.19316090643405914, loss=1.6556646823883057
I0210 11:55:35.523213 139615872681728 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20227807760238647, loss=1.638558268547058
I0210 11:56:11.321692 139615864289024 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.22223863005638123, loss=1.6444551944732666
I0210 11:56:47.097793 139615872681728 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.20870408415794373, loss=1.6733663082122803
I0210 11:57:22.877317 139615864289024 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.21055148541927338, loss=1.6169613599777222
I0210 11:57:58.656580 139615872681728 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.20578500628471375, loss=1.674353003501892
I0210 11:58:34.445803 139615864289024 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.20194493234157562, loss=1.6892306804656982
I0210 11:59:10.212545 139615872681728 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.19548651576042175, loss=1.653259038925171
I0210 11:59:46.003179 139615864289024 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2511654496192932, loss=1.6147910356521606
I0210 12:00:21.735504 139615872681728 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.20136423408985138, loss=1.6491669416427612
I0210 12:00:57.507442 139615864289024 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.20587138831615448, loss=1.538833737373352
I0210 12:01:33.343918 139615872681728 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.19693563878536224, loss=1.6951863765716553
I0210 12:02:09.094094 139615864289024 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.18948477506637573, loss=1.6437312364578247
I0210 12:02:44.875674 139615872681728 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.20716121792793274, loss=1.682883858680725
I0210 12:03:20.670945 139615864289024 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2028866559267044, loss=1.5996999740600586
I0210 12:03:56.441924 139615872681728 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.20806005597114563, loss=1.6293696165084839
I0210 12:04:32.184836 139615864289024 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.19731298089027405, loss=1.631081461906433
I0210 12:05:07.949733 139615872681728 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2246060073375702, loss=1.5803756713867188
I0210 12:05:43.688718 139615864289024 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.20115458965301514, loss=1.652280569076538
I0210 12:06:19.437432 139615872681728 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.49471917748451233, loss=1.678993821144104
I0210 12:06:34.178621 139785736898368 spec.py:321] Evaluating on the training split.
I0210 12:06:37.183528 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:09:42.622357 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 12:09:45.355711 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:12:32.633669 139785736898368 spec.py:349] Evaluating on the test split.
I0210 12:12:35.350911 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:15:10.132270 139785736898368 submission_runner.py:408] Time since start: 43788.34s, 	Step: 72843, 	{'train/accuracy': 0.6621139049530029, 'train/loss': 1.5815826654434204, 'train/bleu': 33.390701136287404, 'validation/accuracy': 0.6767553687095642, 'validation/loss': 1.473919153213501, 'validation/bleu': 29.352687747763873, 'validation/num_examples': 3000, 'test/accuracy': 0.6910812854766846, 'test/loss': 1.3824280500411987, 'test/bleu': 29.189779204845138, 'test/num_examples': 3003, 'score': 26072.20747256279, 'total_duration': 43788.344264507294, 'accumulated_submission_time': 26072.20747256279, 'accumulated_eval_time': 17712.821888685226, 'accumulated_logging_time': 0.9654061794281006}
I0210 12:15:10.158237 139615864289024 logging_writer.py:48] [72843] accumulated_eval_time=17712.821889, accumulated_logging_time=0.965406, accumulated_submission_time=26072.207473, global_step=72843, preemption_count=0, score=26072.207473, test/accuracy=0.691081, test/bleu=29.189779, test/loss=1.382428, test/num_examples=3003, total_duration=43788.344265, train/accuracy=0.662114, train/bleu=33.390701, train/loss=1.581583, validation/accuracy=0.676755, validation/bleu=29.352688, validation/loss=1.473919, validation/num_examples=3000
I0210 12:15:30.889152 139615872681728 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.21402579545974731, loss=1.6641833782196045
I0210 12:16:06.650927 139615864289024 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2063068300485611, loss=1.6753082275390625
I0210 12:16:42.456512 139615872681728 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.21553485095500946, loss=1.6227139234542847
I0210 12:17:18.239811 139615864289024 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.18916389346122742, loss=1.654266119003296
I0210 12:17:54.013057 139615872681728 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.22633546590805054, loss=1.636124849319458
I0210 12:18:29.797019 139615864289024 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.19896288216114044, loss=1.703963279724121
I0210 12:19:05.544561 139615872681728 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.18608351051807404, loss=1.5984373092651367
I0210 12:19:41.277578 139615864289024 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.19367389380931854, loss=1.5885906219482422
I0210 12:20:17.035841 139615872681728 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.19373588263988495, loss=1.665336012840271
I0210 12:20:52.840045 139615864289024 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.19666002690792084, loss=1.693554401397705
I0210 12:21:28.577363 139615872681728 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.21002435684204102, loss=1.6645400524139404
I0210 12:22:04.322133 139615864289024 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.22526611387729645, loss=1.6912317276000977
I0210 12:22:40.098970 139615872681728 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.19703975319862366, loss=1.6452417373657227
I0210 12:23:15.884938 139615864289024 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.20414698123931885, loss=1.6469923257827759
I0210 12:23:51.671486 139615872681728 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.192288339138031, loss=1.6601905822753906
I0210 12:24:27.409577 139615864289024 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.20971955358982086, loss=1.6409342288970947
I0210 12:25:03.143986 139615872681728 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.19722869992256165, loss=1.6403568983078003
I0210 12:25:38.906818 139615864289024 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2253550887107849, loss=1.5705163478851318
I0210 12:26:14.653635 139615872681728 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.193589985370636, loss=1.6491869688034058
I0210 12:26:50.469958 139615864289024 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.20491494238376617, loss=1.695406198501587
I0210 12:27:26.231431 139615872681728 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.19806337356567383, loss=1.579490303993225
I0210 12:28:01.962242 139615864289024 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.19885462522506714, loss=1.6791949272155762
I0210 12:28:37.744976 139615872681728 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.208677738904953, loss=1.621863603591919
I0210 12:29:10.420404 139785736898368 spec.py:321] Evaluating on the training split.
I0210 12:29:13.420123 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:33:07.476018 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 12:33:10.185604 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:36:06.269428 139785736898368 spec.py:349] Evaluating on the test split.
I0210 12:36:08.982402 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:38:59.042809 139785736898368 submission_runner.py:408] Time since start: 45217.25s, 	Step: 75193, 	{'train/accuracy': 0.6858174800872803, 'train/loss': 1.426164984703064, 'train/bleu': 34.366781063424106, 'validation/accuracy': 0.6785904765129089, 'validation/loss': 1.4634785652160645, 'validation/bleu': 29.514415997343658, 'validation/num_examples': 3000, 'test/accuracy': 0.693544864654541, 'test/loss': 1.378287672996521, 'test/bleu': 29.07100048590121, 'test/num_examples': 3003, 'score': 26912.381454229355, 'total_duration': 45217.25480270386, 'accumulated_submission_time': 26912.381454229355, 'accumulated_eval_time': 18301.444242954254, 'accumulated_logging_time': 1.0022211074829102}
I0210 12:38:59.069548 139615864289024 logging_writer.py:48] [75193] accumulated_eval_time=18301.444243, accumulated_logging_time=1.002221, accumulated_submission_time=26912.381454, global_step=75193, preemption_count=0, score=26912.381454, test/accuracy=0.693545, test/bleu=29.071000, test/loss=1.378288, test/num_examples=3003, total_duration=45217.254803, train/accuracy=0.685817, train/bleu=34.366781, train/loss=1.426165, validation/accuracy=0.678590, validation/bleu=29.514416, validation/loss=1.463479, validation/num_examples=3000
I0210 12:39:01.954385 139615872681728 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20557226240634918, loss=1.6050835847854614
I0210 12:39:37.697518 139615864289024 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.2095699906349182, loss=1.6381500959396362
I0210 12:40:13.459197 139615872681728 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.19082970917224884, loss=1.600338101387024
I0210 12:40:49.257204 139615864289024 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.19387809932231903, loss=1.5855847597122192
I0210 12:41:25.083819 139615872681728 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2069600224494934, loss=1.663956880569458
I0210 12:42:00.814508 139615864289024 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.19077003002166748, loss=1.5829131603240967
I0210 12:42:36.554175 139615872681728 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.21265153586864471, loss=1.638692021369934
I0210 12:43:12.338293 139615864289024 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.19663584232330322, loss=1.725097894668579
I0210 12:43:48.071067 139615872681728 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.21950294077396393, loss=1.6997772455215454
I0210 12:44:23.835795 139615864289024 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.18208065629005432, loss=1.5498007535934448
I0210 12:44:59.592207 139615872681728 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.19195064902305603, loss=1.6250661611557007
I0210 12:45:35.335314 139615864289024 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.19586868584156036, loss=1.6768397092819214
I0210 12:46:11.111528 139615872681728 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.19777585566043854, loss=1.7325243949890137
I0210 12:46:46.840489 139615864289024 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.20459286868572235, loss=1.61237633228302
I0210 12:47:22.582705 139615872681728 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.21857689321041107, loss=1.6153029203414917
I0210 12:47:58.342013 139615864289024 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.20741434395313263, loss=1.6079002618789673
I0210 12:48:34.145107 139615872681728 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.19014884531497955, loss=1.6581674814224243
I0210 12:49:09.878184 139615864289024 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.20784814655780792, loss=1.7050834894180298
I0210 12:49:45.633631 139615872681728 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.19207754731178284, loss=1.6028422117233276
I0210 12:50:21.432069 139615864289024 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.199034184217453, loss=1.6497162580490112
I0210 12:50:57.236027 139615872681728 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.19523708522319794, loss=1.6439145803451538
I0210 12:51:33.018462 139615864289024 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21752016246318817, loss=1.6406991481781006
I0210 12:52:08.768764 139615872681728 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.20083603262901306, loss=1.6648582220077515
I0210 12:52:44.508457 139615864289024 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.19425149261951447, loss=1.5986799001693726
I0210 12:52:59.226352 139785736898368 spec.py:321] Evaluating on the training split.
I0210 12:53:02.227659 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 12:57:24.551134 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 12:57:27.276699 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:00:47.022003 139785736898368 spec.py:349] Evaluating on the test split.
I0210 13:00:49.732520 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:04:43.382622 139785736898368 submission_runner.py:408] Time since start: 46761.59s, 	Step: 77543, 	{'train/accuracy': 0.6669697165489197, 'train/loss': 1.5403841733932495, 'train/bleu': 33.21220426533364, 'validation/accuracy': 0.6799419522285461, 'validation/loss': 1.4549518823623657, 'validation/bleu': 29.43968950403044, 'validation/num_examples': 3000, 'test/accuracy': 0.6951484680175781, 'test/loss': 1.3676152229309082, 'test/bleu': 29.485973265345606, 'test/num_examples': 3003, 'score': 27752.452763557434, 'total_duration': 46761.59459590912, 'accumulated_submission_time': 27752.452763557434, 'accumulated_eval_time': 19005.60043978691, 'accumulated_logging_time': 1.0388991832733154}
I0210 13:04:43.413495 139615872681728 logging_writer.py:48] [77543] accumulated_eval_time=19005.600440, accumulated_logging_time=1.038899, accumulated_submission_time=27752.452764, global_step=77543, preemption_count=0, score=27752.452764, test/accuracy=0.695148, test/bleu=29.485973, test/loss=1.367615, test/num_examples=3003, total_duration=46761.594596, train/accuracy=0.666970, train/bleu=33.212204, train/loss=1.540384, validation/accuracy=0.679942, validation/bleu=29.439690, validation/loss=1.454952, validation/num_examples=3000
I0210 13:05:04.132408 139615864289024 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.18731781840324402, loss=1.5532243251800537
I0210 13:05:39.854312 139615872681728 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.19334246218204498, loss=1.6116406917572021
I0210 13:06:15.579721 139615864289024 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.20739680528640747, loss=1.6410391330718994
I0210 13:06:51.302452 139615872681728 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2199227213859558, loss=1.766329288482666
I0210 13:07:27.076461 139615864289024 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.20338350534439087, loss=1.6801507472991943
I0210 13:08:02.830648 139615872681728 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.20153099298477173, loss=1.6016359329223633
I0210 13:08:38.611874 139615864289024 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.22255031764507294, loss=1.6378381252288818
I0210 13:09:14.462657 139615872681728 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.8762691617012024, loss=1.6760069131851196
I0210 13:09:50.269500 139615864289024 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20828236639499664, loss=1.6468689441680908
I0210 13:10:26.025505 139615872681728 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2029116153717041, loss=1.6783533096313477
I0210 13:11:01.761537 139615864289024 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.21475453674793243, loss=1.6913480758666992
I0210 13:11:37.530199 139615872681728 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2074303776025772, loss=1.6474817991256714
I0210 13:12:13.257928 139615864289024 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2079068422317505, loss=1.6251296997070312
I0210 13:12:49.034646 139615872681728 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.18893080949783325, loss=1.6250879764556885
I0210 13:13:24.784890 139615864289024 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.1986803263425827, loss=1.621071696281433
I0210 13:14:00.535448 139615872681728 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.20646902918815613, loss=1.630724310874939
I0210 13:14:36.283749 139615864289024 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.19270716607570648, loss=1.5860730409622192
I0210 13:15:12.015846 139615872681728 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.19293883442878723, loss=1.5932642221450806
I0210 13:15:47.802107 139615864289024 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.20246444642543793, loss=1.5912283658981323
I0210 13:16:23.532652 139615872681728 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.4984617531299591, loss=1.618552327156067
I0210 13:16:59.289640 139615864289024 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2823235094547272, loss=1.6214789152145386
I0210 13:17:35.042794 139615872681728 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2040530890226364, loss=1.7066662311553955
I0210 13:18:10.843224 139615864289024 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.21463949978351593, loss=1.7011102437973022
I0210 13:18:43.481954 139785736898368 spec.py:321] Evaluating on the training split.
I0210 13:18:46.498293 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:22:35.651210 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 13:22:38.365153 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:25:08.722708 139785736898368 spec.py:349] Evaluating on the test split.
I0210 13:25:11.431406 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:27:31.792261 139785736898368 submission_runner.py:408] Time since start: 48130.00s, 	Step: 79893, 	{'train/accuracy': 0.6644576787948608, 'train/loss': 1.573724389076233, 'train/bleu': 32.65712602644397, 'validation/accuracy': 0.6827317476272583, 'validation/loss': 1.4516927003860474, 'validation/bleu': 29.785093513403424, 'validation/num_examples': 3000, 'test/accuracy': 0.6960781216621399, 'test/loss': 1.3559688329696655, 'test/bleu': 29.612354085956323, 'test/num_examples': 3003, 'score': 28592.432891607285, 'total_duration': 48130.004256248474, 'accumulated_submission_time': 28592.432891607285, 'accumulated_eval_time': 19533.910708904266, 'accumulated_logging_time': 1.0820260047912598}
I0210 13:27:31.819143 139615872681728 logging_writer.py:48] [79893] accumulated_eval_time=19533.910709, accumulated_logging_time=1.082026, accumulated_submission_time=28592.432892, global_step=79893, preemption_count=0, score=28592.432892, test/accuracy=0.696078, test/bleu=29.612354, test/loss=1.355969, test/num_examples=3003, total_duration=48130.004256, train/accuracy=0.664458, train/bleu=32.657126, train/loss=1.573724, validation/accuracy=0.682732, validation/bleu=29.785094, validation/loss=1.451693, validation/num_examples=3000
I0210 13:27:34.699341 139615864289024 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.19977009296417236, loss=1.6198770999908447
I0210 13:28:10.436203 139615872681728 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2106362283229828, loss=1.5799381732940674
I0210 13:28:46.176969 139615864289024 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.21404393017292023, loss=1.7172297239303589
I0210 13:29:21.944461 139615872681728 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.2020418494939804, loss=1.6061893701553345
I0210 13:29:57.702997 139615864289024 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.21221056580543518, loss=1.6871877908706665
I0210 13:30:33.477224 139615872681728 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20369179546833038, loss=1.6226557493209839
I0210 13:31:09.239222 139615864289024 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.22433005273342133, loss=1.6456228494644165
I0210 13:31:45.027441 139615872681728 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.21961568295955658, loss=1.6199380159378052
I0210 13:32:20.770331 139615864289024 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20583748817443848, loss=1.7084341049194336
I0210 13:32:56.502514 139615872681728 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.2081202119588852, loss=1.6612180471420288
I0210 13:33:32.242579 139615864289024 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2149527221918106, loss=1.6221245527267456
I0210 13:34:07.988520 139615872681728 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.2026308923959732, loss=1.5637084245681763
I0210 13:34:43.787810 139615864289024 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.20121347904205322, loss=1.6093159914016724
I0210 13:35:19.614786 139615872681728 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.20868298411369324, loss=1.6401782035827637
I0210 13:35:55.441157 139615864289024 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.19544067978858948, loss=1.5834449529647827
I0210 13:36:31.220208 139615872681728 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.20626047253608704, loss=1.5961577892303467
I0210 13:37:06.953937 139615864289024 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.18933331966400146, loss=1.5279489755630493
I0210 13:37:42.689182 139615872681728 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.20771777629852295, loss=1.6066080331802368
I0210 13:38:18.490297 139615864289024 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.19550937414169312, loss=1.631894588470459
I0210 13:38:54.269403 139615872681728 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.1923617124557495, loss=1.6339788436889648
I0210 13:39:30.047411 139615864289024 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.20735837519168854, loss=1.6193666458129883
I0210 13:40:05.792162 139615872681728 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.21201898157596588, loss=1.5862303972244263
I0210 13:40:41.542718 139615864289024 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2236097902059555, loss=1.669210433959961
I0210 13:41:17.275539 139615872681728 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.1928030401468277, loss=1.5646110773086548
I0210 13:41:32.035328 139785736898368 spec.py:321] Evaluating on the training split.
I0210 13:41:35.045261 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:45:39.122059 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 13:45:41.830368 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:49:32.045339 139785736898368 spec.py:349] Evaluating on the test split.
I0210 13:49:34.749334 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 13:53:20.967209 139785736898368 submission_runner.py:408] Time since start: 49679.18s, 	Step: 82243, 	{'train/accuracy': 0.6750671863555908, 'train/loss': 1.4929388761520386, 'train/bleu': 33.87395627976873, 'validation/accuracy': 0.6826077699661255, 'validation/loss': 1.4436779022216797, 'validation/bleu': 29.849369345056633, 'validation/num_examples': 3000, 'test/accuracy': 0.6966823935508728, 'test/loss': 1.3540998697280884, 'test/bleu': 29.61672950952151, 'test/num_examples': 3003, 'score': 29432.561252593994, 'total_duration': 49679.17920422554, 'accumulated_submission_time': 29432.561252593994, 'accumulated_eval_time': 20242.84255671501, 'accumulated_logging_time': 1.1201717853546143}
I0210 13:53:20.994695 139615864289024 logging_writer.py:48] [82243] accumulated_eval_time=20242.842557, accumulated_logging_time=1.120172, accumulated_submission_time=29432.561253, global_step=82243, preemption_count=0, score=29432.561253, test/accuracy=0.696682, test/bleu=29.616730, test/loss=1.354100, test/num_examples=3003, total_duration=49679.179204, train/accuracy=0.675067, train/bleu=33.873956, train/loss=1.492939, validation/accuracy=0.682608, validation/bleu=29.849369, validation/loss=1.443678, validation/num_examples=3000
I0210 13:53:41.719603 139615872681728 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.20103415846824646, loss=1.694692850112915
I0210 13:54:17.491021 139615864289024 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.21165992319583893, loss=1.6386432647705078
I0210 13:54:53.241596 139615872681728 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2088417112827301, loss=1.6615325212478638
I0210 13:55:29.030967 139615864289024 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.19555789232254028, loss=1.5591742992401123
I0210 13:56:04.812319 139615872681728 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.19686709344387054, loss=1.6212973594665527
I0210 13:56:40.623994 139615864289024 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.20215055346488953, loss=1.615281581878662
I0210 13:57:16.372042 139615872681728 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.21049873530864716, loss=1.642937183380127
I0210 13:57:52.099402 139615864289024 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.1980568915605545, loss=1.608506441116333
I0210 13:58:27.829851 139615872681728 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.2177962064743042, loss=1.537637710571289
I0210 13:59:03.600272 139615864289024 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.23595911264419556, loss=1.602929711341858
I0210 13:59:39.330193 139615872681728 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.20253466069698334, loss=1.6438454389572144
I0210 14:00:15.089908 139615864289024 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.49265462160110474, loss=1.5501675605773926
I0210 14:00:50.816449 139615872681728 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3291606605052948, loss=1.6042710542678833
I0210 14:01:26.592201 139615864289024 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.19662363827228546, loss=1.5983924865722656
I0210 14:02:02.373825 139615872681728 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.19508488476276398, loss=1.6042089462280273
I0210 14:02:38.134286 139615864289024 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3249955177307129, loss=1.6752818822860718
I0210 14:03:13.881783 139615872681728 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.19651515781879425, loss=1.5751681327819824
I0210 14:03:49.681014 139615864289024 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.2156265527009964, loss=1.684141993522644
I0210 14:04:25.426925 139615872681728 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.20339137315750122, loss=1.6000014543533325
I0210 14:05:01.206168 139615864289024 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.18938735127449036, loss=1.5594993829727173
I0210 14:05:37.000262 139615872681728 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.20362965762615204, loss=1.6289182901382446
I0210 14:06:12.806425 139615864289024 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.21329540014266968, loss=1.6352992057800293
I0210 14:06:48.555977 139615872681728 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.21380922198295593, loss=1.6151201725006104
I0210 14:07:21.185561 139785736898368 spec.py:321] Evaluating on the training split.
I0210 14:07:24.183183 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 14:11:17.490235 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 14:11:20.193668 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 14:15:02.248608 139785736898368 spec.py:349] Evaluating on the test split.
I0210 14:15:04.960924 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 14:19:01.833873 139785736898368 submission_runner.py:408] Time since start: 51220.05s, 	Step: 84593, 	{'train/accuracy': 0.6679847240447998, 'train/loss': 1.5398011207580566, 'train/bleu': 33.47505030763876, 'validation/accuracy': 0.6833269000053406, 'validation/loss': 1.4395651817321777, 'validation/bleu': 29.752018414615385, 'validation/num_examples': 3000, 'test/accuracy': 0.6992040276527405, 'test/loss': 1.3429124355316162, 'test/bleu': 29.948791249609343, 'test/num_examples': 3003, 'score': 30272.666800498962, 'total_duration': 51220.04585957527, 'accumulated_submission_time': 30272.666800498962, 'accumulated_eval_time': 20943.490812301636, 'accumulated_logging_time': 1.1579155921936035}
I0210 14:19:01.861654 139615864289024 logging_writer.py:48] [84593] accumulated_eval_time=20943.490812, accumulated_logging_time=1.157916, accumulated_submission_time=30272.666800, global_step=84593, preemption_count=0, score=30272.666800, test/accuracy=0.699204, test/bleu=29.948791, test/loss=1.342912, test/num_examples=3003, total_duration=51220.045860, train/accuracy=0.667985, train/bleu=33.475050, train/loss=1.539801, validation/accuracy=0.683327, validation/bleu=29.752018, validation/loss=1.439565, validation/num_examples=3000
I0210 14:19:04.731768 139615872681728 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.199060320854187, loss=1.52354896068573
I0210 14:19:40.403396 139615864289024 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.2173386663198471, loss=1.5766351222991943
I0210 14:20:16.088283 139615872681728 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.22654342651367188, loss=1.6685935258865356
I0210 14:20:51.761628 139615864289024 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2105320245027542, loss=1.6064622402191162
I0210 14:21:27.457256 139615872681728 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.19159254431724548, loss=1.558712363243103
I0210 14:22:03.136505 139615864289024 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.21104246377944946, loss=1.5215091705322266
I0210 14:22:38.883649 139615872681728 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2106403112411499, loss=1.6302357912063599
I0210 14:23:14.621823 139615864289024 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.19730359315872192, loss=1.6439385414123535
I0210 14:23:50.380688 139615872681728 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.20154261589050293, loss=1.585883378982544
I0210 14:24:26.175242 139615864289024 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.20443017780780792, loss=1.5843627452850342
I0210 14:25:01.929577 139615872681728 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.21318694949150085, loss=1.5884021520614624
I0210 14:25:37.692510 139615864289024 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.21673068404197693, loss=1.6150461435317993
I0210 14:26:13.455408 139615872681728 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.21672837436199188, loss=1.5808135271072388
I0210 14:26:49.203821 139615864289024 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.2074958235025406, loss=1.661939024925232
I0210 14:27:25.005020 139615872681728 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.2039516121149063, loss=1.7057971954345703
I0210 14:28:00.780639 139615864289024 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.2126438468694687, loss=1.676208734512329
I0210 14:28:36.526005 139615872681728 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.1961689442396164, loss=1.6070114374160767
I0210 14:29:12.286495 139615864289024 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.2035297453403473, loss=1.599483847618103
I0210 14:29:48.055203 139615872681728 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.19996057450771332, loss=1.5519317388534546
I0210 14:30:23.822049 139615864289024 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.20234301686286926, loss=1.6045154333114624
I0210 14:30:59.546582 139615872681728 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.23300233483314514, loss=1.6171917915344238
I0210 14:31:35.312458 139615864289024 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.20793047547340393, loss=1.5956250429153442
I0210 14:32:11.091607 139615872681728 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.21683481335639954, loss=1.5633317232131958
I0210 14:32:46.882160 139615864289024 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.22055603563785553, loss=1.598937749862671
I0210 14:33:01.987793 139785736898368 spec.py:321] Evaluating on the training split.
I0210 14:33:04.994496 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 14:37:15.397424 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 14:37:18.107809 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 14:40:02.284069 139785736898368 spec.py:349] Evaluating on the test split.
I0210 14:40:04.996590 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 14:42:31.182121 139785736898368 submission_runner.py:408] Time since start: 52629.39s, 	Step: 86944, 	{'train/accuracy': 0.66844642162323, 'train/loss': 1.5463353395462036, 'train/bleu': 33.3239685944154, 'validation/accuracy': 0.6863275170326233, 'validation/loss': 1.4329047203063965, 'validation/bleu': 30.17483410773, 'validation/num_examples': 3000, 'test/accuracy': 0.7015630006790161, 'test/loss': 1.336978793144226, 'test/bleu': 30.01117652283077, 'test/num_examples': 3003, 'score': 31112.70547223091, 'total_duration': 52629.39412069321, 'accumulated_submission_time': 31112.70547223091, 'accumulated_eval_time': 21512.685092926025, 'accumulated_logging_time': 1.1974620819091797}
I0210 14:42:31.208640 139615872681728 logging_writer.py:48] [86944] accumulated_eval_time=21512.685093, accumulated_logging_time=1.197462, accumulated_submission_time=31112.705472, global_step=86944, preemption_count=0, score=31112.705472, test/accuracy=0.701563, test/bleu=30.011177, test/loss=1.336979, test/num_examples=3003, total_duration=52629.394121, train/accuracy=0.668446, train/bleu=33.323969, train/loss=1.546335, validation/accuracy=0.686328, validation/bleu=30.174834, validation/loss=1.432905, validation/num_examples=3000
I0210 14:42:51.576600 139615864289024 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.20224440097808838, loss=1.5452324151992798
I0210 14:43:27.352859 139615872681728 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.19990848004817963, loss=1.636011004447937
I0210 14:44:03.148982 139615864289024 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.20052847266197205, loss=1.5775567293167114
I0210 14:44:38.923278 139615872681728 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.2009028196334839, loss=1.5769919157028198
I0210 14:45:14.711241 139615864289024 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.2116386443376541, loss=1.6358704566955566
I0210 14:45:50.444707 139615872681728 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.20815762877464294, loss=1.5892659425735474
I0210 14:46:26.241262 139615864289024 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.20425939559936523, loss=1.6322704553604126
I0210 14:47:01.970354 139615872681728 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.2009422779083252, loss=1.6258280277252197
I0210 14:47:37.721468 139615864289024 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.7441620826721191, loss=1.5921809673309326
I0210 14:48:13.534803 139615872681728 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.2315511554479599, loss=1.6703906059265137
I0210 14:48:49.300395 139615864289024 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.20923902094364166, loss=1.5580127239227295
I0210 14:49:25.070661 139615872681728 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.26403918862342834, loss=1.6104518175125122
I0210 14:50:00.856880 139615864289024 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21332108974456787, loss=1.5925943851470947
I0210 14:50:36.602921 139615872681728 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.21924516558647156, loss=1.5283257961273193
I0210 14:51:12.368027 139615864289024 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.2131982147693634, loss=1.6651155948638916
I0210 14:51:48.101886 139615872681728 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.23179291188716888, loss=1.5620498657226562
I0210 14:52:23.866520 139615864289024 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.21250741183757782, loss=1.601457118988037
I0210 14:52:59.603286 139615872681728 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.19947530329227448, loss=1.6173009872436523
I0210 14:53:35.425379 139615864289024 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.20470869541168213, loss=1.6503006219863892
I0210 14:54:11.334774 139615872681728 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.20899660885334015, loss=1.4742649793624878
I0210 14:54:47.107890 139615864289024 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.20220263302326202, loss=1.5501306056976318
I0210 14:55:22.879186 139615872681728 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.21274825930595398, loss=1.6311252117156982
I0210 14:55:58.651087 139615864289024 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2149139940738678, loss=1.637764573097229
I0210 14:56:31.267315 139785736898368 spec.py:321] Evaluating on the training split.
I0210 14:56:34.263735 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:00:04.981463 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 15:00:07.694286 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:03:19.021125 139785736898368 spec.py:349] Evaluating on the test split.
I0210 15:03:21.725821 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:06:33.393360 139785736898368 submission_runner.py:408] Time since start: 54071.61s, 	Step: 89293, 	{'train/accuracy': 0.6765536069869995, 'train/loss': 1.4846397638320923, 'train/bleu': 33.80841067188661, 'validation/accuracy': 0.6877037882804871, 'validation/loss': 1.4231077432632446, 'validation/bleu': 30.144694232267003, 'validation/num_examples': 3000, 'test/accuracy': 0.7022950649261475, 'test/loss': 1.3293834924697876, 'test/bleu': 30.109991566006315, 'test/num_examples': 3003, 'score': 31952.67540025711, 'total_duration': 54071.60535812378, 'accumulated_submission_time': 31952.67540025711, 'accumulated_eval_time': 22114.81109213829, 'accumulated_logging_time': 1.2344374656677246}
I0210 15:06:33.420946 139615872681728 logging_writer.py:48] [89293] accumulated_eval_time=22114.811092, accumulated_logging_time=1.234437, accumulated_submission_time=31952.675400, global_step=89293, preemption_count=0, score=31952.675400, test/accuracy=0.702295, test/bleu=30.109992, test/loss=1.329383, test/num_examples=3003, total_duration=54071.605358, train/accuracy=0.676554, train/bleu=33.808411, train/loss=1.484640, validation/accuracy=0.687704, validation/bleu=30.144694, validation/loss=1.423108, validation/num_examples=3000
I0210 15:06:36.285032 139615864289024 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.20180924236774445, loss=1.585207462310791
I0210 15:07:11.972809 139615872681728 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.19912712275981903, loss=1.4769973754882812
I0210 15:07:47.754093 139615864289024 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2007901966571808, loss=1.5610113143920898
I0210 15:08:23.524755 139615872681728 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.22932833433151245, loss=1.5928490161895752
I0210 15:08:59.268269 139615864289024 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.2122286558151245, loss=1.5348490476608276
I0210 15:09:35.090577 139615872681728 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.21537582576274872, loss=1.5786834955215454
I0210 15:10:10.910818 139615864289024 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2097679078578949, loss=1.5377105474472046
I0210 15:10:46.703722 139615872681728 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20526616275310516, loss=1.6107048988342285
I0210 15:11:22.502572 139615864289024 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.20916973054409027, loss=1.6288049221038818
I0210 15:11:58.319254 139615872681728 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.21596460044384003, loss=1.6053059101104736
I0210 15:12:34.114708 139615864289024 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.21201269328594208, loss=1.5787233114242554
I0210 15:13:09.915306 139615872681728 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.22269782423973083, loss=1.6285812854766846
I0210 15:13:45.686035 139615864289024 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.20641319453716278, loss=1.5703264474868774
I0210 15:14:21.461299 139615872681728 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.21940486133098602, loss=1.6188064813613892
I0210 15:14:57.225001 139615864289024 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20747433602809906, loss=1.5034091472625732
I0210 15:15:33.050605 139615872681728 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.23975002765655518, loss=1.5768415927886963
I0210 15:16:08.879588 139615864289024 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.22198443114757538, loss=1.5850011110305786
I0210 15:16:44.665874 139615872681728 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.20124755799770355, loss=1.6260355710983276
I0210 15:17:20.421948 139615864289024 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.21319349110126495, loss=1.5685641765594482
I0210 15:17:56.161002 139615872681728 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.22339613735675812, loss=1.568236231803894
I0210 15:18:31.901554 139615864289024 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.21138806641101837, loss=1.5935264825820923
I0210 15:19:07.617984 139615872681728 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2993173897266388, loss=1.5242958068847656
I0210 15:19:43.363165 139615864289024 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.2111905813217163, loss=1.5754801034927368
I0210 15:20:19.123414 139615872681728 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.1906159222126007, loss=1.5024815797805786
I0210 15:20:33.518031 139785736898368 spec.py:321] Evaluating on the training split.
I0210 15:20:36.523183 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:23:51.072951 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 15:23:53.768802 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:26:35.360320 139785736898368 spec.py:349] Evaluating on the test split.
I0210 15:26:38.069920 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:29:15.826027 139785736898368 submission_runner.py:408] Time since start: 55434.04s, 	Step: 91642, 	{'train/accuracy': 0.6729382872581482, 'train/loss': 1.5041873455047607, 'train/bleu': 33.947059749229915, 'validation/accuracy': 0.6867242455482483, 'validation/loss': 1.4200741052627563, 'validation/bleu': 30.009697354141604, 'validation/num_examples': 3000, 'test/accuracy': 0.7022950649261475, 'test/loss': 1.3214539289474487, 'test/bleu': 30.012051023290432, 'test/num_examples': 3003, 'score': 32792.68555688858, 'total_duration': 55434.03802442551, 'accumulated_submission_time': 32792.68555688858, 'accumulated_eval_time': 22637.1190366745, 'accumulated_logging_time': 1.2718265056610107}
I0210 15:29:15.853835 139615864289024 logging_writer.py:48] [91642] accumulated_eval_time=22637.119037, accumulated_logging_time=1.271827, accumulated_submission_time=32792.685557, global_step=91642, preemption_count=0, score=32792.685557, test/accuracy=0.702295, test/bleu=30.012051, test/loss=1.321454, test/num_examples=3003, total_duration=55434.038024, train/accuracy=0.672938, train/bleu=33.947060, train/loss=1.504187, validation/accuracy=0.686724, validation/bleu=30.009697, validation/loss=1.420074, validation/num_examples=3000
I0210 15:29:36.968828 139615872681728 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.20860017836093903, loss=1.5973008871078491
I0210 15:30:12.741436 139615864289024 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.20744848251342773, loss=1.5296094417572021
I0210 15:30:48.494390 139615872681728 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2048482447862625, loss=1.5339279174804688
I0210 15:31:24.279770 139615864289024 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2307567000389099, loss=1.6271685361862183
I0210 15:32:00.030973 139615872681728 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.2107948660850525, loss=1.6271697282791138
I0210 15:32:35.785225 139615864289024 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.20951226353645325, loss=1.5656362771987915
I0210 15:33:11.534692 139615872681728 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21298174560070038, loss=1.538688063621521
I0210 15:33:47.262824 139615864289024 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.1970428079366684, loss=1.5178451538085938
I0210 15:34:23.044444 139615872681728 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.2868720591068268, loss=1.625794768333435
I0210 15:34:58.838829 139615864289024 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.22522807121276855, loss=1.6596883535385132
I0210 15:35:34.598915 139615872681728 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.2142479121685028, loss=1.6086821556091309
I0210 15:36:10.397009 139615864289024 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.22569800913333893, loss=1.5612154006958008
I0210 15:36:46.193348 139615872681728 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.21797767281532288, loss=1.5199648141860962
I0210 15:37:21.917151 139615864289024 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.2000998556613922, loss=1.5951939821243286
I0210 15:37:57.682281 139615872681728 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21048268675804138, loss=1.5749913454055786
I0210 15:38:33.520320 139615864289024 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.21832634508609772, loss=1.6295932531356812
I0210 15:39:09.319629 139615872681728 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.1995142698287964, loss=1.5590589046478271
I0210 15:39:45.120848 139615864289024 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21566800773143768, loss=1.556527853012085
I0210 15:40:20.919545 139615872681728 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.20977306365966797, loss=1.541643738746643
I0210 15:40:56.722806 139615864289024 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.21718597412109375, loss=1.5996971130371094
I0210 15:41:32.523608 139615872681728 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.19842901825904846, loss=1.5662147998809814
I0210 15:42:08.283791 139615864289024 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.20469427108764648, loss=1.473907232284546
I0210 15:42:44.006393 139615872681728 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.20155087113380432, loss=1.5632425546646118
I0210 15:43:15.902616 139785736898368 spec.py:321] Evaluating on the training split.
I0210 15:43:18.901941 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:46:58.829091 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 15:47:01.555842 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:49:15.314857 139785736898368 spec.py:349] Evaluating on the test split.
I0210 15:49:18.049017 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 15:52:05.753547 139785736898368 submission_runner.py:408] Time since start: 56803.97s, 	Step: 93991, 	{'train/accuracy': 0.6925990581512451, 'train/loss': 1.3899562358856201, 'train/bleu': 34.75326842015933, 'validation/accuracy': 0.6878277659416199, 'validation/loss': 1.4126427173614502, 'validation/bleu': 30.225550510148068, 'validation/num_examples': 3000, 'test/accuracy': 0.7032014727592468, 'test/loss': 1.3106350898742676, 'test/bleu': 29.828300853864768, 'test/num_examples': 3003, 'score': 33632.6453063488, 'total_duration': 56803.9655148983, 'accumulated_submission_time': 33632.6453063488, 'accumulated_eval_time': 23166.96990251541, 'accumulated_logging_time': 1.3109593391418457}
I0210 15:52:05.783632 139615864289024 logging_writer.py:48] [93991] accumulated_eval_time=23166.969903, accumulated_logging_time=1.310959, accumulated_submission_time=33632.645306, global_step=93991, preemption_count=0, score=33632.645306, test/accuracy=0.703201, test/bleu=29.828301, test/loss=1.310635, test/num_examples=3003, total_duration=56803.965515, train/accuracy=0.692599, train/bleu=34.753268, train/loss=1.389956, validation/accuracy=0.687828, validation/bleu=30.225551, validation/loss=1.412643, validation/num_examples=3000
I0210 15:52:09.363917 139615872681728 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.20529629290103912, loss=1.555168628692627
I0210 15:52:45.112102 139615864289024 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2058507353067398, loss=1.5505534410476685
I0210 15:53:20.845013 139615872681728 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2196042686700821, loss=1.6298495531082153
I0210 15:53:56.596660 139615864289024 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.21164296567440033, loss=1.5937604904174805
I0210 15:54:32.370429 139615872681728 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.23811906576156616, loss=1.634069561958313
I0210 15:55:08.105232 139615864289024 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21042825281620026, loss=1.5670865774154663
I0210 15:55:43.839766 139615872681728 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.22113525867462158, loss=1.561940312385559
I0210 15:56:19.606302 139615864289024 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2159181833267212, loss=1.5437296628952026
I0210 15:56:55.372896 139615872681728 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.816951870918274, loss=1.488834023475647
I0210 15:57:31.123188 139615864289024 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.20268401503562927, loss=1.5380072593688965
I0210 15:58:06.871538 139615872681728 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.2125692069530487, loss=1.6164411306381226
I0210 15:58:42.639215 139615864289024 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2135675698518753, loss=1.556920051574707
I0210 15:59:18.453347 139615872681728 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.22058860957622528, loss=1.515748381614685
I0210 15:59:54.230337 139615864289024 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.2110302448272705, loss=1.5483407974243164
I0210 16:00:30.012066 139615872681728 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2048749029636383, loss=1.482672929763794
I0210 16:01:05.815919 139615864289024 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.22278979420661926, loss=1.5541527271270752
I0210 16:01:41.637371 139615872681728 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.21281790733337402, loss=1.5853747129440308
I0210 16:02:17.440937 139615864289024 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.21424056589603424, loss=1.6522308588027954
I0210 16:02:53.234163 139615872681728 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21688894927501678, loss=1.6029000282287598
I0210 16:03:28.941627 139615864289024 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.21371085941791534, loss=1.6403838396072388
I0210 16:04:04.698256 139615872681728 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.20855897665023804, loss=1.5117143392562866
I0210 16:04:40.487987 139615864289024 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.21696898341178894, loss=1.6248542070388794
I0210 16:05:16.276443 139615872681728 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2183249294757843, loss=1.5425387620925903
I0210 16:05:52.026082 139615864289024 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.20171600580215454, loss=1.5775634050369263
I0210 16:06:06.048491 139785736898368 spec.py:321] Evaluating on the training split.
I0210 16:06:09.049058 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:10:01.134128 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 16:10:03.843250 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:13:42.776284 139785736898368 spec.py:349] Evaluating on the test split.
I0210 16:13:45.487672 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:17:42.151258 139785736898368 submission_runner.py:408] Time since start: 58340.36s, 	Step: 96341, 	{'train/accuracy': 0.6803255081176758, 'train/loss': 1.4658100605010986, 'train/bleu': 33.819762322880095, 'validation/accuracy': 0.6885097622871399, 'validation/loss': 1.407971978187561, 'validation/bleu': 29.990235745955843, 'validation/num_examples': 3000, 'test/accuracy': 0.7055023312568665, 'test/loss': 1.3078140020370483, 'test/bleu': 30.228751807442475, 'test/num_examples': 3003, 'score': 34472.82131195068, 'total_duration': 58340.36324286461, 'accumulated_submission_time': 34472.82131195068, 'accumulated_eval_time': 23863.072603464127, 'accumulated_logging_time': 1.3514149188995361}
I0210 16:17:42.181631 139615872681728 logging_writer.py:48] [96341] accumulated_eval_time=23863.072603, accumulated_logging_time=1.351415, accumulated_submission_time=34472.821312, global_step=96341, preemption_count=0, score=34472.821312, test/accuracy=0.705502, test/bleu=30.228752, test/loss=1.307814, test/num_examples=3003, total_duration=58340.363243, train/accuracy=0.680326, train/bleu=33.819762, train/loss=1.465810, validation/accuracy=0.688510, validation/bleu=29.990236, validation/loss=1.407972, validation/num_examples=3000
I0210 16:18:03.546490 139615864289024 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.22403696179389954, loss=1.633255958557129
I0210 16:18:39.255111 139615872681728 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.20604537427425385, loss=1.5445499420166016
I0210 16:19:14.985624 139615864289024 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.20871715247631073, loss=1.5263714790344238
I0210 16:19:50.763384 139615872681728 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.23185668885707855, loss=1.5356383323669434
I0210 16:20:26.530880 139615864289024 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.21830029785633087, loss=1.549763798713684
I0210 16:21:02.266856 139615872681728 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.20262978971004486, loss=1.5468710660934448
I0210 16:21:38.047097 139615864289024 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.21420182287693024, loss=1.5835342407226562
I0210 16:22:13.779138 139615872681728 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.22431343793869019, loss=1.461554765701294
I0210 16:22:49.553274 139615864289024 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22353003919124603, loss=1.582535743713379
I0210 16:23:25.339760 139615872681728 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.21216413378715515, loss=1.5957605838775635
I0210 16:24:01.109107 139615864289024 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.2178918421268463, loss=1.5819478034973145
I0210 16:24:36.900077 139615872681728 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.22007004916667938, loss=1.520245909690857
I0210 16:25:12.709653 139615864289024 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.20487552881240845, loss=1.5083506107330322
I0210 16:25:48.573000 139615872681728 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21712151169776917, loss=1.5441524982452393
I0210 16:26:24.395794 139615864289024 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21247370541095734, loss=1.4896947145462036
I0210 16:27:00.182259 139615872681728 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21530987322330475, loss=1.5303205251693726
I0210 16:27:35.942957 139615864289024 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.2124825417995453, loss=1.4879355430603027
I0210 16:28:11.712409 139615872681728 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.21743836998939514, loss=1.4547990560531616
I0210 16:28:47.460012 139615864289024 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.20766429603099823, loss=1.514278769493103
I0210 16:29:23.196026 139615872681728 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.21816767752170563, loss=1.6009365320205688
I0210 16:29:58.958401 139615864289024 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21875005960464478, loss=1.4693396091461182
I0210 16:30:34.784151 139615872681728 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.20703233778476715, loss=1.532957911491394
I0210 16:31:10.589941 139615864289024 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.20610493421554565, loss=1.5319393873214722
I0210 16:31:42.163315 139785736898368 spec.py:321] Evaluating on the training split.
I0210 16:31:45.159312 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:35:16.628625 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 16:35:19.363368 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:37:54.650405 139785736898368 spec.py:349] Evaluating on the test split.
I0210 16:37:57.385626 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:40:18.170078 139785736898368 submission_runner.py:408] Time since start: 59696.38s, 	Step: 98690, 	{'train/accuracy': 0.678549587726593, 'train/loss': 1.4740723371505737, 'train/bleu': 34.745071742947765, 'validation/accuracy': 0.689811646938324, 'validation/loss': 1.4059818983078003, 'validation/bleu': 30.306885565787923, 'validation/num_examples': 3000, 'test/accuracy': 0.7070013284683228, 'test/loss': 1.3015625476837158, 'test/bleu': 30.51100426873089, 'test/num_examples': 3003, 'score': 35312.71530985832, 'total_duration': 59696.382061719894, 'accumulated_submission_time': 35312.71530985832, 'accumulated_eval_time': 24379.07930803299, 'accumulated_logging_time': 1.391371726989746}
I0210 16:40:18.198391 139615872681728 logging_writer.py:48] [98690] accumulated_eval_time=24379.079308, accumulated_logging_time=1.391372, accumulated_submission_time=35312.715310, global_step=98690, preemption_count=0, score=35312.715310, test/accuracy=0.707001, test/bleu=30.511004, test/loss=1.301563, test/num_examples=3003, total_duration=59696.382062, train/accuracy=0.678550, train/bleu=34.745072, train/loss=1.474072, validation/accuracy=0.689812, validation/bleu=30.306886, validation/loss=1.405982, validation/num_examples=3000
I0210 16:40:22.136232 139615864289024 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.22053156793117523, loss=1.559509038925171
I0210 16:40:57.925370 139615872681728 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.21580322086811066, loss=1.5352907180786133
I0210 16:41:33.670352 139615864289024 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.2225601226091385, loss=1.5422390699386597
I0210 16:42:09.416906 139615872681728 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.20768162608146667, loss=1.5936776399612427
I0210 16:42:45.182226 139615864289024 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21288979053497314, loss=1.4854215383529663
I0210 16:43:20.908027 139615872681728 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.20898105204105377, loss=1.5110974311828613
I0210 16:43:56.639082 139615864289024 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21183215081691742, loss=1.5748580694198608
I0210 16:44:32.417996 139615872681728 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.22388705611228943, loss=1.5301817655563354
I0210 16:45:08.170762 139615864289024 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.2131475955247879, loss=1.5537478923797607
I0210 16:45:43.922550 139615872681728 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.22066199779510498, loss=1.4634836912155151
I0210 16:46:19.705815 139615864289024 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22022224962711334, loss=1.5593037605285645
I0210 16:46:55.441173 139615872681728 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.21417763829231262, loss=1.5188093185424805
I0210 16:47:31.192229 139615864289024 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.21620392799377441, loss=1.5324058532714844
I0210 16:48:06.974952 139615872681728 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.23477958142757416, loss=1.5269092321395874
I0210 16:48:42.745293 139615864289024 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2236769199371338, loss=1.601593017578125
I0210 16:49:18.522640 139615872681728 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.21160218119621277, loss=1.4861549139022827
I0210 16:49:54.353264 139615864289024 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.20815341174602509, loss=1.532085657119751
I0210 16:50:30.135866 139615872681728 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.2271888107061386, loss=1.568514347076416
I0210 16:51:05.909720 139615864289024 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.2273615002632141, loss=1.5436427593231201
I0210 16:51:41.670421 139615872681728 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.20941346883773804, loss=1.5182446241378784
I0210 16:52:17.417792 139615864289024 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.20526431500911713, loss=1.415473461151123
I0210 16:52:53.133665 139615872681728 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.21747156977653503, loss=1.5574766397476196
I0210 16:53:28.909016 139615864289024 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.30679556727409363, loss=1.5766398906707764
I0210 16:54:04.708343 139615872681728 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.21823912858963013, loss=1.4545106887817383
I0210 16:54:18.383282 139785736898368 spec.py:321] Evaluating on the training split.
I0210 16:54:21.387845 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 16:58:24.215680 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 16:58:26.919285 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:01:07.991487 139785736898368 spec.py:349] Evaluating on the test split.
I0210 17:01:10.708847 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:03:51.660136 139785736898368 submission_runner.py:408] Time since start: 61109.87s, 	Step: 101040, 	{'train/accuracy': 0.6873783469200134, 'train/loss': 1.4144078493118286, 'train/bleu': 34.822233159168285, 'validation/accuracy': 0.689452052116394, 'validation/loss': 1.3942186832427979, 'validation/bleu': 30.43498417126523, 'validation/num_examples': 3000, 'test/accuracy': 0.7060601115226746, 'test/loss': 1.2959932088851929, 'test/bleu': 30.27591467570254, 'test/num_examples': 3003, 'score': 36152.8135163784, 'total_duration': 61109.87213683128, 'accumulated_submission_time': 36152.8135163784, 'accumulated_eval_time': 24952.356118440628, 'accumulated_logging_time': 1.4311096668243408}
I0210 17:03:51.689485 139615864289024 logging_writer.py:48] [101040] accumulated_eval_time=24952.356118, accumulated_logging_time=1.431110, accumulated_submission_time=36152.813516, global_step=101040, preemption_count=0, score=36152.813516, test/accuracy=0.706060, test/bleu=30.275915, test/loss=1.295993, test/num_examples=3003, total_duration=61109.872137, train/accuracy=0.687378, train/bleu=34.822233, train/loss=1.414408, validation/accuracy=0.689452, validation/bleu=30.434984, validation/loss=1.394219, validation/num_examples=3000
I0210 17:04:13.507332 139615872681728 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.22061029076576233, loss=1.525787353515625
I0210 17:04:49.267722 139615864289024 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.2092195600271225, loss=1.441972255706787
I0210 17:05:25.029151 139615872681728 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.21118475496768951, loss=1.5098741054534912
I0210 17:06:00.804769 139615864289024 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.22338521480560303, loss=1.50633704662323
I0210 17:06:36.590060 139615872681728 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.21690180897712708, loss=1.5729244947433472
I0210 17:07:12.328422 139615864289024 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22611229121685028, loss=1.557987093925476
I0210 17:07:48.082073 139615872681728 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.23079648613929749, loss=1.5030697584152222
I0210 17:08:23.882009 139615864289024 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.20446863770484924, loss=1.4922593832015991
I0210 17:08:59.613235 139615872681728 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.21127401292324066, loss=1.5592617988586426
I0210 17:09:35.344554 139615864289024 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.2125820368528366, loss=1.515419602394104
I0210 17:10:11.128890 139615872681728 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2115352302789688, loss=1.4821245670318604
I0210 17:10:46.873880 139615864289024 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.23208723962306976, loss=1.587386131286621
I0210 17:11:22.610845 139615872681728 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22053460776805878, loss=1.561262845993042
I0210 17:11:58.358756 139615864289024 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21435946226119995, loss=1.4817427396774292
I0210 17:12:34.158171 139615872681728 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21091634035110474, loss=1.4905365705490112
I0210 17:13:09.922180 139615864289024 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.2246142029762268, loss=1.4894132614135742
I0210 17:13:45.671726 139615872681728 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23246552050113678, loss=1.4955602884292603
I0210 17:14:21.436020 139615864289024 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.2180899828672409, loss=1.5007232427597046
I0210 17:14:57.216530 139615872681728 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.21805566549301147, loss=1.5914369821548462
I0210 17:15:32.964039 139615864289024 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.21984080970287323, loss=1.5546249151229858
I0210 17:16:08.730756 139615872681728 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.2232230007648468, loss=1.560791015625
I0210 17:16:44.480150 139615864289024 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.2067110687494278, loss=1.4658461809158325
I0210 17:17:20.209457 139615872681728 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.2318972498178482, loss=1.5616270303726196
I0210 17:17:51.764423 139785736898368 spec.py:321] Evaluating on the training split.
I0210 17:17:54.757599 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:21:44.925179 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 17:21:47.629218 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:24:32.183051 139785736898368 spec.py:349] Evaluating on the test split.
I0210 17:24:34.906226 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:27:45.969393 139785736898368 submission_runner.py:408] Time since start: 62544.18s, 	Step: 103390, 	{'train/accuracy': 0.6787706017494202, 'train/loss': 1.472381591796875, 'train/bleu': 34.936464710336374, 'validation/accuracy': 0.6904067993164062, 'validation/loss': 1.394535779953003, 'validation/bleu': 30.4547279180483, 'validation/num_examples': 3000, 'test/accuracy': 0.7068851590156555, 'test/loss': 1.2904762029647827, 'test/bleu': 30.54459233862504, 'test/num_examples': 3003, 'score': 36992.80270028114, 'total_duration': 62544.18134522438, 'accumulated_submission_time': 36992.80270028114, 'accumulated_eval_time': 25546.56099653244, 'accumulated_logging_time': 1.4709479808807373}
I0210 17:27:46.004589 139615864289024 logging_writer.py:48] [103390] accumulated_eval_time=25546.560997, accumulated_logging_time=1.470948, accumulated_submission_time=36992.802700, global_step=103390, preemption_count=0, score=36992.802700, test/accuracy=0.706885, test/bleu=30.544592, test/loss=1.290476, test/num_examples=3003, total_duration=62544.181345, train/accuracy=0.678771, train/bleu=34.936465, train/loss=1.472382, validation/accuracy=0.690407, validation/bleu=30.454728, validation/loss=1.394536, validation/num_examples=3000
I0210 17:27:49.959008 139615872681728 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.2508103549480438, loss=1.5998737812042236
I0210 17:28:25.719654 139615864289024 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.21629701554775238, loss=1.435731053352356
I0210 17:29:01.476097 139615872681728 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21229533851146698, loss=1.5066723823547363
I0210 17:29:37.329957 139615864289024 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.21974171698093414, loss=1.489426612854004
I0210 17:30:13.098777 139615872681728 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22858113050460815, loss=1.5739375352859497
I0210 17:30:48.821747 139615864289024 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.21793311834335327, loss=1.4685839414596558
I0210 17:31:24.579837 139615872681728 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.20866093039512634, loss=1.5222880840301514
I0210 17:32:00.328734 139615864289024 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.20869509875774384, loss=1.516508936882019
I0210 17:32:36.147766 139615872681728 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.22634978592395782, loss=1.6165971755981445
I0210 17:33:11.938228 139615864289024 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.21810974180698395, loss=1.4429467916488647
I0210 17:33:47.745752 139615872681728 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.2297920137643814, loss=1.5364365577697754
I0210 17:34:23.507495 139615864289024 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.2265911102294922, loss=1.5209345817565918
I0210 17:34:59.272244 139615872681728 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22341400384902954, loss=1.4782360792160034
I0210 17:35:35.069370 139615864289024 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.22155527770519257, loss=1.5046368837356567
I0210 17:36:10.837861 139615872681728 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.2324066311120987, loss=1.4402916431427002
I0210 17:36:46.582127 139615864289024 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.23024126887321472, loss=1.5096718072891235
I0210 17:37:22.330219 139615872681728 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22404341399669647, loss=1.5136960744857788
I0210 17:37:58.081173 139615864289024 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.21504683792591095, loss=1.5705264806747437
I0210 17:38:33.820331 139615872681728 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.220692977309227, loss=1.4826064109802246
I0210 17:39:09.587662 139615864289024 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.23263022303581238, loss=1.5601609945297241
I0210 17:39:45.316417 139615872681728 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.21298828721046448, loss=1.467415452003479
I0210 17:40:21.065982 139615864289024 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.22853447496891022, loss=1.5431270599365234
I0210 17:40:56.791251 139615872681728 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.22215372323989868, loss=1.524228572845459
I0210 17:41:32.532580 139615864289024 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.2335362434387207, loss=1.5216400623321533
I0210 17:41:46.189781 139785736898368 spec.py:321] Evaluating on the training split.
I0210 17:41:49.179085 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:45:37.995435 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 17:45:40.691475 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:48:04.426132 139785736898368 spec.py:349] Evaluating on the test split.
I0210 17:48:07.147515 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 17:50:33.852813 139785736898368 submission_runner.py:408] Time since start: 63912.06s, 	Step: 105740, 	{'train/accuracy': 0.6793479919433594, 'train/loss': 1.474918246269226, 'train/bleu': 34.524389490258514, 'validation/accuracy': 0.6916218996047974, 'validation/loss': 1.3871182203292847, 'validation/bleu': 30.763161599157005, 'validation/num_examples': 3000, 'test/accuracy': 0.7088722586631775, 'test/loss': 1.2860301733016968, 'test/bleu': 30.587540047698738, 'test/num_examples': 3003, 'score': 37832.90144467354, 'total_duration': 63912.06481075287, 'accumulated_submission_time': 37832.90144467354, 'accumulated_eval_time': 26074.223981142044, 'accumulated_logging_time': 1.5178804397583008}
I0210 17:50:33.882265 139615872681728 logging_writer.py:48] [105740] accumulated_eval_time=26074.223981, accumulated_logging_time=1.517880, accumulated_submission_time=37832.901445, global_step=105740, preemption_count=0, score=37832.901445, test/accuracy=0.708872, test/bleu=30.587540, test/loss=1.286030, test/num_examples=3003, total_duration=63912.064811, train/accuracy=0.679348, train/bleu=34.524389, train/loss=1.474918, validation/accuracy=0.691622, validation/bleu=30.763162, validation/loss=1.387118, validation/num_examples=3000
I0210 17:50:55.709942 139615864289024 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.24621479213237762, loss=1.5585161447525024
I0210 17:51:31.493088 139615872681728 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.21576572954654694, loss=1.471192479133606
I0210 17:52:07.240151 139615864289024 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.23130105435848236, loss=1.5310194492340088
I0210 17:52:43.013884 139615872681728 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.2198474407196045, loss=1.484138011932373
I0210 17:53:18.759416 139615864289024 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.21514694392681122, loss=1.448671579360962
I0210 17:53:54.498070 139615872681728 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.2276989221572876, loss=1.436874508857727
I0210 17:54:30.234241 139615864289024 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.22187384963035583, loss=1.4357730150222778
I0210 17:55:06.025068 139615872681728 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.22053231298923492, loss=1.408535361289978
I0210 17:55:41.769370 139615864289024 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22763188183307648, loss=1.5361448526382446
I0210 17:56:17.499792 139615872681728 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.21517668664455414, loss=1.4946852922439575
I0210 17:56:53.302911 139615864289024 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.23221392929553986, loss=1.5168875455856323
I0210 17:57:29.055965 139615872681728 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.22498483955860138, loss=1.5391170978546143
I0210 17:58:04.766750 139615864289024 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.23031042516231537, loss=1.482445240020752
I0210 17:58:40.492022 139615872681728 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.21233907341957092, loss=1.4960120916366577
I0210 17:59:16.321640 139615864289024 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.22876794636249542, loss=1.5697718858718872
I0210 17:59:52.077309 139615872681728 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.21129940450191498, loss=1.4296317100524902
I0210 18:00:27.781522 139615864289024 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.23656202852725983, loss=1.5054230690002441
I0210 18:01:03.526423 139615872681728 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.24506261944770813, loss=1.527722716331482
I0210 18:01:39.288579 139615864289024 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.21576538681983948, loss=1.4933549165725708
I0210 18:02:15.061732 139615872681728 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.22134405374526978, loss=1.4450427293777466
I0210 18:02:50.824433 139615864289024 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.23868466913700104, loss=1.5525723695755005
I0210 18:03:26.603088 139615872681728 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.2362835705280304, loss=1.5655553340911865
I0210 18:04:02.387855 139615864289024 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.22455206513404846, loss=1.5213857889175415
I0210 18:04:33.966975 139785736898368 spec.py:321] Evaluating on the training split.
I0210 18:04:36.974785 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:08:11.843382 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 18:08:14.547033 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:11:12.918278 139785736898368 spec.py:349] Evaluating on the test split.
I0210 18:11:15.628496 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:14:03.765053 139785736898368 submission_runner.py:408] Time since start: 65321.98s, 	Step: 108090, 	{'train/accuracy': 0.6895817518234253, 'train/loss': 1.4022433757781982, 'train/bleu': 34.956725807169725, 'validation/accuracy': 0.6924774646759033, 'validation/loss': 1.3789490461349487, 'validation/bleu': 30.507768815081356, 'validation/num_examples': 3000, 'test/accuracy': 0.7091395258903503, 'test/loss': 1.276688575744629, 'test/bleu': 30.70939975137136, 'test/num_examples': 3003, 'score': 38672.90054440498, 'total_duration': 65321.97705411911, 'accumulated_submission_time': 38672.90054440498, 'accumulated_eval_time': 26644.022025108337, 'accumulated_logging_time': 1.5571041107177734}
I0210 18:14:03.795254 139615872681728 logging_writer.py:48] [108090] accumulated_eval_time=26644.022025, accumulated_logging_time=1.557104, accumulated_submission_time=38672.900544, global_step=108090, preemption_count=0, score=38672.900544, test/accuracy=0.709140, test/bleu=30.709400, test/loss=1.276689, test/num_examples=3003, total_duration=65321.977054, train/accuracy=0.689582, train/bleu=34.956726, train/loss=1.402243, validation/accuracy=0.692477, validation/bleu=30.507769, validation/loss=1.378949, validation/num_examples=3000
I0210 18:14:07.742785 139615864289024 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22243621945381165, loss=1.5351899862289429
I0210 18:14:43.479075 139615872681728 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.2265177071094513, loss=1.4362467527389526
I0210 18:15:19.244557 139615864289024 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.21774321794509888, loss=1.4532941579818726
I0210 18:15:55.040154 139615872681728 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.22818946838378906, loss=1.4965834617614746
I0210 18:16:30.809525 139615864289024 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.23508469760417938, loss=1.521141529083252
I0210 18:17:06.577998 139615872681728 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.23820170760154724, loss=1.523358702659607
I0210 18:17:42.333862 139615864289024 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.22661153972148895, loss=1.5823173522949219
I0210 18:18:18.087327 139615872681728 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.23468023538589478, loss=1.4929008483886719
I0210 18:18:53.836754 139615864289024 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.21810148656368256, loss=1.5382999181747437
I0210 18:19:29.611095 139615872681728 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.21786977350711823, loss=1.4702568054199219
I0210 18:20:05.374853 139615864289024 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.24612607061862946, loss=1.4344370365142822
I0210 18:20:41.114103 139615872681728 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.22339247167110443, loss=1.499553918838501
I0210 18:21:16.909717 139615864289024 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.23203139007091522, loss=1.492900013923645
I0210 18:21:52.680109 139615872681728 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.22509269416332245, loss=1.5450760126113892
I0210 18:22:28.455696 139615864289024 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22264236211776733, loss=1.5387814044952393
I0210 18:23:04.191795 139615872681728 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.24205105006694794, loss=1.4584872722625732
I0210 18:23:39.958708 139615864289024 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.21900872886180878, loss=1.4164468050003052
I0210 18:24:15.711321 139615872681728 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.2242709994316101, loss=1.5172828435897827
I0210 18:24:51.448404 139615864289024 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.22573426365852356, loss=1.5708494186401367
I0210 18:25:27.222967 139615872681728 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.22636723518371582, loss=1.5461392402648926
I0210 18:26:02.960476 139615864289024 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.33649617433547974, loss=1.466640591621399
I0210 18:26:38.753803 139615872681728 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.2273896038532257, loss=1.5170809030532837
I0210 18:27:14.542252 139615864289024 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.22516581416130066, loss=1.4725666046142578
I0210 18:27:50.384634 139615872681728 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.23168735206127167, loss=1.5021352767944336
I0210 18:28:04.055653 139785736898368 spec.py:321] Evaluating on the training split.
I0210 18:28:07.049570 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:31:52.032079 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 18:31:54.740697 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:34:53.075975 139785736898368 spec.py:349] Evaluating on the test split.
I0210 18:34:55.783179 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:37:54.143744 139785736898368 submission_runner.py:408] Time since start: 66752.36s, 	Step: 110440, 	{'train/accuracy': 0.6895023584365845, 'train/loss': 1.4150652885437012, 'train/bleu': 35.16845176199788, 'validation/accuracy': 0.6943125128746033, 'validation/loss': 1.378193974494934, 'validation/bleu': 30.72178388616816, 'validation/num_examples': 3000, 'test/accuracy': 0.7114055156707764, 'test/loss': 1.2721518278121948, 'test/bleu': 30.801454308740052, 'test/num_examples': 3003, 'score': 39513.07421565056, 'total_duration': 66752.35574197769, 'accumulated_submission_time': 39513.07421565056, 'accumulated_eval_time': 27234.110072135925, 'accumulated_logging_time': 1.5973656177520752}
I0210 18:37:54.173922 139615864289024 logging_writer.py:48] [110440] accumulated_eval_time=27234.110072, accumulated_logging_time=1.597366, accumulated_submission_time=39513.074216, global_step=110440, preemption_count=0, score=39513.074216, test/accuracy=0.711406, test/bleu=30.801454, test/loss=1.272152, test/num_examples=3003, total_duration=66752.355742, train/accuracy=0.689502, train/bleu=35.168452, train/loss=1.415065, validation/accuracy=0.694313, validation/bleu=30.721784, validation/loss=1.378194, validation/num_examples=3000
I0210 18:38:15.991033 139615872681728 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.2352011799812317, loss=1.5323485136032104
I0210 18:38:51.714569 139615864289024 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.23278500139713287, loss=1.5045990943908691
I0210 18:39:27.450818 139615872681728 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.22653722763061523, loss=1.5208138227462769
I0210 18:40:03.197611 139615864289024 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.22394424676895142, loss=1.4886605739593506
I0210 18:40:38.937077 139615872681728 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.22203989326953888, loss=1.487798810005188
I0210 18:41:14.671353 139615864289024 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.22964610159397125, loss=1.5069177150726318
I0210 18:41:50.433700 139615872681728 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.23586750030517578, loss=1.4671838283538818
I0210 18:42:26.146930 139615864289024 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.23076139390468597, loss=1.4660974740982056
I0210 18:43:01.893479 139615872681728 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.22867721319198608, loss=1.4249846935272217
I0210 18:43:37.664818 139615864289024 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.2276092767715454, loss=1.519626498222351
I0210 18:44:13.474348 139615872681728 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.23069670796394348, loss=1.5096068382263184
I0210 18:44:49.192950 139615864289024 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.21773332357406616, loss=1.514622688293457
I0210 18:45:24.936446 139615872681728 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.2334931194782257, loss=1.3854081630706787
I0210 18:46:00.658376 139615864289024 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.24147219955921173, loss=1.5509448051452637
I0210 18:46:36.393962 139615872681728 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.25012296438217163, loss=1.5546070337295532
I0210 18:47:12.150632 139615864289024 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.23224657773971558, loss=1.5068678855895996
I0210 18:47:47.886662 139615872681728 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.2245980054140091, loss=1.4883403778076172
I0210 18:48:23.636135 139615864289024 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.2535666227340698, loss=1.5544812679290771
I0210 18:48:59.380254 139615872681728 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.23724053800106049, loss=1.4990164041519165
I0210 18:49:35.132248 139615864289024 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.2280125916004181, loss=1.4064383506774902
I0210 18:50:10.878653 139615872681728 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.22563688457012177, loss=1.5362061262130737
I0210 18:50:46.635211 139615864289024 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.22393929958343506, loss=1.4079941511154175
I0210 18:51:22.414772 139615872681728 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.23930664360523224, loss=1.4819260835647583
I0210 18:51:54.308150 139785736898368 spec.py:321] Evaluating on the training split.
I0210 18:51:57.309247 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:55:50.682513 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 18:55:53.394564 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 18:58:31.331406 139785736898368 spec.py:349] Evaluating on the test split.
I0210 18:58:34.036623 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:01:19.488102 139785736898368 submission_runner.py:408] Time since start: 68157.70s, 	Step: 112791, 	{'train/accuracy': 0.6971345543861389, 'train/loss': 1.3650158643722534, 'train/bleu': 35.54626248357229, 'validation/accuracy': 0.6938785314559937, 'validation/loss': 1.3746583461761475, 'validation/bleu': 30.809153601006447, 'validation/num_examples': 3000, 'test/accuracy': 0.7100808024406433, 'test/loss': 1.2701268196105957, 'test/bleu': 30.55024025701441, 'test/num_examples': 3003, 'score': 40353.12133765221, 'total_duration': 68157.70009493828, 'accumulated_submission_time': 40353.12133765221, 'accumulated_eval_time': 27799.289972782135, 'accumulated_logging_time': 1.6389942169189453}
I0210 19:01:19.518708 139615864289024 logging_writer.py:48] [112791] accumulated_eval_time=27799.289973, accumulated_logging_time=1.638994, accumulated_submission_time=40353.121338, global_step=112791, preemption_count=0, score=40353.121338, test/accuracy=0.710081, test/bleu=30.550240, test/loss=1.270127, test/num_examples=3003, total_duration=68157.700095, train/accuracy=0.697135, train/bleu=35.546262, train/loss=1.365016, validation/accuracy=0.693879, validation/bleu=30.809154, validation/loss=1.374658, validation/num_examples=3000
I0210 19:01:23.096972 139615872681728 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.22690962255001068, loss=1.4860618114471436
I0210 19:01:58.808620 139615864289024 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.23053337633609772, loss=1.4775193929672241
I0210 19:02:34.590438 139615872681728 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.2318461835384369, loss=1.517651915550232
I0210 19:03:10.323425 139615864289024 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.22881484031677246, loss=1.5084712505340576
I0210 19:03:46.093136 139615872681728 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.22466331720352173, loss=1.4592405557632446
I0210 19:04:21.865540 139615864289024 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.23586362600326538, loss=1.4984626770019531
I0210 19:04:57.608532 139615872681728 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.2371852695941925, loss=1.5982258319854736
I0210 19:05:33.366197 139615864289024 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.2384791374206543, loss=1.4101991653442383
I0210 19:06:09.133761 139615872681728 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.2349213808774948, loss=1.463654637336731
I0210 19:06:44.910946 139615864289024 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.2419617921113968, loss=1.5729676485061646
I0210 19:07:20.688598 139615872681728 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.23001980781555176, loss=1.4533932209014893
I0210 19:07:56.416636 139615864289024 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.23656536638736725, loss=1.5880204439163208
I0210 19:08:32.177582 139615872681728 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.2259463667869568, loss=1.3983488082885742
I0210 19:09:07.925426 139615864289024 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.2314797341823578, loss=1.4628450870513916
I0210 19:09:43.706570 139615872681728 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.23174461722373962, loss=1.4981977939605713
I0210 19:10:19.477279 139615864289024 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.2309650331735611, loss=1.532091498374939
I0210 19:10:55.210202 139615872681728 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.23690731823444366, loss=1.4474365711212158
I0210 19:11:30.921860 139615864289024 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.23868021368980408, loss=1.5334373712539673
I0210 19:12:06.650470 139615872681728 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.22531603276729584, loss=1.4583147764205933
I0210 19:12:42.399678 139615864289024 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.2356947511434555, loss=1.4856547117233276
I0210 19:13:18.225198 139615872681728 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.22775347530841827, loss=1.5193744897842407
I0210 19:13:53.986057 139615864289024 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.2327205389738083, loss=1.5260534286499023
I0210 19:14:29.715831 139615872681728 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.22850537300109863, loss=1.4763588905334473
I0210 19:15:05.475428 139615864289024 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.22108997404575348, loss=1.3927855491638184
I0210 19:15:19.499925 139785736898368 spec.py:321] Evaluating on the training split.
I0210 19:15:22.496772 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:19:39.378358 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 19:19:42.089853 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:23:11.320170 139785736898368 spec.py:349] Evaluating on the test split.
I0210 19:23:14.050278 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:26:50.417911 139785736898368 submission_runner.py:408] Time since start: 69688.63s, 	Step: 115141, 	{'train/accuracy': 0.6917914748191833, 'train/loss': 1.399938702583313, 'train/bleu': 35.197873516392164, 'validation/accuracy': 0.6941389441490173, 'validation/loss': 1.3688514232635498, 'validation/bleu': 30.793833684223024, 'validation/num_examples': 3000, 'test/accuracy': 0.7127999663352966, 'test/loss': 1.2630198001861572, 'test/bleu': 30.843691765002152, 'test/num_examples': 3003, 'score': 41193.01627731323, 'total_duration': 69688.62987804413, 'accumulated_submission_time': 41193.01627731323, 'accumulated_eval_time': 28490.20787382126, 'accumulated_logging_time': 1.6810777187347412}
I0210 19:26:50.455739 139615872681728 logging_writer.py:48] [115141] accumulated_eval_time=28490.207874, accumulated_logging_time=1.681078, accumulated_submission_time=41193.016277, global_step=115141, preemption_count=0, score=41193.016277, test/accuracy=0.712800, test/bleu=30.843692, test/loss=1.263020, test/num_examples=3003, total_duration=69688.629878, train/accuracy=0.691791, train/bleu=35.197874, train/loss=1.399939, validation/accuracy=0.694139, validation/bleu=30.793834, validation/loss=1.368851, validation/num_examples=3000
I0210 19:27:11.892991 139615864289024 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.2307184636592865, loss=1.4236767292022705
I0210 19:27:47.633004 139615872681728 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.230790376663208, loss=1.518349289894104
I0210 19:28:23.361670 139615864289024 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.22097665071487427, loss=1.3884809017181396
I0210 19:28:59.119725 139615872681728 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.23668938875198364, loss=1.5317269563674927
I0210 19:29:34.856112 139615864289024 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.23257991671562195, loss=1.45811128616333
I0210 19:30:10.640009 139615872681728 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.23261107504367828, loss=1.5192091464996338
I0210 19:30:46.385914 139615864289024 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.2335958331823349, loss=1.5186970233917236
I0210 19:31:22.176575 139615872681728 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.245295912027359, loss=1.5048376321792603
I0210 19:31:58.001242 139615864289024 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.23320330679416656, loss=1.4613432884216309
I0210 19:32:33.767118 139615872681728 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.22751367092132568, loss=1.4758262634277344
I0210 19:33:09.532142 139615864289024 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.2279071807861328, loss=1.447417140007019
I0210 19:33:45.263180 139615872681728 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.23013891279697418, loss=1.514560580253601
I0210 19:34:20.995632 139615864289024 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.24853725731372833, loss=1.5026870965957642
I0210 19:34:56.711794 139615872681728 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.2512815594673157, loss=1.4865282773971558
I0210 19:35:32.479015 139615864289024 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.23334679007530212, loss=1.4598151445388794
I0210 19:36:08.241570 139615872681728 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.24434299767017365, loss=1.449324607849121
I0210 19:36:44.034030 139615864289024 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.23735934495925903, loss=1.5037946701049805
I0210 19:37:19.851634 139615872681728 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.23757675290107727, loss=1.456917405128479
I0210 19:37:55.604000 139615864289024 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.24139301478862762, loss=1.4478031396865845
I0210 19:38:31.344770 139615872681728 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.2362530380487442, loss=1.4817901849746704
I0210 19:39:07.091465 139615864289024 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.22786855697631836, loss=1.4504635334014893
I0210 19:39:42.824216 139615872681728 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.23143862187862396, loss=1.3818901777267456
I0210 19:40:18.588917 139615864289024 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.22516579926013947, loss=1.4394137859344482
I0210 19:40:50.477151 139785736898368 spec.py:321] Evaluating on the training split.
I0210 19:40:53.470501 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:44:38.778739 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 19:44:41.479658 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:47:19.453372 139785736898368 spec.py:349] Evaluating on the test split.
I0210 19:47:22.166242 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 19:50:05.219247 139785736898368 submission_runner.py:408] Time since start: 71083.43s, 	Step: 117491, 	{'train/accuracy': 0.6893499493598938, 'train/loss': 1.4066481590270996, 'train/bleu': 35.249029116488906, 'validation/accuracy': 0.6947464942932129, 'validation/loss': 1.3684260845184326, 'validation/bleu': 30.907103979729687, 'validation/num_examples': 3000, 'test/accuracy': 0.7123932838439941, 'test/loss': 1.262866735458374, 'test/bleu': 30.727742222734168, 'test/num_examples': 3003, 'score': 42032.95151424408, 'total_duration': 71083.43123865128, 'accumulated_submission_time': 42032.95151424408, 'accumulated_eval_time': 29044.949915885925, 'accumulated_logging_time': 1.7298576831817627}
I0210 19:50:05.251681 139615872681728 logging_writer.py:48] [117491] accumulated_eval_time=29044.949916, accumulated_logging_time=1.729858, accumulated_submission_time=42032.951514, global_step=117491, preemption_count=0, score=42032.951514, test/accuracy=0.712393, test/bleu=30.727742, test/loss=1.262867, test/num_examples=3003, total_duration=71083.431239, train/accuracy=0.689350, train/bleu=35.249029, train/loss=1.406648, validation/accuracy=0.694746, validation/bleu=30.907104, validation/loss=1.368426, validation/num_examples=3000
I0210 19:50:08.830177 139615864289024 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.2416553646326065, loss=1.5657161474227905
I0210 19:50:44.580507 139615872681728 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.2277318835258484, loss=1.4165390729904175
I0210 19:51:20.365402 139615864289024 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.22329595685005188, loss=1.4692614078521729
I0210 19:51:56.145204 139615872681728 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.23404665291309357, loss=1.4049162864685059
I0210 19:52:31.964748 139615864289024 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.23181559145450592, loss=1.4626681804656982
I0210 19:53:07.740106 139615872681728 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.2465285211801529, loss=1.4708040952682495
I0210 19:53:43.482575 139615864289024 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.2345978319644928, loss=1.516709327697754
I0210 19:54:19.239764 139615872681728 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.2284003645181656, loss=1.4420469999313354
I0210 19:54:55.044166 139615864289024 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.238497793674469, loss=1.4622235298156738
I0210 19:55:30.797816 139615872681728 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.23006153106689453, loss=1.4642082452774048
I0210 19:56:06.555273 139615864289024 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.23962555825710297, loss=1.4791377782821655
I0210 19:56:42.329626 139615872681728 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.24702346324920654, loss=1.48288094997406
I0210 19:57:18.085265 139615864289024 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.24384555220603943, loss=1.5584304332733154
I0210 19:57:53.821664 139615872681728 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.22661887109279633, loss=1.4948004484176636
I0210 19:58:29.571361 139615864289024 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.2374955117702484, loss=1.4571211338043213
I0210 19:59:05.362187 139615872681728 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.2382483184337616, loss=1.4974085092544556
I0210 19:59:41.125636 139615864289024 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.22507616877555847, loss=1.436594009399414
I0210 20:00:16.905130 139615872681728 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.24227260053157806, loss=1.4721742868423462
I0210 20:00:52.669607 139615864289024 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.2326592057943344, loss=1.4612157344818115
I0210 20:01:28.413631 139615872681728 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.24265694618225098, loss=1.5138392448425293
I0210 20:02:04.165250 139615864289024 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23646625876426697, loss=1.4702357053756714
I0210 20:02:39.948549 139615872681728 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.23542854189872742, loss=1.3907129764556885
I0210 20:03:15.692859 139615864289024 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.2510049045085907, loss=1.5508074760437012
I0210 20:03:51.432968 139615872681728 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.23776140809059143, loss=1.4975131750106812
I0210 20:04:05.453214 139785736898368 spec.py:321] Evaluating on the training split.
I0210 20:04:08.463181 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:08:01.397064 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 20:08:04.128102 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:10:36.926794 139785736898368 spec.py:349] Evaluating on the test split.
I0210 20:10:39.635746 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:13:30.343325 139785736898368 submission_runner.py:408] Time since start: 72488.56s, 	Step: 119841, 	{'train/accuracy': 0.6982017755508423, 'train/loss': 1.360303521156311, 'train/bleu': 35.42124147020956, 'validation/accuracy': 0.696234405040741, 'validation/loss': 1.3644174337387085, 'validation/bleu': 30.810002552655714, 'validation/num_examples': 3000, 'test/accuracy': 0.7137296199798584, 'test/loss': 1.258353590965271, 'test/bleu': 30.89031023851516, 'test/num_examples': 3003, 'score': 42873.06673336029, 'total_duration': 72488.55532360077, 'accumulated_submission_time': 42873.06673336029, 'accumulated_eval_time': 29609.83997654915, 'accumulated_logging_time': 1.772599458694458}
I0210 20:13:30.375674 139615864289024 logging_writer.py:48] [119841] accumulated_eval_time=29609.839977, accumulated_logging_time=1.772599, accumulated_submission_time=42873.066733, global_step=119841, preemption_count=0, score=42873.066733, test/accuracy=0.713730, test/bleu=30.890310, test/loss=1.258354, test/num_examples=3003, total_duration=72488.555324, train/accuracy=0.698202, train/bleu=35.421241, train/loss=1.360304, validation/accuracy=0.696234, validation/bleu=30.810003, validation/loss=1.364417, validation/num_examples=3000
I0210 20:13:51.785834 139615872681728 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.23306189477443695, loss=1.4323254823684692
I0210 20:14:27.510392 139615864289024 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.2400718778371811, loss=1.401489496231079
I0210 20:15:03.329874 139615872681728 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.24778105318546295, loss=1.424489974975586
I0210 20:15:39.126642 139615864289024 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.23972612619400024, loss=1.4802701473236084
I0210 20:16:14.906945 139615872681728 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.23987789452075958, loss=1.4421186447143555
I0210 20:16:50.656852 139615864289024 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.24004532396793365, loss=1.4791356325149536
I0210 20:17:26.412519 139615872681728 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.23608019948005676, loss=1.3893201351165771
I0210 20:18:02.199872 139615864289024 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.2377406656742096, loss=1.4848268032073975
I0210 20:18:37.945904 139615872681728 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.24688951671123505, loss=1.4413493871688843
I0210 20:19:13.724281 139615864289024 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.239397332072258, loss=1.4239037036895752
I0210 20:19:49.513670 139615872681728 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.2325146198272705, loss=1.445214033126831
I0210 20:20:25.312095 139615864289024 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.2386302351951599, loss=1.4339567422866821
I0210 20:21:01.109351 139615872681728 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.23447443544864655, loss=1.4994351863861084
I0210 20:21:36.862770 139615864289024 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.23275449872016907, loss=1.426213026046753
I0210 20:22:12.635028 139615872681728 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.2334047555923462, loss=1.5100449323654175
I0210 20:22:48.431266 139615864289024 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.2308473140001297, loss=1.405510663986206
I0210 20:23:24.201774 139615872681728 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.23225954174995422, loss=1.4282090663909912
I0210 20:23:59.943692 139615864289024 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.23957529664039612, loss=1.4310344457626343
I0210 20:24:35.717102 139615872681728 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.25384414196014404, loss=1.4626094102859497
I0210 20:25:11.512723 139615864289024 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23942475020885468, loss=1.4501651525497437
I0210 20:25:47.316969 139615872681728 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.2356748878955841, loss=1.4756219387054443
I0210 20:26:23.052664 139615864289024 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.23463168740272522, loss=1.4776606559753418
I0210 20:26:58.823056 139615872681728 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.22858195006847382, loss=1.416293978691101
I0210 20:27:30.344293 139785736898368 spec.py:321] Evaluating on the training split.
I0210 20:27:33.350205 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:31:45.447406 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 20:31:48.192967 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:35:03.123836 139785736898368 spec.py:349] Evaluating on the test split.
I0210 20:35:05.842817 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:38:36.106654 139785736898368 submission_runner.py:408] Time since start: 73994.32s, 	Step: 122190, 	{'train/accuracy': 0.6944260001182556, 'train/loss': 1.3774758577346802, 'train/bleu': 35.73075424767851, 'validation/accuracy': 0.6958252191543579, 'validation/loss': 1.363082766532898, 'validation/bleu': 30.871959597197357, 'validation/num_examples': 3000, 'test/accuracy': 0.7138806581497192, 'test/loss': 1.2562905550003052, 'test/bleu': 31.077669581654437, 'test/num_examples': 3003, 'score': 43712.94831061363, 'total_duration': 73994.31863379478, 'accumulated_submission_time': 43712.94831061363, 'accumulated_eval_time': 30275.602272987366, 'accumulated_logging_time': 1.814788818359375}
I0210 20:38:36.139184 139615864289024 logging_writer.py:48] [122190] accumulated_eval_time=30275.602273, accumulated_logging_time=1.814789, accumulated_submission_time=43712.948311, global_step=122190, preemption_count=0, score=43712.948311, test/accuracy=0.713881, test/bleu=31.077670, test/loss=1.256291, test/num_examples=3003, total_duration=73994.318634, train/accuracy=0.694426, train/bleu=35.730754, train/loss=1.377476, validation/accuracy=0.695825, validation/bleu=30.871960, validation/loss=1.363083, validation/num_examples=3000
I0210 20:38:40.074229 139615872681728 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.24358956515789032, loss=1.4745447635650635
I0210 20:39:15.803025 139615864289024 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.2413892149925232, loss=1.4944474697113037
I0210 20:39:51.562665 139615872681728 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.23127910494804382, loss=1.3652164936065674
I0210 20:40:27.328850 139615864289024 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.23413562774658203, loss=1.4552887678146362
I0210 20:41:03.076107 139615872681728 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.23006998002529144, loss=1.40976083278656
I0210 20:41:38.877411 139615864289024 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.24026991426944733, loss=1.5326169729232788
I0210 20:42:14.611847 139615872681728 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.2396753579378128, loss=1.4451797008514404
I0210 20:42:50.404236 139615864289024 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.23783843219280243, loss=1.4340999126434326
I0210 20:43:26.162702 139615872681728 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.23824013769626617, loss=1.4723669290542603
I0210 20:44:01.992278 139615864289024 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.2344922423362732, loss=1.4485114812850952
I0210 20:44:37.807612 139615872681728 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.24624362587928772, loss=1.4332419633865356
I0210 20:45:13.583283 139615864289024 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.23730210959911346, loss=1.4705740213394165
I0210 20:45:49.347368 139615872681728 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.23271363973617554, loss=1.4844807386398315
I0210 20:46:25.146005 139615864289024 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.2379755973815918, loss=1.4949705600738525
I0210 20:47:00.948062 139615872681728 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.2235671579837799, loss=1.4293729066848755
I0210 20:47:36.682524 139615864289024 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.2250823825597763, loss=1.5010918378829956
I0210 20:48:12.438245 139615872681728 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.22629804909229279, loss=1.4252171516418457
I0210 20:48:48.222620 139615864289024 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.23737733066082, loss=1.4648293256759644
I0210 20:49:24.005616 139615872681728 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.24247069656848907, loss=1.5065349340438843
I0210 20:49:59.766875 139615864289024 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.228043794631958, loss=1.4279522895812988
I0210 20:50:35.486472 139615872681728 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.23570363223552704, loss=1.4500290155410767
I0210 20:51:11.219604 139615864289024 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.24687562882900238, loss=1.5327266454696655
I0210 20:51:47.028445 139615872681728 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.23895514011383057, loss=1.4405452013015747
I0210 20:52:22.786560 139615864289024 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.22908610105514526, loss=1.53009033203125
I0210 20:52:36.444977 139785736898368 spec.py:321] Evaluating on the training split.
I0210 20:52:39.445864 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:56:43.061735 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 20:56:45.791322 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 20:59:33.147599 139785736898368 spec.py:349] Evaluating on the test split.
I0210 20:59:35.889662 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:02:25.775909 139785736898368 submission_runner.py:408] Time since start: 75423.99s, 	Step: 124540, 	{'train/accuracy': 0.6927077174186707, 'train/loss': 1.3844177722930908, 'train/bleu': 35.60168167605079, 'validation/accuracy': 0.6960111856460571, 'validation/loss': 1.3612251281738281, 'validation/bleu': 31.036330806615823, 'validation/num_examples': 3000, 'test/accuracy': 0.7133693695068359, 'test/loss': 1.2556757926940918, 'test/bleu': 30.946606236975953, 'test/num_examples': 3003, 'score': 44553.167630434036, 'total_duration': 75423.9879014492, 'accumulated_submission_time': 44553.167630434036, 'accumulated_eval_time': 30864.93314909935, 'accumulated_logging_time': 1.8580679893493652}
I0210 21:02:25.807627 139615872681728 logging_writer.py:48] [124540] accumulated_eval_time=30864.933149, accumulated_logging_time=1.858068, accumulated_submission_time=44553.167630, global_step=124540, preemption_count=0, score=44553.167630, test/accuracy=0.713369, test/bleu=30.946606, test/loss=1.255676, test/num_examples=3003, total_duration=75423.987901, train/accuracy=0.692708, train/bleu=35.601682, train/loss=1.384418, validation/accuracy=0.696011, validation/bleu=31.036331, validation/loss=1.361225, validation/num_examples=3000
I0210 21:02:47.616247 139615864289024 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.23632946610450745, loss=1.4590494632720947
I0210 21:03:23.365923 139615872681728 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.23396402597427368, loss=1.4788868427276611
I0210 21:03:59.169039 139615864289024 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.23212261497974396, loss=1.4341777563095093
I0210 21:04:34.928915 139615872681728 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.23902177810668945, loss=1.4784334897994995
I0210 21:05:10.724210 139615864289024 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.22077295184135437, loss=1.4352633953094482
I0210 21:05:46.519137 139615872681728 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.23982034623622894, loss=1.4906022548675537
I0210 21:06:22.242607 139615864289024 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.23172052204608917, loss=1.3797904253005981
I0210 21:06:57.986600 139615872681728 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.24153868854045868, loss=1.5780754089355469
I0210 21:07:33.737770 139615864289024 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.24301570653915405, loss=1.5142130851745605
I0210 21:08:09.481344 139615872681728 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.2429049015045166, loss=1.4776195287704468
I0210 21:08:45.247001 139615864289024 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.2373080849647522, loss=1.436021089553833
I0210 21:09:20.985287 139615872681728 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.235463485121727, loss=1.4480928182601929
I0210 21:09:56.827434 139615864289024 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.2322675734758377, loss=1.4371165037155151
I0210 21:10:32.579301 139615872681728 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.22990049421787262, loss=1.48233962059021
I0210 21:11:08.322600 139615864289024 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.24382083117961884, loss=1.4905849695205688
I0210 21:11:44.076480 139615872681728 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.2301691770553589, loss=1.4639182090759277
I0210 21:12:19.881251 139615864289024 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.26603153347969055, loss=1.430019736289978
I0210 21:12:55.669444 139615872681728 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.23163677752017975, loss=1.4174180030822754
I0210 21:13:31.394214 139615864289024 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.22757063806056976, loss=1.3316643238067627
I0210 21:14:07.149705 139615872681728 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.23863261938095093, loss=1.4580261707305908
I0210 21:14:42.893333 139615864289024 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.22495560348033905, loss=1.4568122625350952
I0210 21:15:18.661867 139615872681728 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.2400844395160675, loss=1.4984419345855713
I0210 21:15:54.420425 139615864289024 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.23899182677268982, loss=1.5262023210525513
I0210 21:16:25.964506 139785736898368 spec.py:321] Evaluating on the training split.
I0210 21:16:28.959939 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:20:26.990361 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 21:20:29.728465 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:23:32.464339 139785736898368 spec.py:349] Evaluating on the test split.
I0210 21:23:35.186545 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:26:33.684913 139785736898368 submission_runner.py:408] Time since start: 76871.90s, 	Step: 126890, 	{'train/accuracy': 0.6990454196929932, 'train/loss': 1.3546652793884277, 'train/bleu': 35.55317402084076, 'validation/accuracy': 0.6965071558952332, 'validation/loss': 1.360289216041565, 'validation/bleu': 30.923319946605933, 'validation/num_examples': 3000, 'test/accuracy': 0.7141130566596985, 'test/loss': 1.2537082433700562, 'test/bleu': 30.891123087192003, 'test/num_examples': 3003, 'score': 45393.23704409599, 'total_duration': 76871.89690589905, 'accumulated_submission_time': 45393.23704409599, 'accumulated_eval_time': 31472.653499126434, 'accumulated_logging_time': 1.9009826183319092}
I0210 21:26:33.717298 139615872681728 logging_writer.py:48] [126890] accumulated_eval_time=31472.653499, accumulated_logging_time=1.900983, accumulated_submission_time=45393.237044, global_step=126890, preemption_count=0, score=45393.237044, test/accuracy=0.714113, test/bleu=30.891123, test/loss=1.253708, test/num_examples=3003, total_duration=76871.896906, train/accuracy=0.699045, train/bleu=35.553174, train/loss=1.354665, validation/accuracy=0.696507, validation/bleu=30.923320, validation/loss=1.360289, validation/num_examples=3000
I0210 21:26:37.653881 139615864289024 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.23740322887897491, loss=1.396790862083435
I0210 21:27:13.353316 139615872681728 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.23429876565933228, loss=1.4390661716461182
I0210 21:27:49.075953 139615864289024 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.22631524503231049, loss=1.3981086015701294
I0210 21:28:24.799108 139615872681728 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.24198056757450104, loss=1.5055773258209229
I0210 21:29:00.558885 139615864289024 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.24173972010612488, loss=1.5139729976654053
I0210 21:29:36.278969 139615872681728 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.22980494797229767, loss=1.4773107767105103
I0210 21:30:12.047332 139615864289024 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.2455410659313202, loss=1.414662480354309
I0210 21:30:47.842334 139615872681728 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.23368634283542633, loss=1.4384394884109497
I0210 21:31:23.618291 139615864289024 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.22664445638656616, loss=1.4469208717346191
I0210 21:31:59.444967 139615872681728 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.22944900393486023, loss=1.4402618408203125
I0210 21:32:35.168489 139615864289024 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.23993253707885742, loss=1.4318534135818481
I0210 21:33:10.960915 139615872681728 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.23684105277061462, loss=1.5246049165725708
I0210 21:33:46.718350 139615864289024 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.2380232810974121, loss=1.4569884538650513
I0210 21:34:22.480923 139615872681728 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.22945544123649597, loss=1.4423179626464844
I0210 21:34:58.250926 139615864289024 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.2298477739095688, loss=1.4060474634170532
I0210 21:35:34.028569 139615872681728 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.2287798672914505, loss=1.437611699104309
I0210 21:36:09.783709 139615864289024 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.22475692629814148, loss=1.376493215560913
I0210 21:36:45.507187 139615872681728 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.2255104035139084, loss=1.4290896654129028
I0210 21:37:21.284657 139615864289024 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.2259921282529831, loss=1.3519635200500488
I0210 21:37:57.035192 139615872681728 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.26376205682754517, loss=1.423284649848938
I0210 21:38:32.819588 139615864289024 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.23274275660514832, loss=1.4440168142318726
I0210 21:39:08.787298 139615872681728 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.24828945100307465, loss=1.4963769912719727
I0210 21:39:44.597025 139615864289024 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.23773929476737976, loss=1.5016943216323853
I0210 21:40:20.432821 139615872681728 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.24229863286018372, loss=1.4579969644546509
I0210 21:40:33.763701 139785736898368 spec.py:321] Evaluating on the training split.
I0210 21:40:36.789386 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:44:50.365899 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 21:44:53.070207 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:47:49.181403 139785736898368 spec.py:349] Evaluating on the test split.
I0210 21:47:51.895156 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 21:51:13.982628 139785736898368 submission_runner.py:408] Time since start: 78352.19s, 	Step: 129239, 	{'train/accuracy': 0.6980040073394775, 'train/loss': 1.359439492225647, 'train/bleu': 35.294106469705945, 'validation/accuracy': 0.6968419551849365, 'validation/loss': 1.3597447872161865, 'validation/bleu': 30.871485970728653, 'validation/num_examples': 3000, 'test/accuracy': 0.714043378829956, 'test/loss': 1.252797245979309, 'test/bleu': 30.942558677133007, 'test/num_examples': 3003, 'score': 46233.19505262375, 'total_duration': 78352.19459056854, 'accumulated_submission_time': 46233.19505262375, 'accumulated_eval_time': 32112.87235379219, 'accumulated_logging_time': 1.943530559539795}
I0210 21:51:14.021413 139615864289024 logging_writer.py:48] [129239] accumulated_eval_time=32112.872354, accumulated_logging_time=1.943531, accumulated_submission_time=46233.195053, global_step=129239, preemption_count=0, score=46233.195053, test/accuracy=0.714043, test/bleu=30.942559, test/loss=1.252797, test/num_examples=3003, total_duration=78352.194591, train/accuracy=0.698004, train/bleu=35.294106, train/loss=1.359439, validation/accuracy=0.696842, validation/bleu=30.871486, validation/loss=1.359745, validation/num_examples=3000
I0210 21:51:36.189447 139615872681728 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.24254640936851501, loss=1.465707540512085
I0210 21:52:11.982995 139615864289024 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.23206961154937744, loss=1.4588714838027954
I0210 21:52:47.829468 139615872681728 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.2494909018278122, loss=1.512251615524292
I0210 21:53:23.598207 139615864289024 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.23926955461502075, loss=1.4470055103302002
I0210 21:53:59.328542 139615872681728 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.23948082327842712, loss=1.4137052297592163
I0210 21:54:35.059966 139615864289024 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.2283087521791458, loss=1.4295035600662231
I0210 21:55:10.815527 139615872681728 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.2326761782169342, loss=1.4785963296890259
I0210 21:55:46.583981 139615864289024 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.23224478960037231, loss=1.4473698139190674
I0210 21:56:22.468979 139615872681728 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.23687660694122314, loss=1.4497041702270508
I0210 21:56:58.210551 139615864289024 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.23586291074752808, loss=1.496432900428772
I0210 21:57:33.974662 139615872681728 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.23180867731571198, loss=1.4259446859359741
I0210 21:58:09.744874 139615864289024 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.2311074584722519, loss=1.3425748348236084
I0210 21:58:45.527593 139615872681728 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.2387249916791916, loss=1.4747719764709473
I0210 21:59:21.301709 139615864289024 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.2266066074371338, loss=1.4610931873321533
I0210 21:59:57.071919 139615872681728 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.246158629655838, loss=1.4722554683685303
I0210 22:00:32.906136 139615864289024 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.22416852414608002, loss=1.4183672666549683
I0210 22:01:08.724364 139615872681728 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.24090184271335602, loss=1.5567020177841187
I0210 22:01:44.503979 139615864289024 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.23507976531982422, loss=1.4195847511291504
I0210 22:02:20.410187 139615872681728 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.23796403408050537, loss=1.4462254047393799
I0210 22:02:56.171671 139615864289024 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.22775322198867798, loss=1.4478405714035034
I0210 22:03:31.983128 139615872681728 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.2302519977092743, loss=1.4906089305877686
I0210 22:04:07.726812 139615864289024 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.23670023679733276, loss=1.4164777994155884
I0210 22:04:43.480978 139615872681728 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.23380659520626068, loss=1.4424381256103516
I0210 22:05:14.292545 139785736898368 spec.py:321] Evaluating on the training split.
I0210 22:05:17.290624 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 22:08:41.813200 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 22:08:44.540121 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 22:11:36.787599 139785736898368 spec.py:349] Evaluating on the test split.
I0210 22:11:39.506952 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 22:15:00.006453 139785736898368 submission_runner.py:408] Time since start: 79778.22s, 	Step: 131588, 	{'train/accuracy': 0.6947080492973328, 'train/loss': 1.3761208057403564, 'train/bleu': 35.40285800418826, 'validation/accuracy': 0.6967551708221436, 'validation/loss': 1.3596832752227783, 'validation/bleu': 30.906944090047656, 'validation/num_examples': 3000, 'test/accuracy': 0.714043378829956, 'test/loss': 1.252558946609497, 'test/bleu': 30.929931785958644, 'test/num_examples': 3003, 'score': 47073.37712454796, 'total_duration': 79778.21843957901, 'accumulated_submission_time': 47073.37712454796, 'accumulated_eval_time': 32698.586216688156, 'accumulated_logging_time': 1.9933269023895264}
I0210 22:15:00.040606 139615864289024 logging_writer.py:48] [131588] accumulated_eval_time=32698.586217, accumulated_logging_time=1.993327, accumulated_submission_time=47073.377125, global_step=131588, preemption_count=0, score=47073.377125, test/accuracy=0.714043, test/bleu=30.929932, test/loss=1.252559, test/num_examples=3003, total_duration=79778.218440, train/accuracy=0.694708, train/bleu=35.402858, train/loss=1.376121, validation/accuracy=0.696755, validation/bleu=30.906944, validation/loss=1.359683, validation/num_examples=3000
I0210 22:15:04.691757 139615872681728 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.23050008714199066, loss=1.4803783893585205
I0210 22:15:40.422963 139615864289024 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.25253382325172424, loss=1.4808745384216309
I0210 22:16:16.195379 139615872681728 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.24098801612854004, loss=1.509702205657959
I0210 22:16:51.923192 139615864289024 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.24296008050441742, loss=1.430781602859497
I0210 22:17:27.672803 139615872681728 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.23223736882209778, loss=1.4180076122283936
I0210 22:18:03.445207 139615864289024 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.2357819825410843, loss=1.4220600128173828
I0210 22:18:39.218837 139615872681728 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.23791880905628204, loss=1.437326431274414
I0210 22:19:14.993177 139615864289024 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.22763928771018982, loss=1.4142930507659912
I0210 22:19:50.769740 139615872681728 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.2315799593925476, loss=1.4141051769256592
I0210 22:20:26.535063 139615864289024 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.23395733535289764, loss=1.5039572715759277
I0210 22:21:02.298591 139615872681728 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.2366732507944107, loss=1.4760239124298096
I0210 22:21:38.080301 139615864289024 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.24532943964004517, loss=1.4669216871261597
I0210 22:22:13.842125 139615872681728 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.22798851132392883, loss=1.4162321090698242
I0210 22:22:49.637371 139615864289024 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2297186702489853, loss=1.4620800018310547
I0210 22:23:25.380515 139615872681728 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.23157630860805511, loss=1.4421783685684204
I0210 22:24:01.131503 139615864289024 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.22880233824253082, loss=1.4445933103561401
I0210 22:24:36.891536 139615872681728 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.23288458585739136, loss=1.3616681098937988
I0210 22:25:12.705258 139615864289024 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.24283859133720398, loss=1.558480978012085
I0210 22:25:23.892090 139785736898368 spec.py:321] Evaluating on the training split.
I0210 22:25:26.895166 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 22:29:04.491967 139785736898368 spec.py:333] Evaluating on the validation split.
I0210 22:29:07.221883 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 22:32:07.187280 139785736898368 spec.py:349] Evaluating on the test split.
I0210 22:32:09.893724 139785736898368 workload.py:181] Translating evaluation dataset.
I0210 22:35:17.501491 139785736898368 submission_runner.py:408] Time since start: 80995.71s, 	Step: 133333, 	{'train/accuracy': 0.6980851888656616, 'train/loss': 1.3578859567642212, 'train/bleu': 35.45882877194227, 'validation/accuracy': 0.6968915462493896, 'validation/loss': 1.359694004058838, 'validation/bleu': 30.90651925945721, 'validation/num_examples': 3000, 'test/accuracy': 0.7140317559242249, 'test/loss': 1.2525193691253662, 'test/bleu': 30.921156057591386, 'test/num_examples': 3003, 'score': 47697.16169524193, 'total_duration': 80995.71348690987, 'accumulated_submission_time': 47697.16169524193, 'accumulated_eval_time': 33292.195563316345, 'accumulated_logging_time': 2.0389645099639893}
I0210 22:35:17.536074 139615872681728 logging_writer.py:48] [133333] accumulated_eval_time=33292.195563, accumulated_logging_time=2.038965, accumulated_submission_time=47697.161695, global_step=133333, preemption_count=0, score=47697.161695, test/accuracy=0.714032, test/bleu=30.921156, test/loss=1.252519, test/num_examples=3003, total_duration=80995.713487, train/accuracy=0.698085, train/bleu=35.458829, train/loss=1.357886, validation/accuracy=0.696892, validation/bleu=30.906519, validation/loss=1.359694, validation/num_examples=3000
I0210 22:35:17.570481 139615864289024 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47697.161695
I0210 22:35:18.795022 139785736898368 checkpoints.py:490] Saving checkpoint at step: 133333
I0210 22:35:22.781662 139785736898368 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5/checkpoint_133333
I0210 22:35:22.786725 139785736898368 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/wmt_jax/trial_5/checkpoint_133333.
I0210 22:35:22.862146 139785736898368 submission_runner.py:583] Tuning trial 5/5
I0210 22:35:22.862344 139785736898368 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0210 22:35:22.875072 139785736898368 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006145372171886265, 'train/loss': 11.063870429992676, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.036645889282227, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.041826248168945, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.065114498138428, 'total_duration': 899.9457039833069, 'accumulated_submission_time': 30.065114498138428, 'accumulated_eval_time': 869.8805475234985, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2349, {'train/accuracy': 0.5141674280166626, 'train/loss': 2.8809711933135986, 'train/bleu': 22.965880254505947, 'validation/accuracy': 0.5140295624732971, 'validation/loss': 2.860582113265991, 'validation/bleu': 18.528226390624614, 'validation/num_examples': 3000, 'test/accuracy': 0.5108012557029724, 'test/loss': 2.912444591522217, 'test/bleu': 16.717221966085745, 'test/num_examples': 3003, 'score': 870.2016100883484, 'total_duration': 2221.916384935379, 'accumulated_submission_time': 870.2016100883484, 'accumulated_eval_time': 1351.6217308044434, 'accumulated_logging_time': 0.01892399787902832, 'global_step': 2349, 'preemption_count': 0}), (4699, {'train/accuracy': 0.5758695602416992, 'train/loss': 2.2680907249450684, 'train/bleu': 27.232874276111815, 'validation/accuracy': 0.589490532875061, 'validation/loss': 2.1476547718048096, 'validation/bleu': 23.513794603427275, 'validation/num_examples': 3000, 'test/accuracy': 0.5915867686271667, 'test/loss': 2.1269924640655518, 'test/bleu': 22.13067104185569, 'test/num_examples': 3003, 'score': 1710.351322889328, 'total_duration': 3532.369195461273, 'accumulated_submission_time': 1710.351322889328, 'accumulated_eval_time': 1821.823566198349, 'accumulated_logging_time': 0.04367566108703613, 'global_step': 4699, 'preemption_count': 0}), (7049, {'train/accuracy': 0.6065484881401062, 'train/loss': 1.9995412826538086, 'train/bleu': 29.012708543686976, 'validation/accuracy': 0.6178224682807922, 'validation/loss': 1.9245233535766602, 'validation/bleu': 25.410653443752153, 'validation/num_examples': 3000, 'test/accuracy': 0.620010495185852, 'test/loss': 1.8880869150161743, 'test/bleu': 23.77728489354792, 'test/num_examples': 3003, 'score': 2550.4531705379486, 'total_duration': 4838.414577007294, 'accumulated_submission_time': 2550.4531705379486, 'accumulated_eval_time': 2287.665424346924, 'accumulated_logging_time': 0.06898212432861328, 'global_step': 7049, 'preemption_count': 0}), (9399, {'train/accuracy': 0.6141012907028198, 'train/loss': 1.939665675163269, 'train/bleu': 29.74698626680871, 'validation/accuracy': 0.6299859881401062, 'validation/loss': 1.8144594430923462, 'validation/bleu': 26.071460310919417, 'validation/num_examples': 3000, 'test/accuracy': 0.6367672085762024, 'test/loss': 1.7608153820037842, 'test/bleu': 25.14278859816317, 'test/num_examples': 3003, 'score': 3390.548141002655, 'total_duration': 6187.265499591827, 'accumulated_submission_time': 3390.548141002655, 'accumulated_eval_time': 2796.3146154880524, 'accumulated_logging_time': 0.09975481033325195, 'global_step': 9399, 'preemption_count': 0}), (11749, {'train/accuracy': 0.6164451241493225, 'train/loss': 1.9157977104187012, 'train/bleu': 29.745159669492597, 'validation/accuracy': 0.6390249133110046, 'validation/loss': 1.7488797903060913, 'validation/bleu': 26.594450639942533, 'validation/num_examples': 3000, 'test/accuracy': 0.6456801295280457, 'test/loss': 1.6872280836105347, 'test/bleu': 25.485975409278893, 'test/num_examples': 3003, 'score': 4230.704621553421, 'total_duration': 7516.816064357758, 'accumulated_submission_time': 4230.704621553421, 'accumulated_eval_time': 3285.607243537903, 'accumulated_logging_time': 0.12725400924682617, 'global_step': 11749, 'preemption_count': 0}), (14099, {'train/accuracy': 0.6286578178405762, 'train/loss': 1.8235194683074951, 'train/bleu': 30.895011292840326, 'validation/accuracy': 0.6437737941741943, 'validation/loss': 1.7144992351531982, 'validation/bleu': 26.906824146885256, 'validation/num_examples': 3000, 'test/accuracy': 0.651943564414978, 'test/loss': 1.6426197290420532, 'test/bleu': 26.12009908804103, 'test/num_examples': 3003, 'score': 5070.610538482666, 'total_duration': 8873.669828891754, 'accumulated_submission_time': 5070.610538482666, 'accumulated_eval_time': 3802.4545085430145, 'accumulated_logging_time': 0.15345525741577148, 'global_step': 14099, 'preemption_count': 0}), (16449, {'train/accuracy': 0.6269233226776123, 'train/loss': 1.8308695554733276, 'train/bleu': 30.456122777081536, 'validation/accuracy': 0.6486342549324036, 'validation/loss': 1.6799792051315308, 'validation/bleu': 27.54467483066716, 'validation/num_examples': 3000, 'test/accuracy': 0.6589855551719666, 'test/loss': 1.607773780822754, 'test/bleu': 26.91923126334531, 'test/num_examples': 3003, 'score': 5910.604150533676, 'total_duration': 10212.38568687439, 'accumulated_submission_time': 5910.604150533676, 'accumulated_eval_time': 4301.075335741043, 'accumulated_logging_time': 0.17943310737609863, 'global_step': 16449, 'preemption_count': 0}), (18800, {'train/accuracy': 0.671360194683075, 'train/loss': 1.5273321866989136, 'train/bleu': 34.06164879315195, 'validation/accuracy': 0.6512752175331116, 'validation/loss': 1.659209132194519, 'validation/bleu': 27.372778668807793, 'validation/num_examples': 3000, 'test/accuracy': 0.6605542898178101, 'test/loss': 1.596093773841858, 'test/bleu': 26.86495937979053, 'test/num_examples': 3003, 'score': 6750.731124639511, 'total_duration': 11513.733264446259, 'accumulated_submission_time': 6750.731124639511, 'accumulated_eval_time': 4762.190878391266, 'accumulated_logging_time': 0.2115025520324707, 'global_step': 18800, 'preemption_count': 0}), (21150, {'train/accuracy': 0.6342859268188477, 'train/loss': 1.768483281135559, 'train/bleu': 30.72447289999782, 'validation/accuracy': 0.6521803736686707, 'validation/loss': 1.6406331062316895, 'validation/bleu': 27.661841579977537, 'validation/num_examples': 3000, 'test/accuracy': 0.6623671054840088, 'test/loss': 1.5717984437942505, 'test/bleu': 26.760107849685753, 'test/num_examples': 3003, 'score': 7590.68705701828, 'total_duration': 12867.510741472244, 'accumulated_submission_time': 7590.68705701828, 'accumulated_eval_time': 5275.910169124603, 'accumulated_logging_time': 0.24033284187316895, 'global_step': 21150, 'preemption_count': 0}), (23500, {'train/accuracy': 0.6302686333656311, 'train/loss': 1.8009165525436401, 'train/bleu': 30.477764659911852, 'validation/accuracy': 0.6556645035743713, 'validation/loss': 1.6268913745880127, 'validation/bleu': 27.54684052250174, 'validation/num_examples': 3000, 'test/accuracy': 0.667851984500885, 'test/loss': 1.5515425205230713, 'test/bleu': 27.18575754374719, 'test/num_examples': 3003, 'score': 8430.784281015396, 'total_duration': 14261.82667016983, 'accumulated_submission_time': 8430.784281015396, 'accumulated_eval_time': 5830.0237646102905, 'accumulated_logging_time': 0.27062082290649414, 'global_step': 23500, 'preemption_count': 0}), (25851, {'train/accuracy': 0.6471864581108093, 'train/loss': 1.6817272901535034, 'train/bleu': 31.524659902627377, 'validation/accuracy': 0.6565448641777039, 'validation/loss': 1.6173309087753296, 'validation/bleu': 27.799491595963826, 'validation/num_examples': 3000, 'test/accuracy': 0.6690372824668884, 'test/loss': 1.541406273841858, 'test/bleu': 27.224962151330093, 'test/num_examples': 3003, 'score': 9270.913045167923, 'total_duration': 15614.465742111206, 'accumulated_submission_time': 9270.913045167923, 'accumulated_eval_time': 6342.428351640701, 'accumulated_logging_time': 0.3031198978424072, 'global_step': 25851, 'preemption_count': 0}), (28201, {'train/accuracy': 0.6392074823379517, 'train/loss': 1.7369545698165894, 'train/bleu': 31.261923161305248, 'validation/accuracy': 0.6590991020202637, 'validation/loss': 1.6041685342788696, 'validation/bleu': 28.174983894107854, 'validation/num_examples': 3000, 'test/accuracy': 0.6693510413169861, 'test/loss': 1.5309597253799438, 'test/bleu': 27.50862562138666, 'test/num_examples': 3003, 'score': 10111.160809516907, 'total_duration': 17149.126772880554, 'accumulated_submission_time': 10111.160809516907, 'accumulated_eval_time': 7036.736354827881, 'accumulated_logging_time': 0.33391356468200684, 'global_step': 28201, 'preemption_count': 0}), (30552, {'train/accuracy': 0.6390373110771179, 'train/loss': 1.7507790327072144, 'train/bleu': 31.27872196488971, 'validation/accuracy': 0.6580327749252319, 'validation/loss': 1.595872402191162, 'validation/bleu': 27.93156543775106, 'validation/num_examples': 3000, 'test/accuracy': 0.6702225208282471, 'test/loss': 1.522372841835022, 'test/bleu': 27.486084020074678, 'test/num_examples': 3003, 'score': 10951.253396511078, 'total_duration': 18556.146406650543, 'accumulated_submission_time': 10951.253396511078, 'accumulated_eval_time': 7603.5600752830505, 'accumulated_logging_time': 0.36229705810546875, 'global_step': 30552, 'preemption_count': 0}), (32903, {'train/accuracy': 0.6409906148910522, 'train/loss': 1.7161387205123901, 'train/bleu': 31.478213278866903, 'validation/accuracy': 0.6610209345817566, 'validation/loss': 1.5890172719955444, 'validation/bleu': 28.024704001168377, 'validation/num_examples': 3000, 'test/accuracy': 0.6717913150787354, 'test/loss': 1.5121753215789795, 'test/bleu': 27.377931603736805, 'test/num_examples': 3003, 'score': 11791.432320356369, 'total_duration': 19927.55940771103, 'accumulated_submission_time': 11791.432320356369, 'accumulated_eval_time': 8134.691674232483, 'accumulated_logging_time': 0.39051246643066406, 'global_step': 32903, 'preemption_count': 0}), (35253, {'train/accuracy': 0.6431419849395752, 'train/loss': 1.7123115062713623, 'train/bleu': 31.608488104923683, 'validation/accuracy': 0.6615169048309326, 'validation/loss': 1.5796931982040405, 'validation/bleu': 28.002704988103364, 'validation/num_examples': 3000, 'test/accuracy': 0.6747545599937439, 'test/loss': 1.4998594522476196, 'test/bleu': 28.059762524152184, 'test/num_examples': 3003, 'score': 12631.56686782837, 'total_duration': 21323.122226953506, 'accumulated_submission_time': 12631.56686782837, 'accumulated_eval_time': 8690.012436389923, 'accumulated_logging_time': 0.4196438789367676, 'global_step': 35253, 'preemption_count': 0}), (37603, {'train/accuracy': 0.6761975288391113, 'train/loss': 1.4833768606185913, 'train/bleu': 34.013655184488385, 'validation/accuracy': 0.6622856259346008, 'validation/loss': 1.571945309638977, 'validation/bleu': 28.19304704726278, 'validation/num_examples': 3000, 'test/accuracy': 0.6743013262748718, 'test/loss': 1.4964731931686401, 'test/bleu': 27.469852057344102, 'test/num_examples': 3003, 'score': 13471.785254716873, 'total_duration': 22719.035972356796, 'accumulated_submission_time': 13471.785254716873, 'accumulated_eval_time': 9245.60012793541, 'accumulated_logging_time': 0.45088791847229004, 'global_step': 37603, 'preemption_count': 0}), (39953, {'train/accuracy': 0.6511838436126709, 'train/loss': 1.662556767463684, 'train/bleu': 31.470296217893207, 'validation/accuracy': 0.665162205696106, 'validation/loss': 1.560990333557129, 'validation/bleu': 28.531168082604847, 'validation/num_examples': 3000, 'test/accuracy': 0.6754401326179504, 'test/loss': 1.4862949848175049, 'test/bleu': 28.240845223490155, 'test/num_examples': 3003, 'score': 14311.73891043663, 'total_duration': 24081.216133594513, 'accumulated_submission_time': 14311.73891043663, 'accumulated_eval_time': 9767.721544027328, 'accumulated_logging_time': 0.4817836284637451, 'global_step': 39953, 'preemption_count': 0}), (42301, {'train/accuracy': 0.6459149718284607, 'train/loss': 1.6967586278915405, 'train/bleu': 31.73541057923894, 'validation/accuracy': 0.6646910905838013, 'validation/loss': 1.5630953311920166, 'validation/bleu': 28.590118849023177, 'validation/num_examples': 3000, 'test/accuracy': 0.6777874827384949, 'test/loss': 1.4830689430236816, 'test/bleu': 28.08367316038388, 'test/num_examples': 3003, 'score': 15151.733073234558, 'total_duration': 25434.131522655487, 'accumulated_submission_time': 15151.733073234558, 'accumulated_eval_time': 10280.534869909286, 'accumulated_logging_time': 0.5133066177368164, 'global_step': 42301, 'preemption_count': 0}), (44650, {'train/accuracy': 0.6544402837753296, 'train/loss': 1.6286474466323853, 'train/bleu': 32.2549238963339, 'validation/accuracy': 0.666154146194458, 'validation/loss': 1.5522340536117554, 'validation/bleu': 28.460654397908776, 'validation/num_examples': 3000, 'test/accuracy': 0.6779733896255493, 'test/loss': 1.475687861442566, 'test/bleu': 27.80825715299147, 'test/num_examples': 3003, 'score': 15991.678433418274, 'total_duration': 26793.424834012985, 'accumulated_submission_time': 15991.678433418274, 'accumulated_eval_time': 10799.775356054306, 'accumulated_logging_time': 0.5443167686462402, 'global_step': 44650, 'preemption_count': 0}), (46999, {'train/accuracy': 0.6529893279075623, 'train/loss': 1.6469743251800537, 'train/bleu': 31.74572989276444, 'validation/accuracy': 0.6668609380722046, 'validation/loss': 1.5455957651138306, 'validation/bleu': 28.479802138785814, 'validation/num_examples': 3000, 'test/accuracy': 0.6801813244819641, 'test/loss': 1.458451747894287, 'test/bleu': 28.38958892681242, 'test/num_examples': 3003, 'score': 16831.602340459824, 'total_duration': 28266.15973854065, 'accumulated_submission_time': 16831.602340459824, 'accumulated_eval_time': 11432.476187705994, 'accumulated_logging_time': 0.577672004699707, 'global_step': 46999, 'preemption_count': 0}), (49349, {'train/accuracy': 0.64932781457901, 'train/loss': 1.6753994226455688, 'train/bleu': 31.92990226128372, 'validation/accuracy': 0.6689067482948303, 'validation/loss': 1.5383224487304688, 'validation/bleu': 28.933472139073533, 'validation/num_examples': 3000, 'test/accuracy': 0.6800883412361145, 'test/loss': 1.4577808380126953, 'test/bleu': 28.21943189274275, 'test/num_examples': 3003, 'score': 17671.68727684021, 'total_duration': 29609.51440000534, 'accumulated_submission_time': 17671.68727684021, 'accumulated_eval_time': 11935.63941526413, 'accumulated_logging_time': 0.6097137928009033, 'global_step': 49349, 'preemption_count': 0}), (51699, {'train/accuracy': 0.6542662978172302, 'train/loss': 1.6272282600402832, 'train/bleu': 32.459203013070265, 'validation/accuracy': 0.6693159341812134, 'validation/loss': 1.5343632698059082, 'validation/bleu': 28.620420565903288, 'validation/num_examples': 3000, 'test/accuracy': 0.6813085079193115, 'test/loss': 1.4497711658477783, 'test/bleu': 28.43325483477639, 'test/num_examples': 3003, 'score': 18511.66797375679, 'total_duration': 31134.161219596863, 'accumulated_submission_time': 18511.66797375679, 'accumulated_eval_time': 12620.1957821846, 'accumulated_logging_time': 0.643932580947876, 'global_step': 51699, 'preemption_count': 0}), (54048, {'train/accuracy': 0.6529883146286011, 'train/loss': 1.654189944267273, 'train/bleu': 32.510313488174106, 'validation/accuracy': 0.6688695549964905, 'validation/loss': 1.5301557779312134, 'validation/bleu': 28.57319480788474, 'validation/num_examples': 3000, 'test/accuracy': 0.68177330493927, 'test/loss': 1.448590636253357, 'test/bleu': 28.328816470806622, 'test/num_examples': 3003, 'score': 19351.668765306473, 'total_duration': 32636.85775065422, 'accumulated_submission_time': 19351.668765306473, 'accumulated_eval_time': 13282.779415607452, 'accumulated_logging_time': 0.6774072647094727, 'global_step': 54048, 'preemption_count': 0}), (56397, {'train/accuracy': 0.68346107006073, 'train/loss': 1.446035623550415, 'train/bleu': 34.018130430164184, 'validation/accuracy': 0.6704070568084717, 'validation/loss': 1.5175199508666992, 'validation/bleu': 29.086346659127816, 'validation/num_examples': 3000, 'test/accuracy': 0.682877242565155, 'test/loss': 1.4382997751235962, 'test/bleu': 28.169233789147054, 'test/num_examples': 3003, 'score': 20191.756650686264, 'total_duration': 34078.80337572098, 'accumulated_submission_time': 20191.756650686264, 'accumulated_eval_time': 13884.517220497131, 'accumulated_logging_time': 0.7161824703216553, 'global_step': 56397, 'preemption_count': 0}), (58747, {'train/accuracy': 0.6575668454170227, 'train/loss': 1.6168078184127808, 'train/bleu': 32.5052435486537, 'validation/accuracy': 0.6729984879493713, 'validation/loss': 1.5107536315917969, 'validation/bleu': 29.125088035511126, 'validation/num_examples': 3000, 'test/accuracy': 0.6855267286300659, 'test/loss': 1.4299596548080444, 'test/bleu': 28.621032891839445, 'test/num_examples': 3003, 'score': 21031.882925748825, 'total_duration': 35508.15443897247, 'accumulated_submission_time': 21031.882925748825, 'accumulated_eval_time': 14473.632624387741, 'accumulated_logging_time': 0.7506313323974609, 'global_step': 58747, 'preemption_count': 0}), (61096, {'train/accuracy': 0.6561620235443115, 'train/loss': 1.6366279125213623, 'train/bleu': 32.63350373980873, 'validation/accuracy': 0.6729736924171448, 'validation/loss': 1.512277603149414, 'validation/bleu': 29.140785546387807, 'validation/num_examples': 3000, 'test/accuracy': 0.6879902482032776, 'test/loss': 1.4211199283599854, 'test/bleu': 29.141972118259318, 'test/num_examples': 3003, 'score': 21871.777238607407, 'total_duration': 36873.4613969326, 'accumulated_submission_time': 21871.777238607407, 'accumulated_eval_time': 14998.93830871582, 'accumulated_logging_time': 0.7828867435455322, 'global_step': 61096, 'preemption_count': 0}), (63444, {'train/accuracy': 0.6674286723136902, 'train/loss': 1.547412633895874, 'train/bleu': 33.49294815381641, 'validation/accuracy': 0.6748335361480713, 'validation/loss': 1.4961423873901367, 'validation/bleu': 29.477780759940647, 'validation/num_examples': 3000, 'test/accuracy': 0.6889896392822266, 'test/loss': 1.410127878189087, 'test/bleu': 28.9947816834239, 'test/num_examples': 3003, 'score': 22711.711676359177, 'total_duration': 38273.14426493645, 'accumulated_submission_time': 22711.711676359177, 'accumulated_eval_time': 15558.572283506393, 'accumulated_logging_time': 0.8171112537384033, 'global_step': 63444, 'preemption_count': 0}), (65794, {'train/accuracy': 0.6565250754356384, 'train/loss': 1.6203813552856445, 'train/bleu': 32.66615467451501, 'validation/accuracy': 0.676073431968689, 'validation/loss': 1.4927388429641724, 'validation/bleu': 29.329756231686886, 'validation/num_examples': 3000, 'test/accuracy': 0.6903724670410156, 'test/loss': 1.4074007272720337, 'test/bleu': 29.564089044554855, 'test/num_examples': 3003, 'score': 23551.849817037582, 'total_duration': 39674.71835613251, 'accumulated_submission_time': 23551.849817037582, 'accumulated_eval_time': 16119.899282455444, 'accumulated_logging_time': 0.8502511978149414, 'global_step': 65794, 'preemption_count': 0}), (68144, {'train/accuracy': 0.6547269821166992, 'train/loss': 1.634839653968811, 'train/bleu': 32.47219353442573, 'validation/accuracy': 0.677561342716217, 'validation/loss': 1.4846450090408325, 'validation/bleu': 29.4492775140218, 'validation/num_examples': 3000, 'test/accuracy': 0.6897798180580139, 'test/loss': 1.4021321535110474, 'test/bleu': 29.068497099009715, 'test/num_examples': 3003, 'score': 24391.992929935455, 'total_duration': 41078.09699392319, 'accumulated_submission_time': 24391.992929935455, 'accumulated_eval_time': 16683.019134521484, 'accumulated_logging_time': 0.8892166614532471, 'global_step': 68144, 'preemption_count': 0}), (70494, {'train/accuracy': 0.6648585796356201, 'train/loss': 1.5671465396881104, 'train/bleu': 32.83949660411032, 'validation/accuracy': 0.6763090491294861, 'validation/loss': 1.4787070751190186, 'validation/bleu': 29.078669687795948, 'validation/num_examples': 3000, 'test/accuracy': 0.6899541020393372, 'test/loss': 1.3922793865203857, 'test/bleu': 29.032294326235682, 'test/num_examples': 3003, 'score': 25232.03781723976, 'total_duration': 42432.101380348206, 'accumulated_submission_time': 25232.03781723976, 'accumulated_eval_time': 17196.86828827858, 'accumulated_logging_time': 0.9239933490753174, 'global_step': 70494, 'preemption_count': 0}), (72843, {'train/accuracy': 0.6621139049530029, 'train/loss': 1.5815826654434204, 'train/bleu': 33.390701136287404, 'validation/accuracy': 0.6767553687095642, 'validation/loss': 1.473919153213501, 'validation/bleu': 29.352687747763873, 'validation/num_examples': 3000, 'test/accuracy': 0.6910812854766846, 'test/loss': 1.3824280500411987, 'test/bleu': 29.189779204845138, 'test/num_examples': 3003, 'score': 26072.20747256279, 'total_duration': 43788.344264507294, 'accumulated_submission_time': 26072.20747256279, 'accumulated_eval_time': 17712.821888685226, 'accumulated_logging_time': 0.9654061794281006, 'global_step': 72843, 'preemption_count': 0}), (75193, {'train/accuracy': 0.6858174800872803, 'train/loss': 1.426164984703064, 'train/bleu': 34.366781063424106, 'validation/accuracy': 0.6785904765129089, 'validation/loss': 1.4634785652160645, 'validation/bleu': 29.514415997343658, 'validation/num_examples': 3000, 'test/accuracy': 0.693544864654541, 'test/loss': 1.378287672996521, 'test/bleu': 29.07100048590121, 'test/num_examples': 3003, 'score': 26912.381454229355, 'total_duration': 45217.25480270386, 'accumulated_submission_time': 26912.381454229355, 'accumulated_eval_time': 18301.444242954254, 'accumulated_logging_time': 1.0022211074829102, 'global_step': 75193, 'preemption_count': 0}), (77543, {'train/accuracy': 0.6669697165489197, 'train/loss': 1.5403841733932495, 'train/bleu': 33.21220426533364, 'validation/accuracy': 0.6799419522285461, 'validation/loss': 1.4549518823623657, 'validation/bleu': 29.43968950403044, 'validation/num_examples': 3000, 'test/accuracy': 0.6951484680175781, 'test/loss': 1.3676152229309082, 'test/bleu': 29.485973265345606, 'test/num_examples': 3003, 'score': 27752.452763557434, 'total_duration': 46761.59459590912, 'accumulated_submission_time': 27752.452763557434, 'accumulated_eval_time': 19005.60043978691, 'accumulated_logging_time': 1.0388991832733154, 'global_step': 77543, 'preemption_count': 0}), (79893, {'train/accuracy': 0.6644576787948608, 'train/loss': 1.573724389076233, 'train/bleu': 32.65712602644397, 'validation/accuracy': 0.6827317476272583, 'validation/loss': 1.4516927003860474, 'validation/bleu': 29.785093513403424, 'validation/num_examples': 3000, 'test/accuracy': 0.6960781216621399, 'test/loss': 1.3559688329696655, 'test/bleu': 29.612354085956323, 'test/num_examples': 3003, 'score': 28592.432891607285, 'total_duration': 48130.004256248474, 'accumulated_submission_time': 28592.432891607285, 'accumulated_eval_time': 19533.910708904266, 'accumulated_logging_time': 1.0820260047912598, 'global_step': 79893, 'preemption_count': 0}), (82243, {'train/accuracy': 0.6750671863555908, 'train/loss': 1.4929388761520386, 'train/bleu': 33.87395627976873, 'validation/accuracy': 0.6826077699661255, 'validation/loss': 1.4436779022216797, 'validation/bleu': 29.849369345056633, 'validation/num_examples': 3000, 'test/accuracy': 0.6966823935508728, 'test/loss': 1.3540998697280884, 'test/bleu': 29.61672950952151, 'test/num_examples': 3003, 'score': 29432.561252593994, 'total_duration': 49679.17920422554, 'accumulated_submission_time': 29432.561252593994, 'accumulated_eval_time': 20242.84255671501, 'accumulated_logging_time': 1.1201717853546143, 'global_step': 82243, 'preemption_count': 0}), (84593, {'train/accuracy': 0.6679847240447998, 'train/loss': 1.5398011207580566, 'train/bleu': 33.47505030763876, 'validation/accuracy': 0.6833269000053406, 'validation/loss': 1.4395651817321777, 'validation/bleu': 29.752018414615385, 'validation/num_examples': 3000, 'test/accuracy': 0.6992040276527405, 'test/loss': 1.3429124355316162, 'test/bleu': 29.948791249609343, 'test/num_examples': 3003, 'score': 30272.666800498962, 'total_duration': 51220.04585957527, 'accumulated_submission_time': 30272.666800498962, 'accumulated_eval_time': 20943.490812301636, 'accumulated_logging_time': 1.1579155921936035, 'global_step': 84593, 'preemption_count': 0}), (86944, {'train/accuracy': 0.66844642162323, 'train/loss': 1.5463353395462036, 'train/bleu': 33.3239685944154, 'validation/accuracy': 0.6863275170326233, 'validation/loss': 1.4329047203063965, 'validation/bleu': 30.17483410773, 'validation/num_examples': 3000, 'test/accuracy': 0.7015630006790161, 'test/loss': 1.336978793144226, 'test/bleu': 30.01117652283077, 'test/num_examples': 3003, 'score': 31112.70547223091, 'total_duration': 52629.39412069321, 'accumulated_submission_time': 31112.70547223091, 'accumulated_eval_time': 21512.685092926025, 'accumulated_logging_time': 1.1974620819091797, 'global_step': 86944, 'preemption_count': 0}), (89293, {'train/accuracy': 0.6765536069869995, 'train/loss': 1.4846397638320923, 'train/bleu': 33.80841067188661, 'validation/accuracy': 0.6877037882804871, 'validation/loss': 1.4231077432632446, 'validation/bleu': 30.144694232267003, 'validation/num_examples': 3000, 'test/accuracy': 0.7022950649261475, 'test/loss': 1.3293834924697876, 'test/bleu': 30.109991566006315, 'test/num_examples': 3003, 'score': 31952.67540025711, 'total_duration': 54071.60535812378, 'accumulated_submission_time': 31952.67540025711, 'accumulated_eval_time': 22114.81109213829, 'accumulated_logging_time': 1.2344374656677246, 'global_step': 89293, 'preemption_count': 0}), (91642, {'train/accuracy': 0.6729382872581482, 'train/loss': 1.5041873455047607, 'train/bleu': 33.947059749229915, 'validation/accuracy': 0.6867242455482483, 'validation/loss': 1.4200741052627563, 'validation/bleu': 30.009697354141604, 'validation/num_examples': 3000, 'test/accuracy': 0.7022950649261475, 'test/loss': 1.3214539289474487, 'test/bleu': 30.012051023290432, 'test/num_examples': 3003, 'score': 32792.68555688858, 'total_duration': 55434.03802442551, 'accumulated_submission_time': 32792.68555688858, 'accumulated_eval_time': 22637.1190366745, 'accumulated_logging_time': 1.2718265056610107, 'global_step': 91642, 'preemption_count': 0}), (93991, {'train/accuracy': 0.6925990581512451, 'train/loss': 1.3899562358856201, 'train/bleu': 34.75326842015933, 'validation/accuracy': 0.6878277659416199, 'validation/loss': 1.4126427173614502, 'validation/bleu': 30.225550510148068, 'validation/num_examples': 3000, 'test/accuracy': 0.7032014727592468, 'test/loss': 1.3106350898742676, 'test/bleu': 29.828300853864768, 'test/num_examples': 3003, 'score': 33632.6453063488, 'total_duration': 56803.9655148983, 'accumulated_submission_time': 33632.6453063488, 'accumulated_eval_time': 23166.96990251541, 'accumulated_logging_time': 1.3109593391418457, 'global_step': 93991, 'preemption_count': 0}), (96341, {'train/accuracy': 0.6803255081176758, 'train/loss': 1.4658100605010986, 'train/bleu': 33.819762322880095, 'validation/accuracy': 0.6885097622871399, 'validation/loss': 1.407971978187561, 'validation/bleu': 29.990235745955843, 'validation/num_examples': 3000, 'test/accuracy': 0.7055023312568665, 'test/loss': 1.3078140020370483, 'test/bleu': 30.228751807442475, 'test/num_examples': 3003, 'score': 34472.82131195068, 'total_duration': 58340.36324286461, 'accumulated_submission_time': 34472.82131195068, 'accumulated_eval_time': 23863.072603464127, 'accumulated_logging_time': 1.3514149188995361, 'global_step': 96341, 'preemption_count': 0}), (98690, {'train/accuracy': 0.678549587726593, 'train/loss': 1.4740723371505737, 'train/bleu': 34.745071742947765, 'validation/accuracy': 0.689811646938324, 'validation/loss': 1.4059818983078003, 'validation/bleu': 30.306885565787923, 'validation/num_examples': 3000, 'test/accuracy': 0.7070013284683228, 'test/loss': 1.3015625476837158, 'test/bleu': 30.51100426873089, 'test/num_examples': 3003, 'score': 35312.71530985832, 'total_duration': 59696.382061719894, 'accumulated_submission_time': 35312.71530985832, 'accumulated_eval_time': 24379.07930803299, 'accumulated_logging_time': 1.391371726989746, 'global_step': 98690, 'preemption_count': 0}), (101040, {'train/accuracy': 0.6873783469200134, 'train/loss': 1.4144078493118286, 'train/bleu': 34.822233159168285, 'validation/accuracy': 0.689452052116394, 'validation/loss': 1.3942186832427979, 'validation/bleu': 30.43498417126523, 'validation/num_examples': 3000, 'test/accuracy': 0.7060601115226746, 'test/loss': 1.2959932088851929, 'test/bleu': 30.27591467570254, 'test/num_examples': 3003, 'score': 36152.8135163784, 'total_duration': 61109.87213683128, 'accumulated_submission_time': 36152.8135163784, 'accumulated_eval_time': 24952.356118440628, 'accumulated_logging_time': 1.4311096668243408, 'global_step': 101040, 'preemption_count': 0}), (103390, {'train/accuracy': 0.6787706017494202, 'train/loss': 1.472381591796875, 'train/bleu': 34.936464710336374, 'validation/accuracy': 0.6904067993164062, 'validation/loss': 1.394535779953003, 'validation/bleu': 30.4547279180483, 'validation/num_examples': 3000, 'test/accuracy': 0.7068851590156555, 'test/loss': 1.2904762029647827, 'test/bleu': 30.54459233862504, 'test/num_examples': 3003, 'score': 36992.80270028114, 'total_duration': 62544.18134522438, 'accumulated_submission_time': 36992.80270028114, 'accumulated_eval_time': 25546.56099653244, 'accumulated_logging_time': 1.4709479808807373, 'global_step': 103390, 'preemption_count': 0}), (105740, {'train/accuracy': 0.6793479919433594, 'train/loss': 1.474918246269226, 'train/bleu': 34.524389490258514, 'validation/accuracy': 0.6916218996047974, 'validation/loss': 1.3871182203292847, 'validation/bleu': 30.763161599157005, 'validation/num_examples': 3000, 'test/accuracy': 0.7088722586631775, 'test/loss': 1.2860301733016968, 'test/bleu': 30.587540047698738, 'test/num_examples': 3003, 'score': 37832.90144467354, 'total_duration': 63912.06481075287, 'accumulated_submission_time': 37832.90144467354, 'accumulated_eval_time': 26074.223981142044, 'accumulated_logging_time': 1.5178804397583008, 'global_step': 105740, 'preemption_count': 0}), (108090, {'train/accuracy': 0.6895817518234253, 'train/loss': 1.4022433757781982, 'train/bleu': 34.956725807169725, 'validation/accuracy': 0.6924774646759033, 'validation/loss': 1.3789490461349487, 'validation/bleu': 30.507768815081356, 'validation/num_examples': 3000, 'test/accuracy': 0.7091395258903503, 'test/loss': 1.276688575744629, 'test/bleu': 30.70939975137136, 'test/num_examples': 3003, 'score': 38672.90054440498, 'total_duration': 65321.97705411911, 'accumulated_submission_time': 38672.90054440498, 'accumulated_eval_time': 26644.022025108337, 'accumulated_logging_time': 1.5571041107177734, 'global_step': 108090, 'preemption_count': 0}), (110440, {'train/accuracy': 0.6895023584365845, 'train/loss': 1.4150652885437012, 'train/bleu': 35.16845176199788, 'validation/accuracy': 0.6943125128746033, 'validation/loss': 1.378193974494934, 'validation/bleu': 30.72178388616816, 'validation/num_examples': 3000, 'test/accuracy': 0.7114055156707764, 'test/loss': 1.2721518278121948, 'test/bleu': 30.801454308740052, 'test/num_examples': 3003, 'score': 39513.07421565056, 'total_duration': 66752.35574197769, 'accumulated_submission_time': 39513.07421565056, 'accumulated_eval_time': 27234.110072135925, 'accumulated_logging_time': 1.5973656177520752, 'global_step': 110440, 'preemption_count': 0}), (112791, {'train/accuracy': 0.6971345543861389, 'train/loss': 1.3650158643722534, 'train/bleu': 35.54626248357229, 'validation/accuracy': 0.6938785314559937, 'validation/loss': 1.3746583461761475, 'validation/bleu': 30.809153601006447, 'validation/num_examples': 3000, 'test/accuracy': 0.7100808024406433, 'test/loss': 1.2701268196105957, 'test/bleu': 30.55024025701441, 'test/num_examples': 3003, 'score': 40353.12133765221, 'total_duration': 68157.70009493828, 'accumulated_submission_time': 40353.12133765221, 'accumulated_eval_time': 27799.289972782135, 'accumulated_logging_time': 1.6389942169189453, 'global_step': 112791, 'preemption_count': 0}), (115141, {'train/accuracy': 0.6917914748191833, 'train/loss': 1.399938702583313, 'train/bleu': 35.197873516392164, 'validation/accuracy': 0.6941389441490173, 'validation/loss': 1.3688514232635498, 'validation/bleu': 30.793833684223024, 'validation/num_examples': 3000, 'test/accuracy': 0.7127999663352966, 'test/loss': 1.2630198001861572, 'test/bleu': 30.843691765002152, 'test/num_examples': 3003, 'score': 41193.01627731323, 'total_duration': 69688.62987804413, 'accumulated_submission_time': 41193.01627731323, 'accumulated_eval_time': 28490.20787382126, 'accumulated_logging_time': 1.6810777187347412, 'global_step': 115141, 'preemption_count': 0}), (117491, {'train/accuracy': 0.6893499493598938, 'train/loss': 1.4066481590270996, 'train/bleu': 35.249029116488906, 'validation/accuracy': 0.6947464942932129, 'validation/loss': 1.3684260845184326, 'validation/bleu': 30.907103979729687, 'validation/num_examples': 3000, 'test/accuracy': 0.7123932838439941, 'test/loss': 1.262866735458374, 'test/bleu': 30.727742222734168, 'test/num_examples': 3003, 'score': 42032.95151424408, 'total_duration': 71083.43123865128, 'accumulated_submission_time': 42032.95151424408, 'accumulated_eval_time': 29044.949915885925, 'accumulated_logging_time': 1.7298576831817627, 'global_step': 117491, 'preemption_count': 0}), (119841, {'train/accuracy': 0.6982017755508423, 'train/loss': 1.360303521156311, 'train/bleu': 35.42124147020956, 'validation/accuracy': 0.696234405040741, 'validation/loss': 1.3644174337387085, 'validation/bleu': 30.810002552655714, 'validation/num_examples': 3000, 'test/accuracy': 0.7137296199798584, 'test/loss': 1.258353590965271, 'test/bleu': 30.89031023851516, 'test/num_examples': 3003, 'score': 42873.06673336029, 'total_duration': 72488.55532360077, 'accumulated_submission_time': 42873.06673336029, 'accumulated_eval_time': 29609.83997654915, 'accumulated_logging_time': 1.772599458694458, 'global_step': 119841, 'preemption_count': 0}), (122190, {'train/accuracy': 0.6944260001182556, 'train/loss': 1.3774758577346802, 'train/bleu': 35.73075424767851, 'validation/accuracy': 0.6958252191543579, 'validation/loss': 1.363082766532898, 'validation/bleu': 30.871959597197357, 'validation/num_examples': 3000, 'test/accuracy': 0.7138806581497192, 'test/loss': 1.2562905550003052, 'test/bleu': 31.077669581654437, 'test/num_examples': 3003, 'score': 43712.94831061363, 'total_duration': 73994.31863379478, 'accumulated_submission_time': 43712.94831061363, 'accumulated_eval_time': 30275.602272987366, 'accumulated_logging_time': 1.814788818359375, 'global_step': 122190, 'preemption_count': 0}), (124540, {'train/accuracy': 0.6927077174186707, 'train/loss': 1.3844177722930908, 'train/bleu': 35.60168167605079, 'validation/accuracy': 0.6960111856460571, 'validation/loss': 1.3612251281738281, 'validation/bleu': 31.036330806615823, 'validation/num_examples': 3000, 'test/accuracy': 0.7133693695068359, 'test/loss': 1.2556757926940918, 'test/bleu': 30.946606236975953, 'test/num_examples': 3003, 'score': 44553.167630434036, 'total_duration': 75423.9879014492, 'accumulated_submission_time': 44553.167630434036, 'accumulated_eval_time': 30864.93314909935, 'accumulated_logging_time': 1.8580679893493652, 'global_step': 124540, 'preemption_count': 0}), (126890, {'train/accuracy': 0.6990454196929932, 'train/loss': 1.3546652793884277, 'train/bleu': 35.55317402084076, 'validation/accuracy': 0.6965071558952332, 'validation/loss': 1.360289216041565, 'validation/bleu': 30.923319946605933, 'validation/num_examples': 3000, 'test/accuracy': 0.7141130566596985, 'test/loss': 1.2537082433700562, 'test/bleu': 30.891123087192003, 'test/num_examples': 3003, 'score': 45393.23704409599, 'total_duration': 76871.89690589905, 'accumulated_submission_time': 45393.23704409599, 'accumulated_eval_time': 31472.653499126434, 'accumulated_logging_time': 1.9009826183319092, 'global_step': 126890, 'preemption_count': 0}), (129239, {'train/accuracy': 0.6980040073394775, 'train/loss': 1.359439492225647, 'train/bleu': 35.294106469705945, 'validation/accuracy': 0.6968419551849365, 'validation/loss': 1.3597447872161865, 'validation/bleu': 30.871485970728653, 'validation/num_examples': 3000, 'test/accuracy': 0.714043378829956, 'test/loss': 1.252797245979309, 'test/bleu': 30.942558677133007, 'test/num_examples': 3003, 'score': 46233.19505262375, 'total_duration': 78352.19459056854, 'accumulated_submission_time': 46233.19505262375, 'accumulated_eval_time': 32112.87235379219, 'accumulated_logging_time': 1.943530559539795, 'global_step': 129239, 'preemption_count': 0}), (131588, {'train/accuracy': 0.6947080492973328, 'train/loss': 1.3761208057403564, 'train/bleu': 35.40285800418826, 'validation/accuracy': 0.6967551708221436, 'validation/loss': 1.3596832752227783, 'validation/bleu': 30.906944090047656, 'validation/num_examples': 3000, 'test/accuracy': 0.714043378829956, 'test/loss': 1.252558946609497, 'test/bleu': 30.929931785958644, 'test/num_examples': 3003, 'score': 47073.37712454796, 'total_duration': 79778.21843957901, 'accumulated_submission_time': 47073.37712454796, 'accumulated_eval_time': 32698.586216688156, 'accumulated_logging_time': 1.9933269023895264, 'global_step': 131588, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6980851888656616, 'train/loss': 1.3578859567642212, 'train/bleu': 35.45882877194227, 'validation/accuracy': 0.6968915462493896, 'validation/loss': 1.359694004058838, 'validation/bleu': 30.90651925945721, 'validation/num_examples': 3000, 'test/accuracy': 0.7140317559242249, 'test/loss': 1.2525193691253662, 'test/bleu': 30.921156057591386, 'test/num_examples': 3003, 'score': 47697.16169524193, 'total_duration': 80995.71348690987, 'accumulated_submission_time': 47697.16169524193, 'accumulated_eval_time': 33292.195563316345, 'accumulated_logging_time': 2.0389645099639893, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0210 22:35:22.875685 139785736898368 submission_runner.py:586] Timing: 47697.16169524193
I0210 22:35:22.875773 139785736898368 submission_runner.py:588] Total number of evals: 58
I0210 22:35:22.875822 139785736898368 submission_runner.py:589] ====================
I0210 22:35:22.876734 139785736898368 submission_runner.py:673] Final wmt score: 46539.62941074371
