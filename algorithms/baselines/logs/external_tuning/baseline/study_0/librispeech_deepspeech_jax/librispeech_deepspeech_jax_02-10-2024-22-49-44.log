python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=2622380006 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_02-10-2024-22-49-44.log
I0210 22:50:05.888770 140218947737408 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax.
I0210 22:50:06.921764 140218947737408 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0210 22:50:06.922608 140218947737408 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0210 22:50:06.922776 140218947737408 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0210 22:50:06.923860 140218947737408 submission_runner.py:542] Using RNG seed 2622380006
I0210 22:50:08.111724 140218947737408 submission_runner.py:551] --- Tuning run 1/5 ---
I0210 22:50:08.111924 140218947737408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1.
I0210 22:50:08.112270 140218947737408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1/hparams.json.
I0210 22:50:08.297938 140218947737408 submission_runner.py:206] Initializing dataset.
I0210 22:50:08.298161 140218947737408 submission_runner.py:213] Initializing model.
I0210 22:50:10.825613 140218947737408 submission_runner.py:255] Initializing optimizer.
I0210 22:50:11.539490 140218947737408 submission_runner.py:262] Initializing metrics bundle.
I0210 22:50:11.539721 140218947737408 submission_runner.py:280] Initializing checkpoint and logger.
I0210 22:50:11.540805 140218947737408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0210 22:50:11.540937 140218947737408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0210 22:50:11.541136 140218947737408 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 22:50:11.541197 140218947737408 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 22:50:11.836056 140218947737408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 22:50:12.104752 140218947737408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0210 22:50:12.117966 140218947737408 submission_runner.py:314] Starting training loop.
I0210 22:50:12.416846 140218947737408 input_pipeline.py:20] Loading split = train-clean-100
I0210 22:50:12.457125 140218947737408 input_pipeline.py:20] Loading split = train-clean-360
I0210 22:50:12.593399 140218947737408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0210 22:50:55.935839 140054622107392 logging_writer.py:48] [0] global_step=0, grad_norm=21.39870834350586, loss=32.6155891418457
I0210 22:50:55.969738 140218947737408 spec.py:321] Evaluating on the training split.
I0210 22:50:56.226074 140218947737408 input_pipeline.py:20] Loading split = train-clean-100
I0210 22:50:56.261235 140218947737408 input_pipeline.py:20] Loading split = train-clean-360
I0210 22:50:56.639688 140218947737408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0210 22:52:31.001500 140218947737408 spec.py:333] Evaluating on the validation split.
I0210 22:52:31.195957 140218947737408 input_pipeline.py:20] Loading split = dev-clean
I0210 22:52:31.201591 140218947737408 input_pipeline.py:20] Loading split = dev-other
I0210 22:53:40.690857 140218947737408 spec.py:349] Evaluating on the test split.
I0210 22:53:40.892370 140218947737408 input_pipeline.py:20] Loading split = test-clean
I0210 22:54:20.867328 140218947737408 submission_runner.py:408] Time since start: 248.75s, 	Step: 1, 	{'train/ctc_loss': Array(29.405813, dtype=float32), 'train/wer': 2.4046810903108926, 'validation/ctc_loss': Array(28.149015, dtype=float32), 'validation/wer': 2.1852631375691516, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.255945, dtype=float32), 'test/wer': 2.343875043162107, 'test/num_examples': 2472, 'score': 43.85169339179993, 'total_duration': 248.746901512146, 'accumulated_submission_time': 43.85169339179993, 'accumulated_eval_time': 204.89513993263245, 'accumulated_logging_time': 0}
I0210 22:54:20.894047 140049437939456 logging_writer.py:48] [1] accumulated_eval_time=204.895140, accumulated_logging_time=0, accumulated_submission_time=43.851693, global_step=1, preemption_count=0, score=43.851693, test/ctc_loss=28.255945205688477, test/num_examples=2472, test/wer=2.343875, total_duration=248.746902, train/ctc_loss=29.405813217163086, train/wer=2.404681, validation/ctc_loss=28.149015426635742, validation/num_examples=5348, validation/wer=2.185263
I0210 22:55:47.149386 140061680506624 logging_writer.py:48] [100] global_step=100, grad_norm=5.108521461486816, loss=9.070860862731934
I0210 22:57:03.576011 140061688899328 logging_writer.py:48] [200] global_step=200, grad_norm=1.7772639989852905, loss=6.5102949142456055
I0210 22:58:20.330102 140061680506624 logging_writer.py:48] [300] global_step=300, grad_norm=0.5284517407417297, loss=5.9052042961120605
I0210 22:59:36.992542 140061688899328 logging_writer.py:48] [400] global_step=400, grad_norm=0.33134540915489197, loss=5.8404035568237305
I0210 23:00:59.331332 140061680506624 logging_writer.py:48] [500] global_step=500, grad_norm=0.3701581358909607, loss=5.83010196685791
I0210 23:02:26.313069 140061688899328 logging_writer.py:48] [600] global_step=600, grad_norm=0.6390092968940735, loss=5.813042640686035
I0210 23:03:51.549137 140061680506624 logging_writer.py:48] [700] global_step=700, grad_norm=0.38070085644721985, loss=5.742334842681885
I0210 23:05:15.883172 140061688899328 logging_writer.py:48] [800] global_step=800, grad_norm=0.5992259383201599, loss=5.628917694091797
I0210 23:06:41.795273 140061680506624 logging_writer.py:48] [900] global_step=900, grad_norm=0.49952641129493713, loss=5.5087456703186035
I0210 23:08:11.615458 140061688899328 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5268377661705017, loss=5.389214038848877
I0210 23:09:35.735752 140054513051392 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7466924786567688, loss=5.221552848815918
I0210 23:10:53.027272 140054408951552 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.876548707485199, loss=4.796853065490723
I0210 23:12:09.638616 140054513051392 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.055216908454895, loss=4.3396077156066895
I0210 23:13:30.741209 140054408951552 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2957212924957275, loss=4.015049934387207
I0210 23:14:53.291705 140054513051392 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.85483980178833, loss=3.718287706375122
I0210 23:16:20.304917 140054408951552 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.1208066940307617, loss=3.517321825027466
I0210 23:17:50.397716 140054513051392 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8177955150604248, loss=3.302433967590332
I0210 23:18:21.085810 140218947737408 spec.py:321] Evaluating on the training split.
I0210 23:18:58.806300 140218947737408 spec.py:333] Evaluating on the validation split.
I0210 23:19:46.056662 140218947737408 spec.py:349] Evaluating on the test split.
I0210 23:20:11.585698 140218947737408 submission_runner.py:408] Time since start: 1799.47s, 	Step: 1737, 	{'train/ctc_loss': Array(6.6251674, dtype=float32), 'train/wer': 0.9446252394389405, 'validation/ctc_loss': Array(6.4828987, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.369111, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1483.9563069343567, 'total_duration': 1799.4650785923004, 'accumulated_submission_time': 1483.9563069343567, 'accumulated_eval_time': 315.39243960380554, 'accumulated_logging_time': 0.04180431365966797}
I0210 23:20:11.613415 140062386222848 logging_writer.py:48] [1737] accumulated_eval_time=315.392440, accumulated_logging_time=0.041804, accumulated_submission_time=1483.956307, global_step=1737, preemption_count=0, score=1483.956307, test/ctc_loss=6.369111061096191, test/num_examples=2472, test/wer=0.899580, total_duration=1799.465079, train/ctc_loss=6.625167369842529, train/wer=0.944625, validation/ctc_loss=6.482898712158203, validation/num_examples=5348, validation/wer=0.896618
I0210 23:21:00.243128 140062377830144 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.4579784870147705, loss=3.148090124130249
I0210 23:22:16.097402 140062386222848 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.0120089054107666, loss=3.057124137878418
I0210 23:23:33.560280 140062377830144 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.452073812484741, loss=2.965277910232544
I0210 23:24:59.595995 140063041582848 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.6451144218444824, loss=2.858618974685669
I0210 23:26:17.537926 140063033190144 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.872898578643799, loss=2.751055955886841
I0210 23:27:35.821684 140063041582848 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.566946029663086, loss=2.7148725986480713
I0210 23:28:57.000715 140063033190144 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.4731783866882324, loss=2.6518383026123047
I0210 23:30:22.212936 140063041582848 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.9669971466064453, loss=2.6543612480163574
I0210 23:31:45.739190 140063033190144 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.65641713142395, loss=2.5630388259887695
I0210 23:33:12.506610 140063041582848 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.0697412490844727, loss=2.444042921066284
I0210 23:34:40.320713 140063033190144 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.5152692794799805, loss=2.4802026748657227
I0210 23:36:04.644851 140063041582848 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.791865587234497, loss=2.474022626876831
I0210 23:37:30.909435 140063033190144 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.7497365474700928, loss=2.3681859970092773
I0210 23:38:58.800356 140062386222848 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.807803630828857, loss=2.3284032344818115
I0210 23:40:13.664092 140062377830144 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.9053826332092285, loss=2.2233316898345947
I0210 23:41:29.402384 140062386222848 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.91341495513916, loss=2.305410861968994
I0210 23:42:46.818024 140062377830144 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.5811586380004883, loss=2.151973009109497
I0210 23:44:09.833322 140062386222848 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.179701805114746, loss=2.172199249267578
I0210 23:44:11.997063 140218947737408 spec.py:321] Evaluating on the training split.
I0210 23:44:58.709856 140218947737408 spec.py:333] Evaluating on the validation split.
I0210 23:45:47.583658 140218947737408 spec.py:349] Evaluating on the test split.
I0210 23:46:13.061633 140218947737408 submission_runner.py:408] Time since start: 3360.94s, 	Step: 3504, 	{'train/ctc_loss': Array(3.5062532, dtype=float32), 'train/wer': 0.7003595876728018, 'validation/ctc_loss': Array(3.887965, dtype=float32), 'validation/wer': 0.7383395927667339, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.4329967, dtype=float32), 'test/wer': 0.6812706924217496, 'test/num_examples': 2472, 'score': 2924.2540802955627, 'total_duration': 3360.9410836696625, 'accumulated_submission_time': 2924.2540802955627, 'accumulated_eval_time': 436.4544777870178, 'accumulated_logging_time': 0.08056855201721191}
I0210 23:46:13.087074 140061505578752 logging_writer.py:48] [3504] accumulated_eval_time=436.454478, accumulated_logging_time=0.080569, accumulated_submission_time=2924.254080, global_step=3504, preemption_count=0, score=2924.254080, test/ctc_loss=3.4329967498779297, test/num_examples=2472, test/wer=0.681271, total_duration=3360.941084, train/ctc_loss=3.506253242492676, train/wer=0.700360, validation/ctc_loss=3.887964963912964, validation/num_examples=5348, validation/wer=0.738340
I0210 23:47:26.344922 140061497186048 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.182952642440796, loss=2.154426097869873
I0210 23:48:41.386876 140061505578752 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.441622734069824, loss=2.1347575187683105
I0210 23:50:04.052641 140061497186048 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.5311472415924072, loss=2.1965298652648926
I0210 23:51:32.685882 140061505578752 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.907694101333618, loss=2.1776480674743652
I0210 23:52:59.682369 140061497186048 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.38476037979126, loss=2.0996594429016113
I0210 23:54:26.212347 140061505578752 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.4973483085632324, loss=2.1148483753204346
I0210 23:55:47.249771 140061505578752 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.187182903289795, loss=2.0517866611480713
I0210 23:57:04.124321 140061497186048 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.5326554775238037, loss=2.081021547317505
I0210 23:58:24.710472 140061505578752 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.543118476867676, loss=2.059230327606201
I0210 23:59:49.730021 140061497186048 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.423924446105957, loss=2.0807313919067383
I0211 00:01:14.228222 140061505578752 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.055083751678467, loss=2.066577434539795
I0211 00:02:44.227775 140061497186048 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.513718366622925, loss=2.0376110076904297
I0211 00:04:10.658476 140061505578752 logging_writer.py:48] [4800] global_step=4800, grad_norm=6.692656517028809, loss=2.026404619216919
I0211 00:05:37.146911 140061497186048 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.8163540363311768, loss=1.9780137538909912
I0211 00:07:04.184397 140061505578752 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.329700946807861, loss=1.9567620754241943
I0211 00:08:33.037392 140061497186048 logging_writer.py:48] [5100] global_step=5100, grad_norm=4.126415252685547, loss=1.9815438985824585
I0211 00:10:00.481629 140061505578752 logging_writer.py:48] [5200] global_step=5200, grad_norm=5.1036057472229, loss=1.9750175476074219
I0211 00:10:13.440128 140218947737408 spec.py:321] Evaluating on the training split.
I0211 00:11:16.347775 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 00:12:07.680045 140218947737408 spec.py:349] Evaluating on the test split.
I0211 00:12:34.464207 140218947737408 submission_runner.py:408] Time since start: 4942.34s, 	Step: 5218, 	{'train/ctc_loss': Array(0.7218365, dtype=float32), 'train/wer': 0.23713964925597356, 'validation/ctc_loss': Array(1.1679587, dtype=float32), 'validation/wer': 0.3178311787365921, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.794886, dtype=float32), 'test/wer': 0.24412487559157475, 'test/num_examples': 2472, 'score': 4364.524667263031, 'total_duration': 4942.343545675278, 'accumulated_submission_time': 4364.524667263031, 'accumulated_eval_time': 577.4759426116943, 'accumulated_logging_time': 0.11744523048400879}
I0211 00:12:34.489942 140061505578752 logging_writer.py:48] [5218] accumulated_eval_time=577.475943, accumulated_logging_time=0.117445, accumulated_submission_time=4364.524667, global_step=5218, preemption_count=0, score=4364.524667, test/ctc_loss=0.7948859930038452, test/num_examples=2472, test/wer=0.244125, total_duration=4942.343546, train/ctc_loss=0.721836507320404, train/wer=0.237140, validation/ctc_loss=1.1679587364196777, validation/num_examples=5348, validation/wer=0.317831
I0211 00:13:36.781664 140061497186048 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.715252876281738, loss=1.9312832355499268
I0211 00:14:51.928358 140061505578752 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.601005554199219, loss=1.905468225479126
I0211 00:16:07.414436 140061497186048 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.131726026535034, loss=1.9735376834869385
I0211 00:17:23.594799 140061505578752 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.436183929443359, loss=1.8797751665115356
I0211 00:18:49.808365 140061497186048 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.9060797691345215, loss=1.9372237920761108
I0211 00:20:17.943072 140061505578752 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.3846518993377686, loss=1.8795784711837769
I0211 00:21:47.799418 140061497186048 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.693007707595825, loss=1.871795415878296
I0211 00:23:17.494137 140061505578752 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.488388538360596, loss=1.8274651765823364
I0211 00:24:42.711956 140061497186048 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.2653231620788574, loss=1.893609642982483
I0211 00:26:13.118808 140063041582848 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.9022789001464844, loss=1.7421424388885498
I0211 00:27:29.838076 140063033190144 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.055802345275879, loss=1.841551423072815
I0211 00:28:50.043346 140063041582848 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.350654125213623, loss=1.8629076480865479
I0211 00:30:14.121046 140063033190144 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.939678192138672, loss=1.8705146312713623
I0211 00:31:40.513977 140063041582848 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.5608222484588623, loss=1.7723870277404785
I0211 00:33:11.348150 140063033190144 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.279329299926758, loss=1.8865468502044678
I0211 00:34:41.240210 140063041582848 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.4084136486053467, loss=1.742469072341919
I0211 00:36:09.452125 140063033190144 logging_writer.py:48] [6900] global_step=6900, grad_norm=8.998476028442383, loss=1.8027106523513794
I0211 00:36:34.604615 140218947737408 spec.py:321] Evaluating on the training split.
I0211 00:37:34.244347 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 00:38:26.877841 140218947737408 spec.py:349] Evaluating on the test split.
I0211 00:38:52.957891 140218947737408 submission_runner.py:408] Time since start: 6520.84s, 	Step: 6930, 	{'train/ctc_loss': Array(0.5146038, dtype=float32), 'train/wer': 0.17154331912576318, 'validation/ctc_loss': Array(0.8652903, dtype=float32), 'validation/wer': 0.2476997789084449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5532459, dtype=float32), 'test/wer': 0.17837629232425406, 'test/num_examples': 2472, 'score': 5804.555928230286, 'total_duration': 6520.837166547775, 'accumulated_submission_time': 5804.555928230286, 'accumulated_eval_time': 715.8265788555145, 'accumulated_logging_time': 0.1547858715057373}
I0211 00:38:52.983118 140063041582848 logging_writer.py:48] [6930] accumulated_eval_time=715.826579, accumulated_logging_time=0.154786, accumulated_submission_time=5804.555928, global_step=6930, preemption_count=0, score=5804.555928, test/ctc_loss=0.5532459020614624, test/num_examples=2472, test/wer=0.178376, total_duration=6520.837167, train/ctc_loss=0.5146037936210632, train/wer=0.171543, validation/ctc_loss=0.8652902841567993, validation/num_examples=5348, validation/wer=0.247700
I0211 00:39:46.387030 140063033190144 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.1535627841949463, loss=1.7718747854232788
I0211 00:41:01.740778 140063041582848 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.436856508255005, loss=1.7199065685272217
I0211 00:42:19.821272 140063033190144 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.982677698135376, loss=1.8539609909057617
I0211 00:43:42.032100 140063041582848 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.9120917320251465, loss=1.7712891101837158
I0211 00:45:00.564615 140063033190144 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.880356788635254, loss=1.80027437210083
I0211 00:46:21.696216 140063041582848 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.429746627807617, loss=1.756798267364502
I0211 00:47:46.926871 140063033190144 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.386213541030884, loss=1.7660504579544067
I0211 00:49:15.050472 140063041582848 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.030092716217041, loss=1.7446017265319824
I0211 00:50:45.611464 140063033190144 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.5532009601593018, loss=1.7625945806503296
I0211 00:52:10.958742 140063041582848 logging_writer.py:48] [7900] global_step=7900, grad_norm=4.003543853759766, loss=1.7037510871887207
I0211 00:53:35.803801 140063033190144 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.7526674270629883, loss=1.7017453908920288
I0211 00:55:05.230293 140063041582848 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.1776041984558105, loss=1.699992299079895
I0211 00:56:33.107447 140063033190144 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.2860679626464844, loss=1.7199623584747314
I0211 00:57:58.304529 140063041582848 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.926849842071533, loss=1.7126951217651367
I0211 00:59:13.867120 140063033190144 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.4740915298461914, loss=1.7072426080703735
I0211 01:00:34.871817 140063041582848 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.3861844539642334, loss=1.6728391647338867
I0211 01:01:58.065276 140063033190144 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.746481418609619, loss=1.6659585237503052
I0211 01:02:54.298468 140218947737408 spec.py:321] Evaluating on the training split.
I0211 01:03:58.313838 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 01:04:50.211415 140218947737408 spec.py:349] Evaluating on the test split.
I0211 01:05:17.757344 140218947737408 submission_runner.py:408] Time since start: 8105.64s, 	Step: 8665, 	{'train/ctc_loss': Array(0.46659666, dtype=float32), 'train/wer': 0.15698862771532324, 'validation/ctc_loss': Array(0.8137787, dtype=float32), 'validation/wer': 0.23474323450186818, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5063116, dtype=float32), 'test/wer': 0.16480815713038002, 'test/num_examples': 2472, 'score': 7245.780788183212, 'total_duration': 8105.636641263962, 'accumulated_submission_time': 7245.780788183212, 'accumulated_eval_time': 859.2827861309052, 'accumulated_logging_time': 0.1961219310760498}
I0211 01:05:17.785655 140063041582848 logging_writer.py:48] [8665] accumulated_eval_time=859.282786, accumulated_logging_time=0.196122, accumulated_submission_time=7245.780788, global_step=8665, preemption_count=0, score=7245.780788, test/ctc_loss=0.5063115954399109, test/num_examples=2472, test/wer=0.164808, total_duration=8105.636641, train/ctc_loss=0.46659666299819946, train/wer=0.156989, validation/ctc_loss=0.8137786984443665, validation/num_examples=5348, validation/wer=0.234743
I0211 01:05:44.750539 140063033190144 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.900932788848877, loss=1.6916409730911255
I0211 01:06:59.651269 140063041582848 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.0621657371521, loss=1.6662778854370117
I0211 01:08:15.617765 140063033190144 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.254005193710327, loss=1.6753441095352173
I0211 01:09:39.344327 140063041582848 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.54015588760376, loss=1.67471182346344
I0211 01:11:08.461460 140063033190144 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.8015284538269043, loss=1.7016533613204956
I0211 01:12:37.089304 140063041582848 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.1764750480651855, loss=1.7374354600906372
I0211 01:14:05.163330 140062713902848 logging_writer.py:48] [9300] global_step=9300, grad_norm=4.650041580200195, loss=1.6830309629440308
I0211 01:15:21.389056 140062705510144 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.8451590538024902, loss=1.6812922954559326
I0211 01:16:37.118568 140062713902848 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.7906134128570557, loss=1.6621111631393433
I0211 01:17:55.110228 140062705510144 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.7979655265808105, loss=1.6681947708129883
I0211 01:19:22.792699 140062713902848 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.6013247966766357, loss=1.698038101196289
I0211 01:20:51.636949 140062705510144 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.263122797012329, loss=1.6938215494155884
I0211 01:22:17.974139 140062713902848 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.2245330810546875, loss=1.6844302415847778
I0211 01:23:43.982775 140062705510144 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.263829708099365, loss=1.6988908052444458
I0211 01:25:13.558719 140062713902848 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.6798219680786133, loss=1.7072343826293945
I0211 01:26:39.117943 140062705510144 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.361384153366089, loss=1.657080054283142
I0211 01:28:07.633533 140062713902848 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.90263032913208, loss=1.678858757019043
I0211 01:29:17.930953 140218947737408 spec.py:321] Evaluating on the training split.
I0211 01:30:12.750574 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 01:31:04.231118 140218947737408 spec.py:349] Evaluating on the test split.
I0211 01:31:30.566706 140218947737408 submission_runner.py:408] Time since start: 9678.45s, 	Step: 10391, 	{'train/ctc_loss': Array(0.43561387, dtype=float32), 'train/wer': 0.14434705603064268, 'validation/ctc_loss': Array(0.7429956, dtype=float32), 'validation/wer': 0.2146808654431003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45595506, dtype=float32), 'test/wer': 0.14790892287693214, 'test/num_examples': 2472, 'score': 8685.838481426239, 'total_duration': 9678.446252822876, 'accumulated_submission_time': 8685.838481426239, 'accumulated_eval_time': 991.9161324501038, 'accumulated_logging_time': 0.239854097366333}
I0211 01:31:30.593402 140062498862848 logging_writer.py:48] [10391] accumulated_eval_time=991.916132, accumulated_logging_time=0.239854, accumulated_submission_time=8685.838481, global_step=10391, preemption_count=0, score=8685.838481, test/ctc_loss=0.45595505833625793, test/num_examples=2472, test/wer=0.147909, total_duration=9678.446253, train/ctc_loss=0.43561387062072754, train/wer=0.144347, validation/ctc_loss=0.7429956197738647, validation/num_examples=5348, validation/wer=0.214681
I0211 01:31:38.151190 140062490470144 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.392834186553955, loss=1.7078441381454468
I0211 01:32:53.155801 140062498862848 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.7410199642181396, loss=1.7292084693908691
I0211 01:34:08.136389 140062490470144 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.205348491668701, loss=1.6771650314331055
I0211 01:35:23.296361 140062498862848 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.623189687728882, loss=1.703762173652649
I0211 01:36:46.024063 140062490470144 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.893519639968872, loss=1.6150246858596802
I0211 01:38:15.983075 140062498862848 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.8665645122528076, loss=1.6930838823318481
I0211 01:39:42.661740 140062490470144 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.8037917613983154, loss=1.7222180366516113
I0211 01:41:10.454888 140062498862848 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.858466386795044, loss=1.6613528728485107
I0211 01:42:37.846347 140062490470144 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.299863576889038, loss=1.6368964910507202
I0211 01:44:04.731532 140062498862848 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.684375047683716, loss=1.6390732526779175
I0211 01:45:27.172105 140062171182848 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.199174404144287, loss=1.6058906316757202
I0211 01:46:43.934946 140062162790144 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.6797268390655518, loss=1.6797431707382202
I0211 01:48:00.310361 140062171182848 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.956852436065674, loss=1.7143075466156006
I0211 01:49:22.479868 140062162790144 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.454987049102783, loss=1.6018586158752441
I0211 01:50:50.845914 140062171182848 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.627811908721924, loss=1.6111356019973755
I0211 01:52:14.221396 140062162790144 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.6002044677734375, loss=1.576728105545044
I0211 01:53:43.816263 140062171182848 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.606466770172119, loss=1.611024022102356
I0211 01:55:11.269929 140062162790144 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.288673162460327, loss=1.6403111219406128
I0211 01:55:30.570265 140218947737408 spec.py:321] Evaluating on the training split.
I0211 01:56:25.770725 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 01:57:17.538065 140218947737408 spec.py:349] Evaluating on the test split.
I0211 01:57:44.519852 140218947737408 submission_runner.py:408] Time since start: 11252.40s, 	Step: 12124, 	{'train/ctc_loss': Array(0.39994377, dtype=float32), 'train/wer': 0.13405334694658924, 'validation/ctc_loss': Array(0.71666336, dtype=float32), 'validation/wer': 0.20626200797474342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44069472, dtype=float32), 'test/wer': 0.14283102796904515, 'test/num_examples': 2472, 'score': 10125.729723930359, 'total_duration': 11252.399189710617, 'accumulated_submission_time': 10125.729723930359, 'accumulated_eval_time': 1125.8630819320679, 'accumulated_logging_time': 0.27825331687927246}
I0211 01:57:44.546856 140062611502848 logging_writer.py:48] [12124] accumulated_eval_time=1125.863082, accumulated_logging_time=0.278253, accumulated_submission_time=10125.729724, global_step=12124, preemption_count=0, score=10125.729724, test/ctc_loss=0.4406947195529938, test/num_examples=2472, test/wer=0.142831, total_duration=11252.399190, train/ctc_loss=0.3999437689781189, train/wer=0.134053, validation/ctc_loss=0.7166633605957031, validation/num_examples=5348, validation/wer=0.206262
I0211 01:58:42.374777 140062603110144 logging_writer.py:48] [12200] global_step=12200, grad_norm=3.0434696674346924, loss=1.5873390436172485
I0211 01:59:57.973938 140062611502848 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.0273067951202393, loss=1.6103655099868774
I0211 02:01:18.148330 140062611502848 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.6783376932144165, loss=1.546745777130127
I0211 02:02:34.177468 140062603110144 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.2396466732025146, loss=1.626021146774292
I0211 02:03:55.140748 140062611502848 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.234346389770508, loss=1.6614056825637817
I0211 02:05:19.455948 140062603110144 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.0881593227386475, loss=1.5546252727508545
I0211 02:06:45.890845 140062611502848 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.786787509918213, loss=1.5838303565979004
I0211 02:08:12.216561 140062603110144 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.3464207649230957, loss=1.6482874155044556
I0211 02:09:39.271200 140062611502848 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.342458724975586, loss=1.668267011642456
I0211 02:11:07.645721 140062603110144 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.3395538330078125, loss=1.6328874826431274
I0211 02:12:34.744036 140062611502848 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.249788999557495, loss=1.6316301822662354
I0211 02:14:00.990897 140062603110144 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.7837815284729004, loss=1.6338846683502197
I0211 02:15:33.444226 140062611502848 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.844358205795288, loss=1.5960108041763306
I0211 02:16:50.745336 140062603110144 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.5556461811065674, loss=1.5586447715759277
I0211 02:18:06.576237 140062611502848 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.7671120166778564, loss=1.651251196861267
I0211 02:19:25.678849 140062603110144 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.77681303024292, loss=1.5769490003585815
I0211 02:20:51.324150 140062611502848 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.1870462894439697, loss=1.5510649681091309
I0211 02:21:46.022634 140218947737408 spec.py:321] Evaluating on the training split.
I0211 02:22:41.494138 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 02:23:35.994069 140218947737408 spec.py:349] Evaluating on the test split.
I0211 02:24:02.978739 140218947737408 submission_runner.py:408] Time since start: 12830.86s, 	Step: 13863, 	{'train/ctc_loss': Array(0.34301874, dtype=float32), 'train/wer': 0.11864063028159859, 'validation/ctc_loss': Array(0.683678, dtype=float32), 'validation/wer': 0.19800727960840728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41449788, dtype=float32), 'test/wer': 0.1320862023439563, 'test/num_examples': 2472, 'score': 11567.119065999985, 'total_duration': 12830.857915639877, 'accumulated_submission_time': 11567.119065999985, 'accumulated_eval_time': 1262.8164167404175, 'accumulated_logging_time': 0.31701135635375977}
I0211 02:24:03.007532 140062319662848 logging_writer.py:48] [13863] accumulated_eval_time=1262.816417, accumulated_logging_time=0.317011, accumulated_submission_time=11567.119066, global_step=13863, preemption_count=0, score=11567.119066, test/ctc_loss=0.41449788212776184, test/num_examples=2472, test/wer=0.132086, total_duration=12830.857916, train/ctc_loss=0.3430187404155731, train/wer=0.118641, validation/ctc_loss=0.6836779713630676, validation/num_examples=5348, validation/wer=0.198007
I0211 02:24:31.821520 140062311270144 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.35628604888916, loss=1.6200021505355835
I0211 02:25:47.384802 140062319662848 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.744652271270752, loss=1.6168766021728516
I0211 02:27:02.805356 140062311270144 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.483431577682495, loss=1.6116185188293457
I0211 02:28:28.144790 140062319662848 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.3770217895507812, loss=1.5610514879226685
I0211 02:29:55.912971 140062311270144 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.8962397575378418, loss=1.5166022777557373
I0211 02:31:23.465094 140062319662848 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.4544849395751953, loss=1.609846591949463
I0211 02:32:44.547542 140061991982848 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.099709987640381, loss=1.6050199270248413
I0211 02:34:04.288888 140061983590144 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.909656047821045, loss=1.5646151304244995
I0211 02:35:22.052376 140061991982848 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.690934419631958, loss=1.5690644979476929
I0211 02:36:43.240739 140061983590144 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.17411994934082, loss=1.5956329107284546
I0211 02:38:09.812524 140061991982848 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.249917507171631, loss=1.4634920358657837
I0211 02:39:36.657585 140061983590144 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.9991021156311035, loss=1.5675048828125
I0211 02:41:03.400105 140061991982848 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.0471560955047607, loss=1.5501620769500732
I0211 02:42:31.607807 140061983590144 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.694098472595215, loss=1.6158174276351929
I0211 02:43:59.637952 140061991982848 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.3427698612213135, loss=1.60804283618927
I0211 02:45:27.302361 140061983590144 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.5872268676757812, loss=1.6694968938827515
I0211 02:46:53.357201 140062319662848 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.925333261489868, loss=1.5057847499847412
I0211 02:48:03.483580 140218947737408 spec.py:321] Evaluating on the training split.
I0211 02:49:02.085530 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 02:49:54.266223 140218947737408 spec.py:349] Evaluating on the test split.
I0211 02:50:21.895053 140218947737408 submission_runner.py:408] Time since start: 14409.77s, 	Step: 15594, 	{'train/ctc_loss': Array(0.31959072, dtype=float32), 'train/wer': 0.10852853396038709, 'validation/ctc_loss': Array(0.67307526, dtype=float32), 'validation/wer': 0.19473435222105293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39703676, dtype=float32), 'test/wer': 0.12692706111754312, 'test/num_examples': 2472, 'score': 13007.510919809341, 'total_duration': 14409.774055480957, 'accumulated_submission_time': 13007.510919809341, 'accumulated_eval_time': 1401.224974155426, 'accumulated_logging_time': 0.3565559387207031}
I0211 02:50:21.923078 140062611502848 logging_writer.py:48] [15594] accumulated_eval_time=1401.224974, accumulated_logging_time=0.356556, accumulated_submission_time=13007.510920, global_step=15594, preemption_count=0, score=13007.510920, test/ctc_loss=0.39703676104545593, test/num_examples=2472, test/wer=0.126927, total_duration=14409.774055, train/ctc_loss=0.3195907175540924, train/wer=0.108529, validation/ctc_loss=0.673075258731842, validation/num_examples=5348, validation/wer=0.194734
I0211 02:50:27.313082 140062603110144 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.448021173477173, loss=1.553350806236267
I0211 02:51:42.214139 140062611502848 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.8171679973602295, loss=1.5075994729995728
I0211 02:52:57.264148 140062603110144 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.5091657638549805, loss=1.4829713106155396
I0211 02:54:14.817638 140062611502848 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.989919662475586, loss=1.5507681369781494
I0211 02:55:40.774820 140062603110144 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.027257204055786, loss=1.4831528663635254
I0211 02:57:08.201546 140062611502848 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.7678916454315186, loss=1.5923656225204468
I0211 02:58:32.566723 140062603110144 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.4699795246124268, loss=1.61447012424469
I0211 02:59:59.215019 140062611502848 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.419902324676514, loss=1.5870776176452637
I0211 03:01:25.019858 140062603110144 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.351787567138672, loss=1.5339221954345703
I0211 03:02:53.353785 140062611502848 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.151810646057129, loss=1.5295863151550293
I0211 03:04:11.215795 140062603110144 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.6055638790130615, loss=1.531622290611267
I0211 03:05:27.006252 140062611502848 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.455812931060791, loss=1.5218369960784912
I0211 03:06:43.300394 140062603110144 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.036628484725952, loss=1.5015993118286133
I0211 03:08:06.836194 140062611502848 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.1007535457611084, loss=1.5981091260910034
I0211 03:09:34.919738 140062603110144 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.0834269523620605, loss=1.504032015800476
I0211 03:11:04.357848 140062611502848 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.7302308082580566, loss=1.536708116531372
I0211 03:12:30.914650 140062603110144 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.4289581775665283, loss=1.5356686115264893
I0211 03:13:58.268542 140062611502848 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.16570520401001, loss=1.5666935443878174
I0211 03:14:21.959257 140218947737408 spec.py:321] Evaluating on the training split.
I0211 03:15:25.561621 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 03:16:18.722506 140218947737408 spec.py:349] Evaluating on the test split.
I0211 03:16:45.338955 140218947737408 submission_runner.py:408] Time since start: 15993.22s, 	Step: 17329, 	{'train/ctc_loss': Array(0.32896024, dtype=float32), 'train/wer': 0.11624040714979057, 'validation/ctc_loss': Array(0.6574222, dtype=float32), 'validation/wer': 0.1909786921806965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39617884, dtype=float32), 'test/wer': 0.1280238864176467, 'test/num_examples': 2472, 'score': 14447.46181845665, 'total_duration': 15993.21812081337, 'accumulated_submission_time': 14447.46181845665, 'accumulated_eval_time': 1544.6018645763397, 'accumulated_logging_time': 0.3955864906311035}
I0211 03:16:45.368395 140063041582848 logging_writer.py:48] [17329] accumulated_eval_time=1544.601865, accumulated_logging_time=0.395586, accumulated_submission_time=14447.461818, global_step=17329, preemption_count=0, score=14447.461818, test/ctc_loss=0.39617884159088135, test/num_examples=2472, test/wer=0.128024, total_duration=15993.218121, train/ctc_loss=0.32896023988723755, train/wer=0.116240, validation/ctc_loss=0.6574221849441528, validation/num_examples=5348, validation/wer=0.190979
I0211 03:17:39.431509 140063033190144 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.3722851276397705, loss=1.5402971506118774
I0211 03:18:55.068295 140063041582848 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.2387235164642334, loss=1.5775318145751953
I0211 03:20:14.944754 140062386222848 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.016010761260986, loss=1.490418553352356
I0211 03:21:33.286727 140062377830144 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.0007131099700928, loss=1.5513249635696411
I0211 03:22:53.034556 140062386222848 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.1084694862365723, loss=1.5676575899124146
I0211 03:24:17.009702 140062377830144 logging_writer.py:48] [17900] global_step=17900, grad_norm=5.395583152770996, loss=1.5127381086349487
I0211 03:25:41.445704 140062386222848 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.220369577407837, loss=1.5345637798309326
I0211 03:27:08.425996 140062377830144 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.4906458854675293, loss=1.5610687732696533
I0211 03:28:36.880319 140062386222848 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.796048641204834, loss=1.510694146156311
I0211 03:30:07.035284 140062377830144 logging_writer.py:48] [18300] global_step=18300, grad_norm=4.158450603485107, loss=1.4789148569107056
I0211 03:31:34.657974 140062386222848 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.8493058681488037, loss=1.5668654441833496
I0211 03:32:59.294625 140062377830144 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.404531478881836, loss=1.5202417373657227
I0211 03:34:23.055382 140063041582848 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.0169460773468018, loss=1.5200459957122803
I0211 03:35:40.970769 140063033190144 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.478393793106079, loss=1.4543766975402832
I0211 03:36:55.958800 140063041582848 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.5046987533569336, loss=1.4733508825302124
I0211 03:38:20.037378 140063033190144 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.5557191371917725, loss=1.4932348728179932
I0211 03:39:48.730235 140063041582848 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.8509440422058105, loss=1.4936566352844238
I0211 03:40:45.357957 140218947737408 spec.py:321] Evaluating on the training split.
I0211 03:41:40.846296 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 03:42:33.327539 140218947737408 spec.py:349] Evaluating on the test split.
I0211 03:42:59.489728 140218947737408 submission_runner.py:408] Time since start: 17567.37s, 	Step: 19065, 	{'train/ctc_loss': Array(0.32634243, dtype=float32), 'train/wer': 0.10938417197704857, 'validation/ctc_loss': Array(0.6247462, dtype=float32), 'validation/wer': 0.1807544145901117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3724943, dtype=float32), 'test/wer': 0.12136168829849897, 'test/num_examples': 2472, 'score': 15887.366396427155, 'total_duration': 17567.369089365005, 'accumulated_submission_time': 15887.366396427155, 'accumulated_eval_time': 1678.731039762497, 'accumulated_logging_time': 0.43723106384277344}
I0211 03:42:59.516844 140062027822848 logging_writer.py:48] [19065] accumulated_eval_time=1678.731040, accumulated_logging_time=0.437231, accumulated_submission_time=15887.366396, global_step=19065, preemption_count=0, score=15887.366396, test/ctc_loss=0.37249431014060974, test/num_examples=2472, test/wer=0.121362, total_duration=17567.369089, train/ctc_loss=0.3263424336910248, train/wer=0.109384, validation/ctc_loss=0.6247462034225464, validation/num_examples=5348, validation/wer=0.180754
I0211 03:43:26.934099 140062019430144 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.3866398334503174, loss=1.5254440307617188
I0211 03:44:41.857770 140062027822848 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.0151734352111816, loss=1.5373561382293701
I0211 03:45:56.761266 140062019430144 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.6043038368225098, loss=1.5412256717681885
I0211 03:47:24.234700 140062027822848 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.9343465566635132, loss=1.5011717081069946
I0211 03:48:49.318092 140062019430144 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.357231616973877, loss=1.4948598146438599
I0211 03:50:17.704556 140061372462848 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.97613263130188, loss=1.4904805421829224
I0211 03:51:35.604016 140061364070144 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.3888628482818604, loss=1.4913147687911987
I0211 03:52:53.092412 140061372462848 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.1466429233551025, loss=1.490572214126587
I0211 03:54:15.008847 140061364070144 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.4987008571624756, loss=1.5068676471710205
I0211 03:55:39.681722 140061372462848 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.931678056716919, loss=1.48209547996521
I0211 03:57:07.121453 140061364070144 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.0600764751434326, loss=1.5328562259674072
I0211 03:58:33.532265 140061372462848 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.8773036003112793, loss=1.4583499431610107
I0211 04:00:00.862782 140061364070144 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.6371731758117676, loss=1.515398621559143
I0211 04:01:26.966727 140061372462848 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.6605567932128906, loss=1.529595971107483
I0211 04:02:54.597405 140061364070144 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.002211570739746, loss=1.5085684061050415
I0211 04:04:22.766050 140061372462848 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.1589832305908203, loss=1.4581496715545654
I0211 04:05:40.465553 140061364070144 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.8310611248016357, loss=1.5445116758346558
I0211 04:06:56.587144 140061372462848 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.591404676437378, loss=1.4632985591888428
I0211 04:06:59.497625 140218947737408 spec.py:321] Evaluating on the training split.
I0211 04:07:54.686128 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 04:08:47.685136 140218947737408 spec.py:349] Evaluating on the test split.
I0211 04:09:15.131823 140218947737408 submission_runner.py:408] Time since start: 19143.01s, 	Step: 20805, 	{'train/ctc_loss': Array(0.31835115, dtype=float32), 'train/wer': 0.10723085445959848, 'validation/ctc_loss': Array(0.6072395, dtype=float32), 'validation/wer': 0.17523195303976752, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35500604, dtype=float32), 'test/wer': 0.11638535128876973, 'test/num_examples': 2472, 'score': 17327.25873041153, 'total_duration': 19143.011053800583, 'accumulated_submission_time': 17327.25873041153, 'accumulated_eval_time': 1814.3625190258026, 'accumulated_logging_time': 0.47852015495300293}
I0211 04:09:15.161579 140062826542848 logging_writer.py:48] [20805] accumulated_eval_time=1814.362519, accumulated_logging_time=0.478520, accumulated_submission_time=17327.258730, global_step=20805, preemption_count=0, score=17327.258730, test/ctc_loss=0.35500603914260864, test/num_examples=2472, test/wer=0.116385, total_duration=19143.011054, train/ctc_loss=0.318351149559021, train/wer=0.107231, validation/ctc_loss=0.6072394847869873, validation/num_examples=5348, validation/wer=0.175232
I0211 04:10:27.029386 140062818150144 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.9798741340637207, loss=1.4959025382995605
I0211 04:11:41.888079 140062826542848 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.061511516571045, loss=1.4814563989639282
I0211 04:13:03.984347 140062818150144 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.5997447967529297, loss=1.5149312019348145
I0211 04:14:31.801822 140062826542848 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.8481497764587402, loss=1.4696213006973267
I0211 04:16:00.932792 140062818150144 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.7388365268707275, loss=1.4543863534927368
I0211 04:17:27.802307 140062826542848 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.2783241271972656, loss=1.4707797765731812
I0211 04:18:57.369824 140062818150144 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.1160149574279785, loss=1.5024179220199585
I0211 04:20:26.000281 140062826542848 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.8501174449920654, loss=1.4634813070297241
I0211 04:21:52.655168 140062171182848 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.8999783992767334, loss=1.4472675323486328
I0211 04:23:08.766442 140062162790144 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.5844080448150635, loss=1.418172836303711
I0211 04:24:25.674065 140062171182848 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.370307445526123, loss=1.4468038082122803
I0211 04:25:50.588304 140062162790144 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.4389491081237793, loss=1.4485054016113281
I0211 04:27:17.537805 140062171182848 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.4233474731445312, loss=1.4544568061828613
I0211 04:28:49.512789 140062162790144 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.377290964126587, loss=1.4741533994674683
I0211 04:30:14.822350 140062171182848 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.2739951610565186, loss=1.4207674264907837
I0211 04:31:40.341869 140062162790144 logging_writer.py:48] [22400] global_step=22400, grad_norm=4.9323859214782715, loss=1.4364787340164185
I0211 04:33:08.096521 140062171182848 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.8989036083221436, loss=1.443710446357727
I0211 04:33:15.377353 140218947737408 spec.py:321] Evaluating on the training split.
I0211 04:34:11.996648 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 04:35:04.349268 140218947737408 spec.py:349] Evaluating on the test split.
I0211 04:35:30.864390 140218947737408 submission_runner.py:408] Time since start: 20718.74s, 	Step: 22510, 	{'train/ctc_loss': Array(0.29313225, dtype=float32), 'train/wer': 0.09935178516308261, 'validation/ctc_loss': Array(0.5852561, dtype=float32), 'validation/wer': 0.16853162381609818, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34486303, dtype=float32), 'test/wer': 0.11037312371783153, 'test/num_examples': 2472, 'score': 18767.391078948975, 'total_duration': 20718.743278980255, 'accumulated_submission_time': 18767.391078948975, 'accumulated_eval_time': 1949.8465313911438, 'accumulated_logging_time': 0.5190324783325195}
I0211 04:35:30.895487 140061664302848 logging_writer.py:48] [22510] accumulated_eval_time=1949.846531, accumulated_logging_time=0.519032, accumulated_submission_time=18767.391079, global_step=22510, preemption_count=0, score=18767.391079, test/ctc_loss=0.34486302733421326, test/num_examples=2472, test/wer=0.110373, total_duration=20718.743279, train/ctc_loss=0.2931322455406189, train/wer=0.099352, validation/ctc_loss=0.5852560997009277, validation/num_examples=5348, validation/wer=0.168532
I0211 04:36:39.004001 140061655910144 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.666377544403076, loss=1.429343342781067
I0211 04:37:58.209760 140061664302848 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.796776056289673, loss=1.3946107625961304
I0211 04:39:15.741608 140061655910144 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.3407669067382812, loss=1.4560599327087402
I0211 04:40:32.699208 140061664302848 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.617860794067383, loss=1.4425272941589355
I0211 04:41:54.649753 140061655910144 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.2824084758758545, loss=1.4243900775909424
I0211 04:43:20.173624 140061664302848 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.923795700073242, loss=1.5125483274459839
I0211 04:44:47.514847 140061655910144 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.931217908859253, loss=1.4210213422775269
I0211 04:46:12.380603 140061664302848 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.5059099197387695, loss=1.4521499872207642
I0211 04:47:41.974254 140061655910144 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.147808790206909, loss=1.4482719898223877
I0211 04:49:09.165575 140061664302848 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.3422739505767822, loss=1.3644447326660156
I0211 04:50:32.813287 140061655910144 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.9857192039489746, loss=1.4481276273727417
I0211 04:52:02.823981 140062826542848 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.4412479400634766, loss=1.4435255527496338
I0211 04:53:20.871394 140062818150144 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.6788690090179443, loss=1.400259256362915
I0211 04:54:36.724243 140062826542848 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.553341865539551, loss=1.4148509502410889
I0211 04:55:57.435696 140062818150144 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.2779502868652344, loss=1.453696608543396
I0211 04:57:22.093563 140062826542848 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.9530746936798096, loss=1.4561457633972168
I0211 04:58:51.759154 140062818150144 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.646280527114868, loss=1.363857626914978
I0211 04:59:30.934581 140218947737408 spec.py:321] Evaluating on the training split.
I0211 05:00:25.732649 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 05:01:19.795880 140218947737408 spec.py:349] Evaluating on the test split.
I0211 05:01:45.844764 140218947737408 submission_runner.py:408] Time since start: 22293.72s, 	Step: 24245, 	{'train/ctc_loss': Array(0.27176955, dtype=float32), 'train/wer': 0.09307567248004729, 'validation/ctc_loss': Array(0.57057077, dtype=float32), 'validation/wer': 0.164611834673721, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33417776, dtype=float32), 'test/wer': 0.10590457619889099, 'test/num_examples': 2472, 'score': 20207.345369815826, 'total_duration': 22293.72416448593, 'accumulated_submission_time': 20207.345369815826, 'accumulated_eval_time': 2084.754161596298, 'accumulated_logging_time': 0.5621540546417236}
I0211 05:01:45.871852 140062826542848 logging_writer.py:48] [24245] accumulated_eval_time=2084.754162, accumulated_logging_time=0.562154, accumulated_submission_time=20207.345370, global_step=24245, preemption_count=0, score=20207.345370, test/ctc_loss=0.33417776226997375, test/num_examples=2472, test/wer=0.105905, total_duration=22293.724164, train/ctc_loss=0.27176955342292786, train/wer=0.093076, validation/ctc_loss=0.5705707669258118, validation/num_examples=5348, validation/wer=0.164612
I0211 05:02:27.967193 140062818150144 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.2988739013671875, loss=1.474118709564209
I0211 05:03:43.123439 140062826542848 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.2325994968414307, loss=1.427539348602295
I0211 05:05:03.156283 140062818150144 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.419949531555176, loss=1.5006964206695557
I0211 05:06:30.678038 140062826542848 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.97768497467041, loss=1.4117271900177002
I0211 05:07:59.581924 140062818150144 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.9343210458755493, loss=1.3775134086608887
I0211 05:09:23.568918 140062826542848 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.513758897781372, loss=1.3802975416183472
I0211 05:10:41.357701 140062818150144 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.0539205074310303, loss=1.3817970752716064
I0211 05:12:01.230413 140062826542848 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.1064672470092773, loss=1.4115872383117676
I0211 05:13:25.815940 140062818150144 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.3091623783111572, loss=1.3573048114776611
I0211 05:14:53.066920 140062826542848 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.9236558675765991, loss=1.412270426750183
I0211 05:16:17.486170 140062818150144 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.678678274154663, loss=1.4242384433746338
I0211 05:17:46.042319 140062826542848 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.9606974124908447, loss=1.4076522588729858
I0211 05:19:13.025371 140062818150144 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.274435043334961, loss=1.4165170192718506
I0211 05:20:42.766643 140062826542848 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.1941680908203125, loss=1.4288862943649292
I0211 05:22:10.613565 140062818150144 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.372857093811035, loss=1.389410138130188
I0211 05:23:36.387003 140062826542848 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.562319755554199, loss=1.4160966873168945
I0211 05:24:56.330826 140062818150144 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.050537347793579, loss=1.4421637058258057
I0211 05:25:46.273873 140218947737408 spec.py:321] Evaluating on the training split.
I0211 05:26:41.350444 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 05:27:32.671732 140218947737408 spec.py:349] Evaluating on the test split.
I0211 05:27:59.345396 140218947737408 submission_runner.py:408] Time since start: 23867.22s, 	Step: 25965, 	{'train/ctc_loss': Array(0.2505488, dtype=float32), 'train/wer': 0.08905813923159185, 'validation/ctc_loss': Array(0.5587426, dtype=float32), 'validation/wer': 0.16262297614335228, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31846216, dtype=float32), 'test/wer': 0.10462494668210347, 'test/num_examples': 2472, 'score': 21647.663966178894, 'total_duration': 23867.22459101677, 'accumulated_submission_time': 21647.663966178894, 'accumulated_eval_time': 2217.822919368744, 'accumulated_logging_time': 0.6007964611053467}
I0211 05:27:59.373414 140062611502848 logging_writer.py:48] [25965] accumulated_eval_time=2217.822919, accumulated_logging_time=0.600796, accumulated_submission_time=21647.663966, global_step=25965, preemption_count=0, score=21647.663966, test/ctc_loss=0.31846216320991516, test/num_examples=2472, test/wer=0.104625, total_duration=23867.224591, train/ctc_loss=0.2505488097667694, train/wer=0.089058, validation/ctc_loss=0.5587425827980042, validation/num_examples=5348, validation/wer=0.162623
I0211 05:28:26.736616 140062603110144 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.774141788482666, loss=1.4211245775222778
I0211 05:29:42.236413 140062611502848 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.7328007221221924, loss=1.4641629457473755
I0211 05:30:57.352246 140062603110144 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7773454189300537, loss=1.3668406009674072
I0211 05:32:20.469002 140062611502848 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.604674816131592, loss=1.4416757822036743
I0211 05:33:46.643804 140062603110144 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.490424633026123, loss=1.3970838785171509
I0211 05:35:14.914023 140062611502848 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.693756103515625, loss=1.405592441558838
I0211 05:36:45.097256 140062603110144 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.7541332244873047, loss=1.4088196754455566
I0211 05:38:13.713509 140062611502848 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.2093725204467773, loss=1.391208529472351
I0211 05:39:45.798343 140062611502848 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.8518643379211426, loss=1.4265013933181763
I0211 05:41:04.506809 140062603110144 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.367398262023926, loss=1.4050557613372803
I0211 05:42:20.818662 140062611502848 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.5918209552764893, loss=1.4442421197891235
I0211 05:43:41.712379 140062603110144 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.4660890102386475, loss=1.4324737787246704
I0211 05:45:06.086628 140062611502848 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.3637053966522217, loss=1.3326020240783691
I0211 05:46:33.450424 140062603110144 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.9746975898742676, loss=1.425936222076416
I0211 05:48:00.514853 140062611502848 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.6590921878814697, loss=1.4062718152999878
I0211 05:49:29.587379 140062603110144 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.1749515533447266, loss=1.3608582019805908
I0211 05:50:59.845062 140062611502848 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.329723358154297, loss=1.3998260498046875
I0211 05:51:59.415063 140218947737408 spec.py:321] Evaluating on the training split.
I0211 05:52:55.060237 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 05:53:47.208675 140218947737408 spec.py:349] Evaluating on the test split.
I0211 05:54:13.858280 140218947737408 submission_runner.py:408] Time since start: 25441.74s, 	Step: 27671, 	{'train/ctc_loss': Array(0.2422054, dtype=float32), 'train/wer': 0.08290801976046067, 'validation/ctc_loss': Array(0.5420699, dtype=float32), 'validation/wer': 0.1576701391235506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31094378, dtype=float32), 'test/wer': 0.10070481181321471, 'test/num_examples': 2472, 'score': 23087.620130300522, 'total_duration': 25441.73743915558, 'accumulated_submission_time': 23087.620130300522, 'accumulated_eval_time': 2352.263339281082, 'accumulated_logging_time': 0.6415128707885742}
I0211 05:54:13.891190 140063041582848 logging_writer.py:48] [27671] accumulated_eval_time=2352.263339, accumulated_logging_time=0.641513, accumulated_submission_time=23087.620130, global_step=27671, preemption_count=0, score=23087.620130, test/ctc_loss=0.3109437823295593, test/num_examples=2472, test/wer=0.100705, total_duration=25441.737439, train/ctc_loss=0.2422053962945938, train/wer=0.082908, validation/ctc_loss=0.5420699119567871, validation/num_examples=5348, validation/wer=0.157670
I0211 05:54:37.169293 140063033190144 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.642868757247925, loss=1.3796077966690063
I0211 05:55:52.300870 140063041582848 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.1601502895355225, loss=1.344170331954956
I0211 05:57:13.502638 140061372462848 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.923819065093994, loss=1.3533973693847656
I0211 05:58:30.806428 140061364070144 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.3620502948760986, loss=1.3759922981262207
I0211 05:59:54.933881 140061372462848 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.453317165374756, loss=1.3341255187988281
I0211 06:01:21.925650 140061364070144 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.343888998031616, loss=1.4218106269836426
I0211 06:02:49.155537 140061372462848 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.1623451709747314, loss=1.3509058952331543
I0211 06:04:19.051910 140061364070144 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.680612087249756, loss=1.417675495147705
I0211 06:05:47.679393 140061372462848 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.855712413787842, loss=1.4059393405914307
I0211 06:07:17.350441 140061364070144 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.3438119888305664, loss=1.3666200637817383
I0211 06:08:46.475683 140061372462848 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.3522980213165283, loss=1.3813786506652832
I0211 06:10:16.405678 140061364070144 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.750121593475342, loss=1.3902660608291626
I0211 06:11:41.456596 140063041582848 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.4256672859191895, loss=1.361472487449646
I0211 06:13:02.330685 140063033190144 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.611290693283081, loss=1.392399549484253
I0211 06:14:22.303392 140063041582848 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.0978190898895264, loss=1.3056466579437256
I0211 06:15:45.328718 140063033190144 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.054591655731201, loss=1.3303158283233643
I0211 06:17:13.815711 140063041582848 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.79121994972229, loss=1.3499414920806885
I0211 06:18:14.473282 140218947737408 spec.py:321] Evaluating on the training split.
I0211 06:19:13.820376 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 06:20:06.088486 140218947737408 spec.py:349] Evaluating on the test split.
I0211 06:20:32.501239 140218947737408 submission_runner.py:408] Time since start: 27020.38s, 	Step: 29371, 	{'train/ctc_loss': Array(0.25566694, dtype=float32), 'train/wer': 0.08810560755763563, 'validation/ctc_loss': Array(0.52995616, dtype=float32), 'validation/wer': 0.15467719667493748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30119932, dtype=float32), 'test/wer': 0.09595190217943249, 'test/num_examples': 2472, 'score': 24528.120047092438, 'total_duration': 27020.380325078964, 'accumulated_submission_time': 24528.120047092438, 'accumulated_eval_time': 2490.2884378433228, 'accumulated_logging_time': 0.685178279876709}
I0211 06:20:32.534907 140063041582848 logging_writer.py:48] [29371] accumulated_eval_time=2490.288438, accumulated_logging_time=0.685178, accumulated_submission_time=24528.120047, global_step=29371, preemption_count=0, score=24528.120047, test/ctc_loss=0.3011993169784546, test/num_examples=2472, test/wer=0.095952, total_duration=27020.380325, train/ctc_loss=0.25566694140434265, train/wer=0.088106, validation/ctc_loss=0.5299561619758606, validation/num_examples=5348, validation/wer=0.154677
I0211 06:20:55.214911 140063033190144 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.5743725299835205, loss=1.372370719909668
I0211 06:22:10.490415 140063041582848 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.4502577781677246, loss=1.3681597709655762
I0211 06:23:25.659626 140063033190144 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.3307418823242188, loss=1.3280388116836548
I0211 06:24:48.038165 140063041582848 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.5098462104797363, loss=1.3044441938400269
I0211 06:26:15.480151 140063033190144 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.8867037296295166, loss=1.3463245630264282
I0211 06:27:44.846069 140063041582848 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.8142502307891846, loss=1.369152307510376
I0211 06:29:01.971734 140063033190144 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.253251791000366, loss=1.3129416704177856
I0211 06:30:18.309687 140063041582848 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.839088201522827, loss=1.3172893524169922
I0211 06:31:42.508483 140063033190144 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.4108057022094727, loss=1.3324722051620483
I0211 06:33:07.261693 140063041582848 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.921865224838257, loss=1.330694317817688
I0211 06:34:34.297662 140063033190144 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.7146999835968018, loss=1.3607571125030518
I0211 06:36:05.081383 140063041582848 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.5413219928741455, loss=1.3107916116714478
I0211 06:37:29.122122 140063033190144 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.6319663524627686, loss=1.330661416053772
I0211 06:38:58.947522 140063041582848 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.527756452560425, loss=1.3471784591674805
I0211 06:40:26.326753 140063033190144 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.4425110816955566, loss=1.280486822128296
I0211 06:41:57.030742 140062713902848 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.3357410430908203, loss=1.2674916982650757
I0211 06:43:14.866808 140062705510144 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.7574517726898193, loss=1.3343470096588135
I0211 06:44:31.081605 140062713902848 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.2037482261657715, loss=1.287999153137207
I0211 06:44:33.204416 140218947737408 spec.py:321] Evaluating on the training split.
I0211 06:45:26.720656 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 06:46:18.957594 140218947737408 spec.py:349] Evaluating on the test split.
I0211 06:46:45.349359 140218947737408 submission_runner.py:408] Time since start: 28593.23s, 	Step: 31104, 	{'train/ctc_loss': Array(0.23302117, dtype=float32), 'train/wer': 0.07783547387668951, 'validation/ctc_loss': Array(0.51413625, dtype=float32), 'validation/wer': 0.1478996302267878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29052296, dtype=float32), 'test/wer': 0.09227550626612231, 'test/num_examples': 2472, 'score': 25968.7009601593, 'total_duration': 28593.22857093811, 'accumulated_submission_time': 25968.7009601593, 'accumulated_eval_time': 2622.43061876297, 'accumulated_logging_time': 0.7326092720031738}
I0211 06:46:45.380587 140062422062848 logging_writer.py:48] [31104] accumulated_eval_time=2622.430619, accumulated_logging_time=0.732609, accumulated_submission_time=25968.700960, global_step=31104, preemption_count=0, score=25968.700960, test/ctc_loss=0.290522962808609, test/num_examples=2472, test/wer=0.092276, total_duration=28593.228571, train/ctc_loss=0.23302116990089417, train/wer=0.077835, validation/ctc_loss=0.5141362547874451, validation/num_examples=5348, validation/wer=0.147900
I0211 06:47:58.604448 140062413670144 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.536019802093506, loss=1.295790195465088
I0211 06:49:13.997375 140062422062848 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.7704379558563232, loss=1.3732497692108154
I0211 06:50:32.998274 140062413670144 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.3713934421539307, loss=1.2955005168914795
I0211 06:52:03.165938 140062422062848 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.9913324117660522, loss=1.3228464126586914
I0211 06:53:26.713178 140062413670144 logging_writer.py:48] [31600] global_step=31600, grad_norm=4.008984565734863, loss=1.3536622524261475
I0211 06:54:55.371498 140062422062848 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.353515863418579, loss=1.3300353288650513
I0211 06:56:25.680742 140062413670144 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.058215618133545, loss=1.3726402521133423
I0211 06:57:53.348677 140062422062848 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.120086431503296, loss=1.3070555925369263
I0211 06:59:15.714328 140063041582848 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.838866949081421, loss=1.3375985622406006
I0211 07:00:32.065184 140063033190144 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.601879596710205, loss=1.3096121549606323
I0211 07:01:50.926909 140063041582848 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.7147510051727295, loss=1.275454044342041
I0211 07:03:12.877182 140063033190144 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.7601139545440674, loss=1.2719471454620361
I0211 07:04:38.637874 140063041582848 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.275773763656616, loss=1.2815362215042114
I0211 07:06:08.272062 140063033190144 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.8597655296325684, loss=1.2566405534744263
I0211 07:07:35.841033 140063041582848 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.772129774093628, loss=1.2746118307113647
I0211 07:09:02.691130 140063033190144 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.7028534412384033, loss=1.317116141319275
I0211 07:10:28.290987 140063041582848 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.442300319671631, loss=1.3466848134994507
I0211 07:10:45.640019 140218947737408 spec.py:321] Evaluating on the training split.
I0211 07:11:41.937057 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 07:12:35.693915 140218947737408 spec.py:349] Evaluating on the test split.
I0211 07:13:02.175794 140218947737408 submission_runner.py:408] Time since start: 30170.06s, 	Step: 32821, 	{'train/ctc_loss': Array(0.2403235, dtype=float32), 'train/wer': 0.08057780129347765, 'validation/ctc_loss': Array(0.50835735, dtype=float32), 'validation/wer': 0.14649005088002162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28367037, dtype=float32), 'test/wer': 0.09006154408628358, 'test/num_examples': 2472, 'score': 27408.875204086304, 'total_duration': 30170.055107831955, 'accumulated_submission_time': 27408.875204086304, 'accumulated_eval_time': 2758.963734149933, 'accumulated_logging_time': 0.7755627632141113}
I0211 07:13:02.206315 140062457902848 logging_writer.py:48] [32821] accumulated_eval_time=2758.963734, accumulated_logging_time=0.775563, accumulated_submission_time=27408.875204, global_step=32821, preemption_count=0, score=27408.875204, test/ctc_loss=0.2836703658103943, test/num_examples=2472, test/wer=0.090062, total_duration=30170.055108, train/ctc_loss=0.2403234988451004, train/wer=0.080578, validation/ctc_loss=0.5083573460578918, validation/num_examples=5348, validation/wer=0.146490
I0211 07:14:02.546502 140062449510144 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.5951461791992188, loss=1.3460968732833862
I0211 07:15:22.051342 140061802542848 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.3925135135650635, loss=1.2577784061431885
I0211 07:16:41.196618 140061794150144 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.452164649963379, loss=1.2685012817382812
I0211 07:18:02.767179 140061802542848 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.8254269361495972, loss=1.2711491584777832
I0211 07:19:23.397815 140061794150144 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8367788791656494, loss=1.2470930814743042
I0211 07:20:50.986162 140061802542848 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.1882758140563965, loss=1.3350348472595215
I0211 07:22:18.833580 140061794150144 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.169059991836548, loss=1.3797919750213623
I0211 07:23:46.991374 140061802542848 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.990133762359619, loss=1.2688039541244507
I0211 07:25:11.974738 140061794150144 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.9839260578155518, loss=1.3296295404434204
I0211 07:26:37.951818 140061802542848 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.71881365776062, loss=1.2821844816207886
I0211 07:28:05.432688 140061794150144 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.024435043334961, loss=1.2797795534133911
I0211 07:29:34.863504 140062457902848 logging_writer.py:48] [34000] global_step=34000, grad_norm=6.0855712890625, loss=1.2563846111297607
I0211 07:30:51.604070 140062449510144 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.2856221199035645, loss=1.2832071781158447
I0211 07:32:10.379792 140062457902848 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.9255479574203491, loss=1.2568821907043457
I0211 07:33:28.551510 140062449510144 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.027212381362915, loss=1.3071306943893433
I0211 07:34:50.599910 140062457902848 logging_writer.py:48] [34400] global_step=34400, grad_norm=6.5896806716918945, loss=1.3098742961883545
I0211 07:36:20.566593 140062449510144 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.100632667541504, loss=1.244191288948059
I0211 07:37:02.209264 140218947737408 spec.py:321] Evaluating on the training split.
I0211 07:37:56.850333 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 07:38:49.844691 140218947737408 spec.py:349] Evaluating on the test split.
I0211 07:39:16.461825 140218947737408 submission_runner.py:408] Time since start: 31744.34s, 	Step: 34549, 	{'train/ctc_loss': Array(0.22173432, dtype=float32), 'train/wer': 0.07328132826070739, 'validation/ctc_loss': Array(0.48096427, dtype=float32), 'validation/wer': 0.13927802504417003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26772746, dtype=float32), 'test/wer': 0.08632421343407877, 'test/num_examples': 2472, 'score': 28848.791372060776, 'total_duration': 31744.340978860855, 'accumulated_submission_time': 28848.791372060776, 'accumulated_eval_time': 2893.2134778499603, 'accumulated_logging_time': 0.820319414138794}
I0211 07:39:16.489442 140062457902848 logging_writer.py:48] [34549] accumulated_eval_time=2893.213478, accumulated_logging_time=0.820319, accumulated_submission_time=28848.791372, global_step=34549, preemption_count=0, score=28848.791372, test/ctc_loss=0.26772746443748474, test/num_examples=2472, test/wer=0.086324, total_duration=31744.340979, train/ctc_loss=0.22173431515693665, train/wer=0.073281, validation/ctc_loss=0.4809642732143402, validation/num_examples=5348, validation/wer=0.139278
I0211 07:39:55.888427 140062449510144 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.6695191860198975, loss=1.2409929037094116
I0211 07:41:10.828282 140062457902848 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.102579116821289, loss=1.2399481534957886
I0211 07:42:27.637858 140062449510144 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.752070426940918, loss=1.2620497941970825
I0211 07:43:54.955868 140062457902848 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.8897006511688232, loss=1.2709543704986572
I0211 07:45:24.197842 140062449510144 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.6336984634399414, loss=1.3449610471725464
I0211 07:46:45.203676 140062457902848 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.7945027351379395, loss=1.2552366256713867
I0211 07:48:04.699321 140062449510144 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.4576737880706787, loss=1.2801377773284912
I0211 07:49:21.601525 140062457902848 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.0344438552856445, loss=1.2404472827911377
I0211 07:50:43.847397 140062449510144 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.633458375930786, loss=1.247674822807312
I0211 07:52:08.905692 140062457902848 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.6059062480926514, loss=1.236117959022522
I0211 07:53:37.572864 140062449510144 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.643599271774292, loss=1.2339915037155151
I0211 07:55:04.025303 140062457902848 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.554224729537964, loss=1.2972372770309448
I0211 07:56:32.063272 140062449510144 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.910985231399536, loss=1.259332537651062
I0211 07:58:02.050016 140062457902848 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.4160118103027344, loss=1.2899487018585205
I0211 07:59:29.549711 140062449510144 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.280582904815674, loss=1.2705353498458862
I0211 08:00:56.086585 140062457902848 logging_writer.py:48] [36100] global_step=36100, grad_norm=4.423746109008789, loss=1.1953073740005493
I0211 08:02:14.400250 140062449510144 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.6439504623413086, loss=1.3013911247253418
I0211 08:03:16.650717 140218947737408 spec.py:321] Evaluating on the training split.
I0211 08:04:14.140441 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 08:05:06.394971 140218947737408 spec.py:349] Evaluating on the test split.
I0211 08:05:32.682952 140218947737408 submission_runner.py:408] Time since start: 33320.56s, 	Step: 36283, 	{'train/ctc_loss': Array(0.16659415, dtype=float32), 'train/wer': 0.05798817174543628, 'validation/ctc_loss': Array(0.46956345, dtype=float32), 'validation/wer': 0.13483688463655058, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2593685, dtype=float32), 'test/wer': 0.08386651229866146, 'test/num_examples': 2472, 'score': 30288.866734981537, 'total_duration': 33320.562363147736, 'accumulated_submission_time': 30288.866734981537, 'accumulated_eval_time': 3029.243176460266, 'accumulated_logging_time': 0.8601865768432617}
I0211 08:05:32.714195 140062457902848 logging_writer.py:48] [36283] accumulated_eval_time=3029.243176, accumulated_logging_time=0.860187, accumulated_submission_time=30288.866735, global_step=36283, preemption_count=0, score=30288.866735, test/ctc_loss=0.25936850905418396, test/num_examples=2472, test/wer=0.083867, total_duration=33320.562363, train/ctc_loss=0.16659414768218994, train/wer=0.057988, validation/ctc_loss=0.46956345438957214, validation/num_examples=5348, validation/wer=0.134837
I0211 08:05:46.253733 140062449510144 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.584808826446533, loss=1.247099757194519
I0211 08:07:01.467793 140062457902848 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.250101327896118, loss=1.256529688835144
I0211 08:08:17.058736 140062449510144 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.5774483680725098, loss=1.2708672285079956
I0211 08:09:41.546938 140062457902848 logging_writer.py:48] [36600] global_step=36600, grad_norm=5.1392717361450195, loss=1.2696502208709717
I0211 08:11:12.566600 140062449510144 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.400268316268921, loss=1.214413046836853
I0211 08:12:37.870464 140062457902848 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.5355613231658936, loss=1.206852674484253
I0211 08:14:09.226806 140062449510144 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.8025336265563965, loss=1.2525303363800049
I0211 08:15:38.571347 140062457902848 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.633777379989624, loss=1.2542927265167236
I0211 08:17:08.074181 140061802542848 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.0206563472747803, loss=1.174370527267456
I0211 08:18:26.680286 140061794150144 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.275617837905884, loss=1.2205283641815186
I0211 08:19:47.316888 140061802542848 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.6969070434570312, loss=1.2525947093963623
I0211 08:21:06.881070 140061794150144 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.466351270675659, loss=1.2307186126708984
I0211 08:22:29.140544 140061802542848 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.292558193206787, loss=1.232094168663025
I0211 08:23:56.974989 140061794150144 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.0758793354034424, loss=1.2307697534561157
I0211 08:25:27.570156 140061802542848 logging_writer.py:48] [37700] global_step=37700, grad_norm=4.7007646560668945, loss=1.231494665145874
I0211 08:26:53.449684 140061794150144 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.83132004737854, loss=1.21212899684906
I0211 08:28:20.533169 140061802542848 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.350562810897827, loss=1.2478162050247192
I0211 08:29:33.132648 140218947737408 spec.py:321] Evaluating on the training split.
I0211 08:30:33.095766 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 08:31:25.183111 140218947737408 spec.py:349] Evaluating on the test split.
I0211 08:31:52.418803 140218947737408 submission_runner.py:408] Time since start: 34900.30s, 	Step: 37984, 	{'train/ctc_loss': Array(0.1848998, dtype=float32), 'train/wer': 0.06309792823805199, 'validation/ctc_loss': Array(0.4517904, dtype=float32), 'validation/wer': 0.13065641986155227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24856593, dtype=float32), 'test/wer': 0.07947921109824711, 'test/num_examples': 2472, 'score': 31729.2007689476, 'total_duration': 34900.29788470268, 'accumulated_submission_time': 31729.2007689476, 'accumulated_eval_time': 3168.526449203491, 'accumulated_logging_time': 0.9034137725830078}
I0211 08:31:52.450638 140061587502848 logging_writer.py:48] [37984] accumulated_eval_time=3168.526449, accumulated_logging_time=0.903414, accumulated_submission_time=31729.200769, global_step=37984, preemption_count=0, score=31729.200769, test/ctc_loss=0.2485659271478653, test/num_examples=2472, test/wer=0.079479, total_duration=34900.297885, train/ctc_loss=0.18489980697631836, train/wer=0.063098, validation/ctc_loss=0.45179039239883423, validation/num_examples=5348, validation/wer=0.130656
I0211 08:32:05.307274 140061579110144 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.434070348739624, loss=1.215354323387146
I0211 08:33:20.456372 140061587502848 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.0229995250701904, loss=1.1985982656478882
I0211 08:34:41.332920 140062457902848 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.008521556854248, loss=1.195805549621582
I0211 08:36:01.106600 140062449510144 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.2934682369232178, loss=1.2012639045715332
I0211 08:37:23.236797 140062457902848 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.5496504306793213, loss=1.1514124870300293
I0211 08:38:48.972310 140062449510144 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.9505813121795654, loss=1.1621323823928833
I0211 08:40:15.832774 140062457902848 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.0049049854278564, loss=1.2061982154846191
I0211 08:41:43.807114 140062449510144 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.9139275550842285, loss=1.2027833461761475
I0211 08:43:07.948666 140062457902848 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.4150595664978027, loss=1.2461687326431274
I0211 08:44:36.334143 140062449510144 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.773776054382324, loss=1.2069246768951416
I0211 08:46:03.644417 140062457902848 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.885612726211548, loss=1.219693899154663
I0211 08:47:31.759101 140062449510144 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.751544237136841, loss=1.2430546283721924
I0211 08:48:55.008740 140061259822848 logging_writer.py:48] [39200] global_step=39200, grad_norm=8.125043869018555, loss=1.2060655355453491
I0211 08:50:16.222344 140061251430144 logging_writer.py:48] [39300] global_step=39300, grad_norm=4.94602108001709, loss=1.1616833209991455
I0211 08:51:34.234921 140061259822848 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.6632938385009766, loss=1.1873013973236084
I0211 08:52:57.573640 140061251430144 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.4359354972839355, loss=1.1395959854125977
I0211 08:54:26.589058 140061259822848 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.0897538661956787, loss=1.2224040031433105
I0211 08:55:52.633086 140218947737408 spec.py:321] Evaluating on the training split.
I0211 08:56:47.033654 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 08:57:39.826966 140218947737408 spec.py:349] Evaluating on the test split.
I0211 08:58:06.086310 140218947737408 submission_runner.py:408] Time since start: 36473.97s, 	Step: 39697, 	{'train/ctc_loss': Array(0.23087613, dtype=float32), 'train/wer': 0.07944652668919469, 'validation/ctc_loss': Array(0.44235966, dtype=float32), 'validation/wer': 0.1274221110864381, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2419337, dtype=float32), 'test/wer': 0.07797615420551256, 'test/num_examples': 2472, 'score': 33169.299060583115, 'total_duration': 36473.96536016464, 'accumulated_submission_time': 33169.299060583115, 'accumulated_eval_time': 3301.976773738861, 'accumulated_logging_time': 0.9474256038665771}
I0211 08:58:06.117111 140061259822848 logging_writer.py:48] [39697] accumulated_eval_time=3301.976774, accumulated_logging_time=0.947426, accumulated_submission_time=33169.299061, global_step=39697, preemption_count=0, score=33169.299061, test/ctc_loss=0.2419337034225464, test/num_examples=2472, test/wer=0.077976, total_duration=36473.965360, train/ctc_loss=0.2308761328458786, train/wer=0.079447, validation/ctc_loss=0.44235965609550476, validation/num_examples=5348, validation/wer=0.127422
I0211 08:58:09.219622 140061251430144 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.897392988204956, loss=1.2224524021148682
I0211 08:59:24.248873 140061259822848 logging_writer.py:48] [39800] global_step=39800, grad_norm=5.04108190536499, loss=1.1990693807601929
I0211 09:00:39.362364 140061251430144 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.558697462081909, loss=1.193958044052124
I0211 09:02:05.463196 140061259822848 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.5262351036071777, loss=1.197662591934204
I0211 09:03:33.780110 140061251430144 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.017735242843628, loss=1.2020949125289917
I0211 09:05:01.154866 140062457902848 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.3218066692352295, loss=1.1877760887145996
I0211 09:06:17.877079 140062449510144 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.8042187690734863, loss=1.1287814378738403
I0211 09:07:39.286096 140062457902848 logging_writer.py:48] [40400] global_step=40400, grad_norm=4.323655605316162, loss=1.2132484912872314
I0211 09:09:03.928534 140062449510144 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.7032008171081543, loss=1.1324279308319092
I0211 09:10:30.637566 140062457902848 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.8644931316375732, loss=1.1543240547180176
I0211 09:11:58.456681 140062449510144 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.2654025554656982, loss=1.1610534191131592
I0211 09:13:29.381720 140062457902848 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.5540618896484375, loss=1.1614232063293457
I0211 09:14:56.641981 140062449510144 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.961439609527588, loss=1.1865816116333008
I0211 09:16:24.122771 140062457902848 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.4184019565582275, loss=1.2015053033828735
I0211 09:17:53.558925 140062449510144 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.224513292312622, loss=1.1861705780029297
I0211 09:19:21.005907 140061587502848 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.109628915786743, loss=1.1167799234390259
I0211 09:20:38.687929 140061579110144 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.2641186714172363, loss=1.196205973625183
I0211 09:21:56.395064 140061587502848 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.0412375926971436, loss=1.2578672170639038
I0211 09:22:06.094365 140218947737408 spec.py:321] Evaluating on the training split.
I0211 09:23:01.018594 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 09:23:53.364265 140218947737408 spec.py:349] Evaluating on the test split.
I0211 09:24:20.279435 140218947737408 submission_runner.py:408] Time since start: 38048.16s, 	Step: 41414, 	{'train/ctc_loss': Array(0.2321512, dtype=float32), 'train/wer': 0.07981702815079633, 'validation/ctc_loss': Array(0.4297312, dtype=float32), 'validation/wer': 0.12318371839307954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23373564, dtype=float32), 'test/wer': 0.07562001096825301, 'test/num_examples': 2472, 'score': 34609.19066643715, 'total_duration': 38048.15858435631, 'accumulated_submission_time': 34609.19066643715, 'accumulated_eval_time': 3436.159049510956, 'accumulated_logging_time': 0.9893152713775635}
I0211 09:24:20.310187 140062611502848 logging_writer.py:48] [41414] accumulated_eval_time=3436.159050, accumulated_logging_time=0.989315, accumulated_submission_time=34609.190666, global_step=41414, preemption_count=0, score=34609.190666, test/ctc_loss=0.23373563587665558, test/num_examples=2472, test/wer=0.075620, total_duration=38048.158584, train/ctc_loss=0.23215119540691376, train/wer=0.079817, validation/ctc_loss=0.42973119020462036, validation/num_examples=5348, validation/wer=0.123184
I0211 09:25:26.456208 140062603110144 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.2958927154541016, loss=1.2034014463424683
I0211 09:26:41.726272 140062611502848 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.26141095161438, loss=1.1598289012908936
I0211 09:27:58.331106 140062603110144 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.2263734340667725, loss=1.1285042762756348
I0211 09:29:27.219280 140062611502848 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.8748908042907715, loss=1.2124577760696411
I0211 09:30:55.994452 140062603110144 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.4221651554107666, loss=1.1373851299285889
I0211 09:32:23.477521 140062611502848 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.3089523315429688, loss=1.1402026414871216
I0211 09:33:51.960868 140062603110144 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.1597700119018555, loss=1.175081729888916
I0211 09:35:19.437612 140062611502848 logging_writer.py:48] [42200] global_step=42200, grad_norm=4.058865070343018, loss=1.167694330215454
I0211 09:36:42.596304 140061300782848 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.812941312789917, loss=1.1436245441436768
I0211 09:38:00.287174 140061292390144 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.165344476699829, loss=1.1927517652511597
I0211 09:39:20.931011 140061300782848 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.245518922805786, loss=1.1474745273590088
I0211 09:40:41.162128 140061292390144 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.7622523307800293, loss=1.1586377620697021
I0211 09:42:07.834169 140061300782848 logging_writer.py:48] [42700] global_step=42700, grad_norm=4.568808078765869, loss=1.1824748516082764
I0211 09:43:35.921420 140061292390144 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.250133752822876, loss=1.157419204711914
I0211 09:45:05.938470 140061300782848 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.2197189331054688, loss=1.1182459592819214
I0211 09:46:36.522258 140061292390144 logging_writer.py:48] [43000] global_step=43000, grad_norm=4.188192844390869, loss=1.1121429204940796
I0211 09:48:05.950098 140061300782848 logging_writer.py:48] [43100] global_step=43100, grad_norm=4.9621477127075195, loss=1.195763111114502
I0211 09:48:20.584982 140218947737408 spec.py:321] Evaluating on the training split.
I0211 09:49:13.166664 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 09:50:06.975466 140218947737408 spec.py:349] Evaluating on the test split.
I0211 09:50:33.966465 140218947737408 submission_runner.py:408] Time since start: 39621.85s, 	Step: 43118, 	{'train/ctc_loss': Array(0.26301807, dtype=float32), 'train/wer': 0.09102608349761043, 'validation/ctc_loss': Array(0.42406932, dtype=float32), 'validation/wer': 0.12240169149521613, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22829898, dtype=float32), 'test/wer': 0.07310137509394106, 'test/num_examples': 2472, 'score': 36049.38084578514, 'total_duration': 39621.84571695328, 'accumulated_submission_time': 36049.38084578514, 'accumulated_eval_time': 3569.5378217697144, 'accumulated_logging_time': 1.0312442779541016}
I0211 09:50:33.997053 140062611502848 logging_writer.py:48] [43118] accumulated_eval_time=3569.537822, accumulated_logging_time=1.031244, accumulated_submission_time=36049.380846, global_step=43118, preemption_count=0, score=36049.380846, test/ctc_loss=0.22829897701740265, test/num_examples=2472, test/wer=0.073101, total_duration=39621.845717, train/ctc_loss=0.26301807165145874, train/wer=0.091026, validation/ctc_loss=0.4240693151950836, validation/num_examples=5348, validation/wer=0.122402
I0211 09:51:36.659029 140062603110144 logging_writer.py:48] [43200] global_step=43200, grad_norm=5.130556106567383, loss=1.131678819656372
I0211 09:52:56.446524 140062611502848 logging_writer.py:48] [43300] global_step=43300, grad_norm=4.496859550476074, loss=1.1526048183441162
I0211 09:54:13.655130 140062603110144 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.111443042755127, loss=1.1440541744232178
I0211 09:55:32.841404 140062611502848 logging_writer.py:48] [43500] global_step=43500, grad_norm=6.1589765548706055, loss=1.202173113822937
I0211 09:56:50.620793 140062603110144 logging_writer.py:48] [43600] global_step=43600, grad_norm=9.224069595336914, loss=1.199474573135376
I0211 09:58:17.810161 140062611502848 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.5045440196990967, loss=1.107608437538147
I0211 09:59:48.023952 140062603110144 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.72027587890625, loss=1.1205768585205078
I0211 10:01:18.105521 140062611502848 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.2079460620880127, loss=1.1173007488250732
I0211 10:02:46.784661 140062603110144 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.3367016315460205, loss=1.094144344329834
I0211 10:04:15.945373 140062611502848 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.6641998291015625, loss=1.1568326950073242
I0211 10:05:43.082320 140062603110144 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.9537553787231445, loss=1.1634454727172852
I0211 10:07:14.828897 140062611502848 logging_writer.py:48] [44300] global_step=44300, grad_norm=5.8085527420043945, loss=1.1319669485092163
I0211 10:08:31.603850 140062603110144 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.5748367309570312, loss=1.1326024532318115
I0211 10:09:49.790788 140062611502848 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.393129348754883, loss=1.0738695859909058
I0211 10:11:11.785523 140062603110144 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.3346240520477295, loss=1.118955135345459
I0211 10:12:36.340718 140062611502848 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.399390697479248, loss=1.1624116897583008
I0211 10:14:05.926704 140062603110144 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.620269536972046, loss=1.1648201942443848
I0211 10:14:34.401104 140218947737408 spec.py:321] Evaluating on the training split.
I0211 10:15:27.260689 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 10:16:19.218424 140218947737408 spec.py:349] Evaluating on the test split.
I0211 10:16:46.278222 140218947737408 submission_runner.py:408] Time since start: 41194.15s, 	Step: 44835, 	{'train/ctc_loss': Array(0.23237117, dtype=float32), 'train/wer': 0.07707434250422435, 'validation/ctc_loss': Array(0.42018595, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22612567, dtype=float32), 'test/wer': 0.07245140454573153, 'test/num_examples': 2472, 'score': 37489.69993138313, 'total_duration': 41194.15355873108, 'accumulated_submission_time': 37489.69993138313, 'accumulated_eval_time': 3701.4083173274994, 'accumulated_logging_time': 1.0733106136322021}
I0211 10:16:46.323621 140063041582848 logging_writer.py:48] [44835] accumulated_eval_time=3701.408317, accumulated_logging_time=1.073311, accumulated_submission_time=37489.699931, global_step=44835, preemption_count=0, score=37489.699931, test/ctc_loss=0.22612567245960236, test/num_examples=2472, test/wer=0.072451, total_duration=41194.153559, train/ctc_loss=0.23237116634845734, train/wer=0.077074, validation/ctc_loss=0.42018595337867737, validation/num_examples=5348, validation/wer=0.120345
I0211 10:17:35.685956 140063033190144 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.2285830974578857, loss=1.155534029006958
I0211 10:18:50.666326 140063041582848 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.7775259017944336, loss=1.168104887008667
I0211 10:20:15.591598 140063033190144 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.0768182277679443, loss=1.1514112949371338
I0211 10:21:45.177343 140063041582848 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.714766502380371, loss=1.1463910341262817
I0211 10:23:13.252351 140063033190144 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.433567523956299, loss=1.1295158863067627
I0211 10:24:35.775671 140063041582848 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.9976439476013184, loss=1.1709120273590088
I0211 10:25:51.952415 140063033190144 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.7873148918151855, loss=1.1663051843643188
I0211 10:27:11.823160 140063041582848 logging_writer.py:48] [45600] global_step=45600, grad_norm=4.415587902069092, loss=1.1503331661224365
I0211 10:28:35.878129 140063033190144 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.0509510040283203, loss=1.0464626550674438
I0211 10:30:01.168292 140063041582848 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.512592315673828, loss=1.059141755104065
I0211 10:31:29.331169 140063033190144 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.452687740325928, loss=1.1613236665725708
I0211 10:32:59.641278 140063041582848 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.836386203765869, loss=1.1509556770324707
I0211 10:34:29.251326 140063033190144 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.5655813217163086, loss=1.1202069520950317
I0211 10:35:59.091155 140063041582848 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.6499106884002686, loss=1.1066709756851196
I0211 10:37:23.877445 140063033190144 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.5941996574401855, loss=1.1597105264663696
I0211 10:38:48.766475 140062386222848 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.4432270526885986, loss=1.1792765855789185
I0211 10:40:06.292901 140062377830144 logging_writer.py:48] [46500] global_step=46500, grad_norm=4.3684773445129395, loss=1.152573585510254
I0211 10:40:46.287174 140218947737408 spec.py:321] Evaluating on the training split.
I0211 10:41:40.588153 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 10:42:32.525077 140218947737408 spec.py:349] Evaluating on the test split.
I0211 10:42:59.233532 140218947737408 submission_runner.py:408] Time since start: 42767.11s, 	Step: 46551, 	{'train/ctc_loss': Array(0.21373685, dtype=float32), 'train/wer': 0.07420417979579794, 'validation/ctc_loss': Array(0.41774118, dtype=float32), 'validation/wer': 0.11968873398534424, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22410408, dtype=float32), 'test/wer': 0.0715983182012065, 'test/num_examples': 2472, 'score': 38929.57273769379, 'total_duration': 42767.10988521576, 'accumulated_submission_time': 38929.57273769379, 'accumulated_eval_time': 3834.3490607738495, 'accumulated_logging_time': 1.1359052658081055}
I0211 10:42:59.274270 140062094382848 logging_writer.py:48] [46551] accumulated_eval_time=3834.349061, accumulated_logging_time=1.135905, accumulated_submission_time=38929.572738, global_step=46551, preemption_count=0, score=38929.572738, test/ctc_loss=0.22410407662391663, test/num_examples=2472, test/wer=0.071598, total_duration=42767.109885, train/ctc_loss=0.21373684704303741, train/wer=0.074204, validation/ctc_loss=0.41774117946624756, validation/num_examples=5348, validation/wer=0.119689
I0211 10:43:36.945577 140062085990144 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.9772000312805176, loss=1.1293185949325562
I0211 10:44:52.412827 140062094382848 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.7484893798828125, loss=1.2045397758483887
I0211 10:46:07.902665 140062085990144 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.3085196018218994, loss=1.1463292837142944
I0211 10:47:34.892121 140062094382848 logging_writer.py:48] [46900] global_step=46900, grad_norm=4.848760604858398, loss=1.1683152914047241
I0211 10:49:01.929499 140062085990144 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.1322779655456543, loss=1.111593246459961
I0211 10:50:32.310536 140062094382848 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.3363118171691895, loss=1.1528900861740112
I0211 10:52:01.188670 140062085990144 logging_writer.py:48] [47200] global_step=47200, grad_norm=10.579948425292969, loss=1.136134147644043
I0211 10:53:33.192970 140062094382848 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.6855063438415527, loss=1.1498421430587769
I0211 10:55:04.350938 140062094382848 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.5297744274139404, loss=1.1221874952316284
I0211 10:56:20.791851 140062085990144 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.458284616470337, loss=1.0979084968566895
I0211 10:57:37.560208 140062094382848 logging_writer.py:48] [47600] global_step=47600, grad_norm=5.255960464477539, loss=1.1370599269866943
I0211 10:58:58.209549 140062085990144 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.0348927974700928, loss=1.1438888311386108
I0211 11:00:21.888809 140062094382848 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.9331483840942383, loss=1.0789859294891357
I0211 11:01:50.716473 140062085990144 logging_writer.py:48] [47900] global_step=47900, grad_norm=4.222039222717285, loss=1.1145975589752197
I0211 11:03:19.115672 140218947737408 spec.py:321] Evaluating on the training split.
I0211 11:04:13.977944 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 11:05:06.051992 140218947737408 spec.py:349] Evaluating on the test split.
I0211 11:05:33.778812 140218947737408 submission_runner.py:408] Time since start: 44121.66s, 	Step: 48000, 	{'train/ctc_loss': Array(0.19231977, dtype=float32), 'train/wer': 0.06766794636157, 'validation/ctc_loss': Array(0.41810927, dtype=float32), 'validation/wer': 0.11975631655676454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22410974, dtype=float32), 'test/wer': 0.07137489082525948, 'test/num_examples': 2472, 'score': 40149.33516192436, 'total_duration': 44121.655792713165, 'accumulated_submission_time': 40149.33516192436, 'accumulated_eval_time': 3969.007215976715, 'accumulated_logging_time': 1.1929059028625488}
I0211 11:05:33.823504 140063041582848 logging_writer.py:48] [48000] accumulated_eval_time=3969.007216, accumulated_logging_time=1.192906, accumulated_submission_time=40149.335162, global_step=48000, preemption_count=0, score=40149.335162, test/ctc_loss=0.2241097390651703, test/num_examples=2472, test/wer=0.071375, total_duration=44121.655793, train/ctc_loss=0.19231976568698883, train/wer=0.067668, validation/ctc_loss=0.418109267950058, validation/num_examples=5348, validation/wer=0.119756
I0211 11:05:33.849756 140063033190144 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40149.335162
I0211 11:05:34.005206 140218947737408 checkpoints.py:490] Saving checkpoint at step: 48000
I0211 11:05:34.935841 140218947737408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1/checkpoint_48000
I0211 11:05:34.954636 140218947737408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_1/checkpoint_48000.
I0211 11:05:36.042664 140218947737408 submission_runner.py:583] Tuning trial 1/5
I0211 11:05:36.042913 140218947737408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0211 11:05:36.055315 140218947737408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(29.405813, dtype=float32), 'train/wer': 2.4046810903108926, 'validation/ctc_loss': Array(28.149015, dtype=float32), 'validation/wer': 2.1852631375691516, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.255945, dtype=float32), 'test/wer': 2.343875043162107, 'test/num_examples': 2472, 'score': 43.85169339179993, 'total_duration': 248.746901512146, 'accumulated_submission_time': 43.85169339179993, 'accumulated_eval_time': 204.89513993263245, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1737, {'train/ctc_loss': Array(6.6251674, dtype=float32), 'train/wer': 0.9446252394389405, 'validation/ctc_loss': Array(6.4828987, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.369111, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1483.9563069343567, 'total_duration': 1799.4650785923004, 'accumulated_submission_time': 1483.9563069343567, 'accumulated_eval_time': 315.39243960380554, 'accumulated_logging_time': 0.04180431365966797, 'global_step': 1737, 'preemption_count': 0}), (3504, {'train/ctc_loss': Array(3.5062532, dtype=float32), 'train/wer': 0.7003595876728018, 'validation/ctc_loss': Array(3.887965, dtype=float32), 'validation/wer': 0.7383395927667339, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.4329967, dtype=float32), 'test/wer': 0.6812706924217496, 'test/num_examples': 2472, 'score': 2924.2540802955627, 'total_duration': 3360.9410836696625, 'accumulated_submission_time': 2924.2540802955627, 'accumulated_eval_time': 436.4544777870178, 'accumulated_logging_time': 0.08056855201721191, 'global_step': 3504, 'preemption_count': 0}), (5218, {'train/ctc_loss': Array(0.7218365, dtype=float32), 'train/wer': 0.23713964925597356, 'validation/ctc_loss': Array(1.1679587, dtype=float32), 'validation/wer': 0.3178311787365921, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.794886, dtype=float32), 'test/wer': 0.24412487559157475, 'test/num_examples': 2472, 'score': 4364.524667263031, 'total_duration': 4942.343545675278, 'accumulated_submission_time': 4364.524667263031, 'accumulated_eval_time': 577.4759426116943, 'accumulated_logging_time': 0.11744523048400879, 'global_step': 5218, 'preemption_count': 0}), (6930, {'train/ctc_loss': Array(0.5146038, dtype=float32), 'train/wer': 0.17154331912576318, 'validation/ctc_loss': Array(0.8652903, dtype=float32), 'validation/wer': 0.2476997789084449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5532459, dtype=float32), 'test/wer': 0.17837629232425406, 'test/num_examples': 2472, 'score': 5804.555928230286, 'total_duration': 6520.837166547775, 'accumulated_submission_time': 5804.555928230286, 'accumulated_eval_time': 715.8265788555145, 'accumulated_logging_time': 0.1547858715057373, 'global_step': 6930, 'preemption_count': 0}), (8665, {'train/ctc_loss': Array(0.46659666, dtype=float32), 'train/wer': 0.15698862771532324, 'validation/ctc_loss': Array(0.8137787, dtype=float32), 'validation/wer': 0.23474323450186818, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5063116, dtype=float32), 'test/wer': 0.16480815713038002, 'test/num_examples': 2472, 'score': 7245.780788183212, 'total_duration': 8105.636641263962, 'accumulated_submission_time': 7245.780788183212, 'accumulated_eval_time': 859.2827861309052, 'accumulated_logging_time': 0.1961219310760498, 'global_step': 8665, 'preemption_count': 0}), (10391, {'train/ctc_loss': Array(0.43561387, dtype=float32), 'train/wer': 0.14434705603064268, 'validation/ctc_loss': Array(0.7429956, dtype=float32), 'validation/wer': 0.2146808654431003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45595506, dtype=float32), 'test/wer': 0.14790892287693214, 'test/num_examples': 2472, 'score': 8685.838481426239, 'total_duration': 9678.446252822876, 'accumulated_submission_time': 8685.838481426239, 'accumulated_eval_time': 991.9161324501038, 'accumulated_logging_time': 0.239854097366333, 'global_step': 10391, 'preemption_count': 0}), (12124, {'train/ctc_loss': Array(0.39994377, dtype=float32), 'train/wer': 0.13405334694658924, 'validation/ctc_loss': Array(0.71666336, dtype=float32), 'validation/wer': 0.20626200797474342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44069472, dtype=float32), 'test/wer': 0.14283102796904515, 'test/num_examples': 2472, 'score': 10125.729723930359, 'total_duration': 11252.399189710617, 'accumulated_submission_time': 10125.729723930359, 'accumulated_eval_time': 1125.8630819320679, 'accumulated_logging_time': 0.27825331687927246, 'global_step': 12124, 'preemption_count': 0}), (13863, {'train/ctc_loss': Array(0.34301874, dtype=float32), 'train/wer': 0.11864063028159859, 'validation/ctc_loss': Array(0.683678, dtype=float32), 'validation/wer': 0.19800727960840728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41449788, dtype=float32), 'test/wer': 0.1320862023439563, 'test/num_examples': 2472, 'score': 11567.119065999985, 'total_duration': 12830.857915639877, 'accumulated_submission_time': 11567.119065999985, 'accumulated_eval_time': 1262.8164167404175, 'accumulated_logging_time': 0.31701135635375977, 'global_step': 13863, 'preemption_count': 0}), (15594, {'train/ctc_loss': Array(0.31959072, dtype=float32), 'train/wer': 0.10852853396038709, 'validation/ctc_loss': Array(0.67307526, dtype=float32), 'validation/wer': 0.19473435222105293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39703676, dtype=float32), 'test/wer': 0.12692706111754312, 'test/num_examples': 2472, 'score': 13007.510919809341, 'total_duration': 14409.774055480957, 'accumulated_submission_time': 13007.510919809341, 'accumulated_eval_time': 1401.224974155426, 'accumulated_logging_time': 0.3565559387207031, 'global_step': 15594, 'preemption_count': 0}), (17329, {'train/ctc_loss': Array(0.32896024, dtype=float32), 'train/wer': 0.11624040714979057, 'validation/ctc_loss': Array(0.6574222, dtype=float32), 'validation/wer': 0.1909786921806965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39617884, dtype=float32), 'test/wer': 0.1280238864176467, 'test/num_examples': 2472, 'score': 14447.46181845665, 'total_duration': 15993.21812081337, 'accumulated_submission_time': 14447.46181845665, 'accumulated_eval_time': 1544.6018645763397, 'accumulated_logging_time': 0.3955864906311035, 'global_step': 17329, 'preemption_count': 0}), (19065, {'train/ctc_loss': Array(0.32634243, dtype=float32), 'train/wer': 0.10938417197704857, 'validation/ctc_loss': Array(0.6247462, dtype=float32), 'validation/wer': 0.1807544145901117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3724943, dtype=float32), 'test/wer': 0.12136168829849897, 'test/num_examples': 2472, 'score': 15887.366396427155, 'total_duration': 17567.369089365005, 'accumulated_submission_time': 15887.366396427155, 'accumulated_eval_time': 1678.731039762497, 'accumulated_logging_time': 0.43723106384277344, 'global_step': 19065, 'preemption_count': 0}), (20805, {'train/ctc_loss': Array(0.31835115, dtype=float32), 'train/wer': 0.10723085445959848, 'validation/ctc_loss': Array(0.6072395, dtype=float32), 'validation/wer': 0.17523195303976752, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35500604, dtype=float32), 'test/wer': 0.11638535128876973, 'test/num_examples': 2472, 'score': 17327.25873041153, 'total_duration': 19143.011053800583, 'accumulated_submission_time': 17327.25873041153, 'accumulated_eval_time': 1814.3625190258026, 'accumulated_logging_time': 0.47852015495300293, 'global_step': 20805, 'preemption_count': 0}), (22510, {'train/ctc_loss': Array(0.29313225, dtype=float32), 'train/wer': 0.09935178516308261, 'validation/ctc_loss': Array(0.5852561, dtype=float32), 'validation/wer': 0.16853162381609818, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34486303, dtype=float32), 'test/wer': 0.11037312371783153, 'test/num_examples': 2472, 'score': 18767.391078948975, 'total_duration': 20718.743278980255, 'accumulated_submission_time': 18767.391078948975, 'accumulated_eval_time': 1949.8465313911438, 'accumulated_logging_time': 0.5190324783325195, 'global_step': 22510, 'preemption_count': 0}), (24245, {'train/ctc_loss': Array(0.27176955, dtype=float32), 'train/wer': 0.09307567248004729, 'validation/ctc_loss': Array(0.57057077, dtype=float32), 'validation/wer': 0.164611834673721, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33417776, dtype=float32), 'test/wer': 0.10590457619889099, 'test/num_examples': 2472, 'score': 20207.345369815826, 'total_duration': 22293.72416448593, 'accumulated_submission_time': 20207.345369815826, 'accumulated_eval_time': 2084.754161596298, 'accumulated_logging_time': 0.5621540546417236, 'global_step': 24245, 'preemption_count': 0}), (25965, {'train/ctc_loss': Array(0.2505488, dtype=float32), 'train/wer': 0.08905813923159185, 'validation/ctc_loss': Array(0.5587426, dtype=float32), 'validation/wer': 0.16262297614335228, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31846216, dtype=float32), 'test/wer': 0.10462494668210347, 'test/num_examples': 2472, 'score': 21647.663966178894, 'total_duration': 23867.22459101677, 'accumulated_submission_time': 21647.663966178894, 'accumulated_eval_time': 2217.822919368744, 'accumulated_logging_time': 0.6007964611053467, 'global_step': 25965, 'preemption_count': 0}), (27671, {'train/ctc_loss': Array(0.2422054, dtype=float32), 'train/wer': 0.08290801976046067, 'validation/ctc_loss': Array(0.5420699, dtype=float32), 'validation/wer': 0.1576701391235506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31094378, dtype=float32), 'test/wer': 0.10070481181321471, 'test/num_examples': 2472, 'score': 23087.620130300522, 'total_duration': 25441.73743915558, 'accumulated_submission_time': 23087.620130300522, 'accumulated_eval_time': 2352.263339281082, 'accumulated_logging_time': 0.6415128707885742, 'global_step': 27671, 'preemption_count': 0}), (29371, {'train/ctc_loss': Array(0.25566694, dtype=float32), 'train/wer': 0.08810560755763563, 'validation/ctc_loss': Array(0.52995616, dtype=float32), 'validation/wer': 0.15467719667493748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30119932, dtype=float32), 'test/wer': 0.09595190217943249, 'test/num_examples': 2472, 'score': 24528.120047092438, 'total_duration': 27020.380325078964, 'accumulated_submission_time': 24528.120047092438, 'accumulated_eval_time': 2490.2884378433228, 'accumulated_logging_time': 0.685178279876709, 'global_step': 29371, 'preemption_count': 0}), (31104, {'train/ctc_loss': Array(0.23302117, dtype=float32), 'train/wer': 0.07783547387668951, 'validation/ctc_loss': Array(0.51413625, dtype=float32), 'validation/wer': 0.1478996302267878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29052296, dtype=float32), 'test/wer': 0.09227550626612231, 'test/num_examples': 2472, 'score': 25968.7009601593, 'total_duration': 28593.22857093811, 'accumulated_submission_time': 25968.7009601593, 'accumulated_eval_time': 2622.43061876297, 'accumulated_logging_time': 0.7326092720031738, 'global_step': 31104, 'preemption_count': 0}), (32821, {'train/ctc_loss': Array(0.2403235, dtype=float32), 'train/wer': 0.08057780129347765, 'validation/ctc_loss': Array(0.50835735, dtype=float32), 'validation/wer': 0.14649005088002162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28367037, dtype=float32), 'test/wer': 0.09006154408628358, 'test/num_examples': 2472, 'score': 27408.875204086304, 'total_duration': 30170.055107831955, 'accumulated_submission_time': 27408.875204086304, 'accumulated_eval_time': 2758.963734149933, 'accumulated_logging_time': 0.7755627632141113, 'global_step': 32821, 'preemption_count': 0}), (34549, {'train/ctc_loss': Array(0.22173432, dtype=float32), 'train/wer': 0.07328132826070739, 'validation/ctc_loss': Array(0.48096427, dtype=float32), 'validation/wer': 0.13927802504417003, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26772746, dtype=float32), 'test/wer': 0.08632421343407877, 'test/num_examples': 2472, 'score': 28848.791372060776, 'total_duration': 31744.340978860855, 'accumulated_submission_time': 28848.791372060776, 'accumulated_eval_time': 2893.2134778499603, 'accumulated_logging_time': 0.820319414138794, 'global_step': 34549, 'preemption_count': 0}), (36283, {'train/ctc_loss': Array(0.16659415, dtype=float32), 'train/wer': 0.05798817174543628, 'validation/ctc_loss': Array(0.46956345, dtype=float32), 'validation/wer': 0.13483688463655058, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2593685, dtype=float32), 'test/wer': 0.08386651229866146, 'test/num_examples': 2472, 'score': 30288.866734981537, 'total_duration': 33320.562363147736, 'accumulated_submission_time': 30288.866734981537, 'accumulated_eval_time': 3029.243176460266, 'accumulated_logging_time': 0.8601865768432617, 'global_step': 36283, 'preemption_count': 0}), (37984, {'train/ctc_loss': Array(0.1848998, dtype=float32), 'train/wer': 0.06309792823805199, 'validation/ctc_loss': Array(0.4517904, dtype=float32), 'validation/wer': 0.13065641986155227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24856593, dtype=float32), 'test/wer': 0.07947921109824711, 'test/num_examples': 2472, 'score': 31729.2007689476, 'total_duration': 34900.29788470268, 'accumulated_submission_time': 31729.2007689476, 'accumulated_eval_time': 3168.526449203491, 'accumulated_logging_time': 0.9034137725830078, 'global_step': 37984, 'preemption_count': 0}), (39697, {'train/ctc_loss': Array(0.23087613, dtype=float32), 'train/wer': 0.07944652668919469, 'validation/ctc_loss': Array(0.44235966, dtype=float32), 'validation/wer': 0.1274221110864381, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2419337, dtype=float32), 'test/wer': 0.07797615420551256, 'test/num_examples': 2472, 'score': 33169.299060583115, 'total_duration': 36473.96536016464, 'accumulated_submission_time': 33169.299060583115, 'accumulated_eval_time': 3301.976773738861, 'accumulated_logging_time': 0.9474256038665771, 'global_step': 39697, 'preemption_count': 0}), (41414, {'train/ctc_loss': Array(0.2321512, dtype=float32), 'train/wer': 0.07981702815079633, 'validation/ctc_loss': Array(0.4297312, dtype=float32), 'validation/wer': 0.12318371839307954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23373564, dtype=float32), 'test/wer': 0.07562001096825301, 'test/num_examples': 2472, 'score': 34609.19066643715, 'total_duration': 38048.15858435631, 'accumulated_submission_time': 34609.19066643715, 'accumulated_eval_time': 3436.159049510956, 'accumulated_logging_time': 0.9893152713775635, 'global_step': 41414, 'preemption_count': 0}), (43118, {'train/ctc_loss': Array(0.26301807, dtype=float32), 'train/wer': 0.09102608349761043, 'validation/ctc_loss': Array(0.42406932, dtype=float32), 'validation/wer': 0.12240169149521613, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22829898, dtype=float32), 'test/wer': 0.07310137509394106, 'test/num_examples': 2472, 'score': 36049.38084578514, 'total_duration': 39621.84571695328, 'accumulated_submission_time': 36049.38084578514, 'accumulated_eval_time': 3569.5378217697144, 'accumulated_logging_time': 1.0312442779541016, 'global_step': 43118, 'preemption_count': 0}), (44835, {'train/ctc_loss': Array(0.23237117, dtype=float32), 'train/wer': 0.07707434250422435, 'validation/ctc_loss': Array(0.42018595, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22612567, dtype=float32), 'test/wer': 0.07245140454573153, 'test/num_examples': 2472, 'score': 37489.69993138313, 'total_duration': 41194.15355873108, 'accumulated_submission_time': 37489.69993138313, 'accumulated_eval_time': 3701.4083173274994, 'accumulated_logging_time': 1.0733106136322021, 'global_step': 44835, 'preemption_count': 0}), (46551, {'train/ctc_loss': Array(0.21373685, dtype=float32), 'train/wer': 0.07420417979579794, 'validation/ctc_loss': Array(0.41774118, dtype=float32), 'validation/wer': 0.11968873398534424, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22410408, dtype=float32), 'test/wer': 0.0715983182012065, 'test/num_examples': 2472, 'score': 38929.57273769379, 'total_duration': 42767.10988521576, 'accumulated_submission_time': 38929.57273769379, 'accumulated_eval_time': 3834.3490607738495, 'accumulated_logging_time': 1.1359052658081055, 'global_step': 46551, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.19231977, dtype=float32), 'train/wer': 0.06766794636157, 'validation/ctc_loss': Array(0.41810927, dtype=float32), 'validation/wer': 0.11975631655676454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22410974, dtype=float32), 'test/wer': 0.07137489082525948, 'test/num_examples': 2472, 'score': 40149.33516192436, 'total_duration': 44121.655792713165, 'accumulated_submission_time': 40149.33516192436, 'accumulated_eval_time': 3969.007215976715, 'accumulated_logging_time': 1.1929059028625488, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0211 11:05:36.055541 140218947737408 submission_runner.py:586] Timing: 40149.33516192436
I0211 11:05:36.055628 140218947737408 submission_runner.py:588] Total number of evals: 29
I0211 11:05:36.055690 140218947737408 submission_runner.py:589] ====================
I0211 11:05:36.055753 140218947737408 submission_runner.py:542] Using RNG seed 2622380006
I0211 11:05:36.059002 140218947737408 submission_runner.py:551] --- Tuning run 2/5 ---
I0211 11:05:36.059135 140218947737408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2.
I0211 11:05:36.060712 140218947737408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2/hparams.json.
I0211 11:05:36.062752 140218947737408 submission_runner.py:206] Initializing dataset.
I0211 11:05:36.062869 140218947737408 submission_runner.py:213] Initializing model.
I0211 11:05:37.199486 140218947737408 submission_runner.py:255] Initializing optimizer.
I0211 11:05:37.342252 140218947737408 submission_runner.py:262] Initializing metrics bundle.
I0211 11:05:37.342435 140218947737408 submission_runner.py:280] Initializing checkpoint and logger.
I0211 11:05:37.444000 140218947737408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2 with prefix checkpoint_
I0211 11:05:37.444141 140218947737408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2/meta_data_0.json.
I0211 11:05:37.444470 140218947737408 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 11:05:37.444553 140218947737408 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 11:05:38.009978 140218947737408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 11:05:38.473279 140218947737408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2/flags_0.json.
I0211 11:05:38.602356 140218947737408 submission_runner.py:314] Starting training loop.
I0211 11:05:38.605810 140218947737408 input_pipeline.py:20] Loading split = train-clean-100
I0211 11:05:38.652784 140218947737408 input_pipeline.py:20] Loading split = train-clean-360
I0211 11:05:39.265165 140218947737408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 11:05:54.040618 140062088259328 logging_writer.py:48] [0] global_step=0, grad_norm=22.45417022705078, loss=33.534976959228516
I0211 11:05:54.058316 140218947737408 spec.py:321] Evaluating on the training split.
I0211 11:07:09.853332 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 11:08:05.285249 140218947737408 spec.py:349] Evaluating on the test split.
I0211 11:08:33.076361 140218947737408 submission_runner.py:408] Time since start: 174.47s, 	Step: 1, 	{'train/ctc_loss': Array(29.524904, dtype=float32), 'train/wer': 2.289365492296078, 'validation/ctc_loss': Array(28.14923, dtype=float32), 'validation/wer': 2.1855334678548326, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256151, dtype=float32), 'test/wer': 2.344362521073264, 'test/num_examples': 2472, 'score': 15.45589280128479, 'total_duration': 174.47113847732544, 'accumulated_submission_time': 15.45589280128479, 'accumulated_eval_time': 159.01518487930298, 'accumulated_logging_time': 0}
I0211 11:08:33.093308 140062317393664 logging_writer.py:48] [1] accumulated_eval_time=159.015185, accumulated_logging_time=0, accumulated_submission_time=15.455893, global_step=1, preemption_count=0, score=15.455893, test/ctc_loss=28.25615119934082, test/num_examples=2472, test/wer=2.344363, total_duration=174.471138, train/ctc_loss=29.524904251098633, train/wer=2.289365, validation/ctc_loss=28.149229049682617, validation/num_examples=5348, validation/wer=2.185533
I0211 11:09:57.916582 140062233466624 logging_writer.py:48] [100] global_step=100, grad_norm=2.757760524749756, loss=7.383606910705566
I0211 11:11:15.501170 140062241859328 logging_writer.py:48] [200] global_step=200, grad_norm=0.7949990034103394, loss=5.957589626312256
I0211 11:12:32.816643 140062233466624 logging_writer.py:48] [300] global_step=300, grad_norm=0.8565822243690491, loss=5.866565227508545
I0211 11:13:49.504323 140062241859328 logging_writer.py:48] [400] global_step=400, grad_norm=1.0740550756454468, loss=5.791781902313232
I0211 11:15:12.335428 140062233466624 logging_writer.py:48] [500] global_step=500, grad_norm=1.607387661933899, loss=5.743721008300781
I0211 11:16:44.273885 140062241859328 logging_writer.py:48] [600] global_step=600, grad_norm=0.585296630859375, loss=5.606692790985107
I0211 11:18:10.633886 140062233466624 logging_writer.py:48] [700] global_step=700, grad_norm=0.5536635518074036, loss=5.47432279586792
I0211 11:19:37.034538 140062241859328 logging_writer.py:48] [800] global_step=800, grad_norm=1.5316026210784912, loss=5.231395244598389
I0211 11:21:04.616904 140062233466624 logging_writer.py:48] [900] global_step=900, grad_norm=1.6449729204177856, loss=4.774050235748291
I0211 11:22:35.936435 140062241859328 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2343004941940308, loss=4.331862449645996
I0211 11:23:59.877732 140062317393664 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5018227100372314, loss=3.9412152767181396
I0211 11:25:18.782136 140062309000960 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.291452407836914, loss=3.672684669494629
I0211 11:26:40.818042 140062317393664 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.1964688301086426, loss=3.488034248352051
I0211 11:28:07.610893 140062309000960 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7445439100265503, loss=3.3404645919799805
I0211 11:29:33.984789 140062317393664 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.117377519607544, loss=3.149235725402832
I0211 11:31:05.282324 140062309000960 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.0232574939727783, loss=3.0987749099731445
I0211 11:32:34.142621 140218947737408 spec.py:321] Evaluating on the training split.
I0211 11:33:15.105597 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 11:34:03.898198 140218947737408 spec.py:349] Evaluating on the test split.
I0211 11:34:28.633038 140218947737408 submission_runner.py:408] Time since start: 1730.03s, 	Step: 1699, 	{'train/ctc_loss': Array(6.5281034, dtype=float32), 'train/wer': 0.9363028259749184, 'validation/ctc_loss': Array(6.363745, dtype=float32), 'validation/wer': 0.8913754984214642, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3231583, dtype=float32), 'test/wer': 0.8925923669083745, 'test/num_examples': 2472, 'score': 1456.418749332428, 'total_duration': 1730.0256130695343, 'accumulated_submission_time': 1456.418749332428, 'accumulated_eval_time': 273.5006091594696, 'accumulated_logging_time': 0.029590606689453125}
I0211 11:34:28.669026 140062353233664 logging_writer.py:48] [1699] accumulated_eval_time=273.500609, accumulated_logging_time=0.029591, accumulated_submission_time=1456.418749, global_step=1699, preemption_count=0, score=1456.418749, test/ctc_loss=6.323158264160156, test/num_examples=2472, test/wer=0.892592, total_duration=1730.025613, train/ctc_loss=6.528103351593018, train/wer=0.936303, validation/ctc_loss=6.363745212554932, validation/num_examples=5348, validation/wer=0.891375
I0211 11:34:30.310317 140062344840960 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.5522985458374023, loss=3.0240345001220703
I0211 11:35:45.930269 140062353233664 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.3199009895324707, loss=2.8394784927368164
I0211 11:37:02.375721 140062344840960 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.476409912109375, loss=2.748875617980957
I0211 11:38:29.213809 140062353233664 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.600126266479492, loss=2.7049667835235596
I0211 11:39:58.767408 140062353233664 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.2794647216796875, loss=2.6523237228393555
I0211 11:41:17.555093 140062344840960 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.516556739807129, loss=2.5089144706726074
I0211 11:42:36.516594 140062353233664 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.5007967948913574, loss=2.4934215545654297
I0211 11:44:00.453730 140062344840960 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.5790810585021973, loss=2.420996904373169
I0211 11:45:25.758085 140062353233664 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.9827996492385864, loss=2.43218994140625
I0211 11:46:56.305703 140062344840960 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.503669500350952, loss=2.403782606124878
I0211 11:48:28.695298 140062353233664 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.39794659614563, loss=2.3775763511657715
I0211 11:49:55.067034 140062344840960 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.223092555999756, loss=2.3018414974212646
I0211 11:51:26.856131 140062353233664 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.276970863342285, loss=2.283496379852295
I0211 11:52:57.216089 140062344840960 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.9150683879852295, loss=2.2473373413085938
I0211 11:54:29.475666 140062353233664 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.7513880729675293, loss=2.1778404712677
I0211 11:55:46.839059 140062344840960 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.3442482948303223, loss=2.131737470626831
I0211 11:57:05.743690 140062353233664 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.9760477542877197, loss=2.2008345127105713
I0211 11:58:23.895766 140062344840960 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.987653970718384, loss=2.1628124713897705
I0211 11:58:29.055086 140218947737408 spec.py:321] Evaluating on the training split.
I0211 11:59:11.107498 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 12:00:00.241297 140218947737408 spec.py:349] Evaluating on the test split.
I0211 12:00:25.715926 140218947737408 submission_runner.py:408] Time since start: 3287.11s, 	Step: 3407, 	{'train/ctc_loss': Array(4.3004704, dtype=float32), 'train/wer': 0.8745647056270951, 'validation/ctc_loss': Array(4.174814, dtype=float32), 'validation/wer': 0.8286105988781293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.7111945, dtype=float32), 'test/wer': 0.786301870696484, 'test/num_examples': 2472, 'score': 2896.712926864624, 'total_duration': 3287.10879445076, 'accumulated_submission_time': 2896.712926864624, 'accumulated_eval_time': 390.15674901008606, 'accumulated_logging_time': 0.08199119567871094}
I0211 12:00:25.749303 140062353233664 logging_writer.py:48] [3407] accumulated_eval_time=390.156749, accumulated_logging_time=0.081991, accumulated_submission_time=2896.712927, global_step=3407, preemption_count=0, score=2896.712927, test/ctc_loss=3.7111945152282715, test/num_examples=2472, test/wer=0.786302, total_duration=3287.108794, train/ctc_loss=4.300470352172852, train/wer=0.874565, validation/ctc_loss=4.174814224243164, validation/num_examples=5348, validation/wer=0.828611
I0211 12:01:36.223036 140062344840960 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.464587450027466, loss=2.127406120300293
I0211 12:02:51.189633 140062353233664 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.174849033355713, loss=2.0433409214019775
I0211 12:04:17.061270 140062344840960 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.302718162536621, loss=2.0049140453338623
I0211 12:05:45.402125 140062353233664 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.324622392654419, loss=1.9901123046875
I0211 12:07:15.574739 140062344840960 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.6569504737854004, loss=2.007429599761963
I0211 12:08:46.057704 140062353233664 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.369035243988037, loss=1.9686253070831299
I0211 12:10:16.166549 140062344840960 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.1618800163269043, loss=1.9609493017196655
I0211 12:11:38.082141 140062353233664 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.367107391357422, loss=1.9355190992355347
I0211 12:12:55.151841 140062344840960 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.299409866333008, loss=1.9714102745056152
I0211 12:14:13.152742 140062353233664 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.0667648315429688, loss=1.9241293668746948
I0211 12:15:37.037902 140062344840960 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.890286445617676, loss=1.9468990564346313
I0211 12:17:03.144422 140062353233664 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.7454607486724854, loss=1.9514379501342773
I0211 12:18:30.612183 140062344840960 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.357128620147705, loss=1.9656603336334229
I0211 12:20:00.597054 140062353233664 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.824305534362793, loss=1.8186811208724976
I0211 12:21:32.043146 140062344840960 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.7409188747406006, loss=1.826940894126892
I0211 12:23:04.188278 140062353233664 logging_writer.py:48] [5000] global_step=5000, grad_norm=5.944154739379883, loss=1.8190298080444336
I0211 12:24:26.271426 140218947737408 spec.py:321] Evaluating on the training split.
I0211 12:25:21.403129 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 12:26:13.474455 140218947737408 spec.py:349] Evaluating on the test split.
I0211 12:26:40.874409 140218947737408 submission_runner.py:408] Time since start: 4862.27s, 	Step: 5093, 	{'train/ctc_loss': Array(0.91354764, dtype=float32), 'train/wer': 0.27189510006901313, 'validation/ctc_loss': Array(0.99934816, dtype=float32), 'validation/wer': 0.279000164129102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6458774, dtype=float32), 'test/wer': 0.20561412061015985, 'test/num_examples': 2472, 'score': 4337.139240503311, 'total_duration': 4862.266368150711, 'accumulated_submission_time': 4337.139240503311, 'accumulated_eval_time': 524.7541451454163, 'accumulated_logging_time': 0.1371448040008545}
I0211 12:26:40.911156 140062353233664 logging_writer.py:48] [5093] accumulated_eval_time=524.754145, accumulated_logging_time=0.137145, accumulated_submission_time=4337.139241, global_step=5093, preemption_count=0, score=4337.139241, test/ctc_loss=0.6458774209022522, test/num_examples=2472, test/wer=0.205614, total_duration=4862.266368, train/ctc_loss=0.9135476350784302, train/wer=0.271895, validation/ctc_loss=0.9993481636047363, validation/num_examples=5348, validation/wer=0.279000
I0211 12:26:47.029665 140062344840960 logging_writer.py:48] [5100] global_step=5100, grad_norm=4.238277912139893, loss=1.8587015867233276
I0211 12:28:07.246264 140062353233664 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.444532871246338, loss=1.8484938144683838
I0211 12:29:23.632302 140062344840960 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.7998979091644287, loss=1.810814380645752
I0211 12:30:45.369041 140062353233664 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.597275733947754, loss=1.8022892475128174
I0211 12:32:10.693069 140062344840960 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.311579942703247, loss=1.8498075008392334
I0211 12:33:37.538354 140062353233664 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.760369300842285, loss=1.7807400226593018
I0211 12:35:05.712214 140062344840960 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.226093053817749, loss=1.7535645961761475
I0211 12:36:34.476050 140062353233664 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.756209135055542, loss=1.7158266305923462
I0211 12:38:05.790740 140062344840960 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.6725165843963623, loss=1.8499257564544678
I0211 12:39:33.994387 140062353233664 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.675284385681152, loss=1.7853041887283325
I0211 12:41:00.567410 140062344840960 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.3951423168182373, loss=1.7717407941818237
I0211 12:42:29.820633 140062353233664 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.812392473220825, loss=1.7649421691894531
I0211 12:43:46.311291 140062344840960 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.0420010089874268, loss=1.654778003692627
I0211 12:45:05.751723 140062353233664 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.7915027141571045, loss=1.7644884586334229
I0211 12:46:25.313099 140062344840960 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.8487389087677, loss=1.7122279405593872
I0211 12:47:49.277596 140062353233664 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.5913288593292236, loss=1.7146998643875122
I0211 12:49:16.758703 140062344840960 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.2334060668945312, loss=1.7278233766555786
I0211 12:50:41.252630 140218947737408 spec.py:321] Evaluating on the training split.
I0211 12:51:34.824471 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 12:52:26.727380 140218947737408 spec.py:349] Evaluating on the test split.
I0211 12:52:54.205537 140218947737408 submission_runner.py:408] Time since start: 6435.60s, 	Step: 6795, 	{'train/ctc_loss': Array(0.68832296, dtype=float32), 'train/wer': 0.21781532227602074, 'validation/ctc_loss': Array(0.79865134, dtype=float32), 'validation/wer': 0.2307944814003109, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49709377, dtype=float32), 'test/wer': 0.16178173176527938, 'test/num_examples': 2472, 'score': 5777.388965606689, 'total_duration': 6435.596847295761, 'accumulated_submission_time': 5777.388965606689, 'accumulated_eval_time': 657.7007877826691, 'accumulated_logging_time': 0.19022035598754883}
I0211 12:52:54.243337 140062353233664 logging_writer.py:48] [6795] accumulated_eval_time=657.700788, accumulated_logging_time=0.190220, accumulated_submission_time=5777.388966, global_step=6795, preemption_count=0, score=5777.388966, test/ctc_loss=0.4970937669277191, test/num_examples=2472, test/wer=0.161782, total_duration=6435.596847, train/ctc_loss=0.6883229613304138, train/wer=0.217815, validation/ctc_loss=0.7986513376235962, validation/num_examples=5348, validation/wer=0.230794
I0211 12:52:58.872391 140062344840960 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.9013047218322754, loss=1.70204758644104
I0211 12:54:13.738718 140062353233664 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.0410728454589844, loss=1.7481199502944946
I0211 12:55:29.075414 140062344840960 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.966644525527954, loss=1.6608808040618896
I0211 12:56:59.013604 140062353233664 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.5791287422180176, loss=1.6923162937164307
I0211 12:58:27.149646 140062344840960 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.0087740421295166, loss=1.7076343297958374
I0211 12:59:48.822023 140062353233664 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.6824607849121094, loss=1.6473277807235718
I0211 13:01:06.851163 140062344840960 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.456550121307373, loss=1.6424577236175537
I0211 13:02:27.059778 140062353233664 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.2798972129821777, loss=1.6936029195785522
I0211 13:03:50.282234 140062344840960 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.6519320011138916, loss=1.6879993677139282
I0211 13:05:16.272511 140062353233664 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1780805587768555, loss=1.7322673797607422
I0211 13:06:44.417672 140062344840960 logging_writer.py:48] [7800] global_step=7800, grad_norm=5.051146984100342, loss=1.7224907875061035
I0211 13:08:15.012125 140062353233664 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.2696967124938965, loss=1.6122395992279053
I0211 13:09:45.034294 140062344840960 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.640651226043701, loss=1.6723978519439697
I0211 13:11:16.280394 140062353233664 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.9984495639801025, loss=1.6053252220153809
I0211 13:12:45.152163 140062344840960 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.042363166809082, loss=1.651631236076355
I0211 13:14:07.002152 140062353233664 logging_writer.py:48] [8300] global_step=8300, grad_norm=4.050504684448242, loss=1.6422730684280396
I0211 13:15:24.710052 140062344840960 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.331899881362915, loss=1.60690438747406
I0211 13:16:44.614960 140062353233664 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.9234849214553833, loss=1.5412119626998901
I0211 13:16:54.516451 140218947737408 spec.py:321] Evaluating on the training split.
I0211 13:17:49.250047 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 13:18:41.344593 140218947737408 spec.py:349] Evaluating on the test split.
I0211 13:19:08.657154 140218947737408 submission_runner.py:408] Time since start: 8010.05s, 	Step: 8514, 	{'train/ctc_loss': Array(0.5999228, dtype=float32), 'train/wer': 0.19357649288273668, 'validation/ctc_loss': Array(0.72859186, dtype=float32), 'validation/wer': 0.2121706556474893, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44906363, dtype=float32), 'test/wer': 0.14691365547498628, 'test/num_examples': 2472, 'score': 7217.569427251816, 'total_duration': 8010.0493178367615, 'accumulated_submission_time': 7217.569427251816, 'accumulated_eval_time': 791.83607172966, 'accumulated_logging_time': 0.2452392578125}
I0211 13:19:08.693652 140062353233664 logging_writer.py:48] [8514] accumulated_eval_time=791.836072, accumulated_logging_time=0.245239, accumulated_submission_time=7217.569427, global_step=8514, preemption_count=0, score=7217.569427, test/ctc_loss=0.44906362891197205, test/num_examples=2472, test/wer=0.146914, total_duration=8010.049318, train/ctc_loss=0.599922776222229, train/wer=0.193576, validation/ctc_loss=0.7285918593406677, validation/num_examples=5348, validation/wer=0.212171
I0211 13:20:15.489542 140062344840960 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.9305113554000854, loss=1.6194788217544556
I0211 13:21:30.488521 140062353233664 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.217231035232544, loss=1.64149808883667
I0211 13:22:52.153893 140062344840960 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.601095676422119, loss=1.5970796346664429
I0211 13:24:21.622505 140062353233664 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.3241567611694336, loss=1.6316901445388794
I0211 13:25:51.016735 140062344840960 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.0985944271087646, loss=1.61375093460083
I0211 13:27:20.534516 140062353233664 logging_writer.py:48] [9100] global_step=9100, grad_norm=4.413600921630859, loss=1.6143436431884766
I0211 13:28:51.921050 140062344840960 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.042652130126953, loss=1.6324726343154907
I0211 13:30:20.063583 140062353233664 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.7825076580047607, loss=1.560013771057129
I0211 13:31:37.109322 140062344840960 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.5395052433013916, loss=1.6186572313308716
I0211 13:32:57.290003 140062353233664 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.0280344486236572, loss=1.5837528705596924
I0211 13:34:19.474945 140062344840960 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.027428388595581, loss=1.585344910621643
I0211 13:35:39.590389 140062353233664 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.572978973388672, loss=1.585974097251892
I0211 13:37:10.139568 140062344840960 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.350795030593872, loss=1.562213659286499
I0211 13:38:38.238004 140062353233664 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.003763675689697, loss=1.5955095291137695
I0211 13:40:06.902868 140062344840960 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.8846819400787354, loss=1.6103026866912842
I0211 13:41:35.037722 140062353233664 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.0537102222442627, loss=1.5565509796142578
I0211 13:43:05.982408 140062344840960 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.238009214401245, loss=1.5751925706863403
I0211 13:43:09.316463 140218947737408 spec.py:321] Evaluating on the training split.
I0211 13:44:03.983719 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 13:44:58.142187 140218947737408 spec.py:349] Evaluating on the test split.
I0211 13:45:25.624370 140218947737408 submission_runner.py:408] Time since start: 9587.02s, 	Step: 10205, 	{'train/ctc_loss': Array(0.53137237, dtype=float32), 'train/wer': 0.17415988521980855, 'validation/ctc_loss': Array(0.68165743, dtype=float32), 'validation/wer': 0.19699354103710284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40845668, dtype=float32), 'test/wer': 0.13220807182174557, 'test/num_examples': 2472, 'score': 8658.101991176605, 'total_duration': 9587.016305446625, 'accumulated_submission_time': 8658.101991176605, 'accumulated_eval_time': 928.1383287906647, 'accumulated_logging_time': 0.29754114151000977}
I0211 13:45:25.657369 140062353233664 logging_writer.py:48] [10205] accumulated_eval_time=928.138329, accumulated_logging_time=0.297541, accumulated_submission_time=8658.101991, global_step=10205, preemption_count=0, score=8658.101991, test/ctc_loss=0.4084566831588745, test/num_examples=2472, test/wer=0.132208, total_duration=9587.016305, train/ctc_loss=0.5313723683357239, train/wer=0.174160, validation/ctc_loss=0.6816574335098267, validation/num_examples=5348, validation/wer=0.196994
I0211 13:46:42.062088 140062353233664 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.0658481121063232, loss=1.5844025611877441
I0211 13:47:58.691052 140062344840960 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.3048696517944336, loss=1.5398074388504028
I0211 13:49:17.674914 140062353233664 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.2718567848205566, loss=1.5984010696411133
I0211 13:50:38.754458 140062344840960 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.2003955841064453, loss=1.6338894367218018
I0211 13:52:00.835055 140062353233664 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.6246020793914795, loss=1.5238399505615234
I0211 13:53:31.836903 140062344840960 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.67592191696167, loss=1.5095700025558472
I0211 13:55:03.718485 140062353233664 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.3145368099212646, loss=1.5444045066833496
I0211 13:56:32.853093 140062344840960 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.697382926940918, loss=1.5769546031951904
I0211 13:58:00.887500 140062353233664 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.098630428314209, loss=1.5652856826782227
I0211 13:59:32.864905 140062344840960 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.898522138595581, loss=1.5455710887908936
I0211 14:01:06.065895 140062353233664 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.7146623134613037, loss=1.569001317024231
I0211 14:02:27.295583 140062353233664 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.2569451332092285, loss=1.5049327611923218
I0211 14:03:44.357700 140062344840960 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.922884702682495, loss=1.4801357984542847
I0211 14:05:02.920556 140062353233664 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.0810179710388184, loss=1.59346604347229
I0211 14:06:23.426621 140062344840960 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.444847345352173, loss=1.4718295335769653
I0211 14:07:50.344752 140062353233664 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.933180570602417, loss=1.5411038398742676
I0211 14:09:20.308166 140062344840960 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.785993814468384, loss=1.5603735446929932
I0211 14:09:26.168526 140218947737408 spec.py:321] Evaluating on the training split.
I0211 14:10:19.810117 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 14:11:12.662657 140218947737408 spec.py:349] Evaluating on the test split.
I0211 14:11:39.139043 140218947737408 submission_runner.py:408] Time since start: 11160.53s, 	Step: 11908, 	{'train/ctc_loss': Array(0.50215024, dtype=float32), 'train/wer': 0.166799780931952, 'validation/ctc_loss': Array(0.64968437, dtype=float32), 'validation/wer': 0.18750301707908126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3866501, dtype=float32), 'test/wer': 0.12453029472102045, 'test/num_examples': 2472, 'score': 10098.519153118134, 'total_duration': 11160.529643058777, 'accumulated_submission_time': 10098.519153118134, 'accumulated_eval_time': 1061.1018662452698, 'accumulated_logging_time': 0.3483004570007324}
I0211 14:11:39.183171 140062783313664 logging_writer.py:48] [11908] accumulated_eval_time=1061.101866, accumulated_logging_time=0.348300, accumulated_submission_time=10098.519153, global_step=11908, preemption_count=0, score=10098.519153, test/ctc_loss=0.38665008544921875, test/num_examples=2472, test/wer=0.124530, total_duration=11160.529643, train/ctc_loss=0.5021502375602722, train/wer=0.166800, validation/ctc_loss=0.6496843695640564, validation/num_examples=5348, validation/wer=0.187503
I0211 14:12:48.994475 140062774920960 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.9104597568511963, loss=1.495105266571045
I0211 14:14:04.154902 140062783313664 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.7787593603134155, loss=1.496201515197754
I0211 14:15:28.572985 140062774920960 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.2195675373077393, loss=1.5094528198242188
I0211 14:16:59.088866 140062783313664 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.6951141357421875, loss=1.544154167175293
I0211 14:18:24.464609 140062127953664 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.1947927474975586, loss=1.4059951305389404
I0211 14:19:43.470268 140062119560960 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.3998477458953857, loss=1.545791745185852
I0211 14:21:01.387703 140062127953664 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.907167911529541, loss=1.5271621942520142
I0211 14:22:25.122790 140062119560960 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.490168571472168, loss=1.4754091501235962
I0211 14:23:47.505429 140062127953664 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.4298181533813477, loss=1.5513904094696045
I0211 14:25:18.267105 140062119560960 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.130854368209839, loss=1.5084304809570312
I0211 14:26:49.348858 140062127953664 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.0411152839660645, loss=1.5296777486801147
I0211 14:28:18.878132 140062119560960 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.0487828254699707, loss=1.5060806274414062
I0211 14:29:50.054572 140062127953664 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.948683500289917, loss=1.5448288917541504
I0211 14:31:19.159395 140062119560960 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.287295341491699, loss=1.446398138999939
I0211 14:32:50.328796 140062783313664 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.6536738872528076, loss=1.462805151939392
I0211 14:34:05.967605 140062774920960 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.1230437755584717, loss=1.4961748123168945
I0211 14:35:23.550547 140062783313664 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.6147067546844482, loss=1.454453706741333
I0211 14:35:39.701365 140218947737408 spec.py:321] Evaluating on the training split.
I0211 14:36:34.189645 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 14:37:26.480036 140218947737408 spec.py:349] Evaluating on the test split.
I0211 14:37:53.235667 140218947737408 submission_runner.py:408] Time since start: 12734.63s, 	Step: 13621, 	{'train/ctc_loss': Array(0.44562355, dtype=float32), 'train/wer': 0.14918760880239254, 'validation/ctc_loss': Array(0.6191698, dtype=float32), 'validation/wer': 0.1785145350801819, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3623559, dtype=float32), 'test/wer': 0.11857900188897691, 'test/num_examples': 2472, 'score': 11538.94403219223, 'total_duration': 12734.627663373947, 'accumulated_submission_time': 11538.94403219223, 'accumulated_eval_time': 1194.6305973529816, 'accumulated_logging_time': 0.4078514575958252}
I0211 14:37:53.274452 140062491473664 logging_writer.py:48] [13621] accumulated_eval_time=1194.630597, accumulated_logging_time=0.407851, accumulated_submission_time=11538.944032, global_step=13621, preemption_count=0, score=11538.944032, test/ctc_loss=0.36235588788986206, test/num_examples=2472, test/wer=0.118579, total_duration=12734.627663, train/ctc_loss=0.4456235468387604, train/wer=0.149188, validation/ctc_loss=0.6191697716712952, validation/num_examples=5348, validation/wer=0.178515
I0211 14:38:53.200824 140062483080960 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.2119314670562744, loss=1.4774820804595947
I0211 14:40:08.376176 140062491473664 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.4405739307403564, loss=1.4226460456848145
I0211 14:41:29.754213 140062483080960 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.9034903049468994, loss=1.477200984954834
I0211 14:43:02.179614 140062491473664 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.556314468383789, loss=1.5087486505508423
I0211 14:44:32.298420 140062483080960 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.553870916366577, loss=1.461632490158081
I0211 14:46:03.918240 140062491473664 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.050806999206543, loss=1.495431661605835
I0211 14:47:36.918152 140062483080960 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.170602321624756, loss=1.5232964754104614
I0211 14:49:06.572850 140062491473664 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.703629493713379, loss=1.503385066986084
I0211 14:50:30.171848 140062491473664 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.233539581298828, loss=1.4025001525878906
I0211 14:51:48.566150 140062483080960 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.7620034217834473, loss=1.4493025541305542
I0211 14:53:08.937682 140062491473664 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.8475090265274048, loss=1.4180734157562256
I0211 14:54:33.643135 140062483080960 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.677426815032959, loss=1.4494450092315674
I0211 14:56:01.305461 140062491473664 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.2018864154815674, loss=1.382703423500061
I0211 14:57:31.793080 140062483080960 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.764919400215149, loss=1.427287220954895
I0211 14:59:03.046745 140062491473664 logging_writer.py:48] [15100] global_step=15100, grad_norm=5.809577465057373, loss=1.4528112411499023
I0211 15:00:28.730859 140062483080960 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.9849666357040405, loss=1.4489229917526245
I0211 15:01:53.496062 140218947737408 spec.py:321] Evaluating on the training split.
I0211 15:02:47.274024 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 15:03:39.767570 140218947737408 spec.py:349] Evaluating on the test split.
I0211 15:04:07.527433 140218947737408 submission_runner.py:408] Time since start: 14308.92s, 	Step: 15293, 	{'train/ctc_loss': Array(0.44809407, dtype=float32), 'train/wer': 0.1485734782534432, 'validation/ctc_loss': Array(0.6029571, dtype=float32), 'validation/wer': 0.17495196810102628, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34927937, dtype=float32), 'test/wer': 0.11449637438303577, 'test/num_examples': 2472, 'score': 12979.073741436005, 'total_duration': 14308.919946432114, 'accumulated_submission_time': 12979.073741436005, 'accumulated_eval_time': 1328.6569051742554, 'accumulated_logging_time': 0.4639625549316406}
I0211 15:04:07.560845 140062491473664 logging_writer.py:48] [15293] accumulated_eval_time=1328.656905, accumulated_logging_time=0.463963, accumulated_submission_time=12979.073741, global_step=15293, preemption_count=0, score=12979.073741, test/ctc_loss=0.34927937388420105, test/num_examples=2472, test/wer=0.114496, total_duration=14308.919946, train/ctc_loss=0.44809406995773315, train/wer=0.148573, validation/ctc_loss=0.6029571294784546, validation/num_examples=5348, validation/wer=0.174952
I0211 15:04:13.714248 140062483080960 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.712820529937744, loss=1.4473061561584473
I0211 15:05:28.982395 140062491473664 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.69348406791687, loss=1.5351483821868896
I0211 15:06:47.844393 140062163793664 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.7221529483795166, loss=1.4454878568649292
I0211 15:08:04.481269 140062155400960 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.9938968420028687, loss=1.3882453441619873
I0211 15:09:24.892346 140062163793664 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.5567829608917236, loss=1.4166147708892822
I0211 15:10:49.126767 140062155400960 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.702705144882202, loss=1.4032739400863647
I0211 15:12:14.654021 140062163793664 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.569162607192993, loss=1.4988725185394287
I0211 15:13:42.942056 140062155400960 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.3134031295776367, loss=1.3897526264190674
I0211 15:15:11.811492 140062163793664 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.0811712741851807, loss=1.4401904344558716
I0211 15:16:41.019813 140062155400960 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.224278926849365, loss=1.528630018234253
I0211 15:18:13.150471 140062163793664 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.5018770694732666, loss=1.4730825424194336
I0211 15:19:43.700437 140062155400960 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.13789439201355, loss=1.4551955461502075
I0211 15:21:15.950097 140062491473664 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.303159713745117, loss=1.4559992551803589
I0211 15:22:31.805529 140062483080960 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.794286012649536, loss=1.3766140937805176
I0211 15:23:51.889726 140062491473664 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.572023868560791, loss=1.4660897254943848
I0211 15:25:12.988657 140062483080960 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.8925855159759521, loss=1.3609554767608643
I0211 15:26:37.820562 140062491473664 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.7511613368988037, loss=1.390016794204712
I0211 15:28:06.018912 140062483080960 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.685072183609009, loss=1.383361577987671
I0211 15:28:07.973590 140218947737408 spec.py:321] Evaluating on the training split.
I0211 15:29:02.500943 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 15:29:56.497315 140218947737408 spec.py:349] Evaluating on the test split.
I0211 15:30:23.700185 140218947737408 submission_runner.py:408] Time since start: 15885.09s, 	Step: 17003, 	{'train/ctc_loss': Array(0.42328086, dtype=float32), 'train/wer': 0.1401969991726482, 'validation/ctc_loss': Array(0.58921814, dtype=float32), 'validation/wer': 0.1701149869179451, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33758086, dtype=float32), 'test/wer': 0.10907318262141247, 'test/num_examples': 2472, 'score': 14419.393539190292, 'total_duration': 15885.092690944672, 'accumulated_submission_time': 14419.393539190292, 'accumulated_eval_time': 1464.3784432411194, 'accumulated_logging_time': 0.5136878490447998}
I0211 15:30:23.738897 140062491473664 logging_writer.py:48] [17003] accumulated_eval_time=1464.378443, accumulated_logging_time=0.513688, accumulated_submission_time=14419.393539, global_step=17003, preemption_count=0, score=14419.393539, test/ctc_loss=0.3375808596611023, test/num_examples=2472, test/wer=0.109073, total_duration=15885.092691, train/ctc_loss=0.42328086495399475, train/wer=0.140197, validation/ctc_loss=0.5892181396484375, validation/num_examples=5348, validation/wer=0.170115
I0211 15:31:37.259514 140062483080960 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.7478837966918945, loss=1.4003068208694458
I0211 15:32:52.121833 140062491473664 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.2943613529205322, loss=1.4216985702514648
I0211 15:34:22.670182 140062483080960 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.324045181274414, loss=1.4833546876907349
I0211 15:35:50.206177 140062491473664 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.6103646755218506, loss=1.3919726610183716
I0211 15:37:21.018119 140062483080960 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.1718640327453613, loss=1.3977292776107788
I0211 15:38:44.244160 140062491473664 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.107323169708252, loss=1.4607731103897095
I0211 15:40:03.337846 140062483080960 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.2290759086608887, loss=1.4353317022323608
I0211 15:41:24.969236 140062491473664 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.039949893951416, loss=1.40000319480896
I0211 15:42:50.423610 140062483080960 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.176649808883667, loss=1.4563251733779907
I0211 15:44:17.974918 140062491473664 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.6093733310699463, loss=1.420220136642456
I0211 15:45:48.510689 140062483080960 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.984625816345215, loss=1.4426782131195068
I0211 15:47:18.772806 140062491473664 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.299595355987549, loss=1.4268735647201538
I0211 15:48:50.327084 140062483080960 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.3974146842956543, loss=1.349682331085205
I0211 15:50:21.272776 140062491473664 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.4260778427124023, loss=1.4452741146087646
I0211 15:51:51.489529 140062483080960 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.060062885284424, loss=1.4072178602218628
I0211 15:53:16.835492 140062491473664 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.767381429672241, loss=1.3939558267593384
I0211 15:54:24.247241 140218947737408 spec.py:321] Evaluating on the training split.
I0211 15:55:19.099974 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 15:56:12.811890 140218947737408 spec.py:349] Evaluating on the test split.
I0211 15:56:40.531818 140218947737408 submission_runner.py:408] Time since start: 17461.92s, 	Step: 18687, 	{'train/ctc_loss': Array(0.40703046, dtype=float32), 'train/wer': 0.13800737190485152, 'validation/ctc_loss': Array(0.56702036, dtype=float32), 'validation/wer': 0.1645442521023007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32256627, dtype=float32), 'test/wer': 0.10584364145999635, 'test/num_examples': 2472, 'score': 15859.808149814606, 'total_duration': 17461.924512147903, 'accumulated_submission_time': 15859.808149814606, 'accumulated_eval_time': 1600.6581590175629, 'accumulated_logging_time': 0.5700757503509521}
I0211 15:56:40.566817 140062491473664 logging_writer.py:48] [18687] accumulated_eval_time=1600.658159, accumulated_logging_time=0.570076, accumulated_submission_time=15859.808150, global_step=18687, preemption_count=0, score=15859.808150, test/ctc_loss=0.32256627082824707, test/num_examples=2472, test/wer=0.105844, total_duration=17461.924512, train/ctc_loss=0.40703046321868896, train/wer=0.138007, validation/ctc_loss=0.5670203566551208, validation/num_examples=5348, validation/wer=0.164544
I0211 15:56:51.097638 140062483080960 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.2927775382995605, loss=1.3910831212997437
I0211 15:58:06.188212 140062491473664 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.2173101902008057, loss=1.4415159225463867
I0211 15:59:21.460631 140062483080960 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.6352391242980957, loss=1.423448920249939
I0211 16:00:40.448270 140062491473664 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.198134183883667, loss=1.459214448928833
I0211 16:02:07.775328 140062483080960 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.147264003753662, loss=1.452486515045166
I0211 16:03:36.093477 140062491473664 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.7228708267211914, loss=1.437644124031067
I0211 16:05:02.882309 140062483080960 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.133770704269409, loss=1.4789021015167236
I0211 16:06:33.233697 140062491473664 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.2853333950042725, loss=1.3690346479415894
I0211 16:08:05.243888 140062483080960 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.2946035861968994, loss=1.3482110500335693
I0211 16:09:37.171864 140062163793664 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.971034526824951, loss=1.4195526838302612
I0211 16:10:55.500060 140062155400960 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.923521041870117, loss=1.4126964807510376
I0211 16:12:11.464515 140062163793664 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.1670279502868652, loss=1.400151252746582
I0211 16:13:31.927765 140062155400960 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.648998260498047, loss=1.4540663957595825
I0211 16:14:59.996069 140062163793664 logging_writer.py:48] [20000] global_step=20000, grad_norm=6.012790679931641, loss=1.3998905420303345
I0211 16:16:29.160635 140062155400960 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.693556785583496, loss=1.4159955978393555
I0211 16:18:01.701560 140062163793664 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.9487035274505615, loss=1.365439772605896
I0211 16:19:32.280099 140062155400960 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.3422067165374756, loss=1.4507735967636108
I0211 16:20:41.023391 140218947737408 spec.py:321] Evaluating on the training split.
I0211 16:21:34.431985 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 16:22:26.624485 140218947737408 spec.py:349] Evaluating on the test split.
I0211 16:22:54.322146 140218947737408 submission_runner.py:408] Time since start: 19035.71s, 	Step: 20376, 	{'train/ctc_loss': Array(0.35868832, dtype=float32), 'train/wer': 0.12001266445415637, 'validation/ctc_loss': Array(0.5564612, dtype=float32), 'validation/wer': 0.160730664143584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31212524, dtype=float32), 'test/wer': 0.10364999085978917, 'test/num_examples': 2472, 'score': 17300.169142961502, 'total_duration': 19035.714549064636, 'accumulated_submission_time': 17300.169142961502, 'accumulated_eval_time': 1733.951742887497, 'accumulated_logging_time': 0.6257908344268799}
I0211 16:22:54.357213 140062163793664 logging_writer.py:48] [20376] accumulated_eval_time=1733.951743, accumulated_logging_time=0.625791, accumulated_submission_time=17300.169143, global_step=20376, preemption_count=0, score=17300.169143, test/ctc_loss=0.31212523579597473, test/num_examples=2472, test/wer=0.103650, total_duration=19035.714549, train/ctc_loss=0.3586883246898651, train/wer=0.120013, validation/ctc_loss=0.5564612150192261, validation/num_examples=5348, validation/wer=0.160731
I0211 16:23:13.306097 140062155400960 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.360807180404663, loss=1.3868082761764526
I0211 16:24:28.695622 140062163793664 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.2255752086639404, loss=1.3669006824493408
I0211 16:25:49.480978 140062491473664 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.304128170013428, loss=1.3559843301773071
I0211 16:27:07.627023 140062483080960 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.791006565093994, loss=1.3347676992416382
I0211 16:28:25.845743 140062491473664 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.085010290145874, loss=1.411177396774292
I0211 16:29:46.691993 140062483080960 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.7211623191833496, loss=1.4280248880386353
I0211 16:31:11.984444 140062491473664 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.495028495788574, loss=1.411330223083496
I0211 16:32:43.450723 140062483080960 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.8138937950134277, loss=1.3803694248199463
I0211 16:34:15.575378 140062491473664 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.073509931564331, loss=1.357228398323059
I0211 16:35:48.294062 140062483080960 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.814108967781067, loss=1.4029161930084229
I0211 16:37:20.807614 140062491473664 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.0724472999572754, loss=1.368106484413147
I0211 16:38:48.102747 140062483080960 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.7664005756378174, loss=1.4075242280960083
I0211 16:40:16.401135 140062491473664 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.5343425273895264, loss=1.359001636505127
I0211 16:41:40.462914 140062163793664 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.071032762527466, loss=1.328081727027893
I0211 16:42:57.669113 140062155400960 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.6662514209747314, loss=1.393857479095459
I0211 16:44:18.248825 140062163793664 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.1199686527252197, loss=1.3811134099960327
I0211 16:45:42.912705 140062155400960 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.879591703414917, loss=1.3609333038330078
I0211 16:46:54.400794 140218947737408 spec.py:321] Evaluating on the training split.
I0211 16:47:47.908139 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 16:48:41.303322 140218947737408 spec.py:349] Evaluating on the test split.
I0211 16:49:09.828587 140218947737408 submission_runner.py:408] Time since start: 20611.22s, 	Step: 22081, 	{'train/ctc_loss': Array(0.32458857, dtype=float32), 'train/wer': 0.1098593623444518, 'validation/ctc_loss': Array(0.5423642, dtype=float32), 'validation/wer': 0.15747704606234975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30617353, dtype=float32), 'test/wer': 0.10113135498547722, 'test/num_examples': 2472, 'score': 18740.1226541996, 'total_duration': 20611.221473693848, 'accumulated_submission_time': 18740.1226541996, 'accumulated_eval_time': 1869.3748636245728, 'accumulated_logging_time': 0.6759476661682129}
I0211 16:49:09.865763 140062163793664 logging_writer.py:48] [22081] accumulated_eval_time=1869.374864, accumulated_logging_time=0.675948, accumulated_submission_time=18740.122654, global_step=22081, preemption_count=0, score=18740.122654, test/ctc_loss=0.30617353320121765, test/num_examples=2472, test/wer=0.101131, total_duration=20611.221474, train/ctc_loss=0.3245885670185089, train/wer=0.109859, validation/ctc_loss=0.5423641800880432, validation/num_examples=5348, validation/wer=0.157477
I0211 16:49:24.929661 140062155400960 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.3349978923797607, loss=1.2993932962417603
I0211 16:50:39.880086 140062163793664 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.6261370182037354, loss=1.4394116401672363
I0211 16:51:58.773074 140062155400960 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.237539052963257, loss=1.3665939569473267
I0211 16:53:29.384297 140062163793664 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.133204936981201, loss=1.2819780111312866
I0211 16:54:59.586341 140062155400960 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.3066623210906982, loss=1.3293544054031372
I0211 16:56:28.496908 140062163793664 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.7513115406036377, loss=1.342190146446228
I0211 16:57:56.126486 140062163793664 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.4229660034179688, loss=1.3814035654067993
I0211 16:59:13.762778 140062155400960 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.5102055072784424, loss=1.3048439025878906
I0211 17:00:32.721923 140062163793664 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.1844756603240967, loss=1.3086962699890137
I0211 17:01:56.395149 140062155400960 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.368421792984009, loss=1.3257354497909546
I0211 17:03:22.440568 140062163793664 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.255460262298584, loss=1.3513745069503784
I0211 17:04:52.139958 140062155400960 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.7862548828125, loss=1.3592246770858765
I0211 17:06:21.686363 140062163793664 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.1913836002349854, loss=1.3166937828063965
I0211 17:07:51.164692 140062155400960 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.531388998031616, loss=1.3683539628982544
I0211 17:09:22.501511 140062163793664 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.735269784927368, loss=1.3437395095825195
I0211 17:10:51.578714 140062155400960 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.586785078048706, loss=1.3532861471176147
I0211 17:12:24.546264 140062163793664 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.9161072969436646, loss=1.340381145477295
I0211 17:13:10.477665 140218947737408 spec.py:321] Evaluating on the training split.
I0211 17:14:04.206561 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 17:14:56.796344 140218947737408 spec.py:349] Evaluating on the test split.
I0211 17:15:24.434244 140218947737408 submission_runner.py:408] Time since start: 22185.83s, 	Step: 23760, 	{'train/ctc_loss': Array(0.36119395, dtype=float32), 'train/wer': 0.12499858879844655, 'validation/ctc_loss': Array(0.5224131, dtype=float32), 'validation/wer': 0.15162632630796413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29169166, dtype=float32), 'test/wer': 0.09639875693132655, 'test/num_examples': 2472, 'score': 20180.643664598465, 'total_duration': 22185.8266851902, 'accumulated_submission_time': 20180.643664598465, 'accumulated_eval_time': 2003.326298236847, 'accumulated_logging_time': 0.7289724349975586}
I0211 17:15:24.472693 140062645073664 logging_writer.py:48] [23760] accumulated_eval_time=2003.326298, accumulated_logging_time=0.728972, accumulated_submission_time=20180.643665, global_step=23760, preemption_count=0, score=20180.643665, test/ctc_loss=0.2916916608810425, test/num_examples=2472, test/wer=0.096399, total_duration=22185.826685, train/ctc_loss=0.3611939549446106, train/wer=0.124999, validation/ctc_loss=0.5224130749702454, validation/num_examples=5348, validation/wer=0.151626
I0211 17:15:55.140063 140062636680960 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.3329076766967773, loss=1.2517337799072266
I0211 17:17:10.002665 140062645073664 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.815141439437866, loss=1.2933440208435059
I0211 17:18:24.827639 140062636680960 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.2925679683685303, loss=1.367401123046875
I0211 17:19:42.625893 140062645073664 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.384296417236328, loss=1.3326095342636108
I0211 17:21:11.976820 140062636680960 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.0881881713867188, loss=1.3173120021820068
I0211 17:22:42.030737 140062645073664 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.285818338394165, loss=1.329161286354065
I0211 17:24:13.898852 140062636680960 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.8473048210144043, loss=1.3380589485168457
I0211 17:25:42.353545 140062645073664 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.2297534942626953, loss=1.34623122215271
I0211 17:27:14.455121 140062636680960 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.1642837524414062, loss=1.3092930316925049
I0211 17:28:46.920387 140062645073664 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.9022164344787598, loss=1.3152570724487305
I0211 17:30:08.799375 140062317393664 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.0498604774475098, loss=1.3271366357803345
I0211 17:31:26.481312 140062309000960 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.4307737350463867, loss=1.3793214559555054
I0211 17:32:48.666069 140062317393664 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.2316627502441406, loss=1.3514926433563232
I0211 17:34:07.499444 140062309000960 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.8324289321899414, loss=1.2769516706466675
I0211 17:35:34.254424 140062317393664 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.5110533237457275, loss=1.267151951789856
I0211 17:37:05.937668 140062309000960 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.2900278568267822, loss=1.3199756145477295
I0211 17:38:34.045091 140062317393664 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.9389684200286865, loss=1.2653230428695679
I0211 17:39:24.944249 140218947737408 spec.py:321] Evaluating on the training split.
I0211 17:40:18.906130 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 17:41:11.717205 140218947737408 spec.py:349] Evaluating on the test split.
I0211 17:41:38.448076 140218947737408 submission_runner.py:408] Time since start: 23759.84s, 	Step: 25457, 	{'train/ctc_loss': Array(0.3251517, dtype=float32), 'train/wer': 0.10952787258248009, 'validation/ctc_loss': Array(0.51066995, dtype=float32), 'validation/wer': 0.1475134441043861, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28612068, dtype=float32), 'test/wer': 0.09260049154022708, 'test/num_examples': 2472, 'score': 21621.02370738983, 'total_duration': 23759.84077358246, 'accumulated_submission_time': 21621.02370738983, 'accumulated_eval_time': 2136.82527923584, 'accumulated_logging_time': 0.7829153537750244}
I0211 17:41:38.492505 140062317393664 logging_writer.py:48] [25457] accumulated_eval_time=2136.825279, accumulated_logging_time=0.782915, accumulated_submission_time=21621.023707, global_step=25457, preemption_count=0, score=21621.023707, test/ctc_loss=0.2861206829547882, test/num_examples=2472, test/wer=0.092600, total_duration=23759.840774, train/ctc_loss=0.3251517117023468, train/wer=0.109528, validation/ctc_loss=0.5106699466705322, validation/num_examples=5348, validation/wer=0.147513
I0211 17:42:11.720815 140062309000960 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.1010797023773193, loss=1.3071569204330444
I0211 17:43:27.115047 140062317393664 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.8505570888519287, loss=1.277966022491455
I0211 17:44:48.327898 140062309000960 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.8195390701293945, loss=1.2961393594741821
I0211 17:46:15.470853 140062645073664 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.940915822982788, loss=1.252374291419983
I0211 17:47:33.181168 140062636680960 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.901991367340088, loss=1.3313196897506714
I0211 17:48:50.823944 140062645073664 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8067820072174072, loss=1.2997829914093018
I0211 17:50:11.958978 140062636680960 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.4425723552703857, loss=1.3291867971420288
I0211 17:51:38.698462 140062645073664 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.3894925117492676, loss=1.2599159479141235
I0211 17:53:05.099402 140062636680960 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.2826194763183594, loss=1.2838629484176636
I0211 17:54:34.426742 140062645073664 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.0937647819519043, loss=1.3435345888137817
I0211 17:56:07.463002 140062636680960 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.1531379222869873, loss=1.2618589401245117
I0211 17:57:37.669703 140062645073664 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.4468822479248047, loss=1.3551307916641235
I0211 17:59:09.844163 140062636680960 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.9292778968811035, loss=1.2854079008102417
I0211 18:00:41.283059 140062317393664 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.3246617317199707, loss=1.3839191198349
I0211 18:01:59.276461 140062309000960 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.429293394088745, loss=1.2812148332595825
I0211 18:03:16.864330 140062317393664 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.729597806930542, loss=1.310594916343689
I0211 18:04:36.375488 140062309000960 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.177309989929199, loss=1.301494836807251
I0211 18:05:38.548654 140218947737408 spec.py:321] Evaluating on the training split.
I0211 18:06:33.194313 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 18:07:26.878717 140218947737408 spec.py:349] Evaluating on the test split.
I0211 18:07:54.459154 140218947737408 submission_runner.py:408] Time since start: 25335.85s, 	Step: 27173, 	{'train/ctc_loss': Array(0.3157251, dtype=float32), 'train/wer': 0.10562322800284062, 'validation/ctc_loss': Array(0.50370485, dtype=float32), 'validation/wer': 0.14554389488013747, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28231353, dtype=float32), 'test/wer': 0.0910364999085979, 'test/num_examples': 2472, 'score': 23060.985421419144, 'total_duration': 25335.851276874542, 'accumulated_submission_time': 23060.985421419144, 'accumulated_eval_time': 2272.7303400039673, 'accumulated_logging_time': 0.8456952571868896}
I0211 18:07:54.493848 140062317393664 logging_writer.py:48] [27173] accumulated_eval_time=2272.730340, accumulated_logging_time=0.845695, accumulated_submission_time=23060.985421, global_step=27173, preemption_count=0, score=23060.985421, test/ctc_loss=0.2823135256767273, test/num_examples=2472, test/wer=0.091036, total_duration=25335.851277, train/ctc_loss=0.31572508811950684, train/wer=0.105623, validation/ctc_loss=0.503704845905304, validation/num_examples=5348, validation/wer=0.145544
I0211 18:08:15.743997 140062309000960 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.1472585201263428, loss=1.2625435590744019
I0211 18:09:31.219506 140062317393664 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.160447597503662, loss=1.3163944482803345
I0211 18:10:50.129055 140062309000960 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.825449824333191, loss=1.3250923156738281
I0211 18:12:18.274837 140062317393664 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5185518264770508, loss=1.2447707653045654
I0211 18:13:48.479860 140062309000960 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.003347873687744, loss=1.2690229415893555
I0211 18:15:17.940342 140062317393664 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.45023512840271, loss=1.3046317100524902
I0211 18:16:47.926916 140062309000960 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.201294183731079, loss=1.2726824283599854
I0211 18:18:08.221437 140062645073664 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.536168336868286, loss=1.2503005266189575
I0211 18:19:24.764034 140062636680960 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.5379676818847656, loss=1.2251125574111938
I0211 18:20:45.280278 140062645073664 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.089761734008789, loss=1.2717493772506714
I0211 18:22:08.368597 140062636680960 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.517984628677368, loss=1.3331302404403687
I0211 18:23:38.167428 140062645073664 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.340289354324341, loss=1.2642382383346558
I0211 18:25:08.695412 140062636680960 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.110873222351074, loss=1.2475136518478394
I0211 18:26:39.812549 140062645073664 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.4350404739379883, loss=1.3381417989730835
I0211 18:28:08.709664 140062636680960 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.0385971069335938, loss=1.3112499713897705
I0211 18:29:39.710471 140062645073664 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.1707804203033447, loss=1.2593932151794434
I0211 18:31:09.206041 140062636680960 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.066068410873413, loss=1.2589824199676514
I0211 18:31:54.722260 140218947737408 spec.py:321] Evaluating on the training split.
I0211 18:32:49.620275 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 18:33:41.827299 140218947737408 spec.py:349] Evaluating on the test split.
I0211 18:34:08.274391 140218947737408 submission_runner.py:408] Time since start: 26909.67s, 	Step: 28848, 	{'train/ctc_loss': Array(0.28317812, dtype=float32), 'train/wer': 0.09876352395672335, 'validation/ctc_loss': Array(0.4912923, dtype=float32), 'validation/wer': 0.14153721386021995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27055633, dtype=float32), 'test/wer': 0.08774602400828713, 'test/num_examples': 2472, 'score': 24501.122561454773, 'total_duration': 26909.666396856308, 'accumulated_submission_time': 24501.122561454773, 'accumulated_eval_time': 2406.2770829200745, 'accumulated_logging_time': 0.8960163593292236}
I0211 18:34:08.313035 140062860113664 logging_writer.py:48] [28848] accumulated_eval_time=2406.277083, accumulated_logging_time=0.896016, accumulated_submission_time=24501.122561, global_step=28848, preemption_count=0, score=24501.122561, test/ctc_loss=0.27055633068084717, test/num_examples=2472, test/wer=0.087746, total_duration=26909.666397, train/ctc_loss=0.2831781208515167, train/wer=0.098764, validation/ctc_loss=0.4912922978401184, validation/num_examples=5348, validation/wer=0.141537
I0211 18:34:48.473528 140062851720960 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.3467092514038086, loss=1.2517238855361938
I0211 18:36:04.173001 140062860113664 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.8634836673736572, loss=1.2264702320098877
I0211 18:37:19.378266 140062851720960 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.147587776184082, loss=1.2567061185836792
I0211 18:38:34.428426 140062860113664 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.111499309539795, loss=1.2352689504623413
I0211 18:39:56.040564 140062851720960 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.7375482320785522, loss=1.274186372756958
I0211 18:41:23.717360 140062860113664 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.799837112426758, loss=1.2866942882537842
I0211 18:42:52.867292 140062851720960 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.6439857482910156, loss=1.3232758045196533
I0211 18:44:19.587455 140062860113664 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.229983329772949, loss=1.3043040037155151
I0211 18:45:51.121843 140062851720960 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.518117666244507, loss=1.230145812034607
I0211 18:47:21.360399 140062860113664 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7945870161056519, loss=1.3088988065719604
I0211 18:48:51.260800 140062204753664 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.613290548324585, loss=1.2801451683044434
I0211 18:50:08.364244 140062196360960 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.5938141345977783, loss=1.2134209871292114
I0211 18:51:25.875711 140062204753664 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.6098906993865967, loss=1.240537166595459
I0211 18:52:47.701549 140062196360960 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.1083521842956543, loss=1.2292855978012085
I0211 18:54:14.018503 140062204753664 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.8567731380462646, loss=1.202427864074707
I0211 18:55:44.211150 140062196360960 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.9527201652526855, loss=1.2724961042404175
I0211 18:57:14.390922 140062204753664 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.564483404159546, loss=1.239951491355896
I0211 18:58:09.018393 140218947737408 spec.py:321] Evaluating on the training split.
I0211 18:59:05.084593 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 18:59:59.285999 140218947737408 spec.py:349] Evaluating on the test split.
I0211 19:00:26.096197 140218947737408 submission_runner.py:408] Time since start: 28487.49s, 	Step: 30561, 	{'train/ctc_loss': Array(0.2763036, dtype=float32), 'train/wer': 0.09663587059888074, 'validation/ctc_loss': Array(0.48683694, dtype=float32), 'validation/wer': 0.14026279965629435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26829788, dtype=float32), 'test/wer': 0.08750228505270854, 'test/num_examples': 2472, 'score': 25941.73451423645, 'total_duration': 28487.4870493412, 'accumulated_submission_time': 25941.73451423645, 'accumulated_eval_time': 2543.348155975342, 'accumulated_logging_time': 0.951606273651123}
I0211 19:00:26.130774 140062204753664 logging_writer.py:48] [30561] accumulated_eval_time=2543.348156, accumulated_logging_time=0.951606, accumulated_submission_time=25941.734514, global_step=30561, preemption_count=0, score=25941.734514, test/ctc_loss=0.26829788088798523, test/num_examples=2472, test/wer=0.087502, total_duration=28487.487049, train/ctc_loss=0.27630358934402466, train/wer=0.096636, validation/ctc_loss=0.4868369400501251, validation/num_examples=5348, validation/wer=0.140263
I0211 19:00:56.457835 140062196360960 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.1194655895233154, loss=1.3066153526306152
I0211 19:02:11.843653 140062204753664 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.323591709136963, loss=1.2156001329421997
I0211 19:03:31.625292 140062196360960 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.8834190368652344, loss=1.2625792026519775
I0211 19:05:03.175661 140062860113664 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.5351868867874146, loss=1.237624168395996
I0211 19:06:20.281283 140062851720960 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.6898550987243652, loss=1.3192006349563599
I0211 19:07:38.317719 140062860113664 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.6598801612854004, loss=1.2636741399765015
I0211 19:08:54.630461 140062851720960 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.577110528945923, loss=1.2324755191802979
I0211 19:10:16.673158 140062860113664 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.3758952617645264, loss=1.2396109104156494
I0211 19:11:47.597141 140062851720960 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.474954128265381, loss=1.2182942628860474
I0211 19:13:17.990872 140062860113664 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.178866386413574, loss=1.226792812347412
I0211 19:14:49.489509 140062851720960 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.623091459274292, loss=1.2221628427505493
I0211 19:16:18.101785 140062860113664 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.7602771520614624, loss=1.2181912660598755
I0211 19:17:49.754126 140062851720960 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.2508182525634766, loss=1.2742905616760254
I0211 19:19:17.898404 140062860113664 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.8397042751312256, loss=1.213122844696045
I0211 19:20:39.057778 140062860113664 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.1322193145751953, loss=1.237296223640442
I0211 19:21:55.534168 140062851720960 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.176450252532959, loss=1.3106322288513184
I0211 19:23:14.502840 140062860113664 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.5214028358459473, loss=1.2106958627700806
I0211 19:24:26.602919 140218947737408 spec.py:321] Evaluating on the training split.
I0211 19:25:20.526696 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 19:26:13.361225 140218947737408 spec.py:349] Evaluating on the test split.
I0211 19:26:41.225470 140218947737408 submission_runner.py:408] Time since start: 30062.62s, 	Step: 32288, 	{'train/ctc_loss': Array(0.25569287, dtype=float32), 'train/wer': 0.0897257371193316, 'validation/ctc_loss': Array(0.46944138, dtype=float32), 'validation/wer': 0.13578304063643473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26102412, dtype=float32), 'test/wer': 0.08433367863018706, 'test/num_examples': 2472, 'score': 27382.11405301094, 'total_duration': 30062.6204662323, 'accumulated_submission_time': 27382.11405301094, 'accumulated_eval_time': 2677.9681293964386, 'accumulated_logging_time': 1.001542568206787}
I0211 19:26:41.259134 140062860113664 logging_writer.py:48] [32288] accumulated_eval_time=2677.968129, accumulated_logging_time=1.001543, accumulated_submission_time=27382.114053, global_step=32288, preemption_count=0, score=27382.114053, test/ctc_loss=0.2610241174697876, test/num_examples=2472, test/wer=0.084334, total_duration=30062.620466, train/ctc_loss=0.25569286942481995, train/wer=0.089726, validation/ctc_loss=0.46944138407707214, validation/num_examples=5348, validation/wer=0.135783
I0211 19:26:51.078414 140062851720960 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.6046268939971924, loss=1.232419490814209
I0211 19:28:06.323778 140062860113664 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.557495355606079, loss=1.2138142585754395
I0211 19:29:21.928322 140062851720960 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.3175182342529297, loss=1.2113049030303955
I0211 19:30:51.828770 140062860113664 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.512939453125, loss=1.167862057685852
I0211 19:32:24.070186 140062851720960 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.5519165992736816, loss=1.2160972356796265
I0211 19:33:54.599322 140062860113664 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.800165891647339, loss=1.2720043659210205
I0211 19:35:26.447131 140062851720960 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.8832368850708008, loss=1.2198879718780518
I0211 19:36:52.786357 140062860113664 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.439828872680664, loss=1.2305172681808472
I0211 19:38:08.150370 140062851720960 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.315761089324951, loss=1.278594970703125
I0211 19:39:23.535442 140062860113664 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.693339228630066, loss=1.1931350231170654
I0211 19:40:40.812161 140062851720960 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.6541221141815186, loss=1.2178839445114136
I0211 19:42:01.596709 140062860113664 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.6624715328216553, loss=1.2100012302398682
I0211 19:43:30.550329 140062851720960 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.4991462230682373, loss=1.2473747730255127
I0211 19:45:01.472958 140062860113664 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.502732515335083, loss=1.2047251462936401
I0211 19:46:29.244843 140062851720960 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.4319067001342773, loss=1.212242841720581
I0211 19:47:58.973587 140062860113664 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.36087703704834, loss=1.2269824743270874
I0211 19:49:28.728797 140062851720960 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.2440662384033203, loss=1.1790313720703125
I0211 19:50:42.110709 140218947737408 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0211 19:51:55.587646 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 19:52:49.597651 140218947737408 spec.py:349] Evaluating on the test split.
I0211 19:53:16.567047 140218947737408 submission_runner.py:408] Time since start: 31657.96s, 	Step: 33983, 	{'train/ctc_loss': Array(0.15601371, dtype=float32), 'train/wer': 0.05595349430368985, 'validation/ctc_loss': Array(0.46224105, dtype=float32), 'validation/wer': 0.1343155333713083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2518262, dtype=float32), 'test/wer': 0.08207909329108524, 'test/num_examples': 2472, 'score': 28822.875962257385, 'total_duration': 31657.9614071846, 'accumulated_submission_time': 28822.875962257385, 'accumulated_eval_time': 2832.4212596416473, 'accumulated_logging_time': 1.0486581325531006}
I0211 19:53:16.602558 140062276433664 logging_writer.py:48] [33983] accumulated_eval_time=2832.421260, accumulated_logging_time=1.048658, accumulated_submission_time=28822.875962, global_step=33983, preemption_count=0, score=28822.875962, test/ctc_loss=0.2518261969089508, test/num_examples=2472, test/wer=0.082079, total_duration=31657.961407, train/ctc_loss=0.15601371228694916, train/wer=0.055953, validation/ctc_loss=0.4622410535812378, validation/num_examples=5348, validation/wer=0.134316
I0211 19:53:34.591413 140062276433664 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.612610340118408, loss=1.1992624998092651
I0211 19:54:52.410633 140062268040960 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.64609432220459, loss=1.2157248258590698
I0211 19:56:09.527648 140062276433664 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.5089235305786133, loss=1.2385871410369873
I0211 19:57:31.723222 140062268040960 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.6186766624450684, loss=1.1493902206420898
I0211 19:58:55.708811 140062276433664 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7183482646942139, loss=1.1579070091247559
I0211 20:00:25.220008 140062268040960 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.08483624458313, loss=1.2209045886993408
I0211 20:01:56.324823 140062276433664 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.317147731781006, loss=1.238412618637085
I0211 20:03:26.920731 140062268040960 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.208791494369507, loss=1.1570477485656738
I0211 20:04:55.526775 140062276433664 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.0471413135528564, loss=1.1831904649734497
I0211 20:06:26.098639 140062268040960 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.234071731567383, loss=1.1724634170532227
I0211 20:07:59.356526 140062276433664 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.2872426509857178, loss=1.257399082183838
I0211 20:09:21.470321 140062276433664 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.8330051898956299, loss=1.1898841857910156
I0211 20:10:39.884063 140062268040960 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.7351889610290527, loss=1.196475625038147
I0211 20:12:00.501823 140062276433664 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.0972931385040283, loss=1.1891212463378906
I0211 20:13:26.973474 140062268040960 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.9959499835968018, loss=1.1948217153549194
I0211 20:14:52.341389 140062276433664 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.3247692584991455, loss=1.2058007717132568
I0211 20:16:20.204115 140062268040960 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.7641448974609375, loss=1.160095453262329
I0211 20:17:16.698173 140218947737408 spec.py:321] Evaluating on the training split.
I0211 20:18:12.919765 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 20:19:05.591860 140218947737408 spec.py:349] Evaluating on the test split.
I0211 20:19:33.119688 140218947737408 submission_runner.py:408] Time since start: 33234.51s, 	Step: 35663, 	{'train/ctc_loss': Array(0.15231837, dtype=float32), 'train/wer': 0.05323829957452998, 'validation/ctc_loss': Array(0.45944563, dtype=float32), 'validation/wer': 0.1315349932900161, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24787179, dtype=float32), 'test/wer': 0.07962139215566795, 'test/num_examples': 2472, 'score': 30262.88162112236, 'total_duration': 33234.50987505913, 'accumulated_submission_time': 30262.88162112236, 'accumulated_eval_time': 2968.835390806198, 'accumulated_logging_time': 1.0996172428131104}
I0211 20:19:33.156795 140062645073664 logging_writer.py:48] [35663] accumulated_eval_time=2968.835391, accumulated_logging_time=1.099617, accumulated_submission_time=30262.881621, global_step=35663, preemption_count=0, score=30262.881621, test/ctc_loss=0.2478717863559723, test/num_examples=2472, test/wer=0.079621, total_duration=33234.509875, train/ctc_loss=0.15231837332248688, train/wer=0.053238, validation/ctc_loss=0.45944562554359436, validation/num_examples=5348, validation/wer=0.131535
I0211 20:20:01.847496 140062636680960 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.5304431915283203, loss=1.2051429748535156
I0211 20:21:17.036119 140062645073664 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.9383175373077393, loss=1.1867790222167969
I0211 20:22:39.237748 140062636680960 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.4899190664291382, loss=1.2163962125778198
I0211 20:24:12.363788 140062645073664 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.9034481048583984, loss=1.2591979503631592
I0211 20:25:41.566690 140062645073664 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.0128939151763916, loss=1.1991921663284302
I0211 20:26:57.791935 140062636680960 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.460343360900879, loss=1.1688319444656372
I0211 20:28:15.179688 140062645073664 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.711495041847229, loss=1.146596908569336
I0211 20:29:38.094328 140062636680960 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.342132568359375, loss=1.2173962593078613
I0211 20:31:06.751467 140062645073664 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.2340142726898193, loss=1.2169904708862305
I0211 20:32:35.734866 140062636680960 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.6316592693328857, loss=1.178218126296997
I0211 20:34:07.786328 140062645073664 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.6290950775146484, loss=1.1752904653549194
I0211 20:35:39.161349 140062636680960 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.3738656044006348, loss=1.185711145401001
I0211 20:37:09.304397 140062645073664 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.649944543838501, loss=1.1814693212509155
I0211 20:38:39.061391 140062636680960 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.402576446533203, loss=1.1829442977905273
I0211 20:40:10.617815 140062317393664 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.0077261924743652, loss=1.1447453498840332
I0211 20:41:27.742799 140062309000960 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.665217638015747, loss=1.115732192993164
I0211 20:42:47.705496 140062317393664 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.91788649559021, loss=1.1376489400863647
I0211 20:43:34.340374 140218947737408 spec.py:321] Evaluating on the training split.
I0211 20:44:30.440982 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 20:45:22.867567 140218947737408 spec.py:349] Evaluating on the test split.
I0211 20:45:49.691049 140218947737408 submission_runner.py:408] Time since start: 34811.08s, 	Step: 37356, 	{'train/ctc_loss': Array(0.1572897, dtype=float32), 'train/wer': 0.055789000766405035, 'validation/ctc_loss': Array(0.44850588, dtype=float32), 'validation/wer': 0.1289282369638047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23940639, dtype=float32), 'test/wer': 0.07734649523693457, 'test/num_examples': 2472, 'score': 31703.973056077957, 'total_duration': 34811.08276438713, 'accumulated_submission_time': 31703.973056077957, 'accumulated_eval_time': 3104.18021607399, 'accumulated_logging_time': 1.1535747051239014}
I0211 20:45:49.733872 140062317393664 logging_writer.py:48] [37356] accumulated_eval_time=3104.180216, accumulated_logging_time=1.153575, accumulated_submission_time=31703.973056, global_step=37356, preemption_count=0, score=31703.973056, test/ctc_loss=0.23940639197826385, test/num_examples=2472, test/wer=0.077346, total_duration=34811.082764, train/ctc_loss=0.15728969871997833, train/wer=0.055789, validation/ctc_loss=0.44850587844848633, validation/num_examples=5348, validation/wer=0.128928
I0211 20:46:23.855759 140062309000960 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.6763150691986084, loss=1.23729407787323
I0211 20:47:38.663638 140062317393664 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.8620678186416626, loss=1.1738721132278442
I0211 20:48:57.617106 140062309000960 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.5114235877990723, loss=1.1401551961898804
I0211 20:50:28.460153 140062317393664 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.4777729511260986, loss=1.1633278131484985
I0211 20:51:55.011800 140062309000960 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.4062466621398926, loss=1.1630393266677856
I0211 20:53:24.286232 140062317393664 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.3565115928649902, loss=1.1812105178833008
I0211 20:54:52.995439 140062309000960 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.398630142211914, loss=1.1551846265792847
I0211 20:56:24.148663 140062317393664 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.3816213607788086, loss=1.1752523183822632
I0211 20:57:45.824966 140062317393664 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.9646855592727661, loss=1.174627423286438
I0211 20:59:02.331133 140062309000960 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.9213504791259766, loss=1.1821051836013794
I0211 21:00:21.029734 140062317393664 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7314457893371582, loss=1.1309033632278442
I0211 21:01:44.754524 140062309000960 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.2443289756774902, loss=1.1373677253723145
I0211 21:03:13.922556 140062317393664 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.769569158554077, loss=1.1728057861328125
I0211 21:04:43.846020 140062309000960 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.2754127979278564, loss=1.1743422746658325
I0211 21:06:14.388682 140062317393664 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.1639935970306396, loss=1.1462781429290771
I0211 21:07:45.643965 140062309000960 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.2480735778808594, loss=1.1778253316879272
I0211 21:09:18.247625 140062317393664 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.1564061641693115, loss=1.1338597536087036
I0211 21:09:50.088463 140218947737408 spec.py:321] Evaluating on the training split.
I0211 21:10:46.488203 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 21:11:40.623929 140218947737408 spec.py:349] Evaluating on the test split.
I0211 21:12:08.152688 140218947737408 submission_runner.py:408] Time since start: 36389.54s, 	Step: 39037, 	{'train/ctc_loss': Array(0.13856104, dtype=float32), 'train/wer': 0.04972243092077803, 'validation/ctc_loss': Array(0.44071487, dtype=float32), 'validation/wer': 0.12696834239261612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2376608, dtype=float32), 'test/wer': 0.07629029309609409, 'test/num_examples': 2472, 'score': 33144.23564553261, 'total_duration': 36389.54399847984, 'accumulated_submission_time': 33144.23564553261, 'accumulated_eval_time': 3242.2381801605225, 'accumulated_logging_time': 1.2139358520507812}
I0211 21:12:08.192713 140062317393664 logging_writer.py:48] [39037] accumulated_eval_time=3242.238180, accumulated_logging_time=1.213936, accumulated_submission_time=33144.235646, global_step=39037, preemption_count=0, score=33144.235646, test/ctc_loss=0.23766079545021057, test/num_examples=2472, test/wer=0.076290, total_duration=36389.543998, train/ctc_loss=0.13856104016304016, train/wer=0.049722, validation/ctc_loss=0.44071486592292786, validation/num_examples=5348, validation/wer=0.126968
I0211 21:12:56.105366 140062309000960 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.041167974472046, loss=1.1757214069366455
I0211 21:14:15.920469 140062317393664 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.8815984725952148, loss=1.1725735664367676
I0211 21:15:35.732844 140062309000960 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.9599320888519287, loss=1.1666852235794067
I0211 21:16:56.106543 140062317393664 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.950187921524048, loss=1.174231767654419
I0211 21:18:20.256117 140062309000960 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.69814932346344, loss=1.1254751682281494
I0211 21:19:47.483925 140062317393664 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.786937713623047, loss=1.1572554111480713
I0211 21:21:19.150191 140062309000960 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.2860536575317383, loss=1.1556233167648315
I0211 21:22:48.204266 140062317393664 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.461228370666504, loss=1.1478099822998047
I0211 21:24:19.979806 140062309000960 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.3080978393554688, loss=1.1281797885894775
I0211 21:25:47.531673 140062317393664 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.670862913131714, loss=1.2182507514953613
I0211 21:27:15.408474 140062309000960 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.080333948135376, loss=1.1484782695770264
I0211 21:28:44.413610 140062645073664 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.4746179580688477, loss=1.1378949880599976
I0211 21:30:03.394185 140062636680960 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.8026341199874878, loss=1.1436504125595093
I0211 21:31:21.634291 140062645073664 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.305178642272949, loss=1.0834425687789917
I0211 21:32:44.556884 140062636680960 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.936809778213501, loss=1.1455243825912476
I0211 21:34:09.792205 140062645073664 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.736811399459839, loss=1.1507500410079956
I0211 21:35:40.420209 140062636680960 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.7818281650543213, loss=1.1729909181594849
I0211 21:36:08.401771 140218947737408 spec.py:321] Evaluating on the training split.
I0211 21:37:04.389248 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 21:37:58.099848 140218947737408 spec.py:349] Evaluating on the test split.
I0211 21:38:25.712993 140218947737408 submission_runner.py:408] Time since start: 37967.10s, 	Step: 40732, 	{'train/ctc_loss': Array(0.1460249, dtype=float32), 'train/wer': 0.051847033923154436, 'validation/ctc_loss': Array(0.4341819, dtype=float32), 'validation/wer': 0.1248636280255269, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23402792, dtype=float32), 'test/wer': 0.07533564885341133, 'test/num_examples': 2472, 'score': 34584.34844779968, 'total_duration': 37967.1045422554, 'accumulated_submission_time': 34584.34844779968, 'accumulated_eval_time': 3379.5433967113495, 'accumulated_logging_time': 1.2748100757598877}
I0211 21:38:25.757457 140063075153664 logging_writer.py:48] [40732] accumulated_eval_time=3379.543397, accumulated_logging_time=1.274810, accumulated_submission_time=34584.348448, global_step=40732, preemption_count=0, score=34584.348448, test/ctc_loss=0.2340279221534729, test/num_examples=2472, test/wer=0.075336, total_duration=37967.104542, train/ctc_loss=0.1460248976945877, train/wer=0.051847, validation/ctc_loss=0.43418189883232117, validation/num_examples=5348, validation/wer=0.124864
I0211 21:39:17.500476 140063066760960 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.9111979007720947, loss=1.1399500370025635
I0211 21:40:32.791881 140063075153664 logging_writer.py:48] [40900] global_step=40900, grad_norm=4.237691879272461, loss=1.0966110229492188
I0211 21:42:00.016333 140063066760960 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.5761948823928833, loss=1.1211214065551758
I0211 21:43:28.257049 140063075153664 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.1860663890838623, loss=1.0701825618743896
I0211 21:44:59.504612 140062419793664 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.0330333709716797, loss=1.139389991760254
I0211 21:46:17.496663 140062411400960 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.8109731674194336, loss=1.1448601484298706
I0211 21:47:37.462987 140062419793664 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.1803181171417236, loss=1.1889591217041016
I0211 21:49:00.827697 140062411400960 logging_writer.py:48] [41500] global_step=41500, grad_norm=4.281904697418213, loss=1.1898868083953857
I0211 21:50:23.729088 140062419793664 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.093144178390503, loss=1.1591525077819824
I0211 21:51:54.004321 140062411400960 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.96708345413208, loss=1.1020126342773438
I0211 21:53:21.734981 140062419793664 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.494123935699463, loss=1.1396234035491943
I0211 21:54:51.864812 140062411400960 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.9968769550323486, loss=1.156110405921936
I0211 21:56:25.586411 140062419793664 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.4081406593322754, loss=1.1326793432235718
I0211 21:57:57.229915 140062411400960 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.3788602352142334, loss=1.2031075954437256
I0211 21:59:26.111372 140062419793664 logging_writer.py:48] [42200] global_step=42200, grad_norm=5.048399925231934, loss=1.0972816944122314
I0211 22:00:49.863272 140062419793664 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.8155951499938965, loss=1.1112850904464722
I0211 22:02:08.011512 140062411400960 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.9656941890716553, loss=1.0919783115386963
I0211 22:02:26.469836 140218947737408 spec.py:321] Evaluating on the training split.
I0211 22:03:22.240913 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 22:04:14.319824 140218947737408 spec.py:349] Evaluating on the test split.
I0211 22:04:40.752180 140218947737408 submission_runner.py:408] Time since start: 39542.14s, 	Step: 42425, 	{'train/ctc_loss': Array(0.13446575, dtype=float32), 'train/wer': 0.04884225373182173, 'validation/ctc_loss': Array(0.43170702, dtype=float32), 'validation/wer': 0.12386919876034255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23212434, dtype=float32), 'test/wer': 0.07478723620335953, 'test/num_examples': 2472, 'score': 36024.96976852417, 'total_duration': 39542.14425396919, 'accumulated_submission_time': 36024.96976852417, 'accumulated_eval_time': 3513.8202295303345, 'accumulated_logging_time': 1.3349525928497314}
I0211 22:04:40.792798 140062204753664 logging_writer.py:48] [42425] accumulated_eval_time=3513.820230, accumulated_logging_time=1.334953, accumulated_submission_time=36024.969769, global_step=42425, preemption_count=0, score=36024.969769, test/ctc_loss=0.23212434351444244, test/num_examples=2472, test/wer=0.074787, total_duration=39542.144254, train/ctc_loss=0.134465754032135, train/wer=0.048842, validation/ctc_loss=0.4317070245742798, validation/num_examples=5348, validation/wer=0.123869
I0211 22:05:37.657162 140062196360960 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.171616315841675, loss=1.153422474861145
I0211 22:06:52.584708 140062204753664 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.781364679336548, loss=1.1089144945144653
I0211 22:08:13.817025 140062196360960 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.2349534034729004, loss=1.141144871711731
I0211 22:09:44.208501 140062204753664 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.9633865356445312, loss=1.130257487297058
I0211 22:11:12.105014 140062196360960 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.877495765686035, loss=1.1413769721984863
I0211 22:12:42.756923 140062204753664 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.4459481239318848, loss=1.1461397409439087
I0211 22:14:11.045678 140062196360960 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.3685050010681152, loss=1.1303315162658691
I0211 22:15:41.839974 140062204753664 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.5800068378448486, loss=1.1067053079605103
I0211 22:17:11.038155 140062204753664 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.590129852294922, loss=1.151967167854309
I0211 22:18:28.878410 140062196360960 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.600414752960205, loss=1.108455777168274
I0211 22:19:49.281037 140062204753664 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.276047706604004, loss=1.2074471712112427
I0211 22:21:11.731151 140062196360960 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.329977512359619, loss=1.1684013605117798
I0211 22:22:38.401986 140062204753664 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.3858773708343506, loss=1.150415062904358
I0211 22:24:09.517334 140062196360960 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.754209280014038, loss=1.119733214378357
I0211 22:25:42.277871 140062204753664 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.8330111503601074, loss=1.123566746711731
I0211 22:27:17.367614 140062196360960 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.9917151927948, loss=1.1722993850708008
I0211 22:28:41.811046 140218947737408 spec.py:321] Evaluating on the training split.
I0211 22:29:37.469122 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 22:30:31.284380 140218947737408 spec.py:349] Evaluating on the test split.
I0211 22:30:57.594237 140218947737408 submission_runner.py:408] Time since start: 41118.99s, 	Step: 44094, 	{'train/ctc_loss': Array(0.15975782, dtype=float32), 'train/wer': 0.0529069018799897, 'validation/ctc_loss': Array(0.4287825, dtype=float32), 'validation/wer': 0.12279753227067786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23024347, dtype=float32), 'test/wer': 0.07438100461072858, 'test/num_examples': 2472, 'score': 37465.898456811905, 'total_duration': 41118.98565912247, 'accumulated_submission_time': 37465.898456811905, 'accumulated_eval_time': 3649.597271680832, 'accumulated_logging_time': 1.391124963760376}
I0211 22:30:57.635946 140062419793664 logging_writer.py:48] [44094] accumulated_eval_time=3649.597272, accumulated_logging_time=1.391125, accumulated_submission_time=37465.898457, global_step=44094, preemption_count=0, score=37465.898457, test/ctc_loss=0.2302434742450714, test/num_examples=2472, test/wer=0.074381, total_duration=41118.985659, train/ctc_loss=0.1597578227519989, train/wer=0.052907, validation/ctc_loss=0.42878249287605286, validation/num_examples=5348, validation/wer=0.122798
I0211 22:31:03.043699 140062411400960 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.1078290939331055, loss=1.1759792566299438
I0211 22:32:18.073566 140062419793664 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.719956159591675, loss=1.141902208328247
I0211 22:33:36.736857 140062092113664 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.273033618927002, loss=1.1246662139892578
I0211 22:34:54.413015 140062083720960 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.278559684753418, loss=1.106320858001709
I0211 22:36:13.621712 140062092113664 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.9163599014282227, loss=1.111636757850647
I0211 22:37:38.667870 140062083720960 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.120738983154297, loss=1.1144602298736572
I0211 22:39:04.724145 140062092113664 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.714238166809082, loss=1.1346256732940674
I0211 22:40:35.848069 140062083720960 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.141653060913086, loss=1.1320456266403198
I0211 22:42:07.773795 140062092113664 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.82255482673645, loss=1.0912747383117676
I0211 22:43:36.888028 140062083720960 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.744271755218506, loss=1.1488991975784302
I0211 22:45:08.984631 140062092113664 logging_writer.py:48] [45100] global_step=45100, grad_norm=4.522619724273682, loss=1.1169779300689697
I0211 22:46:38.326928 140062083720960 logging_writer.py:48] [45200] global_step=45200, grad_norm=4.730421543121338, loss=1.1143503189086914
I0211 22:48:08.814986 140062092113664 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.368523359298706, loss=1.1301512718200684
I0211 22:49:34.053100 140062092113664 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.3416476249694824, loss=1.1129168272018433
I0211 22:50:54.789259 140062083720960 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.718508005142212, loss=1.1710307598114014
I0211 22:52:19.106287 140062092113664 logging_writer.py:48] [45600] global_step=45600, grad_norm=4.237386703491211, loss=1.139358401298523
I0211 22:53:46.080126 140062083720960 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.415870428085327, loss=1.1353336572647095
I0211 22:54:58.076853 140218947737408 spec.py:321] Evaluating on the training split.
I0211 22:55:54.172234 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 22:56:46.419877 140218947737408 spec.py:349] Evaluating on the test split.
I0211 22:57:13.344496 140218947737408 submission_runner.py:408] Time since start: 42694.74s, 	Step: 45785, 	{'train/ctc_loss': Array(0.14198789, dtype=float32), 'train/wer': 0.05093299406276505, 'validation/ctc_loss': Array(0.42813993, dtype=float32), 'validation/wer': 0.1223051449646157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22979745, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 38906.245678424835, 'total_duration': 42694.73621749878, 'accumulated_submission_time': 38906.245678424835, 'accumulated_eval_time': 3784.859577178955, 'accumulated_logging_time': 1.4510438442230225}
I0211 22:57:13.389178 140062092113664 logging_writer.py:48] [45785] accumulated_eval_time=3784.859577, accumulated_logging_time=1.451044, accumulated_submission_time=38906.245678, global_step=45785, preemption_count=0, score=38906.245678, test/ctc_loss=0.22979745268821716, test/num_examples=2472, test/wer=0.074137, total_duration=42694.736217, train/ctc_loss=0.1419878900051117, train/wer=0.050933, validation/ctc_loss=0.42813992500305176, validation/num_examples=5348, validation/wer=0.122305
I0211 22:57:25.537789 140062083720960 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.8461953401565552, loss=1.0937111377716064
I0211 22:58:40.552150 140062092113664 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.4331412315368652, loss=1.119387149810791
I0211 22:59:58.448017 140062083720960 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.379734992980957, loss=1.1584779024124146
I0211 23:01:26.251545 140062092113664 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.631728172302246, loss=1.1289827823638916
I0211 23:02:56.110925 140062083720960 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.1141269207000732, loss=1.1120637655258179
I0211 23:04:28.218615 140062092113664 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.022430419921875, loss=1.0989607572555542
I0211 23:05:56.336928 140062419793664 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.366818904876709, loss=1.1539318561553955
I0211 23:07:15.156206 140062411400960 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.925726890563965, loss=1.1649293899536133
I0211 23:08:33.653588 140062419793664 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.9326982498168945, loss=1.0988271236419678
I0211 23:09:59.261612 140062411400960 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.0938568115234375, loss=1.135497808456421
I0211 23:11:25.220142 140062419793664 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.1974074840545654, loss=1.0843254327774048
I0211 23:12:54.626392 140062411400960 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.933664083480835, loss=1.159222960472107
I0211 23:14:25.434516 140062419793664 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.8459136486053467, loss=1.1474905014038086
I0211 23:15:55.551983 140062411400960 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.8317204713821411, loss=1.1101919412612915
I0211 23:17:27.294856 140062419793664 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.5093348026275635, loss=1.1300517320632935
I0211 23:18:56.102212 140062411400960 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.339556932449341, loss=1.1364892721176147
I0211 23:20:28.635044 140062092113664 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.4949326515197754, loss=1.1349745988845825
I0211 23:21:13.830492 140218947737408 spec.py:321] Evaluating on the training split.
I0211 23:22:09.857507 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 23:23:01.961278 140218947737408 spec.py:349] Evaluating on the test split.
I0211 23:23:29.471750 140218947737408 submission_runner.py:408] Time since start: 44270.86s, 	Step: 47459, 	{'train/ctc_loss': Array(0.1208012, dtype=float32), 'train/wer': 0.04385094853777968, 'validation/ctc_loss': Array(0.4275235, dtype=float32), 'validation/wer': 0.12211205190341486, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22964966, dtype=float32), 'test/wer': 0.07391383827920298, 'test/num_examples': 2472, 'score': 40346.5943479538, 'total_duration': 44270.86405515671, 'accumulated_submission_time': 40346.5943479538, 'accumulated_eval_time': 3920.4955565929413, 'accumulated_logging_time': 1.513451337814331}
I0211 23:23:29.513094 140063075153664 logging_writer.py:48] [47459] accumulated_eval_time=3920.495557, accumulated_logging_time=1.513451, accumulated_submission_time=40346.594348, global_step=47459, preemption_count=0, score=40346.594348, test/ctc_loss=0.22964966297149658, test/num_examples=2472, test/wer=0.073914, total_duration=44270.864055, train/ctc_loss=0.12080120295286179, train/wer=0.043851, validation/ctc_loss=0.42752349376678467, validation/num_examples=5348, validation/wer=0.122112
I0211 23:24:01.410614 140063066760960 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.5071451663970947, loss=1.1223721504211426
I0211 23:25:16.339481 140063075153664 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.3632638454437256, loss=1.1283282041549683
I0211 23:26:31.784028 140063066760960 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.215019464492798, loss=1.1287946701049805
I0211 23:27:48.151174 140063075153664 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.472579002380371, loss=1.1213895082473755
I0211 23:29:19.117182 140063066760960 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.441574811935425, loss=1.096247673034668
I0211 23:30:45.741944 140218947737408 spec.py:321] Evaluating on the training split.
I0211 23:31:41.393815 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 23:32:35.987652 140218947737408 spec.py:349] Evaluating on the test split.
I0211 23:33:02.923634 140218947737408 submission_runner.py:408] Time since start: 44844.32s, 	Step: 48000, 	{'train/ctc_loss': Array(0.12780735, dtype=float32), 'train/wer': 0.0460682191722342, 'validation/ctc_loss': Array(0.42751795, dtype=float32), 'validation/wer': 0.12202516002587448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22965063, dtype=float32), 'test/wer': 0.07399508459772916, 'test/num_examples': 2472, 'score': 40782.78246617317, 'total_duration': 44844.3178961277, 'accumulated_submission_time': 40782.78246617317, 'accumulated_eval_time': 4057.6739218235016, 'accumulated_logging_time': 1.570603847503662}
I0211 23:33:02.962626 140063075153664 logging_writer.py:48] [48000] accumulated_eval_time=4057.673922, accumulated_logging_time=1.570604, accumulated_submission_time=40782.782466, global_step=48000, preemption_count=0, score=40782.782466, test/ctc_loss=0.22965063154697418, test/num_examples=2472, test/wer=0.073995, total_duration=44844.317896, train/ctc_loss=0.1278073489665985, train/wer=0.046068, validation/ctc_loss=0.42751795053482056, validation/num_examples=5348, validation/wer=0.122025
I0211 23:33:02.985494 140063066760960 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40782.782466
I0211 23:33:03.214101 140218947737408 checkpoints.py:490] Saving checkpoint at step: 48000
I0211 23:33:04.240912 140218947737408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2/checkpoint_48000
I0211 23:33:04.259889 140218947737408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_2/checkpoint_48000.
I0211 23:33:05.599826 140218947737408 submission_runner.py:583] Tuning trial 2/5
I0211 23:33:05.600071 140218947737408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0211 23:33:05.612950 140218947737408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(29.524904, dtype=float32), 'train/wer': 2.289365492296078, 'validation/ctc_loss': Array(28.14923, dtype=float32), 'validation/wer': 2.1855334678548326, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256151, dtype=float32), 'test/wer': 2.344362521073264, 'test/num_examples': 2472, 'score': 15.45589280128479, 'total_duration': 174.47113847732544, 'accumulated_submission_time': 15.45589280128479, 'accumulated_eval_time': 159.01518487930298, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1699, {'train/ctc_loss': Array(6.5281034, dtype=float32), 'train/wer': 0.9363028259749184, 'validation/ctc_loss': Array(6.363745, dtype=float32), 'validation/wer': 0.8913754984214642, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3231583, dtype=float32), 'test/wer': 0.8925923669083745, 'test/num_examples': 2472, 'score': 1456.418749332428, 'total_duration': 1730.0256130695343, 'accumulated_submission_time': 1456.418749332428, 'accumulated_eval_time': 273.5006091594696, 'accumulated_logging_time': 0.029590606689453125, 'global_step': 1699, 'preemption_count': 0}), (3407, {'train/ctc_loss': Array(4.3004704, dtype=float32), 'train/wer': 0.8745647056270951, 'validation/ctc_loss': Array(4.174814, dtype=float32), 'validation/wer': 0.8286105988781293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.7111945, dtype=float32), 'test/wer': 0.786301870696484, 'test/num_examples': 2472, 'score': 2896.712926864624, 'total_duration': 3287.10879445076, 'accumulated_submission_time': 2896.712926864624, 'accumulated_eval_time': 390.15674901008606, 'accumulated_logging_time': 0.08199119567871094, 'global_step': 3407, 'preemption_count': 0}), (5093, {'train/ctc_loss': Array(0.91354764, dtype=float32), 'train/wer': 0.27189510006901313, 'validation/ctc_loss': Array(0.99934816, dtype=float32), 'validation/wer': 0.279000164129102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6458774, dtype=float32), 'test/wer': 0.20561412061015985, 'test/num_examples': 2472, 'score': 4337.139240503311, 'total_duration': 4862.266368150711, 'accumulated_submission_time': 4337.139240503311, 'accumulated_eval_time': 524.7541451454163, 'accumulated_logging_time': 0.1371448040008545, 'global_step': 5093, 'preemption_count': 0}), (6795, {'train/ctc_loss': Array(0.68832296, dtype=float32), 'train/wer': 0.21781532227602074, 'validation/ctc_loss': Array(0.79865134, dtype=float32), 'validation/wer': 0.2307944814003109, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49709377, dtype=float32), 'test/wer': 0.16178173176527938, 'test/num_examples': 2472, 'score': 5777.388965606689, 'total_duration': 6435.596847295761, 'accumulated_submission_time': 5777.388965606689, 'accumulated_eval_time': 657.7007877826691, 'accumulated_logging_time': 0.19022035598754883, 'global_step': 6795, 'preemption_count': 0}), (8514, {'train/ctc_loss': Array(0.5999228, dtype=float32), 'train/wer': 0.19357649288273668, 'validation/ctc_loss': Array(0.72859186, dtype=float32), 'validation/wer': 0.2121706556474893, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44906363, dtype=float32), 'test/wer': 0.14691365547498628, 'test/num_examples': 2472, 'score': 7217.569427251816, 'total_duration': 8010.0493178367615, 'accumulated_submission_time': 7217.569427251816, 'accumulated_eval_time': 791.83607172966, 'accumulated_logging_time': 0.2452392578125, 'global_step': 8514, 'preemption_count': 0}), (10205, {'train/ctc_loss': Array(0.53137237, dtype=float32), 'train/wer': 0.17415988521980855, 'validation/ctc_loss': Array(0.68165743, dtype=float32), 'validation/wer': 0.19699354103710284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40845668, dtype=float32), 'test/wer': 0.13220807182174557, 'test/num_examples': 2472, 'score': 8658.101991176605, 'total_duration': 9587.016305446625, 'accumulated_submission_time': 8658.101991176605, 'accumulated_eval_time': 928.1383287906647, 'accumulated_logging_time': 0.29754114151000977, 'global_step': 10205, 'preemption_count': 0}), (11908, {'train/ctc_loss': Array(0.50215024, dtype=float32), 'train/wer': 0.166799780931952, 'validation/ctc_loss': Array(0.64968437, dtype=float32), 'validation/wer': 0.18750301707908126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3866501, dtype=float32), 'test/wer': 0.12453029472102045, 'test/num_examples': 2472, 'score': 10098.519153118134, 'total_duration': 11160.529643058777, 'accumulated_submission_time': 10098.519153118134, 'accumulated_eval_time': 1061.1018662452698, 'accumulated_logging_time': 0.3483004570007324, 'global_step': 11908, 'preemption_count': 0}), (13621, {'train/ctc_loss': Array(0.44562355, dtype=float32), 'train/wer': 0.14918760880239254, 'validation/ctc_loss': Array(0.6191698, dtype=float32), 'validation/wer': 0.1785145350801819, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3623559, dtype=float32), 'test/wer': 0.11857900188897691, 'test/num_examples': 2472, 'score': 11538.94403219223, 'total_duration': 12734.627663373947, 'accumulated_submission_time': 11538.94403219223, 'accumulated_eval_time': 1194.6305973529816, 'accumulated_logging_time': 0.4078514575958252, 'global_step': 13621, 'preemption_count': 0}), (15293, {'train/ctc_loss': Array(0.44809407, dtype=float32), 'train/wer': 0.1485734782534432, 'validation/ctc_loss': Array(0.6029571, dtype=float32), 'validation/wer': 0.17495196810102628, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34927937, dtype=float32), 'test/wer': 0.11449637438303577, 'test/num_examples': 2472, 'score': 12979.073741436005, 'total_duration': 14308.919946432114, 'accumulated_submission_time': 12979.073741436005, 'accumulated_eval_time': 1328.6569051742554, 'accumulated_logging_time': 0.4639625549316406, 'global_step': 15293, 'preemption_count': 0}), (17003, {'train/ctc_loss': Array(0.42328086, dtype=float32), 'train/wer': 0.1401969991726482, 'validation/ctc_loss': Array(0.58921814, dtype=float32), 'validation/wer': 0.1701149869179451, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33758086, dtype=float32), 'test/wer': 0.10907318262141247, 'test/num_examples': 2472, 'score': 14419.393539190292, 'total_duration': 15885.092690944672, 'accumulated_submission_time': 14419.393539190292, 'accumulated_eval_time': 1464.3784432411194, 'accumulated_logging_time': 0.5136878490447998, 'global_step': 17003, 'preemption_count': 0}), (18687, {'train/ctc_loss': Array(0.40703046, dtype=float32), 'train/wer': 0.13800737190485152, 'validation/ctc_loss': Array(0.56702036, dtype=float32), 'validation/wer': 0.1645442521023007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32256627, dtype=float32), 'test/wer': 0.10584364145999635, 'test/num_examples': 2472, 'score': 15859.808149814606, 'total_duration': 17461.924512147903, 'accumulated_submission_time': 15859.808149814606, 'accumulated_eval_time': 1600.6581590175629, 'accumulated_logging_time': 0.5700757503509521, 'global_step': 18687, 'preemption_count': 0}), (20376, {'train/ctc_loss': Array(0.35868832, dtype=float32), 'train/wer': 0.12001266445415637, 'validation/ctc_loss': Array(0.5564612, dtype=float32), 'validation/wer': 0.160730664143584, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31212524, dtype=float32), 'test/wer': 0.10364999085978917, 'test/num_examples': 2472, 'score': 17300.169142961502, 'total_duration': 19035.714549064636, 'accumulated_submission_time': 17300.169142961502, 'accumulated_eval_time': 1733.951742887497, 'accumulated_logging_time': 0.6257908344268799, 'global_step': 20376, 'preemption_count': 0}), (22081, {'train/ctc_loss': Array(0.32458857, dtype=float32), 'train/wer': 0.1098593623444518, 'validation/ctc_loss': Array(0.5423642, dtype=float32), 'validation/wer': 0.15747704606234975, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30617353, dtype=float32), 'test/wer': 0.10113135498547722, 'test/num_examples': 2472, 'score': 18740.1226541996, 'total_duration': 20611.221473693848, 'accumulated_submission_time': 18740.1226541996, 'accumulated_eval_time': 1869.3748636245728, 'accumulated_logging_time': 0.6759476661682129, 'global_step': 22081, 'preemption_count': 0}), (23760, {'train/ctc_loss': Array(0.36119395, dtype=float32), 'train/wer': 0.12499858879844655, 'validation/ctc_loss': Array(0.5224131, dtype=float32), 'validation/wer': 0.15162632630796413, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29169166, dtype=float32), 'test/wer': 0.09639875693132655, 'test/num_examples': 2472, 'score': 20180.643664598465, 'total_duration': 22185.8266851902, 'accumulated_submission_time': 20180.643664598465, 'accumulated_eval_time': 2003.326298236847, 'accumulated_logging_time': 0.7289724349975586, 'global_step': 23760, 'preemption_count': 0}), (25457, {'train/ctc_loss': Array(0.3251517, dtype=float32), 'train/wer': 0.10952787258248009, 'validation/ctc_loss': Array(0.51066995, dtype=float32), 'validation/wer': 0.1475134441043861, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28612068, dtype=float32), 'test/wer': 0.09260049154022708, 'test/num_examples': 2472, 'score': 21621.02370738983, 'total_duration': 23759.84077358246, 'accumulated_submission_time': 21621.02370738983, 'accumulated_eval_time': 2136.82527923584, 'accumulated_logging_time': 0.7829153537750244, 'global_step': 25457, 'preemption_count': 0}), (27173, {'train/ctc_loss': Array(0.3157251, dtype=float32), 'train/wer': 0.10562322800284062, 'validation/ctc_loss': Array(0.50370485, dtype=float32), 'validation/wer': 0.14554389488013747, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28231353, dtype=float32), 'test/wer': 0.0910364999085979, 'test/num_examples': 2472, 'score': 23060.985421419144, 'total_duration': 25335.851276874542, 'accumulated_submission_time': 23060.985421419144, 'accumulated_eval_time': 2272.7303400039673, 'accumulated_logging_time': 0.8456952571868896, 'global_step': 27173, 'preemption_count': 0}), (28848, {'train/ctc_loss': Array(0.28317812, dtype=float32), 'train/wer': 0.09876352395672335, 'validation/ctc_loss': Array(0.4912923, dtype=float32), 'validation/wer': 0.14153721386021995, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27055633, dtype=float32), 'test/wer': 0.08774602400828713, 'test/num_examples': 2472, 'score': 24501.122561454773, 'total_duration': 26909.666396856308, 'accumulated_submission_time': 24501.122561454773, 'accumulated_eval_time': 2406.2770829200745, 'accumulated_logging_time': 0.8960163593292236, 'global_step': 28848, 'preemption_count': 0}), (30561, {'train/ctc_loss': Array(0.2763036, dtype=float32), 'train/wer': 0.09663587059888074, 'validation/ctc_loss': Array(0.48683694, dtype=float32), 'validation/wer': 0.14026279965629435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26829788, dtype=float32), 'test/wer': 0.08750228505270854, 'test/num_examples': 2472, 'score': 25941.73451423645, 'total_duration': 28487.4870493412, 'accumulated_submission_time': 25941.73451423645, 'accumulated_eval_time': 2543.348155975342, 'accumulated_logging_time': 0.951606273651123, 'global_step': 30561, 'preemption_count': 0}), (32288, {'train/ctc_loss': Array(0.25569287, dtype=float32), 'train/wer': 0.0897257371193316, 'validation/ctc_loss': Array(0.46944138, dtype=float32), 'validation/wer': 0.13578304063643473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26102412, dtype=float32), 'test/wer': 0.08433367863018706, 'test/num_examples': 2472, 'score': 27382.11405301094, 'total_duration': 30062.6204662323, 'accumulated_submission_time': 27382.11405301094, 'accumulated_eval_time': 2677.9681293964386, 'accumulated_logging_time': 1.001542568206787, 'global_step': 32288, 'preemption_count': 0}), (33983, {'train/ctc_loss': Array(0.15601371, dtype=float32), 'train/wer': 0.05595349430368985, 'validation/ctc_loss': Array(0.46224105, dtype=float32), 'validation/wer': 0.1343155333713083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2518262, dtype=float32), 'test/wer': 0.08207909329108524, 'test/num_examples': 2472, 'score': 28822.875962257385, 'total_duration': 31657.9614071846, 'accumulated_submission_time': 28822.875962257385, 'accumulated_eval_time': 2832.4212596416473, 'accumulated_logging_time': 1.0486581325531006, 'global_step': 33983, 'preemption_count': 0}), (35663, {'train/ctc_loss': Array(0.15231837, dtype=float32), 'train/wer': 0.05323829957452998, 'validation/ctc_loss': Array(0.45944563, dtype=float32), 'validation/wer': 0.1315349932900161, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24787179, dtype=float32), 'test/wer': 0.07962139215566795, 'test/num_examples': 2472, 'score': 30262.88162112236, 'total_duration': 33234.50987505913, 'accumulated_submission_time': 30262.88162112236, 'accumulated_eval_time': 2968.835390806198, 'accumulated_logging_time': 1.0996172428131104, 'global_step': 35663, 'preemption_count': 0}), (37356, {'train/ctc_loss': Array(0.1572897, dtype=float32), 'train/wer': 0.055789000766405035, 'validation/ctc_loss': Array(0.44850588, dtype=float32), 'validation/wer': 0.1289282369638047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23940639, dtype=float32), 'test/wer': 0.07734649523693457, 'test/num_examples': 2472, 'score': 31703.973056077957, 'total_duration': 34811.08276438713, 'accumulated_submission_time': 31703.973056077957, 'accumulated_eval_time': 3104.18021607399, 'accumulated_logging_time': 1.1535747051239014, 'global_step': 37356, 'preemption_count': 0}), (39037, {'train/ctc_loss': Array(0.13856104, dtype=float32), 'train/wer': 0.04972243092077803, 'validation/ctc_loss': Array(0.44071487, dtype=float32), 'validation/wer': 0.12696834239261612, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2376608, dtype=float32), 'test/wer': 0.07629029309609409, 'test/num_examples': 2472, 'score': 33144.23564553261, 'total_duration': 36389.54399847984, 'accumulated_submission_time': 33144.23564553261, 'accumulated_eval_time': 3242.2381801605225, 'accumulated_logging_time': 1.2139358520507812, 'global_step': 39037, 'preemption_count': 0}), (40732, {'train/ctc_loss': Array(0.1460249, dtype=float32), 'train/wer': 0.051847033923154436, 'validation/ctc_loss': Array(0.4341819, dtype=float32), 'validation/wer': 0.1248636280255269, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23402792, dtype=float32), 'test/wer': 0.07533564885341133, 'test/num_examples': 2472, 'score': 34584.34844779968, 'total_duration': 37967.1045422554, 'accumulated_submission_time': 34584.34844779968, 'accumulated_eval_time': 3379.5433967113495, 'accumulated_logging_time': 1.2748100757598877, 'global_step': 40732, 'preemption_count': 0}), (42425, {'train/ctc_loss': Array(0.13446575, dtype=float32), 'train/wer': 0.04884225373182173, 'validation/ctc_loss': Array(0.43170702, dtype=float32), 'validation/wer': 0.12386919876034255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23212434, dtype=float32), 'test/wer': 0.07478723620335953, 'test/num_examples': 2472, 'score': 36024.96976852417, 'total_duration': 39542.14425396919, 'accumulated_submission_time': 36024.96976852417, 'accumulated_eval_time': 3513.8202295303345, 'accumulated_logging_time': 1.3349525928497314, 'global_step': 42425, 'preemption_count': 0}), (44094, {'train/ctc_loss': Array(0.15975782, dtype=float32), 'train/wer': 0.0529069018799897, 'validation/ctc_loss': Array(0.4287825, dtype=float32), 'validation/wer': 0.12279753227067786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23024347, dtype=float32), 'test/wer': 0.07438100461072858, 'test/num_examples': 2472, 'score': 37465.898456811905, 'total_duration': 41118.98565912247, 'accumulated_submission_time': 37465.898456811905, 'accumulated_eval_time': 3649.597271680832, 'accumulated_logging_time': 1.391124963760376, 'global_step': 44094, 'preemption_count': 0}), (45785, {'train/ctc_loss': Array(0.14198789, dtype=float32), 'train/wer': 0.05093299406276505, 'validation/ctc_loss': Array(0.42813993, dtype=float32), 'validation/wer': 0.1223051449646157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22979745, dtype=float32), 'test/wer': 0.07413726565515, 'test/num_examples': 2472, 'score': 38906.245678424835, 'total_duration': 42694.73621749878, 'accumulated_submission_time': 38906.245678424835, 'accumulated_eval_time': 3784.859577178955, 'accumulated_logging_time': 1.4510438442230225, 'global_step': 45785, 'preemption_count': 0}), (47459, {'train/ctc_loss': Array(0.1208012, dtype=float32), 'train/wer': 0.04385094853777968, 'validation/ctc_loss': Array(0.4275235, dtype=float32), 'validation/wer': 0.12211205190341486, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22964966, dtype=float32), 'test/wer': 0.07391383827920298, 'test/num_examples': 2472, 'score': 40346.5943479538, 'total_duration': 44270.86405515671, 'accumulated_submission_time': 40346.5943479538, 'accumulated_eval_time': 3920.4955565929413, 'accumulated_logging_time': 1.513451337814331, 'global_step': 47459, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.12780735, dtype=float32), 'train/wer': 0.0460682191722342, 'validation/ctc_loss': Array(0.42751795, dtype=float32), 'validation/wer': 0.12202516002587448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22965063, dtype=float32), 'test/wer': 0.07399508459772916, 'test/num_examples': 2472, 'score': 40782.78246617317, 'total_duration': 44844.3178961277, 'accumulated_submission_time': 40782.78246617317, 'accumulated_eval_time': 4057.6739218235016, 'accumulated_logging_time': 1.570603847503662, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0211 23:33:05.613229 140218947737408 submission_runner.py:586] Timing: 40782.78246617317
I0211 23:33:05.613306 140218947737408 submission_runner.py:588] Total number of evals: 30
I0211 23:33:05.613646 140218947737408 submission_runner.py:589] ====================
I0211 23:33:05.613747 140218947737408 submission_runner.py:542] Using RNG seed 2622380006
I0211 23:33:05.617178 140218947737408 submission_runner.py:551] --- Tuning run 3/5 ---
I0211 23:33:05.617319 140218947737408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3.
I0211 23:33:05.619006 140218947737408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3/hparams.json.
I0211 23:33:05.621175 140218947737408 submission_runner.py:206] Initializing dataset.
I0211 23:33:05.621298 140218947737408 submission_runner.py:213] Initializing model.
I0211 23:33:06.749725 140218947737408 submission_runner.py:255] Initializing optimizer.
I0211 23:33:06.890156 140218947737408 submission_runner.py:262] Initializing metrics bundle.
I0211 23:33:06.890341 140218947737408 submission_runner.py:280] Initializing checkpoint and logger.
I0211 23:33:06.895124 140218947737408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3 with prefix checkpoint_
I0211 23:33:06.895257 140218947737408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3/meta_data_0.json.
I0211 23:33:06.895604 140218947737408 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 23:33:06.895678 140218947737408 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 23:33:07.472132 140218947737408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 23:33:07.995648 140218947737408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3/flags_0.json.
I0211 23:33:08.016472 140218947737408 submission_runner.py:314] Starting training loop.
I0211 23:33:08.020015 140218947737408 input_pipeline.py:20] Loading split = train-clean-100
I0211 23:33:08.062151 140218947737408 input_pipeline.py:20] Loading split = train-clean-360
I0211 23:33:08.194853 140218947737408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 23:33:23.224430 140062018848512 logging_writer.py:48] [0] global_step=0, grad_norm=21.116363525390625, loss=32.88337326049805
I0211 23:33:23.238830 140218947737408 spec.py:321] Evaluating on the training split.
I0211 23:34:43.679770 140218947737408 spec.py:333] Evaluating on the validation split.
I0211 23:35:39.382831 140218947737408 spec.py:349] Evaluating on the test split.
I0211 23:36:07.644212 140218947737408 submission_runner.py:408] Time since start: 179.62s, 	Step: 1, 	{'train/ctc_loss': Array(29.058031, dtype=float32), 'train/wer': 2.360243429556226, 'validation/ctc_loss': Array(28.14937, dtype=float32), 'validation/wer': 2.1855045038956527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256304, dtype=float32), 'test/wer': 2.3443422094936324, 'test/num_examples': 2472, 'score': 15.222284317016602, 'total_duration': 179.6249499320984, 'accumulated_submission_time': 15.222284317016602, 'accumulated_eval_time': 164.4026050567627, 'accumulated_logging_time': 0}
I0211 23:36:07.662098 140062455633664 logging_writer.py:48] [1] accumulated_eval_time=164.402605, accumulated_logging_time=0, accumulated_submission_time=15.222284, global_step=1, preemption_count=0, score=15.222284, test/ctc_loss=28.256303787231445, test/num_examples=2472, test/wer=2.344342, total_duration=179.624950, train/ctc_loss=29.05803108215332, train/wer=2.360243, validation/ctc_loss=28.149370193481445, validation/num_examples=5348, validation/wer=2.185505
I0211 23:37:32.432077 140062346528512 logging_writer.py:48] [100] global_step=100, grad_norm=5.789957523345947, loss=9.1045503616333
I0211 23:38:49.131059 140062354921216 logging_writer.py:48] [200] global_step=200, grad_norm=1.6940492391586304, loss=6.49839973449707
I0211 23:40:06.052027 140062346528512 logging_writer.py:48] [300] global_step=300, grad_norm=0.5279651284217834, loss=5.913054943084717
I0211 23:41:22.949269 140062354921216 logging_writer.py:48] [400] global_step=400, grad_norm=0.4028226137161255, loss=5.855238914489746
I0211 23:42:47.224656 140062346528512 logging_writer.py:48] [500] global_step=500, grad_norm=0.36278560757637024, loss=5.818172931671143
I0211 23:44:14.868236 140062354921216 logging_writer.py:48] [600] global_step=600, grad_norm=0.4295154809951782, loss=5.798171520233154
I0211 23:45:46.408117 140062346528512 logging_writer.py:48] [700] global_step=700, grad_norm=0.7810807228088379, loss=5.755054473876953
I0211 23:47:16.992419 140062354921216 logging_writer.py:48] [800] global_step=800, grad_norm=0.5919283032417297, loss=5.636862277984619
I0211 23:48:43.101943 140062346528512 logging_writer.py:48] [900] global_step=900, grad_norm=0.477433979511261, loss=5.523805141448975
I0211 23:50:12.725941 140062354921216 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5241170525550842, loss=5.406254768371582
I0211 23:51:36.127532 140062455633664 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.365254282951355, loss=5.201841354370117
I0211 23:52:53.062930 140062447240960 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8588486909866333, loss=4.808166027069092
I0211 23:54:10.429902 140062455633664 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.786983847618103, loss=4.345128536224365
I0211 23:55:35.266327 140062447240960 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.5031659603118896, loss=4.027467727661133
I0211 23:57:04.399101 140062455633664 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.2064807415008545, loss=3.7243542671203613
I0211 23:58:31.788070 140062447240960 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.9470832347869873, loss=3.495842933654785
I0212 00:00:01.823760 140062455633664 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.671643614768982, loss=3.3120479583740234
I0212 00:00:07.801564 140218947737408 spec.py:321] Evaluating on the training split.
I0212 00:00:46.969540 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 00:01:34.789060 140218947737408 spec.py:349] Evaluating on the test split.
I0212 00:01:59.175786 140218947737408 submission_runner.py:408] Time since start: 1731.15s, 	Step: 1708, 	{'train/ctc_loss': Array(6.197022, dtype=float32), 'train/wer': 0.943809484010489, 'validation/ctc_loss': Array(6.2215743, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.1304574, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1455.2773833274841, 'total_duration': 1731.1534314155579, 'accumulated_submission_time': 1455.2773833274841, 'accumulated_eval_time': 275.77101039886475, 'accumulated_logging_time': 0.028853893280029297}
I0212 00:01:59.214122 140062455633664 logging_writer.py:48] [1708] accumulated_eval_time=275.771010, accumulated_logging_time=0.028854, accumulated_submission_time=1455.277383, global_step=1708, preemption_count=0, score=1455.277383, test/ctc_loss=6.130457401275635, test/num_examples=2472, test/wer=0.899580, total_duration=1731.153431, train/ctc_loss=6.197021961212158, train/wer=0.943809, validation/ctc_loss=6.221574306488037, validation/num_examples=5348, validation/wer=0.896618
I0212 00:03:10.028137 140062447240960 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.171462297439575, loss=3.130810499191284
I0212 00:04:25.807370 140062455633664 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.353652238845825, loss=3.102766990661621
I0212 00:05:51.815768 140062447240960 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.8406260013580322, loss=3.02081298828125
I0212 00:07:21.303645 140062455633664 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.4071874618530273, loss=2.890275239944458
I0212 00:08:39.408374 140062447240960 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.4005696773529053, loss=2.724622964859009
I0212 00:09:55.717524 140062455633664 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.4578659534454346, loss=2.677999258041382
I0212 00:11:17.960137 140062447240960 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.1394710540771484, loss=2.6181061267852783
I0212 00:12:42.437109 140062455633664 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.489163637161255, loss=2.6183059215545654
I0212 00:14:11.060976 140062447240960 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.8208696842193604, loss=2.5581953525543213
I0212 00:15:41.768333 140062455633664 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.9328858852386475, loss=2.491492986679077
I0212 00:17:08.592022 140062447240960 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.931290149688721, loss=2.4877278804779053
I0212 00:18:37.466535 140062455633664 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.386731147766113, loss=2.420811414718628
I0212 00:20:06.780146 140062447240960 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.1416232585906982, loss=2.4097373485565186
I0212 00:21:37.779158 140062455633664 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.3935792446136475, loss=2.250701427459717
I0212 00:22:53.627300 140062447240960 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.135164499282837, loss=2.2768309116363525
I0212 00:24:13.654876 140062455633664 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.748546600341797, loss=2.3250253200531006
I0212 00:25:35.064362 140062447240960 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.360614776611328, loss=2.2279884815216064
I0212 00:25:59.491594 140218947737408 spec.py:321] Evaluating on the training split.
I0212 00:26:41.454117 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 00:27:30.692279 140218947737408 spec.py:349] Evaluating on the test split.
I0212 00:27:55.738667 140218947737408 submission_runner.py:408] Time since start: 3287.72s, 	Step: 3430, 	{'train/ctc_loss': Array(4.192091, dtype=float32), 'train/wer': 0.8861044829697827, 'validation/ctc_loss': Array(4.297596, dtype=float32), 'validation/wer': 0.8586076059356808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.945421, dtype=float32), 'test/wer': 0.8374667397883533, 'test/num_examples': 2472, 'score': 2895.461868286133, 'total_duration': 3287.7166328430176, 'accumulated_submission_time': 2895.461868286133, 'accumulated_eval_time': 392.01263880729675, 'accumulated_logging_time': 0.08412456512451172}
I0212 00:27:55.774462 140062455633664 logging_writer.py:48] [3430] accumulated_eval_time=392.012639, accumulated_logging_time=0.084125, accumulated_submission_time=2895.461868, global_step=3430, preemption_count=0, score=2895.461868, test/ctc_loss=3.945420980453491, test/num_examples=2472, test/wer=0.837467, total_duration=3287.716633, train/ctc_loss=4.19209098815918, train/wer=0.886104, validation/ctc_loss=4.297595977783203, validation/num_examples=5348, validation/wer=0.858608
I0212 00:28:49.203847 140062447240960 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.6239211559295654, loss=2.230114221572876
I0212 00:30:04.213435 140062455633664 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.35054087638855, loss=2.1503262519836426
I0212 00:31:28.157298 140062447240960 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.224592924118042, loss=2.1213719844818115
I0212 00:32:58.570066 140062455633664 logging_writer.py:48] [3800] global_step=3800, grad_norm=6.273107528686523, loss=2.1745569705963135
I0212 00:34:30.642967 140062447240960 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.429079055786133, loss=2.190943717956543
I0212 00:35:59.131503 140062455633664 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.9727067947387695, loss=2.138002872467041
I0212 00:37:30.468505 140062447240960 logging_writer.py:48] [4100] global_step=4100, grad_norm=5.537511825561523, loss=2.0599286556243896
I0212 00:38:53.004748 140062455633664 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.558166265487671, loss=2.056335926055908
I0212 00:40:10.526450 140062447240960 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.794476270675659, loss=2.110255718231201
I0212 00:41:28.376923 140062455633664 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.765286684036255, loss=2.0816562175750732
I0212 00:42:54.111543 140062447240960 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.2204999923706055, loss=2.0683367252349854
I0212 00:44:20.348178 140062455633664 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.9352176189422607, loss=2.000047206878662
I0212 00:45:49.612051 140062447240960 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.056828498840332, loss=1.9950639009475708
I0212 00:47:18.165062 140062455633664 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.0310022830963135, loss=1.998595118522644
I0212 00:48:48.042193 140062447240960 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.406627893447876, loss=1.967948317527771
I0212 00:50:18.683224 140062455633664 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.10538649559021, loss=1.9506752490997314
I0212 00:51:47.908264 140062447240960 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.7991015911102295, loss=2.0305190086364746
I0212 00:51:56.383868 140218947737408 spec.py:321] Evaluating on the training split.
I0212 00:52:52.468628 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 00:53:44.941774 140218947737408 spec.py:349] Evaluating on the test split.
I0212 00:54:11.851070 140218947737408 submission_runner.py:408] Time since start: 4863.83s, 	Step: 5111, 	{'train/ctc_loss': Array(0.75358486, dtype=float32), 'train/wer': 0.24081478531193262, 'validation/ctc_loss': Array(1.097035, dtype=float32), 'validation/wer': 0.30279888392210624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.74433875, dtype=float32), 'test/wer': 0.23441594052769485, 'test/num_examples': 2472, 'score': 4335.978147506714, 'total_duration': 4863.8278868198395, 'accumulated_submission_time': 4335.978147506714, 'accumulated_eval_time': 527.4732053279877, 'accumulated_logging_time': 0.1376943588256836}
I0212 00:54:11.890129 140062491473664 logging_writer.py:48] [5111] accumulated_eval_time=527.473205, accumulated_logging_time=0.137694, accumulated_submission_time=4335.978148, global_step=5111, preemption_count=0, score=4335.978148, test/ctc_loss=0.7443387508392334, test/num_examples=2472, test/wer=0.234416, total_duration=4863.827887, train/ctc_loss=0.7535848617553711, train/wer=0.240815, validation/ctc_loss=1.0970350503921509, validation/num_examples=5348, validation/wer=0.302799
I0212 00:55:25.009484 140062491473664 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.175105094909668, loss=1.9066277742385864
I0212 00:56:43.484642 140062483080960 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.720848083496094, loss=1.9532670974731445
I0212 00:58:03.879247 140062491473664 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.7750213146209717, loss=1.9378342628479004
I0212 00:59:26.438208 140062483080960 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.938262701034546, loss=1.941785454750061
I0212 01:00:53.705623 140062491473664 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.172257423400879, loss=1.8905689716339111
I0212 01:02:22.428753 140062483080960 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.448604583740234, loss=1.8976651430130005
I0212 01:03:53.782024 140062491473664 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.397540092468262, loss=1.882786750793457
I0212 01:05:23.429406 140062483080960 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.6455130577087402, loss=1.9292806386947632
I0212 01:06:53.860189 140062491473664 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.170067548751831, loss=1.8583638668060303
I0212 01:08:23.428351 140062483080960 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.7713325023651123, loss=1.8812336921691895
I0212 01:09:53.314115 140062491473664 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.082235336303711, loss=1.8313541412353516
I0212 01:11:10.001438 140062483080960 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.0285634994506836, loss=1.8536592721939087
I0212 01:12:28.438288 140062491473664 logging_writer.py:48] [6400] global_step=6400, grad_norm=5.103252410888672, loss=1.849382758140564
I0212 01:13:51.975823 140062483080960 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.272852897644043, loss=1.7927507162094116
I0212 01:15:17.062583 140062491473664 logging_writer.py:48] [6600] global_step=6600, grad_norm=4.745210647583008, loss=1.801033616065979
I0212 01:16:45.612014 140062483080960 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.279699325561523, loss=1.8346483707427979
I0212 01:18:12.210672 140218947737408 spec.py:321] Evaluating on the training split.
I0212 01:19:09.278637 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 01:20:03.217973 140218947737408 spec.py:349] Evaluating on the test split.
I0212 01:20:29.853013 140218947737408 submission_runner.py:408] Time since start: 6441.83s, 	Step: 6798, 	{'train/ctc_loss': Array(0.56135374, dtype=float32), 'train/wer': 0.1861137746199915, 'validation/ctc_loss': Array(0.89679164, dtype=float32), 'validation/wer': 0.2572771947440069, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57972324, dtype=float32), 'test/wer': 0.18800398106960778, 'test/num_examples': 2472, 'score': 5776.205506324768, 'total_duration': 6441.831294298172, 'accumulated_submission_time': 5776.205506324768, 'accumulated_eval_time': 665.1103904247284, 'accumulated_logging_time': 0.19458389282226562}
I0212 01:20:29.890074 140062491473664 logging_writer.py:48] [6798] accumulated_eval_time=665.110390, accumulated_logging_time=0.194584, accumulated_submission_time=5776.205506, global_step=6798, preemption_count=0, score=5776.205506, test/ctc_loss=0.5797232389450073, test/num_examples=2472, test/wer=0.188004, total_duration=6441.831294, train/ctc_loss=0.5613537430763245, train/wer=0.186114, validation/ctc_loss=0.8967916369438171, validation/num_examples=5348, validation/wer=0.257277
I0212 01:20:32.289631 140062483080960 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.4808456897735596, loss=1.7959282398223877
I0212 01:21:47.479645 140062491473664 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.1928932666778564, loss=1.7964357137680054
I0212 01:23:02.811465 140062483080960 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.876345157623291, loss=1.713891625404358
I0212 01:24:30.550228 140062491473664 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.2030601501464844, loss=1.7063336372375488
I0212 01:25:59.496655 140062483080960 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.279186725616455, loss=1.8392378091812134
I0212 01:27:21.813166 140062491473664 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.562605381011963, loss=1.7009364366531372
I0212 01:28:40.288326 140062483080960 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.7711563110351562, loss=1.7559216022491455
I0212 01:30:02.060008 140062491473664 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.101651906967163, loss=1.8026376962661743
I0212 01:31:22.647288 140062483080960 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.894918203353882, loss=1.8609739542007446
I0212 01:32:50.438570 140062491473664 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.1233863830566406, loss=1.7908356189727783
I0212 01:34:20.279511 140062483080960 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.004680633544922, loss=1.8395942449569702
I0212 01:35:47.387603 140062491473664 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.551920175552368, loss=1.7347556352615356
I0212 01:37:16.507464 140062483080960 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.738849401473999, loss=1.707541823387146
I0212 01:38:48.143906 140062491473664 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.963561534881592, loss=1.7207883596420288
I0212 01:40:19.138372 140062483080960 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.073959827423096, loss=1.7597922086715698
I0212 01:41:45.735250 140062491473664 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.0750350952148438, loss=1.6528456211090088
I0212 01:43:01.974490 140062483080960 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.0499343872070312, loss=1.743482232093811
I0212 01:44:20.653892 140062491473664 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.788543939590454, loss=1.7268460988998413
I0212 01:44:30.039474 140218947737408 spec.py:321] Evaluating on the training split.
I0212 01:45:25.687175 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 01:46:17.780495 140218947737408 spec.py:349] Evaluating on the test split.
I0212 01:46:45.533539 140218947737408 submission_runner.py:408] Time since start: 8017.51s, 	Step: 8513, 	{'train/ctc_loss': Array(0.43911177, dtype=float32), 'train/wer': 0.14939553226376198, 'validation/ctc_loss': Array(0.7938834, dtype=float32), 'validation/wer': 0.2276953377680373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49849275, dtype=float32), 'test/wer': 0.1604817906688603, 'test/num_examples': 2472, 'score': 7216.254794597626, 'total_duration': 8017.511333227158, 'accumulated_submission_time': 7216.254794597626, 'accumulated_eval_time': 800.5987780094147, 'accumulated_logging_time': 0.25530076026916504}
I0212 01:46:45.572417 140062645073664 logging_writer.py:48] [8513] accumulated_eval_time=800.598778, accumulated_logging_time=0.255301, accumulated_submission_time=7216.254795, global_step=8513, preemption_count=0, score=7216.254795, test/ctc_loss=0.4984927475452423, test/num_examples=2472, test/wer=0.160482, total_duration=8017.511333, train/ctc_loss=0.43911176919937134, train/wer=0.149396, validation/ctc_loss=0.7938833832740784, validation/num_examples=5348, validation/wer=0.227695
I0212 01:47:52.102082 140062636680960 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.959829568862915, loss=1.745983600616455
I0212 01:49:07.039711 140062645073664 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.5424201488494873, loss=1.7753050327301025
I0212 01:50:26.617345 140062636680960 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.9670233726501465, loss=1.692341685295105
I0212 01:51:58.272439 140062645073664 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.075296640396118, loss=1.6654072999954224
I0212 01:53:28.373914 140062636680960 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.239971399307251, loss=1.683237910270691
I0212 01:54:58.241692 140062645073664 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.5955865383148193, loss=1.739936351776123
I0212 01:56:24.372169 140062636680960 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.5920846462249756, loss=1.6417300701141357
I0212 01:57:54.101424 140062645073664 logging_writer.py:48] [9300] global_step=9300, grad_norm=4.168919086456299, loss=1.7379488945007324
I0212 01:59:08.993078 140062636680960 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.778341293334961, loss=1.7048512697219849
I0212 02:00:27.790192 140062645073664 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.482942581176758, loss=1.686780333518982
I0212 02:01:49.300299 140062636680960 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.8203701972961426, loss=1.6392879486083984
I0212 02:03:14.092747 140062645073664 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8278894424438477, loss=1.6264636516571045
I0212 02:04:43.077544 140062636680960 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.3132379055023193, loss=1.583511471748352
I0212 02:06:13.074707 140062645073664 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.276898145675659, loss=1.7779744863510132
I0212 02:07:41.913567 140062636680960 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.5167744159698486, loss=1.7427664995193481
I0212 02:09:11.998846 140062645073664 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.271345853805542, loss=1.6671922206878662
I0212 02:10:41.205868 140062636680960 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.477036237716675, loss=1.687156081199646
I0212 02:10:46.186550 140218947737408 spec.py:321] Evaluating on the training split.
I0212 02:11:44.883618 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 02:12:37.183295 140218947737408 spec.py:349] Evaluating on the test split.
I0212 02:13:03.596286 140218947737408 submission_runner.py:408] Time since start: 9595.57s, 	Step: 10207, 	{'train/ctc_loss': Array(0.39790037, dtype=float32), 'train/wer': 0.134293730165625, 'validation/ctc_loss': Array(0.73437, dtype=float32), 'validation/wer': 0.21133070083126562, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46262303, dtype=float32), 'test/wer': 0.147421444965775, 'test/num_examples': 2472, 'score': 8656.777244329453, 'total_duration': 9595.573912382126, 'accumulated_submission_time': 8656.777244329453, 'accumulated_eval_time': 938.0026862621307, 'accumulated_logging_time': 0.31001925468444824}
I0212 02:13:03.632041 140063075153664 logging_writer.py:48] [10207] accumulated_eval_time=938.002686, accumulated_logging_time=0.310019, accumulated_submission_time=8656.777244, global_step=10207, preemption_count=0, score=8656.777244, test/ctc_loss=0.4626230299472809, test/num_examples=2472, test/wer=0.147421, total_duration=9595.573912, train/ctc_loss=0.39790037274360657, train/wer=0.134294, validation/ctc_loss=0.7343699932098389, validation/num_examples=5348, validation/wer=0.211331
I0212 02:14:18.450563 140063075153664 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.9463331699371338, loss=1.6559460163116455
I0212 02:15:38.292692 140063066760960 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.301753044128418, loss=1.6992629766464233
I0212 02:16:56.787162 140063075153664 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.6499438285827637, loss=1.6415735483169556
I0212 02:18:14.326075 140063066760960 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.303678035736084, loss=1.6774097681045532
I0212 02:19:40.473988 140063075153664 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.9302451610565186, loss=1.6620863676071167
I0212 02:21:07.806848 140063066760960 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.809347152709961, loss=1.664549469947815
I0212 02:22:37.668987 140063075153664 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.1416139602661133, loss=1.6915653944015503
I0212 02:24:06.491533 140063066760960 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.458788871765137, loss=1.6553326845169067
I0212 02:25:39.273916 140063075153664 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.064269542694092, loss=1.6627882719039917
I0212 02:27:07.603723 140063066760960 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.5963194370269775, loss=1.6578761339187622
I0212 02:28:36.700728 140063075153664 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.702910900115967, loss=1.646340012550354
I0212 02:30:02.512543 140063075153664 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.426280975341797, loss=1.6078612804412842
I0212 02:31:19.444730 140063066760960 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.806781053543091, loss=1.649484634399414
I0212 02:32:39.446892 140063075153664 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.911013126373291, loss=1.6146069765090942
I0212 02:34:02.318223 140063066760960 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.5035765171051025, loss=1.5949066877365112
I0212 02:35:29.596291 140063075153664 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.980347156524658, loss=1.6480671167373657
I0212 02:36:59.912184 140063066760960 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.493062496185303, loss=1.5764286518096924
I0212 02:37:03.818436 140218947737408 spec.py:321] Evaluating on the training split.
I0212 02:37:58.863721 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 02:38:50.958366 140218947737408 spec.py:349] Evaluating on the test split.
I0212 02:39:18.278231 140218947737408 submission_runner.py:408] Time since start: 11170.26s, 	Step: 11906, 	{'train/ctc_loss': Array(0.40488553, dtype=float32), 'train/wer': 0.13651806968188704, 'validation/ctc_loss': Array(0.7124341, dtype=float32), 'validation/wer': 0.20631993589310368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43567663, dtype=float32), 'test/wer': 0.13974366786504985, 'test/num_examples': 2472, 'score': 10096.870054721832, 'total_duration': 11170.25594997406, 'accumulated_submission_time': 10096.870054721832, 'accumulated_eval_time': 1072.4567618370056, 'accumulated_logging_time': 0.3640625476837158}
I0212 02:39:18.315128 140063075153664 logging_writer.py:48] [11906] accumulated_eval_time=1072.456762, accumulated_logging_time=0.364063, accumulated_submission_time=10096.870055, global_step=11906, preemption_count=0, score=10096.870055, test/ctc_loss=0.435676634311676, test/num_examples=2472, test/wer=0.139744, total_duration=11170.255950, train/ctc_loss=0.40488553047180176, train/wer=0.136518, validation/ctc_loss=0.7124341130256653, validation/num_examples=5348, validation/wer=0.206320
I0212 02:40:29.978691 140063066760960 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.7392799854278564, loss=1.6832290887832642
I0212 02:41:44.829146 140063075153664 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.4948348999023438, loss=1.6509660482406616
I0212 02:43:14.365640 140063066760960 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.292395830154419, loss=1.6383732557296753
I0212 02:44:42.344419 140063075153664 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.5064098834991455, loss=1.592124581336975
I0212 02:46:10.801412 140062419793664 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.519066095352173, loss=1.595928430557251
I0212 02:47:27.745213 140062411400960 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.8605520725250244, loss=1.6386479139328003
I0212 02:48:49.639989 140062419793664 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.518632888793945, loss=1.6366654634475708
I0212 02:50:12.438431 140062411400960 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.8296632766723633, loss=1.6594361066818237
I0212 02:51:42.653037 140062419793664 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.5375545024871826, loss=1.5938379764556885
I0212 02:53:11.392139 140062411400960 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.1227314472198486, loss=1.6103414297103882
I0212 02:54:40.935689 140062419793664 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.0101420879364014, loss=1.6624704599380493
I0212 02:56:12.071223 140062411400960 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.1009066104888916, loss=1.6315350532531738
I0212 02:57:43.120634 140062419793664 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.4295220375061035, loss=1.608389139175415
I0212 02:59:14.477944 140062411400960 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.0392537117004395, loss=1.6318868398666382
I0212 03:00:47.298521 140063075153664 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.7658350467681885, loss=1.511597990989685
I0212 03:02:03.562658 140063066760960 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.4522202014923096, loss=1.570232629776001
I0212 03:03:18.877506 140218947737408 spec.py:321] Evaluating on the training split.
I0212 03:04:13.702434 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 03:05:06.159602 140218947737408 spec.py:349] Evaluating on the test split.
I0212 03:05:32.726429 140218947737408 submission_runner.py:408] Time since start: 12744.70s, 	Step: 13599, 	{'train/ctc_loss': Array(0.37492138, dtype=float32), 'train/wer': 0.12580418475685157, 'validation/ctc_loss': Array(0.6971934, dtype=float32), 'validation/wer': 0.20115469650598106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41858706, dtype=float32), 'test/wer': 0.13537667824426705, 'test/num_examples': 2472, 'score': 11537.339000701904, 'total_duration': 12744.704414606094, 'accumulated_submission_time': 11537.339000701904, 'accumulated_eval_time': 1206.3002724647522, 'accumulated_logging_time': 0.4186415672302246}
I0212 03:05:32.759217 140063075153664 logging_writer.py:48] [13599] accumulated_eval_time=1206.300272, accumulated_logging_time=0.418642, accumulated_submission_time=11537.339001, global_step=13599, preemption_count=0, score=11537.339001, test/ctc_loss=0.4185870587825775, test/num_examples=2472, test/wer=0.135377, total_duration=12744.704415, train/ctc_loss=0.37492138147354126, train/wer=0.125804, validation/ctc_loss=0.6971933841705322, validation/num_examples=5348, validation/wer=0.201155
I0212 03:05:34.397446 140063066760960 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.664741039276123, loss=1.548018455505371
I0212 03:06:49.625853 140063075153664 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.152838945388794, loss=1.6088296175003052
I0212 03:08:05.469242 140063066760960 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.3888843059539795, loss=1.6151680946350098
I0212 03:09:22.435105 140063075153664 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.138249635696411, loss=1.5957989692687988
I0212 03:10:49.081975 140063066760960 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.699354410171509, loss=1.5836291313171387
I0212 03:12:18.874108 140063075153664 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.2437093257904053, loss=1.5986034870147705
I0212 03:13:48.146605 140063066760960 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.387699842453003, loss=1.6223151683807373
I0212 03:15:18.198299 140063075153664 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.8237099647521973, loss=1.577889323234558
I0212 03:16:46.339954 140063066760960 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.6508288383483887, loss=1.5864832401275635
I0212 03:18:10.800657 140062419793664 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.346616506576538, loss=1.5666265487670898
I0212 03:19:29.745301 140062411400960 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.4689159393310547, loss=1.5665448904037476
I0212 03:20:49.687383 140062419793664 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.2468442916870117, loss=1.5226243734359741
I0212 03:22:12.554902 140062411400960 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.916714668273926, loss=1.5260860919952393
I0212 03:23:40.741107 140062419793664 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.6470413208007812, loss=1.539845585823059
I0212 03:25:10.752499 140062411400960 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.2786102294921875, loss=1.5212469100952148
I0212 03:26:39.081096 140062419793664 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.4634437561035156, loss=1.6219180822372437
I0212 03:28:08.457128 140062411400960 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.121575355529785, loss=1.5527905225753784
I0212 03:29:33.397110 140218947737408 spec.py:321] Evaluating on the training split.
I0212 03:30:32.483147 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 03:31:25.190937 140218947737408 spec.py:349] Evaluating on the test split.
I0212 03:31:52.185231 140218947737408 submission_runner.py:408] Time since start: 14324.16s, 	Step: 15296, 	{'train/ctc_loss': Array(0.36053848, dtype=float32), 'train/wer': 0.12280701754385964, 'validation/ctc_loss': Array(0.6717796, dtype=float32), 'validation/wer': 0.19308340654778572, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40154976, dtype=float32), 'test/wer': 0.13178152864948306, 'test/num_examples': 2472, 'score': 12977.885040283203, 'total_duration': 14324.161016225815, 'accumulated_submission_time': 12977.885040283203, 'accumulated_eval_time': 1345.0807433128357, 'accumulated_logging_time': 0.46816587448120117}
I0212 03:31:52.225399 140063075153664 logging_writer.py:48] [15296] accumulated_eval_time=1345.080743, accumulated_logging_time=0.468166, accumulated_submission_time=12977.885040, global_step=15296, preemption_count=0, score=12977.885040, test/ctc_loss=0.401549756526947, test/num_examples=2472, test/wer=0.131782, total_duration=14324.161016, train/ctc_loss=0.3605384826660156, train/wer=0.122807, validation/ctc_loss=0.6717795729637146, validation/num_examples=5348, validation/wer=0.193083
I0212 03:31:56.187998 140063066760960 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.971043586730957, loss=1.6879253387451172
I0212 03:33:11.454050 140063075153664 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.4895377159118652, loss=1.5852952003479004
I0212 03:34:31.883875 140063075153664 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.807955741882324, loss=1.5001846551895142
I0212 03:35:50.455229 140063066760960 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.3839051723480225, loss=1.4941613674163818
I0212 03:37:08.609597 140063075153664 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.126051902770996, loss=1.5599727630615234
I0212 03:38:30.732095 140063066760960 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.5181727409362793, loss=1.5604299306869507
I0212 03:39:57.267875 140063075153664 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.8993000984191895, loss=1.6185903549194336
I0212 03:41:24.350276 140063066760960 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.8049018383026123, loss=1.5660443305969238
I0212 03:42:53.682327 140063075153664 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.3118648529052734, loss=1.570267677307129
I0212 03:44:24.932757 140063066760960 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.806654691696167, loss=1.5681134462356567
I0212 03:45:54.515441 140063075153664 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.709618330001831, loss=1.5824553966522217
I0212 03:47:22.778485 140063066760960 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.566899538040161, loss=1.5611279010772705
I0212 03:48:51.352007 140063075153664 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.934128522872925, loss=1.5717324018478394
I0212 03:50:09.812484 140063066760960 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.007593870162964, loss=1.5483744144439697
I0212 03:51:26.391838 140063075153664 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.6589691638946533, loss=1.5146312713623047
I0212 03:52:45.705425 140063066760960 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.38873291015625, loss=1.5593430995941162
I0212 03:54:12.280007 140063075153664 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.9144622087478638, loss=1.5000970363616943
I0212 03:55:43.180123 140063066760960 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.7854554653167725, loss=1.4885245561599731
I0212 03:55:52.911706 140218947737408 spec.py:321] Evaluating on the training split.
I0212 03:56:48.353830 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 03:57:42.331504 140218947737408 spec.py:349] Evaluating on the test split.
I0212 03:58:09.716755 140218947737408 submission_runner.py:408] Time since start: 15901.69s, 	Step: 17012, 	{'train/ctc_loss': Array(0.38336515, dtype=float32), 'train/wer': 0.12243530425348607, 'validation/ctc_loss': Array(0.6500841, dtype=float32), 'validation/wer': 0.1883815905075451, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38370186, dtype=float32), 'test/wer': 0.12280381045233887, 'test/num_examples': 2472, 'score': 14418.47855091095, 'total_duration': 15901.694470643997, 'accumulated_submission_time': 14418.47855091095, 'accumulated_eval_time': 1481.8800811767578, 'accumulated_logging_time': 0.5250308513641357}
I0212 03:58:09.753715 140062491473664 logging_writer.py:48] [17012] accumulated_eval_time=1481.880081, accumulated_logging_time=0.525031, accumulated_submission_time=14418.478551, global_step=17012, preemption_count=0, score=14418.478551, test/ctc_loss=0.3837018609046936, test/num_examples=2472, test/wer=0.122804, total_duration=15901.694471, train/ctc_loss=0.3833651542663574, train/wer=0.122435, validation/ctc_loss=0.6500840783119202, validation/num_examples=5348, validation/wer=0.188382
I0212 03:59:17.509929 140062483080960 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.895742177963257, loss=1.5487266778945923
I0212 04:00:32.491681 140062491473664 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.182767868041992, loss=1.5617752075195312
I0212 04:01:55.901561 140062483080960 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.265145778656006, loss=1.5456503629684448
I0212 04:03:22.730897 140062491473664 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.867671012878418, loss=1.5834591388702393
I0212 04:04:54.050104 140062483080960 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.6645097732543945, loss=1.6200358867645264
I0212 04:06:17.411434 140062491473664 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.6686694622039795, loss=1.5961329936981201
I0212 04:07:39.453587 140062483080960 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.841031789779663, loss=1.5567306280136108
I0212 04:09:01.271182 140062491473664 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.6147260665893555, loss=1.4682559967041016
I0212 04:10:24.900677 140062483080960 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.035065650939941, loss=1.5577794313430786
I0212 04:11:51.538107 140062491473664 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.9739768505096436, loss=1.6136102676391602
I0212 04:13:22.594637 140062483080960 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.7989296913146973, loss=1.5657923221588135
I0212 04:14:53.305323 140062491473664 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.6061198711395264, loss=1.5517023801803589
I0212 04:16:26.738122 140062483080960 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.9988503456115723, loss=1.451305866241455
I0212 04:17:56.952051 140062491473664 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.400193452835083, loss=1.541765570640564
I0212 04:19:27.225170 140062483080960 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.9620349407196045, loss=1.5033197402954102
I0212 04:20:54.193550 140062491473664 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.175607442855835, loss=1.5619772672653198
I0212 04:22:09.893440 140218947737408 spec.py:321] Evaluating on the training split.
I0212 04:23:07.059807 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 04:24:01.718682 140218947737408 spec.py:349] Evaluating on the test split.
I0212 04:24:29.901568 140218947737408 submission_runner.py:408] Time since start: 17481.88s, 	Step: 18700, 	{'train/ctc_loss': Array(0.2896522, dtype=float32), 'train/wer': 0.09824230366921673, 'validation/ctc_loss': Array(0.62595093, dtype=float32), 'validation/wer': 0.17998204234530832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37486923, dtype=float32), 'test/wer': 0.11914772611866024, 'test/num_examples': 2472, 'score': 15858.525933265686, 'total_duration': 17481.879634141922, 'accumulated_submission_time': 15858.525933265686, 'accumulated_eval_time': 1621.8828246593475, 'accumulated_logging_time': 0.5793395042419434}
I0212 04:24:29.934375 140062491473664 logging_writer.py:48] [18700] accumulated_eval_time=1621.882825, accumulated_logging_time=0.579340, accumulated_submission_time=15858.525933, global_step=18700, preemption_count=0, score=15858.525933, test/ctc_loss=0.3748692274093628, test/num_examples=2472, test/wer=0.119148, total_duration=17481.879634, train/ctc_loss=0.2896521985530853, train/wer=0.098242, validation/ctc_loss=0.6259509325027466, validation/num_examples=5348, validation/wer=0.179982
I0212 04:24:30.808193 140062483080960 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.567713499069214, loss=1.4924085140228271
I0212 04:25:45.700937 140062491473664 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.2278683185577393, loss=1.4753142595291138
I0212 04:27:00.600872 140062483080960 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.578707218170166, loss=1.5458461046218872
I0212 04:28:20.616853 140062491473664 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.9774750471115112, loss=1.485315203666687
I0212 04:29:47.411742 140062483080960 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.4148664474487305, loss=1.491248607635498
I0212 04:31:15.475326 140062491473664 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.170414686203003, loss=1.5587387084960938
I0212 04:32:45.761131 140062483080960 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.697057008743286, loss=1.484382152557373
I0212 04:34:13.517498 140062491473664 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.703317165374756, loss=1.4991494417190552
I0212 04:35:44.755221 140062483080960 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.016770601272583, loss=1.532636284828186
I0212 04:37:11.271866 140062491473664 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.1388700008392334, loss=1.495270848274231
I0212 04:38:26.654340 140062483080960 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.2046494483947754, loss=1.5146139860153198
I0212 04:39:45.881996 140062491473664 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.2746171951293945, loss=1.5327259302139282
I0212 04:41:05.760456 140062483080960 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.3961875438690186, loss=1.458296775817871
I0212 04:42:32.720807 140062491473664 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.5319221019744873, loss=1.4986916780471802
I0212 04:44:01.911678 140062483080960 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.0864315032958984, loss=1.5398492813110352
I0212 04:45:32.288324 140062491473664 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.9011504650115967, loss=1.4966436624526978
I0212 04:46:59.156200 140062483080960 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6463614702224731, loss=1.4764716625213623
I0212 04:48:30.382481 140062491473664 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.70953631401062, loss=1.4814436435699463
I0212 04:48:30.388596 140218947737408 spec.py:321] Evaluating on the training split.
I0212 04:49:26.358686 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 04:50:18.805512 140218947737408 spec.py:349] Evaluating on the test split.
I0212 04:50:45.186076 140218947737408 submission_runner.py:408] Time since start: 19057.16s, 	Step: 20401, 	{'train/ctc_loss': Array(0.3050606, dtype=float32), 'train/wer': 0.10250043128911009, 'validation/ctc_loss': Array(0.6120852, dtype=float32), 'validation/wer': 0.1774428685905172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36329266, dtype=float32), 'test/wer': 0.11859931346860846, 'test/num_examples': 2472, 'score': 17298.88808941841, 'total_duration': 19057.164207220078, 'accumulated_submission_time': 17298.88808941841, 'accumulated_eval_time': 1756.674932718277, 'accumulated_logging_time': 0.6285579204559326}
I0212 04:50:45.220154 140062491473664 logging_writer.py:48] [20401] accumulated_eval_time=1756.674933, accumulated_logging_time=0.628558, accumulated_submission_time=17298.888089, global_step=20401, preemption_count=0, score=17298.888089, test/ctc_loss=0.3632926642894745, test/num_examples=2472, test/wer=0.118599, total_duration=19057.164207, train/ctc_loss=0.30506059527397156, train/wer=0.102500, validation/ctc_loss=0.612085223197937, validation/num_examples=5348, validation/wer=0.177443
I0212 04:52:00.104339 140062483080960 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.101335048675537, loss=1.5208206176757812
I0212 04:53:19.239230 140062491473664 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.630384922027588, loss=1.4750924110412598
I0212 04:54:35.658036 140062483080960 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.0326032638549805, loss=1.480926513671875
I0212 04:55:55.239051 140062491473664 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.734705686569214, loss=1.4383633136749268
I0212 04:57:17.498012 140062483080960 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.7722604274749756, loss=1.4713009595870972
I0212 04:58:36.786830 140062491473664 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.448904275894165, loss=1.4934029579162598
I0212 05:00:05.517566 140062483080960 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.4379236698150635, loss=1.480064868927002
I0212 05:01:35.301658 140062491473664 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.366682529449463, loss=1.4729305505752563
I0212 05:03:06.355192 140062483080960 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.4455556869506836, loss=1.4258720874786377
I0212 05:04:36.076065 140062491473664 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.6802978515625, loss=1.4520713090896606
I0212 05:06:06.150784 140062483080960 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.58428955078125, loss=1.4067769050598145
I0212 05:07:32.692224 140062491473664 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.0005948543548584, loss=1.5119012594223022
I0212 05:08:54.570491 140062491473664 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.379702568054199, loss=1.425519585609436
I0212 05:10:11.059010 140062483080960 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.945632219314575, loss=1.4986323118209839
I0212 05:11:30.100330 140062491473664 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.870327949523926, loss=1.485714316368103
I0212 05:12:53.737195 140062483080960 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.2334773540496826, loss=1.5287373065948486
I0212 05:14:21.870327 140062491473664 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.9898862838745117, loss=1.4531872272491455
I0212 05:14:46.035905 140218947737408 spec.py:321] Evaluating on the training split.
I0212 05:15:40.692374 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 05:16:33.357740 140218947737408 spec.py:349] Evaluating on the test split.
I0212 05:17:01.010596 140218947737408 submission_runner.py:408] Time since start: 20632.99s, 	Step: 22128, 	{'train/ctc_loss': Array(0.4048885, dtype=float32), 'train/wer': 0.13275771099435005, 'validation/ctc_loss': Array(0.60599583, dtype=float32), 'validation/wer': 0.17508713324386688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34999764, dtype=float32), 'test/wer': 0.11211991956614466, 'test/num_examples': 2472, 'score': 18739.611248254776, 'total_duration': 20632.988973617554, 'accumulated_submission_time': 18739.611248254776, 'accumulated_eval_time': 1891.6445426940918, 'accumulated_logging_time': 0.6785871982574463}
I0212 05:17:01.051728 140062645073664 logging_writer.py:48] [22128] accumulated_eval_time=1891.644543, accumulated_logging_time=0.678587, accumulated_submission_time=18739.611248, global_step=22128, preemption_count=0, score=18739.611248, test/ctc_loss=0.3499976396560669, test/num_examples=2472, test/wer=0.112120, total_duration=20632.988974, train/ctc_loss=0.4048885107040405, train/wer=0.132758, validation/ctc_loss=0.6059958338737488, validation/num_examples=5348, validation/wer=0.175087
I0212 05:17:56.190712 140062636680960 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.2560384273529053, loss=1.5522524118423462
I0212 05:19:11.306067 140062645073664 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.4141175746917725, loss=1.449039340019226
I0212 05:20:36.155162 140062636680960 logging_writer.py:48] [22400] global_step=22400, grad_norm=5.11114501953125, loss=1.4428092241287231
I0212 05:22:03.001344 140062645073664 logging_writer.py:48] [22500] global_step=22500, grad_norm=5.005745887756348, loss=1.4978529214859009
I0212 05:23:31.200562 140062636680960 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.5679361820220947, loss=1.436361312866211
I0212 05:24:58.755507 140062645073664 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.9366657733917236, loss=1.4421954154968262
I0212 05:26:15.614743 140062636680960 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.111881732940674, loss=1.4329293966293335
I0212 05:27:35.089465 140062645073664 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.341198444366455, loss=1.44755220413208
I0212 05:28:56.116702 140062636680960 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.778595447540283, loss=1.4259154796600342
I0212 05:30:22.264652 140062645073664 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.5756402015686035, loss=1.5236455202102661
I0212 05:31:51.578060 140062636680960 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.8709404468536377, loss=1.418512225151062
I0212 05:33:23.336184 140062645073664 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.2823193073272705, loss=1.4562127590179443
I0212 05:34:54.200695 140062636680960 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.1969547271728516, loss=1.4483847618103027
I0212 05:36:21.057533 140062645073664 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.882643699645996, loss=1.4659448862075806
I0212 05:37:50.968492 140062636680960 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.9376132488250732, loss=1.452199101448059
I0212 05:39:23.504857 140062645073664 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.541381597518921, loss=1.4087607860565186
I0212 05:40:39.966210 140062636680960 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.02421236038208, loss=1.392562747001648
I0212 05:41:01.606080 140218947737408 spec.py:321] Evaluating on the training split.
I0212 05:41:55.806598 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 05:42:48.199428 140218947737408 spec.py:349] Evaluating on the test split.
I0212 05:43:15.315296 140218947737408 submission_runner.py:408] Time since start: 22207.29s, 	Step: 23829, 	{'train/ctc_loss': Array(0.4211561, dtype=float32), 'train/wer': 0.13787367125247452, 'validation/ctc_loss': Array(0.5772926, dtype=float32), 'validation/wer': 0.1673344468366529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33509344, dtype=float32), 'test/wer': 0.10805760363983508, 'test/num_examples': 2472, 'score': 20180.074142932892, 'total_duration': 22207.29289650917, 'accumulated_submission_time': 20180.074142932892, 'accumulated_eval_time': 2025.3478963375092, 'accumulated_logging_time': 0.7357115745544434}
I0212 05:43:15.350997 140063075153664 logging_writer.py:48] [23829] accumulated_eval_time=2025.347896, accumulated_logging_time=0.735712, accumulated_submission_time=20180.074143, global_step=23829, preemption_count=0, score=20180.074143, test/ctc_loss=0.3350934386253357, test/num_examples=2472, test/wer=0.108058, total_duration=22207.292897, train/ctc_loss=0.421156108379364, train/wer=0.137874, validation/ctc_loss=0.5772926211357117, validation/num_examples=5348, validation/wer=0.167334
I0212 05:44:09.494615 140063066760960 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.7547929286956787, loss=1.387043833732605
I0212 05:45:24.563168 140063075153664 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.2698745727539062, loss=1.451268196105957
I0212 05:46:39.645379 140063066760960 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.48717999458313, loss=1.4470279216766357
I0212 05:48:05.256031 140063075153664 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.571490526199341, loss=1.4583923816680908
I0212 05:49:31.960754 140063066760960 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.9039130210876465, loss=1.4906240701675415
I0212 05:51:00.207382 140063075153664 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.66135835647583, loss=1.4892328977584839
I0212 05:52:30.887120 140063066760960 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.799943447113037, loss=1.4461551904678345
I0212 05:54:00.408218 140063075153664 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.291133403778076, loss=1.458370327949524
I0212 05:55:31.780576 140063066760960 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.9415225982666016, loss=1.4128215312957764
I0212 05:56:54.452692 140063075153664 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.546121835708618, loss=1.396847128868103
I0212 05:58:13.032384 140063066760960 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.5859835147857666, loss=1.4063206911087036
I0212 05:59:32.511916 140063075153664 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.604884386062622, loss=1.4489835500717163
I0212 06:00:55.912513 140063066760960 logging_writer.py:48] [25100] global_step=25100, grad_norm=5.7260565757751465, loss=1.43626070022583
I0212 06:02:25.658731 140063075153664 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.828895330429077, loss=1.4345625638961792
I0212 06:03:53.002142 140063066760960 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.7222628593444824, loss=1.4330252408981323
I0212 06:05:24.011283 140063075153664 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.2287256717681885, loss=1.3572973012924194
I0212 06:06:53.774607 140063066760960 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.599321126937866, loss=1.489294409751892
I0212 06:07:16.125470 140218947737408 spec.py:321] Evaluating on the training split.
I0212 06:08:09.199456 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 06:09:02.126544 140218947737408 spec.py:349] Evaluating on the test split.
I0212 06:09:28.801395 140218947737408 submission_runner.py:408] Time since start: 23780.78s, 	Step: 25527, 	{'train/ctc_loss': Array(0.4618884, dtype=float32), 'train/wer': 0.15217769630592165, 'validation/ctc_loss': Array(0.565899, dtype=float32), 'validation/wer': 0.1642932311227396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32599014, dtype=float32), 'test/wer': 0.1042187150894725, 'test/num_examples': 2472, 'score': 21620.75711417198, 'total_duration': 23780.77961206436, 'accumulated_submission_time': 21620.75711417198, 'accumulated_eval_time': 2158.018583536148, 'accumulated_logging_time': 0.7869384288787842}
I0212 06:09:28.839237 140063075153664 logging_writer.py:48] [25527] accumulated_eval_time=2158.018584, accumulated_logging_time=0.786938, accumulated_submission_time=21620.757114, global_step=25527, preemption_count=0, score=21620.757114, test/ctc_loss=0.32599014043807983, test/num_examples=2472, test/wer=0.104219, total_duration=23780.779612, train/ctc_loss=0.4618884027004242, train/wer=0.152178, validation/ctc_loss=0.5658990144729614, validation/num_examples=5348, validation/wer=0.164293
I0212 06:10:24.210830 140063066760960 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.6100873947143555, loss=1.4375243186950684
I0212 06:11:39.285057 140063075153664 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.2312822341918945, loss=1.4398279190063477
I0212 06:12:59.517223 140063075153664 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.5644047260284424, loss=1.431404948234558
I0212 06:14:16.886238 140063066760960 logging_writer.py:48] [25900] global_step=25900, grad_norm=4.015627861022949, loss=1.395346760749817
I0212 06:15:36.276734 140063075153664 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.719062328338623, loss=1.4139872789382935
I0212 06:16:56.279036 140063066760960 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.2399137020111084, loss=1.4133026599884033
I0212 06:18:22.886323 140063075153664 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.3510875701904297, loss=1.3612229824066162
I0212 06:19:53.066377 140063066760960 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.518634796142578, loss=1.3923648595809937
I0212 06:21:20.803445 140063075153664 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.0536394119262695, loss=1.3900401592254639
I0212 06:22:51.186651 140063066760960 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.610570192337036, loss=1.4068994522094727
I0212 06:24:21.819481 140063075153664 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.93312406539917, loss=1.4202401638031006
I0212 06:25:52.136467 140063066760960 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.9593584537506104, loss=1.3602534532546997
I0212 06:27:20.959643 140063075153664 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.8752803802490234, loss=1.3616750240325928
I0212 06:28:36.880942 140063066760960 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.0810623168945312, loss=1.4378517866134644
I0212 06:29:56.748505 140063075153664 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.5668349266052246, loss=1.411048173904419
I0212 06:31:20.200105 140063066760960 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.4963290691375732, loss=1.4220200777053833
I0212 06:32:45.956083 140063075153664 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.8402843475341797, loss=1.404245138168335
I0212 06:33:29.231599 140218947737408 spec.py:321] Evaluating on the training split.
I0212 06:34:21.983397 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 06:35:13.606376 140218947737408 spec.py:349] Evaluating on the test split.
I0212 06:35:40.083568 140218947737408 submission_runner.py:408] Time since start: 25352.06s, 	Step: 27250, 	{'train/ctc_loss': Array(0.39754912, dtype=float32), 'train/wer': 0.12864185937316586, 'validation/ctc_loss': Array(0.54623437, dtype=float32), 'validation/wer': 0.15894455332747617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31674242, dtype=float32), 'test/wer': 0.10249223082079094, 'test/num_examples': 2472, 'score': 23061.056136369705, 'total_duration': 25352.061848640442, 'accumulated_submission_time': 23061.056136369705, 'accumulated_eval_time': 2288.8654062747955, 'accumulated_logging_time': 0.8404562473297119}
I0212 06:35:40.121428 140063075153664 logging_writer.py:48] [27250] accumulated_eval_time=2288.865406, accumulated_logging_time=0.840456, accumulated_submission_time=23061.056136, global_step=27250, preemption_count=0, score=23061.056136, test/ctc_loss=0.3167424201965332, test/num_examples=2472, test/wer=0.102492, total_duration=25352.061849, train/ctc_loss=0.3975491225719452, train/wer=0.128642, validation/ctc_loss=0.5462343692779541, validation/num_examples=5348, validation/wer=0.158945
I0212 06:36:18.481437 140063066760960 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.9378340244293213, loss=1.4572354555130005
I0212 06:37:33.126841 140063075153664 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.2620272636413574, loss=1.4196984767913818
I0212 06:38:56.134875 140063066760960 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.1237306594848633, loss=1.3708243370056152
I0212 06:40:25.684570 140063075153664 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.332301139831543, loss=1.3685916662216187
I0212 06:41:52.676667 140063066760960 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.5461976528167725, loss=1.3769261837005615
I0212 06:43:23.222868 140063075153664 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.2582662105560303, loss=1.342855453491211
I0212 06:44:44.459118 140063075153664 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.6977059841156006, loss=1.3964276313781738
I0212 06:46:02.579403 140063066760960 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.1209230422973633, loss=1.39363431930542
I0212 06:47:21.574705 140063075153664 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.545400619506836, loss=1.3598071336746216
I0212 06:48:46.843182 140063066760960 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.728048801422119, loss=1.387122631072998
I0212 06:50:14.193434 140063075153664 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.9739282131195068, loss=1.3071308135986328
I0212 06:51:46.650132 140063066760960 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.739225149154663, loss=1.3628233671188354
I0212 06:53:15.008629 140063075153664 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.7392091751098633, loss=1.3897358179092407
I0212 06:54:43.413376 140063066760960 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.9171979427337646, loss=1.3890234231948853
I0212 06:56:14.623750 140063075153664 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.2497379779815674, loss=1.3670448064804077
I0212 06:57:45.259272 140063066760960 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.868760347366333, loss=1.3417640924453735
I0212 06:59:10.256255 140063075153664 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.358896017074585, loss=1.3637598752975464
I0212 06:59:40.500055 140218947737408 spec.py:321] Evaluating on the training split.
I0212 07:00:35.146603 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 07:01:28.830238 140218947737408 spec.py:349] Evaluating on the test split.
I0212 07:01:55.387644 140218947737408 submission_runner.py:408] Time since start: 26927.36s, 	Step: 28939, 	{'train/ctc_loss': Array(0.35738322, dtype=float32), 'train/wer': 0.118875696087301, 'validation/ctc_loss': Array(0.5297782, dtype=float32), 'validation/wer': 0.15318072545063094, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30347437, dtype=float32), 'test/wer': 0.09702841589990453, 'test/num_examples': 2472, 'score': 24501.343689918518, 'total_duration': 26927.3649828434, 'accumulated_submission_time': 24501.343689918518, 'accumulated_eval_time': 2423.746869325638, 'accumulated_logging_time': 0.89430832862854}
I0212 07:01:55.421429 140062491473664 logging_writer.py:48] [28939] accumulated_eval_time=2423.746869, accumulated_logging_time=0.894308, accumulated_submission_time=24501.343690, global_step=28939, preemption_count=0, score=24501.343690, test/ctc_loss=0.3034743666648865, test/num_examples=2472, test/wer=0.097028, total_duration=26927.364983, train/ctc_loss=0.35738322138786316, train/wer=0.118876, validation/ctc_loss=0.5297781825065613, validation/num_examples=5348, validation/wer=0.153181
I0212 07:02:42.494108 140062483080960 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.8676637411117554, loss=1.3661118745803833
I0212 07:03:57.595363 140062491473664 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.983414888381958, loss=1.3793116807937622
I0212 07:05:12.548385 140062483080960 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.6533000469207764, loss=1.2854764461517334
I0212 07:06:34.212356 140062491473664 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.584996223449707, loss=1.3408700227737427
I0212 07:08:03.623640 140062483080960 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.8566075563430786, loss=1.372245192527771
I0212 07:09:33.913661 140062491473664 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.851430654525757, loss=1.3697015047073364
I0212 07:11:03.931725 140062483080960 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.7276182174682617, loss=1.3644092082977295
I0212 07:12:33.289014 140062491473664 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.450035572052002, loss=1.3359472751617432
I0212 07:14:04.500563 140062483080960 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.9788196086883545, loss=1.3218668699264526
I0212 07:15:35.419032 140062491473664 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.772813558578491, loss=1.3321490287780762
I0212 07:16:51.117384 140062483080960 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.1608192920684814, loss=1.27272367477417
I0212 07:18:08.498269 140062491473664 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.7489795684814453, loss=1.3001031875610352
I0212 07:19:26.542162 140062483080960 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.103996992111206, loss=1.3886590003967285
I0212 07:20:51.269153 140062491473664 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.156407356262207, loss=1.309969425201416
I0212 07:22:19.436356 140062483080960 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.9006083011627197, loss=1.3497726917266846
I0212 07:23:49.626739 140062491473664 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.8699662685394287, loss=1.296032428741455
I0212 07:25:20.109882 140062483080960 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.439988136291504, loss=1.2950620651245117
I0212 07:25:55.732286 140218947737408 spec.py:321] Evaluating on the training split.
I0212 07:26:50.780568 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 07:27:43.902265 140218947737408 spec.py:349] Evaluating on the test split.
I0212 07:28:11.728960 140218947737408 submission_runner.py:408] Time since start: 28503.71s, 	Step: 30640, 	{'train/ctc_loss': Array(0.3072533, dtype=float32), 'train/wer': 0.1059137139551105, 'validation/ctc_loss': Array(0.51826257, dtype=float32), 'validation/wer': 0.15005261785917723, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29775858, dtype=float32), 'test/wer': 0.09668311904616822, 'test/num_examples': 2472, 'score': 25941.561491966248, 'total_duration': 28503.707077503204, 'accumulated_submission_time': 25941.561491966248, 'accumulated_eval_time': 2559.73819565773, 'accumulated_logging_time': 0.9447612762451172}
I0212 07:28:11.765973 140062645073664 logging_writer.py:48] [30640] accumulated_eval_time=2559.738196, accumulated_logging_time=0.944761, accumulated_submission_time=25941.561492, global_step=30640, preemption_count=0, score=25941.561492, test/ctc_loss=0.2977585792541504, test/num_examples=2472, test/wer=0.096683, total_duration=28503.707078, train/ctc_loss=0.30725330114364624, train/wer=0.105914, validation/ctc_loss=0.5182625651359558, validation/num_examples=5348, validation/wer=0.150053
I0212 07:28:57.704643 140062636680960 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.314336061477661, loss=1.3608391284942627
I0212 07:30:12.821973 140062645073664 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.842539072036743, loss=1.3447364568710327
I0212 07:31:38.610224 140062645073664 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.2478930950164795, loss=1.3124605417251587
I0212 07:32:56.596453 140062636680960 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.26381516456604, loss=1.3225131034851074
I0212 07:34:16.272587 140062645073664 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.3769521713256836, loss=1.2922933101654053
I0212 07:35:36.850039 140062636680960 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.0915262699127197, loss=1.3170756101608276
I0212 07:37:01.812381 140062645073664 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.9388383626937866, loss=1.2987667322158813
I0212 07:38:33.455750 140062636680960 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.5820109844207764, loss=1.3147187232971191
I0212 07:40:02.793402 140062645073664 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.422287702560425, loss=1.3066285848617554
I0212 07:41:32.703055 140062636680960 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.3449764251708984, loss=1.3406320810317993
I0212 07:43:03.078914 140062645073664 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.5610899925231934, loss=1.3675774335861206
I0212 07:44:35.956434 140062636680960 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.3698008060455322, loss=1.3229057788848877
I0212 07:46:06.482126 140062645073664 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.0970804691314697, loss=1.323115587234497
I0212 07:47:32.286584 140062645073664 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.4856643676757812, loss=1.280806541442871
I0212 07:48:51.703738 140062636680960 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.0509192943573, loss=1.3712579011917114
I0212 07:50:10.902783 140062645073664 logging_writer.py:48] [32200] global_step=32200, grad_norm=5.8060526847839355, loss=1.3084564208984375
I0212 07:51:33.647653 140062636680960 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.528190851211548, loss=1.307093858718872
I0212 07:52:11.874352 140218947737408 spec.py:321] Evaluating on the training split.
I0212 07:53:06.322127 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 07:53:58.471266 140218947737408 spec.py:349] Evaluating on the test split.
I0212 07:54:26.288749 140218947737408 submission_runner.py:408] Time since start: 30078.27s, 	Step: 32345, 	{'train/ctc_loss': Array(0.34496155, dtype=float32), 'train/wer': 0.11648960537510089, 'validation/ctc_loss': Array(0.49741358, dtype=float32), 'validation/wer': 0.14559216814543768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2845581, dtype=float32), 'test/wer': 0.09113805780675563, 'test/num_examples': 2472, 'score': 27381.578418970108, 'total_duration': 30078.26686859131, 'accumulated_submission_time': 27381.578418970108, 'accumulated_eval_time': 2694.1472787857056, 'accumulated_logging_time': 0.9967937469482422}
I0212 07:54:26.326709 140063075153664 logging_writer.py:48] [32345] accumulated_eval_time=2694.147279, accumulated_logging_time=0.996794, accumulated_submission_time=27381.578419, global_step=32345, preemption_count=0, score=27381.578419, test/ctc_loss=0.28455808758735657, test/num_examples=2472, test/wer=0.091138, total_duration=30078.266869, train/ctc_loss=0.344961553812027, train/wer=0.116490, validation/ctc_loss=0.4974135756492615, validation/num_examples=5348, validation/wer=0.145592
I0212 07:55:08.383777 140063066760960 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.5207631587982178, loss=1.3471271991729736
I0212 07:56:23.237522 140063075153664 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.1532156467437744, loss=1.3225910663604736
I0212 07:57:45.469289 140063066760960 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.40179181098938, loss=1.3275707960128784
I0212 07:59:14.741748 140063075153664 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.7316274642944336, loss=1.2832480669021606
I0212 08:00:42.233096 140063066760960 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.848236083984375, loss=1.3409303426742554
I0212 08:02:14.350260 140063075153664 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.702418088912964, loss=1.2701001167297363
I0212 08:03:42.079238 140062419793664 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.302140474319458, loss=1.2981951236724854
I0212 08:04:58.783175 140062411400960 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.0874834060668945, loss=1.3281054496765137
I0212 08:06:20.298198 140062419793664 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.364908218383789, loss=1.3057482242584229
I0212 08:07:43.112272 140062411400960 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.9696073532104492, loss=1.2062551975250244
I0212 08:09:08.833413 140062419793664 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.5407135486602783, loss=1.2911350727081299
I0212 08:10:37.121075 140062411400960 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.2930426597595215, loss=1.2938709259033203
I0212 08:12:08.474287 140062419793664 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.665881633758545, loss=1.2982596158981323
I0212 08:13:40.044498 140062411400960 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.970259189605713, loss=1.30108642578125
I0212 08:15:12.229713 140062419793664 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.030579090118408, loss=1.2860487699508667
I0212 08:16:41.374829 140062411400960 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.02032732963562, loss=1.3381898403167725
I0212 08:18:13.549188 140063075153664 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.016838788986206, loss=1.2854933738708496
I0212 08:18:26.971634 140218947737408 spec.py:321] Evaluating on the training split.
I0212 08:19:22.795966 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 08:20:16.013235 140218947737408 spec.py:349] Evaluating on the test split.
I0212 08:20:43.376183 140218947737408 submission_runner.py:408] Time since start: 31655.35s, 	Step: 34019, 	{'train/ctc_loss': Array(0.2937716, dtype=float32), 'train/wer': 0.09893059565821838, 'validation/ctc_loss': Array(0.48239866, dtype=float32), 'validation/wer': 0.13939388088089055, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26872075, dtype=float32), 'test/wer': 0.08601953973960555, 'test/num_examples': 2472, 'score': 28822.132719278336, 'total_duration': 31655.35311293602, 'accumulated_submission_time': 28822.132719278336, 'accumulated_eval_time': 2830.5453424453735, 'accumulated_logging_time': 1.0511739253997803}
I0212 08:20:43.414302 140063075153664 logging_writer.py:48] [34019] accumulated_eval_time=2830.545342, accumulated_logging_time=1.051174, accumulated_submission_time=28822.132719, global_step=34019, preemption_count=0, score=28822.132719, test/ctc_loss=0.26872074604034424, test/num_examples=2472, test/wer=0.086020, total_duration=31655.353113, train/ctc_loss=0.2937715947628021, train/wer=0.098931, validation/ctc_loss=0.48239865899086, validation/num_examples=5348, validation/wer=0.139394
I0212 08:21:44.919915 140063066760960 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.2032485008239746, loss=1.2444124221801758
I0212 08:23:00.060515 140063075153664 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.6393792629241943, loss=1.2330501079559326
I0212 08:24:15.486016 140063066760960 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8955432176589966, loss=1.2679648399353027
I0212 08:25:31.661247 140063075153664 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.88311767578125, loss=1.2543283700942993
I0212 08:26:59.261520 140063066760960 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.657219886779785, loss=1.3013672828674316
I0212 08:28:29.235713 140063075153664 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.3937952518463135, loss=1.2945178747177124
I0212 08:29:58.189149 140063066760960 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.4194087982177734, loss=1.227292776107788
I0212 08:31:29.725762 140063075153664 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.7682669162750244, loss=1.2559441328048706
I0212 08:32:58.879513 140063066760960 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.318958044052124, loss=1.2795662879943848
I0212 08:34:29.720240 140063075153664 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.9618537425994873, loss=1.3061414957046509
I0212 08:35:52.494896 140062419793664 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.1833114624023438, loss=1.2902400493621826
I0212 08:37:08.887720 140062411400960 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.739147424697876, loss=1.221463680267334
I0212 08:38:31.217836 140062419793664 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.188058614730835, loss=1.2424356937408447
I0212 08:39:55.089026 140062411400960 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.047966957092285, loss=1.2820332050323486
I0212 08:41:24.183946 140062419793664 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.4708220958709717, loss=1.2104884386062622
I0212 08:42:54.551684 140062411400960 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.894937038421631, loss=1.2569494247436523
I0212 08:44:24.621921 140062419793664 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.7578041553497314, loss=1.2114609479904175
I0212 08:44:43.702323 140218947737408 spec.py:321] Evaluating on the training split.
I0212 08:45:38.870874 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 08:46:31.727182 140218947737408 spec.py:349] Evaluating on the test split.
I0212 08:46:58.136458 140218947737408 submission_runner.py:408] Time since start: 33230.11s, 	Step: 35722, 	{'train/ctc_loss': Array(0.279953, dtype=float32), 'train/wer': 0.09680143393389304, 'validation/ctc_loss': Array(0.4692077, dtype=float32), 'validation/wer': 0.13608233488129604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25975537, dtype=float32), 'test/wer': 0.08329778806897813, 'test/num_examples': 2472, 'score': 30262.328066825867, 'total_duration': 33230.11437892914, 'accumulated_submission_time': 30262.328066825867, 'accumulated_eval_time': 2964.973935842514, 'accumulated_logging_time': 1.1049518585205078}
I0212 08:46:58.178020 140062419793664 logging_writer.py:48] [35722] accumulated_eval_time=2964.973936, accumulated_logging_time=1.104952, accumulated_submission_time=30262.328067, global_step=35722, preemption_count=0, score=30262.328067, test/ctc_loss=0.25975537300109863, test/num_examples=2472, test/wer=0.083298, total_duration=33230.114379, train/ctc_loss=0.2799530029296875, train/wer=0.096801, validation/ctc_loss=0.4692077040672302, validation/num_examples=5348, validation/wer=0.136082
I0212 08:47:57.392916 140062411400960 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.1759045124053955, loss=1.2493586540222168
I0212 08:49:12.564043 140062419793664 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.8517613410949707, loss=1.2892147302627563
I0212 08:50:41.844549 140062411400960 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.9770557880401611, loss=1.2125904560089111
I0212 08:52:08.492845 140062419793664 logging_writer.py:48] [36100] global_step=36100, grad_norm=4.591520309448242, loss=1.2524551153182983
I0212 08:53:27.309181 140062411400960 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.636260986328125, loss=1.212112545967102
I0212 08:54:44.665524 140062419793664 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.588975429534912, loss=1.2380694150924683
I0212 08:56:06.486171 140062411400960 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.479743242263794, loss=1.289979338645935
I0212 08:57:31.330902 140062419793664 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.9296798706054688, loss=1.2567617893218994
I0212 08:59:01.383267 140062411400960 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.12542986869812, loss=1.2302876710891724
I0212 09:00:34.575031 140062419793664 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.1383113861083984, loss=1.207348108291626
I0212 09:02:02.900466 140062411400960 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.3625590801239014, loss=1.2108973264694214
I0212 09:03:34.474868 140062419793664 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.541163444519043, loss=1.260182499885559
I0212 09:05:02.065070 140062411400960 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.6228439807891846, loss=1.2765023708343506
I0212 09:06:31.077179 140063075153664 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.6350739002227783, loss=1.1778417825698853
I0212 09:07:47.750734 140063066760960 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.246641159057617, loss=1.242329716682434
I0212 09:09:07.722255 140063075153664 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.8010659217834473, loss=1.2743432521820068
I0212 09:10:31.726341 140063066760960 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.9595630168914795, loss=1.1685212850570679
I0212 09:10:58.147115 140218947737408 spec.py:321] Evaluating on the training split.
I0212 09:11:53.262225 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 09:12:47.074625 140218947737408 spec.py:349] Evaluating on the test split.
I0212 09:13:14.790771 140218947737408 submission_runner.py:408] Time since start: 34806.77s, 	Step: 37434, 	{'train/ctc_loss': Array(0.27683035, dtype=float32), 'train/wer': 0.09356260346207396, 'validation/ctc_loss': Array(0.4581124, dtype=float32), 'validation/wer': 0.13316662965716328, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2533827, dtype=float32), 'test/wer': 0.0805760363983507, 'test/num_examples': 2472, 'score': 31702.204449653625, 'total_duration': 34806.76887631416, 'accumulated_submission_time': 31702.204449653625, 'accumulated_eval_time': 3101.6122257709503, 'accumulated_logging_time': 1.1635775566101074}
I0212 09:13:14.829010 140063075153664 logging_writer.py:48] [37434] accumulated_eval_time=3101.612226, accumulated_logging_time=1.163578, accumulated_submission_time=31702.204450, global_step=37434, preemption_count=0, score=31702.204450, test/ctc_loss=0.25338271260261536, test/num_examples=2472, test/wer=0.080576, total_duration=34806.768876, train/ctc_loss=0.2768303453922272, train/wer=0.093563, validation/ctc_loss=0.4581123888492584, validation/num_examples=5348, validation/wer=0.133167
I0212 09:14:05.095030 140063066760960 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.687818765640259, loss=1.2181298732757568
I0212 09:15:20.155130 140063075153664 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.566861391067505, loss=1.2357734441757202
I0212 09:16:39.906560 140063066760960 logging_writer.py:48] [37700] global_step=37700, grad_norm=8.556174278259277, loss=1.2332109212875366
I0212 09:18:08.946834 140063075153664 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.7993314266204834, loss=1.1861838102340698
I0212 09:19:37.860523 140063066760960 logging_writer.py:48] [37900] global_step=37900, grad_norm=4.593949317932129, loss=1.2823011875152588
I0212 09:21:08.837939 140063075153664 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.422743320465088, loss=1.1809990406036377
I0212 09:22:38.419286 140063066760960 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.151219129562378, loss=1.1909723281860352
I0212 09:23:59.577738 140062419793664 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.9847567081451416, loss=1.1990187168121338
I0212 09:25:16.276061 140062411400960 logging_writer.py:48] [38300] global_step=38300, grad_norm=5.4773712158203125, loss=1.2163244485855103
I0212 09:26:37.091878 140062419793664 logging_writer.py:48] [38400] global_step=38400, grad_norm=7.959402561187744, loss=1.2365593910217285
I0212 09:28:01.053620 140062411400960 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.725536346435547, loss=1.211031436920166
I0212 09:29:31.896452 140062419793664 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.8242380619049072, loss=1.2249016761779785
I0212 09:30:59.933760 140062411400960 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.4260501861572266, loss=1.2307921648025513
I0212 09:32:31.961058 140062419793664 logging_writer.py:48] [38800] global_step=38800, grad_norm=5.136106014251709, loss=1.2033522129058838
I0212 09:34:00.403161 140062411400960 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.9893929958343506, loss=1.2105090618133545
I0212 09:35:33.205370 140062419793664 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.1404001712799072, loss=1.1740537881851196
I0212 09:37:01.651502 140062411400960 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.403836727142334, loss=1.168159008026123
I0212 09:37:15.433794 140218947737408 spec.py:321] Evaluating on the training split.
I0212 09:38:09.636438 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 09:39:02.820733 140218947737408 spec.py:349] Evaluating on the test split.
I0212 09:39:29.685956 140218947737408 submission_runner.py:408] Time since start: 36381.66s, 	Step: 39117, 	{'train/ctc_loss': Array(0.27329835, dtype=float32), 'train/wer': 0.09385282379626174, 'validation/ctc_loss': Array(0.44634154, dtype=float32), 'validation/wer': 0.13023161512691042, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24226296, dtype=float32), 'test/wer': 0.07698088680356671, 'test/num_examples': 2472, 'score': 33142.71718811989, 'total_duration': 36381.66338968277, 'accumulated_submission_time': 33142.71718811989, 'accumulated_eval_time': 3235.8583705425262, 'accumulated_logging_time': 1.2178065776824951}
I0212 09:39:29.730312 140062419793664 logging_writer.py:48] [39117] accumulated_eval_time=3235.858371, accumulated_logging_time=1.217807, accumulated_submission_time=33142.717188, global_step=39117, preemption_count=0, score=33142.717188, test/ctc_loss=0.24226295948028564, test/num_examples=2472, test/wer=0.076981, total_duration=36381.663390, train/ctc_loss=0.27329835295677185, train/wer=0.093853, validation/ctc_loss=0.44634154438972473, validation/num_examples=5348, validation/wer=0.130232
I0212 09:40:39.108740 140063075153664 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.2937216758728027, loss=1.223215937614441
I0212 09:41:58.139106 140063066760960 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.0128602981567383, loss=1.193541407585144
I0212 09:43:15.060667 140063075153664 logging_writer.py:48] [39400] global_step=39400, grad_norm=6.380681037902832, loss=1.2172343730926514
I0212 09:44:38.068800 140063066760960 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.074406862258911, loss=1.1628574132919312
I0212 09:46:05.250228 140063075153664 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.495786666870117, loss=1.1978983879089355
I0212 09:47:33.407046 140063066760960 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.595940351486206, loss=1.171472430229187
I0212 09:49:04.517726 140063075153664 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.971513509750366, loss=1.2135276794433594
I0212 09:50:34.257965 140063066760960 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.4051198959350586, loss=1.1905068159103394
I0212 09:52:05.583931 140063075153664 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.347440242767334, loss=1.1835886240005493
I0212 09:53:35.207467 140063066760960 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.0487170219421387, loss=1.245364785194397
I0212 09:55:02.344881 140063075153664 logging_writer.py:48] [40200] global_step=40200, grad_norm=4.29736328125, loss=1.1713175773620605
I0212 09:56:19.211770 140063066760960 logging_writer.py:48] [40300] global_step=40300, grad_norm=4.760406494140625, loss=1.180314064025879
I0212 09:57:38.808013 140063075153664 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.6350886821746826, loss=1.1720702648162842
I0212 09:59:01.374957 140063066760960 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.7147717475891113, loss=1.1520187854766846
I0212 10:00:29.406402 140063075153664 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.9616990089416504, loss=1.1507515907287598
I0212 10:01:56.408182 140063066760960 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.0578434467315674, loss=1.2056050300598145
I0212 10:03:25.421437 140063075153664 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.682955026626587, loss=1.2274446487426758
I0212 10:03:30.356612 140218947737408 spec.py:321] Evaluating on the training split.
I0212 10:04:25.174966 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 10:05:17.343255 140218947737408 spec.py:349] Evaluating on the test split.
I0212 10:05:44.340217 140218947737408 submission_runner.py:408] Time since start: 37956.32s, 	Step: 40807, 	{'train/ctc_loss': Array(0.26125973, dtype=float32), 'train/wer': 0.08696342444830096, 'validation/ctc_loss': Array(0.43017662, dtype=float32), 'validation/wer': 0.12485397337246686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23748754, dtype=float32), 'test/wer': 0.07631060467572563, 'test/num_examples': 2472, 'score': 34583.25195026398, 'total_duration': 37956.318457603455, 'accumulated_submission_time': 34583.25195026398, 'accumulated_eval_time': 3369.8367607593536, 'accumulated_logging_time': 1.2777178287506104}
I0212 10:05:44.383004 140063075153664 logging_writer.py:48] [40807] accumulated_eval_time=3369.836761, accumulated_logging_time=1.277718, accumulated_submission_time=34583.251950, global_step=40807, preemption_count=0, score=34583.251950, test/ctc_loss=0.2374875396490097, test/num_examples=2472, test/wer=0.076311, total_duration=37956.318458, train/ctc_loss=0.2612597346305847, train/wer=0.086963, validation/ctc_loss=0.43017661571502686, validation/num_examples=5348, validation/wer=0.124854
I0212 10:06:54.952856 140063066760960 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.4317638874053955, loss=1.1633621454238892
I0212 10:08:10.058246 140063075153664 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.6372768878936768, loss=1.1768701076507568
I0212 10:09:38.262372 140063066760960 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.467069387435913, loss=1.1150914430618286
I0212 10:11:12.508059 140062419793664 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.4691405296325684, loss=1.1138252019882202
I0212 10:12:31.880635 140062411400960 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.2590441703796387, loss=1.1745562553405762
I0212 10:13:51.617443 140062419793664 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.1143248081207275, loss=1.1556953191757202
I0212 10:15:09.831902 140062411400960 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.09674072265625, loss=1.1516658067703247
I0212 10:16:37.028298 140062419793664 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.8503379821777344, loss=1.2085427045822144
I0212 10:18:08.727384 140062411400960 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.5570011138916016, loss=1.156692624092102
I0212 10:19:41.278005 140062419793664 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.4421370029449463, loss=1.166289210319519
I0212 10:21:13.095046 140062411400960 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.794769525527954, loss=1.1545721292495728
I0212 10:22:43.369820 140062419793664 logging_writer.py:48] [42000] global_step=42000, grad_norm=5.3524489402771, loss=1.194398283958435
I0212 10:24:14.785707 140062411400960 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.4136624336242676, loss=1.217606544494629
I0212 10:25:46.525364 140062419793664 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.9762070178985596, loss=1.1185425519943237
I0212 10:27:12.203923 140062419793664 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.429255723953247, loss=1.17314875125885
I0212 10:28:29.035886 140062411400960 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.6425836086273193, loss=1.1415519714355469
I0212 10:29:44.430733 140218947737408 spec.py:321] Evaluating on the training split.
I0212 10:30:39.182944 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 10:31:31.742043 140218947737408 spec.py:349] Evaluating on the test split.
I0212 10:31:58.998053 140218947737408 submission_runner.py:408] Time since start: 39530.98s, 	Step: 42493, 	{'train/ctc_loss': Array(0.23121312, dtype=float32), 'train/wer': 0.08058338490357758, 'validation/ctc_loss': Array(0.42062643, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23134708, dtype=float32), 'test/wer': 0.07397477301809761, 'test/num_examples': 2472, 'score': 36023.209208488464, 'total_duration': 39530.97588849068, 'accumulated_submission_time': 36023.209208488464, 'accumulated_eval_time': 3504.3984801769257, 'accumulated_logging_time': 1.3354296684265137}
I0212 10:31:59.041623 140063075153664 logging_writer.py:48] [42493] accumulated_eval_time=3504.398480, accumulated_logging_time=1.335430, accumulated_submission_time=36023.209208, global_step=42493, preemption_count=0, score=36023.209208, test/ctc_loss=0.23134708404541016, test/num_examples=2472, test/wer=0.073975, total_duration=39530.975888, train/ctc_loss=0.23121312260627747, train/wer=0.080583, validation/ctc_loss=0.4206264317035675, validation/num_examples=5348, validation/wer=0.121871
I0212 10:32:05.230921 140063066760960 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.64912748336792, loss=1.1920592784881592
I0212 10:33:20.213030 140063075153664 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.781497478485107, loss=1.1586523056030273
I0212 10:34:35.417593 140063066760960 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.774794816970825, loss=1.169579267501831
I0212 10:36:05.771012 140063075153664 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.3171236515045166, loss=1.1689460277557373
I0212 10:37:37.720027 140063066760960 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.6794543266296387, loss=1.155411958694458
I0212 10:39:05.337656 140063075153664 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.230217218399048, loss=1.1432418823242188
I0212 10:40:33.203762 140063066760960 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.471071481704712, loss=1.1590319871902466
I0212 10:42:03.326200 140063075153664 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.7840325832366943, loss=1.1598670482635498
I0212 10:43:32.748387 140063075153664 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.2584362030029297, loss=1.1357550621032715
I0212 10:44:49.189620 140063066760960 logging_writer.py:48] [43400] global_step=43400, grad_norm=4.082797527313232, loss=1.1221216917037964
I0212 10:46:08.739029 140063075153664 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.5922272205352783, loss=1.2080395221710205
I0212 10:47:29.729424 140063066760960 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.93759822845459, loss=1.1584022045135498
I0212 10:48:56.100022 140063075153664 logging_writer.py:48] [43700] global_step=43700, grad_norm=5.7080888748168945, loss=1.1486550569534302
I0212 10:50:25.772983 140063066760960 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.1318371295928955, loss=1.0845738649368286
I0212 10:51:54.569663 140063075153664 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.7480416297912598, loss=1.1166670322418213
I0212 10:53:21.929533 140063066760960 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.0576224327087402, loss=1.1701663732528687
I0212 10:54:51.553630 140063075153664 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.9245526790618896, loss=1.1172860860824585
I0212 10:55:59.409147 140218947737408 spec.py:321] Evaluating on the training split.
I0212 10:56:53.128064 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 10:57:47.515119 140218947737408 spec.py:349] Evaluating on the test split.
I0212 10:58:14.853258 140218947737408 submission_runner.py:408] Time since start: 41106.83s, 	Step: 44175, 	{'train/ctc_loss': Array(0.2266846, dtype=float32), 'train/wer': 0.07835038481035993, 'validation/ctc_loss': Array(0.4167648, dtype=float32), 'validation/wer': 0.12135898896473155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22691439, dtype=float32), 'test/wer': 0.07241078138646842, 'test/num_examples': 2472, 'score': 37463.483968019485, 'total_duration': 41106.83026814461, 'accumulated_submission_time': 37463.483968019485, 'accumulated_eval_time': 3639.836142063141, 'accumulated_logging_time': 1.3962428569793701}
I0212 10:58:14.890840 140062491473664 logging_writer.py:48] [44175] accumulated_eval_time=3639.836142, accumulated_logging_time=1.396243, accumulated_submission_time=37463.483968, global_step=44175, preemption_count=0, score=37463.483968, test/ctc_loss=0.2269143909215927, test/num_examples=2472, test/wer=0.072411, total_duration=41106.830268, train/ctc_loss=0.2266846001148224, train/wer=0.078350, validation/ctc_loss=0.4167647957801819, validation/num_examples=5348, validation/wer=0.121359
I0212 10:58:34.508339 140062483080960 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.5622544288635254, loss=1.0905747413635254
I0212 10:59:54.185518 140062491473664 logging_writer.py:48] [44300] global_step=44300, grad_norm=4.626583099365234, loss=1.1768677234649658
I0212 11:01:13.922857 140062483080960 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.254254102706909, loss=1.1524449586868286
I0212 11:02:33.628053 140062491473664 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.5006914138793945, loss=1.1734095811843872
I0212 11:03:55.929721 140062483080960 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.578619956970215, loss=1.1513116359710693
I0212 11:05:19.163363 140062491473664 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.664163589477539, loss=1.1291035413742065
I0212 11:06:43.992813 140062483080960 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.4231443405151367, loss=1.1372464895248413
I0212 11:08:14.724188 140062491473664 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.436784505844116, loss=1.16110098361969
I0212 11:09:46.230766 140062483080960 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.801507472991943, loss=1.1569929122924805
I0212 11:11:15.121460 140062491473664 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.357518196105957, loss=1.1273707151412964
I0212 11:12:44.551172 140062483080960 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.209317922592163, loss=1.1423604488372803
I0212 11:14:14.371771 140062491473664 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.408294677734375, loss=1.1311225891113281
I0212 11:15:38.268910 140062491473664 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.896212339401245, loss=1.132179856300354
I0212 11:16:54.543005 140062483080960 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.3205480575561523, loss=1.1579467058181763
I0212 11:18:14.850036 140062491473664 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.7969188690185547, loss=1.1734472513198853
I0212 11:19:37.539832 140062483080960 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.9439492225646973, loss=1.1002165079116821
I0212 11:21:05.658771 140062491473664 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.2256999015808105, loss=1.1605223417282104
I0212 11:22:15.304070 140218947737408 spec.py:321] Evaluating on the training split.
I0212 11:23:09.942149 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 11:24:01.451221 140218947737408 spec.py:349] Evaluating on the test split.
I0212 11:24:28.139256 140218947737408 submission_runner.py:408] Time since start: 42680.12s, 	Step: 45875, 	{'train/ctc_loss': Array(0.21549411, dtype=float32), 'train/wer': 0.07470870837705008, 'validation/ctc_loss': Array(0.4130225, dtype=float32), 'validation/wer': 0.12059627137298821, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2248237, dtype=float32), 'test/wer': 0.07178112241789043, 'test/num_examples': 2472, 'score': 38903.80582237244, 'total_duration': 42680.11693024635, 'accumulated_submission_time': 38903.80582237244, 'accumulated_eval_time': 3772.6655526161194, 'accumulated_logging_time': 1.4497272968292236}
I0212 11:24:28.181433 140062491473664 logging_writer.py:48] [45875] accumulated_eval_time=3772.665553, accumulated_logging_time=1.449727, accumulated_submission_time=38903.805822, global_step=45875, preemption_count=0, score=38903.805822, test/ctc_loss=0.2248236984014511, test/num_examples=2472, test/wer=0.071781, total_duration=42680.116930, train/ctc_loss=0.21549411118030548, train/wer=0.074709, validation/ctc_loss=0.4130224883556366, validation/num_examples=5348, validation/wer=0.120596
I0212 11:24:47.759094 140062483080960 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.53206205368042, loss=1.129285216331482
I0212 11:26:02.692716 140062491473664 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.648900032043457, loss=1.1104892492294312
I0212 11:27:19.284335 140062483080960 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.292836904525757, loss=1.1771842241287231
I0212 11:28:50.207663 140062491473664 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.9841322898864746, loss=1.1027213335037231
I0212 11:30:19.701839 140062483080960 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.183504819869995, loss=1.093874454498291
I0212 11:31:49.936431 140062491473664 logging_writer.py:48] [46400] global_step=46400, grad_norm=5.083072185516357, loss=1.1468396186828613
I0212 11:33:07.792129 140062483080960 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.989853858947754, loss=1.1967065334320068
I0212 11:34:26.483907 140062491473664 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.0111947059631348, loss=1.1047054529190063
I0212 11:35:50.233855 140062483080960 logging_writer.py:48] [46700] global_step=46700, grad_norm=4.018653869628906, loss=1.16936457157135
I0212 11:37:14.735244 140062491473664 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.824887275695801, loss=1.1298561096191406
I0212 11:38:44.633638 140062483080960 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.909651279449463, loss=1.173443078994751
I0212 11:40:15.301003 140062491473664 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.8059582710266113, loss=1.1628113985061646
I0212 11:41:43.395080 140062483080960 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.6109707355499268, loss=1.1617988348007202
I0212 11:43:13.090094 140062491473664 logging_writer.py:48] [47200] global_step=47200, grad_norm=5.651705741882324, loss=1.1211638450622559
I0212 11:44:42.883528 140062483080960 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.260162591934204, loss=1.1427478790283203
I0212 11:46:15.562851 140062491473664 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.966339349746704, loss=1.151246428489685
I0212 11:47:35.462710 140062483080960 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.3106203079223633, loss=1.1454010009765625
I0212 11:48:28.584814 140218947737408 spec.py:321] Evaluating on the training split.
I0212 11:49:22.263864 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 11:50:15.237919 140218947737408 spec.py:349] Evaluating on the test split.
I0212 11:50:42.205660 140218947737408 submission_runner.py:408] Time since start: 44254.18s, 	Step: 47567, 	{'train/ctc_loss': Array(0.22340734, dtype=float32), 'train/wer': 0.07659547994480012, 'validation/ctc_loss': Array(0.413083, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22458875, dtype=float32), 'test/wer': 0.07147644872341723, 'test/num_examples': 2472, 'score': 40344.11418509483, 'total_duration': 44254.18384337425, 'accumulated_submission_time': 40344.11418509483, 'accumulated_eval_time': 3906.2811391353607, 'accumulated_logging_time': 1.5107195377349854}
I0212 11:50:42.252817 140062491473664 logging_writer.py:48] [47567] accumulated_eval_time=3906.281139, accumulated_logging_time=1.510720, accumulated_submission_time=40344.114185, global_step=47567, preemption_count=0, score=40344.114185, test/ctc_loss=0.22458875179290771, test/num_examples=2472, test/wer=0.071476, total_duration=44254.183843, train/ctc_loss=0.2234073430299759, train/wer=0.076595, validation/ctc_loss=0.4130829870700836, validation/num_examples=5348, validation/wer=0.120345
I0212 11:51:07.882045 140062483080960 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.8014867305755615, loss=1.1014277935028076
I0212 11:52:22.653028 140062491473664 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.843789577484131, loss=1.1307140588760376
I0212 11:53:37.612219 140062483080960 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.2378389835357666, loss=1.1293890476226807
I0212 11:55:08.048122 140062491473664 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.816023111343384, loss=1.1263140439987183
I0212 11:56:38.475754 140218947737408 spec.py:321] Evaluating on the training split.
I0212 11:57:31.887189 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 11:58:24.960929 140218947737408 spec.py:349] Evaluating on the test split.
I0212 11:58:52.050579 140218947737408 submission_runner.py:408] Time since start: 44744.03s, 	Step: 48000, 	{'train/ctc_loss': Array(0.2246082, dtype=float32), 'train/wer': 0.0775943396226415, 'validation/ctc_loss': Array(0.41315985, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22453754, dtype=float32), 'test/wer': 0.07129364450673328, 'test/num_examples': 2472, 'score': 40700.30122280121, 'total_duration': 44744.03060722351, 'accumulated_submission_time': 40700.30122280121, 'accumulated_eval_time': 4039.852513551712, 'accumulated_logging_time': 1.5739483833312988}
I0212 11:58:52.082048 140062491473664 logging_writer.py:48] [48000] accumulated_eval_time=4039.852514, accumulated_logging_time=1.573948, accumulated_submission_time=40700.301223, global_step=48000, preemption_count=0, score=40700.301223, test/ctc_loss=0.22453753650188446, test/num_examples=2472, test/wer=0.071294, total_duration=44744.030607, train/ctc_loss=0.22460819780826569, train/wer=0.077594, validation/ctc_loss=0.4131598472595215, validation/num_examples=5348, validation/wer=0.120345
I0212 11:58:52.105680 140062483080960 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40700.301223
I0212 11:58:52.314521 140218947737408 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 11:58:53.327193 140218947737408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3/checkpoint_48000
I0212 11:58:53.345976 140218947737408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_3/checkpoint_48000.
I0212 11:58:54.494459 140218947737408 submission_runner.py:583] Tuning trial 3/5
I0212 11:58:54.494707 140218947737408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0212 11:58:54.508389 140218947737408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(29.058031, dtype=float32), 'train/wer': 2.360243429556226, 'validation/ctc_loss': Array(28.14937, dtype=float32), 'validation/wer': 2.1855045038956527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256304, dtype=float32), 'test/wer': 2.3443422094936324, 'test/num_examples': 2472, 'score': 15.222284317016602, 'total_duration': 179.6249499320984, 'accumulated_submission_time': 15.222284317016602, 'accumulated_eval_time': 164.4026050567627, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1708, {'train/ctc_loss': Array(6.197022, dtype=float32), 'train/wer': 0.943809484010489, 'validation/ctc_loss': Array(6.2215743, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.1304574, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1455.2773833274841, 'total_duration': 1731.1534314155579, 'accumulated_submission_time': 1455.2773833274841, 'accumulated_eval_time': 275.77101039886475, 'accumulated_logging_time': 0.028853893280029297, 'global_step': 1708, 'preemption_count': 0}), (3430, {'train/ctc_loss': Array(4.192091, dtype=float32), 'train/wer': 0.8861044829697827, 'validation/ctc_loss': Array(4.297596, dtype=float32), 'validation/wer': 0.8586076059356808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.945421, dtype=float32), 'test/wer': 0.8374667397883533, 'test/num_examples': 2472, 'score': 2895.461868286133, 'total_duration': 3287.7166328430176, 'accumulated_submission_time': 2895.461868286133, 'accumulated_eval_time': 392.01263880729675, 'accumulated_logging_time': 0.08412456512451172, 'global_step': 3430, 'preemption_count': 0}), (5111, {'train/ctc_loss': Array(0.75358486, dtype=float32), 'train/wer': 0.24081478531193262, 'validation/ctc_loss': Array(1.097035, dtype=float32), 'validation/wer': 0.30279888392210624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.74433875, dtype=float32), 'test/wer': 0.23441594052769485, 'test/num_examples': 2472, 'score': 4335.978147506714, 'total_duration': 4863.8278868198395, 'accumulated_submission_time': 4335.978147506714, 'accumulated_eval_time': 527.4732053279877, 'accumulated_logging_time': 0.1376943588256836, 'global_step': 5111, 'preemption_count': 0}), (6798, {'train/ctc_loss': Array(0.56135374, dtype=float32), 'train/wer': 0.1861137746199915, 'validation/ctc_loss': Array(0.89679164, dtype=float32), 'validation/wer': 0.2572771947440069, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57972324, dtype=float32), 'test/wer': 0.18800398106960778, 'test/num_examples': 2472, 'score': 5776.205506324768, 'total_duration': 6441.831294298172, 'accumulated_submission_time': 5776.205506324768, 'accumulated_eval_time': 665.1103904247284, 'accumulated_logging_time': 0.19458389282226562, 'global_step': 6798, 'preemption_count': 0}), (8513, {'train/ctc_loss': Array(0.43911177, dtype=float32), 'train/wer': 0.14939553226376198, 'validation/ctc_loss': Array(0.7938834, dtype=float32), 'validation/wer': 0.2276953377680373, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49849275, dtype=float32), 'test/wer': 0.1604817906688603, 'test/num_examples': 2472, 'score': 7216.254794597626, 'total_duration': 8017.511333227158, 'accumulated_submission_time': 7216.254794597626, 'accumulated_eval_time': 800.5987780094147, 'accumulated_logging_time': 0.25530076026916504, 'global_step': 8513, 'preemption_count': 0}), (10207, {'train/ctc_loss': Array(0.39790037, dtype=float32), 'train/wer': 0.134293730165625, 'validation/ctc_loss': Array(0.73437, dtype=float32), 'validation/wer': 0.21133070083126562, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46262303, dtype=float32), 'test/wer': 0.147421444965775, 'test/num_examples': 2472, 'score': 8656.777244329453, 'total_duration': 9595.573912382126, 'accumulated_submission_time': 8656.777244329453, 'accumulated_eval_time': 938.0026862621307, 'accumulated_logging_time': 0.31001925468444824, 'global_step': 10207, 'preemption_count': 0}), (11906, {'train/ctc_loss': Array(0.40488553, dtype=float32), 'train/wer': 0.13651806968188704, 'validation/ctc_loss': Array(0.7124341, dtype=float32), 'validation/wer': 0.20631993589310368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43567663, dtype=float32), 'test/wer': 0.13974366786504985, 'test/num_examples': 2472, 'score': 10096.870054721832, 'total_duration': 11170.25594997406, 'accumulated_submission_time': 10096.870054721832, 'accumulated_eval_time': 1072.4567618370056, 'accumulated_logging_time': 0.3640625476837158, 'global_step': 11906, 'preemption_count': 0}), (13599, {'train/ctc_loss': Array(0.37492138, dtype=float32), 'train/wer': 0.12580418475685157, 'validation/ctc_loss': Array(0.6971934, dtype=float32), 'validation/wer': 0.20115469650598106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41858706, dtype=float32), 'test/wer': 0.13537667824426705, 'test/num_examples': 2472, 'score': 11537.339000701904, 'total_duration': 12744.704414606094, 'accumulated_submission_time': 11537.339000701904, 'accumulated_eval_time': 1206.3002724647522, 'accumulated_logging_time': 0.4186415672302246, 'global_step': 13599, 'preemption_count': 0}), (15296, {'train/ctc_loss': Array(0.36053848, dtype=float32), 'train/wer': 0.12280701754385964, 'validation/ctc_loss': Array(0.6717796, dtype=float32), 'validation/wer': 0.19308340654778572, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40154976, dtype=float32), 'test/wer': 0.13178152864948306, 'test/num_examples': 2472, 'score': 12977.885040283203, 'total_duration': 14324.161016225815, 'accumulated_submission_time': 12977.885040283203, 'accumulated_eval_time': 1345.0807433128357, 'accumulated_logging_time': 0.46816587448120117, 'global_step': 15296, 'preemption_count': 0}), (17012, {'train/ctc_loss': Array(0.38336515, dtype=float32), 'train/wer': 0.12243530425348607, 'validation/ctc_loss': Array(0.6500841, dtype=float32), 'validation/wer': 0.1883815905075451, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38370186, dtype=float32), 'test/wer': 0.12280381045233887, 'test/num_examples': 2472, 'score': 14418.47855091095, 'total_duration': 15901.694470643997, 'accumulated_submission_time': 14418.47855091095, 'accumulated_eval_time': 1481.8800811767578, 'accumulated_logging_time': 0.5250308513641357, 'global_step': 17012, 'preemption_count': 0}), (18700, {'train/ctc_loss': Array(0.2896522, dtype=float32), 'train/wer': 0.09824230366921673, 'validation/ctc_loss': Array(0.62595093, dtype=float32), 'validation/wer': 0.17998204234530832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37486923, dtype=float32), 'test/wer': 0.11914772611866024, 'test/num_examples': 2472, 'score': 15858.525933265686, 'total_duration': 17481.879634141922, 'accumulated_submission_time': 15858.525933265686, 'accumulated_eval_time': 1621.8828246593475, 'accumulated_logging_time': 0.5793395042419434, 'global_step': 18700, 'preemption_count': 0}), (20401, {'train/ctc_loss': Array(0.3050606, dtype=float32), 'train/wer': 0.10250043128911009, 'validation/ctc_loss': Array(0.6120852, dtype=float32), 'validation/wer': 0.1774428685905172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36329266, dtype=float32), 'test/wer': 0.11859931346860846, 'test/num_examples': 2472, 'score': 17298.88808941841, 'total_duration': 19057.164207220078, 'accumulated_submission_time': 17298.88808941841, 'accumulated_eval_time': 1756.674932718277, 'accumulated_logging_time': 0.6285579204559326, 'global_step': 20401, 'preemption_count': 0}), (22128, {'train/ctc_loss': Array(0.4048885, dtype=float32), 'train/wer': 0.13275771099435005, 'validation/ctc_loss': Array(0.60599583, dtype=float32), 'validation/wer': 0.17508713324386688, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34999764, dtype=float32), 'test/wer': 0.11211991956614466, 'test/num_examples': 2472, 'score': 18739.611248254776, 'total_duration': 20632.988973617554, 'accumulated_submission_time': 18739.611248254776, 'accumulated_eval_time': 1891.6445426940918, 'accumulated_logging_time': 0.6785871982574463, 'global_step': 22128, 'preemption_count': 0}), (23829, {'train/ctc_loss': Array(0.4211561, dtype=float32), 'train/wer': 0.13787367125247452, 'validation/ctc_loss': Array(0.5772926, dtype=float32), 'validation/wer': 0.1673344468366529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33509344, dtype=float32), 'test/wer': 0.10805760363983508, 'test/num_examples': 2472, 'score': 20180.074142932892, 'total_duration': 22207.29289650917, 'accumulated_submission_time': 20180.074142932892, 'accumulated_eval_time': 2025.3478963375092, 'accumulated_logging_time': 0.7357115745544434, 'global_step': 23829, 'preemption_count': 0}), (25527, {'train/ctc_loss': Array(0.4618884, dtype=float32), 'train/wer': 0.15217769630592165, 'validation/ctc_loss': Array(0.565899, dtype=float32), 'validation/wer': 0.1642932311227396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32599014, dtype=float32), 'test/wer': 0.1042187150894725, 'test/num_examples': 2472, 'score': 21620.75711417198, 'total_duration': 23780.77961206436, 'accumulated_submission_time': 21620.75711417198, 'accumulated_eval_time': 2158.018583536148, 'accumulated_logging_time': 0.7869384288787842, 'global_step': 25527, 'preemption_count': 0}), (27250, {'train/ctc_loss': Array(0.39754912, dtype=float32), 'train/wer': 0.12864185937316586, 'validation/ctc_loss': Array(0.54623437, dtype=float32), 'validation/wer': 0.15894455332747617, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31674242, dtype=float32), 'test/wer': 0.10249223082079094, 'test/num_examples': 2472, 'score': 23061.056136369705, 'total_duration': 25352.061848640442, 'accumulated_submission_time': 23061.056136369705, 'accumulated_eval_time': 2288.8654062747955, 'accumulated_logging_time': 0.8404562473297119, 'global_step': 27250, 'preemption_count': 0}), (28939, {'train/ctc_loss': Array(0.35738322, dtype=float32), 'train/wer': 0.118875696087301, 'validation/ctc_loss': Array(0.5297782, dtype=float32), 'validation/wer': 0.15318072545063094, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30347437, dtype=float32), 'test/wer': 0.09702841589990453, 'test/num_examples': 2472, 'score': 24501.343689918518, 'total_duration': 26927.3649828434, 'accumulated_submission_time': 24501.343689918518, 'accumulated_eval_time': 2423.746869325638, 'accumulated_logging_time': 0.89430832862854, 'global_step': 28939, 'preemption_count': 0}), (30640, {'train/ctc_loss': Array(0.3072533, dtype=float32), 'train/wer': 0.1059137139551105, 'validation/ctc_loss': Array(0.51826257, dtype=float32), 'validation/wer': 0.15005261785917723, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29775858, dtype=float32), 'test/wer': 0.09668311904616822, 'test/num_examples': 2472, 'score': 25941.561491966248, 'total_duration': 28503.707077503204, 'accumulated_submission_time': 25941.561491966248, 'accumulated_eval_time': 2559.73819565773, 'accumulated_logging_time': 0.9447612762451172, 'global_step': 30640, 'preemption_count': 0}), (32345, {'train/ctc_loss': Array(0.34496155, dtype=float32), 'train/wer': 0.11648960537510089, 'validation/ctc_loss': Array(0.49741358, dtype=float32), 'validation/wer': 0.14559216814543768, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2845581, dtype=float32), 'test/wer': 0.09113805780675563, 'test/num_examples': 2472, 'score': 27381.578418970108, 'total_duration': 30078.26686859131, 'accumulated_submission_time': 27381.578418970108, 'accumulated_eval_time': 2694.1472787857056, 'accumulated_logging_time': 0.9967937469482422, 'global_step': 32345, 'preemption_count': 0}), (34019, {'train/ctc_loss': Array(0.2937716, dtype=float32), 'train/wer': 0.09893059565821838, 'validation/ctc_loss': Array(0.48239866, dtype=float32), 'validation/wer': 0.13939388088089055, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26872075, dtype=float32), 'test/wer': 0.08601953973960555, 'test/num_examples': 2472, 'score': 28822.132719278336, 'total_duration': 31655.35311293602, 'accumulated_submission_time': 28822.132719278336, 'accumulated_eval_time': 2830.5453424453735, 'accumulated_logging_time': 1.0511739253997803, 'global_step': 34019, 'preemption_count': 0}), (35722, {'train/ctc_loss': Array(0.279953, dtype=float32), 'train/wer': 0.09680143393389304, 'validation/ctc_loss': Array(0.4692077, dtype=float32), 'validation/wer': 0.13608233488129604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25975537, dtype=float32), 'test/wer': 0.08329778806897813, 'test/num_examples': 2472, 'score': 30262.328066825867, 'total_duration': 33230.11437892914, 'accumulated_submission_time': 30262.328066825867, 'accumulated_eval_time': 2964.973935842514, 'accumulated_logging_time': 1.1049518585205078, 'global_step': 35722, 'preemption_count': 0}), (37434, {'train/ctc_loss': Array(0.27683035, dtype=float32), 'train/wer': 0.09356260346207396, 'validation/ctc_loss': Array(0.4581124, dtype=float32), 'validation/wer': 0.13316662965716328, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2533827, dtype=float32), 'test/wer': 0.0805760363983507, 'test/num_examples': 2472, 'score': 31702.204449653625, 'total_duration': 34806.76887631416, 'accumulated_submission_time': 31702.204449653625, 'accumulated_eval_time': 3101.6122257709503, 'accumulated_logging_time': 1.1635775566101074, 'global_step': 37434, 'preemption_count': 0}), (39117, {'train/ctc_loss': Array(0.27329835, dtype=float32), 'train/wer': 0.09385282379626174, 'validation/ctc_loss': Array(0.44634154, dtype=float32), 'validation/wer': 0.13023161512691042, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24226296, dtype=float32), 'test/wer': 0.07698088680356671, 'test/num_examples': 2472, 'score': 33142.71718811989, 'total_duration': 36381.66338968277, 'accumulated_submission_time': 33142.71718811989, 'accumulated_eval_time': 3235.8583705425262, 'accumulated_logging_time': 1.2178065776824951, 'global_step': 39117, 'preemption_count': 0}), (40807, {'train/ctc_loss': Array(0.26125973, dtype=float32), 'train/wer': 0.08696342444830096, 'validation/ctc_loss': Array(0.43017662, dtype=float32), 'validation/wer': 0.12485397337246686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23748754, dtype=float32), 'test/wer': 0.07631060467572563, 'test/num_examples': 2472, 'score': 34583.25195026398, 'total_duration': 37956.318457603455, 'accumulated_submission_time': 34583.25195026398, 'accumulated_eval_time': 3369.8367607593536, 'accumulated_logging_time': 1.2777178287506104, 'global_step': 40807, 'preemption_count': 0}), (42493, {'train/ctc_loss': Array(0.23121312, dtype=float32), 'train/wer': 0.08058338490357758, 'validation/ctc_loss': Array(0.42062643, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23134708, dtype=float32), 'test/wer': 0.07397477301809761, 'test/num_examples': 2472, 'score': 36023.209208488464, 'total_duration': 39530.97588849068, 'accumulated_submission_time': 36023.209208488464, 'accumulated_eval_time': 3504.3984801769257, 'accumulated_logging_time': 1.3354296684265137, 'global_step': 42493, 'preemption_count': 0}), (44175, {'train/ctc_loss': Array(0.2266846, dtype=float32), 'train/wer': 0.07835038481035993, 'validation/ctc_loss': Array(0.4167648, dtype=float32), 'validation/wer': 0.12135898896473155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22691439, dtype=float32), 'test/wer': 0.07241078138646842, 'test/num_examples': 2472, 'score': 37463.483968019485, 'total_duration': 41106.83026814461, 'accumulated_submission_time': 37463.483968019485, 'accumulated_eval_time': 3639.836142063141, 'accumulated_logging_time': 1.3962428569793701, 'global_step': 44175, 'preemption_count': 0}), (45875, {'train/ctc_loss': Array(0.21549411, dtype=float32), 'train/wer': 0.07470870837705008, 'validation/ctc_loss': Array(0.4130225, dtype=float32), 'validation/wer': 0.12059627137298821, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2248237, dtype=float32), 'test/wer': 0.07178112241789043, 'test/num_examples': 2472, 'score': 38903.80582237244, 'total_duration': 42680.11693024635, 'accumulated_submission_time': 38903.80582237244, 'accumulated_eval_time': 3772.6655526161194, 'accumulated_logging_time': 1.4497272968292236, 'global_step': 45875, 'preemption_count': 0}), (47567, {'train/ctc_loss': Array(0.22340734, dtype=float32), 'train/wer': 0.07659547994480012, 'validation/ctc_loss': Array(0.413083, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22458875, dtype=float32), 'test/wer': 0.07147644872341723, 'test/num_examples': 2472, 'score': 40344.11418509483, 'total_duration': 44254.18384337425, 'accumulated_submission_time': 40344.11418509483, 'accumulated_eval_time': 3906.2811391353607, 'accumulated_logging_time': 1.5107195377349854, 'global_step': 47567, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.2246082, dtype=float32), 'train/wer': 0.0775943396226415, 'validation/ctc_loss': Array(0.41315985, dtype=float32), 'validation/wer': 0.12034525039342711, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22453754, dtype=float32), 'test/wer': 0.07129364450673328, 'test/num_examples': 2472, 'score': 40700.30122280121, 'total_duration': 44744.03060722351, 'accumulated_submission_time': 40700.30122280121, 'accumulated_eval_time': 4039.852513551712, 'accumulated_logging_time': 1.5739483833312988, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 11:58:54.508664 140218947737408 submission_runner.py:586] Timing: 40700.30122280121
I0212 11:58:54.508741 140218947737408 submission_runner.py:588] Total number of evals: 30
I0212 11:58:54.508813 140218947737408 submission_runner.py:589] ====================
I0212 11:58:54.508894 140218947737408 submission_runner.py:542] Using RNG seed 2622380006
I0212 11:58:54.512105 140218947737408 submission_runner.py:551] --- Tuning run 4/5 ---
I0212 11:58:54.512233 140218947737408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4.
I0212 11:58:54.513815 140218947737408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4/hparams.json.
I0212 11:58:54.516162 140218947737408 submission_runner.py:206] Initializing dataset.
I0212 11:58:54.516290 140218947737408 submission_runner.py:213] Initializing model.
I0212 11:58:55.733303 140218947737408 submission_runner.py:255] Initializing optimizer.
I0212 11:58:55.876550 140218947737408 submission_runner.py:262] Initializing metrics bundle.
I0212 11:58:55.876729 140218947737408 submission_runner.py:280] Initializing checkpoint and logger.
I0212 11:58:55.882152 140218947737408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4 with prefix checkpoint_
I0212 11:58:55.882279 140218947737408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4/meta_data_0.json.
I0212 11:58:55.882639 140218947737408 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 11:58:55.882749 140218947737408 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 11:58:56.420675 140218947737408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 11:58:56.899914 140218947737408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4/flags_0.json.
I0212 11:58:56.920421 140218947737408 submission_runner.py:314] Starting training loop.
I0212 11:58:56.923979 140218947737408 input_pipeline.py:20] Loading split = train-clean-100
I0212 11:58:57.451826 140218947737408 input_pipeline.py:20] Loading split = train-clean-360
I0212 11:58:57.587893 140218947737408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 11:59:12.707721 140062382368512 logging_writer.py:48] [0] global_step=0, grad_norm=21.40202522277832, loss=33.18222427368164
I0212 11:59:12.722286 140218947737408 spec.py:321] Evaluating on the training split.
I0212 12:00:28.030236 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 12:01:23.433338 140218947737408 spec.py:349] Evaluating on the test split.
I0212 12:01:52.846130 140218947737408 submission_runner.py:408] Time since start: 175.92s, 	Step: 1, 	{'train/ctc_loss': Array(28.983004, dtype=float32), 'train/wer': 2.33386175402295, 'validation/ctc_loss': Array(28.14912, dtype=float32), 'validation/wer': 2.185417612018112, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256042, dtype=float32), 'test/wer': 2.3442000284362114, 'test/num_examples': 2472, 'score': 15.801752090454102, 'total_duration': 175.9230306148529, 'accumulated_submission_time': 15.801752090454102, 'accumulated_eval_time': 160.12119221687317, 'accumulated_logging_time': 0}
I0212 12:01:52.864181 140062491473664 logging_writer.py:48] [1] accumulated_eval_time=160.121192, accumulated_logging_time=0, accumulated_submission_time=15.801752, global_step=1, preemption_count=0, score=15.801752, test/ctc_loss=28.25604248046875, test/num_examples=2472, test/wer=2.344200, total_duration=175.923031, train/ctc_loss=28.983003616333008, train/wer=2.333862, validation/ctc_loss=28.149120330810547, validation/num_examples=5348, validation/wer=2.185418
I0212 12:03:18.440241 140062348797696 logging_writer.py:48] [100] global_step=100, grad_norm=2.6790850162506104, loss=5.772139072418213
I0212 12:04:34.461263 140062357190400 logging_writer.py:48] [200] global_step=200, grad_norm=2.7632884979248047, loss=4.759521961212158
I0212 12:05:49.862025 140062348797696 logging_writer.py:48] [300] global_step=300, grad_norm=2.3266255855560303, loss=3.6058547496795654
I0212 12:07:06.474971 140062357190400 logging_writer.py:48] [400] global_step=400, grad_norm=2.1590006351470947, loss=3.2106542587280273
I0212 12:08:34.771002 140062348797696 logging_writer.py:48] [500] global_step=500, grad_norm=2.180483102798462, loss=3.0468225479125977
I0212 12:10:02.571083 140062357190400 logging_writer.py:48] [600] global_step=600, grad_norm=2.3486359119415283, loss=2.821340322494507
I0212 12:11:32.410181 140062348797696 logging_writer.py:48] [700] global_step=700, grad_norm=2.122840166091919, loss=2.6121034622192383
I0212 12:13:01.057017 140062357190400 logging_writer.py:48] [800] global_step=800, grad_norm=2.4508519172668457, loss=2.5945804119110107
I0212 12:14:32.400150 140062348797696 logging_writer.py:48] [900] global_step=900, grad_norm=3.2810428142547607, loss=2.5915443897247314
I0212 12:16:04.946733 140062357190400 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.0836682319641113, loss=2.5805156230926514
I0212 12:17:30.149654 140062491473664 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.1638102531433105, loss=2.372736930847168
I0212 12:18:47.494430 140062483080960 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.3843753337860107, loss=2.3358304500579834
I0212 12:20:10.193728 140062491473664 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.778975486755371, loss=2.3662304878234863
I0212 12:21:35.647275 140062483080960 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.680866003036499, loss=2.3000335693359375
I0212 12:23:05.083587 140062491473664 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.1726651191711426, loss=2.239424228668213
I0212 12:24:34.716309 140062483080960 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.257554531097412, loss=2.279357433319092
I0212 12:25:53.194530 140218947737408 spec.py:321] Evaluating on the training split.
I0212 12:26:44.858088 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 12:27:36.820700 140218947737408 spec.py:349] Evaluating on the test split.
I0212 12:28:04.079339 140218947737408 submission_runner.py:408] Time since start: 1747.15s, 	Step: 1688, 	{'train/ctc_loss': Array(1.6165359, dtype=float32), 'train/wer': 0.4345034367663282, 'validation/ctc_loss': Array(1.767953, dtype=float32), 'validation/wer': 0.4371240719464746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.3364433, dtype=float32), 'test/wer': 0.37432210102979707, 'test/num_examples': 2472, 'score': 1456.0460293293, 'total_duration': 1747.1521837711334, 'accumulated_submission_time': 1456.0460293293, 'accumulated_eval_time': 290.99935603141785, 'accumulated_logging_time': 0.02970576286315918}
I0212 12:28:04.113879 140062860113664 logging_writer.py:48] [1688] accumulated_eval_time=290.999356, accumulated_logging_time=0.029706, accumulated_submission_time=1456.046029, global_step=1688, preemption_count=0, score=1456.046029, test/ctc_loss=1.336443305015564, test/num_examples=2472, test/wer=0.374322, total_duration=1747.152184, train/ctc_loss=1.6165359020233154, train/wer=0.434503, validation/ctc_loss=1.7679530382156372, validation/num_examples=5348, validation/wer=0.437124
I0212 12:28:13.932006 140062851720960 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.4285354614257812, loss=2.215106964111328
I0212 12:29:29.073544 140062860113664 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.03835391998291, loss=2.184033155441284
I0212 12:30:48.026294 140062851720960 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.056318998336792, loss=2.1802570819854736
I0212 12:32:16.177799 140062860113664 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.786306381225586, loss=2.1576147079467773
I0212 12:33:43.667276 140062860113664 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.2626357078552246, loss=2.1446781158447266
I0212 12:35:00.747150 140062851720960 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.669500827789307, loss=2.095284938812256
I0212 12:36:20.020081 140062860113664 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.141225814819336, loss=2.1571526527404785
I0212 12:37:46.138979 140062851720960 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.0999765396118164, loss=2.114281177520752
I0212 12:39:11.115615 140062860113664 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.6034653186798096, loss=2.1122825145721436
I0212 12:40:39.330572 140062851720960 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.110374450683594, loss=2.0720584392547607
I0212 12:42:09.330092 140062860113664 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.4062600135803223, loss=2.050753116607666
I0212 12:43:39.018560 140062851720960 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.6083953380584717, loss=2.073024272918701
I0212 12:45:05.868645 140062860113664 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.797830104827881, loss=2.0460290908813477
I0212 12:46:37.570810 140062851720960 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.8778178691864014, loss=2.0617384910583496
I0212 12:48:11.123215 140062860113664 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.441262722015381, loss=2.013286828994751
I0212 12:49:29.097514 140062851720960 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.44944167137146, loss=2.044227361679077
I0212 12:50:50.003192 140062860113664 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.3756604194641113, loss=2.093614339828491
I0212 12:52:04.205880 140218947737408 spec.py:321] Evaluating on the training split.
I0212 12:52:57.728307 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 12:53:50.401273 140218947737408 spec.py:349] Evaluating on the test split.
I0212 12:54:18.513827 140218947737408 submission_runner.py:408] Time since start: 3321.59s, 	Step: 3393, 	{'train/ctc_loss': Array(0.9240469, dtype=float32), 'train/wer': 0.2783773278611686, 'validation/ctc_loss': Array(1.0730573, dtype=float32), 'validation/wer': 0.29669714318815954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71331537, dtype=float32), 'test/wer': 0.22472731704344648, 'test/num_examples': 2472, 'score': 2896.040989637375, 'total_duration': 3321.5905945301056, 'accumulated_submission_time': 2896.040989637375, 'accumulated_eval_time': 425.304594039917, 'accumulated_logging_time': 0.08489155769348145}
I0212 12:54:18.550322 140062568273664 logging_writer.py:48] [3393] accumulated_eval_time=425.304594, accumulated_logging_time=0.084892, accumulated_submission_time=2896.040990, global_step=3393, preemption_count=0, score=2896.040990, test/ctc_loss=0.7133153676986694, test/num_examples=2472, test/wer=0.224727, total_duration=3321.590595, train/ctc_loss=0.9240468740463257, train/wer=0.278377, validation/ctc_loss=1.0730572938919067, validation/num_examples=5348, validation/wer=0.296697
I0212 12:54:24.695046 140062559880960 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.483943223953247, loss=2.0671133995056152
I0212 12:55:39.880302 140062568273664 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.7382452487945557, loss=2.024446964263916
I0212 12:56:54.949051 140062559880960 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.122016906738281, loss=2.035463333129883
I0212 12:58:25.782397 140062568273664 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.8532073497772217, loss=2.0609683990478516
I0212 12:59:56.918260 140062559880960 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.297232627868652, loss=1.9819506406784058
I0212 13:01:26.609808 140062568273664 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.376824378967285, loss=1.9752278327941895
I0212 13:02:55.857998 140062559880960 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.632413625717163, loss=2.0129268169403076
I0212 13:04:22.742066 140062568273664 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.7815678119659424, loss=1.9922221899032593
I0212 13:05:46.278993 140062568273664 logging_writer.py:48] [4200] global_step=4200, grad_norm=5.22413444519043, loss=1.9282914400100708
I0212 13:07:06.311882 140062559880960 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.375809669494629, loss=1.9902560710906982
I0212 13:08:26.463287 140062568273664 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.2320103645324707, loss=1.9671951532363892
I0212 13:09:51.799009 140062559880960 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.057142972946167, loss=2.0341947078704834
I0212 13:11:20.430269 140062568273664 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.797654151916504, loss=2.0119948387145996
I0212 13:12:53.066103 140062559880960 logging_writer.py:48] [4700] global_step=4700, grad_norm=7.244880199432373, loss=2.005096197128296
I0212 13:14:21.841903 140062568273664 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.255034923553467, loss=1.9337037801742554
I0212 13:15:53.031498 140062559880960 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.2504687309265137, loss=1.8924411535263062
I0212 13:17:27.469644 140062568273664 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.7521684169769287, loss=2.0069475173950195
I0212 13:18:18.965909 140218947737408 spec.py:321] Evaluating on the training split.
I0212 13:19:13.412187 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 13:20:06.162152 140218947737408 spec.py:349] Evaluating on the test split.
I0212 13:20:32.840676 140218947737408 submission_runner.py:408] Time since start: 4895.91s, 	Step: 5060, 	{'train/ctc_loss': Array(0.891339, dtype=float32), 'train/wer': 0.26996244674341974, 'validation/ctc_loss': Array(0.94086546, dtype=float32), 'validation/wer': 0.2633403168657134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6102613, dtype=float32), 'test/wer': 0.19052261694391973, 'test/num_examples': 2472, 'score': 4336.36754155159, 'total_duration': 4895.912990808487, 'accumulated_submission_time': 4336.36754155159, 'accumulated_eval_time': 559.1721725463867, 'accumulated_logging_time': 0.1350078582763672}
I0212 13:20:32.879957 140063075153664 logging_writer.py:48] [5060] accumulated_eval_time=559.172173, accumulated_logging_time=0.135008, accumulated_submission_time=4336.367542, global_step=5060, preemption_count=0, score=4336.367542, test/ctc_loss=0.6102613210678101, test/num_examples=2472, test/wer=0.190523, total_duration=4895.912991, train/ctc_loss=0.8913390040397644, train/wer=0.269962, validation/ctc_loss=0.9408654570579529, validation/num_examples=5348, validation/wer=0.263340
I0212 13:21:03.721609 140063066760960 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.226483106613159, loss=2.087047815322876
I0212 13:22:24.419324 140063075153664 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.9891715049743652, loss=1.9791204929351807
I0212 13:23:42.830600 140063066760960 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.9661471843719482, loss=1.933200478553772
I0212 13:25:03.005048 140063075153664 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.728000640869141, loss=1.9262583255767822
I0212 13:26:26.798687 140063066760960 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.087809085845947, loss=1.9495469331741333
I0212 13:27:55.576146 140063075153664 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.793912887573242, loss=1.9593377113342285
I0212 13:29:24.277193 140063066760960 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.518941640853882, loss=1.9835649728775024
I0212 13:30:56.081334 140063075153664 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.6883296966552734, loss=1.9103695154190063
I0212 13:32:26.537362 140063066760960 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.7731575965881348, loss=1.9524264335632324
I0212 13:33:56.530245 140063075153664 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.896665096282959, loss=1.9514834880828857
I0212 13:35:25.132081 140063066760960 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.0712268352508545, loss=1.9826469421386719
I0212 13:36:55.958625 140062532433664 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.5499930381774902, loss=1.9048714637756348
I0212 13:38:12.321277 140062524040960 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.941758871078491, loss=1.8999378681182861
I0212 13:39:33.488410 140062532433664 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.86666202545166, loss=1.9200985431671143
I0212 13:40:56.496163 140062524040960 logging_writer.py:48] [6500] global_step=6500, grad_norm=6.466559886932373, loss=2.0359861850738525
I0212 13:42:21.633788 140062532433664 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.5624003410339355, loss=1.9022434949874878
I0212 13:43:51.693401 140062524040960 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.918457508087158, loss=1.9310429096221924
I0212 13:44:33.414923 140218947737408 spec.py:321] Evaluating on the training split.
I0212 13:45:27.756675 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 13:46:22.240051 140218947737408 spec.py:349] Evaluating on the test split.
I0212 13:46:50.278202 140218947737408 submission_runner.py:408] Time since start: 6473.35s, 	Step: 6747, 	{'train/ctc_loss': Array(0.77726054, dtype=float32), 'train/wer': 0.23642880380520748, 'validation/ctc_loss': Array(0.8973155, dtype=float32), 'validation/wer': 0.24943761645925253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57886106, dtype=float32), 'test/wer': 0.18176832612272256, 'test/num_examples': 2472, 'score': 5776.8087022304535, 'total_duration': 6473.349189996719, 'accumulated_submission_time': 5776.8087022304535, 'accumulated_eval_time': 696.0269293785095, 'accumulated_logging_time': 0.1918809413909912}
I0212 13:46:50.316492 140062532433664 logging_writer.py:48] [6747] accumulated_eval_time=696.026929, accumulated_logging_time=0.191881, accumulated_submission_time=5776.808702, global_step=6747, preemption_count=0, score=5776.808702, test/ctc_loss=0.5788610577583313, test/num_examples=2472, test/wer=0.181768, total_duration=6473.349190, train/ctc_loss=0.7772605419158936, train/wer=0.236429, validation/ctc_loss=0.897315502166748, validation/num_examples=5348, validation/wer=0.249438
I0212 13:47:30.888008 140062524040960 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.717616319656372, loss=1.9436808824539185
I0212 13:48:46.476031 140062532433664 logging_writer.py:48] [6900] global_step=6900, grad_norm=5.144005298614502, loss=1.9615952968597412
I0212 13:50:08.385574 140062524040960 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.9231066703796387, loss=1.8999319076538086
I0212 13:51:39.109961 140062532433664 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.6946511268615723, loss=1.809941291809082
I0212 13:53:09.684464 140062524040960 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.1283676624298096, loss=1.959496259689331
I0212 13:54:32.628044 140062532433664 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.7589526176452637, loss=1.802437663078308
I0212 13:55:49.148948 140062524040960 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.3087644577026367, loss=1.9079921245574951
I0212 13:57:08.073343 140062532433664 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8875259160995483, loss=1.8183579444885254
I0212 13:58:31.968487 140062524040960 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.430999994277954, loss=1.914157509803772
I0212 14:00:02.420425 140062532433664 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.089833974838257, loss=1.9173094034194946
I0212 14:01:32.898992 140062524040960 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.7078211307525635, loss=1.923012375831604
I0212 14:03:02.098443 140062532433664 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.581008195877075, loss=1.829885482788086
I0212 14:04:34.211075 140062524040960 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.697707176208496, loss=1.7644106149673462
I0212 14:06:03.186312 140062532433664 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.499476909637451, loss=1.9230084419250488
I0212 14:07:32.852248 140062524040960 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.9069833755493164, loss=1.9074573516845703
I0212 14:08:57.476796 140062532433664 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.627471685409546, loss=1.844000220298767
I0212 14:10:15.219036 140062524040960 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.483013868331909, loss=2.214313268661499
I0212 14:10:50.715680 140218947737408 spec.py:321] Evaluating on the training split.
I0212 14:11:45.584806 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 14:12:38.531100 140218947737408 spec.py:349] Evaluating on the test split.
I0212 14:13:05.852470 140218947737408 submission_runner.py:408] Time since start: 8048.93s, 	Step: 8445, 	{'train/ctc_loss': Array(0.8788927, dtype=float32), 'train/wer': 0.2620743476444855, 'validation/ctc_loss': Array(0.9430648, dtype=float32), 'validation/wer': 0.2627127644168107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.59052604, dtype=float32), 'test/wer': 0.1910100948550769, 'test/num_examples': 2472, 'score': 7217.110549926758, 'total_duration': 8048.926936626434, 'accumulated_submission_time': 7217.110549926758, 'accumulated_eval_time': 831.1587114334106, 'accumulated_logging_time': 0.25078248977661133}
I0212 14:13:05.888317 140062532433664 logging_writer.py:48] [8445] accumulated_eval_time=831.158711, accumulated_logging_time=0.250782, accumulated_submission_time=7217.110550, global_step=8445, preemption_count=0, score=7217.110550, test/ctc_loss=0.5905260443687439, test/num_examples=2472, test/wer=0.191010, total_duration=8048.926937, train/ctc_loss=0.878892719745636, train/wer=0.262074, validation/ctc_loss=0.94306480884552, validation/num_examples=5348, validation/wer=0.262713
I0212 14:13:47.933218 140062524040960 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.9773184061050415, loss=1.85650634765625
I0212 14:15:03.327759 140062532433664 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.8339154720306396, loss=1.8514671325683594
I0212 14:16:20.180721 140062524040960 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.168994426727295, loss=1.8494700193405151
I0212 14:17:50.476541 140062532433664 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.7861597537994385, loss=1.9215717315673828
I0212 14:19:21.944051 140062524040960 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.8640302419662476, loss=1.7972557544708252
I0212 14:20:50.735941 140062532433664 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.317404270172119, loss=1.8224235773086548
I0212 14:22:21.474602 140062524040960 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.000896453857422, loss=1.8580442667007446
I0212 14:23:50.988988 140062532433664 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.6861746311187744, loss=1.786644697189331
I0212 14:25:20.659201 140062532433664 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.3915202617645264, loss=1.783522367477417
I0212 14:26:38.106661 140062524040960 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.6509902477264404, loss=1.791294813156128
I0212 14:27:56.288326 140062532433664 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.4303596019744873, loss=1.906684160232544
I0212 14:29:19.080139 140062524040960 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.9804890155792236, loss=1.7466239929199219
I0212 14:30:46.374299 140062532433664 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.3174376487731934, loss=1.8691227436065674
I0212 14:32:16.886246 140062524040960 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.051123857498169, loss=1.8000434637069702
I0212 14:33:47.530231 140062532433664 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.831766128540039, loss=1.8144952058792114
I0212 14:35:20.463533 140062524040960 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.5085501670837402, loss=1.8967697620391846
I0212 14:36:49.904397 140062532433664 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.228172540664673, loss=1.8144651651382446
I0212 14:37:06.149431 140218947737408 spec.py:321] Evaluating on the training split.
I0212 14:38:01.413260 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 14:38:54.137329 140218947737408 spec.py:349] Evaluating on the test split.
I0212 14:39:21.174432 140218947737408 submission_runner.py:408] Time since start: 9624.25s, 	Step: 10120, 	{'train/ctc_loss': Array(0.711383, dtype=float32), 'train/wer': 0.21622274832054403, 'validation/ctc_loss': Array(0.83408403, dtype=float32), 'validation/wer': 0.2332853818898018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52912104, dtype=float32), 'test/wer': 0.16547843925822112, 'test/num_examples': 2472, 'score': 8657.196389913559, 'total_duration': 9624.246983766556, 'accumulated_submission_time': 8657.196389913559, 'accumulated_eval_time': 966.1767551898956, 'accumulated_logging_time': 0.3870253562927246}
I0212 14:39:21.211296 140062532433664 logging_writer.py:48] [10120] accumulated_eval_time=966.176755, accumulated_logging_time=0.387025, accumulated_submission_time=8657.196390, global_step=10120, preemption_count=0, score=8657.196390, test/ctc_loss=0.5291210412979126, test/num_examples=2472, test/wer=0.165478, total_duration=9624.246984, train/ctc_loss=0.7113829851150513, train/wer=0.216223, validation/ctc_loss=0.8340840339660645, validation/num_examples=5348, validation/wer=0.233285
I0212 14:40:22.149232 140062524040960 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.725741147994995, loss=1.857109785079956
I0212 14:41:40.504003 140063075153664 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.6226394176483154, loss=1.8040175437927246
I0212 14:42:58.383355 140063066760960 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.8272833824157715, loss=1.809807300567627
I0212 14:44:19.042758 140063075153664 logging_writer.py:48] [10500] global_step=10500, grad_norm=5.191779136657715, loss=1.8444441556930542
I0212 14:45:42.483686 140063066760960 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.606715679168701, loss=1.859323263168335
I0212 14:47:08.383865 140063075153664 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.533125638961792, loss=1.861956000328064
I0212 14:48:37.144589 140063066760960 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.0315120220184326, loss=1.8340915441513062
I0212 14:50:07.358838 140063075153664 logging_writer.py:48] [10900] global_step=10900, grad_norm=14.249970436096191, loss=2.2990036010742188
I0212 14:51:37.787906 140063066760960 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.5948877334594727, loss=1.9076045751571655
I0212 14:53:08.112173 140063075153664 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.755032539367676, loss=1.86347496509552
I0212 14:54:40.329756 140063066760960 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.9268877506256104, loss=1.7585794925689697
I0212 14:56:12.335906 140063075153664 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.4003636837005615, loss=1.775038242340088
I0212 14:57:35.081259 140062532433664 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.820584297180176, loss=1.8111521005630493
I0212 14:58:53.233307 140062524040960 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.8867106437683105, loss=1.844537615776062
I0212 15:00:13.374073 140062532433664 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.0911805629730225, loss=1.8408862352371216
I0212 15:01:37.382304 140062524040960 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.5667474269866943, loss=1.7542973756790161
I0212 15:03:05.449096 140062532433664 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.1948132514953613, loss=1.7882570028305054
I0212 15:03:21.802120 140218947737408 spec.py:321] Evaluating on the training split.
I0212 15:04:15.602962 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 15:05:09.468256 140218947737408 spec.py:349] Evaluating on the test split.
I0212 15:05:38.262006 140218947737408 submission_runner.py:408] Time since start: 11201.34s, 	Step: 11820, 	{'train/ctc_loss': Array(0.7196112, dtype=float32), 'train/wer': 0.2243399922553017, 'validation/ctc_loss': Array(0.83782446, dtype=float32), 'validation/wer': 0.23663554650163646, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51529604, dtype=float32), 'test/wer': 0.16371133183027645, 'test/num_examples': 2472, 'score': 10097.694558858871, 'total_duration': 11201.33668255806, 'accumulated_submission_time': 10097.694558858871, 'accumulated_eval_time': 1102.631802558899, 'accumulated_logging_time': 0.44161391258239746}
I0212 15:05:38.301133 140062532433664 logging_writer.py:48] [11820] accumulated_eval_time=1102.631803, accumulated_logging_time=0.441614, accumulated_submission_time=10097.694559, global_step=11820, preemption_count=0, score=10097.694559, test/ctc_loss=0.5152960419654846, test/num_examples=2472, test/wer=0.163711, total_duration=11201.336683, train/ctc_loss=0.7196112275123596, train/wer=0.224340, validation/ctc_loss=0.8378244638442993, validation/num_examples=5348, validation/wer=0.236636
I0212 15:06:39.780817 140062524040960 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.444537878036499, loss=1.7899476289749146
I0212 15:07:54.748246 140062532433664 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.1031150817871094, loss=1.75755774974823
I0212 15:09:21.224119 140062524040960 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.9072937965393066, loss=1.834804654121399
I0212 15:10:51.616698 140062532433664 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.385542631149292, loss=1.7550338506698608
I0212 15:12:21.824048 140062524040960 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.9582512378692627, loss=1.8392292261123657
I0212 15:13:49.823346 140062532433664 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.737426519393921, loss=1.816420316696167
I0212 15:15:05.436406 140062524040960 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.171058177947998, loss=1.834415316581726
I0212 15:16:24.923577 140062532433664 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.5440096855163574, loss=1.7873690128326416
I0212 15:17:43.447180 140062524040960 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.899662494659424, loss=1.7143142223358154
I0212 15:19:09.792122 140062532433664 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.3319342136383057, loss=1.8052905797958374
I0212 15:20:39.767179 140062524040960 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.0133144855499268, loss=1.7891710996627808
I0212 15:22:08.441605 140062532433664 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.804450511932373, loss=1.7630743980407715
I0212 15:23:39.540971 140062524040960 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.067433834075928, loss=1.8030670881271362
I0212 15:25:11.160677 140062532433664 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.278536796569824, loss=1.8735724687576294
I0212 15:26:43.365782 140062524040960 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.8315958976745605, loss=1.7398247718811035
I0212 15:28:15.991428 140063075153664 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.767216205596924, loss=1.8314522504806519
I0212 15:29:32.370101 140063066760960 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.283509254455566, loss=1.7373546361923218
I0212 15:29:38.264451 140218947737408 spec.py:321] Evaluating on the training split.
I0212 15:30:32.244472 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 15:31:26.992057 140218947737408 spec.py:349] Evaluating on the test split.
I0212 15:31:53.901184 140218947737408 submission_runner.py:408] Time since start: 12776.98s, 	Step: 13509, 	{'train/ctc_loss': Array(0.68961746, dtype=float32), 'train/wer': 0.21113669643755378, 'validation/ctc_loss': Array(0.8070212, dtype=float32), 'validation/wer': 0.2289504426658428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5031106, dtype=float32), 'test/wer': 0.15944590010765136, 'test/num_examples': 2472, 'score': 11537.564745664597, 'total_duration': 12776.975717544556, 'accumulated_submission_time': 11537.564745664597, 'accumulated_eval_time': 1238.2635581493378, 'accumulated_logging_time': 0.4966261386871338}
I0212 15:31:53.940448 140062645073664 logging_writer.py:48] [13509] accumulated_eval_time=1238.263558, accumulated_logging_time=0.496626, accumulated_submission_time=11537.564746, global_step=13509, preemption_count=0, score=11537.564746, test/ctc_loss=0.5031105875968933, test/num_examples=2472, test/wer=0.159446, total_duration=12776.975718, train/ctc_loss=0.6896174550056458, train/wer=0.211137, validation/ctc_loss=0.8070212006568909, validation/num_examples=5348, validation/wer=0.228950
I0212 15:33:03.067796 140062636680960 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.9557632207870483, loss=1.7704851627349854
I0212 15:34:18.146305 140062645073664 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.9038727283477783, loss=1.7863279581069946
I0212 15:35:33.000967 140062636680960 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.907681703567505, loss=1.7231453657150269
I0212 15:36:55.959242 140062645073664 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.773221492767334, loss=1.8083724975585938
I0212 15:38:29.604908 140062636680960 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.601806402206421, loss=1.7965450286865234
I0212 15:39:58.830741 140062645073664 logging_writer.py:48] [14100] global_step=14100, grad_norm=6.036249160766602, loss=1.7502131462097168
I0212 15:41:25.545255 140062636680960 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.5084173679351807, loss=1.717922568321228
I0212 15:42:55.016675 140062645073664 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.3091413974761963, loss=1.7781729698181152
I0212 15:44:24.807743 140062636680960 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.941159963607788, loss=1.7332478761672974
I0212 15:45:45.837480 140062645073664 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.861908435821533, loss=1.7795617580413818
I0212 15:47:02.476987 140062636680960 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.3991668224334717, loss=1.7322200536727905
I0212 15:48:19.992157 140062645073664 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.927577018737793, loss=1.6968350410461426
I0212 15:49:35.776433 140062636680960 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.1981396675109863, loss=1.7597377300262451
I0212 15:51:03.396627 140062645073664 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.717881441116333, loss=1.760589599609375
I0212 15:52:33.476211 140062636680960 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.269092082977295, loss=1.7499210834503174
I0212 15:54:03.825387 140062645073664 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.713646173477173, loss=1.713161826133728
I0212 15:55:37.242970 140062636680960 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.1235544681549072, loss=1.7205947637557983
I0212 15:55:54.127728 140218947737408 spec.py:321] Evaluating on the training split.
I0212 15:56:58.478835 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 15:57:53.285957 140218947737408 spec.py:349] Evaluating on the test split.
I0212 15:58:19.888898 140218947737408 submission_runner.py:408] Time since start: 14362.96s, 	Step: 15220, 	{'train/ctc_loss': Array(0.44087306, dtype=float32), 'train/wer': 0.14397901555097561, 'validation/ctc_loss': Array(0.77578115, dtype=float32), 'validation/wer': 0.218031030054935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4793862, dtype=float32), 'test/wer': 0.14955416082708753, 'test/num_examples': 2472, 'score': 12977.657584190369, 'total_duration': 14362.963431596756, 'accumulated_submission_time': 12977.657584190369, 'accumulated_eval_time': 1384.0197823047638, 'accumulated_logging_time': 0.5527534484863281}
I0212 15:58:19.928611 140063075153664 logging_writer.py:48] [15220] accumulated_eval_time=1384.019782, accumulated_logging_time=0.552753, accumulated_submission_time=12977.657584, global_step=15220, preemption_count=0, score=12977.657584, test/ctc_loss=0.47938621044158936, test/num_examples=2472, test/wer=0.149554, total_duration=14362.963432, train/ctc_loss=0.44087305665016174, train/wer=0.143979, validation/ctc_loss=0.7757811546325684, validation/num_examples=5348, validation/wer=0.218031
I0212 15:59:20.736190 140063066760960 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.1244311332702637, loss=1.753174901008606
I0212 16:00:36.340438 140063075153664 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.6752336025238037, loss=1.7667889595031738
I0212 16:01:56.300590 140062532433664 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.657508134841919, loss=1.7531359195709229
I0212 16:03:14.393519 140062524040960 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.660933494567871, loss=1.6920135021209717
I0212 16:04:34.346161 140062532433664 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.8268651962280273, loss=1.7215349674224854
I0212 16:05:55.314674 140062524040960 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.048255205154419, loss=1.7460228204727173
I0212 16:07:21.386656 140062532433664 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.6414835453033447, loss=1.7851446866989136
I0212 16:08:52.606542 140062524040960 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.6753511428833008, loss=1.6529675722122192
I0212 16:10:22.219508 140062532433664 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.8851959705352783, loss=1.717687726020813
I0212 16:11:50.148942 140062524040960 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.7562754154205322, loss=1.701650619506836
I0212 16:13:21.607223 140062532433664 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.2309036254882812, loss=1.733763337135315
I0212 16:14:53.182610 140062524040960 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.5874013900756836, loss=1.735010027885437
I0212 16:16:24.649163 140062532433664 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.3049747943878174, loss=1.8194934129714966
I0212 16:17:39.949892 140062524040960 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.6829144954681396, loss=1.6794694662094116
I0212 16:18:59.245203 140062532433664 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.8906102180480957, loss=1.6699676513671875
I0212 16:20:17.436282 140062524040960 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.6293771266937256, loss=1.71700918674469
I0212 16:21:38.905566 140062532433664 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.682140827178955, loss=1.745823860168457
I0212 16:22:20.532907 140218947737408 spec.py:321] Evaluating on the training split.
I0212 16:23:20.793203 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 16:24:15.506475 140218947737408 spec.py:349] Evaluating on the test split.
I0212 16:24:42.787143 140218947737408 submission_runner.py:408] Time since start: 15945.86s, 	Step: 16948, 	{'train/ctc_loss': Array(0.42074928, dtype=float32), 'train/wer': 0.13690937458024652, 'validation/ctc_loss': Array(0.75233626, dtype=float32), 'validation/wer': 0.2137443640962762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46206802, dtype=float32), 'test/wer': 0.14872138606219407, 'test/num_examples': 2472, 'score': 14418.166572093964, 'total_duration': 15945.859293460846, 'accumulated_submission_time': 14418.166572093964, 'accumulated_eval_time': 1526.2666816711426, 'accumulated_logging_time': 0.6094729900360107}
I0212 16:24:42.830253 140062532433664 logging_writer.py:48] [16948] accumulated_eval_time=1526.266682, accumulated_logging_time=0.609473, accumulated_submission_time=14418.166572, global_step=16948, preemption_count=0, score=14418.166572, test/ctc_loss=0.46206802129745483, test/num_examples=2472, test/wer=0.148721, total_duration=15945.859293, train/ctc_loss=0.4207492768764496, train/wer=0.136909, validation/ctc_loss=0.7523362636566162, validation/num_examples=5348, validation/wer=0.213744
I0212 16:25:22.590339 140062524040960 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.859753131866455, loss=1.6409502029418945
I0212 16:26:37.417222 140062532433664 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.7847076654434204, loss=1.6500641107559204
I0212 16:28:01.885931 140062524040960 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.9909204244613647, loss=1.684577226638794
I0212 16:29:32.619227 140062532433664 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.291795253753662, loss=1.7222108840942383
I0212 16:31:02.100679 140062524040960 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.902858018875122, loss=1.722293496131897
I0212 16:32:33.793566 140062532433664 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.331965208053589, loss=1.7102234363555908
I0212 16:33:55.355763 140063075153664 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.264520645141602, loss=1.6788514852523804
I0212 16:35:15.320255 140063066760960 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.315838098526001, loss=1.692755937576294
I0212 16:36:36.686950 140063075153664 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.7232327461242676, loss=1.6831632852554321
I0212 16:38:01.052887 140063066760960 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.1863853931427, loss=1.7206754684448242
I0212 16:39:32.554522 140063075153664 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.0093178749084473, loss=1.7550791501998901
I0212 16:41:03.422708 140063066760960 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.975095510482788, loss=1.8448002338409424
I0212 16:42:34.992172 140063075153664 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.6647450923919678, loss=1.7788349390029907
I0212 16:44:05.554406 140063066760960 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.1613080501556396, loss=1.7111443281173706
I0212 16:45:34.410508 140063075153664 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.72737717628479, loss=1.7701866626739502
I0212 16:47:03.728809 140063066760960 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.2467081546783447, loss=1.7429008483886719
I0212 16:48:27.298212 140063075153664 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.0474627017974854, loss=1.6983312368392944
I0212 16:48:43.461609 140218947737408 spec.py:321] Evaluating on the training split.
I0212 16:49:41.152345 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 16:50:34.479135 140218947737408 spec.py:349] Evaluating on the test split.
I0212 16:51:01.276966 140218947737408 submission_runner.py:408] Time since start: 17524.35s, 	Step: 18622, 	{'train/ctc_loss': Array(0.41944084, dtype=float32), 'train/wer': 0.13867461707128548, 'validation/ctc_loss': Array(0.7515029, dtype=float32), 'validation/wer': 0.21325197679021404, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.458045, dtype=float32), 'test/wer': 0.14557309121930412, 'test/num_examples': 2472, 'score': 15858.704028606415, 'total_duration': 17524.35138106346, 'accumulated_submission_time': 15858.704028606415, 'accumulated_eval_time': 1664.0769486427307, 'accumulated_logging_time': 0.6706218719482422}
I0212 16:51:01.315992 140062645073664 logging_writer.py:48] [18622] accumulated_eval_time=1664.076949, accumulated_logging_time=0.670622, accumulated_submission_time=15858.704029, global_step=18622, preemption_count=0, score=15858.704029, test/ctc_loss=0.45804500579833984, test/num_examples=2472, test/wer=0.145573, total_duration=17524.351381, train/ctc_loss=0.4194408357143402, train/wer=0.138675, validation/ctc_loss=0.7515028715133667, validation/num_examples=5348, validation/wer=0.213252
I0212 16:52:00.822109 140062636680960 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.621798276901245, loss=1.7134703397750854
I0212 16:53:16.165480 140062645073664 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.7596323490142822, loss=1.6711714267730713
I0212 16:54:31.700683 140062636680960 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.367072820663452, loss=1.6936856508255005
I0212 16:55:57.999401 140062645073664 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.0139145851135254, loss=1.688962697982788
I0212 16:57:28.842628 140062636680960 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.5081844329833984, loss=1.7013142108917236
I0212 16:58:58.724169 140062645073664 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.762791633605957, loss=1.7470372915267944
I0212 17:00:27.836668 140062636680960 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.808156967163086, loss=1.6995892524719238
I0212 17:01:58.328902 140062645073664 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.947392463684082, loss=1.6901183128356934
I0212 17:03:26.042271 140062636680960 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.1359705924987793, loss=1.6885020732879639
I0212 17:04:55.808709 140062645073664 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.086726665496826, loss=1.7037839889526367
I0212 17:06:13.639248 140062636680960 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.7978649139404297, loss=1.7224310636520386
I0212 17:07:31.679269 140062645073664 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6558167934417725, loss=1.651530385017395
I0212 17:08:51.923060 140062636680960 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.8963373899459839, loss=1.6313271522521973
I0212 17:10:19.160983 140062645073664 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.087204694747925, loss=1.674186110496521
I0212 17:11:48.941542 140062636680960 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.7535407543182373, loss=1.7033820152282715
I0212 17:13:19.101850 140062645073664 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.6909451484680176, loss=1.6338543891906738
I0212 17:14:51.063467 140062636680960 logging_writer.py:48] [20300] global_step=20300, grad_norm=5.098876953125, loss=1.640363097190857
I0212 17:15:01.402529 140218947737408 spec.py:321] Evaluating on the training split.
I0212 17:15:57.536359 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 17:16:50.331286 140218947737408 spec.py:349] Evaluating on the test split.
I0212 17:17:19.355898 140218947737408 submission_runner.py:408] Time since start: 19102.43s, 	Step: 20312, 	{'train/ctc_loss': Array(0.37821147, dtype=float32), 'train/wer': 0.12641768516383303, 'validation/ctc_loss': Array(0.7153456, dtype=float32), 'validation/wer': 0.20442762389333538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4342024, dtype=float32), 'test/wer': 0.13826092255194686, 'test/num_examples': 2472, 'score': 17298.691420793533, 'total_duration': 19102.427579641342, 'accumulated_submission_time': 17298.691420793533, 'accumulated_eval_time': 1802.022501707077, 'accumulated_logging_time': 0.7339036464691162}
I0212 17:17:19.400234 140062645073664 logging_writer.py:48] [20312] accumulated_eval_time=1802.022502, accumulated_logging_time=0.733904, accumulated_submission_time=17298.691421, global_step=20312, preemption_count=0, score=17298.691421, test/ctc_loss=0.4342024028301239, test/num_examples=2472, test/wer=0.138261, total_duration=19102.427580, train/ctc_loss=0.37821146845817566, train/wer=0.126418, validation/ctc_loss=0.7153456211090088, validation/num_examples=5348, validation/wer=0.204428
I0212 17:18:26.479107 140062636680960 logging_writer.py:48] [20400] global_step=20400, grad_norm=7.290337085723877, loss=1.7055498361587524
I0212 17:19:41.352053 140062645073664 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.6134254932403564, loss=1.6812785863876343
I0212 17:21:11.827859 140062645073664 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.625906467437744, loss=1.660837173461914
I0212 17:22:29.601063 140062636680960 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.207573652267456, loss=1.7029340267181396
I0212 17:23:50.431172 140062645073664 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.508246660232544, loss=1.6683164834976196
I0212 17:25:12.704307 140062636680960 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.104189157485962, loss=1.7460182905197144
I0212 17:26:41.099797 140062645073664 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.1619455814361572, loss=1.6237508058547974
I0212 17:28:08.086875 140062636680960 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.6147165298461914, loss=1.6358530521392822
I0212 17:29:36.937092 140062645073664 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.373502492904663, loss=1.6452263593673706
I0212 17:31:09.680096 140062636680960 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.9886012077331543, loss=1.6913418769836426
I0212 17:32:43.148022 140062645073664 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.097799777984619, loss=1.6304075717926025
I0212 17:34:15.136065 140062636680960 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.5858335494995117, loss=1.691409707069397
I0212 17:35:42.609955 140062645073664 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.44877552986145, loss=1.653933048248291
I0212 17:37:04.984795 140062645073664 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.2446813583374023, loss=1.6288487911224365
I0212 17:38:21.962199 140062636680960 logging_writer.py:48] [21800] global_step=21800, grad_norm=5.054332733154297, loss=1.714192271232605
I0212 17:39:43.714488 140062645073664 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.336348295211792, loss=1.6000115871429443
I0212 17:41:07.201438 140062636680960 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.081191062927246, loss=1.6411935091018677
I0212 17:41:19.784124 140218947737408 spec.py:321] Evaluating on the training split.
I0212 17:42:15.145547 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 17:43:09.193058 140218947737408 spec.py:349] Evaluating on the test split.
I0212 17:43:35.510639 140218947737408 submission_runner.py:408] Time since start: 20678.58s, 	Step: 22015, 	{'train/ctc_loss': Array(0.38218117, dtype=float32), 'train/wer': 0.12722010191680347, 'validation/ctc_loss': Array(0.69446677, dtype=float32), 'validation/wer': 0.19737007250644448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41897473, dtype=float32), 'test/wer': 0.13385330977190096, 'test/num_examples': 2472, 'score': 18738.981009721756, 'total_duration': 20678.58448934555, 'accumulated_submission_time': 18738.981009721756, 'accumulated_eval_time': 1937.7433910369873, 'accumulated_logging_time': 0.7955629825592041}
I0212 17:43:35.550931 140063075153664 logging_writer.py:48] [22015] accumulated_eval_time=1937.743391, accumulated_logging_time=0.795563, accumulated_submission_time=18738.981010, global_step=22015, preemption_count=0, score=18738.981010, test/ctc_loss=0.41897472739219666, test/num_examples=2472, test/wer=0.133853, total_duration=20678.584489, train/ctc_loss=0.38218116760253906, train/wer=0.127220, validation/ctc_loss=0.694466769695282, validation/num_examples=5348, validation/wer=0.197370
I0212 17:44:40.222959 140063066760960 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.4874515533447266, loss=1.653382420539856
I0212 17:45:55.123682 140063075153664 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.1513636112213135, loss=1.7041137218475342
I0212 17:47:26.289992 140063066760960 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.124624490737915, loss=1.6204642057418823
I0212 17:48:58.471385 140063075153664 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.0804941654205322, loss=1.60145103931427
I0212 17:50:26.847453 140063066760960 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.361971378326416, loss=1.6760914325714111
I0212 17:51:55.459005 140063075153664 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.0688042640686035, loss=1.6343708038330078
I0212 17:53:25.970289 140063075153664 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.1243107318878174, loss=1.6105329990386963
I0212 17:54:45.436757 140063066760960 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.199070930480957, loss=1.6383864879608154
I0212 17:56:04.532040 140063075153664 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.3798105716705322, loss=1.6039780378341675
I0212 17:57:26.481597 140063066760960 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.8751509189605713, loss=1.5740947723388672
I0212 17:58:50.649166 140063075153664 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.9980969429016113, loss=1.6920182704925537
I0212 18:00:20.837405 140063066760960 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.193021297454834, loss=1.5966421365737915
I0212 18:01:54.148284 140063075153664 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.4366824626922607, loss=1.5630035400390625
I0212 18:03:24.117023 140063066760960 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.412787437438965, loss=1.5405921936035156
I0212 18:04:54.857220 140063075153664 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.8406189680099487, loss=1.577896237373352
I0212 18:06:20.781867 140063066760960 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.101005792617798, loss=1.571284532546997
I0212 18:07:35.661929 140218947737408 spec.py:321] Evaluating on the training split.
I0212 18:08:31.888599 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 18:09:24.885073 140218947737408 spec.py:349] Evaluating on the test split.
I0212 18:09:51.907458 140218947737408 submission_runner.py:408] Time since start: 22254.98s, 	Step: 23682, 	{'train/ctc_loss': Array(0.35579237, dtype=float32), 'train/wer': 0.12106769858550498, 'validation/ctc_loss': Array(0.69894844, dtype=float32), 'validation/wer': 0.19710939687382334, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40583488, dtype=float32), 'test/wer': 0.13182215180874618, 'test/num_examples': 2472, 'score': 20178.99764108658, 'total_duration': 22254.981443166733, 'accumulated_submission_time': 20178.99764108658, 'accumulated_eval_time': 2073.9834084510803, 'accumulated_logging_time': 0.8556327819824219}
I0212 18:09:51.940925 140062783313664 logging_writer.py:48] [23682] accumulated_eval_time=2073.983408, accumulated_logging_time=0.855633, accumulated_submission_time=20178.997641, global_step=23682, preemption_count=0, score=20178.997641, test/ctc_loss=0.4058348834514618, test/num_examples=2472, test/wer=0.131822, total_duration=22254.981443, train/ctc_loss=0.355792373418808, train/wer=0.121068, validation/ctc_loss=0.6989484429359436, validation/num_examples=5348, validation/wer=0.197109
I0212 18:10:09.714632 140062455633664 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.077993154525757, loss=1.5268558263778687
I0212 18:11:30.392466 140062407546624 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.52995765209198, loss=1.5389426946640015
I0212 18:12:48.918986 140062455633664 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.41599178314209, loss=1.5366259813308716
I0212 18:14:09.576891 140062407546624 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.196763277053833, loss=1.6713199615478516
I0212 18:15:36.201401 140062455633664 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.4086194038391113, loss=1.6443527936935425
I0212 18:17:06.748349 140062407546624 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.8917248249053955, loss=1.569823980331421
I0212 18:18:38.379771 140062455633664 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.2544426918029785, loss=1.6301623582839966
I0212 18:20:09.920049 140062407546624 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.0433268547058105, loss=1.5305063724517822
I0212 18:21:40.219094 140062455633664 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.227259874343872, loss=1.6086617708206177
I0212 18:23:11.840591 140062407546624 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.161775827407837, loss=1.5644956827163696
I0212 18:24:42.472827 140062455633664 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.3001527786254883, loss=1.5969657897949219
I0212 18:26:04.324328 140062783313664 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.854851007461548, loss=1.5756200551986694
I0212 18:27:21.354770 140062774920960 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.188765048980713, loss=1.5607198476791382
I0212 18:28:40.424779 140062783313664 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.2209854125976562, loss=1.600305199623108
I0212 18:30:03.926351 140062774920960 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.460204601287842, loss=1.5800676345825195
I0212 18:31:32.370855 140062783313664 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.250077962875366, loss=1.5271399021148682
I0212 18:33:04.190355 140062774920960 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.4497053623199463, loss=1.5877501964569092
I0212 18:33:52.276696 140218947737408 spec.py:321] Evaluating on the training split.
I0212 18:34:47.533816 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 18:35:40.555473 140218947737408 spec.py:349] Evaluating on the test split.
I0212 18:36:07.552163 140218947737408 submission_runner.py:408] Time since start: 23830.63s, 	Step: 25354, 	{'train/ctc_loss': Array(0.3752873, dtype=float32), 'train/wer': 0.12097418804069449, 'validation/ctc_loss': Array(0.635908, dtype=float32), 'validation/wer': 0.18156540544715527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38012487, dtype=float32), 'test/wer': 0.12128044197997279, 'test/num_examples': 2472, 'score': 21619.240039110184, 'total_duration': 23830.625638246536, 'accumulated_submission_time': 21619.240039110184, 'accumulated_eval_time': 2209.2528836727142, 'accumulated_logging_time': 0.9065461158752441}
I0212 18:36:07.587763 140062491473664 logging_writer.py:48] [25354] accumulated_eval_time=2209.252884, accumulated_logging_time=0.906546, accumulated_submission_time=21619.240039, global_step=25354, preemption_count=0, score=21619.240039, test/ctc_loss=0.38012486696243286, test/num_examples=2472, test/wer=0.121280, total_duration=23830.625638, train/ctc_loss=0.3752872943878174, train/wer=0.120974, validation/ctc_loss=0.6359080076217651, validation/num_examples=5348, validation/wer=0.181565
I0212 18:36:42.911761 140062483080960 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.2493784427642822, loss=1.5868879556655884
I0212 18:37:58.228374 140062491473664 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.6280736923217773, loss=1.5435229539871216
I0212 18:39:19.691928 140062483080960 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.379195213317871, loss=1.4893139600753784
I0212 18:40:50.341577 140062491473664 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.5259876251220703, loss=1.565574288368225
I0212 18:42:17.324063 140062491473664 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.7513148784637451, loss=1.541692852973938
I0212 18:43:35.858255 140062483080960 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.4915294647216797, loss=1.5562725067138672
I0212 18:44:56.472693 140062491473664 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.5399389266967773, loss=1.5513960123062134
I0212 18:46:18.383618 140062483080960 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.197734832763672, loss=1.5344574451446533
I0212 18:47:43.705608 140062491473664 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.8235132694244385, loss=1.512681007385254
I0212 18:49:13.663600 140062483080960 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.8021786212921143, loss=1.5530623197555542
I0212 18:50:44.454089 140062491473664 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.9628384113311768, loss=1.5185297727584839
I0212 18:52:13.583872 140062483080960 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.8101431131362915, loss=1.5755363702774048
I0212 18:53:43.284078 140062491473664 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.24078369140625, loss=1.5939830541610718
I0212 18:55:12.831476 140062483080960 logging_writer.py:48] [26700] global_step=26700, grad_norm=5.270584583282471, loss=1.5891882181167603
I0212 18:56:43.765511 140062491473664 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.8981127738952637, loss=1.5452536344528198
I0212 18:58:01.709982 140062483080960 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.085317611694336, loss=1.562096118927002
I0212 18:59:23.392524 140062491473664 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.9911623001098633, loss=1.5389680862426758
I0212 19:00:07.894474 140218947737408 spec.py:321] Evaluating on the training split.
I0212 19:01:04.432392 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 19:01:57.222911 140218947737408 spec.py:349] Evaluating on the test split.
I0212 19:02:25.132972 140218947737408 submission_runner.py:408] Time since start: 25408.21s, 	Step: 27057, 	{'train/ctc_loss': Array(0.32275435, dtype=float32), 'train/wer': 0.10982695565441794, 'validation/ctc_loss': Array(0.6262908, dtype=float32), 'validation/wer': 0.1797986039371675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36712766, dtype=float32), 'test/wer': 0.11859931346860846, 'test/num_examples': 2472, 'score': 23059.453444242477, 'total_duration': 25408.206853866577, 'accumulated_submission_time': 23059.453444242477, 'accumulated_eval_time': 2346.4857637882233, 'accumulated_logging_time': 0.9582936763763428}
I0212 19:02:25.174343 140062860113664 logging_writer.py:48] [27057] accumulated_eval_time=2346.485764, accumulated_logging_time=0.958294, accumulated_submission_time=23059.453444, global_step=27057, preemption_count=0, score=23059.453444, test/ctc_loss=0.3671276569366455, test/num_examples=2472, test/wer=0.118599, total_duration=25408.206854, train/ctc_loss=0.3227543532848358, train/wer=0.109827, validation/ctc_loss=0.6262907981872559, validation/num_examples=5348, validation/wer=0.179799
I0212 19:02:58.138831 140062851720960 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.0404348373413086, loss=1.5387879610061646
I0212 19:04:13.116418 140062860113664 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.397289514541626, loss=1.5682088136672974
I0212 19:05:32.272656 140062851720960 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.797088146209717, loss=1.5857118368148804
I0212 19:07:02.691086 140062860113664 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.4265010356903076, loss=1.573418140411377
I0212 19:08:33.368657 140062851720960 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.957628846168518, loss=1.515749216079712
I0212 19:10:00.988170 140062860113664 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.069267511367798, loss=1.5734659433364868
I0212 19:11:31.415889 140062851720960 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.257077693939209, loss=1.5680972337722778
I0212 19:13:01.852078 140062860113664 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.3939449787139893, loss=1.5420688390731812
I0212 19:14:24.102422 140062860113664 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.4204602241516113, loss=1.5523720979690552
I0212 19:15:39.583218 140062851720960 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.6195971965789795, loss=1.5290690660476685
I0212 19:16:58.881612 140062860113664 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.8332327604293823, loss=1.5047457218170166
I0212 19:18:21.684573 140062851720960 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.7429999113082886, loss=1.5870052576065063
I0212 19:19:51.202199 140062860113664 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.8257203102111816, loss=1.547961711883545
I0212 19:21:21.423748 140062851720960 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.554783582687378, loss=1.5405019521713257
I0212 19:22:52.045806 140062860113664 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.995633363723755, loss=1.5815993547439575
I0212 19:24:22.427996 140062851720960 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.109907865524292, loss=1.5111976861953735
I0212 19:25:52.557374 140062860113664 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.5100908279418945, loss=1.5244897603988647
I0212 19:26:26.253492 140218947737408 spec.py:321] Evaluating on the training split.
I0212 19:27:22.537212 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 19:28:16.698180 140218947737408 spec.py:349] Evaluating on the test split.
I0212 19:28:44.501188 140218947737408 submission_runner.py:408] Time since start: 26987.57s, 	Step: 28739, 	{'train/ctc_loss': Array(0.2909292, dtype=float32), 'train/wer': 0.09875202618699148, 'validation/ctc_loss': Array(0.6012382, dtype=float32), 'validation/wer': 0.17249968622377554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3531975, dtype=float32), 'test/wer': 0.11508541019235066, 'test/num_examples': 2472, 'score': 24500.440947532654, 'total_duration': 26987.574753046036, 'accumulated_submission_time': 24500.440947532654, 'accumulated_eval_time': 2484.727519273758, 'accumulated_logging_time': 1.015852451324463}
I0212 19:28:44.541161 140062568273664 logging_writer.py:48] [28739] accumulated_eval_time=2484.727519, accumulated_logging_time=1.015852, accumulated_submission_time=24500.440948, global_step=28739, preemption_count=0, score=24500.440948, test/ctc_loss=0.35319748520851135, test/num_examples=2472, test/wer=0.115085, total_duration=26987.574753, train/ctc_loss=0.2909291982650757, train/wer=0.098752, validation/ctc_loss=0.6012381911277771, validation/num_examples=5348, validation/wer=0.172500
I0212 19:29:31.421804 140062559880960 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.480125665664673, loss=1.506630778312683
I0212 19:30:51.641230 140062568273664 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.381265163421631, loss=1.459147572517395
I0212 19:32:11.956126 140062559880960 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.677496910095215, loss=1.4472490549087524
I0212 19:33:32.099534 140062568273664 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.9448704719543457, loss=1.441462755203247
I0212 19:34:57.761573 140062559880960 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.389190435409546, loss=1.4949086904525757
I0212 19:36:27.505744 140062568273664 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.278566598892212, loss=1.4879692792892456
I0212 19:37:53.506927 140062559880960 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.244893789291382, loss=1.428909182548523
I0212 19:39:24.445179 140062568273664 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.9058754444122314, loss=1.5871771574020386
I0212 19:40:57.228668 140062559880960 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.639812707901001, loss=1.4979139566421509
I0212 19:42:25.454051 140062568273664 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.2880172729492188, loss=1.4825245141983032
I0212 19:43:54.146170 140062559880960 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.5556254386901855, loss=1.5485087633132935
I0212 19:45:25.409871 140062568273664 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.228269100189209, loss=1.5199764966964722
I0212 19:46:42.116804 140062559880960 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.5770692825317383, loss=1.4517375230789185
I0212 19:48:00.725839 140062568273664 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.6594903469085693, loss=1.4600340127944946
I0212 19:49:22.071405 140062559880960 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.9548685550689697, loss=1.4818918704986572
I0212 19:50:50.553425 140062568273664 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.2980854511260986, loss=1.4912973642349243
I0212 19:52:19.935233 140062559880960 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.2442786693573, loss=1.4784914255142212
I0212 19:52:45.245523 140218947737408 spec.py:321] Evaluating on the training split.
I0212 19:53:42.840770 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 19:54:36.169564 140218947737408 spec.py:349] Evaluating on the test split.
I0212 19:55:03.861426 140218947737408 submission_runner.py:408] Time since start: 28566.94s, 	Step: 30429, 	{'train/ctc_loss': Array(0.27494448, dtype=float32), 'train/wer': 0.09241236346240732, 'validation/ctc_loss': Array(0.58237994, dtype=float32), 'validation/wer': 0.16846404124467787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33602273, dtype=float32), 'test/wer': 0.1067982857026791, 'test/num_examples': 2472, 'score': 25941.052482128143, 'total_duration': 28566.935750246048, 'accumulated_submission_time': 25941.052482128143, 'accumulated_eval_time': 2623.338259458542, 'accumulated_logging_time': 1.0719337463378906}
I0212 19:55:03.902037 140062568273664 logging_writer.py:48] [30429] accumulated_eval_time=2623.338259, accumulated_logging_time=1.071934, accumulated_submission_time=25941.052482, global_step=30429, preemption_count=0, score=25941.052482, test/ctc_loss=0.3360227346420288, test/num_examples=2472, test/wer=0.106798, total_duration=28566.935750, train/ctc_loss=0.2749444842338562, train/wer=0.092412, validation/ctc_loss=0.582379937171936, validation/num_examples=5348, validation/wer=0.168464
I0212 19:55:57.871105 140062559880960 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.236567258834839, loss=1.4523531198501587
I0212 19:57:13.001766 140062568273664 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.1447601318359375, loss=1.4567970037460327
I0212 19:58:40.438060 140062559880960 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.6983795166015625, loss=1.4088891744613647
I0212 20:00:09.609945 140062568273664 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.9785447120666504, loss=1.450005292892456
I0212 20:01:43.806809 140062568273664 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.6248233318328857, loss=1.4468364715576172
I0212 20:03:02.173809 140062559880960 logging_writer.py:48] [31000] global_step=31000, grad_norm=4.072220802307129, loss=1.3973033428192139
I0212 20:04:19.827124 140062568273664 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.1316263675689697, loss=1.395068645477295
I0212 20:05:39.868743 140062559880960 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.8320255279541016, loss=1.491640567779541
I0212 20:07:03.427511 140062568273664 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.927760124206543, loss=1.4992411136627197
I0212 20:08:30.994837 140062559880960 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.5428733825683594, loss=1.44172203540802
I0212 20:10:01.912202 140062568273664 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.7442431449890137, loss=1.4617574214935303
I0212 20:11:29.962988 140062559880960 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.9712092876434326, loss=1.4991620779037476
I0212 20:12:59.229443 140062568273664 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.931349039077759, loss=1.4691665172576904
I0212 20:14:30.031821 140062559880960 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7820459604263306, loss=1.5105525255203247
I0212 20:16:02.429495 140062568273664 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.3791069984436035, loss=1.4009844064712524
I0212 20:17:26.481798 140062568273664 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.323225259780884, loss=1.4504501819610596
I0212 20:18:47.033538 140062559880960 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.731692314147949, loss=1.4429258108139038
I0212 20:19:04.277071 140218947737408 spec.py:321] Evaluating on the training split.
I0212 20:20:00.035635 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 20:20:54.487318 140218947737408 spec.py:349] Evaluating on the test split.
I0212 20:21:21.688872 140218947737408 submission_runner.py:408] Time since start: 30144.76s, 	Step: 32123, 	{'train/ctc_loss': Array(0.2601245, dtype=float32), 'train/wer': 0.09085917312661498, 'validation/ctc_loss': Array(0.5635271, dtype=float32), 'validation/wer': 0.16050860712320303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31771612, dtype=float32), 'test/wer': 0.10330469400605286, 'test/num_examples': 2472, 'score': 27381.335985183716, 'total_duration': 30144.762478351593, 'accumulated_submission_time': 27381.335985183716, 'accumulated_eval_time': 2760.7441635131836, 'accumulated_logging_time': 1.128051519393921}
I0212 20:21:21.726181 140062568273664 logging_writer.py:48] [32123] accumulated_eval_time=2760.744164, accumulated_logging_time=1.128052, accumulated_submission_time=27381.335985, global_step=32123, preemption_count=0, score=27381.335985, test/ctc_loss=0.317716121673584, test/num_examples=2472, test/wer=0.103305, total_duration=30144.762478, train/ctc_loss=0.2601245045661926, train/wer=0.090859, validation/ctc_loss=0.5635271072387695, validation/num_examples=5348, validation/wer=0.160509
I0212 20:22:20.620596 140062559880960 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.3420910835266113, loss=1.4297441244125366
I0212 20:23:35.712027 140062568273664 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.551750421524048, loss=1.4344054460525513
I0212 20:24:52.790447 140062559880960 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.8314573764801025, loss=1.4636794328689575
I0212 20:26:23.164124 140062568273664 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.664888381958008, loss=1.4042160511016846
I0212 20:27:53.809100 140062559880960 logging_writer.py:48] [32600] global_step=32600, grad_norm=4.462697505950928, loss=1.3904401063919067
I0212 20:29:24.523815 140062568273664 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.5264573097229004, loss=1.4938524961471558
I0212 20:30:53.444996 140062559880960 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.3558104038238525, loss=1.457536220550537
I0212 20:32:24.743633 140062568273664 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.7017300128936768, loss=1.4111435413360596
I0212 20:33:55.253360 140062568273664 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.9615967273712158, loss=1.3394873142242432
I0212 20:35:11.828024 140062559880960 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.991293430328369, loss=1.3793050050735474
I0212 20:36:31.002436 140062568273664 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.0992863178253174, loss=1.4362907409667969
I0212 20:37:55.656481 140062559880960 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.514253616333008, loss=1.376883625984192
I0212 20:39:21.427415 140062568273664 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.5052459239959717, loss=1.4238667488098145
I0212 20:40:47.851947 140062559880960 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.715693235397339, loss=1.3854321241378784
I0212 20:42:21.304042 140062568273664 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.435697078704834, loss=1.4061789512634277
I0212 20:43:53.504590 140062559880960 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.4975385665893555, loss=1.4235721826553345
I0212 20:45:21.860904 140218947737408 spec.py:321] Evaluating on the training split.
I0212 20:46:18.177188 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 20:47:11.206118 140218947737408 spec.py:349] Evaluating on the test split.
I0212 20:47:38.432847 140218947737408 submission_runner.py:408] Time since start: 31721.51s, 	Step: 33800, 	{'train/ctc_loss': Array(0.26487103, dtype=float32), 'train/wer': 0.08863111246782912, 'validation/ctc_loss': Array(0.5352882, dtype=float32), 'validation/wer': 0.15320968940981106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.306206, dtype=float32), 'test/wer': 0.09812524120000812, 'test/num_examples': 2472, 'score': 28821.37881207466, 'total_duration': 31721.506538152695, 'accumulated_submission_time': 28821.37881207466, 'accumulated_eval_time': 2897.3103017807007, 'accumulated_logging_time': 1.1814250946044922}
I0212 20:47:38.470224 140062645073664 logging_writer.py:48] [33800] accumulated_eval_time=2897.310302, accumulated_logging_time=1.181425, accumulated_submission_time=28821.378812, global_step=33800, preemption_count=0, score=28821.378812, test/ctc_loss=0.30620598793029785, test/num_examples=2472, test/wer=0.098125, total_duration=31721.506538, train/ctc_loss=0.2648710310459137, train/wer=0.088631, validation/ctc_loss=0.5352882146835327, validation/num_examples=5348, validation/wer=0.153210
I0212 20:47:39.340013 140062636680960 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.4305684566497803, loss=1.4425073862075806
I0212 20:48:54.333660 140062645073664 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.320402145385742, loss=1.3690441846847534
I0212 20:50:13.580819 140062645073664 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7690889835357666, loss=1.3802570104599
I0212 20:51:32.292999 140062636680960 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.028743267059326, loss=1.3733879327774048
I0212 20:52:53.393645 140062645073664 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.7895545959472656, loss=1.3629146814346313
I0212 20:54:17.094591 140062636680960 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.593036651611328, loss=1.3825613260269165
I0212 20:55:42.748479 140062645073664 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.9940539598464966, loss=1.4276241064071655
I0212 20:57:12.218810 140062636680960 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.233292818069458, loss=1.3698656558990479
I0212 20:58:46.476263 140062645073664 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.913724422454834, loss=1.4100655317306519
I0212 21:00:14.727085 140062636680960 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.9542567729949951, loss=1.3591445684432983
I0212 21:01:44.123137 140062645073664 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.781310796737671, loss=1.3661773204803467
I0212 21:03:16.464708 140062636680960 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.518691062927246, loss=1.4032869338989258
I0212 21:04:48.095552 140062645073664 logging_writer.py:48] [35000] global_step=35000, grad_norm=5.139729022979736, loss=1.3805482387542725
I0212 21:06:12.299015 140062645073664 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.2664988040924072, loss=1.3502671718597412
I0212 21:07:33.936850 140062636680960 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.25801420211792, loss=1.4447485208511353
I0212 21:08:51.641928 140062645073664 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.4775172472000122, loss=1.3492991924285889
I0212 21:10:16.174107 140062636680960 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.087332248687744, loss=1.353578805923462
I0212 21:11:38.776059 140218947737408 spec.py:321] Evaluating on the training split.
I0212 21:12:35.073357 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 21:13:27.789005 140218947737408 spec.py:349] Evaluating on the test split.
I0212 21:13:54.695732 140218947737408 submission_runner.py:408] Time since start: 33297.77s, 	Step: 35496, 	{'train/ctc_loss': Array(0.24313995, dtype=float32), 'train/wer': 0.08220079720714221, 'validation/ctc_loss': Array(0.5057727, dtype=float32), 'validation/wer': 0.14514805410467574, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28559756, dtype=float32), 'test/wer': 0.09182865151422827, 'test/num_examples': 2472, 'score': 30261.592493772507, 'total_duration': 33297.76939201355, 'accumulated_submission_time': 30261.592493772507, 'accumulated_eval_time': 3033.224186897278, 'accumulated_logging_time': 1.2355103492736816}
I0212 21:13:54.736152 140062645073664 logging_writer.py:48] [35496] accumulated_eval_time=3033.224187, accumulated_logging_time=1.235510, accumulated_submission_time=30261.592494, global_step=35496, preemption_count=0, score=30261.592494, test/ctc_loss=0.285597562789917, test/num_examples=2472, test/wer=0.091829, total_duration=33297.769392, train/ctc_loss=0.24313995242118835, train/wer=0.082201, validation/ctc_loss=0.5057727098464966, validation/num_examples=5348, validation/wer=0.145148
I0212 21:13:58.621799 140062636680960 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5918313264846802, loss=1.3787267208099365
I0212 21:15:13.867747 140062645073664 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.48684823513031, loss=1.3237119913101196
I0212 21:16:28.844608 140062636680960 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.528686761856079, loss=1.3167784214019775
I0212 21:17:55.468685 140062645073664 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.6101272106170654, loss=1.3200281858444214
I0212 21:19:25.058856 140062636680960 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.9475386142730713, loss=1.368876338005066
I0212 21:20:53.540048 140062645073664 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.600283622741699, loss=1.3684289455413818
I0212 21:22:20.389411 140062645073664 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.6401959657669067, loss=1.3293006420135498
I0212 21:23:38.741125 140062636680960 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.6255605220794678, loss=1.2838226556777954
I0212 21:24:58.885528 140062645073664 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.946795701980591, loss=1.284827709197998
I0212 21:26:20.689862 140062636680960 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.9000855684280396, loss=1.3367129564285278
I0212 21:27:45.962099 140062645073664 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.831459879875183, loss=1.2524546384811401
I0212 21:29:15.171226 140062636680960 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.2613229751586914, loss=1.3111064434051514
I0212 21:30:45.812560 140062645073664 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.6385968923568726, loss=1.2845261096954346
I0212 21:32:16.973104 140062636680960 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.285848617553711, loss=1.2863552570343018
I0212 21:33:44.552217 140062645073664 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.281198501586914, loss=1.3067322969436646
I0212 21:35:14.690932 140062636680960 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.4535012245178223, loss=1.2880383729934692
I0212 21:36:44.425364 140062645073664 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.9771273136138916, loss=1.2848873138427734
I0212 21:37:55.157546 140218947737408 spec.py:321] Evaluating on the training split.
I0212 21:38:52.893766 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 21:39:45.360873 140218947737408 spec.py:349] Evaluating on the test split.
I0212 21:40:12.987493 140218947737408 submission_runner.py:408] Time since start: 34876.06s, 	Step: 37194, 	{'train/ctc_loss': Array(0.21898215, dtype=float32), 'train/wer': 0.07491937741738129, 'validation/ctc_loss': Array(0.48544854, dtype=float32), 'validation/wer': 0.13985730422777257, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2720657, dtype=float32), 'test/wer': 0.08705543030081449, 'test/num_examples': 2472, 'score': 31701.920878887177, 'total_duration': 34876.05977129936, 'accumulated_submission_time': 31701.920878887177, 'accumulated_eval_time': 3171.0469262599945, 'accumulated_logging_time': 1.2923777103424072}
I0212 21:40:13.023918 140062645073664 logging_writer.py:48] [37194] accumulated_eval_time=3171.046926, accumulated_logging_time=1.292378, accumulated_submission_time=31701.920879, global_step=37194, preemption_count=0, score=31701.920879, test/ctc_loss=0.2720656991004944, test/num_examples=2472, test/wer=0.087055, total_duration=34876.059771, train/ctc_loss=0.21898214519023895, train/wer=0.074919, validation/ctc_loss=0.48544853925704956, validation/num_examples=5348, validation/wer=0.139857
I0212 21:40:18.426124 140062636680960 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.547441840171814, loss=1.264140248298645
I0212 21:41:33.689428 140062645073664 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.761056661605835, loss=1.302808403968811
I0212 21:42:48.567440 140062636680960 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.1564860343933105, loss=1.2768223285675049
I0212 21:44:05.586114 140062645073664 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.6623255014419556, loss=1.2683550119400024
I0212 21:45:33.437482 140062636680960 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.847097396850586, loss=1.2821685075759888
I0212 21:47:03.143021 140062645073664 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.0934090614318848, loss=1.23506760597229
I0212 21:48:35.098631 140062636680960 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.81567120552063, loss=1.2808912992477417
I0212 21:50:07.237955 140062645073664 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.207944393157959, loss=1.3093799352645874
I0212 21:51:36.357556 140062636680960 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.5299839973449707, loss=1.3005139827728271
I0212 21:53:05.447914 140062645073664 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.4685468673706055, loss=1.3076704740524292
I0212 21:54:26.638375 140062645073664 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.712459087371826, loss=1.3504345417022705
I0212 21:55:46.176782 140062636680960 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.1216092109680176, loss=1.373801589012146
I0212 21:57:06.967411 140062645073664 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.4373496770858765, loss=1.2986167669296265
I0212 21:58:28.434671 140062636680960 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.1068882942199707, loss=1.2422314882278442
I0212 21:59:57.313379 140062645073664 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.3449621200561523, loss=1.2505593299865723
I0212 22:01:28.775600 140062636680960 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.4221842288970947, loss=1.2485482692718506
I0212 22:02:55.038371 140062645073664 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.2067017555236816, loss=1.2706034183502197
I0212 22:04:13.155816 140218947737408 spec.py:321] Evaluating on the training split.
I0212 22:05:10.462575 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 22:06:04.912781 140218947737408 spec.py:349] Evaluating on the test split.
I0212 22:06:31.308824 140218947737408 submission_runner.py:408] Time since start: 36454.38s, 	Step: 38888, 	{'train/ctc_loss': Array(0.2012175, dtype=float32), 'train/wer': 0.06860388103445579, 'validation/ctc_loss': Array(0.46407133, dtype=float32), 'validation/wer': 0.13293491798372226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2549981, dtype=float32), 'test/wer': 0.08283062173745252, 'test/num_examples': 2472, 'score': 33141.95910906792, 'total_duration': 36454.38322663307, 'accumulated_submission_time': 33141.95910906792, 'accumulated_eval_time': 3309.1948528289795, 'accumulated_logging_time': 1.345574140548706}
I0212 22:06:31.349071 140062645073664 logging_writer.py:48] [38888] accumulated_eval_time=3309.194853, accumulated_logging_time=1.345574, accumulated_submission_time=33141.959109, global_step=38888, preemption_count=0, score=33141.959109, test/ctc_loss=0.2549980878829956, test/num_examples=2472, test/wer=0.082831, total_duration=36454.383227, train/ctc_loss=0.20121750235557556, train/wer=0.068604, validation/ctc_loss=0.4640713334083557, validation/num_examples=5348, validation/wer=0.132935
I0212 22:06:41.336050 140062636680960 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.2299797534942627, loss=1.251278281211853
I0212 22:07:56.273731 140062645073664 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.950097680091858, loss=1.2254575490951538
I0212 22:09:14.482116 140062636680960 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.5072635412216187, loss=1.284814476966858
I0212 22:10:39.938239 140062645073664 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.4226441383361816, loss=1.2558730840682983
I0212 22:11:58.235635 140062636680960 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.3435258865356445, loss=1.2596250772476196
I0212 22:13:19.826026 140062645073664 logging_writer.py:48] [39400] global_step=39400, grad_norm=4.247791767120361, loss=1.3209632635116577
I0212 22:14:43.687179 140062636680960 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.4742777347564697, loss=1.2690238952636719
I0212 22:16:11.345504 140062645073664 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.834733724594116, loss=1.211102843284607
I0212 22:17:41.446999 140062636680960 logging_writer.py:48] [39700] global_step=39700, grad_norm=4.252398490905762, loss=1.259332537651062
I0212 22:19:09.099216 140062645073664 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.5614144802093506, loss=1.2597848176956177
I0212 22:20:38.298486 140062636680960 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.268920660018921, loss=1.2708827257156372
I0212 22:22:09.088675 140062645073664 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.0356805324554443, loss=1.3291664123535156
I0212 22:23:40.885458 140062636680960 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.3224692344665527, loss=1.2574491500854492
I0212 22:25:07.855318 140062645073664 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.9208508729934692, loss=1.2680587768554688
I0212 22:26:24.649322 140062636680960 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.3660805225372314, loss=1.2048563957214355
I0212 22:27:46.186056 140062645073664 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.3168349266052246, loss=1.2386068105697632
I0212 22:29:09.886112 140062636680960 logging_writer.py:48] [40500] global_step=40500, grad_norm=4.50802755355835, loss=1.1732733249664307
I0212 22:30:31.612395 140218947737408 spec.py:321] Evaluating on the training split.
I0212 22:31:27.308555 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 22:32:21.653970 140218947737408 spec.py:349] Evaluating on the test split.
I0212 22:32:48.447668 140218947737408 submission_runner.py:408] Time since start: 38031.52s, 	Step: 40597, 	{'train/ctc_loss': Array(0.16660897, dtype=float32), 'train/wer': 0.05763320483709141, 'validation/ctc_loss': Array(0.44376504, dtype=float32), 'validation/wer': 0.12745107504561823, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24118751, dtype=float32), 'test/wer': 0.0770215099628298, 'test/num_examples': 2472, 'score': 34582.12906932831, 'total_duration': 38031.52192878723, 'accumulated_submission_time': 34582.12906932831, 'accumulated_eval_time': 3446.0249009132385, 'accumulated_logging_time': 1.402059555053711}
I0212 22:32:48.489925 140062645073664 logging_writer.py:48] [40597] accumulated_eval_time=3446.024901, accumulated_logging_time=1.402060, accumulated_submission_time=34582.129069, global_step=40597, preemption_count=0, score=34582.129069, test/ctc_loss=0.24118751287460327, test/num_examples=2472, test/wer=0.077022, total_duration=38031.521929, train/ctc_loss=0.16660897433757782, train/wer=0.057633, validation/ctc_loss=0.4437650442123413, validation/num_examples=5348, validation/wer=0.127451
I0212 22:32:51.663911 140062636680960 logging_writer.py:48] [40600] global_step=40600, grad_norm=4.967044353485107, loss=1.2369434833526611
I0212 22:34:06.537308 140062645073664 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.0832149982452393, loss=1.2213451862335205
I0212 22:35:21.502861 140062636680960 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7926558256149292, loss=1.2593261003494263
I0212 22:36:52.530649 140062645073664 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7358640432357788, loss=1.2366799116134644
I0212 22:38:21.740011 140062636680960 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.6829185485839844, loss=1.2604821920394897
I0212 22:39:51.222068 140062645073664 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.8313071727752686, loss=1.1852595806121826
I0212 22:41:26.411338 140062645073664 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.2261993885040283, loss=1.1769763231277466
I0212 22:42:44.675264 140062636680960 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.1215784549713135, loss=1.1714965105056763
I0212 22:44:04.609583 140062645073664 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.2134947776794434, loss=1.2056998014450073
I0212 22:45:27.479804 140062636680960 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.439511299133301, loss=1.2217479944229126
I0212 22:46:55.427659 140062645073664 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.76735520362854, loss=1.1779619455337524
I0212 22:48:24.327307 140062636680960 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.5303142070770264, loss=1.224452018737793
I0212 22:49:53.574337 140062645073664 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.4103590250015259, loss=1.1857550144195557
I0212 22:51:21.823891 140062636680960 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.2150487899780273, loss=1.1696072816848755
I0212 22:52:47.996089 140062645073664 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.3873019218444824, loss=1.1741193532943726
I0212 22:54:20.706923 140062636680960 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.077610492706299, loss=1.2406625747680664
I0212 22:55:52.055323 140062645073664 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.1869821548461914, loss=1.1702295541763306
I0212 22:56:48.776578 140218947737408 spec.py:321] Evaluating on the training split.
I0212 22:57:46.076321 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 22:58:39.763868 140218947737408 spec.py:349] Evaluating on the test split.
I0212 22:59:07.489288 140218947737408 submission_runner.py:408] Time since start: 39610.56s, 	Step: 42267, 	{'train/ctc_loss': Array(0.16422431, dtype=float32), 'train/wer': 0.05539039913369497, 'validation/ctc_loss': Array(0.4237094, dtype=float32), 'validation/wer': 0.12222790774013535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23010056, dtype=float32), 'test/wer': 0.07393414985883452, 'test/num_examples': 2472, 'score': 36022.32275509834, 'total_duration': 39610.562725782394, 'accumulated_submission_time': 36022.32275509834, 'accumulated_eval_time': 3584.731765270233, 'accumulated_logging_time': 1.4607226848602295}
I0212 22:59:07.528927 140063075153664 logging_writer.py:48] [42267] accumulated_eval_time=3584.731765, accumulated_logging_time=1.460723, accumulated_submission_time=36022.322755, global_step=42267, preemption_count=0, score=36022.322755, test/ctc_loss=0.23010055720806122, test/num_examples=2472, test/wer=0.073934, total_duration=39610.562726, train/ctc_loss=0.164224311709404, train/wer=0.055390, validation/ctc_loss=0.4237093925476074, validation/num_examples=5348, validation/wer=0.122228
I0212 22:59:33.137377 140063066760960 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6719017028808594, loss=1.1699138879776
I0212 23:00:48.487979 140063075153664 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.7699222564697266, loss=1.1805105209350586
I0212 23:02:03.617812 140063066760960 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.9244565963745117, loss=1.1513818502426147
I0212 23:03:18.703649 140063075153664 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.440613269805908, loss=1.205075740814209
I0212 23:04:44.655046 140063066760960 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.6554675102233887, loss=1.2025271654129028
I0212 23:06:18.823425 140063075153664 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.605693817138672, loss=1.2566981315612793
I0212 23:07:43.620546 140063066760960 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.1985950469970703, loss=1.1528030633926392
I0212 23:09:14.029161 140063075153664 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.3868134021759033, loss=1.2137850522994995
I0212 23:10:46.945943 140063066760960 logging_writer.py:48] [43100] global_step=43100, grad_norm=4.295318126678467, loss=1.1392734050750732
I0212 23:12:18.746226 140063075153664 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.4747416973114014, loss=1.1744500398635864
I0212 23:13:47.624519 140062532433664 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.653672218322754, loss=1.175148844718933
I0212 23:15:06.824425 140062524040960 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.724787473678589, loss=1.1712074279785156
I0212 23:16:28.951211 140062532433664 logging_writer.py:48] [43500] global_step=43500, grad_norm=6.571683406829834, loss=1.2211405038833618
I0212 23:17:49.053951 140062524040960 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.255737781524658, loss=1.226899266242981
I0212 23:19:17.299864 140062532433664 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.6360268592834473, loss=1.2042385339736938
I0212 23:20:46.612142 140062524040960 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.9502686262130737, loss=1.1207911968231201
I0212 23:22:15.145701 140062532433664 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.2976818084716797, loss=1.1812020540237427
I0212 23:23:08.493220 140218947737408 spec.py:321] Evaluating on the training split.
I0212 23:24:03.818717 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 23:24:58.077340 140218947737408 spec.py:349] Evaluating on the test split.
I0212 23:25:25.732328 140218947737408 submission_runner.py:408] Time since start: 41188.80s, 	Step: 43960, 	{'train/ctc_loss': Array(0.16956198, dtype=float32), 'train/wer': 0.05703789740957835, 'validation/ctc_loss': Array(0.41393122, dtype=float32), 'validation/wer': 0.11902256292420132, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22181489, dtype=float32), 'test/wer': 0.07178112241789043, 'test/num_examples': 2472, 'score': 37463.19466948509, 'total_duration': 41188.80369210243, 'accumulated_submission_time': 37463.19466948509, 'accumulated_eval_time': 3721.962750196457, 'accumulated_logging_time': 1.517216682434082}
I0212 23:25:25.773050 140062532433664 logging_writer.py:48] [43960] accumulated_eval_time=3721.962750, accumulated_logging_time=1.517217, accumulated_submission_time=37463.194669, global_step=43960, preemption_count=0, score=37463.194669, test/ctc_loss=0.22181488573551178, test/num_examples=2472, test/wer=0.071781, total_duration=41188.803692, train/ctc_loss=0.1695619821548462, train/wer=0.057038, validation/ctc_loss=0.4139312207698822, validation/num_examples=5348, validation/wer=0.119023
I0212 23:25:56.836819 140062524040960 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.3847780227661133, loss=1.1597307920455933
I0212 23:27:11.880391 140062532433664 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.8874590396881104, loss=1.1163864135742188
I0212 23:28:32.837114 140062524040960 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.2114064693450928, loss=1.2097418308258057
I0212 23:30:06.230827 140062532433664 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.6158651113510132, loss=1.1732536554336548
I0212 23:31:25.109529 140062524040960 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.6200127601623535, loss=1.1659395694732666
I0212 23:32:47.608427 140062532433664 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.0131006240844727, loss=1.159670352935791
I0212 23:34:13.483238 140062524040960 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.4385366439819336, loss=1.2156896591186523
I0212 23:35:42.324164 140062532433664 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.973863363265991, loss=1.204849123954773
I0212 23:37:12.271513 140062524040960 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.656305193901062, loss=1.1376068592071533
I0212 23:38:42.626980 140062532433664 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.412569284439087, loss=1.1613483428955078
I0212 23:40:14.564611 140062524040960 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.9115748405456543, loss=1.185433030128479
I0212 23:41:44.590393 140062532433664 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.0843217372894287, loss=1.17183256149292
I0212 23:43:14.583082 140062524040960 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.6739661693573, loss=1.2487213611602783
I0212 23:44:41.169820 140062532433664 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.2955760955810547, loss=1.11359703540802
I0212 23:46:05.794083 140063075153664 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.672925353050232, loss=1.1013728380203247
I0212 23:47:25.386906 140063066760960 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.256196975708008, loss=1.1740690469741821
I0212 23:48:43.801807 140063075153664 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.6389074325561523, loss=1.1587494611740112
I0212 23:49:26.142749 140218947737408 spec.py:321] Evaluating on the training split.
I0212 23:50:21.777910 140218947737408 spec.py:333] Evaluating on the validation split.
I0212 23:51:14.344878 140218947737408 spec.py:349] Evaluating on the test split.
I0212 23:51:41.173139 140218947737408 submission_runner.py:408] Time since start: 42764.25s, 	Step: 45652, 	{'train/ctc_loss': Array(0.14950223, dtype=float32), 'train/wer': 0.04942652078491429, 'validation/ctc_loss': Array(0.4087675, dtype=float32), 'validation/wer': 0.1171012869652529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21725857, dtype=float32), 'test/wer': 0.07001401498994576, 'test/num_examples': 2472, 'score': 38903.469370126724, 'total_duration': 42764.24932599068, 'accumulated_submission_time': 38903.469370126724, 'accumulated_eval_time': 3856.989844560623, 'accumulated_logging_time': 1.5770831108093262}
I0212 23:51:41.206871 140062783313664 logging_writer.py:48] [45652] accumulated_eval_time=3856.989845, accumulated_logging_time=1.577083, accumulated_submission_time=38903.469370, global_step=45652, preemption_count=0, score=38903.469370, test/ctc_loss=0.21725857257843018, test/num_examples=2472, test/wer=0.070014, total_duration=42764.249326, train/ctc_loss=0.149502232670784, train/wer=0.049427, validation/ctc_loss=0.4087674915790558, validation/num_examples=5348, validation/wer=0.117101
I0212 23:52:18.167135 140062774920960 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.0311992168426514, loss=1.0757637023925781
I0212 23:53:33.166754 140062783313664 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.4514195919036865, loss=1.1362595558166504
I0212 23:54:53.048974 140062774920960 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.297854423522949, loss=1.1113275289535522
I0212 23:56:21.885686 140062783313664 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.048060894012451, loss=1.1754122972488403
I0212 23:57:52.302867 140062774920960 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.9653621912002563, loss=1.1163756847381592
I0212 23:59:24.420132 140062783313664 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.297558307647705, loss=1.1130465269088745
I0213 00:00:53.343875 140062774920960 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.902396559715271, loss=1.179909348487854
I0213 00:02:20.980991 140062455633664 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.0053505897521973, loss=1.1217800378799438
I0213 00:03:40.095913 140062407546624 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.657594680786133, loss=1.2047202587127686
I0213 00:04:59.081922 140062455633664 logging_writer.py:48] [46600] global_step=46600, grad_norm=4.1253814697265625, loss=1.1193739175796509
I0213 00:06:23.286626 140062407546624 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.5226900577545166, loss=1.1822608709335327
I0213 00:07:51.942038 140062455633664 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.332273244857788, loss=1.1607736349105835
I0213 00:09:24.804892 140062407546624 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.9389585256576538, loss=1.122841477394104
I0213 00:10:56.505892 140062455633664 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.096834182739258, loss=1.1422940492630005
I0213 00:12:28.719487 140062407546624 logging_writer.py:48] [47100] global_step=47100, grad_norm=5.201059341430664, loss=1.132096290588379
I0213 00:13:57.001218 140062455633664 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.6955955028533936, loss=1.1121788024902344
I0213 00:15:27.303103 140062407546624 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.5975241661071777, loss=1.1311759948730469
I0213 00:15:41.552794 140218947737408 spec.py:321] Evaluating on the training split.
I0213 00:16:38.067295 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 00:17:31.424946 140218947737408 spec.py:349] Evaluating on the test split.
I0213 00:17:59.385254 140218947737408 submission_runner.py:408] Time since start: 44342.46s, 	Step: 47317, 	{'train/ctc_loss': Array(0.16066147, dtype=float32), 'train/wer': 0.0549748414936208, 'validation/ctc_loss': Array(0.4061986, dtype=float32), 'validation/wer': 0.1159137646388677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21572423, dtype=float32), 'test/wer': 0.06909999390652612, 'test/num_examples': 2472, 'score': 40343.726199388504, 'total_duration': 44342.45700645447, 'accumulated_submission_time': 40343.726199388504, 'accumulated_eval_time': 3994.8145458698273, 'accumulated_logging_time': 1.6236886978149414}
I0213 00:17:59.430341 140062491473664 logging_writer.py:48] [47317] accumulated_eval_time=3994.814546, accumulated_logging_time=1.623689, accumulated_submission_time=40343.726199, global_step=47317, preemption_count=0, score=40343.726199, test/ctc_loss=0.21572422981262207, test/num_examples=2472, test/wer=0.069100, total_duration=44342.457006, train/ctc_loss=0.1606614738702774, train/wer=0.054975, validation/ctc_loss=0.4061985909938812, validation/num_examples=5348, validation/wer=0.115914
I0213 00:19:06.053660 140062491473664 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.629746675491333, loss=1.1454483270645142
I0213 00:20:24.040529 140062483080960 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.600527286529541, loss=1.091243028640747
I0213 00:21:43.387063 140062491473664 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.209500789642334, loss=1.1006104946136475
I0213 00:23:08.321609 140062483080960 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.949296951293945, loss=1.1267473697662354
I0213 00:24:33.317341 140062491473664 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.623222589492798, loss=1.1916836500167847
I0213 00:26:00.990375 140062483080960 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.2475788593292236, loss=1.1449460983276367
I0213 00:27:31.661519 140218947737408 spec.py:321] Evaluating on the training split.
I0213 00:28:26.749845 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 00:29:20.479379 140218947737408 spec.py:349] Evaluating on the test split.
I0213 00:29:48.199078 140218947737408 submission_runner.py:408] Time since start: 45051.28s, 	Step: 48000, 	{'train/ctc_loss': Array(0.18988594, dtype=float32), 'train/wer': 0.05882446800023402, 'validation/ctc_loss': Array(0.40646556, dtype=float32), 'validation/wer': 0.11600065651640808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21571341, dtype=float32), 'test/wer': 0.06903905916763146, 'test/num_examples': 2472, 'score': 40915.90963792801, 'total_duration': 45051.27542638779, 'accumulated_submission_time': 40915.90963792801, 'accumulated_eval_time': 4131.348959207535, 'accumulated_logging_time': 1.6855216026306152}
I0213 00:29:48.241042 140062491473664 logging_writer.py:48] [48000] accumulated_eval_time=4131.348959, accumulated_logging_time=1.685522, accumulated_submission_time=40915.909638, global_step=48000, preemption_count=0, score=40915.909638, test/ctc_loss=0.21571341156959534, test/num_examples=2472, test/wer=0.069039, total_duration=45051.275426, train/ctc_loss=0.1898859441280365, train/wer=0.058824, validation/ctc_loss=0.4064655601978302, validation/num_examples=5348, validation/wer=0.116001
I0213 00:29:48.265217 140062483080960 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40915.909638
I0213 00:29:48.465099 140218947737408 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 00:29:49.482228 140218947737408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4/checkpoint_48000
I0213 00:29:49.501306 140218947737408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_4/checkpoint_48000.
I0213 00:29:50.609851 140218947737408 submission_runner.py:583] Tuning trial 4/5
I0213 00:29:50.610091 140218947737408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0213 00:29:50.624147 140218947737408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(28.983004, dtype=float32), 'train/wer': 2.33386175402295, 'validation/ctc_loss': Array(28.14912, dtype=float32), 'validation/wer': 2.185417612018112, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256042, dtype=float32), 'test/wer': 2.3442000284362114, 'test/num_examples': 2472, 'score': 15.801752090454102, 'total_duration': 175.9230306148529, 'accumulated_submission_time': 15.801752090454102, 'accumulated_eval_time': 160.12119221687317, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1688, {'train/ctc_loss': Array(1.6165359, dtype=float32), 'train/wer': 0.4345034367663282, 'validation/ctc_loss': Array(1.767953, dtype=float32), 'validation/wer': 0.4371240719464746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.3364433, dtype=float32), 'test/wer': 0.37432210102979707, 'test/num_examples': 2472, 'score': 1456.0460293293, 'total_duration': 1747.1521837711334, 'accumulated_submission_time': 1456.0460293293, 'accumulated_eval_time': 290.99935603141785, 'accumulated_logging_time': 0.02970576286315918, 'global_step': 1688, 'preemption_count': 0}), (3393, {'train/ctc_loss': Array(0.9240469, dtype=float32), 'train/wer': 0.2783773278611686, 'validation/ctc_loss': Array(1.0730573, dtype=float32), 'validation/wer': 0.29669714318815954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71331537, dtype=float32), 'test/wer': 0.22472731704344648, 'test/num_examples': 2472, 'score': 2896.040989637375, 'total_duration': 3321.5905945301056, 'accumulated_submission_time': 2896.040989637375, 'accumulated_eval_time': 425.304594039917, 'accumulated_logging_time': 0.08489155769348145, 'global_step': 3393, 'preemption_count': 0}), (5060, {'train/ctc_loss': Array(0.891339, dtype=float32), 'train/wer': 0.26996244674341974, 'validation/ctc_loss': Array(0.94086546, dtype=float32), 'validation/wer': 0.2633403168657134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6102613, dtype=float32), 'test/wer': 0.19052261694391973, 'test/num_examples': 2472, 'score': 4336.36754155159, 'total_duration': 4895.912990808487, 'accumulated_submission_time': 4336.36754155159, 'accumulated_eval_time': 559.1721725463867, 'accumulated_logging_time': 0.1350078582763672, 'global_step': 5060, 'preemption_count': 0}), (6747, {'train/ctc_loss': Array(0.77726054, dtype=float32), 'train/wer': 0.23642880380520748, 'validation/ctc_loss': Array(0.8973155, dtype=float32), 'validation/wer': 0.24943761645925253, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57886106, dtype=float32), 'test/wer': 0.18176832612272256, 'test/num_examples': 2472, 'score': 5776.8087022304535, 'total_duration': 6473.349189996719, 'accumulated_submission_time': 5776.8087022304535, 'accumulated_eval_time': 696.0269293785095, 'accumulated_logging_time': 0.1918809413909912, 'global_step': 6747, 'preemption_count': 0}), (8445, {'train/ctc_loss': Array(0.8788927, dtype=float32), 'train/wer': 0.2620743476444855, 'validation/ctc_loss': Array(0.9430648, dtype=float32), 'validation/wer': 0.2627127644168107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.59052604, dtype=float32), 'test/wer': 0.1910100948550769, 'test/num_examples': 2472, 'score': 7217.110549926758, 'total_duration': 8048.926936626434, 'accumulated_submission_time': 7217.110549926758, 'accumulated_eval_time': 831.1587114334106, 'accumulated_logging_time': 0.25078248977661133, 'global_step': 8445, 'preemption_count': 0}), (10120, {'train/ctc_loss': Array(0.711383, dtype=float32), 'train/wer': 0.21622274832054403, 'validation/ctc_loss': Array(0.83408403, dtype=float32), 'validation/wer': 0.2332853818898018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52912104, dtype=float32), 'test/wer': 0.16547843925822112, 'test/num_examples': 2472, 'score': 8657.196389913559, 'total_duration': 9624.246983766556, 'accumulated_submission_time': 8657.196389913559, 'accumulated_eval_time': 966.1767551898956, 'accumulated_logging_time': 0.3870253562927246, 'global_step': 10120, 'preemption_count': 0}), (11820, {'train/ctc_loss': Array(0.7196112, dtype=float32), 'train/wer': 0.2243399922553017, 'validation/ctc_loss': Array(0.83782446, dtype=float32), 'validation/wer': 0.23663554650163646, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51529604, dtype=float32), 'test/wer': 0.16371133183027645, 'test/num_examples': 2472, 'score': 10097.694558858871, 'total_duration': 11201.33668255806, 'accumulated_submission_time': 10097.694558858871, 'accumulated_eval_time': 1102.631802558899, 'accumulated_logging_time': 0.44161391258239746, 'global_step': 11820, 'preemption_count': 0}), (13509, {'train/ctc_loss': Array(0.68961746, dtype=float32), 'train/wer': 0.21113669643755378, 'validation/ctc_loss': Array(0.8070212, dtype=float32), 'validation/wer': 0.2289504426658428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5031106, dtype=float32), 'test/wer': 0.15944590010765136, 'test/num_examples': 2472, 'score': 11537.564745664597, 'total_duration': 12776.975717544556, 'accumulated_submission_time': 11537.564745664597, 'accumulated_eval_time': 1238.2635581493378, 'accumulated_logging_time': 0.4966261386871338, 'global_step': 13509, 'preemption_count': 0}), (15220, {'train/ctc_loss': Array(0.44087306, dtype=float32), 'train/wer': 0.14397901555097561, 'validation/ctc_loss': Array(0.77578115, dtype=float32), 'validation/wer': 0.218031030054935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4793862, dtype=float32), 'test/wer': 0.14955416082708753, 'test/num_examples': 2472, 'score': 12977.657584190369, 'total_duration': 14362.963431596756, 'accumulated_submission_time': 12977.657584190369, 'accumulated_eval_time': 1384.0197823047638, 'accumulated_logging_time': 0.5527534484863281, 'global_step': 15220, 'preemption_count': 0}), (16948, {'train/ctc_loss': Array(0.42074928, dtype=float32), 'train/wer': 0.13690937458024652, 'validation/ctc_loss': Array(0.75233626, dtype=float32), 'validation/wer': 0.2137443640962762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46206802, dtype=float32), 'test/wer': 0.14872138606219407, 'test/num_examples': 2472, 'score': 14418.166572093964, 'total_duration': 15945.859293460846, 'accumulated_submission_time': 14418.166572093964, 'accumulated_eval_time': 1526.2666816711426, 'accumulated_logging_time': 0.6094729900360107, 'global_step': 16948, 'preemption_count': 0}), (18622, {'train/ctc_loss': Array(0.41944084, dtype=float32), 'train/wer': 0.13867461707128548, 'validation/ctc_loss': Array(0.7515029, dtype=float32), 'validation/wer': 0.21325197679021404, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.458045, dtype=float32), 'test/wer': 0.14557309121930412, 'test/num_examples': 2472, 'score': 15858.704028606415, 'total_duration': 17524.35138106346, 'accumulated_submission_time': 15858.704028606415, 'accumulated_eval_time': 1664.0769486427307, 'accumulated_logging_time': 0.6706218719482422, 'global_step': 18622, 'preemption_count': 0}), (20312, {'train/ctc_loss': Array(0.37821147, dtype=float32), 'train/wer': 0.12641768516383303, 'validation/ctc_loss': Array(0.7153456, dtype=float32), 'validation/wer': 0.20442762389333538, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4342024, dtype=float32), 'test/wer': 0.13826092255194686, 'test/num_examples': 2472, 'score': 17298.691420793533, 'total_duration': 19102.427579641342, 'accumulated_submission_time': 17298.691420793533, 'accumulated_eval_time': 1802.022501707077, 'accumulated_logging_time': 0.7339036464691162, 'global_step': 20312, 'preemption_count': 0}), (22015, {'train/ctc_loss': Array(0.38218117, dtype=float32), 'train/wer': 0.12722010191680347, 'validation/ctc_loss': Array(0.69446677, dtype=float32), 'validation/wer': 0.19737007250644448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41897473, dtype=float32), 'test/wer': 0.13385330977190096, 'test/num_examples': 2472, 'score': 18738.981009721756, 'total_duration': 20678.58448934555, 'accumulated_submission_time': 18738.981009721756, 'accumulated_eval_time': 1937.7433910369873, 'accumulated_logging_time': 0.7955629825592041, 'global_step': 22015, 'preemption_count': 0}), (23682, {'train/ctc_loss': Array(0.35579237, dtype=float32), 'train/wer': 0.12106769858550498, 'validation/ctc_loss': Array(0.69894844, dtype=float32), 'validation/wer': 0.19710939687382334, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40583488, dtype=float32), 'test/wer': 0.13182215180874618, 'test/num_examples': 2472, 'score': 20178.99764108658, 'total_duration': 22254.981443166733, 'accumulated_submission_time': 20178.99764108658, 'accumulated_eval_time': 2073.9834084510803, 'accumulated_logging_time': 0.8556327819824219, 'global_step': 23682, 'preemption_count': 0}), (25354, {'train/ctc_loss': Array(0.3752873, dtype=float32), 'train/wer': 0.12097418804069449, 'validation/ctc_loss': Array(0.635908, dtype=float32), 'validation/wer': 0.18156540544715527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38012487, dtype=float32), 'test/wer': 0.12128044197997279, 'test/num_examples': 2472, 'score': 21619.240039110184, 'total_duration': 23830.625638246536, 'accumulated_submission_time': 21619.240039110184, 'accumulated_eval_time': 2209.2528836727142, 'accumulated_logging_time': 0.9065461158752441, 'global_step': 25354, 'preemption_count': 0}), (27057, {'train/ctc_loss': Array(0.32275435, dtype=float32), 'train/wer': 0.10982695565441794, 'validation/ctc_loss': Array(0.6262908, dtype=float32), 'validation/wer': 0.1797986039371675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36712766, dtype=float32), 'test/wer': 0.11859931346860846, 'test/num_examples': 2472, 'score': 23059.453444242477, 'total_duration': 25408.206853866577, 'accumulated_submission_time': 23059.453444242477, 'accumulated_eval_time': 2346.4857637882233, 'accumulated_logging_time': 0.9582936763763428, 'global_step': 27057, 'preemption_count': 0}), (28739, {'train/ctc_loss': Array(0.2909292, dtype=float32), 'train/wer': 0.09875202618699148, 'validation/ctc_loss': Array(0.6012382, dtype=float32), 'validation/wer': 0.17249968622377554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3531975, dtype=float32), 'test/wer': 0.11508541019235066, 'test/num_examples': 2472, 'score': 24500.440947532654, 'total_duration': 26987.574753046036, 'accumulated_submission_time': 24500.440947532654, 'accumulated_eval_time': 2484.727519273758, 'accumulated_logging_time': 1.015852451324463, 'global_step': 28739, 'preemption_count': 0}), (30429, {'train/ctc_loss': Array(0.27494448, dtype=float32), 'train/wer': 0.09241236346240732, 'validation/ctc_loss': Array(0.58237994, dtype=float32), 'validation/wer': 0.16846404124467787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33602273, dtype=float32), 'test/wer': 0.1067982857026791, 'test/num_examples': 2472, 'score': 25941.052482128143, 'total_duration': 28566.935750246048, 'accumulated_submission_time': 25941.052482128143, 'accumulated_eval_time': 2623.338259458542, 'accumulated_logging_time': 1.0719337463378906, 'global_step': 30429, 'preemption_count': 0}), (32123, {'train/ctc_loss': Array(0.2601245, dtype=float32), 'train/wer': 0.09085917312661498, 'validation/ctc_loss': Array(0.5635271, dtype=float32), 'validation/wer': 0.16050860712320303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31771612, dtype=float32), 'test/wer': 0.10330469400605286, 'test/num_examples': 2472, 'score': 27381.335985183716, 'total_duration': 30144.762478351593, 'accumulated_submission_time': 27381.335985183716, 'accumulated_eval_time': 2760.7441635131836, 'accumulated_logging_time': 1.128051519393921, 'global_step': 32123, 'preemption_count': 0}), (33800, {'train/ctc_loss': Array(0.26487103, dtype=float32), 'train/wer': 0.08863111246782912, 'validation/ctc_loss': Array(0.5352882, dtype=float32), 'validation/wer': 0.15320968940981106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.306206, dtype=float32), 'test/wer': 0.09812524120000812, 'test/num_examples': 2472, 'score': 28821.37881207466, 'total_duration': 31721.506538152695, 'accumulated_submission_time': 28821.37881207466, 'accumulated_eval_time': 2897.3103017807007, 'accumulated_logging_time': 1.1814250946044922, 'global_step': 33800, 'preemption_count': 0}), (35496, {'train/ctc_loss': Array(0.24313995, dtype=float32), 'train/wer': 0.08220079720714221, 'validation/ctc_loss': Array(0.5057727, dtype=float32), 'validation/wer': 0.14514805410467574, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28559756, dtype=float32), 'test/wer': 0.09182865151422827, 'test/num_examples': 2472, 'score': 30261.592493772507, 'total_duration': 33297.76939201355, 'accumulated_submission_time': 30261.592493772507, 'accumulated_eval_time': 3033.224186897278, 'accumulated_logging_time': 1.2355103492736816, 'global_step': 35496, 'preemption_count': 0}), (37194, {'train/ctc_loss': Array(0.21898215, dtype=float32), 'train/wer': 0.07491937741738129, 'validation/ctc_loss': Array(0.48544854, dtype=float32), 'validation/wer': 0.13985730422777257, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2720657, dtype=float32), 'test/wer': 0.08705543030081449, 'test/num_examples': 2472, 'score': 31701.920878887177, 'total_duration': 34876.05977129936, 'accumulated_submission_time': 31701.920878887177, 'accumulated_eval_time': 3171.0469262599945, 'accumulated_logging_time': 1.2923777103424072, 'global_step': 37194, 'preemption_count': 0}), (38888, {'train/ctc_loss': Array(0.2012175, dtype=float32), 'train/wer': 0.06860388103445579, 'validation/ctc_loss': Array(0.46407133, dtype=float32), 'validation/wer': 0.13293491798372226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2549981, dtype=float32), 'test/wer': 0.08283062173745252, 'test/num_examples': 2472, 'score': 33141.95910906792, 'total_duration': 36454.38322663307, 'accumulated_submission_time': 33141.95910906792, 'accumulated_eval_time': 3309.1948528289795, 'accumulated_logging_time': 1.345574140548706, 'global_step': 38888, 'preemption_count': 0}), (40597, {'train/ctc_loss': Array(0.16660897, dtype=float32), 'train/wer': 0.05763320483709141, 'validation/ctc_loss': Array(0.44376504, dtype=float32), 'validation/wer': 0.12745107504561823, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24118751, dtype=float32), 'test/wer': 0.0770215099628298, 'test/num_examples': 2472, 'score': 34582.12906932831, 'total_duration': 38031.52192878723, 'accumulated_submission_time': 34582.12906932831, 'accumulated_eval_time': 3446.0249009132385, 'accumulated_logging_time': 1.402059555053711, 'global_step': 40597, 'preemption_count': 0}), (42267, {'train/ctc_loss': Array(0.16422431, dtype=float32), 'train/wer': 0.05539039913369497, 'validation/ctc_loss': Array(0.4237094, dtype=float32), 'validation/wer': 0.12222790774013535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23010056, dtype=float32), 'test/wer': 0.07393414985883452, 'test/num_examples': 2472, 'score': 36022.32275509834, 'total_duration': 39610.562725782394, 'accumulated_submission_time': 36022.32275509834, 'accumulated_eval_time': 3584.731765270233, 'accumulated_logging_time': 1.4607226848602295, 'global_step': 42267, 'preemption_count': 0}), (43960, {'train/ctc_loss': Array(0.16956198, dtype=float32), 'train/wer': 0.05703789740957835, 'validation/ctc_loss': Array(0.41393122, dtype=float32), 'validation/wer': 0.11902256292420132, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22181489, dtype=float32), 'test/wer': 0.07178112241789043, 'test/num_examples': 2472, 'score': 37463.19466948509, 'total_duration': 41188.80369210243, 'accumulated_submission_time': 37463.19466948509, 'accumulated_eval_time': 3721.962750196457, 'accumulated_logging_time': 1.517216682434082, 'global_step': 43960, 'preemption_count': 0}), (45652, {'train/ctc_loss': Array(0.14950223, dtype=float32), 'train/wer': 0.04942652078491429, 'validation/ctc_loss': Array(0.4087675, dtype=float32), 'validation/wer': 0.1171012869652529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21725857, dtype=float32), 'test/wer': 0.07001401498994576, 'test/num_examples': 2472, 'score': 38903.469370126724, 'total_duration': 42764.24932599068, 'accumulated_submission_time': 38903.469370126724, 'accumulated_eval_time': 3856.989844560623, 'accumulated_logging_time': 1.5770831108093262, 'global_step': 45652, 'preemption_count': 0}), (47317, {'train/ctc_loss': Array(0.16066147, dtype=float32), 'train/wer': 0.0549748414936208, 'validation/ctc_loss': Array(0.4061986, dtype=float32), 'validation/wer': 0.1159137646388677, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21572423, dtype=float32), 'test/wer': 0.06909999390652612, 'test/num_examples': 2472, 'score': 40343.726199388504, 'total_duration': 44342.45700645447, 'accumulated_submission_time': 40343.726199388504, 'accumulated_eval_time': 3994.8145458698273, 'accumulated_logging_time': 1.6236886978149414, 'global_step': 47317, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.18988594, dtype=float32), 'train/wer': 0.05882446800023402, 'validation/ctc_loss': Array(0.40646556, dtype=float32), 'validation/wer': 0.11600065651640808, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21571341, dtype=float32), 'test/wer': 0.06903905916763146, 'test/num_examples': 2472, 'score': 40915.90963792801, 'total_duration': 45051.27542638779, 'accumulated_submission_time': 40915.90963792801, 'accumulated_eval_time': 4131.348959207535, 'accumulated_logging_time': 1.6855216026306152, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 00:29:50.624380 140218947737408 submission_runner.py:586] Timing: 40915.90963792801
I0213 00:29:50.624443 140218947737408 submission_runner.py:588] Total number of evals: 30
I0213 00:29:50.624507 140218947737408 submission_runner.py:589] ====================
I0213 00:29:50.624570 140218947737408 submission_runner.py:542] Using RNG seed 2622380006
I0213 00:29:50.628011 140218947737408 submission_runner.py:551] --- Tuning run 5/5 ---
I0213 00:29:50.628144 140218947737408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5.
I0213 00:29:50.629676 140218947737408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5/hparams.json.
I0213 00:29:50.631706 140218947737408 submission_runner.py:206] Initializing dataset.
I0213 00:29:50.631827 140218947737408 submission_runner.py:213] Initializing model.
I0213 00:29:51.863875 140218947737408 submission_runner.py:255] Initializing optimizer.
I0213 00:29:52.006643 140218947737408 submission_runner.py:262] Initializing metrics bundle.
I0213 00:29:52.006848 140218947737408 submission_runner.py:280] Initializing checkpoint and logger.
I0213 00:29:52.011738 140218947737408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5 with prefix checkpoint_
I0213 00:29:52.011868 140218947737408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5/meta_data_0.json.
I0213 00:29:52.012192 140218947737408 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0213 00:29:52.012262 140218947737408 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0213 00:29:52.550615 140218947737408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0213 00:29:53.036539 140218947737408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5/flags_0.json.
I0213 00:29:53.057974 140218947737408 submission_runner.py:314] Starting training loop.
I0213 00:29:53.061583 140218947737408 input_pipeline.py:20] Loading split = train-clean-100
I0213 00:29:53.104295 140218947737408 input_pipeline.py:20] Loading split = train-clean-360
I0213 00:29:53.735237 140218947737408 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0213 00:30:09.644416 140062415939328 logging_writer.py:48] [0] global_step=0, grad_norm=19.971559524536133, loss=33.36280059814453
I0213 00:30:09.660757 140218947737408 spec.py:321] Evaluating on the training split.
I0213 00:31:30.086886 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 00:32:23.932768 140218947737408 spec.py:349] Evaluating on the test split.
I0213 00:32:52.702677 140218947737408 submission_runner.py:408] Time since start: 179.64s, 	Step: 1, 	{'train/ctc_loss': Array(28.652134, dtype=float32), 'train/wer': 2.3640764239661616, 'validation/ctc_loss': Array(28.14943, dtype=float32), 'validation/wer': 2.1854755399364723, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256365, dtype=float32), 'test/wer': 2.3440578473787905, 'test/num_examples': 2472, 'score': 16.60268473625183, 'total_duration': 179.64115977287292, 'accumulated_submission_time': 16.60268473625183, 'accumulated_eval_time': 163.03839111328125, 'accumulated_logging_time': 0}
I0213 00:32:52.722718 140062491473664 logging_writer.py:48] [1] accumulated_eval_time=163.038391, accumulated_logging_time=0, accumulated_submission_time=16.602685, global_step=1, preemption_count=0, score=16.602685, test/ctc_loss=28.256364822387695, test/num_examples=2472, test/wer=2.344058, total_duration=179.641160, train/ctc_loss=28.65213394165039, train/wer=2.364076, validation/ctc_loss=28.149429321289062, validation/num_examples=5348, validation/wer=2.185476
I0213 00:34:20.124286 140062415939328 logging_writer.py:48] [100] global_step=100, grad_norm=0.6764528155326843, loss=5.974327087402344
I0213 00:35:36.773239 140062424332032 logging_writer.py:48] [200] global_step=200, grad_norm=0.816213846206665, loss=5.84616231918335
I0213 00:36:54.623231 140062415939328 logging_writer.py:48] [300] global_step=300, grad_norm=0.3513769209384918, loss=5.696354389190674
I0213 00:38:11.423843 140062424332032 logging_writer.py:48] [400] global_step=400, grad_norm=0.4978052079677582, loss=5.4592084884643555
I0213 00:39:42.476910 140062415939328 logging_writer.py:48] [500] global_step=500, grad_norm=3.1219499111175537, loss=4.865957260131836
I0213 00:41:10.771035 140062424332032 logging_writer.py:48] [600] global_step=600, grad_norm=2.518604040145874, loss=3.9639368057250977
I0213 00:42:42.074223 140062415939328 logging_writer.py:48] [700] global_step=700, grad_norm=2.133241891860962, loss=3.5196468830108643
I0213 00:44:15.465412 140062424332032 logging_writer.py:48] [800] global_step=800, grad_norm=2.7007181644439697, loss=3.182034492492676
I0213 00:45:47.848655 140062415939328 logging_writer.py:48] [900] global_step=900, grad_norm=1.9007668495178223, loss=2.956608772277832
I0213 00:47:15.009831 140062424332032 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9064112901687622, loss=2.8953914642333984
I0213 00:48:37.789963 140062491473664 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.9166580438613892, loss=2.684314250946045
I0213 00:49:54.686080 140062483080960 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.3349862098693848, loss=2.601039171218872
I0213 00:51:16.934926 140062491473664 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.759474277496338, loss=2.5207836627960205
I0213 00:52:38.179838 140062483080960 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.137584924697876, loss=2.4555141925811768
I0213 00:54:03.804512 140062491473664 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.061460018157959, loss=2.3973934650421143
I0213 00:55:34.972024 140062483080960 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.5934317111968994, loss=2.3687000274658203
I0213 00:56:53.274924 140218947737408 spec.py:321] Evaluating on the training split.
I0213 00:57:46.531497 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 00:58:37.581633 140218947737408 spec.py:349] Evaluating on the test split.
I0213 00:59:03.796413 140218947737408 submission_runner.py:408] Time since start: 1750.73s, 	Step: 1692, 	{'train/ctc_loss': Array(2.7661793, dtype=float32), 'train/wer': 0.616729432815946, 'validation/ctc_loss': Array(3.1783566, dtype=float32), 'validation/wer': 0.6568639755930371, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6875026, dtype=float32), 'test/wer': 0.5876749334795767, 'test/num_examples': 2472, 'score': 1457.0688242912292, 'total_duration': 1750.7331321239471, 'accumulated_submission_time': 1457.0688242912292, 'accumulated_eval_time': 293.5546774864197, 'accumulated_logging_time': 0.03145146369934082}
I0213 00:59:03.829039 140063075153664 logging_writer.py:48] [1692] accumulated_eval_time=293.554677, accumulated_logging_time=0.031451, accumulated_submission_time=1457.068824, global_step=1692, preemption_count=0, score=1457.068824, test/ctc_loss=2.68750262260437, test/num_examples=2472, test/wer=0.587675, total_duration=1750.733132, train/ctc_loss=2.766179323196411, train/wer=0.616729, validation/ctc_loss=3.178356647491455, validation/num_examples=5348, validation/wer=0.656864
I0213 00:59:10.671329 140063066760960 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.0659968852996826, loss=2.267953634262085
I0213 01:00:25.718680 140063075153664 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.064570426940918, loss=2.2984719276428223
I0213 01:01:40.869032 140063066760960 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.110337495803833, loss=2.2384889125823975
I0213 01:03:12.093210 140063075153664 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.160015821456909, loss=2.1203362941741943
I0213 01:04:38.948331 140062419793664 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.3110313415527344, loss=2.161285638809204
I0213 01:05:58.155175 140062411400960 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.8614470958709717, loss=2.1284632682800293
I0213 01:07:16.894239 140062419793664 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.114490509033203, loss=2.1432502269744873
I0213 01:08:41.603942 140062411400960 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.0346124172210693, loss=2.036210060119629
I0213 01:10:07.999486 140062419793664 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.053013563156128, loss=2.0510997772216797
I0213 01:11:38.729087 140062411400960 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.403419256210327, loss=2.0400166511535645
I0213 01:13:08.775974 140062419793664 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.8855661153793335, loss=2.01754093170166
I0213 01:14:35.431205 140062411400960 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.0689659118652344, loss=2.0046606063842773
I0213 01:16:06.958312 140062419793664 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.218613386154175, loss=1.9746677875518799
I0213 01:17:35.940518 140062411400960 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.9830955266952515, loss=1.9679113626480103
I0213 01:19:09.505186 140063075153664 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.1880626678466797, loss=1.9469040632247925
I0213 01:20:25.649429 140063066760960 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.4529263973236084, loss=1.922611117362976
I0213 01:21:44.702576 140063075153664 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.297567844390869, loss=1.9544233083724976
I0213 01:23:04.437664 140218947737408 spec.py:321] Evaluating on the training split.
I0213 01:23:59.662168 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 01:24:52.802370 140218947737408 spec.py:349] Evaluating on the test split.
I0213 01:25:19.680168 140218947737408 submission_runner.py:408] Time since start: 3326.62s, 	Step: 3396, 	{'train/ctc_loss': Array(0.77670133, dtype=float32), 'train/wer': 0.2395568610625213, 'validation/ctc_loss': Array(0.9372019, dtype=float32), 'validation/wer': 0.2630506772739122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.61222464, dtype=float32), 'test/wer': 0.19214754331444356, 'test/num_examples': 2472, 'score': 2897.5816576480865, 'total_duration': 3326.616904258728, 'accumulated_submission_time': 2897.5816576480865, 'accumulated_eval_time': 428.7920391559601, 'accumulated_logging_time': 0.08203959465026855}
I0213 01:25:19.712496 140063075153664 logging_writer.py:48] [3396] accumulated_eval_time=428.792039, accumulated_logging_time=0.082040, accumulated_submission_time=2897.581658, global_step=3396, preemption_count=0, score=2897.581658, test/ctc_loss=0.6122246384620667, test/num_examples=2472, test/wer=0.192148, total_duration=3326.616904, train/ctc_loss=0.7767013311386108, train/wer=0.239557, validation/ctc_loss=0.9372019171714783, validation/num_examples=5348, validation/wer=0.263051
I0213 01:25:23.582802 140063066760960 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.0261194705963135, loss=1.9084652662277222
I0213 01:26:38.162181 140063075153664 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.9404007196426392, loss=1.813302993774414
I0213 01:27:53.091645 140063066760960 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.1117308139801025, loss=1.9079797267913818
I0213 01:29:23.514475 140063075153664 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.162548303604126, loss=1.862596869468689
I0213 01:30:53.421472 140063066760960 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.045628786087036, loss=1.8474366664886475
I0213 01:32:25.206087 140063075153664 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.9961656332015991, loss=1.8193163871765137
I0213 01:33:55.700312 140063066760960 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.5389270782470703, loss=1.8759089708328247
I0213 01:35:26.027999 140063075153664 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.453230142593384, loss=1.7647384405136108
I0213 01:36:48.654354 140063075153664 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.2374987602233887, loss=1.7748615741729736
I0213 01:38:04.876800 140063066760960 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.7782673835754395, loss=1.7877647876739502
I0213 01:39:27.110385 140063075153664 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.7902932167053223, loss=1.8578670024871826
I0213 01:40:48.613658 140063066760960 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.5017404556274414, loss=1.8071869611740112
I0213 01:42:21.685766 140063075153664 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.4341750144958496, loss=1.8035120964050293
I0213 01:43:48.251194 140063066760960 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.7115916013717651, loss=1.8133580684661865
I0213 01:45:16.853325 140063075153664 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.6897261142730713, loss=1.8069055080413818
I0213 01:46:48.827470 140063066760960 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.025162935256958, loss=1.84865140914917
I0213 01:48:18.145371 140063075153664 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.5317940711975098, loss=1.745615005493164
I0213 01:49:20.969139 140218947737408 spec.py:321] Evaluating on the training split.
I0213 01:50:15.609100 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 01:51:09.509010 140218947737408 spec.py:349] Evaluating on the test split.
I0213 01:51:36.468290 140218947737408 submission_runner.py:408] Time since start: 4903.40s, 	Step: 5071, 	{'train/ctc_loss': Array(0.67181903, dtype=float32), 'train/wer': 0.21078466440312346, 'validation/ctc_loss': Array(0.81510663, dtype=float32), 'validation/wer': 0.2328991957674001, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5147943, dtype=float32), 'test/wer': 0.16588467085085207, 'test/num_examples': 2472, 'score': 4338.7473294734955, 'total_duration': 4903.404318571091, 'accumulated_submission_time': 4338.7473294734955, 'accumulated_eval_time': 564.2852621078491, 'accumulated_logging_time': 0.13069558143615723}
I0213 01:51:36.510276 140063075153664 logging_writer.py:48] [5071] accumulated_eval_time=564.285262, accumulated_logging_time=0.130696, accumulated_submission_time=4338.747329, global_step=5071, preemption_count=0, score=4338.747329, test/ctc_loss=0.5147942900657654, test/num_examples=2472, test/wer=0.165885, total_duration=4903.404319, train/ctc_loss=0.6718190312385559, train/wer=0.210785, validation/ctc_loss=0.8151066303253174, validation/num_examples=5348, validation/wer=0.232899
I0213 01:51:59.052331 140063066760960 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.9572677612304688, loss=1.7375916242599487
I0213 01:53:21.206449 140062419793664 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.0506253242492676, loss=1.8035393953323364
I0213 01:54:41.780963 140062411400960 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.5056750774383545, loss=1.79783034324646
I0213 01:56:03.519796 140062419793664 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.587892532348633, loss=1.6870784759521484
I0213 01:57:29.186237 140062411400960 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.12143874168396, loss=1.7670186758041382
I0213 01:58:55.859274 140062419793664 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.9207842350006104, loss=1.711941123008728
I0213 02:00:24.815793 140062411400960 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.937987804412842, loss=1.7436164617538452
I0213 02:01:54.804564 140062419793664 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.9911861419677734, loss=1.6827504634857178
I0213 02:03:26.851278 140062411400960 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.8512428998947144, loss=1.729753017425537
I0213 02:04:58.073352 140062419793664 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.321225166320801, loss=1.8009891510009766
I0213 02:06:28.463025 140062411400960 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.090806245803833, loss=1.673174262046814
I0213 02:07:55.883615 140063075153664 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.2451422214508057, loss=1.7037317752838135
I0213 02:09:16.395168 140063066760960 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.5402348041534424, loss=1.754525065422058
I0213 02:10:38.810548 140063075153664 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.877429723739624, loss=1.755735158920288
I0213 02:11:58.860547 140063066760960 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.5243418216705322, loss=1.71239173412323
I0213 02:13:26.864027 140063075153664 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.8174993991851807, loss=1.7201679944992065
I0213 02:14:57.338397 140063066760960 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.8274699449539185, loss=1.7176809310913086
I0213 02:15:36.574909 140218947737408 spec.py:321] Evaluating on the training split.
I0213 02:16:30.593738 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 02:17:23.587174 140218947737408 spec.py:349] Evaluating on the test split.
I0213 02:17:50.806308 140218947737408 submission_runner.py:408] Time since start: 6477.74s, 	Step: 6744, 	{'train/ctc_loss': Array(0.7183804, dtype=float32), 'train/wer': 0.2258982279304178, 'validation/ctc_loss': Array(0.7535185, dtype=float32), 'validation/wer': 0.2154146190756635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45724174, dtype=float32), 'test/wer': 0.14762456076209046, 'test/num_examples': 2472, 'score': 5778.721389293671, 'total_duration': 6477.741781234741, 'accumulated_submission_time': 5778.721389293671, 'accumulated_eval_time': 698.5102126598358, 'accumulated_logging_time': 0.18908905982971191}
I0213 02:17:50.845794 140063075153664 logging_writer.py:48] [6744] accumulated_eval_time=698.510213, accumulated_logging_time=0.189089, accumulated_submission_time=5778.721389, global_step=6744, preemption_count=0, score=5778.721389, test/ctc_loss=0.4572417438030243, test/num_examples=2472, test/wer=0.147625, total_duration=6477.741781, train/ctc_loss=0.7183803915977478, train/wer=0.225898, validation/ctc_loss=0.7535185217857361, validation/num_examples=5348, validation/wer=0.215415
I0213 02:18:33.581940 140063066760960 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.9208033084869385, loss=1.6455631256103516
I0213 02:19:48.592423 140063075153664 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.039463996887207, loss=1.6850963830947876
I0213 02:21:11.486519 140063066760960 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.6599514484405518, loss=1.6460751295089722
I0213 02:22:42.755410 140063075153664 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.12538743019104, loss=1.5906414985656738
I0213 02:24:14.205230 140063066760960 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.1795849800109863, loss=1.6991525888442993
I0213 02:25:35.976432 140062419793664 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.570181131362915, loss=1.6107624769210815
I0213 02:26:54.625807 140062411400960 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.94384503364563, loss=1.6780831813812256
I0213 02:28:14.739623 140062419793664 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.540797233581543, loss=1.6164218187332153
I0213 02:29:38.526065 140062411400960 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.0505387783050537, loss=1.7718902826309204
I0213 02:31:07.470549 140062419793664 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.1217997074127197, loss=1.7207695245742798
I0213 02:32:38.281949 140062411400960 logging_writer.py:48] [7800] global_step=7800, grad_norm=4.219166278839111, loss=1.6296584606170654
I0213 02:34:10.995132 140062419793664 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.1005911827087402, loss=1.694193720817566
I0213 02:35:43.282479 140062411400960 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.0662150382995605, loss=1.626691460609436
I0213 02:37:12.293581 140062419793664 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.860813021659851, loss=1.6627873182296753
I0213 02:38:43.116980 140062411400960 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.069490432739258, loss=1.646905541419983
I0213 02:40:08.635628 140063075153664 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.713393211364746, loss=1.6515063047409058
I0213 02:41:25.488929 140063066760960 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.398157835006714, loss=1.634666085243225
I0213 02:41:51.278332 140218947737408 spec.py:321] Evaluating on the training split.
I0213 02:42:45.227727 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 02:43:38.270253 140218947737408 spec.py:349] Evaluating on the test split.
I0213 02:44:05.622700 140218947737408 submission_runner.py:408] Time since start: 8052.56s, 	Step: 8435, 	{'train/ctc_loss': Array(0.58081234, dtype=float32), 'train/wer': 0.18172251712929402, 'validation/ctc_loss': Array(0.719909, dtype=float32), 'validation/wer': 0.2052289600973189, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4333417, dtype=float32), 'test/wer': 0.13846403834826235, 'test/num_examples': 2472, 'score': 7219.063415050507, 'total_duration': 8052.558868169785, 'accumulated_submission_time': 7219.063415050507, 'accumulated_eval_time': 832.8487877845764, 'accumulated_logging_time': 0.24363970756530762}
I0213 02:44:05.656151 140063075153664 logging_writer.py:48] [8435] accumulated_eval_time=832.848788, accumulated_logging_time=0.243640, accumulated_submission_time=7219.063415, global_step=8435, preemption_count=0, score=7219.063415, test/ctc_loss=0.43334171175956726, test/num_examples=2472, test/wer=0.138464, total_duration=8052.558868, train/ctc_loss=0.5808123350143433, train/wer=0.181723, validation/ctc_loss=0.7199090123176575, validation/num_examples=5348, validation/wer=0.205229
I0213 02:44:55.183869 140063066760960 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.078345537185669, loss=1.6561188697814941
I0213 02:46:10.445877 140063075153664 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.0826704502105713, loss=1.6548957824707031
I0213 02:47:27.373339 140063066760960 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.272387742996216, loss=1.6590768098831177
I0213 02:48:57.474040 140063075153664 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.44217586517334, loss=1.6477842330932617
I0213 02:50:28.443851 140063066760960 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.348625659942627, loss=1.6430461406707764
I0213 02:51:58.933244 140063075153664 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.9846010208129883, loss=1.6193453073501587
I0213 02:53:24.986776 140063066760960 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.4364421367645264, loss=1.576337218284607
I0213 02:54:56.774820 140063075153664 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.7280566692352295, loss=1.6227519512176514
I0213 02:56:26.000177 140063075153664 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.6167972087860107, loss=1.618720293045044
I0213 02:57:45.630392 140063066760960 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.658752918243408, loss=1.6046912670135498
I0213 02:59:05.359091 140063075153664 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.607163190841675, loss=1.6001781225204468
I0213 03:00:28.626821 140063066760960 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.058335065841675, loss=1.596551537513733
I0213 03:01:55.147222 140063075153664 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.5237553119659424, loss=1.5369958877563477
I0213 03:03:23.218127 140063066760960 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.9531843662261963, loss=1.639488697052002
I0213 03:04:53.478474 140063075153664 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.5695600509643555, loss=1.605666995048523
I0213 03:06:25.551960 140063066760960 logging_writer.py:48] [10000] global_step=10000, grad_norm=7.6572957038879395, loss=1.6757822036743164
I0213 03:07:56.651428 140063075153664 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.2904112339019775, loss=1.5753306150436401
I0213 03:08:06.124385 140218947737408 spec.py:321] Evaluating on the training split.
I0213 03:09:00.138014 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 03:09:54.101154 140218947737408 spec.py:349] Evaluating on the test split.
I0213 03:10:20.804150 140218947737408 submission_runner.py:408] Time since start: 9627.74s, 	Step: 10111, 	{'train/ctc_loss': Array(0.57047266, dtype=float32), 'train/wer': 0.18313599708829517, 'validation/ctc_loss': Array(0.67904717, dtype=float32), 'validation/wer': 0.19588325593519798, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4085987, dtype=float32), 'test/wer': 0.13123311599943127, 'test/num_examples': 2472, 'score': 8659.439930438995, 'total_duration': 9627.739500045776, 'accumulated_submission_time': 8659.439930438995, 'accumulated_eval_time': 967.5219428539276, 'accumulated_logging_time': 0.2929205894470215}
I0213 03:10:20.839291 140063075153664 logging_writer.py:48] [10111] accumulated_eval_time=967.521943, accumulated_logging_time=0.292921, accumulated_submission_time=8659.439930, global_step=10111, preemption_count=0, score=8659.439930, test/ctc_loss=0.4085986912250519, test/num_examples=2472, test/wer=0.131233, total_duration=9627.739500, train/ctc_loss=0.5704726576805115, train/wer=0.183136, validation/ctc_loss=0.679047167301178, validation/num_examples=5348, validation/wer=0.195883
I0213 03:11:28.187615 140063066760960 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.9424567222595215, loss=1.5551798343658447
I0213 03:12:47.149070 140063075153664 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.310699939727783, loss=1.5873509645462036
I0213 03:14:06.019654 140063066760960 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.56066632270813, loss=1.5882353782653809
I0213 03:15:25.561895 140063075153664 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.1025631427764893, loss=1.6061687469482422
I0213 03:16:50.979025 140063066760960 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.758632183074951, loss=1.6235285997390747
I0213 03:18:17.853092 140063075153664 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.784179210662842, loss=1.634376049041748
I0213 03:19:43.216572 140063066760960 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.4558346271514893, loss=1.5740240812301636
I0213 03:21:10.982686 140063075153664 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.8614821434020996, loss=1.5658291578292847
I0213 03:22:42.313787 140063066760960 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.3821022510528564, loss=1.6171767711639404
I0213 03:24:12.738700 140063075153664 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.414247512817383, loss=1.625176191329956
I0213 03:25:44.471570 140063066760960 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.8930466175079346, loss=1.6164032220840454
I0213 03:27:15.848183 140063075153664 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.6598658561706543, loss=1.5379886627197266
I0213 03:28:38.173970 140063075153664 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.942611575126648, loss=1.5041699409484863
I0213 03:29:57.382572 140063066760960 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.280858993530273, loss=1.5847508907318115
I0213 03:31:21.504881 140063075153664 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.0955705642700195, loss=1.6311858892440796
I0213 03:32:46.809149 140063066760960 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.4320147037506104, loss=1.5604827404022217
I0213 03:34:16.245301 140063075153664 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.335886001586914, loss=1.6059857606887817
I0213 03:34:21.883301 140218947737408 spec.py:321] Evaluating on the training split.
I0213 03:35:17.428132 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 03:36:11.598053 140218947737408 spec.py:349] Evaluating on the test split.
I0213 03:36:39.145097 140218947737408 submission_runner.py:408] Time since start: 11206.08s, 	Step: 11807, 	{'train/ctc_loss': Array(0.43884623, dtype=float32), 'train/wer': 0.14534793017188047, 'validation/ctc_loss': Array(0.6524572, dtype=float32), 'validation/wer': 0.18889328711972736, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39502275, dtype=float32), 'test/wer': 0.1270489305953324, 'test/num_examples': 2472, 'score': 10100.390930175781, 'total_duration': 11206.082193851471, 'accumulated_submission_time': 10100.390930175781, 'accumulated_eval_time': 1104.7788660526276, 'accumulated_logging_time': 0.3447701930999756}
I0213 03:36:39.184483 140063075153664 logging_writer.py:48] [11807] accumulated_eval_time=1104.778866, accumulated_logging_time=0.344770, accumulated_submission_time=10100.390930, global_step=11807, preemption_count=0, score=10100.390930, test/ctc_loss=0.39502274990081787, test/num_examples=2472, test/wer=0.127049, total_duration=11206.082194, train/ctc_loss=0.438846230506897, train/wer=0.145348, validation/ctc_loss=0.6524571776390076, validation/num_examples=5348, validation/wer=0.188893
I0213 03:37:49.645198 140063066760960 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.092085361480713, loss=1.5988738536834717
I0213 03:39:04.880098 140063075153664 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.2883141040802, loss=1.530616044998169
I0213 03:40:28.755341 140063066760960 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.3767528533935547, loss=1.5885010957717896
I0213 03:41:59.369076 140063075153664 logging_writer.py:48] [12200] global_step=12200, grad_norm=3.072230339050293, loss=1.5417876243591309
I0213 03:43:29.792701 140063066760960 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.8821794986724854, loss=1.5957863330841064
I0213 03:44:58.632468 140063075153664 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.2884535789489746, loss=1.5034352540969849
I0213 03:46:18.780176 140063066760960 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.10591721534729, loss=1.5422148704528809
I0213 03:47:36.193697 140063075153664 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.764752149581909, loss=1.6530241966247559
I0213 03:48:59.242294 140063066760960 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.7508106231689453, loss=1.5468508005142212
I0213 03:50:24.854564 140063075153664 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.6007583141326904, loss=1.5990493297576904
I0213 03:51:56.692506 140063066760960 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.736133337020874, loss=1.5737155675888062
I0213 03:53:26.809618 140063075153664 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.0956592559814453, loss=1.5803629159927368
I0213 03:54:58.989317 140063066760960 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.8551025390625, loss=1.547702670097351
I0213 03:56:30.463560 140063075153664 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.579977512359619, loss=1.6153066158294678
I0213 03:57:59.869191 140063066760960 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.3266444206237793, loss=1.5489656925201416
I0213 03:59:34.734782 140063075153664 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.0414600372314453, loss=1.5632435083389282
I0213 04:00:39.624006 140218947737408 spec.py:321] Evaluating on the training split.
I0213 04:01:35.064296 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 04:02:27.567304 140218947737408 spec.py:349] Evaluating on the test split.
I0213 04:02:54.561497 140218947737408 submission_runner.py:408] Time since start: 12781.50s, 	Step: 13483, 	{'train/ctc_loss': Array(0.5149647, dtype=float32), 'train/wer': 0.16686553766324755, 'validation/ctc_loss': Array(0.643958, dtype=float32), 'validation/wer': 0.1846355851202487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37856, dtype=float32), 'test/wer': 0.12428655576544188, 'test/num_examples': 2472, 'score': 11540.737623214722, 'total_duration': 12781.498124361038, 'accumulated_submission_time': 11540.737623214722, 'accumulated_eval_time': 1239.711059808731, 'accumulated_logging_time': 0.4017910957336426}
I0213 04:02:54.595549 140063075153664 logging_writer.py:48] [13483] accumulated_eval_time=1239.711060, accumulated_logging_time=0.401791, accumulated_submission_time=11540.737623, global_step=13483, preemption_count=0, score=11540.737623, test/ctc_loss=0.37856000661849976, test/num_examples=2472, test/wer=0.124287, total_duration=12781.498124, train/ctc_loss=0.5149646997451782, train/wer=0.166866, validation/ctc_loss=0.6439579725265503, validation/num_examples=5348, validation/wer=0.184636
I0213 04:03:08.165055 140063066760960 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.162764072418213, loss=1.5210471153259277
I0213 04:04:23.398501 140063075153664 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.993951439857483, loss=1.5242784023284912
I0213 04:05:38.275065 140063066760960 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.1878275871276855, loss=1.603498101234436
I0213 04:06:53.861984 140063075153664 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.435185670852661, loss=1.549018383026123
I0213 04:08:24.868973 140063066760960 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.0774755477905273, loss=1.5147368907928467
I0213 04:09:56.142847 140063075153664 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.879833459854126, loss=1.5144215822219849
I0213 04:11:25.427375 140063066760960 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.3405184745788574, loss=1.5274395942687988
I0213 04:12:55.288725 140063075153664 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.5253195762634277, loss=1.5351319313049316
I0213 04:14:24.671812 140063066760960 logging_writer.py:48] [14300] global_step=14300, grad_norm=4.4247870445251465, loss=1.5619252920150757
I0213 04:15:52.821615 140063075153664 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.802725315093994, loss=1.5031720399856567
I0213 04:17:16.145432 140063075153664 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.863609552383423, loss=1.4872937202453613
I0213 04:18:31.582226 140063066760960 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.0417640209198, loss=1.475258708000183
I0213 04:19:50.244879 140063075153664 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.3670520782470703, loss=1.5227888822555542
I0213 04:21:13.825597 140063066760960 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.9100396633148193, loss=1.5645382404327393
I0213 04:22:43.513197 140063075153664 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.643540143966675, loss=1.4285389184951782
I0213 04:24:12.233753 140063066760960 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.1778552532196045, loss=1.4858529567718506
I0213 04:25:43.732438 140063075153664 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.8184242248535156, loss=1.4721685647964478
I0213 04:26:55.053702 140218947737408 spec.py:321] Evaluating on the training split.
I0213 04:27:51.092620 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 04:28:43.470504 140218947737408 spec.py:349] Evaluating on the test split.
I0213 04:29:11.624335 140218947737408 submission_runner.py:408] Time since start: 14358.56s, 	Step: 15183, 	{'train/ctc_loss': Array(0.4569584, dtype=float32), 'train/wer': 0.1493814295257181, 'validation/ctc_loss': Array(0.6158217, dtype=float32), 'validation/wer': 0.17738494067215693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36153093, dtype=float32), 'test/wer': 0.1167915828814007, 'test/num_examples': 2472, 'score': 12981.104486703873, 'total_duration': 14358.558901309967, 'accumulated_submission_time': 12981.104486703873, 'accumulated_eval_time': 1376.2743470668793, 'accumulated_logging_time': 0.45090579986572266}
I0213 04:29:11.659725 140062491473664 logging_writer.py:48] [15183] accumulated_eval_time=1376.274347, accumulated_logging_time=0.450906, accumulated_submission_time=12981.104487, global_step=15183, preemption_count=0, score=12981.104487, test/ctc_loss=0.36153092980384827, test/num_examples=2472, test/wer=0.116792, total_duration=14358.558901, train/ctc_loss=0.4569584131240845, train/wer=0.149381, validation/ctc_loss=0.6158217191696167, validation/num_examples=5348, validation/wer=0.177385
I0213 04:29:25.220110 140062483080960 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.3516013622283936, loss=1.5157370567321777
I0213 04:30:40.206551 140062491473664 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.4710354804992676, loss=1.54985511302948
I0213 04:31:57.000523 140062483080960 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.266119956970215, loss=1.564621925354004
I0213 04:33:24.172653 140062491473664 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.4310290813446045, loss=1.5436053276062012
I0213 04:34:41.580040 140062483080960 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.572479248046875, loss=1.4485716819763184
I0213 04:36:03.256587 140062491473664 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.612815022468567, loss=1.4945826530456543
I0213 04:37:27.613405 140062483080960 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.032862424850464, loss=1.4689719676971436
I0213 04:38:54.699213 140062491473664 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.051431179046631, loss=1.5275633335113525
I0213 04:40:25.755887 140062483080960 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.994744062423706, loss=1.5492465496063232
I0213 04:41:54.530451 140062491473664 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.3277530670166016, loss=1.5301117897033691
I0213 04:43:22.182565 140062483080960 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.8974673748016357, loss=1.5678987503051758
I0213 04:44:49.936853 140062491473664 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.8818225860595703, loss=1.5471349954605103
I0213 04:46:21.693741 140062483080960 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.1268880367279053, loss=1.5089366436004639
I0213 04:47:50.185816 140062491473664 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.5421078205108643, loss=1.4991934299468994
I0213 04:49:08.966524 140062483080960 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.9994895458221436, loss=1.4687808752059937
I0213 04:50:28.650017 140062491473664 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.723695755004883, loss=1.4805136919021606
I0213 04:51:49.778245 140062483080960 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.194183826446533, loss=1.4863322973251343
I0213 04:53:12.386492 140218947737408 spec.py:321] Evaluating on the training split.
I0213 04:54:06.962641 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 04:54:59.028593 140218947737408 spec.py:349] Evaluating on the test split.
I0213 04:55:26.303314 140218947737408 submission_runner.py:408] Time since start: 15933.24s, 	Step: 16899, 	{'train/ctc_loss': Array(0.43455628, dtype=float32), 'train/wer': 0.1444184104049378, 'validation/ctc_loss': Array(0.59773993, dtype=float32), 'validation/wer': 0.1722390105911544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35192952, dtype=float32), 'test/wer': 0.11346048382182683, 'test/num_examples': 2472, 'score': 14421.737709760666, 'total_duration': 15933.23994922638, 'accumulated_submission_time': 14421.737709760666, 'accumulated_eval_time': 1510.1858768463135, 'accumulated_logging_time': 0.5034916400909424}
I0213 04:55:26.337997 140062491473664 logging_writer.py:48] [16899] accumulated_eval_time=1510.185877, accumulated_logging_time=0.503492, accumulated_submission_time=14421.737710, global_step=16899, preemption_count=0, score=14421.737710, test/ctc_loss=0.35192951560020447, test/num_examples=2472, test/wer=0.113460, total_duration=15933.239949, train/ctc_loss=0.4345562756061554, train/wer=0.144418, validation/ctc_loss=0.5977399349212646, validation/num_examples=5348, validation/wer=0.172239
I0213 04:55:27.945151 140062483080960 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.7593460083007812, loss=1.468843698501587
I0213 04:56:42.820075 140062491473664 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.781632661819458, loss=1.4913047552108765
I0213 04:57:58.478260 140062483080960 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.623720169067383, loss=1.4472687244415283
I0213 04:59:28.466647 140062491473664 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.019327640533447, loss=1.484925389289856
I0213 05:00:57.333343 140062483080960 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.9134063720703125, loss=1.4420878887176514
I0213 05:02:29.972886 140062491473664 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.9635744094848633, loss=1.4924187660217285
I0213 05:03:59.324395 140062483080960 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.90163254737854, loss=1.4651527404785156
I0213 05:05:23.396584 140062491473664 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.540172576904297, loss=1.4832450151443481
I0213 05:06:45.606012 140062483080960 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.5135565996170044, loss=1.4824507236480713
I0213 05:08:07.261958 140062491473664 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.7223639488220215, loss=1.4407764673233032
I0213 05:09:30.592339 140062483080960 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.8236098289489746, loss=1.4374101161956787
I0213 05:10:59.475800 140062491473664 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.6224722862243652, loss=1.5083043575286865
I0213 05:12:32.745764 140062483080960 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.8981049060821533, loss=1.520796298980713
I0213 05:14:04.099028 140062491473664 logging_writer.py:48] [18200] global_step=18200, grad_norm=5.410393714904785, loss=1.4440749883651733
I0213 05:15:34.362670 140062483080960 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.914170742034912, loss=1.5172772407531738
I0213 05:17:05.965281 140062491473664 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.148390769958496, loss=1.5137312412261963
I0213 05:18:36.314699 140062483080960 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.044664144515991, loss=1.5541263818740845
I0213 05:19:26.553417 140218947737408 spec.py:321] Evaluating on the training split.
I0213 05:20:22.110988 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 05:21:15.347251 140218947737408 spec.py:349] Evaluating on the test split.
I0213 05:21:41.857415 140218947737408 submission_runner.py:408] Time since start: 17508.79s, 	Step: 18554, 	{'train/ctc_loss': Array(0.42747203, dtype=float32), 'train/wer': 0.14028345598255657, 'validation/ctc_loss': Array(0.5999306, dtype=float32), 'validation/wer': 0.17292449095841742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34360522, dtype=float32), 'test/wer': 0.11045437003635773, 'test/num_examples': 2472, 'score': 15861.863068819046, 'total_duration': 17508.79342675209, 'accumulated_submission_time': 15861.863068819046, 'accumulated_eval_time': 1645.4841482639313, 'accumulated_logging_time': 0.5546281337738037}
I0213 05:21:41.905154 140062491473664 logging_writer.py:48] [18554] accumulated_eval_time=1645.484148, accumulated_logging_time=0.554628, accumulated_submission_time=15861.863069, global_step=18554, preemption_count=0, score=15861.863069, test/ctc_loss=0.3436052203178406, test/num_examples=2472, test/wer=0.110454, total_duration=17508.793427, train/ctc_loss=0.4274720251560211, train/wer=0.140283, validation/ctc_loss=0.5999305844306946, validation/num_examples=5348, validation/wer=0.172924
I0213 05:22:17.519355 140062483080960 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.635314702987671, loss=1.4548701047897339
I0213 05:23:32.849910 140062491473664 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.3644542694091797, loss=1.5016155242919922
I0213 05:24:47.761996 140062483080960 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.8532259464263916, loss=1.483191967010498
I0213 05:26:02.808125 140062491473664 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.3315041065216064, loss=1.503237009048462
I0213 05:27:25.585212 140062483080960 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.0118675231933594, loss=1.4997408390045166
I0213 05:28:56.091302 140062491473664 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.428699016571045, loss=1.4414821863174438
I0213 05:30:22.704745 140062483080960 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.2585577964782715, loss=1.4820870161056519
I0213 05:31:55.659501 140062491473664 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.996088981628418, loss=1.4813307523727417
I0213 05:33:25.393038 140062483080960 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.543515920639038, loss=1.476072072982788
I0213 05:34:54.812233 140062491473664 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7930731773376465, loss=1.4302771091461182
I0213 05:36:23.496737 140062491473664 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.2695491313934326, loss=1.4222781658172607
I0213 05:37:42.113468 140062483080960 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.158600091934204, loss=1.4975788593292236
I0213 05:39:02.746015 140062491473664 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.3164117336273193, loss=1.4679714441299438
I0213 05:40:24.353610 140062483080960 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.978175163269043, loss=1.4607139825820923
I0213 05:41:53.075499 140062491473664 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.5170724391937256, loss=1.4280892610549927
I0213 05:43:23.715240 140062483080960 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9577319622039795, loss=1.495890736579895
I0213 05:44:54.093989 140062491473664 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.915583848953247, loss=1.47588312625885
I0213 05:45:41.912430 140218947737408 spec.py:321] Evaluating on the training split.
I0213 05:46:36.107049 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 05:47:29.577280 140218947737408 spec.py:349] Evaluating on the test split.
I0213 05:47:55.989115 140218947737408 submission_runner.py:408] Time since start: 19082.92s, 	Step: 20255, 	{'train/ctc_loss': Array(0.4165472, dtype=float32), 'train/wer': 0.1373503718636462, 'validation/ctc_loss': Array(0.57209927, dtype=float32), 'validation/wer': 0.16542282553076454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32855484, dtype=float32), 'test/wer': 0.10513273617289216, 'test/num_examples': 2472, 'score': 17301.77611398697, 'total_duration': 19082.924453496933, 'accumulated_submission_time': 17301.77611398697, 'accumulated_eval_time': 1779.5542323589325, 'accumulated_logging_time': 0.6197030544281006}
I0213 05:47:56.025167 140062491473664 logging_writer.py:48] [20255] accumulated_eval_time=1779.554232, accumulated_logging_time=0.619703, accumulated_submission_time=17301.776114, global_step=20255, preemption_count=0, score=17301.776114, test/ctc_loss=0.32855483889579773, test/num_examples=2472, test/wer=0.105133, total_duration=19082.924453, train/ctc_loss=0.4165472090244293, train/wer=0.137350, validation/ctc_loss=0.5720992684364319, validation/num_examples=5348, validation/wer=0.165423
I0213 05:48:30.611315 140062483080960 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.3074145317077637, loss=1.477262020111084
I0213 05:49:46.092285 140062491473664 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.315167188644409, loss=1.4706445932388306
I0213 05:51:06.476823 140062483080960 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.5834145545959473, loss=1.5012177228927612
I0213 05:52:39.869928 140062491473664 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.069309711456299, loss=1.483813762664795
I0213 05:53:56.336345 140062483080960 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.2923219203948975, loss=1.4202914237976074
I0213 05:55:14.079000 140062491473664 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.506834030151367, loss=1.4653115272521973
I0213 05:56:35.483951 140062483080960 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.2439918518066406, loss=1.4329298734664917
I0213 05:58:02.226036 140062491473664 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.676276683807373, loss=1.4712871313095093
I0213 05:59:32.228867 140062483080960 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.5089144706726074, loss=1.4161051511764526
I0213 06:01:02.114508 140062491473664 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.492255687713623, loss=1.4232882261276245
I0213 06:02:30.603893 140062483080960 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.861952543258667, loss=1.4123767614364624
I0213 06:04:01.335193 140062491473664 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.8819122314453125, loss=1.4506841897964478
I0213 06:05:32.840183 140062483080960 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.9128077030181885, loss=1.4930241107940674
I0213 06:07:05.942608 140062491473664 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.6293301582336426, loss=1.4272252321243286
I0213 06:08:29.292550 140062491473664 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.469381093978882, loss=1.4338573217391968
I0213 06:09:48.463459 140062483080960 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.441314697265625, loss=1.4372471570968628
I0213 06:11:07.278322 140062491473664 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.307255506515503, loss=1.4330912828445435
I0213 06:11:56.234719 140218947737408 spec.py:321] Evaluating on the training split.
I0213 06:12:50.944158 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 06:13:42.617343 140218947737408 spec.py:349] Evaluating on the test split.
I0213 06:14:10.531408 140218947737408 submission_runner.py:408] Time since start: 20657.47s, 	Step: 21960, 	{'train/ctc_loss': Array(0.45802358, dtype=float32), 'train/wer': 0.14519755682835153, 'validation/ctc_loss': Array(0.5748465, dtype=float32), 'validation/wer': 0.16414841132683897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32678494, dtype=float32), 'test/wer': 0.10367030243942071, 'test/num_examples': 2472, 'score': 18741.892809152603, 'total_duration': 20657.470319747925, 'accumulated_submission_time': 18741.892809152603, 'accumulated_eval_time': 1913.8478994369507, 'accumulated_logging_time': 0.6720757484436035}
I0213 06:14:10.562341 140062491473664 logging_writer.py:48] [21960] accumulated_eval_time=1913.847899, accumulated_logging_time=0.672076, accumulated_submission_time=18741.892809, global_step=21960, preemption_count=0, score=18741.892809, test/ctc_loss=0.3267849385738373, test/num_examples=2472, test/wer=0.103670, total_duration=20657.470320, train/ctc_loss=0.4580235779285431, train/wer=0.145198, validation/ctc_loss=0.5748465061187744, validation/num_examples=5348, validation/wer=0.164148
I0213 06:14:41.639312 140062483080960 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.89813494682312, loss=1.45753812789917
I0213 06:15:56.518841 140062491473664 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.921259641647339, loss=1.4267773628234863
I0213 06:17:17.710289 140062483080960 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.716111421585083, loss=1.449222207069397
I0213 06:18:44.443333 140062491473664 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.1634464263916016, loss=1.3427547216415405
I0213 06:20:13.933238 140062483080960 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.311856985092163, loss=1.3846430778503418
I0213 06:21:43.630127 140062491473664 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.4116439819335938, loss=1.4380255937576294
I0213 06:23:17.023535 140062483080960 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.2996506690979004, loss=1.4652910232543945
I0213 06:24:44.521770 140062491473664 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.7114676237106323, loss=1.3867863416671753
I0213 06:26:02.896490 140062483080960 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.0983939170837402, loss=1.4066917896270752
I0213 06:27:22.133257 140062491473664 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.309527635574341, loss=1.465535283088684
I0213 06:28:43.406067 140062483080960 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.1994130611419678, loss=1.4106868505477905
I0213 06:30:11.018262 140062491473664 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.222837924957275, loss=1.4760401248931885
I0213 06:31:41.853470 140062483080960 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.9220622777938843, loss=1.437085747718811
I0213 06:33:08.385120 140062491473664 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.651474714279175, loss=1.412952184677124
I0213 06:34:34.584819 140062483080960 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.6184065341949463, loss=1.3948864936828613
I0213 06:36:06.754691 140062491473664 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.38421368598938, loss=1.3896552324295044
I0213 06:37:37.623936 140062483080960 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.4107799530029297, loss=1.4409575462341309
I0213 06:38:11.682164 140218947737408 spec.py:321] Evaluating on the training split.
I0213 06:39:06.909368 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 06:39:59.905185 140218947737408 spec.py:349] Evaluating on the test split.
I0213 06:40:27.289273 140218947737408 submission_runner.py:408] Time since start: 22234.23s, 	Step: 23637, 	{'train/ctc_loss': Array(0.36998138, dtype=float32), 'train/wer': 0.12440898834251919, 'validation/ctc_loss': Array(0.5380809, dtype=float32), 'validation/wer': 0.15562335267482164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31266233, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 20182.922751426697, 'total_duration': 22234.225139141083, 'accumulated_submission_time': 20182.922751426697, 'accumulated_eval_time': 2049.448930501938, 'accumulated_logging_time': 0.7180941104888916}
I0213 06:40:27.322881 140062645073664 logging_writer.py:48] [23637] accumulated_eval_time=2049.448931, accumulated_logging_time=0.718094, accumulated_submission_time=20182.922751, global_step=23637, preemption_count=0, score=20182.922751, test/ctc_loss=0.3126623332500458, test/num_examples=2472, test/wer=0.100502, total_duration=22234.225139, train/ctc_loss=0.3699813783168793, train/wer=0.124409, validation/ctc_loss=0.5380808711051941, validation/num_examples=5348, validation/wer=0.155623
I0213 06:41:19.636096 140062645073664 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.1878631114959717, loss=1.3674142360687256
I0213 06:42:38.455396 140062636680960 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.311896800994873, loss=1.381426215171814
I0213 06:43:59.355004 140062645073664 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.1974523067474365, loss=1.318935513496399
I0213 06:45:19.659074 140062636680960 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7703434228897095, loss=1.398817539215088
I0213 06:46:44.232032 140062645073664 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.260206460952759, loss=1.4365028142929077
I0213 06:48:14.821776 140062636680960 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.581181764602661, loss=1.390325665473938
I0213 06:49:45.014694 140062645073664 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.9723966121673584, loss=1.4166691303253174
I0213 06:51:14.364574 140062636680960 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.178689479827881, loss=1.4245336055755615
I0213 06:52:45.577206 140062645073664 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.1629695892333984, loss=1.4025447368621826
I0213 06:54:13.368491 140062636680960 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.7200325727462769, loss=1.3388965129852295
I0213 06:55:45.222812 140062645073664 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.289841413497925, loss=1.4164317846298218
I0213 06:57:08.711787 140062645073664 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.887059211730957, loss=1.2925851345062256
I0213 06:58:26.727315 140062636680960 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.5251102447509766, loss=1.3898441791534424
I0213 06:59:45.104348 140062645073664 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.4677162170410156, loss=1.3834606409072876
I0213 07:01:09.609708 140062636680960 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.869021415710449, loss=1.4489591121673584
I0213 07:02:37.094533 140062645073664 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.9675427675247192, loss=1.3866021633148193
I0213 07:04:09.517236 140062636680960 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.992570161819458, loss=1.3786176443099976
I0213 07:04:27.612178 140218947737408 spec.py:321] Evaluating on the training split.
I0213 07:05:21.392995 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 07:06:15.955792 140218947737408 spec.py:349] Evaluating on the test split.
I0213 07:06:43.133750 140218947737408 submission_runner.py:408] Time since start: 23810.07s, 	Step: 25321, 	{'train/ctc_loss': Array(0.3866852, dtype=float32), 'train/wer': 0.13302970526804364, 'validation/ctc_loss': Array(0.53082854, dtype=float32), 'validation/wer': 0.1544261756953764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30099636, dtype=float32), 'test/wer': 0.09682530010358906, 'test/num_examples': 2472, 'score': 21623.12057375908, 'total_duration': 23810.06982064247, 'accumulated_submission_time': 21623.12057375908, 'accumulated_eval_time': 2184.9646167755127, 'accumulated_logging_time': 0.7669081687927246}
I0213 07:06:43.174267 140063075153664 logging_writer.py:48] [25321] accumulated_eval_time=2184.964617, accumulated_logging_time=0.766908, accumulated_submission_time=21623.120574, global_step=25321, preemption_count=0, score=21623.120574, test/ctc_loss=0.3009963631629944, test/num_examples=2472, test/wer=0.096825, total_duration=23810.069821, train/ctc_loss=0.38668519258499146, train/wer=0.133030, validation/ctc_loss=0.5308285355567932, validation/num_examples=5348, validation/wer=0.154426
I0213 07:07:43.257142 140063066760960 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.568793773651123, loss=1.3764095306396484
I0213 07:08:59.008893 140063075153664 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.878805160522461, loss=1.4112507104873657
I0213 07:10:25.984152 140063066760960 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.5245325565338135, loss=1.3773976564407349
I0213 07:11:55.407719 140063075153664 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.6513891220092773, loss=1.3754515647888184
I0213 07:13:23.870650 140062419793664 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.842153787612915, loss=1.4303131103515625
I0213 07:14:41.646289 140062411400960 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.859649658203125, loss=1.4088126420974731
I0213 07:16:01.090857 140062419793664 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.1510190963745117, loss=1.4167914390563965
I0213 07:17:23.990436 140062411400960 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.117917537689209, loss=1.3771207332611084
I0213 07:18:51.834512 140062419793664 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.2649428844451904, loss=1.3555641174316406
I0213 07:20:22.319322 140062411400960 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7839308977127075, loss=1.313407063484192
I0213 07:21:54.989984 140062419793664 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.8003156185150146, loss=1.3608347177505493
I0213 07:23:24.944489 140062411400960 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.4909911155700684, loss=1.432616949081421
I0213 07:24:57.134646 140062419793664 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.2538504600524902, loss=1.3841629028320312
I0213 07:26:29.605608 140062411400960 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.9491499662399292, loss=1.3718549013137817
I0213 07:28:00.774366 140063075153664 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.8289998769760132, loss=1.3198541402816772
I0213 07:29:19.173579 140063066760960 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.7807791233062744, loss=1.3787062168121338
I0213 07:30:41.461707 140063075153664 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.3978185653686523, loss=1.3454018831253052
I0213 07:30:43.691516 140218947737408 spec.py:321] Evaluating on the training split.
I0213 07:31:39.045493 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 07:32:33.065266 140218947737408 spec.py:349] Evaluating on the test split.
I0213 07:33:00.161251 140218947737408 submission_runner.py:408] Time since start: 25387.10s, 	Step: 27004, 	{'train/ctc_loss': Array(0.3206794, dtype=float32), 'train/wer': 0.10802226606074261, 'validation/ctc_loss': Array(0.5141854, dtype=float32), 'validation/wer': 0.1482665070430694, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29252425, dtype=float32), 'test/wer': 0.09446915686632949, 'test/num_examples': 2472, 'score': 23063.543320417404, 'total_duration': 25387.0976998806, 'accumulated_submission_time': 23063.543320417404, 'accumulated_eval_time': 2321.4288444519043, 'accumulated_logging_time': 0.825782060623169}
I0213 07:33:00.206459 140063075153664 logging_writer.py:48] [27004] accumulated_eval_time=2321.428844, accumulated_logging_time=0.825782, accumulated_submission_time=23063.543320, global_step=27004, preemption_count=0, score=23063.543320, test/ctc_loss=0.2925242483615875, test/num_examples=2472, test/wer=0.094469, total_duration=25387.097700, train/ctc_loss=0.3206793963909149, train/wer=0.108022, validation/ctc_loss=0.5141854286193848, validation/num_examples=5348, validation/wer=0.148267
I0213 07:34:13.128803 140063066760960 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.5013153553009033, loss=1.2921777963638306
I0213 07:35:27.891032 140063075153664 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.8965117931365967, loss=1.313823938369751
I0213 07:36:55.186350 140063066760960 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.325678825378418, loss=1.346968650817871
I0213 07:38:26.970780 140063075153664 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.1509711742401123, loss=1.3836170434951782
I0213 07:39:55.344585 140063066760960 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.1266708374023438, loss=1.343035101890564
I0213 07:41:25.608227 140063075153664 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.9277961254119873, loss=1.3352257013320923
I0213 07:42:55.673152 140063066760960 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.514477491378784, loss=1.4021743535995483
I0213 07:44:27.752559 140063075153664 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.9312796592712402, loss=1.333028793334961
I0213 07:45:48.242432 140062419793664 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.652247428894043, loss=1.3617931604385376
I0213 07:47:08.575214 140062411400960 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.9888768196105957, loss=1.3196905851364136
I0213 07:48:30.661948 140062419793664 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.6055635213851929, loss=1.329074501991272
I0213 07:49:57.684180 140062411400960 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.805569052696228, loss=1.3328815698623657
I0213 07:51:30.468092 140062419793664 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.013360023498535, loss=1.2710622549057007
I0213 07:52:59.475277 140062411400960 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.1484766006469727, loss=1.3542394638061523
I0213 07:54:29.724715 140062419793664 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.1555662155151367, loss=1.3079164028167725
I0213 07:55:57.556899 140062411400960 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.6379623413085938, loss=1.3304022550582886
I0213 07:57:00.376864 140218947737408 spec.py:321] Evaluating on the training split.
I0213 07:57:54.152787 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 07:58:48.577256 140218947737408 spec.py:349] Evaluating on the test split.
I0213 07:59:15.497728 140218947737408 submission_runner.py:408] Time since start: 26962.43s, 	Step: 28672, 	{'train/ctc_loss': Array(0.34442776, dtype=float32), 'train/wer': 0.11347605984332136, 'validation/ctc_loss': Array(0.5025977, dtype=float32), 'validation/wer': 0.14449153769659287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28394303, dtype=float32), 'test/wer': 0.09199114415128065, 'test/num_examples': 2472, 'score': 24503.62016057968, 'total_duration': 26962.433699131012, 'accumulated_submission_time': 24503.62016057968, 'accumulated_eval_time': 2456.543736219406, 'accumulated_logging_time': 0.8892910480499268}
I0213 07:59:15.539724 140062419793664 logging_writer.py:48] [28672] accumulated_eval_time=2456.543736, accumulated_logging_time=0.889291, accumulated_submission_time=24503.620161, global_step=28672, preemption_count=0, score=24503.620161, test/ctc_loss=0.2839430272579193, test/num_examples=2472, test/wer=0.091991, total_duration=26962.433699, train/ctc_loss=0.34442776441574097, train/wer=0.113476, validation/ctc_loss=0.5025976896286011, validation/num_examples=5348, validation/wer=0.144492
I0213 07:59:37.833284 140062411400960 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.241623640060425, loss=1.3769726753234863
I0213 08:00:52.912564 140062419793664 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.812845230102539, loss=1.3384629487991333
I0213 08:02:14.631973 140062419793664 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.316174268722534, loss=1.3209437131881714
I0213 08:03:33.118832 140062411400960 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.5300991535186768, loss=1.3161993026733398
I0213 08:04:53.722691 140062419793664 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.4014134407043457, loss=1.2913764715194702
I0213 08:06:19.003216 140062411400960 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.155550479888916, loss=1.3227579593658447
I0213 08:07:46.223602 140062419793664 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.550412654876709, loss=1.3522025346755981
I0213 08:09:17.030553 140062411400960 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.1874725818634033, loss=1.3513953685760498
I0213 08:10:46.439177 140062419793664 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.1509475708007812, loss=1.3190032243728638
I0213 08:12:15.918087 140062411400960 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.5553646087646484, loss=1.3587125539779663
I0213 08:13:47.458905 140062419793664 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.1583292484283447, loss=1.2819894552230835
I0213 08:15:14.499835 140062411400960 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.281728982925415, loss=1.3173390626907349
I0213 08:16:43.582209 140063075153664 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.528208017349243, loss=1.3389325141906738
I0213 08:18:03.560750 140063066760960 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.450570821762085, loss=1.3073819875717163
I0213 08:19:25.894979 140063075153664 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.9556846618652344, loss=1.2707566022872925
I0213 08:20:49.480944 140063066760960 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.990920066833496, loss=1.2962379455566406
I0213 08:22:16.994491 140063075153664 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.6120810508728027, loss=1.31462562084198
I0213 08:23:16.017175 140218947737408 spec.py:321] Evaluating on the training split.
I0213 08:24:10.581497 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 08:25:04.077926 140218947737408 spec.py:349] Evaluating on the test split.
I0213 08:25:31.900970 140218947737408 submission_runner.py:408] Time since start: 28538.84s, 	Step: 30368, 	{'train/ctc_loss': Array(0.3366657, dtype=float32), 'train/wer': 0.11299470381768845, 'validation/ctc_loss': Array(0.48924, dtype=float32), 'validation/wer': 0.14052347528891548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27818447, dtype=float32), 'test/wer': 0.08855848719354904, 'test/num_examples': 2472, 'score': 25944.00483107567, 'total_duration': 28538.838061094284, 'accumulated_submission_time': 25944.00483107567, 'accumulated_eval_time': 2592.422735452652, 'accumulated_logging_time': 0.9475498199462891}
I0213 08:25:31.941955 140063075153664 logging_writer.py:48] [30368] accumulated_eval_time=2592.422735, accumulated_logging_time=0.947550, accumulated_submission_time=25944.004831, global_step=30368, preemption_count=0, score=25944.004831, test/ctc_loss=0.2781844735145569, test/num_examples=2472, test/wer=0.088558, total_duration=28538.838061, train/ctc_loss=0.33666568994522095, train/wer=0.112995, validation/ctc_loss=0.48923999071121216, validation/num_examples=5348, validation/wer=0.140523
I0213 08:25:56.843847 140063066760960 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.4790380001068115, loss=1.301284670829773
I0213 08:27:12.159665 140063075153664 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.6023792028427124, loss=1.2843016386032104
I0213 08:28:30.935985 140063066760960 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.8652592897415161, loss=1.2877137660980225
I0213 08:30:01.406927 140063075153664 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.6772758960723877, loss=1.2956188917160034
I0213 08:31:31.848642 140063066760960 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.6460564136505127, loss=1.3011808395385742
I0213 08:33:05.663621 140063075153664 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.6564210653305054, loss=1.2739157676696777
I0213 08:34:23.374949 140063066760960 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.971874713897705, loss=1.2577788829803467
I0213 08:35:41.691623 140063075153664 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.154374837875366, loss=1.2551665306091309
I0213 08:37:03.412322 140063066760960 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.3368172645568848, loss=1.295780062675476
I0213 08:38:28.960714 140063075153664 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.4090254306793213, loss=1.3305156230926514
I0213 08:39:57.955470 140063066760960 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.2875192165374756, loss=1.304681658744812
I0213 08:41:27.804369 140063075153664 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7559480667114258, loss=1.2692312002182007
I0213 08:42:58.411947 140063066760960 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.6094610691070557, loss=1.2658400535583496
I0213 08:44:28.203382 140063075153664 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.175816774368286, loss=1.2553088665008545
I0213 08:45:58.734948 140063066760960 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.6014790534973145, loss=1.3116568326950073
I0213 08:47:31.291543 140063075153664 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.5538930892944336, loss=1.274301290512085
I0213 08:48:54.970280 140062419793664 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.2674684524536133, loss=1.3202582597732544
I0213 08:49:32.005994 140218947737408 spec.py:321] Evaluating on the training split.
I0213 08:50:27.430230 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 08:51:20.537221 140218947737408 spec.py:349] Evaluating on the test split.
I0213 08:51:48.565013 140218947737408 submission_runner.py:408] Time since start: 30115.50s, 	Step: 32050, 	{'train/ctc_loss': Array(0.31884333, dtype=float32), 'train/wer': 0.10849346994415374, 'validation/ctc_loss': Array(0.4744674, dtype=float32), 'validation/wer': 0.1363912837792174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26197803, dtype=float32), 'test/wer': 0.0844758596876079, 'test/num_examples': 2472, 'score': 27383.977959632874, 'total_duration': 30115.500896692276, 'accumulated_submission_time': 27383.977959632874, 'accumulated_eval_time': 2728.975687980652, 'accumulated_logging_time': 1.0051610469818115}
I0213 08:51:48.600899 140063075153664 logging_writer.py:48] [32050] accumulated_eval_time=2728.975688, accumulated_logging_time=1.005161, accumulated_submission_time=27383.977960, global_step=32050, preemption_count=0, score=27383.977960, test/ctc_loss=0.26197803020477295, test/num_examples=2472, test/wer=0.084476, total_duration=30115.500897, train/ctc_loss=0.3188433349132538, train/wer=0.108493, validation/ctc_loss=0.474467396736145, validation/num_examples=5348, validation/wer=0.136391
I0213 08:52:26.898796 140063066760960 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.06360125541687, loss=1.3492356538772583
I0213 08:53:43.050537 140063075153664 logging_writer.py:48] [32200] global_step=32200, grad_norm=5.330936908721924, loss=1.2863352298736572
I0213 08:54:58.159872 140063066760960 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.437819004058838, loss=1.2655836343765259
I0213 08:56:20.605064 140063075153664 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.5743353366851807, loss=1.3249645233154297
I0213 08:57:49.930461 140063066760960 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.1291277408599854, loss=1.258089542388916
I0213 08:59:21.087157 140063075153664 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.9510602951049805, loss=1.2595255374908447
I0213 09:00:50.074975 140063066760960 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.0743250846862793, loss=1.2308615446090698
I0213 09:02:21.037591 140063075153664 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.9651377201080322, loss=1.3403277397155762
I0213 09:03:53.790064 140063066760960 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.462733268737793, loss=1.235679268836975
I0213 09:05:22.567007 140063075153664 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.2202670574188232, loss=1.2507085800170898
I0213 09:06:39.096486 140063066760960 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.9868969917297363, loss=1.225366234779358
I0213 09:07:58.455692 140063075153664 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.162100076675415, loss=1.2110130786895752
I0213 09:09:22.278433 140063066760960 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.0568065643310547, loss=1.2485390901565552
I0213 09:10:51.110429 140063075153664 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.701922655105591, loss=1.2700175046920776
I0213 09:12:22.266228 140063066760960 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.204134941101074, loss=1.299034595489502
I0213 09:13:51.802766 140063075153664 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.9835412502288818, loss=1.2593486309051514
I0213 09:15:23.967029 140063066760960 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.741156816482544, loss=1.2534302473068237
I0213 09:15:48.644534 140218947737408 spec.py:321] Evaluating on the training split.
I0213 09:16:42.966884 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 09:17:36.054892 140218947737408 spec.py:349] Evaluating on the test split.
I0213 09:18:02.995887 140218947737408 submission_runner.py:408] Time since start: 31689.93s, 	Step: 33728, 	{'train/ctc_loss': Array(0.26275682, dtype=float32), 'train/wer': 0.0906634322464177, 'validation/ctc_loss': Array(0.46272892, dtype=float32), 'validation/wer': 0.13340799598366432, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25633433, dtype=float32), 'test/wer': 0.08222127434850608, 'test/num_examples': 2472, 'score': 28823.93050980568, 'total_duration': 31689.932183027267, 'accumulated_submission_time': 28823.93050980568, 'accumulated_eval_time': 2863.3213727474213, 'accumulated_logging_time': 1.0570778846740723}
I0213 09:18:03.042562 140062783313664 logging_writer.py:48] [33728] accumulated_eval_time=2863.321373, accumulated_logging_time=1.057078, accumulated_submission_time=28823.930510, global_step=33728, preemption_count=0, score=28823.930510, test/ctc_loss=0.2563343346118927, test/num_examples=2472, test/wer=0.082221, total_duration=31689.932183, train/ctc_loss=0.2627568244934082, train/wer=0.090663, validation/ctc_loss=0.46272891759872437, validation/num_examples=5348, validation/wer=0.133408
I0213 09:18:58.277716 140062774920960 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.7618474960327148, loss=1.3101288080215454
I0213 09:20:13.232958 140062783313664 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.62363338470459, loss=1.220479130744934
I0213 09:21:42.003890 140062783313664 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.1571850776672363, loss=1.2646945714950562
I0213 09:23:00.532498 140062774920960 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.4555068016052246, loss=1.2342257499694824
I0213 09:24:18.569825 140062783313664 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.7468916177749634, loss=1.2831388711929321
I0213 09:25:40.261747 140062774920960 logging_writer.py:48] [34300] global_step=34300, grad_norm=4.1285786628723145, loss=1.2325578927993774
I0213 09:27:06.309983 140062783313664 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.6511170864105225, loss=1.2895694971084595
I0213 09:28:37.446291 140062774920960 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.0690746307373047, loss=1.2626392841339111
I0213 09:30:07.946876 140062783313664 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.8736743927001953, loss=1.236680030822754
I0213 09:31:36.023900 140062774920960 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7970433235168457, loss=1.240071415901184
I0213 09:33:06.966562 140062783313664 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.9718077182769775, loss=1.2559913396835327
I0213 09:34:36.312185 140062774920960 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.9346122741699219, loss=1.2940993309020996
I0213 09:36:05.200470 140062783313664 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.943481206893921, loss=1.29942786693573
I0213 09:37:27.862162 140062455633664 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.3711979389190674, loss=1.2221717834472656
I0213 09:38:47.171830 140062447240960 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.7940995693206787, loss=1.2416713237762451
I0213 09:40:07.719648 140062455633664 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.4908127784729004, loss=1.198793649673462
I0213 09:41:33.094514 140062447240960 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.956739902496338, loss=1.2406597137451172
I0213 09:42:03.002877 140218947737408 spec.py:321] Evaluating on the training split.
I0213 09:42:55.873844 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 09:43:50.483624 140218947737408 spec.py:349] Evaluating on the test split.
I0213 09:44:17.626052 140218947737408 submission_runner.py:408] Time since start: 33264.56s, 	Step: 35434, 	{'train/ctc_loss': Array(0.23715778, dtype=float32), 'train/wer': 0.0822559468865204, 'validation/ctc_loss': Array(0.45252073, dtype=float32), 'validation/wer': 0.13064676520849222, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24987762, dtype=float32), 'test/wer': 0.08037292060203521, 'test/num_examples': 2472, 'score': 30263.796778678894, 'total_duration': 33264.56129407883, 'accumulated_submission_time': 30263.796778678894, 'accumulated_eval_time': 2997.9378378391266, 'accumulated_logging_time': 1.1216213703155518}
I0213 09:44:17.666613 140063075153664 logging_writer.py:48] [35434] accumulated_eval_time=2997.937838, accumulated_logging_time=1.121621, accumulated_submission_time=30263.796779, global_step=35434, preemption_count=0, score=30263.796779, test/ctc_loss=0.24987761676311493, test/num_examples=2472, test/wer=0.080373, total_duration=33264.561294, train/ctc_loss=0.23715777695178986, train/wer=0.082256, validation/ctc_loss=0.4525207281112671, validation/num_examples=5348, validation/wer=0.130647
I0213 09:45:07.954917 140063066760960 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.3996663093566895, loss=1.2035706043243408
I0213 09:46:22.924127 140063075153664 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.0465941429138184, loss=1.220994472503662
I0213 09:47:49.396152 140063066760960 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.9488319158554077, loss=1.214857816696167
I0213 09:49:19.127073 140063075153664 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.9091029167175293, loss=1.2398611307144165
I0213 09:50:49.030469 140063066760960 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.2966222763061523, loss=1.248620867729187
I0213 09:52:20.342486 140063075153664 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.8471343517303467, loss=1.200826644897461
I0213 09:53:47.060005 140062419793664 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.187443256378174, loss=1.1926237344741821
I0213 09:55:04.492983 140062411400960 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.181226968765259, loss=1.27165949344635
I0213 09:56:27.305732 140062419793664 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.8928148746490479, loss=1.205456256866455
I0213 09:57:52.640124 140062411400960 logging_writer.py:48] [36400] global_step=36400, grad_norm=5.675743103027344, loss=1.1901732683181763
I0213 09:59:23.745151 140062419793664 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.5565810203552246, loss=1.1767016649246216
I0213 10:00:50.111720 140062411400960 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.469177007675171, loss=1.186784267425537
I0213 10:02:21.002521 140062419793664 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.8420766592025757, loss=1.1729607582092285
I0213 10:03:50.323900 140062411400960 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.193924903869629, loss=1.147230863571167
I0213 10:05:22.324490 140062419793664 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.3896138668060303, loss=1.1890672445297241
I0213 10:06:53.309779 140062411400960 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.6170135736465454, loss=1.2025705575942993
I0213 10:08:17.763795 140218947737408 spec.py:321] Evaluating on the training split.
I0213 10:09:12.334925 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 10:10:05.176564 140218947737408 spec.py:349] Evaluating on the test split.
I0213 10:10:31.706837 140218947737408 submission_runner.py:408] Time since start: 34838.64s, 	Step: 37095, 	{'train/ctc_loss': Array(0.2662122, dtype=float32), 'train/wer': 0.09212208194111184, 'validation/ctc_loss': Array(0.4400807, dtype=float32), 'validation/wer': 0.1263697539028935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2403281, dtype=float32), 'test/wer': 0.0776917920906709, 'test/num_examples': 2472, 'score': 31703.80060362816, 'total_duration': 34838.64279150963, 'accumulated_submission_time': 31703.80060362816, 'accumulated_eval_time': 3131.8751130104065, 'accumulated_logging_time': 1.180323600769043}
I0213 10:10:31.742265 140062419793664 logging_writer.py:48] [37095] accumulated_eval_time=3131.875113, accumulated_logging_time=1.180324, accumulated_submission_time=31703.800604, global_step=37095, preemption_count=0, score=31703.800604, test/ctc_loss=0.2403281033039093, test/num_examples=2472, test/wer=0.077692, total_duration=34838.642792, train/ctc_loss=0.26621219515800476, train/wer=0.092122, validation/ctc_loss=0.4400807023048401, validation/num_examples=5348, validation/wer=0.126370
I0213 10:10:36.358435 140062411400960 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.336183547973633, loss=1.1661473512649536
I0213 10:11:51.542945 140062419793664 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.9316827058792114, loss=1.1321003437042236
I0213 10:13:07.586213 140062411400960 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.3031058311462402, loss=1.2532219886779785
I0213 10:14:22.717201 140062419793664 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.5597245693206787, loss=1.2742499113082886
I0213 10:15:45.262802 140062411400960 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.6544251441955566, loss=1.1502381563186646
I0213 10:17:14.097461 140062419793664 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.0734949111938477, loss=1.225051999092102
I0213 10:18:44.219252 140062411400960 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.5894598960876465, loss=1.1588855981826782
I0213 10:20:14.280894 140062419793664 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.8835221529006958, loss=1.2097727060317993
I0213 10:21:43.783877 140062411400960 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.1896870136260986, loss=1.210874319076538
I0213 10:23:15.399634 140062419793664 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.1082603931427, loss=1.196885108947754
I0213 10:24:43.762750 140062411400960 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.183546543121338, loss=1.1932542324066162
I0213 10:26:04.577040 140063075153664 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.348595380783081, loss=1.1775479316711426
I0213 10:27:22.707411 140063066760960 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.2433900833129883, loss=1.1399569511413574
I0213 10:28:44.948542 140063075153664 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.2114627361297607, loss=1.1909892559051514
I0213 10:30:07.515003 140063066760960 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.5765256881713867, loss=1.14115309715271
I0213 10:31:37.193423 140063075153664 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.9615416526794434, loss=1.1922364234924316
I0213 10:33:08.093526 140063066760960 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.9555089473724365, loss=1.2159698009490967
I0213 10:34:32.372375 140218947737408 spec.py:321] Evaluating on the training split.
I0213 10:35:26.606962 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 10:36:20.995433 140218947737408 spec.py:349] Evaluating on the test split.
I0213 10:36:47.382468 140218947737408 submission_runner.py:408] Time since start: 36414.32s, 	Step: 38792, 	{'train/ctc_loss': Array(0.23776919, dtype=float32), 'train/wer': 0.08161365662127247, 'validation/ctc_loss': Array(0.43106434, dtype=float32), 'validation/wer': 0.12449675120924529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23511161, dtype=float32), 'test/wer': 0.07533564885341133, 'test/num_examples': 2472, 'score': 33144.33669090271, 'total_duration': 36414.3170132637, 'accumulated_submission_time': 33144.33669090271, 'accumulated_eval_time': 3266.8777928352356, 'accumulated_logging_time': 1.2335264682769775}
I0213 10:36:47.421817 140063075153664 logging_writer.py:48] [38792] accumulated_eval_time=3266.877793, accumulated_logging_time=1.233526, accumulated_submission_time=33144.336691, global_step=38792, preemption_count=0, score=33144.336691, test/ctc_loss=0.23511160910129547, test/num_examples=2472, test/wer=0.075336, total_duration=36414.317013, train/ctc_loss=0.23776918649673462, train/wer=0.081614, validation/ctc_loss=0.43106433749198914, validation/num_examples=5348, validation/wer=0.124497
I0213 10:36:54.393699 140063066760960 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.205559730529785, loss=1.2253611087799072
I0213 10:38:10.198526 140063075153664 logging_writer.py:48] [38900] global_step=38900, grad_norm=4.8729248046875, loss=1.226258635520935
I0213 10:39:25.506911 140063066760960 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.331059694290161, loss=1.175147533416748
I0213 10:40:56.182542 140063075153664 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.803906798362732, loss=1.1534587144851685
I0213 10:42:22.030818 140062419793664 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.093341827392578, loss=1.230281114578247
I0213 10:43:38.105468 140062411400960 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.6893317699432373, loss=1.2079654932022095
I0213 10:45:00.815227 140062419793664 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.553830146789551, loss=1.1890798807144165
I0213 10:46:23.007889 140062411400960 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.481144905090332, loss=1.1726943254470825
I0213 10:47:50.767711 140062419793664 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.867782711982727, loss=1.123473882675171
I0213 10:49:22.368541 140062411400960 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7503324747085571, loss=1.1991419792175293
I0213 10:50:52.864625 140062419793664 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.8347432613372803, loss=1.207425594329834
I0213 10:52:22.469495 140062411400960 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.725566864013672, loss=1.2384365797042847
I0213 10:53:53.187973 140062419793664 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.2800393104553223, loss=1.1819840669631958
I0213 10:55:25.307168 140062411400960 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.312507390975952, loss=1.205406904220581
I0213 10:56:55.951545 140063075153664 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.9404571056365967, loss=1.1983771324157715
I0213 10:58:12.081092 140063066760960 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.5420907735824585, loss=1.1824244260787964
I0213 10:59:31.301342 140063075153664 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.299628734588623, loss=1.2383344173431396
I0213 11:00:48.099185 140218947737408 spec.py:321] Evaluating on the training split.
I0213 11:01:43.178714 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 11:02:36.868619 140218947737408 spec.py:349] Evaluating on the test split.
I0213 11:03:05.095111 140218947737408 submission_runner.py:408] Time since start: 37992.03s, 	Step: 40493, 	{'train/ctc_loss': Array(0.245853, dtype=float32), 'train/wer': 0.08215494866687519, 'validation/ctc_loss': Array(0.42453703, dtype=float32), 'validation/wer': 0.1226913310870174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2303546, dtype=float32), 'test/wer': 0.07395446143846607, 'test/num_examples': 2472, 'score': 34584.92214655876, 'total_duration': 37992.03388214111, 'accumulated_submission_time': 34584.92214655876, 'accumulated_eval_time': 3403.8705430030823, 'accumulated_logging_time': 1.2884671688079834}
I0213 11:03:05.136490 140063075153664 logging_writer.py:48] [40493] accumulated_eval_time=3403.870543, accumulated_logging_time=1.288467, accumulated_submission_time=34584.922147, global_step=40493, preemption_count=0, score=34584.922147, test/ctc_loss=0.23035460710525513, test/num_examples=2472, test/wer=0.073954, total_duration=37992.033882, train/ctc_loss=0.2458530068397522, train/wer=0.082155, validation/ctc_loss=0.4245370328426361, validation/num_examples=5348, validation/wer=0.122691
I0213 11:03:11.334454 140063066760960 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.6214957237243652, loss=1.1687091588974
I0213 11:04:26.647737 140063075153664 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.7395962476730347, loss=1.1584070920944214
I0213 11:05:42.324982 140063066760960 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.150782585144043, loss=1.126476526260376
I0213 11:07:09.788574 140063075153664 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.5817363262176514, loss=1.1800663471221924
I0213 11:08:41.114225 140063066760960 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.4159326553344727, loss=1.1689151525497437
I0213 11:10:11.441396 140063075153664 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.003809690475464, loss=1.0973221063613892
I0213 11:11:39.523806 140063066760960 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.241854190826416, loss=1.161218285560608
I0213 11:13:13.787303 140063075153664 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.7508327960968018, loss=1.1357700824737549
I0213 11:14:31.838658 140063066760960 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.6633007526397705, loss=1.159368872642517
I0213 11:15:50.659110 140063075153664 logging_writer.py:48] [41400] global_step=41400, grad_norm=4.583046913146973, loss=1.1285004615783691
I0213 11:17:13.790352 140063066760960 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.7472996711730957, loss=1.1380895376205444
I0213 11:18:37.686340 140063075153664 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.697176933288574, loss=1.1695116758346558
I0213 11:20:08.322013 140063066760960 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.3037686347961426, loss=1.1819857358932495
I0213 11:21:38.124066 140063075153664 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.3967394828796387, loss=1.1236377954483032
I0213 11:23:08.274967 140063066760960 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.8711681365966797, loss=1.1168479919433594
I0213 11:24:36.416049 140063075153664 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9421112537384033, loss=1.1994069814682007
I0213 11:26:06.532109 140063066760960 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.779494047164917, loss=1.1871037483215332
I0213 11:27:05.907001 140218947737408 spec.py:321] Evaluating on the training split.
I0213 11:28:01.445434 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 11:28:54.172638 140218947737408 spec.py:349] Evaluating on the test split.
I0213 11:29:21.993833 140218947737408 submission_runner.py:408] Time since start: 39568.93s, 	Step: 42166, 	{'train/ctc_loss': Array(0.19963317, dtype=float32), 'train/wer': 0.06902416748245821, 'validation/ctc_loss': Array(0.41599953, dtype=float32), 'validation/wer': 0.11988182704654508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683771, dtype=float32), 'test/wer': 0.07224828874941604, 'test/num_examples': 2472, 'score': 36025.603747844696, 'total_duration': 39568.92971038818, 'accumulated_submission_time': 36025.603747844696, 'accumulated_eval_time': 3539.95130443573, 'accumulated_logging_time': 1.3435046672821045}
I0213 11:29:22.035073 140063075153664 logging_writer.py:48] [42166] accumulated_eval_time=3539.951304, accumulated_logging_time=1.343505, accumulated_submission_time=36025.603748, global_step=42166, preemption_count=0, score=36025.603748, test/ctc_loss=0.22683770954608917, test/num_examples=2472, test/wer=0.072248, total_duration=39568.929710, train/ctc_loss=0.1996331661939621, train/wer=0.069024, validation/ctc_loss=0.41599953174591064, validation/num_examples=5348, validation/wer=0.119882
I0213 11:29:48.280640 140063066760960 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7416813373565674, loss=1.1055902242660522
I0213 11:31:10.084444 140063075153664 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.526549816131592, loss=1.1254130601882935
I0213 11:32:26.488987 140063066760960 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.4563326835632324, loss=1.1540731191635132
I0213 11:33:48.867206 140063075153664 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.926795244216919, loss=1.1814868450164795
I0213 11:35:11.230269 140063066760960 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.26053524017334, loss=1.0751992464065552
I0213 11:36:41.089680 140063075153664 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.3166420459747314, loss=1.1804172992706299
I0213 11:38:12.304203 140063066760960 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.881025791168213, loss=1.1641467809677124
I0213 11:39:42.915114 140063075153664 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.877114772796631, loss=1.146735668182373
I0213 11:41:09.290620 140063066760960 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8070698976516724, loss=1.2093274593353271
I0213 11:42:40.177240 140063075153664 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.1196298599243164, loss=1.1481679677963257
I0213 11:44:10.361238 140063066760960 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.7274417877197266, loss=1.1212568283081055
I0213 11:45:37.284640 140062419793664 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.023648738861084, loss=1.144303560256958
I0213 11:46:56.741952 140062411400960 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.0204672813415527, loss=1.1309200525283813
I0213 11:48:15.535091 140062419793664 logging_writer.py:48] [43500] global_step=43500, grad_norm=6.681983947753906, loss=1.195360541343689
I0213 11:49:34.510238 140062411400960 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.5208613872528076, loss=1.1541523933410645
I0213 11:51:01.431475 140062419793664 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.866675615310669, loss=1.1300437450408936
I0213 11:52:31.636975 140062411400960 logging_writer.py:48] [43800] global_step=43800, grad_norm=9.776021957397461, loss=1.1217418909072876
I0213 11:53:22.244407 140218947737408 spec.py:321] Evaluating on the training split.
I0213 11:54:16.666276 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 11:55:09.202449 140218947737408 spec.py:349] Evaluating on the test split.
I0213 11:55:36.205000 140218947737408 submission_runner.py:408] Time since start: 41143.14s, 	Step: 43858, 	{'train/ctc_loss': Array(0.19884929, dtype=float32), 'train/wer': 0.06965301939584159, 'validation/ctc_loss': Array(0.41424644, dtype=float32), 'validation/wer': 0.11921565598540217, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22416562, dtype=float32), 'test/wer': 0.0712123981882071, 'test/num_examples': 2472, 'score': 37465.7214448452, 'total_duration': 41143.13983345032, 'accumulated_submission_time': 37465.7214448452, 'accumulated_eval_time': 3673.9047722816467, 'accumulated_logging_time': 1.4003708362579346}
I0213 11:55:36.251296 140062419793664 logging_writer.py:48] [43858] accumulated_eval_time=3673.904772, accumulated_logging_time=1.400371, accumulated_submission_time=37465.721445, global_step=43858, preemption_count=0, score=37465.721445, test/ctc_loss=0.22416561841964722, test/num_examples=2472, test/wer=0.071212, total_duration=41143.139833, train/ctc_loss=0.19884929060935974, train/wer=0.069653, validation/ctc_loss=0.41424643993377686, validation/num_examples=5348, validation/wer=0.119216
I0213 11:56:08.626702 140062411400960 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.5643515586853027, loss=1.126248836517334
I0213 11:57:24.435928 140062419793664 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8983699083328247, loss=1.134223461151123
I0213 11:58:43.387788 140062411400960 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.719872236251831, loss=1.154719352722168
I0213 12:00:13.206679 140062419793664 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.5709733963012695, loss=1.1706851720809937
I0213 12:01:46.539012 140062419793664 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.615109443664551, loss=1.120150089263916
I0213 12:03:03.245478 140062411400960 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.3254470825195312, loss=1.106425166130066
I0213 12:04:23.517039 140062419793664 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.5532751083374023, loss=1.1273353099822998
I0213 12:05:42.813337 140062411400960 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.6544859409332275, loss=1.1209638118743896
I0213 12:07:07.449361 140062419793664 logging_writer.py:48] [44700] global_step=44700, grad_norm=6.709338665008545, loss=1.179887056350708
I0213 12:08:37.404603 140062411400960 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.8456391096115112, loss=1.161547303199768
I0213 12:10:09.324948 140062419793664 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.5577359199523926, loss=1.1152288913726807
I0213 12:11:40.213602 140062411400960 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.39836311340332, loss=1.1580779552459717
I0213 12:13:13.600088 140062419793664 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.0388779640197754, loss=1.1476153135299683
I0213 12:14:44.606469 140062411400960 logging_writer.py:48] [45200] global_step=45200, grad_norm=5.875288009643555, loss=1.1552075147628784
I0213 12:16:13.517445 140062419793664 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.3726935386657715, loss=1.1632367372512817
I0213 12:17:36.846409 140062419793664 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.2580013275146484, loss=1.0750840902328491
I0213 12:18:52.207014 140062411400960 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.625054121017456, loss=1.1901085376739502
I0213 12:19:36.260614 140218947737408 spec.py:321] Evaluating on the training split.
I0213 12:20:29.796943 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 12:21:22.597098 140218947737408 spec.py:349] Evaluating on the test split.
I0213 12:21:50.743252 140218947737408 submission_runner.py:408] Time since start: 42717.68s, 	Step: 45559, 	{'train/ctc_loss': Array(0.21639155, dtype=float32), 'train/wer': 0.07584706425101238, 'validation/ctc_loss': Array(0.40952334, dtype=float32), 'validation/wer': 0.11805709761819709, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22256012, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 38905.63683629036, 'total_duration': 42717.67965936661, 'accumulated_submission_time': 38905.63683629036, 'accumulated_eval_time': 3808.381865978241, 'accumulated_logging_time': 1.463677167892456}
I0213 12:21:50.781748 140062419793664 logging_writer.py:48] [45559] accumulated_eval_time=3808.381866, accumulated_logging_time=1.463677, accumulated_submission_time=38905.636836, global_step=45559, preemption_count=0, score=38905.636836, test/ctc_loss=0.2225601226091385, test/num_examples=2472, test/wer=0.070867, total_duration=42717.679659, train/ctc_loss=0.21639154851436615, train/wer=0.075847, validation/ctc_loss=0.4095233380794525, validation/num_examples=5348, validation/wer=0.118057
I0213 12:22:22.864992 140062411400960 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.88907790184021, loss=1.1844239234924316
I0213 12:23:37.910395 140062419793664 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.014709234237671, loss=1.1109675168991089
I0213 12:24:53.005791 140062411400960 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.3497872352600098, loss=1.1420018672943115
I0213 12:26:19.731721 140062419793664 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.3332571983337402, loss=1.1309856176376343
I0213 12:27:50.898826 140062411400960 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.555143356323242, loss=1.178215503692627
I0213 12:29:22.507145 140062419793664 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.033644676208496, loss=1.122721552848816
I0213 12:30:51.581468 140062411400960 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.572377920150757, loss=1.1155178546905518
I0213 12:32:21.455037 140062419793664 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.1547038555145264, loss=1.116888403892517
I0213 12:33:47.580177 140063075153664 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.3434090614318848, loss=1.1626161336898804
I0213 12:35:02.885606 140063066760960 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.6067299842834473, loss=1.1908226013183594
I0213 12:36:20.472272 140063075153664 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.589470863342285, loss=1.158323049545288
I0213 12:37:40.691877 140063066760960 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.2103111743927, loss=1.179595947265625
I0213 12:39:06.286293 140063075153664 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.0047736167907715, loss=1.105790376663208
I0213 12:40:36.088115 140063066760960 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.9321478605270386, loss=1.1593838930130005
I0213 12:42:04.271030 140063075153664 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.376549243927002, loss=1.1077394485473633
I0213 12:43:33.243524 140063066760960 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.6833522319793701, loss=1.1074891090393066
I0213 12:45:03.936670 140063075153664 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.136232852935791, loss=1.1525959968566895
I0213 12:45:51.477916 140218947737408 spec.py:321] Evaluating on the training split.
I0213 12:46:54.713264 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 12:47:47.351023 140218947737408 spec.py:349] Evaluating on the test split.
I0213 12:48:14.302619 140218947737408 submission_runner.py:408] Time since start: 44301.24s, 	Step: 47255, 	{'train/ctc_loss': Array(0.14392106, dtype=float32), 'train/wer': 0.05145027250170652, 'validation/ctc_loss': Array(0.40911275, dtype=float32), 'validation/wer': 0.11772883941415566, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22253822, dtype=float32), 'test/wer': 0.07076554343631304, 'test/num_examples': 2472, 'score': 40346.240510463715, 'total_duration': 44301.23940682411, 'accumulated_submission_time': 40346.240510463715, 'accumulated_eval_time': 3951.201397418976, 'accumulated_logging_time': 1.5179204940795898}
I0213 12:48:14.344953 140063075153664 logging_writer.py:48] [47255] accumulated_eval_time=3951.201397, accumulated_logging_time=1.517920, accumulated_submission_time=40346.240510, global_step=47255, preemption_count=0, score=40346.240510, test/ctc_loss=0.22253821790218353, test/num_examples=2472, test/wer=0.070766, total_duration=44301.239407, train/ctc_loss=0.14392106235027313, train/wer=0.051450, validation/ctc_loss=0.40911275148391724, validation/num_examples=5348, validation/wer=0.117729
I0213 12:48:49.481111 140063066760960 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.450357437133789, loss=1.139459490776062
I0213 12:50:08.176855 140062419793664 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.367508888244629, loss=1.1401917934417725
I0213 12:51:25.978367 140062411400960 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.0219357013702393, loss=1.1041359901428223
I0213 12:52:42.221265 140062419793664 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.7378664016723633, loss=1.097608208656311
I0213 12:54:04.811398 140062411400960 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.44454288482666, loss=1.143659234046936
I0213 12:55:30.364581 140062419793664 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.8784186840057373, loss=1.1606178283691406
I0213 12:57:01.152064 140062411400960 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.5054430961608887, loss=1.1507067680358887
I0213 12:58:31.730279 140218947737408 spec.py:321] Evaluating on the training split.
I0213 12:59:27.252877 140218947737408 spec.py:333] Evaluating on the validation split.
I0213 13:00:21.904277 140218947737408 spec.py:349] Evaluating on the test split.
I0213 13:00:48.451936 140218947737408 submission_runner.py:408] Time since start: 45055.39s, 	Step: 48000, 	{'train/ctc_loss': Array(0.13062766, dtype=float32), 'train/wer': 0.046137194585132935, 'validation/ctc_loss': Array(0.40922388, dtype=float32), 'validation/wer': 0.11775780337333577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22244853, dtype=float32), 'test/wer': 0.07060305079926066, 'test/num_examples': 2472, 'score': 40963.57310843468, 'total_duration': 45055.39094734192, 'accumulated_submission_time': 40963.57310843468, 'accumulated_eval_time': 4087.9201798439026, 'accumulated_logging_time': 1.5780909061431885}
I0213 13:00:48.494841 140062419793664 logging_writer.py:48] [48000] accumulated_eval_time=4087.920180, accumulated_logging_time=1.578091, accumulated_submission_time=40963.573108, global_step=48000, preemption_count=0, score=40963.573108, test/ctc_loss=0.22244852781295776, test/num_examples=2472, test/wer=0.070603, total_duration=45055.390947, train/ctc_loss=0.13062766194343567, train/wer=0.046137, validation/ctc_loss=0.40922388434410095, validation/num_examples=5348, validation/wer=0.117758
I0213 13:00:48.516875 140062411400960 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40963.573108
I0213 13:00:48.757059 140218947737408 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 13:00:49.765725 140218947737408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5/checkpoint_48000
I0213 13:00:49.784939 140218947737408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/librispeech_deepspeech_jax/trial_5/checkpoint_48000.
I0213 13:00:50.884143 140218947737408 submission_runner.py:583] Tuning trial 5/5
I0213 13:00:50.884425 140218947737408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0213 13:00:50.899971 140218947737408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(28.652134, dtype=float32), 'train/wer': 2.3640764239661616, 'validation/ctc_loss': Array(28.14943, dtype=float32), 'validation/wer': 2.1854755399364723, 'validation/num_examples': 5348, 'test/ctc_loss': Array(28.256365, dtype=float32), 'test/wer': 2.3440578473787905, 'test/num_examples': 2472, 'score': 16.60268473625183, 'total_duration': 179.64115977287292, 'accumulated_submission_time': 16.60268473625183, 'accumulated_eval_time': 163.03839111328125, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1692, {'train/ctc_loss': Array(2.7661793, dtype=float32), 'train/wer': 0.616729432815946, 'validation/ctc_loss': Array(3.1783566, dtype=float32), 'validation/wer': 0.6568639755930371, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6875026, dtype=float32), 'test/wer': 0.5876749334795767, 'test/num_examples': 2472, 'score': 1457.0688242912292, 'total_duration': 1750.7331321239471, 'accumulated_submission_time': 1457.0688242912292, 'accumulated_eval_time': 293.5546774864197, 'accumulated_logging_time': 0.03145146369934082, 'global_step': 1692, 'preemption_count': 0}), (3396, {'train/ctc_loss': Array(0.77670133, dtype=float32), 'train/wer': 0.2395568610625213, 'validation/ctc_loss': Array(0.9372019, dtype=float32), 'validation/wer': 0.2630506772739122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.61222464, dtype=float32), 'test/wer': 0.19214754331444356, 'test/num_examples': 2472, 'score': 2897.5816576480865, 'total_duration': 3326.616904258728, 'accumulated_submission_time': 2897.5816576480865, 'accumulated_eval_time': 428.7920391559601, 'accumulated_logging_time': 0.08203959465026855, 'global_step': 3396, 'preemption_count': 0}), (5071, {'train/ctc_loss': Array(0.67181903, dtype=float32), 'train/wer': 0.21078466440312346, 'validation/ctc_loss': Array(0.81510663, dtype=float32), 'validation/wer': 0.2328991957674001, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5147943, dtype=float32), 'test/wer': 0.16588467085085207, 'test/num_examples': 2472, 'score': 4338.7473294734955, 'total_duration': 4903.404318571091, 'accumulated_submission_time': 4338.7473294734955, 'accumulated_eval_time': 564.2852621078491, 'accumulated_logging_time': 0.13069558143615723, 'global_step': 5071, 'preemption_count': 0}), (6744, {'train/ctc_loss': Array(0.7183804, dtype=float32), 'train/wer': 0.2258982279304178, 'validation/ctc_loss': Array(0.7535185, dtype=float32), 'validation/wer': 0.2154146190756635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45724174, dtype=float32), 'test/wer': 0.14762456076209046, 'test/num_examples': 2472, 'score': 5778.721389293671, 'total_duration': 6477.741781234741, 'accumulated_submission_time': 5778.721389293671, 'accumulated_eval_time': 698.5102126598358, 'accumulated_logging_time': 0.18908905982971191, 'global_step': 6744, 'preemption_count': 0}), (8435, {'train/ctc_loss': Array(0.58081234, dtype=float32), 'train/wer': 0.18172251712929402, 'validation/ctc_loss': Array(0.719909, dtype=float32), 'validation/wer': 0.2052289600973189, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4333417, dtype=float32), 'test/wer': 0.13846403834826235, 'test/num_examples': 2472, 'score': 7219.063415050507, 'total_duration': 8052.558868169785, 'accumulated_submission_time': 7219.063415050507, 'accumulated_eval_time': 832.8487877845764, 'accumulated_logging_time': 0.24363970756530762, 'global_step': 8435, 'preemption_count': 0}), (10111, {'train/ctc_loss': Array(0.57047266, dtype=float32), 'train/wer': 0.18313599708829517, 'validation/ctc_loss': Array(0.67904717, dtype=float32), 'validation/wer': 0.19588325593519798, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4085987, dtype=float32), 'test/wer': 0.13123311599943127, 'test/num_examples': 2472, 'score': 8659.439930438995, 'total_duration': 9627.739500045776, 'accumulated_submission_time': 8659.439930438995, 'accumulated_eval_time': 967.5219428539276, 'accumulated_logging_time': 0.2929205894470215, 'global_step': 10111, 'preemption_count': 0}), (11807, {'train/ctc_loss': Array(0.43884623, dtype=float32), 'train/wer': 0.14534793017188047, 'validation/ctc_loss': Array(0.6524572, dtype=float32), 'validation/wer': 0.18889328711972736, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39502275, dtype=float32), 'test/wer': 0.1270489305953324, 'test/num_examples': 2472, 'score': 10100.390930175781, 'total_duration': 11206.082193851471, 'accumulated_submission_time': 10100.390930175781, 'accumulated_eval_time': 1104.7788660526276, 'accumulated_logging_time': 0.3447701930999756, 'global_step': 11807, 'preemption_count': 0}), (13483, {'train/ctc_loss': Array(0.5149647, dtype=float32), 'train/wer': 0.16686553766324755, 'validation/ctc_loss': Array(0.643958, dtype=float32), 'validation/wer': 0.1846355851202487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37856, dtype=float32), 'test/wer': 0.12428655576544188, 'test/num_examples': 2472, 'score': 11540.737623214722, 'total_duration': 12781.498124361038, 'accumulated_submission_time': 11540.737623214722, 'accumulated_eval_time': 1239.711059808731, 'accumulated_logging_time': 0.4017910957336426, 'global_step': 13483, 'preemption_count': 0}), (15183, {'train/ctc_loss': Array(0.4569584, dtype=float32), 'train/wer': 0.1493814295257181, 'validation/ctc_loss': Array(0.6158217, dtype=float32), 'validation/wer': 0.17738494067215693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36153093, dtype=float32), 'test/wer': 0.1167915828814007, 'test/num_examples': 2472, 'score': 12981.104486703873, 'total_duration': 14358.558901309967, 'accumulated_submission_time': 12981.104486703873, 'accumulated_eval_time': 1376.2743470668793, 'accumulated_logging_time': 0.45090579986572266, 'global_step': 15183, 'preemption_count': 0}), (16899, {'train/ctc_loss': Array(0.43455628, dtype=float32), 'train/wer': 0.1444184104049378, 'validation/ctc_loss': Array(0.59773993, dtype=float32), 'validation/wer': 0.1722390105911544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35192952, dtype=float32), 'test/wer': 0.11346048382182683, 'test/num_examples': 2472, 'score': 14421.737709760666, 'total_duration': 15933.23994922638, 'accumulated_submission_time': 14421.737709760666, 'accumulated_eval_time': 1510.1858768463135, 'accumulated_logging_time': 0.5034916400909424, 'global_step': 16899, 'preemption_count': 0}), (18554, {'train/ctc_loss': Array(0.42747203, dtype=float32), 'train/wer': 0.14028345598255657, 'validation/ctc_loss': Array(0.5999306, dtype=float32), 'validation/wer': 0.17292449095841742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34360522, dtype=float32), 'test/wer': 0.11045437003635773, 'test/num_examples': 2472, 'score': 15861.863068819046, 'total_duration': 17508.79342675209, 'accumulated_submission_time': 15861.863068819046, 'accumulated_eval_time': 1645.4841482639313, 'accumulated_logging_time': 0.5546281337738037, 'global_step': 18554, 'preemption_count': 0}), (20255, {'train/ctc_loss': Array(0.4165472, dtype=float32), 'train/wer': 0.1373503718636462, 'validation/ctc_loss': Array(0.57209927, dtype=float32), 'validation/wer': 0.16542282553076454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32855484, dtype=float32), 'test/wer': 0.10513273617289216, 'test/num_examples': 2472, 'score': 17301.77611398697, 'total_duration': 19082.924453496933, 'accumulated_submission_time': 17301.77611398697, 'accumulated_eval_time': 1779.5542323589325, 'accumulated_logging_time': 0.6197030544281006, 'global_step': 20255, 'preemption_count': 0}), (21960, {'train/ctc_loss': Array(0.45802358, dtype=float32), 'train/wer': 0.14519755682835153, 'validation/ctc_loss': Array(0.5748465, dtype=float32), 'validation/wer': 0.16414841132683897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32678494, dtype=float32), 'test/wer': 0.10367030243942071, 'test/num_examples': 2472, 'score': 18741.892809152603, 'total_duration': 20657.470319747925, 'accumulated_submission_time': 18741.892809152603, 'accumulated_eval_time': 1913.8478994369507, 'accumulated_logging_time': 0.6720757484436035, 'global_step': 21960, 'preemption_count': 0}), (23637, {'train/ctc_loss': Array(0.36998138, dtype=float32), 'train/wer': 0.12440898834251919, 'validation/ctc_loss': Array(0.5380809, dtype=float32), 'validation/wer': 0.15562335267482164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31266233, dtype=float32), 'test/wer': 0.10050169601689923, 'test/num_examples': 2472, 'score': 20182.922751426697, 'total_duration': 22234.225139141083, 'accumulated_submission_time': 20182.922751426697, 'accumulated_eval_time': 2049.448930501938, 'accumulated_logging_time': 0.7180941104888916, 'global_step': 23637, 'preemption_count': 0}), (25321, {'train/ctc_loss': Array(0.3866852, dtype=float32), 'train/wer': 0.13302970526804364, 'validation/ctc_loss': Array(0.53082854, dtype=float32), 'validation/wer': 0.1544261756953764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30099636, dtype=float32), 'test/wer': 0.09682530010358906, 'test/num_examples': 2472, 'score': 21623.12057375908, 'total_duration': 23810.06982064247, 'accumulated_submission_time': 21623.12057375908, 'accumulated_eval_time': 2184.9646167755127, 'accumulated_logging_time': 0.7669081687927246, 'global_step': 25321, 'preemption_count': 0}), (27004, {'train/ctc_loss': Array(0.3206794, dtype=float32), 'train/wer': 0.10802226606074261, 'validation/ctc_loss': Array(0.5141854, dtype=float32), 'validation/wer': 0.1482665070430694, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29252425, dtype=float32), 'test/wer': 0.09446915686632949, 'test/num_examples': 2472, 'score': 23063.543320417404, 'total_duration': 25387.0976998806, 'accumulated_submission_time': 23063.543320417404, 'accumulated_eval_time': 2321.4288444519043, 'accumulated_logging_time': 0.825782060623169, 'global_step': 27004, 'preemption_count': 0}), (28672, {'train/ctc_loss': Array(0.34442776, dtype=float32), 'train/wer': 0.11347605984332136, 'validation/ctc_loss': Array(0.5025977, dtype=float32), 'validation/wer': 0.14449153769659287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28394303, dtype=float32), 'test/wer': 0.09199114415128065, 'test/num_examples': 2472, 'score': 24503.62016057968, 'total_duration': 26962.433699131012, 'accumulated_submission_time': 24503.62016057968, 'accumulated_eval_time': 2456.543736219406, 'accumulated_logging_time': 0.8892910480499268, 'global_step': 28672, 'preemption_count': 0}), (30368, {'train/ctc_loss': Array(0.3366657, dtype=float32), 'train/wer': 0.11299470381768845, 'validation/ctc_loss': Array(0.48924, dtype=float32), 'validation/wer': 0.14052347528891548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27818447, dtype=float32), 'test/wer': 0.08855848719354904, 'test/num_examples': 2472, 'score': 25944.00483107567, 'total_duration': 28538.838061094284, 'accumulated_submission_time': 25944.00483107567, 'accumulated_eval_time': 2592.422735452652, 'accumulated_logging_time': 0.9475498199462891, 'global_step': 30368, 'preemption_count': 0}), (32050, {'train/ctc_loss': Array(0.31884333, dtype=float32), 'train/wer': 0.10849346994415374, 'validation/ctc_loss': Array(0.4744674, dtype=float32), 'validation/wer': 0.1363912837792174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26197803, dtype=float32), 'test/wer': 0.0844758596876079, 'test/num_examples': 2472, 'score': 27383.977959632874, 'total_duration': 30115.500896692276, 'accumulated_submission_time': 27383.977959632874, 'accumulated_eval_time': 2728.975687980652, 'accumulated_logging_time': 1.0051610469818115, 'global_step': 32050, 'preemption_count': 0}), (33728, {'train/ctc_loss': Array(0.26275682, dtype=float32), 'train/wer': 0.0906634322464177, 'validation/ctc_loss': Array(0.46272892, dtype=float32), 'validation/wer': 0.13340799598366432, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25633433, dtype=float32), 'test/wer': 0.08222127434850608, 'test/num_examples': 2472, 'score': 28823.93050980568, 'total_duration': 31689.932183027267, 'accumulated_submission_time': 28823.93050980568, 'accumulated_eval_time': 2863.3213727474213, 'accumulated_logging_time': 1.0570778846740723, 'global_step': 33728, 'preemption_count': 0}), (35434, {'train/ctc_loss': Array(0.23715778, dtype=float32), 'train/wer': 0.0822559468865204, 'validation/ctc_loss': Array(0.45252073, dtype=float32), 'validation/wer': 0.13064676520849222, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24987762, dtype=float32), 'test/wer': 0.08037292060203521, 'test/num_examples': 2472, 'score': 30263.796778678894, 'total_duration': 33264.56129407883, 'accumulated_submission_time': 30263.796778678894, 'accumulated_eval_time': 2997.9378378391266, 'accumulated_logging_time': 1.1216213703155518, 'global_step': 35434, 'preemption_count': 0}), (37095, {'train/ctc_loss': Array(0.2662122, dtype=float32), 'train/wer': 0.09212208194111184, 'validation/ctc_loss': Array(0.4400807, dtype=float32), 'validation/wer': 0.1263697539028935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2403281, dtype=float32), 'test/wer': 0.0776917920906709, 'test/num_examples': 2472, 'score': 31703.80060362816, 'total_duration': 34838.64279150963, 'accumulated_submission_time': 31703.80060362816, 'accumulated_eval_time': 3131.8751130104065, 'accumulated_logging_time': 1.180323600769043, 'global_step': 37095, 'preemption_count': 0}), (38792, {'train/ctc_loss': Array(0.23776919, dtype=float32), 'train/wer': 0.08161365662127247, 'validation/ctc_loss': Array(0.43106434, dtype=float32), 'validation/wer': 0.12449675120924529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23511161, dtype=float32), 'test/wer': 0.07533564885341133, 'test/num_examples': 2472, 'score': 33144.33669090271, 'total_duration': 36414.3170132637, 'accumulated_submission_time': 33144.33669090271, 'accumulated_eval_time': 3266.8777928352356, 'accumulated_logging_time': 1.2335264682769775, 'global_step': 38792, 'preemption_count': 0}), (40493, {'train/ctc_loss': Array(0.245853, dtype=float32), 'train/wer': 0.08215494866687519, 'validation/ctc_loss': Array(0.42453703, dtype=float32), 'validation/wer': 0.1226913310870174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2303546, dtype=float32), 'test/wer': 0.07395446143846607, 'test/num_examples': 2472, 'score': 34584.92214655876, 'total_duration': 37992.03388214111, 'accumulated_submission_time': 34584.92214655876, 'accumulated_eval_time': 3403.8705430030823, 'accumulated_logging_time': 1.2884671688079834, 'global_step': 40493, 'preemption_count': 0}), (42166, {'train/ctc_loss': Array(0.19963317, dtype=float32), 'train/wer': 0.06902416748245821, 'validation/ctc_loss': Array(0.41599953, dtype=float32), 'validation/wer': 0.11988182704654508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683771, dtype=float32), 'test/wer': 0.07224828874941604, 'test/num_examples': 2472, 'score': 36025.603747844696, 'total_duration': 39568.92971038818, 'accumulated_submission_time': 36025.603747844696, 'accumulated_eval_time': 3539.95130443573, 'accumulated_logging_time': 1.3435046672821045, 'global_step': 42166, 'preemption_count': 0}), (43858, {'train/ctc_loss': Array(0.19884929, dtype=float32), 'train/wer': 0.06965301939584159, 'validation/ctc_loss': Array(0.41424644, dtype=float32), 'validation/wer': 0.11921565598540217, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22416562, dtype=float32), 'test/wer': 0.0712123981882071, 'test/num_examples': 2472, 'score': 37465.7214448452, 'total_duration': 41143.13983345032, 'accumulated_submission_time': 37465.7214448452, 'accumulated_eval_time': 3673.9047722816467, 'accumulated_logging_time': 1.4003708362579346, 'global_step': 43858, 'preemption_count': 0}), (45559, {'train/ctc_loss': Array(0.21639155, dtype=float32), 'train/wer': 0.07584706425101238, 'validation/ctc_loss': Array(0.40952334, dtype=float32), 'validation/wer': 0.11805709761819709, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22256012, dtype=float32), 'test/wer': 0.07086710133447079, 'test/num_examples': 2472, 'score': 38905.63683629036, 'total_duration': 42717.67965936661, 'accumulated_submission_time': 38905.63683629036, 'accumulated_eval_time': 3808.381865978241, 'accumulated_logging_time': 1.463677167892456, 'global_step': 45559, 'preemption_count': 0}), (47255, {'train/ctc_loss': Array(0.14392106, dtype=float32), 'train/wer': 0.05145027250170652, 'validation/ctc_loss': Array(0.40911275, dtype=float32), 'validation/wer': 0.11772883941415566, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22253822, dtype=float32), 'test/wer': 0.07076554343631304, 'test/num_examples': 2472, 'score': 40346.240510463715, 'total_duration': 44301.23940682411, 'accumulated_submission_time': 40346.240510463715, 'accumulated_eval_time': 3951.201397418976, 'accumulated_logging_time': 1.5179204940795898, 'global_step': 47255, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.13062766, dtype=float32), 'train/wer': 0.046137194585132935, 'validation/ctc_loss': Array(0.40922388, dtype=float32), 'validation/wer': 0.11775780337333577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22244853, dtype=float32), 'test/wer': 0.07060305079926066, 'test/num_examples': 2472, 'score': 40963.57310843468, 'total_duration': 45055.39094734192, 'accumulated_submission_time': 40963.57310843468, 'accumulated_eval_time': 4087.9201798439026, 'accumulated_logging_time': 1.5780909061431885, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 13:00:50.900201 140218947737408 submission_runner.py:586] Timing: 40963.57310843468
I0213 13:00:50.900272 140218947737408 submission_runner.py:588] Total number of evals: 30
I0213 13:00:50.900333 140218947737408 submission_runner.py:589] ====================
I0213 13:00:50.959777 140218947737408 submission_runner.py:673] Final librispeech_deepspeech score: 40149.33516192436
