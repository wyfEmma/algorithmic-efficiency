python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3078694106 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_01-26-2024-19-01-13.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0126 19:01:34.294003 140027215431488 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax because --overwrite was set.
I0126 19:01:34.297422 140027215431488 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax.
I0126 19:01:35.315256 140027215431488 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0126 19:01:35.316312 140027215431488 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0126 19:01:35.316502 140027215431488 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0126 19:01:35.317837 140027215431488 submission_runner.py:542] Using RNG seed 3078694106
I0126 19:01:36.451489 140027215431488 submission_runner.py:551] --- Tuning run 1/5 ---
I0126 19:01:36.451778 140027215431488 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1.
I0126 19:01:36.452351 140027215431488 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1/hparams.json.
I0126 19:01:36.633152 140027215431488 submission_runner.py:206] Initializing dataset.
I0126 19:01:36.649070 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:01:36.660140 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:01:37.056719 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:01:38.281338 140027215431488 submission_runner.py:213] Initializing model.
I0126 19:01:48.696093 140027215431488 submission_runner.py:255] Initializing optimizer.
I0126 19:01:50.401749 140027215431488 submission_runner.py:262] Initializing metrics bundle.
I0126 19:01:50.401961 140027215431488 submission_runner.py:280] Initializing checkpoint and logger.
I0126 19:01:50.403102 140027215431488 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0126 19:01:50.403252 140027215431488 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0126 19:01:50.718555 140027215431488 logger_utils.py:220] Unable to record git information. Continuing without it.
I0126 19:01:51.020132 140027215431488 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1/flags_0.json.
I0126 19:01:51.030018 140027215431488 submission_runner.py:314] Starting training loop.
I0126 19:02:46.756762 139865651922688 logging_writer.py:48] [0] global_step=0, grad_norm=0.5947058200836182, loss=6.9173197746276855
I0126 19:02:46.772364 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:02:47.751879 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:02:47.761080 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:02:47.845111 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:01.280727 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:03:02.856331 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:02.880289 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:03:02.938511 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:19.038320 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:03:19.842441 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0126 19:03:19.847306 140027215431488 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0126 19:03:19.885212 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0126 19:03:24.133100 140027215431488 submission_runner.py:408] Time since start: 93.10s, 	Step: 1, 	{'train/accuracy': 0.0007573341717943549, 'train/loss': 6.911828994750977, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 55.74225401878357, 'total_duration': 93.10303163528442, 'accumulated_submission_time': 55.74225401878357, 'accumulated_eval_time': 37.360684394836426, 'accumulated_logging_time': 0}
I0126 19:03:24.150724 139841895384832 logging_writer.py:48] [1] accumulated_eval_time=37.360684, accumulated_logging_time=0, accumulated_submission_time=55.742254, global_step=1, preemption_count=0, score=55.742254, test/accuracy=0.001300, test/loss=6.911728, test/num_examples=10000, total_duration=93.103032, train/accuracy=0.000757, train/loss=6.911829, validation/accuracy=0.000680, validation/loss=6.912051, validation/num_examples=50000
I0126 19:03:58.053431 139841886992128 logging_writer.py:48] [100] global_step=100, grad_norm=0.5897785425186157, loss=6.904664993286133
I0126 19:04:32.058275 139841895384832 logging_writer.py:48] [200] global_step=200, grad_norm=0.5946515202522278, loss=6.863030433654785
I0126 19:05:06.103307 139841886992128 logging_writer.py:48] [300] global_step=300, grad_norm=0.6285222172737122, loss=6.792759418487549
I0126 19:05:40.172488 139841895384832 logging_writer.py:48] [400] global_step=400, grad_norm=0.6912412643432617, loss=6.6924662590026855
I0126 19:06:14.213891 139841886992128 logging_writer.py:48] [500] global_step=500, grad_norm=0.7275424003601074, loss=6.603534698486328
I0126 19:06:48.261812 139841895384832 logging_writer.py:48] [600] global_step=600, grad_norm=0.7714500427246094, loss=6.51727294921875
I0126 19:07:22.318661 139841886992128 logging_writer.py:48] [700] global_step=700, grad_norm=0.7807509899139404, loss=6.472713470458984
I0126 19:07:56.393471 139841895384832 logging_writer.py:48] [800] global_step=800, grad_norm=0.8867546916007996, loss=6.3975725173950195
I0126 19:08:30.448606 139841886992128 logging_writer.py:48] [900] global_step=900, grad_norm=2.1269869804382324, loss=6.262697696685791
I0126 19:09:04.564388 139841895384832 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.631439447402954, loss=6.156896591186523
I0126 19:09:38.631809 139841886992128 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.416382908821106, loss=6.125342845916748
I0126 19:10:12.696088 139841895384832 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.5352035760879517, loss=5.994123458862305
I0126 19:10:46.752965 139841886992128 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.484358072280884, loss=5.945635795593262
I0126 19:11:20.819044 139841895384832 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.1613659858703613, loss=5.867789268493652
I0126 19:11:54.302551 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:12:01.730392 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:12:09.903072 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:12:12.216146 140027215431488 submission_runner.py:408] Time since start: 621.19s, 	Step: 1500, 	{'train/accuracy': 0.07481664419174194, 'train/loss': 5.330939769744873, 'validation/accuracy': 0.06864000111818314, 'validation/loss': 5.407910346984863, 'validation/num_examples': 50000, 'test/accuracy': 0.048100002110004425, 'test/loss': 5.63258171081543, 'test/num_examples': 10000, 'score': 565.8360676765442, 'total_duration': 621.1860675811768, 'accumulated_submission_time': 565.8360676765442, 'accumulated_eval_time': 55.27424716949463, 'accumulated_logging_time': 0.026404380798339844}
I0126 19:12:12.232747 139841903777536 logging_writer.py:48] [1500] accumulated_eval_time=55.274247, accumulated_logging_time=0.026404, accumulated_submission_time=565.836068, global_step=1500, preemption_count=0, score=565.836068, test/accuracy=0.048100, test/loss=5.632582, test/num_examples=10000, total_duration=621.186068, train/accuracy=0.074817, train/loss=5.330940, validation/accuracy=0.068640, validation/loss=5.407910, validation/num_examples=50000
I0126 19:12:12.608013 139841912170240 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.05230975151062, loss=5.844819068908691
I0126 19:12:46.658708 139841903777536 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.986332893371582, loss=5.683002948760986
I0126 19:13:20.736019 139841912170240 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.422471284866333, loss=5.621425151824951
I0126 19:13:54.815859 139841903777536 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.1792802810668945, loss=5.617961883544922
I0126 19:14:28.899274 139841912170240 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.752351760864258, loss=5.60574197769165
I0126 19:15:03.008786 139841903777536 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.9233224391937256, loss=5.4698662757873535
I0126 19:15:37.105419 139841912170240 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.222137689590454, loss=5.392376899719238
I0126 19:16:11.177444 139841903777536 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.0470194816589355, loss=5.3886399269104
I0126 19:16:45.278600 139841912170240 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.265991449356079, loss=5.349082946777344
I0126 19:17:19.350997 139841903777536 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.0803778171539307, loss=5.285293102264404
I0126 19:17:53.434614 139841912170240 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.0570995807647705, loss=5.246421813964844
I0126 19:18:27.511109 139841903777536 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.283740997314453, loss=5.136783599853516
I0126 19:19:01.616799 139841912170240 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.385471820831299, loss=5.27421236038208
I0126 19:19:35.699229 139841903777536 logging_writer.py:48] [2800] global_step=2800, grad_norm=7.6286492347717285, loss=5.1225457191467285
I0126 19:20:09.793159 139841912170240 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.6394553184509277, loss=5.0128912925720215
I0126 19:20:42.257385 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:20:49.506535 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:20:57.932666 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:21:00.609695 140027215431488 submission_runner.py:408] Time since start: 1149.58s, 	Step: 2997, 	{'train/accuracy': 0.18203921616077423, 'train/loss': 4.255775451660156, 'validation/accuracy': 0.16411998867988586, 'validation/loss': 4.366084098815918, 'validation/num_examples': 50000, 'test/accuracy': 0.12210000306367874, 'test/loss': 4.7546820640563965, 'test/num_examples': 10000, 'score': 1075.7997715473175, 'total_duration': 1149.57959151268, 'accumulated_submission_time': 1075.7997715473175, 'accumulated_eval_time': 73.62649869918823, 'accumulated_logging_time': 0.052556514739990234}
I0126 19:21:00.626679 139866171975424 logging_writer.py:48] [2997] accumulated_eval_time=73.626499, accumulated_logging_time=0.052557, accumulated_submission_time=1075.799772, global_step=2997, preemption_count=0, score=1075.799772, test/accuracy=0.122100, test/loss=4.754682, test/num_examples=10000, total_duration=1149.579592, train/accuracy=0.182039, train/loss=4.255775, validation/accuracy=0.164120, validation/loss=4.366084, validation/num_examples=50000
I0126 19:21:02.003703 139866180368128 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.590938568115234, loss=5.02161979675293
I0126 19:21:36.049746 139866171975424 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.700305938720703, loss=4.976178169250488
I0126 19:22:10.105253 139866180368128 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.254267930984497, loss=4.934547424316406
I0126 19:22:44.168300 139866171975424 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.63081693649292, loss=4.9903740882873535
I0126 19:23:18.219699 139866180368128 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.804769039154053, loss=4.86601448059082
I0126 19:23:52.320254 139866171975424 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.350659370422363, loss=4.832844257354736
I0126 19:24:26.376783 139866180368128 logging_writer.py:48] [3600] global_step=3600, grad_norm=7.870210647583008, loss=4.796961784362793
I0126 19:25:00.477607 139866171975424 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.033082962036133, loss=4.672513008117676
I0126 19:25:34.541561 139866180368128 logging_writer.py:48] [3800] global_step=3800, grad_norm=6.208363056182861, loss=4.695699214935303
I0126 19:26:08.586585 139866171975424 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.6737308502197266, loss=4.60322380065918
I0126 19:26:42.675148 139866180368128 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.466648101806641, loss=4.606625556945801
I0126 19:27:16.755149 139866171975424 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.354639053344727, loss=4.5237507820129395
I0126 19:27:50.848795 139866180368128 logging_writer.py:48] [4200] global_step=4200, grad_norm=8.962296485900879, loss=4.394168853759766
I0126 19:28:24.923992 139866171975424 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.149289846420288, loss=4.490426540374756
I0126 19:28:59.028696 139866180368128 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.597526550292969, loss=4.4484148025512695
I0126 19:29:30.810005 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:29:37.965582 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:29:46.390844 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:29:48.702998 140027215431488 submission_runner.py:408] Time since start: 1677.67s, 	Step: 4495, 	{'train/accuracy': 0.28310346603393555, 'train/loss': 3.511129856109619, 'validation/accuracy': 0.25633999705314636, 'validation/loss': 3.6584579944610596, 'validation/num_examples': 50000, 'test/accuracy': 0.18560001254081726, 'test/loss': 4.202426910400391, 'test/num_examples': 10000, 'score': 1585.9234120845795, 'total_duration': 1677.6729209423065, 'accumulated_submission_time': 1585.9234120845795, 'accumulated_eval_time': 91.51947140693665, 'accumulated_logging_time': 0.07996487617492676}
I0126 19:29:48.719218 139866054575872 logging_writer.py:48] [4495] accumulated_eval_time=91.519471, accumulated_logging_time=0.079965, accumulated_submission_time=1585.923412, global_step=4495, preemption_count=0, score=1585.923412, test/accuracy=0.185600, test/loss=4.202427, test/num_examples=10000, total_duration=1677.672921, train/accuracy=0.283103, train/loss=3.511130, validation/accuracy=0.256340, validation/loss=3.658458, validation/num_examples=50000
I0126 19:29:50.762294 139866062968576 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.89129638671875, loss=4.474341869354248
I0126 19:30:24.831419 139866054575872 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.899651050567627, loss=4.376626014709473
I0126 19:30:58.853045 139866062968576 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.5735368728637695, loss=4.393108367919922
I0126 19:31:32.911441 139866054575872 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.238992214202881, loss=4.4457855224609375
I0126 19:32:06.971498 139866062968576 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.6857500076293945, loss=4.284969329833984
I0126 19:32:41.048173 139866054575872 logging_writer.py:48] [5000] global_step=5000, grad_norm=5.191555500030518, loss=4.376892566680908
I0126 19:33:15.133927 139866062968576 logging_writer.py:48] [5100] global_step=5100, grad_norm=5.954275608062744, loss=4.199761390686035
I0126 19:33:49.229498 139866054575872 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.917846202850342, loss=4.3182220458984375
I0126 19:34:23.332082 139866062968576 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.443948268890381, loss=4.048494338989258
I0126 19:34:57.403799 139866054575872 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.1872711181640625, loss=4.195636749267578
I0126 19:35:31.445954 139866062968576 logging_writer.py:48] [5500] global_step=5500, grad_norm=7.430347919464111, loss=4.020741939544678
I0126 19:36:05.510655 139866054575872 logging_writer.py:48] [5600] global_step=5600, grad_norm=7.0498948097229, loss=4.044888019561768
I0126 19:36:39.611797 139866062968576 logging_writer.py:48] [5700] global_step=5700, grad_norm=6.823385238647461, loss=3.957829475402832
I0126 19:37:13.666666 139866054575872 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.914869785308838, loss=3.9352097511291504
I0126 19:37:47.730281 139866062968576 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.329785346984863, loss=4.05817174911499
I0126 19:38:18.839065 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:38:26.028424 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:38:34.395334 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:38:36.745340 140027215431488 submission_runner.py:408] Time since start: 2205.72s, 	Step: 5993, 	{'train/accuracy': 0.37711256742477417, 'train/loss': 2.901236057281494, 'validation/accuracy': 0.3469599783420563, 'validation/loss': 3.06270432472229, 'validation/num_examples': 50000, 'test/accuracy': 0.25780001282691956, 'test/loss': 3.6963891983032227, 'test/num_examples': 10000, 'score': 2095.9836592674255, 'total_duration': 2205.7152502536774, 'accumulated_submission_time': 2095.9836592674255, 'accumulated_eval_time': 109.42569613456726, 'accumulated_logging_time': 0.10534834861755371}
I0126 19:38:36.763203 139866171975424 logging_writer.py:48] [5993] accumulated_eval_time=109.425696, accumulated_logging_time=0.105348, accumulated_submission_time=2095.983659, global_step=5993, preemption_count=0, score=2095.983659, test/accuracy=0.257800, test/loss=3.696389, test/num_examples=10000, total_duration=2205.715250, train/accuracy=0.377113, train/loss=2.901236, validation/accuracy=0.346960, validation/loss=3.062704, validation/num_examples=50000
I0126 19:38:39.498368 139866180368128 logging_writer.py:48] [6000] global_step=6000, grad_norm=6.636725425720215, loss=3.906776189804077
I0126 19:39:13.561289 139866171975424 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.990069389343262, loss=3.9521799087524414
I0126 19:39:47.629025 139866180368128 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.738757610321045, loss=4.009368419647217
I0126 19:40:21.713575 139866171975424 logging_writer.py:48] [6300] global_step=6300, grad_norm=6.8290114402771, loss=3.9513187408447266
I0126 19:40:55.799698 139866180368128 logging_writer.py:48] [6400] global_step=6400, grad_norm=4.581392288208008, loss=3.890519618988037
I0126 19:41:29.893594 139866171975424 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.1918625831604, loss=3.8161754608154297
I0126 19:42:03.970680 139866180368128 logging_writer.py:48] [6600] global_step=6600, grad_norm=4.686832904815674, loss=3.746361255645752
I0126 19:42:38.023526 139866171975424 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.748834133148193, loss=3.816920042037964
I0126 19:43:12.067770 139866180368128 logging_writer.py:48] [6800] global_step=6800, grad_norm=5.022634506225586, loss=3.739455461502075
I0126 19:43:46.130467 139866171975424 logging_writer.py:48] [6900] global_step=6900, grad_norm=4.402507305145264, loss=3.744476079940796
I0126 19:44:20.180490 139866180368128 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.854137897491455, loss=3.8172121047973633
I0126 19:44:54.235397 139866171975424 logging_writer.py:48] [7100] global_step=7100, grad_norm=5.9660420417785645, loss=3.6761512756347656
I0126 19:45:28.295891 139866180368128 logging_writer.py:48] [7200] global_step=7200, grad_norm=7.680814266204834, loss=3.655021905899048
I0126 19:46:02.387937 139866171975424 logging_writer.py:48] [7300] global_step=7300, grad_norm=5.345281600952148, loss=3.6928868293762207
I0126 19:46:36.432240 139866180368128 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.242701530456543, loss=3.5615837574005127
I0126 19:47:06.826805 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:47:14.025876 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:47:22.459473 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:47:24.779993 140027215431488 submission_runner.py:408] Time since start: 2733.75s, 	Step: 7491, 	{'train/accuracy': 0.437220960855484, 'train/loss': 2.591935873031616, 'validation/accuracy': 0.4038199782371521, 'validation/loss': 2.7630527019500732, 'validation/num_examples': 50000, 'test/accuracy': 0.3150000274181366, 'test/loss': 3.4180727005004883, 'test/num_examples': 10000, 'score': 2605.98605966568, 'total_duration': 2733.749913215637, 'accumulated_submission_time': 2605.98605966568, 'accumulated_eval_time': 127.3788537979126, 'accumulated_logging_time': 0.13325881958007812}
I0126 19:47:24.797773 139866054575872 logging_writer.py:48] [7491] accumulated_eval_time=127.378854, accumulated_logging_time=0.133259, accumulated_submission_time=2605.986060, global_step=7491, preemption_count=0, score=2605.986060, test/accuracy=0.315000, test/loss=3.418073, test/num_examples=10000, total_duration=2733.749913, train/accuracy=0.437221, train/loss=2.591936, validation/accuracy=0.403820, validation/loss=2.763053, validation/num_examples=50000
I0126 19:47:28.203422 139866062968576 logging_writer.py:48] [7500] global_step=7500, grad_norm=5.3935933113098145, loss=3.7178781032562256
I0126 19:48:02.257250 139866054575872 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.797349452972412, loss=3.5542168617248535
I0126 19:48:36.259454 139866062968576 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.926175117492676, loss=3.602637529373169
I0126 19:49:10.305381 139866054575872 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.9583873748779297, loss=3.5876102447509766
I0126 19:49:44.369378 139866062968576 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.679549694061279, loss=3.617227554321289
I0126 19:50:18.423532 139866054575872 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.045203685760498, loss=3.5798118114471436
I0126 19:50:52.463972 139866062968576 logging_writer.py:48] [8100] global_step=8100, grad_norm=6.405930995941162, loss=3.5874013900756836
I0126 19:51:26.489139 139866054575872 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.008140563964844, loss=3.449998140335083
I0126 19:52:00.587002 139866062968576 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.463372707366943, loss=3.4598164558410645
I0126 19:52:34.639581 139866054575872 logging_writer.py:48] [8400] global_step=8400, grad_norm=4.536727428436279, loss=3.49654221534729
I0126 19:53:08.663146 139866062968576 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.618472576141357, loss=3.5871870517730713
I0126 19:53:42.705193 139866054575872 logging_writer.py:48] [8600] global_step=8600, grad_norm=4.765381336212158, loss=3.5143485069274902
I0126 19:54:16.749472 139866062968576 logging_writer.py:48] [8700] global_step=8700, grad_norm=4.630004405975342, loss=3.472400665283203
I0126 19:54:50.787935 139866054575872 logging_writer.py:48] [8800] global_step=8800, grad_norm=5.222556114196777, loss=3.5324594974517822
I0126 19:55:24.828224 139866062968576 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.8463594913482666, loss=3.373746871948242
I0126 19:55:54.874460 140027215431488 spec.py:321] Evaluating on the training split.
I0126 19:56:02.323980 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 19:56:10.770205 140027215431488 spec.py:349] Evaluating on the test split.
I0126 19:56:13.087135 140027215431488 submission_runner.py:408] Time since start: 3262.06s, 	Step: 8990, 	{'train/accuracy': 0.4954759180545807, 'train/loss': 2.285950183868408, 'validation/accuracy': 0.4610599875450134, 'validation/loss': 2.458083391189575, 'validation/num_examples': 50000, 'test/accuracy': 0.35450002551078796, 'test/loss': 3.1170003414154053, 'test/num_examples': 10000, 'score': 3116.0023124217987, 'total_duration': 3262.0570573806763, 'accumulated_submission_time': 3116.0023124217987, 'accumulated_eval_time': 145.59150886535645, 'accumulated_logging_time': 0.16101622581481934}
I0126 19:56:13.104731 139865794500352 logging_writer.py:48] [8990] accumulated_eval_time=145.591509, accumulated_logging_time=0.161016, accumulated_submission_time=3116.002312, global_step=8990, preemption_count=0, score=3116.002312, test/accuracy=0.354500, test/loss=3.117000, test/num_examples=10000, total_duration=3262.057057, train/accuracy=0.495476, train/loss=2.285950, validation/accuracy=0.461060, validation/loss=2.458083, validation/num_examples=50000
I0126 19:56:16.866718 139866163582720 logging_writer.py:48] [9000] global_step=9000, grad_norm=6.754250526428223, loss=3.5909225940704346
I0126 19:56:50.920614 139865794500352 logging_writer.py:48] [9100] global_step=9100, grad_norm=6.751666069030762, loss=3.380505084991455
I0126 19:57:24.945953 139866163582720 logging_writer.py:48] [9200] global_step=9200, grad_norm=7.729611396789551, loss=3.4928126335144043
I0126 19:57:58.961039 139865794500352 logging_writer.py:48] [9300] global_step=9300, grad_norm=4.355314254760742, loss=3.3975486755371094
I0126 19:58:33.122840 139866163582720 logging_writer.py:48] [9400] global_step=9400, grad_norm=5.062829494476318, loss=3.3307600021362305
I0126 19:59:07.136029 139865794500352 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.5728604793548584, loss=3.370016098022461
I0126 19:59:41.177963 139866163582720 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.42012357711792, loss=3.304575204849243
I0126 20:00:15.223343 139865794500352 logging_writer.py:48] [9700] global_step=9700, grad_norm=4.936296463012695, loss=3.3271450996398926
I0126 20:00:49.277387 139866163582720 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.936412334442139, loss=3.31876802444458
I0126 20:01:23.308646 139865794500352 logging_writer.py:48] [9900] global_step=9900, grad_norm=6.283237457275391, loss=3.3286120891571045
I0126 20:01:57.315511 139866163582720 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.109427452087402, loss=3.3304152488708496
I0126 20:02:31.340708 139865794500352 logging_writer.py:48] [10100] global_step=10100, grad_norm=5.102067470550537, loss=3.376965284347534
I0126 20:03:05.334631 139866163582720 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.1643805503845215, loss=3.2862181663513184
I0126 20:03:39.353089 139865794500352 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.984278678894043, loss=3.3384923934936523
I0126 20:04:13.357736 139866163582720 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.125537633895874, loss=3.244198799133301
I0126 20:04:43.186931 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:04:50.526495 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:05:02.812155 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:05:05.111688 140027215431488 submission_runner.py:408] Time since start: 3794.08s, 	Step: 10489, 	{'train/accuracy': 0.5600087642669678, 'train/loss': 1.9418503046035767, 'validation/accuracy': 0.49837997555732727, 'validation/loss': 2.2500064373016357, 'validation/num_examples': 50000, 'test/accuracy': 0.3785000145435333, 'test/loss': 2.9464919567108154, 'test/num_examples': 10000, 'score': 3626.0225105285645, 'total_duration': 3794.081609249115, 'accumulated_submission_time': 3626.0225105285645, 'accumulated_eval_time': 167.5162229537964, 'accumulated_logging_time': 0.18806004524230957}
I0126 20:05:05.129356 139865794500352 logging_writer.py:48] [10489] accumulated_eval_time=167.516223, accumulated_logging_time=0.188060, accumulated_submission_time=3626.022511, global_step=10489, preemption_count=0, score=3626.022511, test/accuracy=0.378500, test/loss=2.946492, test/num_examples=10000, total_duration=3794.081609, train/accuracy=0.560009, train/loss=1.941850, validation/accuracy=0.498380, validation/loss=2.250006, validation/num_examples=50000
I0126 20:05:09.244295 139865811285760 logging_writer.py:48] [10500] global_step=10500, grad_norm=8.269349098205566, loss=3.315396547317505
I0126 20:05:43.218231 139865794500352 logging_writer.py:48] [10600] global_step=10600, grad_norm=5.054336071014404, loss=3.241335391998291
I0126 20:06:17.228900 139865811285760 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.125518798828125, loss=3.232785224914551
I0126 20:06:51.207978 139865794500352 logging_writer.py:48] [10800] global_step=10800, grad_norm=10.041556358337402, loss=3.2572555541992188
I0126 20:07:25.224665 139865811285760 logging_writer.py:48] [10900] global_step=10900, grad_norm=5.535765647888184, loss=3.3797948360443115
I0126 20:07:59.273572 139865794500352 logging_writer.py:48] [11000] global_step=11000, grad_norm=6.064060688018799, loss=3.325612783432007
I0126 20:08:33.284039 139865811285760 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.2071404457092285, loss=3.1474967002868652
I0126 20:09:07.282285 139865794500352 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.337063789367676, loss=3.208059072494507
I0126 20:09:41.291954 139865811285760 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.387247085571289, loss=3.197551727294922
I0126 20:10:15.310241 139865794500352 logging_writer.py:48] [11400] global_step=11400, grad_norm=5.4184794425964355, loss=3.231735944747925
I0126 20:10:49.326077 139865811285760 logging_writer.py:48] [11500] global_step=11500, grad_norm=7.789032936096191, loss=3.2930920124053955
I0126 20:11:23.356816 139865794500352 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.00344181060791, loss=3.2468791007995605
I0126 20:11:57.396933 139865811285760 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.363957643508911, loss=3.2191500663757324
I0126 20:12:31.395089 139865794500352 logging_writer.py:48] [11800] global_step=11800, grad_norm=6.101145267486572, loss=3.263805389404297
I0126 20:13:05.409086 139865811285760 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.529275894165039, loss=3.241187334060669
I0126 20:13:35.416231 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:13:42.734171 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:13:51.184955 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:13:53.497574 140027215431488 submission_runner.py:408] Time since start: 4322.47s, 	Step: 11990, 	{'train/accuracy': 0.5775271058082581, 'train/loss': 1.8790894746780396, 'validation/accuracy': 0.5301399827003479, 'validation/loss': 2.1053919792175293, 'validation/num_examples': 50000, 'test/accuracy': 0.406900018453598, 'test/loss': 2.8000824451446533, 'test/num_examples': 10000, 'score': 4136.248651504517, 'total_duration': 4322.467486619949, 'accumulated_submission_time': 4136.248651504517, 'accumulated_eval_time': 185.59751963615417, 'accumulated_logging_time': 0.21561455726623535}
I0126 20:13:53.517328 139865786107648 logging_writer.py:48] [11990] accumulated_eval_time=185.597520, accumulated_logging_time=0.215615, accumulated_submission_time=4136.248652, global_step=11990, preemption_count=0, score=4136.248652, test/accuracy=0.406900, test/loss=2.800082, test/num_examples=10000, total_duration=4322.467487, train/accuracy=0.577527, train/loss=1.879089, validation/accuracy=0.530140, validation/loss=2.105392, validation/num_examples=50000
I0126 20:13:57.257668 139865802893056 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.230744361877441, loss=3.1757664680480957
I0126 20:14:31.224292 139865786107648 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.9678001403808594, loss=3.0888776779174805
I0126 20:15:05.226531 139865802893056 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.156247138977051, loss=3.159257411956787
I0126 20:15:39.241226 139865786107648 logging_writer.py:48] [12300] global_step=12300, grad_norm=5.739269256591797, loss=3.192025661468506
I0126 20:16:13.236739 139865802893056 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.34734582901001, loss=3.118171453475952
I0126 20:16:47.264600 139865786107648 logging_writer.py:48] [12500] global_step=12500, grad_norm=8.349034309387207, loss=3.165196418762207
I0126 20:17:21.287921 139865802893056 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.501214504241943, loss=3.239062786102295
I0126 20:17:55.290759 139865786107648 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.464661121368408, loss=3.1401784420013428
I0126 20:18:29.307920 139865802893056 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.171933650970459, loss=3.1055407524108887
I0126 20:19:03.307512 139865786107648 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.7153446674346924, loss=3.098449230194092
I0126 20:19:37.309293 139865802893056 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.422653675079346, loss=3.0858302116394043
I0126 20:20:11.319383 139865786107648 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.283851146697998, loss=3.119652271270752
I0126 20:20:45.305530 139865802893056 logging_writer.py:48] [13200] global_step=13200, grad_norm=6.779594421386719, loss=3.139528512954712
I0126 20:21:19.316243 139865786107648 logging_writer.py:48] [13300] global_step=13300, grad_norm=5.451035976409912, loss=3.185553789138794
I0126 20:21:53.313439 139865802893056 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.24547815322876, loss=3.15221905708313
I0126 20:22:23.673305 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:22:30.906471 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:22:39.361584 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:22:41.655200 140027215431488 submission_runner.py:408] Time since start: 4850.63s, 	Step: 13491, 	{'train/accuracy': 0.5838648080825806, 'train/loss': 1.8349441289901733, 'validation/accuracy': 0.5408999919891357, 'validation/loss': 2.0403449535369873, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.7253708839416504, 'test/num_examples': 10000, 'score': 4646.346125364304, 'total_duration': 4850.625118970871, 'accumulated_submission_time': 4646.346125364304, 'accumulated_eval_time': 203.5793845653534, 'accumulated_logging_time': 0.24520635604858398}
I0126 20:22:41.677339 139865811285760 logging_writer.py:48] [13491] accumulated_eval_time=203.579385, accumulated_logging_time=0.245206, accumulated_submission_time=4646.346125, global_step=13491, preemption_count=0, score=4646.346125, test/accuracy=0.424200, test/loss=2.725371, test/num_examples=10000, total_duration=4850.625119, train/accuracy=0.583865, train/loss=1.834944, validation/accuracy=0.540900, validation/loss=2.040345, validation/num_examples=50000
I0126 20:22:45.079745 139865819678464 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.219427108764648, loss=3.1473042964935303
I0126 20:23:19.151222 139865811285760 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.931342124938965, loss=3.0503547191619873
I0126 20:23:53.099784 139865819678464 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.5744645595550537, loss=3.003293991088867
I0126 20:24:27.117795 139865811285760 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.145054340362549, loss=3.1245930194854736
I0126 20:25:01.110741 139865819678464 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.532994270324707, loss=3.064148187637329
I0126 20:25:35.136533 139865811285760 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.556020259857178, loss=3.057934284210205
I0126 20:26:09.125831 139865819678464 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.722156047821045, loss=3.0736138820648193
I0126 20:26:43.126260 139865811285760 logging_writer.py:48] [14200] global_step=14200, grad_norm=5.36614465713501, loss=3.0542681217193604
I0126 20:27:17.091395 139865819678464 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.757628440856934, loss=3.1413328647613525
I0126 20:27:51.064502 139865811285760 logging_writer.py:48] [14400] global_step=14400, grad_norm=6.116804122924805, loss=3.0402445793151855
I0126 20:28:25.021026 139865819678464 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.592983722686768, loss=3.0694804191589355
I0126 20:28:58.970608 139865811285760 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.250258922576904, loss=3.0131731033325195
I0126 20:29:33.022755 139865819678464 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.325042247772217, loss=2.963894844055176
I0126 20:30:07.004427 139865811285760 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.325377941131592, loss=3.0798094272613525
I0126 20:30:40.989435 139865819678464 logging_writer.py:48] [14900] global_step=14900, grad_norm=6.493980884552002, loss=3.0476574897766113
I0126 20:31:11.670321 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:31:19.064689 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:31:27.434419 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:31:29.738742 140027215431488 submission_runner.py:408] Time since start: 5378.71s, 	Step: 14992, 	{'train/accuracy': 0.5969586968421936, 'train/loss': 1.7865535020828247, 'validation/accuracy': 0.5550999641418457, 'validation/loss': 1.9808624982833862, 'validation/num_examples': 50000, 'test/accuracy': 0.43540000915527344, 'test/loss': 2.653869152069092, 'test/num_examples': 10000, 'score': 5156.277529478073, 'total_duration': 5378.708662033081, 'accumulated_submission_time': 5156.277529478073, 'accumulated_eval_time': 221.64778184890747, 'accumulated_logging_time': 0.2781078815460205}
I0126 20:31:29.758099 139865794500352 logging_writer.py:48] [14992] accumulated_eval_time=221.647782, accumulated_logging_time=0.278108, accumulated_submission_time=5156.277529, global_step=14992, preemption_count=0, score=5156.277529, test/accuracy=0.435400, test/loss=2.653869, test/num_examples=10000, total_duration=5378.708662, train/accuracy=0.596959, train/loss=1.786554, validation/accuracy=0.555100, validation/loss=1.980862, validation/num_examples=50000
I0126 20:31:32.861248 139866163582720 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.188420295715332, loss=3.0100176334381104
I0126 20:32:06.804815 139865794500352 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.880401372909546, loss=3.14949369430542
I0126 20:32:40.764349 139866163582720 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.1632463932037354, loss=3.108001708984375
I0126 20:33:14.714221 139865794500352 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.432971954345703, loss=3.0921518802642822
I0126 20:33:48.685482 139866163582720 logging_writer.py:48] [15400] global_step=15400, grad_norm=8.397526741027832, loss=3.1166911125183105
I0126 20:34:22.650111 139865794500352 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.3731634616851807, loss=3.0637340545654297
I0126 20:34:56.598767 139866163582720 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.544257640838623, loss=3.075291156768799
I0126 20:35:30.630131 139865794500352 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.430717468261719, loss=2.9739248752593994
I0126 20:36:04.584216 139866163582720 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.1713833808898926, loss=2.969977855682373
I0126 20:36:38.571117 139865794500352 logging_writer.py:48] [15900] global_step=15900, grad_norm=6.888949871063232, loss=3.0347628593444824
I0126 20:37:12.532075 139866163582720 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.6542751789093018, loss=3.0912697315216064
I0126 20:37:46.521014 139865794500352 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.255438327789307, loss=3.0173630714416504
I0126 20:38:20.482966 139866163582720 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.825980186462402, loss=3.0692334175109863
I0126 20:38:54.444906 139865794500352 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.179112434387207, loss=3.000079870223999
I0126 20:39:28.406031 139866163582720 logging_writer.py:48] [16400] global_step=16400, grad_norm=6.248291969299316, loss=3.1468141078948975
I0126 20:40:00.053174 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:40:07.488519 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:40:15.969656 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:40:18.242916 140027215431488 submission_runner.py:408] Time since start: 5907.21s, 	Step: 16495, 	{'train/accuracy': 0.5969586968421936, 'train/loss': 1.7959184646606445, 'validation/accuracy': 0.5592600107192993, 'validation/loss': 1.978746771812439, 'validation/num_examples': 50000, 'test/accuracy': 0.4358000159263611, 'test/loss': 2.657799005508423, 'test/num_examples': 10000, 'score': 5666.510581970215, 'total_duration': 5907.21281003952, 'accumulated_submission_time': 5666.510581970215, 'accumulated_eval_time': 239.83745956420898, 'accumulated_logging_time': 0.3100862503051758}
I0126 20:40:18.263419 139865802893056 logging_writer.py:48] [16495] accumulated_eval_time=239.837460, accumulated_logging_time=0.310086, accumulated_submission_time=5666.510582, global_step=16495, preemption_count=0, score=5666.510582, test/accuracy=0.435800, test/loss=2.657799, test/num_examples=10000, total_duration=5907.212810, train/accuracy=0.596959, train/loss=1.795918, validation/accuracy=0.559260, validation/loss=1.978747, validation/num_examples=50000
I0126 20:40:20.311083 139865819678464 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.795391082763672, loss=3.0095715522766113
I0126 20:40:54.265765 139865802893056 logging_writer.py:48] [16600] global_step=16600, grad_norm=7.063298225402832, loss=3.0591845512390137
I0126 20:41:28.223318 139865819678464 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.83107852935791, loss=2.977250337600708
I0126 20:42:02.196946 139865802893056 logging_writer.py:48] [16800] global_step=16800, grad_norm=5.165567874908447, loss=2.9586126804351807
I0126 20:42:36.179339 139865819678464 logging_writer.py:48] [16900] global_step=16900, grad_norm=9.23054027557373, loss=3.1029672622680664
I0126 20:43:10.140315 139865802893056 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.4611833095550537, loss=2.9971446990966797
I0126 20:43:44.051057 139865819678464 logging_writer.py:48] [17100] global_step=17100, grad_norm=5.840898036956787, loss=2.9403557777404785
I0126 20:44:17.999298 139865802893056 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.9219655990600586, loss=3.0277605056762695
I0126 20:44:51.930129 139865819678464 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.5136215686798096, loss=2.9868297576904297
I0126 20:45:25.875922 139865802893056 logging_writer.py:48] [17400] global_step=17400, grad_norm=4.306870460510254, loss=2.99161958694458
I0126 20:45:59.851365 139865819678464 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.217706203460693, loss=3.017280340194702
I0126 20:46:33.805332 139865802893056 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.708049774169922, loss=3.043438196182251
I0126 20:47:07.711421 139865819678464 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.8986637592315674, loss=3.0322377681732178
I0126 20:47:41.681454 139865802893056 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.3481974601745605, loss=3.113626003265381
I0126 20:48:15.632805 139865819678464 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.435002565383911, loss=2.894394874572754
I0126 20:48:48.302753 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:48:56.394984 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:49:05.151700 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:49:07.477719 140027215431488 submission_runner.py:408] Time since start: 6436.45s, 	Step: 17998, 	{'train/accuracy': 0.5971181392669678, 'train/loss': 1.7574492692947388, 'validation/accuracy': 0.5580799579620361, 'validation/loss': 1.9484416246414185, 'validation/num_examples': 50000, 'test/accuracy': 0.4360000193119049, 'test/loss': 2.6286208629608154, 'test/num_examples': 10000, 'score': 6176.487642765045, 'total_duration': 6436.447620630264, 'accumulated_submission_time': 6176.487642765045, 'accumulated_eval_time': 259.01236724853516, 'accumulated_logging_time': 0.342235803604126}
I0126 20:49:07.510011 139865811285760 logging_writer.py:48] [17998] accumulated_eval_time=259.012367, accumulated_logging_time=0.342236, accumulated_submission_time=6176.487643, global_step=17998, preemption_count=0, score=6176.487643, test/accuracy=0.436000, test/loss=2.628621, test/num_examples=10000, total_duration=6436.447621, train/accuracy=0.597118, train/loss=1.757449, validation/accuracy=0.558080, validation/loss=1.948442, validation/num_examples=50000
I0126 20:49:08.535917 139865819678464 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.5119500160217285, loss=2.9928040504455566
I0126 20:49:42.461745 139865811285760 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.9764230251312256, loss=3.0350117683410645
I0126 20:50:16.419271 139865819678464 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.9752278327941895, loss=3.05686616897583
I0126 20:50:50.353718 139865811285760 logging_writer.py:48] [18300] global_step=18300, grad_norm=6.455244541168213, loss=3.025620460510254
I0126 20:51:24.298162 139865819678464 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.6625359058380127, loss=2.9153575897216797
I0126 20:51:58.224861 139865811285760 logging_writer.py:48] [18500] global_step=18500, grad_norm=5.3109307289123535, loss=3.0346643924713135
I0126 20:52:32.183070 139865819678464 logging_writer.py:48] [18600] global_step=18600, grad_norm=6.230252742767334, loss=3.050051689147949
I0126 20:53:06.129542 139865811285760 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.591784954071045, loss=2.9949049949645996
I0126 20:53:40.064463 139865819678464 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.1748459339141846, loss=3.0093891620635986
I0126 20:54:14.030396 139865811285760 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.110123872756958, loss=2.960434913635254
I0126 20:54:47.945794 139865819678464 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.5039408206939697, loss=2.959526300430298
I0126 20:55:21.901927 139865811285760 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.996878147125244, loss=2.993182897567749
I0126 20:55:55.843494 139865819678464 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.408698558807373, loss=3.0612411499023438
I0126 20:56:29.790136 139865811285760 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.236629009246826, loss=3.0351386070251465
I0126 20:57:03.710097 139865819678464 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.0577406883239746, loss=2.986241579055786
I0126 20:57:37.643300 139865811285760 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.3918721675872803, loss=2.915894031524658
I0126 20:57:37.651086 140027215431488 spec.py:321] Evaluating on the training split.
I0126 20:57:45.456222 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 20:57:53.982562 140027215431488 spec.py:349] Evaluating on the test split.
I0126 20:57:56.291946 140027215431488 submission_runner.py:408] Time since start: 6965.26s, 	Step: 19501, 	{'train/accuracy': 0.6540975570678711, 'train/loss': 1.5059301853179932, 'validation/accuracy': 0.5709599852561951, 'validation/loss': 1.8825407028198242, 'validation/num_examples': 50000, 'test/accuracy': 0.44700002670288086, 'test/loss': 2.5772900581359863, 'test/num_examples': 10000, 'score': 6686.56877040863, 'total_duration': 6965.261632204056, 'accumulated_submission_time': 6686.56877040863, 'accumulated_eval_time': 277.65295243263245, 'accumulated_logging_time': 0.3832590579986572}
I0126 20:57:56.314188 139865786107648 logging_writer.py:48] [19501] accumulated_eval_time=277.652952, accumulated_logging_time=0.383259, accumulated_submission_time=6686.568770, global_step=19501, preemption_count=0, score=6686.568770, test/accuracy=0.447000, test/loss=2.577290, test/num_examples=10000, total_duration=6965.261632, train/accuracy=0.654098, train/loss=1.505930, validation/accuracy=0.570960, validation/loss=1.882541, validation/num_examples=50000
I0126 20:58:30.258211 139865802893056 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.178633689880371, loss=2.943085193634033
I0126 20:59:04.216441 139865786107648 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.242802619934082, loss=2.9310388565063477
I0126 20:59:38.155088 139865802893056 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.147324085235596, loss=2.9615848064422607
I0126 21:00:12.118386 139865786107648 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.1059367656707764, loss=3.1272788047790527
I0126 21:00:46.061242 139865802893056 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.304784059524536, loss=2.9267196655273438
I0126 21:01:20.027139 139865786107648 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.827824831008911, loss=2.971731185913086
I0126 21:01:53.990397 139865802893056 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.786509990692139, loss=2.9898953437805176
I0126 21:02:27.933346 139865786107648 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.7883598804473877, loss=3.0087451934814453
I0126 21:03:01.873652 139865802893056 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.2107608318328857, loss=3.0716049671173096
I0126 21:03:35.849892 139865786107648 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.9802377223968506, loss=2.9920525550842285
I0126 21:04:09.793276 139865802893056 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.2477149963378906, loss=3.0232725143432617
I0126 21:04:43.710493 139865786107648 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.445465087890625, loss=2.921717643737793
I0126 21:05:17.641283 139865802893056 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.424194097518921, loss=2.9553141593933105
I0126 21:05:51.550622 139865786107648 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.124820232391357, loss=2.9457077980041504
I0126 21:06:25.768261 139865802893056 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.9748573303222656, loss=2.983762502670288
I0126 21:06:26.543138 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:06:34.750316 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:06:46.464339 140027215431488 spec.py:349] Evaluating on the test split.
I0126 21:06:48.782996 140027215431488 submission_runner.py:408] Time since start: 7497.75s, 	Step: 21004, 	{'train/accuracy': 0.6272919178009033, 'train/loss': 1.6106610298156738, 'validation/accuracy': 0.5689799785614014, 'validation/loss': 1.8841341733932495, 'validation/num_examples': 50000, 'test/accuracy': 0.4496000111103058, 'test/loss': 2.5562584400177, 'test/num_examples': 10000, 'score': 7196.738107442856, 'total_duration': 7497.752900362015, 'accumulated_submission_time': 7196.738107442856, 'accumulated_eval_time': 299.892765045166, 'accumulated_logging_time': 0.415421724319458}
I0126 21:06:48.814826 139865802893056 logging_writer.py:48] [21004] accumulated_eval_time=299.892765, accumulated_logging_time=0.415422, accumulated_submission_time=7196.738107, global_step=21004, preemption_count=0, score=7196.738107, test/accuracy=0.449600, test/loss=2.556258, test/num_examples=10000, total_duration=7497.752900, train/accuracy=0.627292, train/loss=1.610661, validation/accuracy=0.568980, validation/loss=1.884134, validation/num_examples=50000
I0126 21:07:21.737674 139865811285760 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.2318387031555176, loss=2.9482195377349854
I0126 21:07:55.626904 139865802893056 logging_writer.py:48] [21200] global_step=21200, grad_norm=4.115872859954834, loss=2.9153666496276855
I0126 21:08:29.550651 139865811285760 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.070008754730225, loss=2.8843603134155273
I0126 21:09:03.473978 139865802893056 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.005812644958496, loss=2.966805934906006
I0126 21:09:37.442128 139865811285760 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.125378131866455, loss=2.981228828430176
I0126 21:10:11.375208 139865802893056 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.2393927574157715, loss=2.9482033252716064
I0126 21:10:45.293448 139865811285760 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.925282955169678, loss=2.8631625175476074
I0126 21:11:19.237305 139865802893056 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.930776834487915, loss=2.913217067718506
I0126 21:11:53.195078 139865811285760 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.7593750953674316, loss=2.903672456741333
I0126 21:12:27.153769 139865802893056 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.040014266967773, loss=2.9529263973236084
I0126 21:13:01.101856 139865811285760 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.9850525856018066, loss=2.99639892578125
I0126 21:13:35.054179 139865802893056 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.743013381958008, loss=2.907219648361206
I0126 21:14:08.990120 139865811285760 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.3009464740753174, loss=2.8619496822357178
I0126 21:14:42.936436 139865802893056 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.034716844558716, loss=2.955714464187622
I0126 21:15:16.858770 139865811285760 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.6193289756774902, loss=2.8716108798980713
I0126 21:15:18.999931 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:15:26.541802 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:15:37.939239 140027215431488 spec.py:349] Evaluating on the test split.
I0126 21:15:40.243177 140027215431488 submission_runner.py:408] Time since start: 8029.21s, 	Step: 22508, 	{'train/accuracy': 0.6285873651504517, 'train/loss': 1.606850266456604, 'validation/accuracy': 0.5818799734115601, 'validation/loss': 1.833842158317566, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4872632026672363, 'test/num_examples': 10000, 'score': 7706.86282658577, 'total_duration': 8029.213074207306, 'accumulated_submission_time': 7706.86282658577, 'accumulated_eval_time': 321.13594365119934, 'accumulated_logging_time': 0.4556243419647217}
I0126 21:15:40.285169 139865802893056 logging_writer.py:48] [22508] accumulated_eval_time=321.135944, accumulated_logging_time=0.455624, accumulated_submission_time=7706.862827, global_step=22508, preemption_count=0, score=7706.862827, test/accuracy=0.461100, test/loss=2.487263, test/num_examples=10000, total_duration=8029.213074, train/accuracy=0.628587, train/loss=1.606850, validation/accuracy=0.581880, validation/loss=1.833842, validation/num_examples=50000
I0126 21:16:11.788667 139866171975424 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.375377893447876, loss=2.8932619094848633
I0126 21:16:45.720371 139865802893056 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.0974483489990234, loss=2.9074039459228516
I0126 21:17:19.625339 139866171975424 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.2981483936309814, loss=2.885503053665161
I0126 21:17:53.544598 139865802893056 logging_writer.py:48] [22900] global_step=22900, grad_norm=4.1202216148376465, loss=2.8860931396484375
I0126 21:18:27.507854 139866171975424 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.260986566543579, loss=2.8797240257263184
I0126 21:19:01.483388 139865802893056 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.256460666656494, loss=2.904667854309082
I0126 21:19:35.360036 139866171975424 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.2517921924591064, loss=2.901181221008301
I0126 21:20:09.279799 139865802893056 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.4656519889831543, loss=3.0206375122070312
I0126 21:20:43.197205 139866171975424 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.852980375289917, loss=2.952785015106201
I0126 21:21:17.126233 139865802893056 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.9439644813537598, loss=3.0064244270324707
I0126 21:21:51.077044 139866171975424 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.884455919265747, loss=2.9431166648864746
I0126 21:22:25.019778 139865802893056 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.309366464614868, loss=2.9160211086273193
I0126 21:22:58.954286 139866171975424 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.185027837753296, loss=2.9947357177734375
I0126 21:23:32.872464 139865802893056 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.0097837448120117, loss=2.8770430088043213
I0126 21:24:06.786436 139866171975424 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.935239315032959, loss=2.9388580322265625
I0126 21:24:10.270589 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:24:18.200994 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:24:29.963584 140027215431488 spec.py:349] Evaluating on the test split.
I0126 21:24:32.279895 140027215431488 submission_runner.py:408] Time since start: 8561.25s, 	Step: 24012, 	{'train/accuracy': 0.6264548897743225, 'train/loss': 1.6274893283843994, 'validation/accuracy': 0.5815799832344055, 'validation/loss': 1.8446393013000488, 'validation/num_examples': 50000, 'test/accuracy': 0.4627000093460083, 'test/loss': 2.5038511753082275, 'test/num_examples': 10000, 'score': 8216.786780834198, 'total_duration': 8561.249808549881, 'accumulated_submission_time': 8216.786780834198, 'accumulated_eval_time': 343.1451985836029, 'accumulated_logging_time': 0.5067691802978516}
I0126 21:24:32.305934 139865786107648 logging_writer.py:48] [24012] accumulated_eval_time=343.145199, accumulated_logging_time=0.506769, accumulated_submission_time=8216.786781, global_step=24012, preemption_count=0, score=8216.786781, test/accuracy=0.462700, test/loss=2.503851, test/num_examples=10000, total_duration=8561.249809, train/accuracy=0.626455, train/loss=1.627489, validation/accuracy=0.581580, validation/loss=1.844639, validation/num_examples=50000
I0126 21:25:02.532836 139865794500352 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.239133834838867, loss=2.9008123874664307
I0126 21:25:36.446807 139865786107648 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.0210461616516113, loss=2.8923087120056152
I0126 21:26:10.385514 139865794500352 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.5271711349487305, loss=2.8321990966796875
I0126 21:26:44.325465 139865786107648 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.2007055282592773, loss=2.915640115737915
I0126 21:27:18.260706 139865794500352 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.6335151195526123, loss=2.941455364227295
I0126 21:27:52.174252 139865786107648 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.4450409412384033, loss=2.9094948768615723
I0126 21:28:26.098388 139865794500352 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.996265172958374, loss=2.849275827407837
I0126 21:29:00.015302 139865786107648 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.698643684387207, loss=2.8491837978363037
I0126 21:29:33.926682 139865794500352 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.3587241172790527, loss=2.9092607498168945
I0126 21:30:07.869860 139865786107648 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.593792676925659, loss=2.8835866451263428
I0126 21:30:41.773956 139865794500352 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.637678861618042, loss=2.843048095703125
I0126 21:31:15.751290 139865786107648 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.8856735229492188, loss=2.831639289855957
I0126 21:31:49.703604 139865794500352 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.993525743484497, loss=2.8364763259887695
I0126 21:32:23.642999 139865786107648 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.0333855152130127, loss=2.877121686935425
I0126 21:32:57.582736 139865794500352 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.9922430515289307, loss=2.869243621826172
I0126 21:33:02.454668 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:33:10.308714 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:33:21.895117 140027215431488 spec.py:349] Evaluating on the test split.
I0126 21:33:24.186538 140027215431488 submission_runner.py:408] Time since start: 9093.16s, 	Step: 25516, 	{'train/accuracy': 0.631277859210968, 'train/loss': 1.6501520872116089, 'validation/accuracy': 0.5895199775695801, 'validation/loss': 1.8403079509735107, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.4981985092163086, 'test/num_examples': 10000, 'score': 8726.874703884125, 'total_duration': 9093.15644454956, 'accumulated_submission_time': 8726.874703884125, 'accumulated_eval_time': 364.8770282268524, 'accumulated_logging_time': 0.5437021255493164}
I0126 21:33:24.208037 139865786107648 logging_writer.py:48] [25516] accumulated_eval_time=364.877028, accumulated_logging_time=0.543702, accumulated_submission_time=8726.874704, global_step=25516, preemption_count=0, score=8726.874704, test/accuracy=0.468600, test/loss=2.498199, test/num_examples=10000, total_duration=9093.156445, train/accuracy=0.631278, train/loss=1.650152, validation/accuracy=0.589520, validation/loss=1.840308, validation/num_examples=50000
I0126 21:33:52.994740 139865794500352 logging_writer.py:48] [25600] global_step=25600, grad_norm=4.0663251876831055, loss=2.8273963928222656
I0126 21:34:26.909914 139865786107648 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.484320640563965, loss=2.870953321456909
I0126 21:35:00.792883 139865794500352 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.022348642349243, loss=2.850498914718628
I0126 21:35:34.726245 139865786107648 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.6563198566436768, loss=2.9467287063598633
I0126 21:36:08.665268 139865794500352 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.512103319168091, loss=2.839597702026367
I0126 21:36:42.565662 139865786107648 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.8082542419433594, loss=2.8270187377929688
I0126 21:37:16.542448 139865794500352 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.0156285762786865, loss=2.902731418609619
I0126 21:37:50.481871 139865786107648 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.91984224319458, loss=2.8114171028137207
I0126 21:38:24.381154 139865794500352 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.9762723445892334, loss=2.8081564903259277
I0126 21:38:58.315018 139865786107648 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.344130754470825, loss=2.8342692852020264
I0126 21:39:32.265748 139865794500352 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.4426543712615967, loss=2.783909797668457
I0126 21:40:06.181143 139865786107648 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.6893928050994873, loss=2.8969779014587402
I0126 21:40:40.106546 139865794500352 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.9897327423095703, loss=2.9220504760742188
I0126 21:41:14.032658 139865786107648 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.9328160285949707, loss=2.897387742996216
I0126 21:41:47.926702 139865794500352 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.602327585220337, loss=2.846972703933716
I0126 21:41:54.481235 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:42:02.382722 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:42:14.204547 140027215431488 spec.py:349] Evaluating on the test split.
I0126 21:42:16.430299 140027215431488 submission_runner.py:408] Time since start: 9625.40s, 	Step: 27021, 	{'train/accuracy': 0.6287069320678711, 'train/loss': 1.6057114601135254, 'validation/accuracy': 0.5907599925994873, 'validation/loss': 1.7863342761993408, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.4951579570770264, 'test/num_examples': 10000, 'score': 9237.083882570267, 'total_duration': 9625.399262428284, 'accumulated_submission_time': 9237.083882570267, 'accumulated_eval_time': 386.82509112358093, 'accumulated_logging_time': 0.5786194801330566}
I0126 21:42:16.452026 139866171975424 logging_writer.py:48] [27021] accumulated_eval_time=386.825091, accumulated_logging_time=0.578619, accumulated_submission_time=9237.083883, global_step=27021, preemption_count=0, score=9237.083883, test/accuracy=0.469800, test/loss=2.495158, test/num_examples=10000, total_duration=9625.399262, train/accuracy=0.628707, train/loss=1.605711, validation/accuracy=0.590760, validation/loss=1.786334, validation/num_examples=50000
I0126 21:42:44.090662 139866180368128 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.231228828430176, loss=2.802253007888794
I0126 21:43:18.005959 139866171975424 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.6639280319213867, loss=2.8043456077575684
I0126 21:43:51.964200 139866180368128 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.3744232654571533, loss=2.7199318408966064
I0126 21:44:25.894932 139866171975424 logging_writer.py:48] [27400] global_step=27400, grad_norm=5.672576904296875, loss=2.905423164367676
I0126 21:44:59.796818 139866180368128 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.7526769638061523, loss=2.8442163467407227
I0126 21:45:33.689294 139866171975424 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.5813262462615967, loss=2.848789691925049
I0126 21:46:07.579487 139866180368128 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.402665615081787, loss=2.846226215362549
I0126 21:46:41.523545 139866171975424 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.892568826675415, loss=2.9003636837005615
I0126 21:47:15.421360 139866180368128 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.8204054832458496, loss=2.7987442016601562
I0126 21:47:49.357246 139866171975424 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.168412446975708, loss=2.828751802444458
I0126 21:48:23.298330 139866180368128 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.7436180114746094, loss=2.8694820404052734
I0126 21:48:57.214911 139866171975424 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.0900838375091553, loss=2.8609166145324707
I0126 21:49:31.151516 139866180368128 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.8263373374938965, loss=2.8359949588775635
I0126 21:50:05.116555 139866171975424 logging_writer.py:48] [28400] global_step=28400, grad_norm=4.04834508895874, loss=2.8210153579711914
I0126 21:50:39.030759 139866180368128 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.180586099624634, loss=2.781421184539795
I0126 21:50:46.613531 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:50:54.510947 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:51:06.599891 140027215431488 spec.py:349] Evaluating on the test split.
I0126 21:51:08.848232 140027215431488 submission_runner.py:408] Time since start: 10157.82s, 	Step: 28524, 	{'train/accuracy': 0.683992326259613, 'train/loss': 1.3645371198654175, 'validation/accuracy': 0.597000002861023, 'validation/loss': 1.757430911064148, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.4163501262664795, 'test/num_examples': 10000, 'score': 9746.664439201355, 'total_duration': 10157.818138360977, 'accumulated_submission_time': 9746.664439201355, 'accumulated_eval_time': 409.05973839759827, 'accumulated_logging_time': 1.1308567523956299}
I0126 21:51:08.872520 139865794500352 logging_writer.py:48] [28524] accumulated_eval_time=409.059738, accumulated_logging_time=1.130857, accumulated_submission_time=9746.664439, global_step=28524, preemption_count=0, score=9746.664439, test/accuracy=0.473000, test/loss=2.416350, test/num_examples=10000, total_duration=10157.818138, train/accuracy=0.683992, train/loss=1.364537, validation/accuracy=0.597000, validation/loss=1.757431, validation/num_examples=50000
I0126 21:51:34.964154 139865802893056 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.3425955772399902, loss=2.8799209594726562
I0126 21:52:08.890765 139865794500352 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.4789507389068604, loss=2.835526466369629
I0126 21:52:42.772994 139865802893056 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.2795541286468506, loss=2.844378709793091
I0126 21:53:16.719147 139865794500352 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.3601717948913574, loss=2.7620041370391846
I0126 21:53:50.625130 139865802893056 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.3544070720672607, loss=2.833785057067871
I0126 21:54:24.540481 139865794500352 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.4684324264526367, loss=2.893625259399414
I0126 21:54:58.488170 139865802893056 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.801163673400879, loss=2.9048705101013184
I0126 21:55:32.372965 139865794500352 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.736060619354248, loss=2.9605650901794434
I0126 21:56:06.479937 139865802893056 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.823113203048706, loss=2.9023327827453613
I0126 21:56:40.373567 139865794500352 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.58182692527771, loss=2.8147220611572266
I0126 21:57:14.300627 139865802893056 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.133814573287964, loss=2.9398274421691895
I0126 21:57:48.178561 139865794500352 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.8421196937561035, loss=2.818331718444824
I0126 21:58:22.123744 139865802893056 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.537855625152588, loss=2.936948537826538
I0126 21:58:56.029395 139865794500352 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.1780500411987305, loss=2.799076557159424
I0126 21:59:29.958797 139865802893056 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.168227195739746, loss=2.854801654815674
I0126 21:59:38.886246 140027215431488 spec.py:321] Evaluating on the training split.
I0126 21:59:46.725492 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 21:59:59.568051 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:00:01.842380 140027215431488 submission_runner.py:408] Time since start: 10690.81s, 	Step: 30028, 	{'train/accuracy': 0.6511877775192261, 'train/loss': 1.5284302234649658, 'validation/accuracy': 0.5967199802398682, 'validation/loss': 1.7889021635055542, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.4623258113861084, 'test/num_examples': 10000, 'score': 10256.616863250732, 'total_duration': 10690.812278747559, 'accumulated_submission_time': 10256.616863250732, 'accumulated_eval_time': 432.0158112049103, 'accumulated_logging_time': 1.1649293899536133}
I0126 22:00:01.874710 139866171975424 logging_writer.py:48] [30028] accumulated_eval_time=432.015811, accumulated_logging_time=1.164929, accumulated_submission_time=10256.616863, global_step=30028, preemption_count=0, score=10256.616863, test/accuracy=0.475200, test/loss=2.462326, test/num_examples=10000, total_duration=10690.812279, train/accuracy=0.651188, train/loss=1.528430, validation/accuracy=0.596720, validation/loss=1.788902, validation/num_examples=50000
I0126 22:00:26.607574 139866180368128 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.469663381576538, loss=2.7864677906036377
I0126 22:01:00.488503 139866171975424 logging_writer.py:48] [30200] global_step=30200, grad_norm=4.041597366333008, loss=2.83115553855896
I0126 22:01:34.405419 139866180368128 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.0838160514831543, loss=2.8921432495117188
I0126 22:02:08.414266 139866171975424 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.9443395137786865, loss=2.8860292434692383
I0126 22:02:42.354386 139866180368128 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.095974922180176, loss=2.9705677032470703
I0126 22:03:16.251755 139866171975424 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.5720934867858887, loss=2.8000073432922363
I0126 22:03:50.173999 139866180368128 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.4148406982421875, loss=2.796963930130005
I0126 22:04:24.121334 139866171975424 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.9598207473754883, loss=2.889857292175293
I0126 22:04:58.010716 139866180368128 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.7788708209991455, loss=2.877000570297241
I0126 22:05:31.962816 139866171975424 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.1594622135162354, loss=2.8005762100219727
I0126 22:06:05.872255 139866180368128 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.4404265880584717, loss=2.790116310119629
I0126 22:06:39.771045 139866171975424 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.484100341796875, loss=2.8438735008239746
I0126 22:07:13.655815 139866180368128 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.789618492126465, loss=2.8322551250457764
I0126 22:07:47.580110 139866171975424 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.3610754013061523, loss=2.829519271850586
I0126 22:08:21.589291 139866180368128 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.8408801555633545, loss=2.769162654876709
I0126 22:08:31.916775 140027215431488 spec.py:321] Evaluating on the training split.
I0126 22:08:39.634446 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 22:08:54.802196 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:08:56.993594 140027215431488 submission_runner.py:408] Time since start: 11225.96s, 	Step: 31532, 	{'train/accuracy': 0.6303810477256775, 'train/loss': 1.6111574172973633, 'validation/accuracy': 0.5847600102424622, 'validation/loss': 1.82930588722229, 'validation/num_examples': 50000, 'test/accuracy': 0.4620000123977661, 'test/loss': 2.532831907272339, 'test/num_examples': 10000, 'score': 10766.595863342285, 'total_duration': 11225.963517665863, 'accumulated_submission_time': 10766.595863342285, 'accumulated_eval_time': 457.0925896167755, 'accumulated_logging_time': 1.210730791091919}
I0126 22:08:57.017595 139864133596928 logging_writer.py:48] [31532] accumulated_eval_time=457.092590, accumulated_logging_time=1.210731, accumulated_submission_time=10766.595863, global_step=31532, preemption_count=0, score=10766.595863, test/accuracy=0.462000, test/loss=2.532832, test/num_examples=10000, total_duration=11225.963518, train/accuracy=0.630381, train/loss=1.611157, validation/accuracy=0.584760, validation/loss=1.829306, validation/num_examples=50000
I0126 22:09:20.383889 139865760950016 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.890263319015503, loss=2.7673237323760986
I0126 22:09:54.280237 139864133596928 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.158388614654541, loss=2.7803361415863037
I0126 22:10:28.180101 139865760950016 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.43841552734375, loss=2.873995304107666
I0126 22:11:02.082367 139864133596928 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.353032350540161, loss=2.791604995727539
I0126 22:11:36.007964 139865760950016 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.4932119846343994, loss=2.871811628341675
I0126 22:12:09.921526 139864133596928 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.469967842102051, loss=2.8192412853240967
I0126 22:12:43.836534 139865760950016 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.0574183464050293, loss=2.738560914993286
I0126 22:13:17.749561 139864133596928 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.6888816356658936, loss=2.74007248878479
I0126 22:13:51.661360 139865760950016 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.6125755310058594, loss=2.8221864700317383
I0126 22:14:25.664530 139864133596928 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.9233176708221436, loss=2.8761308193206787
I0126 22:14:59.573675 139865760950016 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.1957151889801025, loss=2.8479976654052734
I0126 22:15:33.480851 139864133596928 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.1279287338256836, loss=2.7823548316955566
I0126 22:16:07.396736 139865760950016 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.318464994430542, loss=2.872012138366699
I0126 22:16:41.338915 139864133596928 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.1264071464538574, loss=2.7943928241729736
I0126 22:17:15.258458 139865760950016 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.016247272491455, loss=2.802680492401123
I0126 22:17:27.241705 140027215431488 spec.py:321] Evaluating on the training split.
I0126 22:17:35.583995 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 22:17:48.095462 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:17:50.383099 140027215431488 submission_runner.py:408] Time since start: 11759.35s, 	Step: 33037, 	{'train/accuracy': 0.649832546710968, 'train/loss': 1.4842784404754639, 'validation/accuracy': 0.6013000011444092, 'validation/loss': 1.7228082418441772, 'validation/num_examples': 50000, 'test/accuracy': 0.48250001668930054, 'test/loss': 2.3736181259155273, 'test/num_examples': 10000, 'score': 11276.758833408356, 'total_duration': 11759.353006362915, 'accumulated_submission_time': 11276.758833408356, 'accumulated_eval_time': 480.23394536972046, 'accumulated_logging_time': 1.2460718154907227}
I0126 22:17:50.407451 139865760950016 logging_writer.py:48] [33037] accumulated_eval_time=480.233945, accumulated_logging_time=1.246072, accumulated_submission_time=11276.758833, global_step=33037, preemption_count=0, score=11276.758833, test/accuracy=0.482500, test/loss=2.373618, test/num_examples=10000, total_duration=11759.353006, train/accuracy=0.649833, train/loss=1.484278, validation/accuracy=0.601300, validation/loss=1.722808, validation/num_examples=50000
I0126 22:18:12.096318 139865769342720 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.0423567295074463, loss=2.785088539123535
I0126 22:18:45.982721 139865760950016 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.1011927127838135, loss=2.697859287261963
I0126 22:19:19.847624 139865769342720 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.432281494140625, loss=2.807992696762085
I0126 22:19:53.774799 139865760950016 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.101425886154175, loss=2.858184814453125
I0126 22:20:27.683220 139865769342720 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.3832125663757324, loss=2.8021280765533447
I0126 22:21:01.593456 139865760950016 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.406822919845581, loss=2.8053479194641113
I0126 22:21:35.508630 139865769342720 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.1041147708892822, loss=2.8165719509124756
I0126 22:22:09.407785 139865760950016 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.1273698806762695, loss=2.9573395252227783
I0126 22:22:43.327031 139865769342720 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.029963254928589, loss=2.7710788249969482
I0126 22:23:17.210702 139865760950016 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.0990407466888428, loss=2.8107967376708984
I0126 22:23:51.121358 139865769342720 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.314610719680786, loss=2.7799854278564453
I0126 22:24:25.027613 139865760950016 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.3363757133483887, loss=2.825960159301758
I0126 22:24:58.940768 139865769342720 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.2661004066467285, loss=2.796752452850342
I0126 22:25:32.832351 139865760950016 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.71557354927063, loss=2.796627998352051
I0126 22:26:06.746943 139865769342720 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.6436448097229, loss=2.819765090942383
I0126 22:26:20.427633 140027215431488 spec.py:321] Evaluating on the training split.
I0126 22:26:28.777862 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 22:26:41.248930 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:26:43.496470 140027215431488 submission_runner.py:408] Time since start: 12292.47s, 	Step: 34542, 	{'train/accuracy': 0.6471221446990967, 'train/loss': 1.524101734161377, 'validation/accuracy': 0.602180004119873, 'validation/loss': 1.7275532484054565, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3868050575256348, 'test/num_examples': 10000, 'score': 11786.717982053757, 'total_duration': 12292.46637749672, 'accumulated_submission_time': 11786.717982053757, 'accumulated_eval_time': 503.3027272224426, 'accumulated_logging_time': 1.2807552814483643}
I0126 22:26:43.522335 139866180368128 logging_writer.py:48] [34542] accumulated_eval_time=503.302727, accumulated_logging_time=1.280755, accumulated_submission_time=11786.717982, global_step=34542, preemption_count=0, score=11786.717982, test/accuracy=0.479300, test/loss=2.386805, test/num_examples=10000, total_duration=12292.466377, train/accuracy=0.647122, train/loss=1.524102, validation/accuracy=0.602180, validation/loss=1.727553, validation/num_examples=50000
I0126 22:27:03.551703 139866188760832 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.0183143615722656, loss=2.7459280490875244
I0126 22:27:37.395838 139866180368128 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.1578216552734375, loss=2.8098185062408447
I0126 22:28:11.291355 139866188760832 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.9423952102661133, loss=2.7810215950012207
I0126 22:28:45.183928 139866180368128 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.845980405807495, loss=2.7964696884155273
I0126 22:29:19.104023 139866188760832 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.9080471992492676, loss=2.8085241317749023
I0126 22:29:52.981152 139866180368128 logging_writer.py:48] [35100] global_step=35100, grad_norm=4.754563808441162, loss=2.606017827987671
I0126 22:30:26.879329 139866188760832 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.134141683578491, loss=2.758352279663086
I0126 22:31:00.778191 139866180368128 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.1955296993255615, loss=2.8031105995178223
I0126 22:31:34.694502 139866188760832 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.4235191345214844, loss=2.765806198120117
I0126 22:32:08.592723 139866180368128 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.9749937057495117, loss=2.8296520709991455
I0126 22:32:42.497898 139866188760832 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.8791146278381348, loss=2.76009464263916
I0126 22:33:16.405173 139866180368128 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.0011823177337646, loss=2.776468276977539
I0126 22:33:50.318212 139866188760832 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.4330661296844482, loss=2.7901244163513184
I0126 22:34:24.202493 139866180368128 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.1522538661956787, loss=2.7551722526550293
I0126 22:34:58.119078 139866188760832 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.5695979595184326, loss=2.7179031372070312
I0126 22:35:13.497047 140027215431488 spec.py:321] Evaluating on the training split.
I0126 22:35:21.638558 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 22:35:34.660671 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:35:36.910092 140027215431488 submission_runner.py:408] Time since start: 12825.88s, 	Step: 36047, 	{'train/accuracy': 0.6511877775192261, 'train/loss': 1.5404627323150635, 'validation/accuracy': 0.6087799668312073, 'validation/loss': 1.7445299625396729, 'validation/num_examples': 50000, 'test/accuracy': 0.48510003089904785, 'test/loss': 2.3932998180389404, 'test/num_examples': 10000, 'score': 12296.627568244934, 'total_duration': 12825.880003213882, 'accumulated_submission_time': 12296.627568244934, 'accumulated_eval_time': 526.7157201766968, 'accumulated_logging_time': 1.3211784362792969}
I0126 22:35:36.934762 139862715918080 logging_writer.py:48] [36047] accumulated_eval_time=526.715720, accumulated_logging_time=1.321178, accumulated_submission_time=12296.627568, global_step=36047, preemption_count=0, score=12296.627568, test/accuracy=0.485100, test/loss=2.393300, test/num_examples=10000, total_duration=12825.880003, train/accuracy=0.651188, train/loss=1.540463, validation/accuracy=0.608780, validation/loss=1.744530, validation/num_examples=50000
I0126 22:35:55.211604 139862724310784 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.9600765705108643, loss=2.8549046516418457
I0126 22:36:29.068465 139862715918080 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.2378125190734863, loss=2.735287666320801
I0126 22:37:02.937732 139862724310784 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.986042022705078, loss=2.80375599861145
I0126 22:37:36.855737 139862715918080 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.5901999473571777, loss=2.803386688232422
I0126 22:38:10.761530 139862724310784 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.5558106899261475, loss=2.8130922317504883
I0126 22:38:44.695548 139862715918080 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.943072557449341, loss=2.7726495265960693
I0126 22:39:18.647017 139862724310784 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.2345523834228516, loss=2.741487979888916
I0126 22:39:52.550052 139862715918080 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.2150604724884033, loss=2.817976474761963
I0126 22:40:26.437592 139862724310784 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.0903403759002686, loss=2.7568342685699463
I0126 22:41:00.359470 139862715918080 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.271841049194336, loss=2.8890199661254883
I0126 22:41:34.238281 139862724310784 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.8027260303497314, loss=2.8329718112945557
I0126 22:42:08.142722 139862715918080 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.42486572265625, loss=2.7914071083068848
I0126 22:42:42.084715 139862724310784 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.0704476833343506, loss=2.8223116397857666
I0126 22:43:15.964138 139862715918080 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.0390164852142334, loss=2.8240609169006348
I0126 22:43:49.868932 139862724310784 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.994917154312134, loss=2.820707082748413
I0126 22:44:06.953851 140027215431488 spec.py:321] Evaluating on the training split.
I0126 22:44:14.155136 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 22:44:26.244399 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:44:28.778673 140027215431488 submission_runner.py:408] Time since start: 13357.75s, 	Step: 37552, 	{'train/accuracy': 0.6801658272743225, 'train/loss': 1.3729093074798584, 'validation/accuracy': 0.5968199968338013, 'validation/loss': 1.7451149225234985, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.4134533405303955, 'test/num_examples': 10000, 'score': 12806.58596277237, 'total_duration': 13357.748576164246, 'accumulated_submission_time': 12806.58596277237, 'accumulated_eval_time': 548.5404841899872, 'accumulated_logging_time': 1.3562448024749756}
I0126 22:44:28.800553 139866171975424 logging_writer.py:48] [37552] accumulated_eval_time=548.540484, accumulated_logging_time=1.356245, accumulated_submission_time=12806.585963, global_step=37552, preemption_count=0, score=12806.585963, test/accuracy=0.479100, test/loss=2.413453, test/num_examples=10000, total_duration=13357.748576, train/accuracy=0.680166, train/loss=1.372909, validation/accuracy=0.596820, validation/loss=1.745115, validation/num_examples=50000
I0126 22:44:45.402738 139866180368128 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.6902577877044678, loss=2.8064026832580566
I0126 22:45:19.298302 139866171975424 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.5796988010406494, loss=2.78541898727417
I0126 22:45:53.221924 139866180368128 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.82961106300354, loss=2.809300422668457
I0126 22:46:27.149764 139866171975424 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.5084500312805176, loss=2.790382146835327
I0126 22:47:01.064476 139866180368128 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.8055386543273926, loss=2.7904534339904785
I0126 22:47:34.970064 139866171975424 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.046154499053955, loss=2.7834534645080566
I0126 22:48:08.871544 139866180368128 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.6161038875579834, loss=2.770663022994995
I0126 22:48:42.778650 139866171975424 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.8959145545959473, loss=2.6969258785247803
I0126 22:49:16.671603 139866180368128 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.565748453140259, loss=2.766781806945801
I0126 22:49:50.593674 139866171975424 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.100179433822632, loss=2.7513175010681152
I0126 22:50:24.478032 139866180368128 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.706676959991455, loss=2.7713592052459717
I0126 22:50:58.387119 139866171975424 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.6184661388397217, loss=2.7687478065490723
I0126 22:51:32.289954 139866180368128 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.0611133575439453, loss=2.8277344703674316
I0126 22:52:06.234200 139866171975424 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.9526803493499756, loss=2.720820188522339
I0126 22:52:40.161016 139866180368128 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.0932700634002686, loss=2.877281427383423
I0126 22:52:58.917276 140027215431488 spec.py:321] Evaluating on the training split.
I0126 22:53:06.039460 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 22:53:17.518560 140027215431488 spec.py:349] Evaluating on the test split.
I0126 22:53:19.799748 140027215431488 submission_runner.py:408] Time since start: 13888.77s, 	Step: 39057, 	{'train/accuracy': 0.6672711968421936, 'train/loss': 1.4364635944366455, 'validation/accuracy': 0.604640007019043, 'validation/loss': 1.7295138835906982, 'validation/num_examples': 50000, 'test/accuracy': 0.48180001974105835, 'test/loss': 2.3832080364227295, 'test/num_examples': 10000, 'score': 13316.641247034073, 'total_duration': 13888.769673347473, 'accumulated_submission_time': 13316.641247034073, 'accumulated_eval_time': 569.4229211807251, 'accumulated_logging_time': 1.3880341053009033}
I0126 22:53:19.819752 139863663834880 logging_writer.py:48] [39057] accumulated_eval_time=569.422921, accumulated_logging_time=1.388034, accumulated_submission_time=13316.641247, global_step=39057, preemption_count=0, score=13316.641247, test/accuracy=0.481800, test/loss=2.383208, test/num_examples=10000, total_duration=13888.769673, train/accuracy=0.667271, train/loss=1.436464, validation/accuracy=0.604640, validation/loss=1.729514, validation/num_examples=50000
I0126 22:53:34.697271 139865760950016 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.796919345855713, loss=2.8222780227661133
I0126 22:54:08.580390 139863663834880 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.8234028816223145, loss=2.725693702697754
I0126 22:54:42.441422 139865760950016 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.0798354148864746, loss=2.8387887477874756
I0126 22:55:16.356965 139863663834880 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.078389883041382, loss=2.701366901397705
I0126 22:55:50.241009 139865760950016 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.5749247074127197, loss=2.8278322219848633
I0126 22:56:24.170131 139863663834880 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.921278953552246, loss=2.72098708152771
I0126 22:56:58.062086 139865760950016 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.4755239486694336, loss=2.785947322845459
I0126 22:57:31.988692 139863663834880 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.8451709747314453, loss=2.7471203804016113
I0126 22:58:06.019541 139865760950016 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.2869927883148193, loss=2.742701768875122
I0126 22:58:39.939641 139863663834880 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.9858603477478027, loss=2.6943626403808594
I0126 22:59:13.812957 139865760950016 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.562692403793335, loss=2.753115177154541
I0126 22:59:47.725206 139863663834880 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.8724896907806396, loss=2.7669923305511475
I0126 23:00:21.605544 139865760950016 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.8510868549346924, loss=2.7542521953582764
I0126 23:00:55.519845 139863663834880 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.351783037185669, loss=2.8205015659332275
I0126 23:01:29.420578 139865760950016 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.2244629859924316, loss=2.693643569946289
I0126 23:01:49.900083 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:01:56.971761 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:02:08.059076 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:02:10.366864 140027215431488 submission_runner.py:408] Time since start: 14419.34s, 	Step: 40562, 	{'train/accuracy': 0.6653180718421936, 'train/loss': 1.4576128721237183, 'validation/accuracy': 0.6128999590873718, 'validation/loss': 1.690018653869629, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.349595308303833, 'test/num_examples': 10000, 'score': 13826.662028312683, 'total_duration': 14419.336757183075, 'accumulated_submission_time': 13826.662028312683, 'accumulated_eval_time': 589.8896474838257, 'accumulated_logging_time': 1.4172828197479248}
I0126 23:02:10.391108 139862724310784 logging_writer.py:48] [40562] accumulated_eval_time=589.889647, accumulated_logging_time=1.417283, accumulated_submission_time=13826.662028, global_step=40562, preemption_count=0, score=13826.662028, test/accuracy=0.490200, test/loss=2.349595, test/num_examples=10000, total_duration=14419.336757, train/accuracy=0.665318, train/loss=1.457613, validation/accuracy=0.612900, validation/loss=1.690019, validation/num_examples=50000
I0126 23:02:23.607659 139863663834880 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.9057812690734863, loss=2.749431610107422
I0126 23:02:57.478932 139862724310784 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.2706406116485596, loss=2.8139262199401855
I0126 23:03:31.333898 139863663834880 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.4896483421325684, loss=2.8289244174957275
I0126 23:04:05.279540 139862724310784 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.296980857849121, loss=2.862431526184082
I0126 23:04:39.138764 139863663834880 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.415332555770874, loss=2.716050624847412
I0126 23:05:13.064384 139862724310784 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.060603380203247, loss=2.8466687202453613
I0126 23:05:46.950599 139863663834880 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.1203532218933105, loss=2.801658868789673
I0126 23:06:20.864856 139862724310784 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.235222816467285, loss=2.8121862411499023
I0126 23:06:54.764664 139863663834880 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.9181480407714844, loss=2.878551721572876
I0126 23:07:28.687019 139862724310784 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.9979798793792725, loss=2.6898374557495117
I0126 23:08:02.593636 139863663834880 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.8142659664154053, loss=2.7918074131011963
I0126 23:08:36.494551 139862724310784 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.8829379081726074, loss=2.807396650314331
I0126 23:09:10.408056 139863663834880 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.294826030731201, loss=2.711153984069824
I0126 23:09:44.310917 139862724310784 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.7506215572357178, loss=2.855494976043701
I0126 23:10:18.232866 139863663834880 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.4303035736083984, loss=2.770707607269287
I0126 23:10:40.403545 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:10:47.289034 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:10:58.626664 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:11:00.876655 140027215431488 submission_runner.py:408] Time since start: 14949.85s, 	Step: 42067, 	{'train/accuracy': 0.6569873690605164, 'train/loss': 1.500503659248352, 'validation/accuracy': 0.608460009098053, 'validation/loss': 1.7169100046157837, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.381596803665161, 'test/num_examples': 10000, 'score': 14336.611745595932, 'total_duration': 14949.84656405449, 'accumulated_submission_time': 14336.611745595932, 'accumulated_eval_time': 610.3627202510834, 'accumulated_logging_time': 1.4521598815917969}
I0126 23:11:00.901091 139862724310784 logging_writer.py:48] [42067] accumulated_eval_time=610.362720, accumulated_logging_time=1.452160, accumulated_submission_time=14336.611746, global_step=42067, preemption_count=0, score=14336.611746, test/accuracy=0.483600, test/loss=2.381597, test/num_examples=10000, total_duration=14949.846564, train/accuracy=0.656987, train/loss=1.500504, validation/accuracy=0.608460, validation/loss=1.716910, validation/num_examples=50000
I0126 23:11:12.446550 139863663834880 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.0578057765960693, loss=2.8178300857543945
I0126 23:11:46.304885 139862724310784 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.780042886734009, loss=2.8753013610839844
I0126 23:12:20.152932 139863663834880 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.837104320526123, loss=2.729206085205078
I0126 23:12:54.036439 139862724310784 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.020371675491333, loss=2.8042616844177246
I0126 23:13:27.926373 139863663834880 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.260411500930786, loss=2.7630653381347656
I0126 23:14:01.787543 139862724310784 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.082160472869873, loss=2.7789039611816406
I0126 23:14:35.701946 139863663834880 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.558635711669922, loss=2.8010997772216797
I0126 23:15:09.597140 139862724310784 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.924065351486206, loss=2.787487030029297
I0126 23:15:43.481686 139863663834880 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.057048797607422, loss=2.7780091762542725
I0126 23:16:17.389545 139862724310784 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.1334426403045654, loss=2.6995537281036377
I0126 23:16:51.439138 139863663834880 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.6922004222869873, loss=2.7363977432250977
I0126 23:17:25.337190 139862724310784 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.05753493309021, loss=2.7088236808776855
I0126 23:17:59.194448 139863663834880 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.122596502304077, loss=2.6819682121276855
I0126 23:18:33.096567 139862724310784 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.6477270126342773, loss=2.847303867340088
I0126 23:19:06.965631 139863663834880 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.374798059463501, loss=2.800208330154419
I0126 23:19:31.156346 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:19:37.926892 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:19:50.897305 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:19:53.177803 140027215431488 submission_runner.py:408] Time since start: 15482.15s, 	Step: 43573, 	{'train/accuracy': 0.6497528553009033, 'train/loss': 1.5108873844146729, 'validation/accuracy': 0.6085599660873413, 'validation/loss': 1.701366662979126, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.3547487258911133, 'test/num_examples': 10000, 'score': 14846.805841684341, 'total_duration': 15482.147728919983, 'accumulated_submission_time': 14846.805841684341, 'accumulated_eval_time': 632.3841454982758, 'accumulated_logging_time': 1.4866185188293457}
I0126 23:19:53.200347 139866171975424 logging_writer.py:48] [43573] accumulated_eval_time=632.384145, accumulated_logging_time=1.486619, accumulated_submission_time=14846.805842, global_step=43573, preemption_count=0, score=14846.805842, test/accuracy=0.491000, test/loss=2.354749, test/num_examples=10000, total_duration=15482.147729, train/accuracy=0.649753, train/loss=1.510887, validation/accuracy=0.608560, validation/loss=1.701367, validation/num_examples=50000
I0126 23:20:02.712286 139866180368128 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.043248176574707, loss=2.672149658203125
I0126 23:20:36.568906 139866171975424 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.168687582015991, loss=2.7468690872192383
I0126 23:21:10.415334 139866180368128 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.8799664974212646, loss=2.846619129180908
I0126 23:21:44.337064 139866171975424 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.184509515762329, loss=2.82503604888916
I0126 23:22:18.213424 139866180368128 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.0249741077423096, loss=2.7648816108703613
I0126 23:22:52.239205 139866171975424 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.2057316303253174, loss=2.815105438232422
I0126 23:23:26.129559 139866180368128 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.4930038452148438, loss=2.770519495010376
I0126 23:23:59.990302 139866171975424 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.4616708755493164, loss=2.7751808166503906
I0126 23:24:33.907486 139866180368128 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.7811524868011475, loss=2.7749578952789307
I0126 23:25:07.764301 139866171975424 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.3180899620056152, loss=2.7424752712249756
I0126 23:25:41.677393 139866180368128 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.696805238723755, loss=2.7859177589416504
I0126 23:26:15.542345 139866171975424 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.651911735534668, loss=2.6236395835876465
I0126 23:26:49.475328 139866180368128 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.833068370819092, loss=2.7550508975982666
I0126 23:27:23.341197 139866171975424 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.954533815383911, loss=2.715343713760376
I0126 23:27:57.265116 139866180368128 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.569815158843994, loss=2.7833175659179688
I0126 23:28:23.485741 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:28:30.014289 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:28:40.332741 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:28:42.616367 140027215431488 submission_runner.py:408] Time since start: 16011.59s, 	Step: 45079, 	{'train/accuracy': 0.6457469463348389, 'train/loss': 1.5189896821975708, 'validation/accuracy': 0.6038999557495117, 'validation/loss': 1.7213490009307861, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.438140869140625, 'test/num_examples': 10000, 'score': 15357.030562639236, 'total_duration': 16011.586278438568, 'accumulated_submission_time': 15357.030562639236, 'accumulated_eval_time': 651.5147202014923, 'accumulated_logging_time': 1.5189547538757324}
I0126 23:28:42.642843 139862724310784 logging_writer.py:48] [45079] accumulated_eval_time=651.514720, accumulated_logging_time=1.518955, accumulated_submission_time=15357.030563, global_step=45079, preemption_count=0, score=15357.030563, test/accuracy=0.471600, test/loss=2.438141, test/num_examples=10000, total_duration=16011.586278, train/accuracy=0.645747, train/loss=1.518990, validation/accuracy=0.603900, validation/loss=1.721349, validation/num_examples=50000
I0126 23:28:50.144890 139863663834880 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.875825881958008, loss=2.8183350563049316
I0126 23:29:24.018503 139862724310784 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.78493332862854, loss=2.843212604522705
I0126 23:29:57.904268 139863663834880 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.662090301513672, loss=2.7550060749053955
I0126 23:30:31.767328 139862724310784 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.18037748336792, loss=2.8335072994232178
I0126 23:31:05.662225 139863663834880 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.1983680725097656, loss=2.784249782562256
I0126 23:31:39.531202 139862724310784 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.3365638256073, loss=2.748023509979248
I0126 23:32:13.407498 139863663834880 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.527371406555176, loss=2.7197773456573486
I0126 23:32:47.247328 139862724310784 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.00303053855896, loss=2.7615957260131836
I0126 23:33:21.134876 139863663834880 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.120743751525879, loss=2.6469063758850098
I0126 23:33:55.022246 139862724310784 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.654905080795288, loss=2.7621800899505615
I0126 23:34:28.917382 139863663834880 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.033606767654419, loss=2.7614569664001465
I0126 23:35:02.910676 139862724310784 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.050907850265503, loss=2.8799335956573486
I0126 23:35:36.761072 139863663834880 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.233462810516357, loss=2.799858808517456
I0126 23:36:10.659715 139862724310784 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.1149649620056152, loss=2.697044849395752
I0126 23:36:44.537527 139863663834880 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.435882329940796, loss=2.7703397274017334
I0126 23:37:12.798144 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:37:19.310271 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:37:30.989404 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:37:33.253218 140027215431488 submission_runner.py:408] Time since start: 16542.22s, 	Step: 46585, 	{'train/accuracy': 0.6690250039100647, 'train/loss': 1.4443752765655518, 'validation/accuracy': 0.603119969367981, 'validation/loss': 1.738900899887085, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.3925979137420654, 'test/num_examples': 10000, 'score': 15867.12254691124, 'total_duration': 16542.223130464554, 'accumulated_submission_time': 15867.12254691124, 'accumulated_eval_time': 671.9697597026825, 'accumulated_logging_time': 1.5562007427215576}
I0126 23:37:33.278483 139866171975424 logging_writer.py:48] [46585] accumulated_eval_time=671.969760, accumulated_logging_time=1.556201, accumulated_submission_time=15867.122547, global_step=46585, preemption_count=0, score=15867.122547, test/accuracy=0.482700, test/loss=2.392598, test/num_examples=10000, total_duration=16542.223130, train/accuracy=0.669025, train/loss=1.444375, validation/accuracy=0.603120, validation/loss=1.738901, validation/num_examples=50000
I0126 23:37:38.742704 139866180368128 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.1504123210906982, loss=2.809274673461914
I0126 23:38:12.642175 139866171975424 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.9297473430633545, loss=2.7675602436065674
I0126 23:38:46.487189 139866180368128 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.9922568798065186, loss=2.7557060718536377
I0126 23:39:20.368045 139866171975424 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.346012592315674, loss=2.738713502883911
I0126 23:39:54.265124 139866180368128 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.951792001724243, loss=2.762814521789551
I0126 23:40:28.141216 139866171975424 logging_writer.py:48] [47100] global_step=47100, grad_norm=4.154130935668945, loss=2.7946207523345947
I0126 23:41:02.037055 139866180368128 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.0940706729888916, loss=2.7198946475982666
I0126 23:41:35.989073 139866171975424 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.8568413257598877, loss=2.7700414657592773
I0126 23:42:09.882158 139866180368128 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.891033172607422, loss=2.7686004638671875
I0126 23:42:43.791705 139866171975424 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.7734787464141846, loss=2.673442840576172
I0126 23:43:17.695847 139866180368128 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.422447443008423, loss=2.786782741546631
I0126 23:43:51.589989 139866171975424 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.7067503929138184, loss=2.7960686683654785
I0126 23:44:25.484373 139866180368128 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.237757921218872, loss=2.8630411624908447
I0126 23:44:59.402848 139866171975424 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.994225263595581, loss=2.7743189334869385
I0126 23:45:33.276560 139866180368128 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.130668878555298, loss=2.7559614181518555
I0126 23:46:03.565443 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:46:09.846543 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:46:21.262146 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:46:23.536922 140027215431488 submission_runner.py:408] Time since start: 17072.51s, 	Step: 48091, 	{'train/accuracy': 0.6783920526504517, 'train/loss': 1.3911480903625488, 'validation/accuracy': 0.6177600026130676, 'validation/loss': 1.6825976371765137, 'validation/num_examples': 50000, 'test/accuracy': 0.49320003390312195, 'test/loss': 2.372044324874878, 'test/num_examples': 10000, 'score': 16377.347776174545, 'total_duration': 17072.506830453873, 'accumulated_submission_time': 16377.347776174545, 'accumulated_eval_time': 691.9411878585815, 'accumulated_logging_time': 1.5927386283874512}
I0126 23:46:23.565610 139862724310784 logging_writer.py:48] [48091] accumulated_eval_time=691.941188, accumulated_logging_time=1.592739, accumulated_submission_time=16377.347776, global_step=48091, preemption_count=0, score=16377.347776, test/accuracy=0.493200, test/loss=2.372044, test/num_examples=10000, total_duration=17072.506830, train/accuracy=0.678392, train/loss=1.391148, validation/accuracy=0.617760, validation/loss=1.682598, validation/num_examples=50000
I0126 23:46:26.959851 139863663834880 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.794417142868042, loss=2.864179849624634
I0126 23:47:00.829110 139862724310784 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.1075456142425537, loss=2.6148409843444824
I0126 23:47:34.713405 139863663834880 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.389247417449951, loss=2.752373218536377
I0126 23:48:08.609305 139862724310784 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.3097970485687256, loss=2.797927141189575
I0126 23:48:42.489035 139863663834880 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.9260318279266357, loss=2.624178409576416
I0126 23:49:16.401649 139862724310784 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.8419084548950195, loss=2.8042495250701904
I0126 23:49:50.264386 139863663834880 logging_writer.py:48] [48700] global_step=48700, grad_norm=2.9488649368286133, loss=2.6300160884857178
I0126 23:50:24.150075 139862724310784 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.1081693172454834, loss=2.7716941833496094
I0126 23:50:58.049982 139863663834880 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.0565404891967773, loss=2.657738208770752
I0126 23:51:31.918148 139862724310784 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.387312889099121, loss=2.787097454071045
I0126 23:52:05.830027 139863663834880 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.953984498977661, loss=2.8099591732025146
I0126 23:52:39.699459 139862724310784 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.165621757507324, loss=2.662210464477539
I0126 23:53:13.606624 139863663834880 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.838277816772461, loss=2.739572048187256
I0126 23:53:47.540119 139862724310784 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.1372768878936768, loss=2.6416404247283936
I0126 23:54:21.428899 139863663834880 logging_writer.py:48] [49500] global_step=49500, grad_norm=4.658722877502441, loss=2.855185031890869
I0126 23:54:53.762839 140027215431488 spec.py:321] Evaluating on the training split.
I0126 23:54:59.979070 140027215431488 spec.py:333] Evaluating on the validation split.
I0126 23:55:13.911307 140027215431488 spec.py:349] Evaluating on the test split.
I0126 23:55:16.088269 140027215431488 submission_runner.py:408] Time since start: 17605.06s, 	Step: 49597, 	{'train/accuracy': 0.6721141338348389, 'train/loss': 1.4001306295394897, 'validation/accuracy': 0.6206600069999695, 'validation/loss': 1.6346431970596313, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.282940149307251, 'test/num_examples': 10000, 'score': 16887.48326063156, 'total_duration': 17605.0581843853, 'accumulated_submission_time': 16887.48326063156, 'accumulated_eval_time': 714.2665731906891, 'accumulated_logging_time': 1.6319713592529297}
I0126 23:55:16.113930 139866171975424 logging_writer.py:48] [49597] accumulated_eval_time=714.266573, accumulated_logging_time=1.631971, accumulated_submission_time=16887.483261, global_step=49597, preemption_count=0, score=16887.483261, test/accuracy=0.501000, test/loss=2.282940, test/num_examples=10000, total_duration=17605.058184, train/accuracy=0.672114, train/loss=1.400131, validation/accuracy=0.620660, validation/loss=1.634643, validation/num_examples=50000
I0126 23:55:17.466499 139866180368128 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.245177745819092, loss=2.71985125541687
I0126 23:55:51.337286 139866171975424 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.462332248687744, loss=2.8064565658569336
I0126 23:56:25.169945 139866180368128 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.0727744102478027, loss=2.665303945541382
I0126 23:56:59.014406 139866171975424 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.6960434913635254, loss=2.749094009399414
I0126 23:57:32.906025 139866180368128 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.1937553882598877, loss=2.7230794429779053
I0126 23:58:06.792101 139866171975424 logging_writer.py:48] [50100] global_step=50100, grad_norm=2.957174301147461, loss=2.732754707336426
I0126 23:58:40.671534 139866180368128 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.3370132446289062, loss=2.82755184173584
I0126 23:59:14.555694 139866171975424 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.387118339538574, loss=2.742201328277588
I0126 23:59:48.436047 139866180368128 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.187387228012085, loss=2.7527055740356445
I0127 00:00:22.315425 139866171975424 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.2189645767211914, loss=2.7098469734191895
I0127 00:00:56.193172 139866180368128 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.3243207931518555, loss=2.6439483165740967
I0127 00:01:30.055438 139866171975424 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.652092218399048, loss=2.7383954524993896
I0127 00:02:03.944972 139866180368128 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.921099901199341, loss=2.692936420440674
I0127 00:02:37.836582 139866171975424 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.9679059982299805, loss=2.621575355529785
I0127 00:03:11.688856 139866180368128 logging_writer.py:48] [51000] global_step=51000, grad_norm=2.904762029647827, loss=2.700634717941284
I0127 00:03:45.588704 139866171975424 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.0718226432800293, loss=2.6831202507019043
I0127 00:03:46.396092 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:03:52.758515 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:04:05.076647 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:04:07.388940 140027215431488 submission_runner.py:408] Time since start: 18136.36s, 	Step: 51104, 	{'train/accuracy': 0.6506895422935486, 'train/loss': 1.5444890260696411, 'validation/accuracy': 0.6065199971199036, 'validation/loss': 1.7607401609420776, 'validation/num_examples': 50000, 'test/accuracy': 0.48190003633499146, 'test/loss': 2.446437358856201, 'test/num_examples': 10000, 'score': 17397.699914216995, 'total_duration': 18136.358857870102, 'accumulated_submission_time': 17397.699914216995, 'accumulated_eval_time': 735.2593734264374, 'accumulated_logging_time': 1.6691064834594727}
I0127 00:04:07.412963 139862724310784 logging_writer.py:48] [51104] accumulated_eval_time=735.259373, accumulated_logging_time=1.669106, accumulated_submission_time=17397.699914, global_step=51104, preemption_count=0, score=17397.699914, test/accuracy=0.481900, test/loss=2.446437, test/num_examples=10000, total_duration=18136.358858, train/accuracy=0.650690, train/loss=1.544489, validation/accuracy=0.606520, validation/loss=1.760740, validation/num_examples=50000
I0127 00:04:40.264393 139863663834880 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.395953893661499, loss=2.709695816040039
I0127 00:05:14.076581 139862724310784 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.0417349338531494, loss=2.8489208221435547
I0127 00:05:47.954840 139863663834880 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.9059433937072754, loss=2.7694077491760254
I0127 00:06:21.902842 139862724310784 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.5145013332366943, loss=2.94545316696167
I0127 00:06:55.789417 139863663834880 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.289933204650879, loss=2.754456043243408
I0127 00:07:29.685714 139862724310784 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.2821176052093506, loss=2.7523608207702637
I0127 00:08:03.584626 139863663834880 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.4769277572631836, loss=2.733975887298584
I0127 00:08:37.503722 139862724310784 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.1007018089294434, loss=2.747967481613159
I0127 00:09:11.415683 139863663834880 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.2362470626831055, loss=2.7380173206329346
I0127 00:09:45.284620 139862724310784 logging_writer.py:48] [52100] global_step=52100, grad_norm=4.010904312133789, loss=2.6980834007263184
I0127 00:10:19.156599 139863663834880 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.48140025138855, loss=2.641768455505371
I0127 00:10:53.028498 139862724310784 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.784059524536133, loss=2.6320369243621826
I0127 00:11:26.889374 139863663834880 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.1392247676849365, loss=2.6985130310058594
I0127 00:12:00.839756 139862724310784 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.3598077297210693, loss=2.7259633541107178
I0127 00:12:34.711616 139863663834880 logging_writer.py:48] [52600] global_step=52600, grad_norm=2.7488515377044678, loss=2.6996681690216064
I0127 00:12:37.564771 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:12:43.822144 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:12:53.851122 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:12:56.140529 140027215431488 submission_runner.py:408] Time since start: 18665.11s, 	Step: 52610, 	{'train/accuracy': 0.6633649468421936, 'train/loss': 1.458517074584961, 'validation/accuracy': 0.6189799904823303, 'validation/loss': 1.6728732585906982, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.3692007064819336, 'test/num_examples': 10000, 'score': 17907.79274368286, 'total_duration': 18665.110434770584, 'accumulated_submission_time': 17907.79274368286, 'accumulated_eval_time': 753.8350718021393, 'accumulated_logging_time': 1.702005386352539}
I0127 00:12:56.166574 139862724310784 logging_writer.py:48] [52610] accumulated_eval_time=753.835072, accumulated_logging_time=1.702005, accumulated_submission_time=17907.792744, global_step=52610, preemption_count=0, score=17907.792744, test/accuracy=0.490600, test/loss=2.369201, test/num_examples=10000, total_duration=18665.110435, train/accuracy=0.663365, train/loss=1.458517, validation/accuracy=0.618980, validation/loss=1.672873, validation/num_examples=50000
I0127 00:13:26.961353 139866163582720 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.0349788665771484, loss=2.787881374359131
I0127 00:14:00.809835 139862724310784 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.0588088035583496, loss=2.7316863536834717
I0127 00:14:34.640096 139866163582720 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.5170836448669434, loss=2.6836304664611816
I0127 00:15:08.522454 139862724310784 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.8480942249298096, loss=2.6898627281188965
I0127 00:15:42.397071 139866163582720 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.413367509841919, loss=2.685788154602051
I0127 00:16:16.246761 139862724310784 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.3987090587615967, loss=2.7383079528808594
I0127 00:16:50.102661 139866163582720 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.2282583713531494, loss=2.771650791168213
I0127 00:17:24.020896 139862724310784 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.050050973892212, loss=2.6858272552490234
I0127 00:17:57.852227 139866163582720 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.974879026412964, loss=2.791515827178955
I0127 00:18:31.852911 139862724310784 logging_writer.py:48] [53600] global_step=53600, grad_norm=2.880388021469116, loss=2.784947633743286
I0127 00:19:05.712049 139866163582720 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.266765832901001, loss=2.7893261909484863
I0127 00:19:39.641283 139862724310784 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.8703970909118652, loss=2.799380302429199
I0127 00:20:13.484912 139866163582720 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.6592626571655273, loss=2.743741750717163
I0127 00:20:47.335062 139862724310784 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.6207330226898193, loss=2.6783199310302734
I0127 00:21:21.209244 139866163582720 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.545917510986328, loss=2.70988130569458
I0127 00:21:26.429450 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:21:32.730221 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:21:41.554200 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:21:43.815066 140027215431488 submission_runner.py:408] Time since start: 19192.78s, 	Step: 54117, 	{'train/accuracy': 0.652363657951355, 'train/loss': 1.5211600065231323, 'validation/accuracy': 0.6086999773979187, 'validation/loss': 1.7170997858047485, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.394115447998047, 'test/num_examples': 10000, 'score': 18417.992211580276, 'total_duration': 19192.784980773926, 'accumulated_submission_time': 18417.992211580276, 'accumulated_eval_time': 771.2206614017487, 'accumulated_logging_time': 1.7383487224578857}
I0127 00:21:43.844125 139865769342720 logging_writer.py:48] [54117] accumulated_eval_time=771.220661, accumulated_logging_time=1.738349, accumulated_submission_time=18417.992212, global_step=54117, preemption_count=0, score=18417.992212, test/accuracy=0.487200, test/loss=2.394115, test/num_examples=10000, total_duration=19192.784981, train/accuracy=0.652364, train/loss=1.521160, validation/accuracy=0.608700, validation/loss=1.717100, validation/num_examples=50000
I0127 00:22:12.302336 139866188760832 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.8490896224975586, loss=2.715195417404175
I0127 00:22:46.179209 139865769342720 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.1812314987182617, loss=2.714792251586914
I0127 00:23:20.030092 139866188760832 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.242509603500366, loss=2.7108566761016846
I0127 00:23:53.873803 139865769342720 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.1241066455841064, loss=2.7092599868774414
I0127 00:24:27.833488 139866188760832 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.349083662033081, loss=2.689728260040283
I0127 00:25:01.697241 139865769342720 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.4844727516174316, loss=2.7058632373809814
I0127 00:25:35.573346 139866188760832 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.706815242767334, loss=2.6310923099517822
I0127 00:26:09.455810 139865769342720 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.223092555999756, loss=2.6691126823425293
I0127 00:26:43.325057 139866188760832 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.0119400024414062, loss=2.6608548164367676
I0127 00:27:17.227852 139865769342720 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.735022783279419, loss=2.717212200164795
I0127 00:27:51.102032 139866188760832 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.9054901599884033, loss=2.657322645187378
I0127 00:28:25.012500 139865769342720 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.3614695072174072, loss=2.7525763511657715
I0127 00:28:58.864896 139866188760832 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.334202766418457, loss=2.673112630844116
I0127 00:29:32.713442 139865769342720 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.2337610721588135, loss=2.648477077484131
I0127 00:30:06.608488 139866188760832 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.6005961894989014, loss=2.6921634674072266
I0127 00:30:13.869128 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:30:20.258312 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:30:29.098969 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:30:31.417124 140027215431488 submission_runner.py:408] Time since start: 19720.39s, 	Step: 55623, 	{'train/accuracy': 0.6726123690605164, 'train/loss': 1.4373608827590942, 'validation/accuracy': 0.6170600056648254, 'validation/loss': 1.6829478740692139, 'validation/num_examples': 50000, 'test/accuracy': 0.4896000325679779, 'test/loss': 2.3452506065368652, 'test/num_examples': 10000, 'score': 18927.954924583435, 'total_duration': 19720.387036561966, 'accumulated_submission_time': 18927.954924583435, 'accumulated_eval_time': 788.768620967865, 'accumulated_logging_time': 1.7768800258636475}
I0127 00:30:31.446673 139863663834880 logging_writer.py:48] [55623] accumulated_eval_time=788.768621, accumulated_logging_time=1.776880, accumulated_submission_time=18927.954925, global_step=55623, preemption_count=0, score=18927.954925, test/accuracy=0.489600, test/loss=2.345251, test/num_examples=10000, total_duration=19720.387037, train/accuracy=0.672612, train/loss=1.437361, validation/accuracy=0.617060, validation/loss=1.682948, validation/num_examples=50000
I0127 00:30:58.029053 139865760950016 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.1729021072387695, loss=2.641843795776367
I0127 00:31:31.907254 139863663834880 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.8547000885009766, loss=2.785999298095703
I0127 00:32:05.798111 139865760950016 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.106106758117676, loss=2.7366695404052734
I0127 00:32:39.651104 139863663834880 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.6741676330566406, loss=2.725466251373291
I0127 00:33:13.511506 139865760950016 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.038886070251465, loss=2.782048463821411
I0127 00:33:47.399233 139863663834880 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.9837441444396973, loss=2.6404123306274414
I0127 00:34:21.290972 139865760950016 logging_writer.py:48] [56300] global_step=56300, grad_norm=4.044872760772705, loss=2.6696534156799316
I0127 00:34:55.185410 139863663834880 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.2694380283355713, loss=2.6588385105133057
I0127 00:35:29.034879 139865760950016 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.172858476638794, loss=2.7431278228759766
I0127 00:36:02.906147 139863663834880 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.2490386962890625, loss=2.6803436279296875
I0127 00:36:36.748669 139865760950016 logging_writer.py:48] [56700] global_step=56700, grad_norm=4.037984848022461, loss=2.7655587196350098
I0127 00:37:10.802637 139863663834880 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.0764477252960205, loss=2.6210060119628906
I0127 00:37:44.679538 139865760950016 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.585609197616577, loss=2.707679033279419
I0127 00:38:18.540772 139863663834880 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.739978075027466, loss=2.7188310623168945
I0127 00:38:52.371169 139865760950016 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.4373762607574463, loss=2.7154386043548584
I0127 00:39:01.685501 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:39:07.937646 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:39:16.538208 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:39:18.794641 140027215431488 submission_runner.py:408] Time since start: 20247.76s, 	Step: 57129, 	{'train/accuracy': 0.6888552308082581, 'train/loss': 1.3610378503799438, 'validation/accuracy': 0.6221599578857422, 'validation/loss': 1.6592100858688354, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.3328607082366943, 'test/num_examples': 10000, 'score': 19438.130825281143, 'total_duration': 20247.76454782486, 'accumulated_submission_time': 19438.130825281143, 'accumulated_eval_time': 805.8777091503143, 'accumulated_logging_time': 1.8174619674682617}
I0127 00:39:18.830097 139866163582720 logging_writer.py:48] [57129] accumulated_eval_time=805.877709, accumulated_logging_time=1.817462, accumulated_submission_time=19438.130825, global_step=57129, preemption_count=0, score=19438.130825, test/accuracy=0.496300, test/loss=2.332861, test/num_examples=10000, total_duration=20247.764548, train/accuracy=0.688855, train/loss=1.361038, validation/accuracy=0.622160, validation/loss=1.659210, validation/num_examples=50000
I0127 00:39:43.179111 139866171975424 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.5113773345947266, loss=2.692471981048584
I0127 00:40:17.038738 139866163582720 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.050189971923828, loss=2.6549582481384277
I0127 00:40:50.886545 139866171975424 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.1526918411254883, loss=2.7171735763549805
I0127 00:41:24.742601 139866163582720 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.9426324367523193, loss=2.6846561431884766
I0127 00:41:58.607741 139866171975424 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.4647819995880127, loss=2.704826831817627
I0127 00:42:32.493904 139866163582720 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.4537720680236816, loss=2.6951143741607666
I0127 00:43:06.465409 139866171975424 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.0926737785339355, loss=2.730593681335449
I0127 00:43:40.335735 139866163582720 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.216808319091797, loss=2.643551826477051
I0127 00:44:14.192640 139866171975424 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.3810486793518066, loss=2.7848386764526367
I0127 00:44:48.076227 139866163582720 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.0433337688446045, loss=2.7666306495666504
I0127 00:45:21.929636 139866171975424 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.1476128101348877, loss=2.738492012023926
I0127 00:45:55.773713 139866163582720 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.8450541496276855, loss=2.620433807373047
I0127 00:46:29.653623 139866171975424 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.2744266986846924, loss=2.7192842960357666
I0127 00:47:03.522751 139866163582720 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.8992528915405273, loss=2.71547794342041
I0127 00:47:37.368500 139866171975424 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.3675241470336914, loss=2.603573799133301
I0127 00:47:49.006356 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:47:55.162394 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:48:04.003215 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:48:06.321258 140027215431488 submission_runner.py:408] Time since start: 20775.29s, 	Step: 58636, 	{'train/accuracy': 0.6800262928009033, 'train/loss': 1.3760676383972168, 'validation/accuracy': 0.620959997177124, 'validation/loss': 1.6456332206726074, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.325051784515381, 'test/num_examples': 10000, 'score': 19948.24530482292, 'total_duration': 20775.291144371033, 'accumulated_submission_time': 19948.24530482292, 'accumulated_eval_time': 823.1925563812256, 'accumulated_logging_time': 1.8636653423309326}
I0127 00:48:06.355062 139863663834880 logging_writer.py:48] [58636] accumulated_eval_time=823.192556, accumulated_logging_time=1.863665, accumulated_submission_time=19948.245305, global_step=58636, preemption_count=0, score=19948.245305, test/accuracy=0.494600, test/loss=2.325052, test/num_examples=10000, total_duration=20775.291144, train/accuracy=0.680026, train/loss=1.376068, validation/accuracy=0.620960, validation/loss=1.645633, validation/num_examples=50000
I0127 00:48:28.351115 139865760950016 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.22942852973938, loss=2.7028591632843018
I0127 00:49:02.258313 139863663834880 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.314990520477295, loss=2.649020195007324
I0127 00:49:36.127603 139865760950016 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.633786916732788, loss=2.7435178756713867
I0127 00:50:09.968351 139863663834880 logging_writer.py:48] [59000] global_step=59000, grad_norm=4.523768424987793, loss=2.6478748321533203
I0127 00:50:43.858579 139865760950016 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.978677272796631, loss=2.6858906745910645
I0127 00:51:17.719331 139863663834880 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.9615066051483154, loss=2.6887383460998535
I0127 00:51:51.556254 139865760950016 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.9488368034362793, loss=2.687838315963745
I0127 00:52:25.413048 139863663834880 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.949899911880493, loss=2.743248701095581
I0127 00:52:59.318179 139865760950016 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.3376195430755615, loss=2.6945669651031494
I0127 00:53:33.158105 139863663834880 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.6886024475097656, loss=2.792475700378418
I0127 00:54:07.001067 139865760950016 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.4992611408233643, loss=2.68721079826355
I0127 00:54:40.888102 139863663834880 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.222555637359619, loss=2.6473231315612793
I0127 00:55:14.933265 139865760950016 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.3176937103271484, loss=2.6984167098999023
I0127 00:55:48.762221 139863663834880 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.2399349212646484, loss=2.6569924354553223
I0127 00:56:22.627617 139865760950016 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.1305079460144043, loss=2.655916452407837
I0127 00:56:36.636111 140027215431488 spec.py:321] Evaluating on the training split.
I0127 00:56:42.851874 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 00:56:51.687680 140027215431488 spec.py:349] Evaluating on the test split.
I0127 00:56:53.958354 140027215431488 submission_runner.py:408] Time since start: 21302.93s, 	Step: 60143, 	{'train/accuracy': 0.6727718114852905, 'train/loss': 1.4089183807373047, 'validation/accuracy': 0.6265400052070618, 'validation/loss': 1.6337388753890991, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.290309190750122, 'test/num_examples': 10000, 'score': 20458.460973262787, 'total_duration': 21302.928270578384, 'accumulated_submission_time': 20458.460973262787, 'accumulated_eval_time': 840.5147912502289, 'accumulated_logging_time': 1.9102842807769775}
I0127 00:56:53.985355 139862724310784 logging_writer.py:48] [60143] accumulated_eval_time=840.514791, accumulated_logging_time=1.910284, accumulated_submission_time=20458.460973, global_step=60143, preemption_count=0, score=20458.460973, test/accuracy=0.503600, test/loss=2.290309, test/num_examples=10000, total_duration=21302.928271, train/accuracy=0.672772, train/loss=1.408918, validation/accuracy=0.626540, validation/loss=1.633739, validation/num_examples=50000
I0127 00:57:13.603384 139863663834880 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.9067893028259277, loss=2.694253921508789
I0127 00:57:47.463492 139862724310784 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.466590166091919, loss=2.7574329376220703
I0127 00:58:21.299021 139863663834880 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.044281005859375, loss=2.7240190505981445
I0127 00:58:55.129728 139862724310784 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.1165974140167236, loss=2.597533702850342
I0127 00:59:28.994732 139863663834880 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.2383553981781006, loss=2.77400803565979
I0127 01:00:02.839911 139862724310784 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.509821891784668, loss=2.6461949348449707
I0127 01:00:36.683012 139863663834880 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.9665639400482178, loss=2.6696245670318604
I0127 01:01:10.536845 139862724310784 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.734482765197754, loss=2.839860439300537
I0127 01:01:44.470464 139863663834880 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.2647945880889893, loss=2.836163282394409
I0127 01:02:18.342711 139862724310784 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.5672030448913574, loss=2.792086601257324
I0127 01:02:52.186508 139863663834880 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.494544744491577, loss=2.7051634788513184
I0127 01:03:26.002890 139862724310784 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.6342709064483643, loss=2.752014398574829
I0127 01:03:59.863079 139863663834880 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.0384585857391357, loss=2.6570322513580322
I0127 01:04:33.658381 139862724310784 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.250309944152832, loss=2.6752429008483887
I0127 01:05:07.533852 139863663834880 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.2243878841400146, loss=2.653719425201416
I0127 01:05:24.250705 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:05:30.412800 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:05:40.102974 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:05:42.393448 140027215431488 submission_runner.py:408] Time since start: 21831.36s, 	Step: 61651, 	{'train/accuracy': 0.6753029227256775, 'train/loss': 1.3835617303848267, 'validation/accuracy': 0.6267799735069275, 'validation/loss': 1.610917329788208, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.281240224838257, 'test/num_examples': 10000, 'score': 20968.66392183304, 'total_duration': 21831.363361120224, 'accumulated_submission_time': 20968.66392183304, 'accumulated_eval_time': 858.6574947834015, 'accumulated_logging_time': 1.9475953578948975}
I0127 01:05:42.421340 139866163582720 logging_writer.py:48] [61651] accumulated_eval_time=858.657495, accumulated_logging_time=1.947595, accumulated_submission_time=20968.663922, global_step=61651, preemption_count=0, score=20968.663922, test/accuracy=0.501000, test/loss=2.281240, test/num_examples=10000, total_duration=21831.363361, train/accuracy=0.675303, train/loss=1.383562, validation/accuracy=0.626780, validation/loss=1.610917, validation/num_examples=50000
I0127 01:05:59.374754 139866180368128 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.331810474395752, loss=2.6141579151153564
I0127 01:06:33.210650 139866163582720 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.0319602489471436, loss=2.6719088554382324
I0127 01:07:07.093028 139866180368128 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.1354854106903076, loss=2.7014217376708984
I0127 01:07:40.989788 139866163582720 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.9000415802001953, loss=2.590985059738159
I0127 01:08:14.834908 139866180368128 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.2864367961883545, loss=2.6681323051452637
I0127 01:08:48.731597 139866163582720 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.1454293727874756, loss=2.6196632385253906
I0127 01:09:22.606279 139866180368128 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.3621461391448975, loss=2.7025115489959717
I0127 01:09:56.448301 139866163582720 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.152947187423706, loss=2.6786768436431885
I0127 01:10:30.293662 139866180368128 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.3622524738311768, loss=2.721635580062866
I0127 01:11:04.165264 139866163582720 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.871793508529663, loss=2.6539998054504395
I0127 01:11:38.039257 139866180368128 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.027092456817627, loss=2.662410020828247
I0127 01:12:11.870727 139866163582720 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.5200955867767334, loss=2.702456474304199
I0127 01:12:45.710783 139866180368128 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.363856554031372, loss=2.7491745948791504
I0127 01:13:19.592142 139866163582720 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.2560172080993652, loss=2.5937180519104004
I0127 01:13:53.517588 139866180368128 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.986508369445801, loss=2.803269624710083
I0127 01:14:12.614276 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:14:18.830999 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:14:28.005844 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:14:30.307660 140027215431488 submission_runner.py:408] Time since start: 22359.28s, 	Step: 63158, 	{'train/accuracy': 0.6793287396430969, 'train/loss': 1.402336597442627, 'validation/accuracy': 0.631060004234314, 'validation/loss': 1.6216193437576294, 'validation/num_examples': 50000, 'test/accuracy': 0.5006000399589539, 'test/loss': 2.285511016845703, 'test/num_examples': 10000, 'score': 21478.79666543007, 'total_duration': 22359.277554273605, 'accumulated_submission_time': 21478.79666543007, 'accumulated_eval_time': 876.3508095741272, 'accumulated_logging_time': 1.9852290153503418}
I0127 01:14:30.337838 139862724310784 logging_writer.py:48] [63158] accumulated_eval_time=876.350810, accumulated_logging_time=1.985229, accumulated_submission_time=21478.796665, global_step=63158, preemption_count=0, score=21478.796665, test/accuracy=0.500600, test/loss=2.285511, test/num_examples=10000, total_duration=22359.277554, train/accuracy=0.679329, train/loss=1.402337, validation/accuracy=0.631060, validation/loss=1.621619, validation/num_examples=50000
I0127 01:14:44.858489 139863663834880 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.288576126098633, loss=2.684494972229004
I0127 01:15:18.718185 139862724310784 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.8527865409851074, loss=2.629523754119873
I0127 01:15:52.503769 139863663834880 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.036651134490967, loss=2.6222615242004395
I0127 01:16:26.386570 139862724310784 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.064147472381592, loss=2.7067408561706543
I0127 01:17:00.209845 139863663834880 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.5929057598114014, loss=2.7632460594177246
I0127 01:17:34.058822 139862724310784 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.1991004943847656, loss=2.7398674488067627
I0127 01:18:07.953015 139863663834880 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.64557147026062, loss=2.629152536392212
I0127 01:18:41.844516 139862724310784 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.468452215194702, loss=2.677499771118164
I0127 01:19:15.677002 139863663834880 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.343921184539795, loss=2.5988380908966064
I0127 01:19:49.619316 139862724310784 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.449746608734131, loss=2.6800365447998047
I0127 01:20:23.456810 139863663834880 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.3651764392852783, loss=2.533494710922241
I0127 01:20:57.297170 139862724310784 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.380669593811035, loss=2.6429619789123535
I0127 01:21:31.175544 139863663834880 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.332576274871826, loss=2.6453261375427246
I0127 01:22:05.060735 139862724310784 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.6228280067443848, loss=2.697408676147461
I0127 01:22:38.886187 139863663834880 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.681668281555176, loss=2.6658082008361816
I0127 01:23:00.349457 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:23:06.512873 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:23:15.963347 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:23:18.256045 140027215431488 submission_runner.py:408] Time since start: 22887.23s, 	Step: 64665, 	{'train/accuracy': 0.6847297549247742, 'train/loss': 1.3605037927627563, 'validation/accuracy': 0.6293999552726746, 'validation/loss': 1.600814700126648, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2789101600646973, 'test/num_examples': 10000, 'score': 21988.747662067413, 'total_duration': 22887.225957870483, 'accumulated_submission_time': 21988.747662067413, 'accumulated_eval_time': 894.2573552131653, 'accumulated_logging_time': 2.025351047515869}
I0127 01:23:18.284452 139862724310784 logging_writer.py:48] [64665] accumulated_eval_time=894.257355, accumulated_logging_time=2.025351, accumulated_submission_time=21988.747662, global_step=64665, preemption_count=0, score=21988.747662, test/accuracy=0.501000, test/loss=2.278910, test/num_examples=10000, total_duration=22887.225958, train/accuracy=0.684730, train/loss=1.360504, validation/accuracy=0.629400, validation/loss=1.600815, validation/num_examples=50000
I0127 01:23:30.467310 139866163582720 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.338559865951538, loss=2.6673924922943115
I0127 01:24:04.315411 139862724310784 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.5947105884552, loss=2.628549098968506
I0127 01:24:38.167233 139866163582720 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.5088531970977783, loss=2.6607346534729004
I0127 01:25:12.006567 139862724310784 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.948617935180664, loss=2.619030475616455
I0127 01:25:45.852591 139866163582720 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.214707851409912, loss=2.75309157371521
I0127 01:26:19.795749 139862724310784 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.4332664012908936, loss=2.6726508140563965
I0127 01:26:53.609082 139866163582720 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.8477370738983154, loss=2.817253351211548
I0127 01:27:27.468391 139862724310784 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.3386645317077637, loss=2.6787338256835938
I0127 01:28:01.338213 139866163582720 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.6326799392700195, loss=2.720897674560547
I0127 01:28:35.208041 139862724310784 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.1382508277893066, loss=2.6446945667266846
I0127 01:29:09.089997 139866163582720 logging_writer.py:48] [65700] global_step=65700, grad_norm=4.025920867919922, loss=2.6330039501190186
I0127 01:29:42.920089 139862724310784 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.251178026199341, loss=2.6344246864318848
I0127 01:30:16.782443 139866163582720 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.1827845573425293, loss=2.6675734519958496
I0127 01:30:50.622101 139862724310784 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.8210508823394775, loss=2.6901626586914062
I0127 01:31:24.450883 139866163582720 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.3779757022857666, loss=2.5786221027374268
I0127 01:31:48.292132 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:31:54.468893 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:32:03.529445 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:32:05.809468 140027215431488 submission_runner.py:408] Time since start: 23414.78s, 	Step: 66172, 	{'train/accuracy': 0.7006337642669678, 'train/loss': 1.267736554145813, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.588563084602356, 'validation/num_examples': 50000, 'test/accuracy': 0.5067000389099121, 'test/loss': 2.267800807952881, 'test/num_examples': 10000, 'score': 22498.691545009613, 'total_duration': 23414.779324054718, 'accumulated_submission_time': 22498.691545009613, 'accumulated_eval_time': 911.7745883464813, 'accumulated_logging_time': 2.06522798538208}
I0127 01:32:05.840758 139862715918080 logging_writer.py:48] [66172] accumulated_eval_time=911.774588, accumulated_logging_time=2.065228, accumulated_submission_time=22498.691545, global_step=66172, preemption_count=0, score=22498.691545, test/accuracy=0.506700, test/loss=2.267801, test/num_examples=10000, total_duration=23414.779324, train/accuracy=0.700634, train/loss=1.267737, validation/accuracy=0.630680, validation/loss=1.588563, validation/num_examples=50000
I0127 01:32:15.645680 139862724310784 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.684713363647461, loss=2.680445432662964
I0127 01:32:49.464014 139862715918080 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.6242880821228027, loss=2.7056691646575928
I0127 01:33:23.287685 139862724310784 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.5550365447998047, loss=2.627244234085083
I0127 01:33:57.118023 139862715918080 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.532315969467163, loss=2.711510181427002
I0127 01:34:30.974531 139862724310784 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.348646402359009, loss=2.6389389038085938
I0127 01:35:04.877282 139862715918080 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.6454954147338867, loss=2.7308552265167236
I0127 01:35:38.723659 139862724310784 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.2764081954956055, loss=2.6573524475097656
I0127 01:36:12.565330 139862715918080 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.6054527759552, loss=2.710347890853882
I0127 01:36:46.412666 139862724310784 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.3899850845336914, loss=2.6790266036987305
I0127 01:37:20.293153 139862715918080 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.288236379623413, loss=2.6924831867218018
I0127 01:37:54.144548 139862724310784 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.4839870929718018, loss=2.629387855529785
I0127 01:38:28.053070 139862715918080 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.175732374191284, loss=2.6319141387939453
I0127 01:39:01.930975 139862724310784 logging_writer.py:48] [67400] global_step=67400, grad_norm=4.007474422454834, loss=2.5985612869262695
I0127 01:39:35.788506 139862715918080 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.4790265560150146, loss=2.6260907649993896
I0127 01:40:09.635585 139862724310784 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.3999571800231934, loss=2.720773696899414
I0127 01:40:35.844278 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:40:41.961518 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:40:50.891619 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:40:53.134036 140027215431488 submission_runner.py:408] Time since start: 23942.10s, 	Step: 67679, 	{'train/accuracy': 0.6891342401504517, 'train/loss': 1.355064868927002, 'validation/accuracy': 0.6316999793052673, 'validation/loss': 1.623766541481018, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.2622320652008057, 'test/num_examples': 10000, 'score': 23008.63331103325, 'total_duration': 23942.103929281235, 'accumulated_submission_time': 23008.63331103325, 'accumulated_eval_time': 929.0643086433411, 'accumulated_logging_time': 2.106299638748169}
I0127 01:40:53.162496 139866180368128 logging_writer.py:48] [67679] accumulated_eval_time=929.064309, accumulated_logging_time=2.106300, accumulated_submission_time=23008.633311, global_step=67679, preemption_count=0, score=23008.633311, test/accuracy=0.509800, test/loss=2.262232, test/num_examples=10000, total_duration=23942.103929, train/accuracy=0.689134, train/loss=1.355065, validation/accuracy=0.631700, validation/loss=1.623767, validation/num_examples=50000
I0127 01:41:00.610773 139866188760832 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.2919061183929443, loss=2.6421096324920654
I0127 01:41:34.453265 139866180368128 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.824000597000122, loss=2.713869333267212
I0127 01:42:08.322072 139866188760832 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.067838668823242, loss=2.6318130493164062
I0127 01:42:42.171544 139866180368128 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.3581905364990234, loss=2.694295644760132
I0127 01:43:16.008879 139866188760832 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.2525079250335693, loss=2.6748857498168945
I0127 01:43:49.901200 139866180368128 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.109182596206665, loss=2.6337060928344727
I0127 01:44:23.842464 139866188760832 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.2150025367736816, loss=2.5913796424865723
I0127 01:44:57.689048 139866180368128 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.681739091873169, loss=2.6269748210906982
I0127 01:45:31.563748 139866188760832 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.5839765071868896, loss=2.696021556854248
I0127 01:46:05.444903 139866180368128 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.5572173595428467, loss=2.5864741802215576
I0127 01:46:39.316066 139866188760832 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.470890760421753, loss=2.647257089614868
I0127 01:47:13.161151 139866180368128 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.686768054962158, loss=2.6592206954956055
I0127 01:47:47.033874 139866188760832 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.057267665863037, loss=2.7213549613952637
I0127 01:48:20.906525 139866180368128 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.8540215492248535, loss=2.560821533203125
I0127 01:48:54.779178 139866188760832 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.3808083534240723, loss=2.73453950881958
I0127 01:49:23.335414 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:49:29.536712 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:49:38.117025 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:49:40.397231 140027215431488 submission_runner.py:408] Time since start: 24469.37s, 	Step: 69186, 	{'train/accuracy': 0.6904296875, 'train/loss': 1.3263139724731445, 'validation/accuracy': 0.6378600001335144, 'validation/loss': 1.573560118675232, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.2438392639160156, 'test/num_examples': 10000, 'score': 23518.742659330368, 'total_duration': 24469.36714053154, 'accumulated_submission_time': 23518.742659330368, 'accumulated_eval_time': 946.1260778903961, 'accumulated_logging_time': 2.1450648307800293}
I0127 01:49:40.427434 139863663834880 logging_writer.py:48] [69186] accumulated_eval_time=946.126078, accumulated_logging_time=2.145065, accumulated_submission_time=23518.742659, global_step=69186, preemption_count=0, score=23518.742659, test/accuracy=0.509300, test/loss=2.243839, test/num_examples=10000, total_duration=24469.367141, train/accuracy=0.690430, train/loss=1.326314, validation/accuracy=0.637860, validation/loss=1.573560, validation/num_examples=50000
I0127 01:49:45.534200 139865760950016 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.184699535369873, loss=2.6140809059143066
I0127 01:50:19.405518 139863663834880 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.602540969848633, loss=2.571263074874878
I0127 01:50:53.314933 139865760950016 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.522493362426758, loss=2.5485353469848633
I0127 01:51:27.199858 139863663834880 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.5327258110046387, loss=2.7038679122924805
I0127 01:52:01.052165 139865760950016 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.049445390701294, loss=2.5574698448181152
I0127 01:52:34.888615 139863663834880 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.3558008670806885, loss=2.5950584411621094
I0127 01:53:08.737474 139865760950016 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.3570258617401123, loss=2.6118369102478027
I0127 01:53:42.599762 139863663834880 logging_writer.py:48] [69900] global_step=69900, grad_norm=4.177038669586182, loss=2.632934808731079
I0127 01:54:16.481857 139865760950016 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.588010311126709, loss=2.69020938873291
I0127 01:54:50.351772 139863663834880 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.780726909637451, loss=2.664992094039917
I0127 01:55:24.197683 139865760950016 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.065652370452881, loss=2.6016147136688232
I0127 01:55:58.050897 139863663834880 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.451659917831421, loss=2.612799644470215
I0127 01:56:31.917324 139865760950016 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.5324771404266357, loss=2.647261142730713
I0127 01:57:05.833312 139863663834880 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.5514044761657715, loss=2.6497621536254883
I0127 01:57:39.711752 139865760950016 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.439509391784668, loss=2.6768949031829834
I0127 01:58:10.665562 140027215431488 spec.py:321] Evaluating on the training split.
I0127 01:58:16.871393 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 01:58:25.587918 140027215431488 spec.py:349] Evaluating on the test split.
I0127 01:58:28.560076 140027215431488 submission_runner.py:408] Time since start: 24997.53s, 	Step: 70693, 	{'train/accuracy': 0.6878587007522583, 'train/loss': 1.3525885343551636, 'validation/accuracy': 0.6403399705886841, 'validation/loss': 1.573731780052185, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2697505950927734, 'test/num_examples': 10000, 'score': 24028.91832447052, 'total_duration': 24997.529990196228, 'accumulated_submission_time': 24028.91832447052, 'accumulated_eval_time': 964.0205476284027, 'accumulated_logging_time': 2.1862633228302}
I0127 01:58:28.588977 139862724310784 logging_writer.py:48] [70693] accumulated_eval_time=964.020548, accumulated_logging_time=2.186263, accumulated_submission_time=24028.918324, global_step=70693, preemption_count=0, score=24028.918324, test/accuracy=0.509100, test/loss=2.269751, test/num_examples=10000, total_duration=24997.529990, train/accuracy=0.687859, train/loss=1.352589, validation/accuracy=0.640340, validation/loss=1.573732, validation/num_examples=50000
I0127 01:58:31.306347 139866163582720 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.5223567485809326, loss=2.5808517932891846
I0127 01:59:05.157899 139862724310784 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.092473030090332, loss=2.6255369186401367
I0127 01:59:39.002682 139866163582720 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.1912171840667725, loss=2.71608829498291
I0127 02:00:12.864083 139862724310784 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.0152807235717773, loss=2.7043116092681885
I0127 02:00:46.736369 139866163582720 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.1782853603363037, loss=2.7916271686553955
I0127 02:01:20.589473 139862724310784 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.8802318572998047, loss=2.7227768898010254
I0127 02:01:54.444511 139866163582720 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.398730993270874, loss=2.6280505657196045
I0127 02:02:28.329025 139862724310784 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.307565450668335, loss=2.6011319160461426
I0127 02:03:02.234243 139866163582720 logging_writer.py:48] [71500] global_step=71500, grad_norm=4.0283942222595215, loss=2.6648991107940674
I0127 02:03:36.104736 139862724310784 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.207429885864258, loss=2.681943416595459
I0127 02:04:09.983951 139866163582720 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.0753173828125, loss=2.6216824054718018
I0127 02:04:43.820889 139862724310784 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.494230031967163, loss=2.6941802501678467
I0127 02:05:17.668828 139866163582720 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.9030585289001465, loss=2.65437388420105
I0127 02:05:51.554478 139862724310784 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.050317764282227, loss=2.698744058609009
I0127 02:06:25.411061 139866163582720 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.119777202606201, loss=2.6731584072113037
I0127 02:06:58.710439 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:07:04.971717 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:07:13.551920 140027215431488 spec.py:349] Evaluating on the test split.
I0127 02:07:15.844061 140027215431488 submission_runner.py:408] Time since start: 25524.81s, 	Step: 72200, 	{'train/accuracy': 0.6857063174247742, 'train/loss': 1.3495975732803345, 'validation/accuracy': 0.6386199593544006, 'validation/loss': 1.5710564851760864, 'validation/num_examples': 50000, 'test/accuracy': 0.5123000144958496, 'test/loss': 2.2388885021209717, 'test/num_examples': 10000, 'score': 24538.978356838226, 'total_duration': 25524.813975811005, 'accumulated_submission_time': 24538.978356838226, 'accumulated_eval_time': 981.154138803482, 'accumulated_logging_time': 2.2252821922302246}
I0127 02:07:15.873003 139865760950016 logging_writer.py:48] [72200] accumulated_eval_time=981.154139, accumulated_logging_time=2.225282, accumulated_submission_time=24538.978357, global_step=72200, preemption_count=0, score=24538.978357, test/accuracy=0.512300, test/loss=2.238889, test/num_examples=10000, total_duration=25524.813976, train/accuracy=0.685706, train/loss=1.349598, validation/accuracy=0.638620, validation/loss=1.571056, validation/num_examples=50000
I0127 02:07:16.224387 139865769342720 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.88686466217041, loss=2.5906331539154053
I0127 02:07:50.046889 139865760950016 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.5413429737091064, loss=2.670623779296875
I0127 02:08:23.901507 139865769342720 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.7678449153900146, loss=2.6849539279937744
I0127 02:08:57.801017 139865760950016 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.338312864303589, loss=2.730234146118164
I0127 02:09:31.671909 139865769342720 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.6461384296417236, loss=2.7602274417877197
I0127 02:10:05.508831 139865760950016 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.6988368034362793, loss=2.6174862384796143
I0127 02:10:39.366258 139865769342720 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.168266534805298, loss=2.6479127407073975
I0127 02:11:13.212402 139865760950016 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.7586090564727783, loss=2.608914852142334
I0127 02:11:47.091891 139865769342720 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.476369619369507, loss=2.6552796363830566
I0127 02:12:20.932857 139865760950016 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.464674472808838, loss=2.622915744781494
I0127 02:12:54.814041 139865769342720 logging_writer.py:48] [73200] global_step=73200, grad_norm=4.380324840545654, loss=2.595903158187866
I0127 02:13:28.659247 139865760950016 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.90814471244812, loss=2.548147678375244
I0127 02:14:02.506928 139865769342720 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.676306962966919, loss=2.699319362640381
I0127 02:14:36.373262 139865760950016 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.3990914821624756, loss=2.6303977966308594
I0127 02:15:10.327158 139865769342720 logging_writer.py:48] [73600] global_step=73600, grad_norm=5.147223472595215, loss=2.647428035736084
I0127 02:15:44.175296 139865760950016 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.3582205772399902, loss=2.6637747287750244
I0127 02:15:46.001899 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:15:52.170048 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:16:00.655783 140027215431488 spec.py:349] Evaluating on the test split.
I0127 02:16:02.974931 140027215431488 submission_runner.py:408] Time since start: 26051.94s, 	Step: 73707, 	{'train/accuracy': 0.6907086968421936, 'train/loss': 1.3185800313949585, 'validation/accuracy': 0.6348400115966797, 'validation/loss': 1.5660574436187744, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.202910900115967, 'test/num_examples': 10000, 'score': 25049.043762922287, 'total_duration': 26051.94483280182, 'accumulated_submission_time': 25049.043762922287, 'accumulated_eval_time': 998.1271076202393, 'accumulated_logging_time': 2.2647223472595215}
I0127 02:16:03.003620 139866171975424 logging_writer.py:48] [73707] accumulated_eval_time=998.127108, accumulated_logging_time=2.264722, accumulated_submission_time=25049.043763, global_step=73707, preemption_count=0, score=25049.043763, test/accuracy=0.517200, test/loss=2.202911, test/num_examples=10000, total_duration=26051.944833, train/accuracy=0.690709, train/loss=1.318580, validation/accuracy=0.634840, validation/loss=1.566057, validation/num_examples=50000
I0127 02:16:34.788645 139866180368128 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.6482226848602295, loss=2.719620943069458
I0127 02:17:08.599614 139866171975424 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.2165181636810303, loss=2.6588664054870605
I0127 02:17:42.427043 139866180368128 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.4732232093811035, loss=2.603830337524414
I0127 02:18:16.307414 139866171975424 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.6148247718811035, loss=2.6184909343719482
I0127 02:18:50.180169 139866180368128 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.337998151779175, loss=2.5873804092407227
I0127 02:19:24.020459 139866171975424 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.090444564819336, loss=2.6738016605377197
I0127 02:19:57.866301 139866180368128 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.524981737136841, loss=2.694533109664917
I0127 02:20:31.690083 139866171975424 logging_writer.py:48] [74500] global_step=74500, grad_norm=4.284049987792969, loss=2.614104747772217
I0127 02:21:05.542469 139866180368128 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.484220266342163, loss=2.647170066833496
I0127 02:21:39.484159 139866171975424 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.9062016010284424, loss=2.651430130004883
I0127 02:22:13.308254 139866180368128 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.6596755981445312, loss=2.5112738609313965
I0127 02:22:47.145170 139866171975424 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.585726499557495, loss=2.689159870147705
I0127 02:23:20.998989 139866180368128 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.317040205001831, loss=2.705981969833374
I0127 02:23:54.884379 139866171975424 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.225839138031006, loss=2.60561203956604
I0127 02:24:28.733263 139866180368128 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.01277756690979, loss=2.613328695297241
I0127 02:24:33.273368 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:24:40.099711 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:24:48.901018 140027215431488 spec.py:349] Evaluating on the test split.
I0127 02:24:51.239737 140027215431488 submission_runner.py:408] Time since start: 26580.21s, 	Step: 75215, 	{'train/accuracy': 0.7098612785339355, 'train/loss': 1.2410625219345093, 'validation/accuracy': 0.6350199580192566, 'validation/loss': 1.5747349262237549, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.2694790363311768, 'test/num_examples': 10000, 'score': 25559.250710248947, 'total_duration': 26580.20965051651, 'accumulated_submission_time': 25559.250710248947, 'accumulated_eval_time': 1016.0934247970581, 'accumulated_logging_time': 2.3037054538726807}
I0127 02:24:51.273119 139865769342720 logging_writer.py:48] [75215] accumulated_eval_time=1016.093425, accumulated_logging_time=2.303705, accumulated_submission_time=25559.250710, global_step=75215, preemption_count=0, score=25559.250710, test/accuracy=0.506900, test/loss=2.269479, test/num_examples=10000, total_duration=26580.209651, train/accuracy=0.709861, train/loss=1.241063, validation/accuracy=0.635020, validation/loss=1.574735, validation/num_examples=50000
I0127 02:25:20.390370 139866163582720 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.4417827129364014, loss=2.556425094604492
I0127 02:25:54.257968 139865769342720 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.715758800506592, loss=2.51776123046875
I0127 02:26:28.096371 139866163582720 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.0704874992370605, loss=2.625483751296997
I0127 02:27:01.939349 139865769342720 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.8731629848480225, loss=2.6563429832458496
I0127 02:27:35.846909 139866163582720 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.9343507289886475, loss=2.66237735748291
I0127 02:28:09.706192 139865769342720 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.2048611640930176, loss=2.6365673542022705
I0127 02:28:43.512505 139866163582720 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.380592107772827, loss=2.6758639812469482
I0127 02:29:17.354854 139865769342720 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.491391658782959, loss=2.5453286170959473
I0127 02:29:51.205576 139866163582720 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.4550673961639404, loss=2.61252760887146
I0127 02:30:25.085611 139865769342720 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.3082475662231445, loss=2.483072519302368
I0127 02:30:58.917955 139866163582720 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.7069170475006104, loss=2.588418960571289
I0127 02:31:32.742598 139865769342720 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.354419708251953, loss=2.6664507389068604
I0127 02:32:06.599925 139866163582720 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.5156679153442383, loss=2.5939440727233887
I0127 02:32:40.485000 139865769342720 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.7062342166900635, loss=2.63507342338562
I0127 02:33:14.324849 139866163582720 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.487370014190674, loss=2.7049496173858643
I0127 02:33:21.240027 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:33:27.418047 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:33:36.228375 140027215431488 spec.py:349] Evaluating on the test split.
I0127 02:33:38.486039 140027215431488 submission_runner.py:408] Time since start: 27107.46s, 	Step: 76722, 	{'train/accuracy': 0.7016701102256775, 'train/loss': 1.2833776473999023, 'validation/accuracy': 0.6454399824142456, 'validation/loss': 1.553180456161499, 'validation/num_examples': 50000, 'test/accuracy': 0.5156000256538391, 'test/loss': 2.2022013664245605, 'test/num_examples': 10000, 'score': 26069.155710458755, 'total_duration': 27107.455953359604, 'accumulated_submission_time': 26069.155710458755, 'accumulated_eval_time': 1033.3393914699554, 'accumulated_logging_time': 2.347717046737671}
I0127 02:33:38.517265 139866171975424 logging_writer.py:48] [76722] accumulated_eval_time=1033.339391, accumulated_logging_time=2.347717, accumulated_submission_time=26069.155710, global_step=76722, preemption_count=0, score=26069.155710, test/accuracy=0.515600, test/loss=2.202201, test/num_examples=10000, total_duration=27107.455953, train/accuracy=0.701670, train/loss=1.283378, validation/accuracy=0.645440, validation/loss=1.553180, validation/num_examples=50000
I0127 02:34:05.224208 139866180368128 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.295701503753662, loss=2.620296001434326
I0127 02:34:39.040190 139866171975424 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.959437131881714, loss=2.560121774673462
I0127 02:35:13.135307 139866180368128 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.1007134914398193, loss=2.5580334663391113
I0127 02:35:46.973913 139866171975424 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.4985036849975586, loss=2.6562695503234863
I0127 02:36:20.791920 139866180368128 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.280289888381958, loss=2.581019878387451
I0127 02:36:54.658671 139866171975424 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.501077175140381, loss=2.649449110031128
I0127 02:37:28.522460 139866180368128 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.629927158355713, loss=2.6559557914733887
I0127 02:38:02.359786 139866171975424 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.204089403152466, loss=2.614441394805908
I0127 02:38:36.236363 139866180368128 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.7006211280822754, loss=2.6077699661254883
I0127 02:39:10.075918 139866171975424 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.421905040740967, loss=2.578914165496826
I0127 02:39:43.996667 139866180368128 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.9065747261047363, loss=2.640455484390259
I0127 02:40:17.836163 139866171975424 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.0734636783599854, loss=2.543691396713257
I0127 02:40:51.675737 139866180368128 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.658893585205078, loss=2.5538930892944336
I0127 02:41:25.552756 139866171975424 logging_writer.py:48] [78100] global_step=78100, grad_norm=4.111752986907959, loss=2.5586647987365723
I0127 02:41:59.434015 139866180368128 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.5434470176696777, loss=2.6024487018585205
I0127 02:42:08.698553 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:42:14.812576 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:42:23.515481 140027215431488 spec.py:349] Evaluating on the test split.
I0127 02:42:25.709155 140027215431488 submission_runner.py:408] Time since start: 27634.68s, 	Step: 78229, 	{'train/accuracy': 0.7018494606018066, 'train/loss': 1.2792024612426758, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.5287810564041138, 'validation/num_examples': 50000, 'test/accuracy': 0.5181000232696533, 'test/loss': 2.1945981979370117, 'test/num_examples': 10000, 'score': 26579.27547645569, 'total_duration': 27634.679057359695, 'accumulated_submission_time': 26579.27547645569, 'accumulated_eval_time': 1050.3499314785004, 'accumulated_logging_time': 2.3884060382843018}
I0127 02:42:25.739803 139862715918080 logging_writer.py:48] [78229] accumulated_eval_time=1050.349931, accumulated_logging_time=2.388406, accumulated_submission_time=26579.275476, global_step=78229, preemption_count=0, score=26579.275476, test/accuracy=0.518100, test/loss=2.194598, test/num_examples=10000, total_duration=27634.679057, train/accuracy=0.701849, train/loss=1.279202, validation/accuracy=0.646920, validation/loss=1.528781, validation/num_examples=50000
I0127 02:42:50.112378 139862724310784 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.2073092460632324, loss=2.5773391723632812
I0127 02:43:23.997103 139862715918080 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.3469886779785156, loss=2.620189666748047
I0127 02:43:57.831318 139862724310784 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.647461175918579, loss=2.4999637603759766
I0127 02:44:31.675713 139862715918080 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.382039785385132, loss=2.6205430030822754
I0127 02:45:05.516948 139862724310784 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.299281597137451, loss=2.6247081756591797
I0127 02:45:39.370398 139862715918080 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.157899856567383, loss=2.465014934539795
I0127 02:46:13.402343 139862724310784 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.199950933456421, loss=2.589113235473633
I0127 02:46:47.238753 139862715918080 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.5379550457000732, loss=2.633307695388794
I0127 02:47:21.091039 139862724310784 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.6781227588653564, loss=2.6270790100097656
I0127 02:47:54.948325 139862715918080 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.4671995639801025, loss=2.7006683349609375
I0127 02:48:28.823463 139862724310784 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.335820198059082, loss=2.5422637462615967
I0127 02:49:02.662047 139862715918080 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.5534634590148926, loss=2.622300148010254
I0127 02:49:36.484681 139862724310784 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.2630767822265625, loss=2.758714437484741
I0127 02:50:10.320719 139862715918080 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.8752224445343018, loss=2.624077796936035
I0127 02:50:44.156738 139862724310784 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.3130476474761963, loss=2.557814598083496
I0127 02:50:55.786334 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:51:01.898123 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:51:10.655402 140027215431488 spec.py:349] Evaluating on the test split.
I0127 02:51:12.856517 140027215431488 submission_runner.py:408] Time since start: 28161.83s, 	Step: 79736, 	{'train/accuracy': 0.702168345451355, 'train/loss': 1.2795171737670898, 'validation/accuracy': 0.6496399641036987, 'validation/loss': 1.5087718963623047, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.1807165145874023, 'test/num_examples': 10000, 'score': 27089.258211374283, 'total_duration': 28161.826422214508, 'accumulated_submission_time': 27089.258211374283, 'accumulated_eval_time': 1067.4200563430786, 'accumulated_logging_time': 2.4313290119171143}
I0127 02:51:12.891759 139862724310784 logging_writer.py:48] [79736] accumulated_eval_time=1067.420056, accumulated_logging_time=2.431329, accumulated_submission_time=27089.258211, global_step=79736, preemption_count=0, score=27089.258211, test/accuracy=0.521800, test/loss=2.180717, test/num_examples=10000, total_duration=28161.826422, train/accuracy=0.702168, train/loss=1.279517, validation/accuracy=0.649640, validation/loss=1.508772, validation/num_examples=50000
I0127 02:51:34.901503 139866171975424 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.454420328140259, loss=2.574822187423706
I0127 02:52:08.777573 139862724310784 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.422590970993042, loss=2.6635944843292236
I0127 02:52:42.654923 139866171975424 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.5759148597717285, loss=2.5628294944763184
I0127 02:53:16.499023 139862724310784 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.529167413711548, loss=2.5961532592773438
I0127 02:53:50.346495 139866171975424 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.8239059448242188, loss=2.6396944522857666
I0127 02:54:24.194885 139862724310784 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.523183584213257, loss=2.648336410522461
I0127 02:54:58.063180 139866171975424 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.8228204250335693, loss=2.6099233627319336
I0127 02:55:31.899871 139862724310784 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.853062391281128, loss=2.6236603260040283
I0127 02:56:05.726298 139866171975424 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.4447555541992188, loss=2.559640645980835
I0127 02:56:39.591635 139862724310784 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.561264991760254, loss=2.649686813354492
I0127 02:57:13.470990 139866171975424 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.295494318008423, loss=2.6095340251922607
I0127 02:57:47.302712 139862724310784 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.0571792125701904, loss=2.5157556533813477
I0127 02:58:21.340416 139866171975424 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.516728162765503, loss=2.5978381633758545
I0127 02:58:55.189416 139862724310784 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.7405779361724854, loss=2.6651077270507812
I0127 02:59:29.056215 139866171975424 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.196880340576172, loss=2.5837724208831787
I0127 02:59:43.078468 140027215431488 spec.py:321] Evaluating on the training split.
I0127 02:59:49.328651 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 02:59:57.885323 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:00:00.176149 140027215431488 submission_runner.py:408] Time since start: 28689.15s, 	Step: 81243, 	{'train/accuracy': 0.6947743892669678, 'train/loss': 1.3432766199111938, 'validation/accuracy': 0.6473199725151062, 'validation/loss': 1.5561820268630981, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.222846031188965, 'test/num_examples': 10000, 'score': 27599.380972385406, 'total_duration': 28689.146060228348, 'accumulated_submission_time': 27599.380972385406, 'accumulated_eval_time': 1084.517686367035, 'accumulated_logging_time': 2.4775948524475098}
I0127 03:00:00.206067 139865769342720 logging_writer.py:48] [81243] accumulated_eval_time=1084.517686, accumulated_logging_time=2.477595, accumulated_submission_time=27599.380972, global_step=81243, preemption_count=0, score=27599.380972, test/accuracy=0.524300, test/loss=2.222846, test/num_examples=10000, total_duration=28689.146060, train/accuracy=0.694774, train/loss=1.343277, validation/accuracy=0.647320, validation/loss=1.556182, validation/num_examples=50000
I0127 03:00:19.832907 139866163582720 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.527538776397705, loss=2.5721797943115234
I0127 03:00:53.661393 139865769342720 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.3307738304138184, loss=2.577422618865967
I0127 03:01:27.527054 139866163582720 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.823168992996216, loss=2.6979970932006836
I0127 03:02:01.419172 139865769342720 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.9252467155456543, loss=2.609452724456787
I0127 03:02:35.268708 139866163582720 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.663663148880005, loss=2.626408100128174
I0127 03:03:09.129225 139865769342720 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.2153570652008057, loss=2.5820181369781494
I0127 03:03:43.005053 139866163582720 logging_writer.py:48] [81900] global_step=81900, grad_norm=4.3689117431640625, loss=2.5412373542785645
I0127 03:04:16.952121 139865769342720 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.6163418292999268, loss=2.583132266998291
I0127 03:04:50.813387 139866163582720 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.8075239658355713, loss=2.5110855102539062
I0127 03:05:24.667523 139865769342720 logging_writer.py:48] [82200] global_step=82200, grad_norm=4.627460956573486, loss=2.5497794151306152
I0127 03:05:58.484681 139866163582720 logging_writer.py:48] [82300] global_step=82300, grad_norm=4.432035446166992, loss=2.578845500946045
I0127 03:06:32.333986 139865769342720 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.2748517990112305, loss=2.546555519104004
I0127 03:07:06.176250 139866163582720 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.1058576107025146, loss=2.5613317489624023
I0127 03:07:40.013336 139865769342720 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.618910074234009, loss=2.625608444213867
I0127 03:08:13.895440 139866163582720 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.6480183601379395, loss=2.6613047122955322
I0127 03:08:30.292852 140027215431488 spec.py:321] Evaluating on the training split.
I0127 03:08:36.514151 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 03:08:45.152230 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:08:47.511308 140027215431488 submission_runner.py:408] Time since start: 29216.48s, 	Step: 82750, 	{'train/accuracy': 0.6988998651504517, 'train/loss': 1.2671300172805786, 'validation/accuracy': 0.6509999632835388, 'validation/loss': 1.4982835054397583, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.15671443939209, 'test/num_examples': 10000, 'score': 28109.40303182602, 'total_duration': 29216.481212615967, 'accumulated_submission_time': 28109.40303182602, 'accumulated_eval_time': 1101.7360954284668, 'accumulated_logging_time': 2.5183119773864746}
I0127 03:08:47.547116 139863663834880 logging_writer.py:48] [82750] accumulated_eval_time=1101.736095, accumulated_logging_time=2.518312, accumulated_submission_time=28109.403032, global_step=82750, preemption_count=0, score=28109.403032, test/accuracy=0.520600, test/loss=2.156714, test/num_examples=10000, total_duration=29216.481213, train/accuracy=0.698900, train/loss=1.267130, validation/accuracy=0.651000, validation/loss=1.498284, validation/num_examples=50000
I0127 03:09:04.800511 139865760950016 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.3569207191467285, loss=2.5810837745666504
I0127 03:09:38.635171 139863663834880 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.537382125854492, loss=2.6213653087615967
I0127 03:10:12.446028 139865760950016 logging_writer.py:48] [83000] global_step=83000, grad_norm=3.351473569869995, loss=2.539936065673828
I0127 03:10:46.357887 139863663834880 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.514399528503418, loss=2.613905191421509
I0127 03:11:20.211995 139865760950016 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.4731040000915527, loss=2.463834524154663
I0127 03:11:54.046831 139863663834880 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.805060386657715, loss=2.641862630844116
I0127 03:12:27.900525 139865760950016 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.633732795715332, loss=2.572232246398926
I0127 03:13:01.773313 139863663834880 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.259243488311768, loss=2.5707595348358154
I0127 03:13:35.629527 139865760950016 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.4779348373413086, loss=2.614814043045044
I0127 03:14:09.476139 139863663834880 logging_writer.py:48] [83700] global_step=83700, grad_norm=4.040493488311768, loss=2.605332851409912
I0127 03:14:43.301297 139865760950016 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.810152053833008, loss=2.666426658630371
I0127 03:15:17.155798 139863663834880 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.399124622344971, loss=2.6575822830200195
I0127 03:15:50.979834 139865760950016 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.414910316467285, loss=2.6203367710113525
I0127 03:16:24.819283 139863663834880 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.481343984603882, loss=2.608217477798462
I0127 03:16:58.737310 139865760950016 logging_writer.py:48] [84200] global_step=84200, grad_norm=4.247290134429932, loss=2.6389083862304688
I0127 03:17:17.812020 140027215431488 spec.py:321] Evaluating on the training split.
I0127 03:17:23.925561 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 03:17:32.802253 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:17:35.124841 140027215431488 submission_runner.py:408] Time since start: 29744.09s, 	Step: 84258, 	{'train/accuracy': 0.7250677347183228, 'train/loss': 1.181086778640747, 'validation/accuracy': 0.653499960899353, 'validation/loss': 1.501522421836853, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.192289113998413, 'test/num_examples': 10000, 'score': 28619.604907035828, 'total_duration': 29744.09475159645, 'accumulated_submission_time': 28619.604907035828, 'accumulated_eval_time': 1119.0488619804382, 'accumulated_logging_time': 2.564819812774658}
I0127 03:17:35.160472 139866180368128 logging_writer.py:48] [84258] accumulated_eval_time=1119.048862, accumulated_logging_time=2.564820, accumulated_submission_time=28619.604907, global_step=84258, preemption_count=0, score=28619.604907, test/accuracy=0.523100, test/loss=2.192289, test/num_examples=10000, total_duration=29744.094752, train/accuracy=0.725068, train/loss=1.181087, validation/accuracy=0.653500, validation/loss=1.501522, validation/num_examples=50000
I0127 03:17:49.756901 139866188760832 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.818485736846924, loss=2.5699656009674072
I0127 03:18:23.591959 139866180368128 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.605785608291626, loss=2.5889666080474854
I0127 03:18:57.428603 139866188760832 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.484225034713745, loss=2.5197904109954834
I0127 03:19:31.311884 139866180368128 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.81683349609375, loss=2.5968496799468994
I0127 03:20:05.165021 139866188760832 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.5380866527557373, loss=2.502923011779785
I0127 03:20:39.026291 139866180368128 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.62395977973938, loss=2.4996843338012695
I0127 03:21:12.887465 139866188760832 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.030401706695557, loss=2.7101104259490967
I0127 03:21:46.772525 139866180368128 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.63865065574646, loss=2.5995092391967773
I0127 03:22:20.646605 139866188760832 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.635669469833374, loss=2.6408796310424805
I0127 03:22:54.644755 139866180368128 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.3430402278900146, loss=2.5908870697021484
I0127 03:23:28.518750 139866188760832 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.9292991161346436, loss=2.5316689014434814
I0127 03:24:02.412699 139866180368128 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.83212947845459, loss=2.5871686935424805
I0127 03:24:36.269219 139866188760832 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.1320369243621826, loss=2.5525259971618652
I0127 03:25:10.116079 139866180368128 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.293064594268799, loss=2.5196192264556885
I0127 03:25:43.954664 139866188760832 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.821779251098633, loss=2.6679210662841797
I0127 03:26:05.433661 140027215431488 spec.py:321] Evaluating on the training split.
I0127 03:26:11.549018 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 03:26:20.283875 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:26:22.613489 140027215431488 submission_runner.py:408] Time since start: 30271.58s, 	Step: 85765, 	{'train/accuracy': 0.7072703838348389, 'train/loss': 1.2629165649414062, 'validation/accuracy': 0.644540011882782, 'validation/loss': 1.5403581857681274, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.1950738430023193, 'test/num_examples': 10000, 'score': 29129.81632399559, 'total_duration': 30271.58340406418, 'accumulated_submission_time': 29129.81632399559, 'accumulated_eval_time': 1136.2286508083344, 'accumulated_logging_time': 2.6103806495666504}
I0127 03:26:22.645156 139865760950016 logging_writer.py:48] [85765] accumulated_eval_time=1136.228651, accumulated_logging_time=2.610381, accumulated_submission_time=29129.816324, global_step=85765, preemption_count=0, score=29129.816324, test/accuracy=0.526000, test/loss=2.195074, test/num_examples=10000, total_duration=30271.583404, train/accuracy=0.707270, train/loss=1.262917, validation/accuracy=0.644540, validation/loss=1.540358, validation/num_examples=50000
I0127 03:26:34.850129 139865769342720 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.1756033897399902, loss=2.5860915184020996
I0127 03:27:08.683127 139865760950016 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.6558027267456055, loss=2.6027493476867676
I0127 03:27:42.510891 139865769342720 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.856623411178589, loss=2.623638391494751
I0127 03:28:16.347197 139865760950016 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.511293888092041, loss=2.5535812377929688
I0127 03:28:50.192694 139865769342720 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.467045545578003, loss=2.50346302986145
I0127 03:29:24.137029 139865760950016 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.8512494564056396, loss=2.617920398712158
I0127 03:29:57.974958 139865769342720 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.7044639587402344, loss=2.5159189701080322
I0127 03:30:31.831351 139865760950016 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.797870635986328, loss=2.5386345386505127
I0127 03:31:05.690037 139865769342720 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.5695173740386963, loss=2.5934643745422363
I0127 03:31:39.540895 139865760950016 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.6801867485046387, loss=2.6195483207702637
I0127 03:32:13.387080 139865769342720 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.979945182800293, loss=2.6467299461364746
I0127 03:32:47.253494 139865760950016 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.555457353591919, loss=2.5718905925750732
I0127 03:33:21.122618 139865769342720 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.2667951583862305, loss=2.536100387573242
I0127 03:33:54.978407 139865760950016 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.6189281940460205, loss=2.605863094329834
I0127 03:34:28.883187 139865769342720 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.4143013954162598, loss=2.6135082244873047
I0127 03:34:52.714827 140027215431488 spec.py:321] Evaluating on the training split.
I0127 03:34:59.138903 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 03:35:07.908999 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:35:10.241190 140027215431488 submission_runner.py:408] Time since start: 30799.21s, 	Step: 87272, 	{'train/accuracy': 0.7074697017669678, 'train/loss': 1.2444090843200684, 'validation/accuracy': 0.6504600048065186, 'validation/loss': 1.5148764848709106, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.167389154434204, 'test/num_examples': 10000, 'score': 29639.82198214531, 'total_duration': 30799.211097955704, 'accumulated_submission_time': 29639.82198214531, 'accumulated_eval_time': 1153.7549715042114, 'accumulated_logging_time': 2.653380870819092}
I0127 03:35:10.275235 139862724310784 logging_writer.py:48] [87272] accumulated_eval_time=1153.754972, accumulated_logging_time=2.653381, accumulated_submission_time=29639.821982, global_step=87272, preemption_count=0, score=29639.821982, test/accuracy=0.524700, test/loss=2.167389, test/num_examples=10000, total_duration=30799.211098, train/accuracy=0.707470, train/loss=1.244409, validation/accuracy=0.650460, validation/loss=1.514876, validation/num_examples=50000
I0127 03:35:20.064276 139863663834880 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.5377111434936523, loss=2.5163421630859375
I0127 03:35:53.901123 139862724310784 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.17360782623291, loss=2.5759689807891846
I0127 03:36:27.717107 139863663834880 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.584613800048828, loss=2.6142146587371826
I0127 03:37:01.579937 139862724310784 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.273701190948486, loss=2.5761733055114746
I0127 03:37:35.456188 139863663834880 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.779956817626953, loss=2.5639455318450928
I0127 03:38:09.255329 139862724310784 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.120995044708252, loss=2.5577316284179688
I0127 03:38:43.125832 139863663834880 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.25164794921875, loss=2.5560977458953857
I0127 03:39:16.927524 139862724310784 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.5220935344696045, loss=2.551149606704712
I0127 03:39:50.763498 139863663834880 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.4153330326080322, loss=2.6604063510894775
I0127 03:40:24.604861 139862724310784 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.820277214050293, loss=2.4863924980163574
I0127 03:40:58.447442 139863663834880 logging_writer.py:48] [88300] global_step=88300, grad_norm=4.070560455322266, loss=2.5718889236450195
I0127 03:41:32.392344 139862724310784 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.194842338562012, loss=2.5625338554382324
I0127 03:42:06.268510 139863663834880 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.800077199935913, loss=2.4988110065460205
I0127 03:42:40.122923 139862724310784 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.8147292137145996, loss=2.5491628646850586
I0127 03:43:13.954169 139863663834880 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.054216384887695, loss=2.508291244506836
I0127 03:43:40.452995 140027215431488 spec.py:321] Evaluating on the training split.
I0127 03:43:46.566574 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 03:43:55.262034 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:43:57.563410 140027215431488 submission_runner.py:408] Time since start: 31326.53s, 	Step: 88780, 	{'train/accuracy': 0.702566921710968, 'train/loss': 1.2763031721115112, 'validation/accuracy': 0.6502799987792969, 'validation/loss': 1.5065932273864746, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.174659013748169, 'test/num_examples': 10000, 'score': 30149.93711233139, 'total_duration': 31326.533321619034, 'accumulated_submission_time': 30149.93711233139, 'accumulated_eval_time': 1170.8653423786163, 'accumulated_logging_time': 2.6973180770874023}
I0127 03:43:57.596655 139862724310784 logging_writer.py:48] [88780] accumulated_eval_time=1170.865342, accumulated_logging_time=2.697318, accumulated_submission_time=30149.937112, global_step=88780, preemption_count=0, score=30149.937112, test/accuracy=0.526100, test/loss=2.174659, test/num_examples=10000, total_duration=31326.533322, train/accuracy=0.702567, train/loss=1.276303, validation/accuracy=0.650280, validation/loss=1.506593, validation/num_examples=50000
I0127 03:44:04.713460 139865769342720 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.1274149417877197, loss=2.5386064052581787
I0127 03:44:38.547087 139862724310784 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.807929039001465, loss=2.551884889602661
I0127 03:45:12.402743 139865769342720 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.640165090560913, loss=2.5782861709594727
I0127 03:45:46.283923 139862724310784 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.7998454570770264, loss=2.562791585922241
I0127 03:46:20.115035 139865769342720 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.6049258708953857, loss=2.4467508792877197
I0127 03:46:53.961351 139862724310784 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.067104339599609, loss=2.562185287475586
I0127 03:47:27.912757 139865769342720 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.629438877105713, loss=2.492069959640503
I0127 03:48:01.777105 139862724310784 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.4577460289001465, loss=2.514319658279419
I0127 03:48:35.637344 139865769342720 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.658207416534424, loss=2.5816242694854736
I0127 03:49:09.502609 139862724310784 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.6140544414520264, loss=2.4494357109069824
I0127 03:49:43.348140 139865769342720 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.496642589569092, loss=2.6445302963256836
I0127 03:50:17.194966 139862724310784 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.8655316829681396, loss=2.674419641494751
I0127 03:50:51.086565 139865769342720 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.488267183303833, loss=2.568040609359741
I0127 03:51:24.965376 139862724310784 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.593981981277466, loss=2.535006284713745
I0127 03:51:58.824463 139865769342720 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.9349334239959717, loss=2.5552234649658203
I0127 03:52:27.715876 140027215431488 spec.py:321] Evaluating on the training split.
I0127 03:52:33.924302 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 03:52:42.493257 140027215431488 spec.py:349] Evaluating on the test split.
I0127 03:52:44.772909 140027215431488 submission_runner.py:408] Time since start: 31853.74s, 	Step: 90287, 	{'train/accuracy': 0.7116549611091614, 'train/loss': 1.246743083000183, 'validation/accuracy': 0.6574999690055847, 'validation/loss': 1.4754823446273804, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.1418609619140625, 'test/num_examples': 10000, 'score': 30659.994156122208, 'total_duration': 31853.742823123932, 'accumulated_submission_time': 30659.994156122208, 'accumulated_eval_time': 1187.9223272800446, 'accumulated_logging_time': 2.740720748901367}
I0127 03:52:44.881357 139863663834880 logging_writer.py:48] [90287] accumulated_eval_time=1187.922327, accumulated_logging_time=2.740721, accumulated_submission_time=30659.994156, global_step=90287, preemption_count=0, score=30659.994156, test/accuracy=0.533300, test/loss=2.141861, test/num_examples=10000, total_duration=31853.742823, train/accuracy=0.711655, train/loss=1.246743, validation/accuracy=0.657500, validation/loss=1.475482, validation/num_examples=50000
I0127 03:52:49.614704 139865760950016 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.7239797115325928, loss=2.4964728355407715
I0127 03:53:23.541701 139863663834880 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.3827884197235107, loss=2.6535844802856445
I0127 03:53:57.388356 139865760950016 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.585740089416504, loss=2.5577592849731445
I0127 03:54:31.277810 139863663834880 logging_writer.py:48] [90600] global_step=90600, grad_norm=3.617692470550537, loss=2.6151175498962402
I0127 03:55:05.146775 139865760950016 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.440145492553711, loss=2.495643138885498
I0127 03:55:38.994979 139863663834880 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.6717727184295654, loss=2.49589467048645
I0127 03:56:12.900504 139865760950016 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.7643511295318604, loss=2.491976737976074
I0127 03:56:46.737136 139863663834880 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.5957374572753906, loss=2.49191951751709
I0127 03:57:20.586654 139865760950016 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.489534378051758, loss=2.495076894760132
I0127 03:57:54.488766 139863663834880 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.840304136276245, loss=2.5398569107055664
I0127 03:58:28.325317 139865760950016 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.5536999702453613, loss=2.5019912719726562
I0127 03:59:02.184675 139863663834880 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.818606376647949, loss=2.476015329360962
I0127 03:59:36.118775 139865760950016 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.011664390563965, loss=2.5367603302001953
I0127 04:00:09.985106 139863663834880 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.102978229522705, loss=2.5782108306884766
I0127 04:00:43.868749 139865760950016 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.73280930519104, loss=2.654862880706787
I0127 04:01:14.810567 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:01:20.974739 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:01:29.574263 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:01:31.854477 140027215431488 submission_runner.py:408] Time since start: 32380.82s, 	Step: 91793, 	{'train/accuracy': 0.7194674611091614, 'train/loss': 1.1982098817825317, 'validation/accuracy': 0.6654799580574036, 'validation/loss': 1.4386276006698608, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.08927321434021, 'test/num_examples': 10000, 'score': 31169.86101269722, 'total_duration': 32380.824385404587, 'accumulated_submission_time': 31169.86101269722, 'accumulated_eval_time': 1204.966181755066, 'accumulated_logging_time': 2.859598398208618}
I0127 04:01:31.888586 139862724310784 logging_writer.py:48] [91793] accumulated_eval_time=1204.966182, accumulated_logging_time=2.859598, accumulated_submission_time=31169.861013, global_step=91793, preemption_count=0, score=31169.861013, test/accuracy=0.543300, test/loss=2.089273, test/num_examples=10000, total_duration=32380.824385, train/accuracy=0.719467, train/loss=1.198210, validation/accuracy=0.665480, validation/loss=1.438628, validation/num_examples=50000
I0127 04:01:34.608795 139863663834880 logging_writer.py:48] [91800] global_step=91800, grad_norm=4.610188961029053, loss=2.585259437561035
I0127 04:02:08.462229 139862724310784 logging_writer.py:48] [91900] global_step=91900, grad_norm=3.7139360904693604, loss=2.516451835632324
I0127 04:02:42.297637 139863663834880 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.056230545043945, loss=2.603020668029785
I0127 04:03:16.123700 139862724310784 logging_writer.py:48] [92100] global_step=92100, grad_norm=3.4235918521881104, loss=2.5082411766052246
I0127 04:03:49.997888 139863663834880 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.304975986480713, loss=2.464064598083496
I0127 04:04:23.868225 139862724310784 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.972975730895996, loss=2.4834554195404053
I0127 04:04:57.712702 139863663834880 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.998084306716919, loss=2.5889575481414795
I0127 04:05:31.532747 139862724310784 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.3951311111450195, loss=2.6064884662628174
I0127 04:06:05.446835 139863663834880 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.6187057495117188, loss=2.512587547302246
I0127 04:06:39.282295 139862724310784 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.274896621704102, loss=2.5080063343048096
I0127 04:07:13.157234 139863663834880 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.8773787021636963, loss=2.6748108863830566
I0127 04:07:47.030991 139862724310784 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.85636305809021, loss=2.5437676906585693
I0127 04:08:20.852078 139863663834880 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.310001373291016, loss=2.563887357711792
I0127 04:08:54.731327 139862724310784 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.346063613891602, loss=2.5211453437805176
I0127 04:09:28.598459 139863663834880 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.784370183944702, loss=2.5679171085357666
I0127 04:10:01.889306 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:10:08.061610 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:10:16.609913 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:10:18.975847 140027215431488 submission_runner.py:408] Time since start: 32907.95s, 	Step: 93300, 	{'train/accuracy': 0.722676157951355, 'train/loss': 1.201149821281433, 'validation/accuracy': 0.6536999940872192, 'validation/loss': 1.5257678031921387, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.1804299354553223, 'test/num_examples': 10000, 'score': 31679.799534082413, 'total_duration': 32907.94575691223, 'accumulated_submission_time': 31679.799534082413, 'accumulated_eval_time': 1222.0526728630066, 'accumulated_logging_time': 2.903446912765503}
I0127 04:10:19.008733 139863663834880 logging_writer.py:48] [93300] accumulated_eval_time=1222.052673, accumulated_logging_time=2.903447, accumulated_submission_time=31679.799534, global_step=93300, preemption_count=0, score=31679.799534, test/accuracy=0.526800, test/loss=2.180430, test/num_examples=10000, total_duration=32907.945757, train/accuracy=0.722676, train/loss=1.201150, validation/accuracy=0.653700, validation/loss=1.525768, validation/num_examples=50000
I0127 04:10:19.357157 139865760950016 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.307816982269287, loss=2.5793962478637695
I0127 04:10:53.171675 139863663834880 logging_writer.py:48] [93400] global_step=93400, grad_norm=4.2858567237854, loss=2.515488624572754
I0127 04:11:27.013422 139865760950016 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.910944700241089, loss=2.509392261505127
I0127 04:12:00.946722 139863663834880 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.7775814533233643, loss=2.4831056594848633
I0127 04:12:34.788275 139865760950016 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.313738822937012, loss=2.6455116271972656
I0127 04:13:08.640877 139863663834880 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.640321731567383, loss=2.560328722000122
I0127 04:13:42.532128 139865760950016 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.6891424655914307, loss=2.5562400817871094
I0127 04:14:16.372916 139863663834880 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.134583950042725, loss=2.582362174987793
I0127 04:14:50.201843 139865760950016 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.9473180770874023, loss=2.449223041534424
I0127 04:15:24.030144 139863663834880 logging_writer.py:48] [94200] global_step=94200, grad_norm=3.469409942626953, loss=2.5072669982910156
I0127 04:15:57.874567 139865760950016 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.2368388175964355, loss=2.6835107803344727
I0127 04:16:31.747408 139863663834880 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.158811569213867, loss=2.480886697769165
I0127 04:17:05.592747 139865760950016 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.667879581451416, loss=2.6628522872924805
I0127 04:17:39.435952 139863663834880 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.6723546981811523, loss=2.4598641395568848
I0127 04:18:13.380300 139865760950016 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.522397518157959, loss=2.561934232711792
I0127 04:18:47.211109 139863663834880 logging_writer.py:48] [94800] global_step=94800, grad_norm=3.782411813735962, loss=2.5822319984436035
I0127 04:18:49.045168 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:18:55.175587 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:19:04.001014 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:19:06.261762 140027215431488 submission_runner.py:408] Time since start: 33435.23s, 	Step: 94807, 	{'train/accuracy': 0.7135283946990967, 'train/loss': 1.2340788841247559, 'validation/accuracy': 0.650439977645874, 'validation/loss': 1.527204155921936, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.177957057952881, 'test/num_examples': 10000, 'score': 32189.77259206772, 'total_duration': 33435.2316634655, 'accumulated_submission_time': 32189.77259206772, 'accumulated_eval_time': 1239.2692143917084, 'accumulated_logging_time': 2.946500778198242}
I0127 04:19:06.294800 139863663834880 logging_writer.py:48] [94807] accumulated_eval_time=1239.269214, accumulated_logging_time=2.946501, accumulated_submission_time=32189.772592, global_step=94807, preemption_count=0, score=32189.772592, test/accuracy=0.524700, test/loss=2.177957, test/num_examples=10000, total_duration=33435.231663, train/accuracy=0.713528, train/loss=1.234079, validation/accuracy=0.650440, validation/loss=1.527204, validation/num_examples=50000
I0127 04:19:38.090210 139866171975424 logging_writer.py:48] [94900] global_step=94900, grad_norm=3.6634950637817383, loss=2.602593183517456
I0127 04:20:11.943161 139863663834880 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.024054527282715, loss=2.56856107711792
I0127 04:20:45.763087 139866171975424 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.9076955318450928, loss=2.4657952785491943
I0127 04:21:19.594928 139863663834880 logging_writer.py:48] [95200] global_step=95200, grad_norm=3.804202079772949, loss=2.4642558097839355
I0127 04:21:53.408339 139866171975424 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.936786651611328, loss=2.514714241027832
I0127 04:22:27.222039 139863663834880 logging_writer.py:48] [95400] global_step=95400, grad_norm=3.9005215167999268, loss=2.548454523086548
I0127 04:23:01.042453 139866171975424 logging_writer.py:48] [95500] global_step=95500, grad_norm=3.4467904567718506, loss=2.501328945159912
I0127 04:23:34.876604 139863663834880 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.197838306427002, loss=2.4810569286346436
I0127 04:24:08.821600 139866171975424 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.158247470855713, loss=2.506458282470703
I0127 04:24:42.643855 139863663834880 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.936751127243042, loss=2.558837413787842
I0127 04:25:16.458373 139866171975424 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.358774185180664, loss=2.468688726425171
I0127 04:25:50.274768 139863663834880 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.8611278533935547, loss=2.5093414783477783
I0127 04:26:24.121966 139866171975424 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.718382358551025, loss=2.55814790725708
I0127 04:26:57.996219 139863663834880 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.410684585571289, loss=2.533351421356201
I0127 04:27:31.857665 139866171975424 logging_writer.py:48] [96300] global_step=96300, grad_norm=3.659285068511963, loss=2.4597740173339844
I0127 04:27:36.396708 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:27:42.613420 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:27:51.312423 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:27:53.620357 140027215431488 submission_runner.py:408] Time since start: 33962.59s, 	Step: 96315, 	{'train/accuracy': 0.7221978306770325, 'train/loss': 1.189970850944519, 'validation/accuracy': 0.6604399681091309, 'validation/loss': 1.467153549194336, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.1280345916748047, 'test/num_examples': 10000, 'score': 32699.811596155167, 'total_duration': 33962.59027004242, 'accumulated_submission_time': 32699.811596155167, 'accumulated_eval_time': 1256.4928257465363, 'accumulated_logging_time': 2.9893760681152344}
I0127 04:27:53.657042 139863663834880 logging_writer.py:48] [96315] accumulated_eval_time=1256.492826, accumulated_logging_time=2.989376, accumulated_submission_time=32699.811596, global_step=96315, preemption_count=0, score=32699.811596, test/accuracy=0.532800, test/loss=2.128035, test/num_examples=10000, total_duration=33962.590270, train/accuracy=0.722198, train/loss=1.189971, validation/accuracy=0.660440, validation/loss=1.467154, validation/num_examples=50000
I0127 04:28:22.731947 139865760950016 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.09390115737915, loss=2.6082921028137207
I0127 04:28:56.583434 139863663834880 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.301922798156738, loss=2.5779738426208496
I0127 04:29:30.447215 139865760950016 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.999382734298706, loss=2.59898042678833
I0127 04:30:04.313502 139863663834880 logging_writer.py:48] [96700] global_step=96700, grad_norm=3.857175350189209, loss=2.4815096855163574
I0127 04:30:38.228393 139865760950016 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.391590595245361, loss=2.5705058574676514
I0127 04:31:12.084918 139863663834880 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.8123865127563477, loss=2.514308452606201
I0127 04:31:45.921139 139865760950016 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.997316598892212, loss=2.419250249862671
I0127 04:32:19.800127 139863663834880 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.6552016735076904, loss=2.562239170074463
I0127 04:32:53.646965 139865760950016 logging_writer.py:48] [97200] global_step=97200, grad_norm=3.657749652862549, loss=2.4935848712921143
I0127 04:33:27.494516 139863663834880 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.859752655029297, loss=2.538317918777466
I0127 04:34:01.362949 139865760950016 logging_writer.py:48] [97400] global_step=97400, grad_norm=3.885061025619507, loss=2.5114591121673584
I0127 04:34:35.239963 139863663834880 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.759625196456909, loss=2.4905107021331787
I0127 04:35:09.086129 139865760950016 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.947620153427124, loss=2.5692973136901855
I0127 04:35:42.916513 139863663834880 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.124799728393555, loss=2.4726505279541016
I0127 04:36:16.788830 139865760950016 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.5368475914001465, loss=2.621795654296875
I0127 04:36:23.697473 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:36:30.110865 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:36:38.878535 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:36:41.180118 140027215431488 submission_runner.py:408] Time since start: 34490.15s, 	Step: 97822, 	{'train/accuracy': 0.725027859210968, 'train/loss': 1.1888576745986938, 'validation/accuracy': 0.668999969959259, 'validation/loss': 1.4445313215255737, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.084092617034912, 'test/num_examples': 10000, 'score': 33209.789820194244, 'total_duration': 34490.15003442764, 'accumulated_submission_time': 33209.789820194244, 'accumulated_eval_time': 1273.975423336029, 'accumulated_logging_time': 3.0374257564544678}
I0127 04:36:41.214193 139866163582720 logging_writer.py:48] [97822] accumulated_eval_time=1273.975423, accumulated_logging_time=3.037426, accumulated_submission_time=33209.789820, global_step=97822, preemption_count=0, score=33209.789820, test/accuracy=0.542300, test/loss=2.084093, test/num_examples=10000, total_duration=34490.150034, train/accuracy=0.725028, train/loss=1.188858, validation/accuracy=0.669000, validation/loss=1.444531, validation/num_examples=50000
I0127 04:37:07.968451 139866171975424 logging_writer.py:48] [97900] global_step=97900, grad_norm=3.612415075302124, loss=2.4731264114379883
I0127 04:37:41.840213 139866163582720 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.057305812835693, loss=2.5276668071746826
I0127 04:38:15.694242 139866171975424 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.113020896911621, loss=2.5510787963867188
I0127 04:38:49.591902 139866163582720 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.962815761566162, loss=2.418785572052002
I0127 04:39:23.442244 139866171975424 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.451956272125244, loss=2.5056793689727783
I0127 04:39:57.333198 139866163582720 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.519541025161743, loss=2.4663267135620117
I0127 04:40:31.188061 139866171975424 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.0465989112854, loss=2.477024555206299
I0127 04:41:05.037015 139866163582720 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.829838514328003, loss=2.509269952774048
I0127 04:41:38.887349 139866171975424 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.9763431549072266, loss=2.4970290660858154
I0127 04:42:12.785594 139866163582720 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.469264984130859, loss=2.575209617614746
I0127 04:42:46.784954 139866171975424 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.694951057434082, loss=2.444672107696533
I0127 04:43:20.630716 139866163582720 logging_writer.py:48] [99000] global_step=99000, grad_norm=3.6160755157470703, loss=2.574274778366089
I0127 04:43:54.480007 139866171975424 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.615631103515625, loss=2.49287748336792
I0127 04:44:28.351035 139866163582720 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.101039886474609, loss=2.494493007659912
I0127 04:45:02.208266 139866171975424 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.027071475982666, loss=2.49922251701355
I0127 04:45:11.477941 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:45:17.551413 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:45:26.269036 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:45:28.562944 140027215431488 submission_runner.py:408] Time since start: 35017.53s, 	Step: 99329, 	{'train/accuracy': 0.7132493257522583, 'train/loss': 1.2412854433059692, 'validation/accuracy': 0.6611199975013733, 'validation/loss': 1.480249047279358, 'validation/num_examples': 50000, 'test/accuracy': 0.5276000499725342, 'test/loss': 2.150489568710327, 'test/num_examples': 10000, 'score': 33719.99088358879, 'total_duration': 35017.532859802246, 'accumulated_submission_time': 33719.99088358879, 'accumulated_eval_time': 1291.060376405716, 'accumulated_logging_time': 3.081228733062744}
I0127 04:45:28.599685 139862715918080 logging_writer.py:48] [99329] accumulated_eval_time=1291.060376, accumulated_logging_time=3.081229, accumulated_submission_time=33719.990884, global_step=99329, preemption_count=0, score=33719.990884, test/accuracy=0.527600, test/loss=2.150490, test/num_examples=10000, total_duration=35017.532860, train/accuracy=0.713249, train/loss=1.241285, validation/accuracy=0.661120, validation/loss=1.480249, validation/num_examples=50000
I0127 04:45:52.975796 139862724310784 logging_writer.py:48] [99400] global_step=99400, grad_norm=3.4775302410125732, loss=2.4771974086761475
I0127 04:46:26.818577 139862715918080 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.9668984413146973, loss=2.5184364318847656
I0127 04:47:00.649132 139862724310784 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.9668238162994385, loss=2.5327305793762207
I0127 04:47:34.509058 139862715918080 logging_writer.py:48] [99700] global_step=99700, grad_norm=3.7898921966552734, loss=2.515409469604492
I0127 04:48:08.376553 139862724310784 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.7173590660095215, loss=2.421020746231079
I0127 04:48:42.205919 139862715918080 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.278476715087891, loss=2.5391016006469727
I0127 04:49:16.144367 139862724310784 logging_writer.py:48] [100000] global_step=100000, grad_norm=3.93833065032959, loss=2.3947598934173584
I0127 04:49:49.977295 139862715918080 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.697810411453247, loss=2.4799246788024902
I0127 04:50:23.817764 139862724310784 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.064404010772705, loss=2.5050554275512695
I0127 04:50:57.678543 139862715918080 logging_writer.py:48] [100300] global_step=100300, grad_norm=3.6670329570770264, loss=2.4735279083251953
I0127 04:51:31.549743 139862724310784 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.01118278503418, loss=2.494117498397827
I0127 04:52:05.385087 139862715918080 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.8430898189544678, loss=2.413336992263794
I0127 04:52:39.255016 139862724310784 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.271946907043457, loss=2.4213345050811768
I0127 04:53:13.090365 139862715918080 logging_writer.py:48] [100700] global_step=100700, grad_norm=3.8636748790740967, loss=2.445124864578247
I0127 04:53:46.922421 139862724310784 logging_writer.py:48] [100800] global_step=100800, grad_norm=3.6960461139678955, loss=2.425732374191284
I0127 04:53:58.602714 140027215431488 spec.py:321] Evaluating on the training split.
I0127 04:54:04.774133 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 04:54:13.325412 140027215431488 spec.py:349] Evaluating on the test split.
I0127 04:54:15.620448 140027215431488 submission_runner.py:408] Time since start: 35544.59s, 	Step: 100836, 	{'train/accuracy': 0.7284757494926453, 'train/loss': 1.1622774600982666, 'validation/accuracy': 0.6675999760627747, 'validation/loss': 1.4319740533828735, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.0938448905944824, 'test/num_examples': 10000, 'score': 34229.93264293671, 'total_duration': 35544.59036445618, 'accumulated_submission_time': 34229.93264293671, 'accumulated_eval_time': 1308.0780620574951, 'accumulated_logging_time': 3.127570152282715}
I0127 04:54:15.654933 139862724310784 logging_writer.py:48] [100836] accumulated_eval_time=1308.078062, accumulated_logging_time=3.127570, accumulated_submission_time=34229.932643, global_step=100836, preemption_count=0, score=34229.932643, test/accuracy=0.544800, test/loss=2.093845, test/num_examples=10000, total_duration=35544.590364, train/accuracy=0.728476, train/loss=1.162277, validation/accuracy=0.667600, validation/loss=1.431974, validation/num_examples=50000
I0127 04:54:37.647899 139866171975424 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.308781623840332, loss=2.577040672302246
I0127 04:55:11.537978 139862724310784 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.270845890045166, loss=2.4948275089263916
I0127 04:55:45.358548 139866171975424 logging_writer.py:48] [101100] global_step=101100, grad_norm=3.627732753753662, loss=2.5470688343048096
I0127 04:56:19.174996 139862724310784 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.326176166534424, loss=2.434504270553589
I0127 04:56:53.045697 139866171975424 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.297858238220215, loss=2.4825997352600098
I0127 04:57:26.879887 139862724310784 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.860114812850952, loss=2.4357409477233887
I0127 04:58:00.726975 139866171975424 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.946592330932617, loss=2.522183895111084
I0127 04:58:34.565413 139862724310784 logging_writer.py:48] [101600] global_step=101600, grad_norm=3.5638561248779297, loss=2.462099075317383
I0127 04:59:08.434057 139866171975424 logging_writer.py:48] [101700] global_step=101700, grad_norm=3.7324676513671875, loss=2.5600857734680176
I0127 04:59:42.295509 139862724310784 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.375863552093506, loss=2.393338203430176
I0127 05:00:16.153083 139866171975424 logging_writer.py:48] [101900] global_step=101900, grad_norm=3.9711074829101562, loss=2.5020933151245117
I0127 05:00:49.995577 139862724310784 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.003223419189453, loss=2.4438352584838867
I0127 05:01:23.987754 139866171975424 logging_writer.py:48] [102100] global_step=102100, grad_norm=3.6485161781311035, loss=2.4969282150268555
I0127 05:01:57.815670 139862724310784 logging_writer.py:48] [102200] global_step=102200, grad_norm=3.6477363109588623, loss=2.489839792251587
I0127 05:02:31.697683 139866171975424 logging_writer.py:48] [102300] global_step=102300, grad_norm=3.730567216873169, loss=2.4655418395996094
I0127 05:02:45.683083 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:02:51.857492 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:03:00.373474 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:03:02.701801 140027215431488 submission_runner.py:408] Time since start: 36071.67s, 	Step: 102343, 	{'train/accuracy': 0.7571149468421936, 'train/loss': 1.0458910465240479, 'validation/accuracy': 0.675879955291748, 'validation/loss': 1.4058371782302856, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.0450947284698486, 'test/num_examples': 10000, 'score': 34739.89816379547, 'total_duration': 36071.67170572281, 'accumulated_submission_time': 34739.89816379547, 'accumulated_eval_time': 1325.0967199802399, 'accumulated_logging_time': 3.1725218296051025}
I0127 05:03:02.741502 139866180368128 logging_writer.py:48] [102343] accumulated_eval_time=1325.096720, accumulated_logging_time=3.172522, accumulated_submission_time=34739.898164, global_step=102343, preemption_count=0, score=34739.898164, test/accuracy=0.551700, test/loss=2.045095, test/num_examples=10000, total_duration=36071.671706, train/accuracy=0.757115, train/loss=1.045891, validation/accuracy=0.675880, validation/loss=1.405837, validation/num_examples=50000
I0127 05:03:22.382635 139866188760832 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.130424976348877, loss=2.431002616882324
I0127 05:03:56.237655 139866180368128 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.244378089904785, loss=2.4094088077545166
I0127 05:04:30.121129 139866188760832 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.058506965637207, loss=2.4345827102661133
I0127 05:05:03.986938 139866180368128 logging_writer.py:48] [102700] global_step=102700, grad_norm=3.5979583263397217, loss=2.457841157913208
I0127 05:05:37.843795 139866188760832 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.291100978851318, loss=2.521484851837158
I0127 05:06:11.729899 139866180368128 logging_writer.py:48] [102900] global_step=102900, grad_norm=3.6123478412628174, loss=2.430180549621582
I0127 05:06:45.603741 139866188760832 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.498337745666504, loss=2.4799752235412598
I0127 05:07:19.446992 139866180368128 logging_writer.py:48] [103100] global_step=103100, grad_norm=3.5528438091278076, loss=2.4204659461975098
I0127 05:07:53.421618 139866188760832 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.1594085693359375, loss=2.4618101119995117
I0127 05:08:27.269949 139866180368128 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.567279815673828, loss=2.5162079334259033
I0127 05:09:01.115596 139866188760832 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.302914142608643, loss=2.4748799800872803
I0127 05:09:34.971156 139866180368128 logging_writer.py:48] [103500] global_step=103500, grad_norm=3.927100896835327, loss=2.437976837158203
I0127 05:10:08.858120 139866188760832 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.216721057891846, loss=2.5087738037109375
I0127 05:10:42.717431 139866180368128 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.138796329498291, loss=2.4304466247558594
I0127 05:11:16.552521 139866188760832 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.205580711364746, loss=2.3983983993530273
I0127 05:11:32.965247 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:11:39.068191 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:11:47.822592 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:11:50.109126 140027215431488 submission_runner.py:408] Time since start: 36599.08s, 	Step: 103850, 	{'train/accuracy': 0.7410514950752258, 'train/loss': 1.109355092048645, 'validation/accuracy': 0.6710000038146973, 'validation/loss': 1.4239152669906616, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.0953757762908936, 'test/num_examples': 10000, 'score': 35250.05817198753, 'total_duration': 36599.079026699066, 'accumulated_submission_time': 35250.05817198753, 'accumulated_eval_time': 1342.2405395507812, 'accumulated_logging_time': 3.224278211593628}
I0127 05:11:50.143648 139863663834880 logging_writer.py:48] [103850] accumulated_eval_time=1342.240540, accumulated_logging_time=3.224278, accumulated_submission_time=35250.058172, global_step=103850, preemption_count=0, score=35250.058172, test/accuracy=0.548700, test/loss=2.095376, test/num_examples=10000, total_duration=36599.079027, train/accuracy=0.741051, train/loss=1.109355, validation/accuracy=0.671000, validation/loss=1.423915, validation/num_examples=50000
I0127 05:12:07.375864 139865760950016 logging_writer.py:48] [103900] global_step=103900, grad_norm=3.8216166496276855, loss=2.451230525970459
I0127 05:12:41.217922 139863663834880 logging_writer.py:48] [104000] global_step=104000, grad_norm=3.7989656925201416, loss=2.4078409671783447
I0127 05:13:15.052952 139865760950016 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.199044227600098, loss=2.4891912937164307
I0127 05:13:48.978353 139863663834880 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.5382208824157715, loss=2.483330249786377
I0127 05:14:22.857958 139865760950016 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.058105945587158, loss=2.480752468109131
I0127 05:14:56.726118 139863663834880 logging_writer.py:48] [104400] global_step=104400, grad_norm=3.8757266998291016, loss=2.4844322204589844
I0127 05:15:30.592954 139865760950016 logging_writer.py:48] [104500] global_step=104500, grad_norm=3.9925577640533447, loss=2.4610235691070557
I0127 05:16:04.454899 139863663834880 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.285327434539795, loss=2.5217599868774414
I0127 05:16:38.302135 139865760950016 logging_writer.py:48] [104700] global_step=104700, grad_norm=3.9023234844207764, loss=2.479269504547119
I0127 05:17:12.184291 139863663834880 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.455448150634766, loss=2.4681155681610107
I0127 05:17:46.078467 139865760950016 logging_writer.py:48] [104900] global_step=104900, grad_norm=6.432748794555664, loss=2.5409092903137207
I0127 05:18:19.920020 139863663834880 logging_writer.py:48] [105000] global_step=105000, grad_norm=3.614821672439575, loss=2.386096239089966
I0127 05:18:53.804890 139865760950016 logging_writer.py:48] [105100] global_step=105100, grad_norm=3.9457643032073975, loss=2.4586102962493896
I0127 05:19:27.673928 139863663834880 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.176881790161133, loss=2.531811237335205
I0127 05:20:01.685649 139865760950016 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.490621089935303, loss=2.418071746826172
I0127 05:20:20.127413 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:20:26.314468 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:20:35.072562 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:20:37.358132 140027215431488 submission_runner.py:408] Time since start: 37126.33s, 	Step: 105356, 	{'train/accuracy': 0.7430245280265808, 'train/loss': 1.116188883781433, 'validation/accuracy': 0.6763399839401245, 'validation/loss': 1.4128817319869995, 'validation/num_examples': 50000, 'test/accuracy': 0.5460000038146973, 'test/loss': 2.064647674560547, 'test/num_examples': 10000, 'score': 35759.976840257645, 'total_duration': 37126.32803606987, 'accumulated_submission_time': 35759.976840257645, 'accumulated_eval_time': 1359.4712007045746, 'accumulated_logging_time': 3.270002841949463}
I0127 05:20:37.395638 139866163582720 logging_writer.py:48] [105356] accumulated_eval_time=1359.471201, accumulated_logging_time=3.270003, accumulated_submission_time=35759.976840, global_step=105356, preemption_count=0, score=35759.976840, test/accuracy=0.546000, test/loss=2.064648, test/num_examples=10000, total_duration=37126.328036, train/accuracy=0.743025, train/loss=1.116189, validation/accuracy=0.676340, validation/loss=1.412882, validation/num_examples=50000
I0127 05:20:52.622976 139866171975424 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.21462869644165, loss=2.5404744148254395
I0127 05:21:26.451218 139866163582720 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.677290439605713, loss=2.511392593383789
I0127 05:22:00.298895 139866171975424 logging_writer.py:48] [105600] global_step=105600, grad_norm=3.8394739627838135, loss=2.447819471359253
I0127 05:22:34.135338 139866163582720 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.528244972229004, loss=2.4868853092193604
I0127 05:23:07.969895 139866171975424 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.112986087799072, loss=2.4323229789733887
I0127 05:23:41.805344 139866163582720 logging_writer.py:48] [105900] global_step=105900, grad_norm=3.9827115535736084, loss=2.380706787109375
I0127 05:24:15.645328 139866171975424 logging_writer.py:48] [106000] global_step=106000, grad_norm=3.6533305644989014, loss=2.493591785430908
I0127 05:24:49.526500 139866163582720 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.1197590827941895, loss=2.447575569152832
I0127 05:25:23.383731 139866171975424 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.0235161781311035, loss=2.3651251792907715
I0127 05:25:57.373184 139866163582720 logging_writer.py:48] [106300] global_step=106300, grad_norm=3.621521234512329, loss=2.360139846801758
I0127 05:26:31.204391 139866171975424 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.230387210845947, loss=2.5019099712371826
I0127 05:27:05.079917 139866163582720 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.141031742095947, loss=2.5381009578704834
I0127 05:27:38.955264 139866171975424 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.098635196685791, loss=2.48152756690979
I0127 05:28:12.799808 139866163582720 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.266774654388428, loss=2.501180410385132
I0127 05:28:46.677427 139866171975424 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.2424187660217285, loss=2.4172260761260986
I0127 05:29:07.471313 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:29:13.575118 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:29:22.075310 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:29:24.369685 140027215431488 submission_runner.py:408] Time since start: 37653.34s, 	Step: 106863, 	{'train/accuracy': 0.7424864172935486, 'train/loss': 1.1154100894927979, 'validation/accuracy': 0.6801799535751343, 'validation/loss': 1.388283610343933, 'validation/num_examples': 50000, 'test/accuracy': 0.5570999979972839, 'test/loss': 2.026524066925049, 'test/num_examples': 10000, 'score': 36269.989028692245, 'total_duration': 37653.339567899704, 'accumulated_submission_time': 36269.989028692245, 'accumulated_eval_time': 1376.36949300766, 'accumulated_logging_time': 3.318311929702759}
I0127 05:29:24.408594 139862715918080 logging_writer.py:48] [106863] accumulated_eval_time=1376.369493, accumulated_logging_time=3.318312, accumulated_submission_time=36269.989029, global_step=106863, preemption_count=0, score=36269.989029, test/accuracy=0.557100, test/loss=2.026524, test/num_examples=10000, total_duration=37653.339568, train/accuracy=0.742486, train/loss=1.115410, validation/accuracy=0.680180, validation/loss=1.388284, validation/num_examples=50000
I0127 05:29:37.266109 139862724310784 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.852457046508789, loss=2.4690890312194824
I0127 05:30:11.097688 139862715918080 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.621821403503418, loss=2.3945631980895996
I0127 05:30:44.921196 139862724310784 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.154458999633789, loss=2.461200714111328
I0127 05:31:18.786403 139862715918080 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.086269378662109, loss=2.4025869369506836
I0127 05:31:52.664024 139862724310784 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.115684509277344, loss=2.368159532546997
I0127 05:32:26.669267 139862715918080 logging_writer.py:48] [107400] global_step=107400, grad_norm=3.954038619995117, loss=2.419802665710449
I0127 05:33:00.516232 139862724310784 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.0171308517456055, loss=2.3966612815856934
I0127 05:33:34.356827 139862715918080 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.002015113830566, loss=2.4150030612945557
I0127 05:34:08.209436 139862724310784 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.683950424194336, loss=2.519961357116699
I0127 05:34:42.105746 139862715918080 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.477202892303467, loss=2.4761338233947754
I0127 05:35:15.949531 139862724310784 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.06637716293335, loss=2.447235584259033
I0127 05:35:49.790850 139862715918080 logging_writer.py:48] [108000] global_step=108000, grad_norm=3.8870792388916016, loss=2.49448823928833
I0127 05:36:23.648462 139862724310784 logging_writer.py:48] [108100] global_step=108100, grad_norm=3.986448287963867, loss=2.4766244888305664
I0127 05:36:57.495056 139862715918080 logging_writer.py:48] [108200] global_step=108200, grad_norm=3.8827977180480957, loss=2.410283088684082
I0127 05:37:31.381086 139862724310784 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.225885391235352, loss=2.395214319229126
I0127 05:37:54.512902 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:38:00.688470 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:38:09.558782 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:38:11.854386 140027215431488 submission_runner.py:408] Time since start: 38180.82s, 	Step: 108370, 	{'train/accuracy': 0.7401745915412903, 'train/loss': 1.10150146484375, 'validation/accuracy': 0.6816999912261963, 'validation/loss': 1.3689839839935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.0306382179260254, 'test/num_examples': 10000, 'score': 36780.031766176224, 'total_duration': 38180.82426738739, 'accumulated_submission_time': 36780.031766176224, 'accumulated_eval_time': 1393.7109084129333, 'accumulated_logging_time': 3.367356061935425}
I0127 05:38:11.890026 139862724310784 logging_writer.py:48] [108370] accumulated_eval_time=1393.710908, accumulated_logging_time=3.367356, accumulated_submission_time=36780.031766, global_step=108370, preemption_count=0, score=36780.031766, test/accuracy=0.552800, test/loss=2.030638, test/num_examples=10000, total_duration=38180.824267, train/accuracy=0.740175, train/loss=1.101501, validation/accuracy=0.681700, validation/loss=1.368984, validation/num_examples=50000
I0127 05:38:22.370119 139866163582720 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.561560153961182, loss=2.507351875305176
I0127 05:38:56.215363 139862724310784 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.429436206817627, loss=2.3753623962402344
I0127 05:39:30.051016 139866163582720 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.228979587554932, loss=2.4695706367492676
I0127 05:40:03.886552 139862724310784 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.438536643981934, loss=2.4456114768981934
I0127 05:40:37.755449 139866163582720 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.463716506958008, loss=2.4551522731781006
I0127 05:41:11.643007 139862724310784 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.710737705230713, loss=2.410580635070801
I0127 05:41:45.489653 139866163582720 logging_writer.py:48] [109000] global_step=109000, grad_norm=4.215517997741699, loss=2.466264247894287
I0127 05:42:19.311959 139862724310784 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.3094563484191895, loss=2.5215306282043457
I0127 05:42:53.183423 139866163582720 logging_writer.py:48] [109200] global_step=109200, grad_norm=3.9427223205566406, loss=2.4494619369506836
I0127 05:43:27.076788 139862724310784 logging_writer.py:48] [109300] global_step=109300, grad_norm=5.073618412017822, loss=2.4485244750976562
I0127 05:44:00.930246 139866163582720 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.193089485168457, loss=2.40574312210083
I0127 05:44:34.917653 139862724310784 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.002015113830566, loss=2.420375108718872
I0127 05:45:08.792498 139866163582720 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.519369125366211, loss=2.5322256088256836
I0127 05:45:42.664379 139862724310784 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.147477626800537, loss=2.4466118812561035
I0127 05:46:16.493302 139866163582720 logging_writer.py:48] [109800] global_step=109800, grad_norm=3.9554877281188965, loss=2.482595443725586
I0127 05:46:42.038353 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:46:48.225946 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:46:56.984385 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:46:59.286421 140027215431488 submission_runner.py:408] Time since start: 38708.26s, 	Step: 109877, 	{'train/accuracy': 0.7429846525192261, 'train/loss': 1.0977813005447388, 'validation/accuracy': 0.6827200055122375, 'validation/loss': 1.3649893999099731, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.0068023204803467, 'test/num_examples': 10000, 'score': 37290.11735486984, 'total_duration': 38708.2563290596, 'accumulated_submission_time': 37290.11735486984, 'accumulated_eval_time': 1410.958943605423, 'accumulated_logging_time': 3.413281202316284}
I0127 05:46:59.325923 139862724310784 logging_writer.py:48] [109877] accumulated_eval_time=1410.958944, accumulated_logging_time=3.413281, accumulated_submission_time=37290.117355, global_step=109877, preemption_count=0, score=37290.117355, test/accuracy=0.559200, test/loss=2.006802, test/num_examples=10000, total_duration=38708.256329, train/accuracy=0.742985, train/loss=1.097781, validation/accuracy=0.682720, validation/loss=1.364989, validation/num_examples=50000
I0127 05:47:07.437208 139863663834880 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.042036533355713, loss=2.4157323837280273
I0127 05:47:41.255024 139862724310784 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.2216362953186035, loss=2.473360300064087
I0127 05:48:15.074877 139863663834880 logging_writer.py:48] [110100] global_step=110100, grad_norm=3.8520700931549072, loss=2.410721778869629
I0127 05:48:48.897668 139862724310784 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.369595050811768, loss=2.4336190223693848
I0127 05:49:22.726119 139863663834880 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.1893792152404785, loss=2.3327102661132812
I0127 05:49:56.558069 139862724310784 logging_writer.py:48] [110400] global_step=110400, grad_norm=3.8405497074127197, loss=2.465684175491333
I0127 05:50:30.466468 139863663834880 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.025469779968262, loss=2.442380905151367
I0127 05:51:04.343060 139862724310784 logging_writer.py:48] [110600] global_step=110600, grad_norm=3.9430623054504395, loss=2.3826537132263184
I0127 05:51:38.183375 139863663834880 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.101853370666504, loss=2.4563894271850586
I0127 05:52:12.039745 139862724310784 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.159399509429932, loss=2.388929843902588
I0127 05:52:45.892797 139863663834880 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.836022853851318, loss=2.48246169090271
I0127 05:53:19.764104 139862724310784 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.483530521392822, loss=2.346944808959961
I0127 05:53:53.604835 139863663834880 logging_writer.py:48] [111100] global_step=111100, grad_norm=3.8269245624542236, loss=2.3150997161865234
I0127 05:54:27.432200 139862724310784 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.481098651885986, loss=2.4637222290039062
I0127 05:55:01.274175 139863663834880 logging_writer.py:48] [111300] global_step=111300, grad_norm=5.693347454071045, loss=2.3789334297180176
I0127 05:55:29.509844 140027215431488 spec.py:321] Evaluating on the training split.
I0127 05:55:35.778280 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 05:55:44.410205 140027215431488 spec.py:349] Evaluating on the test split.
I0127 05:55:46.719997 140027215431488 submission_runner.py:408] Time since start: 39235.69s, 	Step: 111385, 	{'train/accuracy': 0.7664620280265808, 'train/loss': 1.0001466274261475, 'validation/accuracy': 0.6792399883270264, 'validation/loss': 1.3854286670684814, 'validation/num_examples': 50000, 'test/accuracy': 0.551300048828125, 'test/loss': 2.0405306816101074, 'test/num_examples': 10000, 'score': 37800.23961639404, 'total_duration': 39235.68991231918, 'accumulated_submission_time': 37800.23961639404, 'accumulated_eval_time': 1428.169054031372, 'accumulated_logging_time': 3.4625115394592285}
I0127 05:55:46.753870 139862715918080 logging_writer.py:48] [111385] accumulated_eval_time=1428.169054, accumulated_logging_time=3.462512, accumulated_submission_time=37800.239616, global_step=111385, preemption_count=0, score=37800.239616, test/accuracy=0.551300, test/loss=2.040531, test/num_examples=10000, total_duration=39235.689912, train/accuracy=0.766462, train/loss=1.000147, validation/accuracy=0.679240, validation/loss=1.385429, validation/num_examples=50000
I0127 05:55:52.170406 139862724310784 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.1684980392456055, loss=2.388610363006592
I0127 05:56:25.990038 139862715918080 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.2533464431762695, loss=2.37953519821167
I0127 05:56:59.953016 139862724310784 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.546812057495117, loss=2.385948657989502
I0127 05:57:33.796398 139862715918080 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.355477333068848, loss=2.3838791847229004
I0127 05:58:07.667732 139862724310784 logging_writer.py:48] [111800] global_step=111800, grad_norm=3.9746222496032715, loss=2.4893617630004883
I0127 05:58:41.541842 139862715918080 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.357768535614014, loss=2.3792176246643066
I0127 05:59:15.397433 139862724310784 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.923567771911621, loss=2.4506256580352783
I0127 05:59:49.257529 139862715918080 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.182967185974121, loss=2.381723165512085
I0127 06:00:23.108228 139862724310784 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.5282135009765625, loss=2.46350359916687
I0127 06:00:56.968895 139862715918080 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.922781467437744, loss=2.412144660949707
I0127 06:01:30.783905 139862724310784 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.469871520996094, loss=2.349536895751953
I0127 06:02:04.623632 139862715918080 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.178268909454346, loss=2.355754852294922
I0127 06:02:38.458793 139862724310784 logging_writer.py:48] [112600] global_step=112600, grad_norm=3.996601104736328, loss=2.4576468467712402
I0127 06:03:12.484238 139862715918080 logging_writer.py:48] [112700] global_step=112700, grad_norm=5.108054161071777, loss=2.5082499980926514
I0127 06:03:46.344736 139862724310784 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.0010199546813965, loss=2.3641533851623535
I0127 06:04:16.956110 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:04:23.209660 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:04:31.937875 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:04:34.237048 140027215431488 submission_runner.py:408] Time since start: 39763.21s, 	Step: 112892, 	{'train/accuracy': 0.7581512928009033, 'train/loss': 1.031977891921997, 'validation/accuracy': 0.6830799579620361, 'validation/loss': 1.3659889698028564, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 2.0102922916412354, 'test/num_examples': 10000, 'score': 38310.37893438339, 'total_duration': 39763.20695757866, 'accumulated_submission_time': 38310.37893438339, 'accumulated_eval_time': 1445.4499547481537, 'accumulated_logging_time': 3.506695508956909}
I0127 06:04:34.277337 139862724310784 logging_writer.py:48] [112892] accumulated_eval_time=1445.449955, accumulated_logging_time=3.506696, accumulated_submission_time=38310.378934, global_step=112892, preemption_count=0, score=38310.378934, test/accuracy=0.560500, test/loss=2.010292, test/num_examples=10000, total_duration=39763.206958, train/accuracy=0.758151, train/loss=1.031978, validation/accuracy=0.683080, validation/loss=1.365989, validation/num_examples=50000
I0127 06:04:37.319472 139863663834880 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.064518451690674, loss=2.418519973754883
I0127 06:05:11.143458 139862724310784 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.761756420135498, loss=2.444363832473755
I0127 06:05:44.975234 139863663834880 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.497021675109863, loss=2.336599588394165
I0127 06:06:18.836816 139862724310784 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.265775680541992, loss=2.4134228229522705
I0127 06:06:52.699480 139863663834880 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.647787094116211, loss=2.448664665222168
I0127 06:07:26.543830 139862724310784 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.131575107574463, loss=2.310668706893921
I0127 06:08:00.366644 139863663834880 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.0449371337890625, loss=2.358053684234619
I0127 06:08:34.218814 139862724310784 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.3572187423706055, loss=2.392854690551758
I0127 06:09:08.163786 139863663834880 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.14415168762207, loss=2.3368403911590576
I0127 06:09:41.991221 139862724310784 logging_writer.py:48] [113800] global_step=113800, grad_norm=5.732185363769531, loss=2.4274916648864746
I0127 06:10:15.846226 139863663834880 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.334296226501465, loss=2.378936767578125
I0127 06:10:49.678080 139862724310784 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.5206217765808105, loss=2.385711431503296
I0127 06:11:23.529941 139863663834880 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.891864776611328, loss=2.4670169353485107
I0127 06:11:57.403980 139862724310784 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.892726898193359, loss=2.447587490081787
I0127 06:12:31.274862 139863663834880 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.321483135223389, loss=2.423485279083252
I0127 06:13:04.268090 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:13:10.423750 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:13:19.051623 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:13:21.345198 140027215431488 submission_runner.py:408] Time since start: 40290.32s, 	Step: 114399, 	{'train/accuracy': 0.7623365521430969, 'train/loss': 1.0210669040679932, 'validation/accuracy': 0.6910600066184998, 'validation/loss': 1.3425350189208984, 'validation/num_examples': 50000, 'test/accuracy': 0.5693000555038452, 'test/loss': 1.9787520170211792, 'test/num_examples': 10000, 'score': 38820.30667281151, 'total_duration': 40290.31508851051, 'accumulated_submission_time': 38820.30667281151, 'accumulated_eval_time': 1462.527009487152, 'accumulated_logging_time': 3.55802059173584}
I0127 06:13:21.400107 139863663834880 logging_writer.py:48] [114399] accumulated_eval_time=1462.527009, accumulated_logging_time=3.558021, accumulated_submission_time=38820.306673, global_step=114399, preemption_count=0, score=38820.306673, test/accuracy=0.569300, test/loss=1.978752, test/num_examples=10000, total_duration=40290.315089, train/accuracy=0.762337, train/loss=1.021067, validation/accuracy=0.691060, validation/loss=1.342535, validation/num_examples=50000
I0127 06:13:22.080220 139865760950016 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.257882595062256, loss=2.398911476135254
I0127 06:13:55.942220 139863663834880 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.465198040008545, loss=2.313673734664917
I0127 06:14:29.802106 139865760950016 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.893187046051025, loss=2.4804270267486572
I0127 06:15:03.680845 139863663834880 logging_writer.py:48] [114700] global_step=114700, grad_norm=3.808417558670044, loss=2.2927942276000977
I0127 06:15:37.540250 139865760950016 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.15878963470459, loss=2.3384294509887695
I0127 06:16:11.380405 139863663834880 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.409584999084473, loss=2.3530097007751465
I0127 06:16:45.263305 139865760950016 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.162621974945068, loss=2.38631010055542
I0127 06:17:19.115178 139863663834880 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.3978729248046875, loss=2.364142894744873
I0127 06:17:52.953191 139865760950016 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.577371597290039, loss=2.4127683639526367
I0127 06:18:26.817424 139863663834880 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.4733052253723145, loss=2.286802053451538
I0127 06:19:00.698687 139865760950016 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.618923187255859, loss=2.460541009902954
I0127 06:19:34.555382 139863663834880 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.462703227996826, loss=2.3598201274871826
I0127 06:20:08.390846 139865760950016 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.036560535430908, loss=2.3561015129089355
I0127 06:20:42.240727 139863663834880 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.914153099060059, loss=2.47914457321167
I0127 06:21:16.147372 139865760950016 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.38975715637207, loss=2.365358352661133
I0127 06:21:49.992435 139863663834880 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.197872161865234, loss=2.4469079971313477
I0127 06:21:51.483875 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:21:57.692560 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:22:06.335334 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:22:08.701087 140027215431488 submission_runner.py:408] Time since start: 40817.67s, 	Step: 115906, 	{'train/accuracy': 0.7527702450752258, 'train/loss': 1.0518150329589844, 'validation/accuracy': 0.6881200075149536, 'validation/loss': 1.3431140184402466, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.0154571533203125, 'test/num_examples': 10000, 'score': 39330.32674264908, 'total_duration': 40817.67096376419, 'accumulated_submission_time': 39330.32674264908, 'accumulated_eval_time': 1479.7441306114197, 'accumulated_logging_time': 3.622938871383667}
I0127 06:22:08.745838 139863663834880 logging_writer.py:48] [115906] accumulated_eval_time=1479.744131, accumulated_logging_time=3.622939, accumulated_submission_time=39330.326743, global_step=115906, preemption_count=0, score=39330.326743, test/accuracy=0.559300, test/loss=2.015457, test/num_examples=10000, total_duration=40817.670964, train/accuracy=0.752770, train/loss=1.051815, validation/accuracy=0.688120, validation/loss=1.343114, validation/num_examples=50000
I0127 06:22:40.886862 139866171975424 logging_writer.py:48] [116000] global_step=116000, grad_norm=3.8951869010925293, loss=2.302168369293213
I0127 06:23:14.702876 139863663834880 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.81298828125, loss=2.451246500015259
I0127 06:23:48.558838 139866171975424 logging_writer.py:48] [116200] global_step=116200, grad_norm=4.524417877197266, loss=2.394059181213379
I0127 06:24:22.449589 139863663834880 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.4382452964782715, loss=2.3426513671875
I0127 06:24:56.309334 139866171975424 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.63962984085083, loss=2.3900814056396484
I0127 06:25:30.167520 139863663834880 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.4380011558532715, loss=2.333404064178467
I0127 06:26:04.021260 139866171975424 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.2193193435668945, loss=2.397719144821167
I0127 06:26:37.899204 139863663834880 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.693877220153809, loss=2.430311441421509
I0127 06:27:11.795118 139866171975424 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.3410749435424805, loss=2.3627514839172363
I0127 06:27:45.715060 139863663834880 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.560609340667725, loss=2.3975672721862793
I0127 06:28:19.595288 139866171975424 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.384147644042969, loss=2.23149037361145
I0127 06:28:53.466861 139863663834880 logging_writer.py:48] [117100] global_step=117100, grad_norm=5.107865810394287, loss=2.3334217071533203
I0127 06:29:27.295186 139866171975424 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.947049140930176, loss=2.3371567726135254
I0127 06:30:01.153855 139863663834880 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.91975736618042, loss=2.4194390773773193
I0127 06:30:35.047116 139866171975424 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.780759334564209, loss=2.396925210952759
I0127 06:30:38.917112 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:30:45.044054 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:30:53.703656 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:30:56.070258 140027215431488 submission_runner.py:408] Time since start: 41345.04s, 	Step: 117413, 	{'train/accuracy': 0.7529894709587097, 'train/loss': 1.057737946510315, 'validation/accuracy': 0.6910399794578552, 'validation/loss': 1.3415244817733765, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.969575047492981, 'test/num_examples': 10000, 'score': 39840.43624377251, 'total_duration': 41345.04013109207, 'accumulated_submission_time': 39840.43624377251, 'accumulated_eval_time': 1496.8971843719482, 'accumulated_logging_time': 3.6775968074798584}
I0127 06:30:56.108317 139862715918080 logging_writer.py:48] [117413] accumulated_eval_time=1496.897184, accumulated_logging_time=3.677597, accumulated_submission_time=39840.436244, global_step=117413, preemption_count=0, score=39840.436244, test/accuracy=0.570100, test/loss=1.969575, test/num_examples=10000, total_duration=41345.040131, train/accuracy=0.752989, train/loss=1.057738, validation/accuracy=0.691040, validation/loss=1.341524, validation/num_examples=50000
I0127 06:31:25.917271 139862724310784 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.893808841705322, loss=2.4066219329833984
I0127 06:31:59.767236 139862715918080 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.059171676635742, loss=2.4325780868530273
I0127 06:32:33.608112 139862724310784 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.542750358581543, loss=2.362518548965454
I0127 06:33:07.459350 139862715918080 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.007482528686523, loss=2.4585041999816895
I0127 06:33:41.408892 139862724310784 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.181112289428711, loss=2.38718843460083
I0127 06:34:15.245712 139862715918080 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.457276821136475, loss=2.4276270866394043
I0127 06:34:49.081973 139862724310784 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.426079750061035, loss=2.451918363571167
I0127 06:35:22.924608 139862715918080 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.645481586456299, loss=2.3940820693969727
I0127 06:35:56.782951 139862724310784 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.420291423797607, loss=2.3961658477783203
I0127 06:36:30.651950 139862715918080 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.198207378387451, loss=2.252697706222534
I0127 06:37:04.504433 139862724310784 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.823433876037598, loss=2.367257833480835
I0127 06:37:38.345309 139862715918080 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.883321285247803, loss=2.3489036560058594
I0127 06:38:12.210429 139862724310784 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.54495906829834, loss=2.336747407913208
I0127 06:38:46.102080 139862715918080 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.711831569671631, loss=2.330984592437744
I0127 06:39:19.950386 139862724310784 logging_writer.py:48] [118900] global_step=118900, grad_norm=5.267984867095947, loss=2.369894027709961
I0127 06:39:26.347818 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:39:32.594693 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:39:41.363601 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:39:43.573158 140027215431488 submission_runner.py:408] Time since start: 41872.54s, 	Step: 118920, 	{'train/accuracy': 0.7530691623687744, 'train/loss': 1.0459004640579224, 'validation/accuracy': 0.6911999583244324, 'validation/loss': 1.3308132886886597, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9982550144195557, 'test/num_examples': 10000, 'score': 40350.61412215233, 'total_duration': 41872.54306650162, 'accumulated_submission_time': 40350.61412215233, 'accumulated_eval_time': 1514.1224842071533, 'accumulated_logging_time': 3.7258994579315186}
I0127 06:39:43.611023 139862724310784 logging_writer.py:48] [118920] accumulated_eval_time=1514.122484, accumulated_logging_time=3.725899, accumulated_submission_time=40350.614122, global_step=118920, preemption_count=0, score=40350.614122, test/accuracy=0.565400, test/loss=1.998255, test/num_examples=10000, total_duration=41872.543067, train/accuracy=0.753069, train/loss=1.045900, validation/accuracy=0.691200, validation/loss=1.330813, validation/num_examples=50000
I0127 06:40:11.017380 139866171975424 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.491677761077881, loss=2.390939235687256
I0127 06:40:44.860089 139862724310784 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.183333873748779, loss=2.3862597942352295
I0127 06:41:18.717570 139866171975424 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.521051406860352, loss=2.3073437213897705
I0127 06:41:52.575330 139862724310784 logging_writer.py:48] [119300] global_step=119300, grad_norm=4.192122936248779, loss=2.3478260040283203
I0127 06:42:26.409322 139866171975424 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.955075263977051, loss=2.363750457763672
I0127 06:43:00.249454 139862724310784 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.907557010650635, loss=2.383763074874878
I0127 06:43:34.129406 139866171975424 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.7372331619262695, loss=2.390476703643799
I0127 06:44:07.949553 139862724310784 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.348208427429199, loss=2.3050010204315186
I0127 06:44:41.788327 139866171975424 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.6358537673950195, loss=2.4443087577819824
I0127 06:45:15.628852 139862724310784 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.450558662414551, loss=2.3745007514953613
I0127 06:45:49.562320 139866171975424 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.471681594848633, loss=2.330141305923462
I0127 06:46:23.405031 139862724310784 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.332887172698975, loss=2.320091724395752
I0127 06:46:57.252628 139866171975424 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.320814609527588, loss=2.3525869846343994
I0127 06:47:31.109903 139862724310784 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.398516654968262, loss=2.3331992626190186
I0127 06:48:04.986056 139866171975424 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.335782527923584, loss=2.2784371376037598
I0127 06:48:13.580695 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:48:19.766961 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:48:28.599867 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:48:30.924808 140027215431488 submission_runner.py:408] Time since start: 42399.89s, 	Step: 120427, 	{'train/accuracy': 0.7836814522743225, 'train/loss': 0.9314776659011841, 'validation/accuracy': 0.693120002746582, 'validation/loss': 1.3276666402816772, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 1.9804224967956543, 'test/num_examples': 10000, 'score': 40860.520089387894, 'total_duration': 42399.89472198486, 'accumulated_submission_time': 40860.520089387894, 'accumulated_eval_time': 1531.4665586948395, 'accumulated_logging_time': 3.774169683456421}
I0127 06:48:30.968893 139865760950016 logging_writer.py:48] [120427] accumulated_eval_time=1531.466559, accumulated_logging_time=3.774170, accumulated_submission_time=40860.520089, global_step=120427, preemption_count=0, score=40860.520089, test/accuracy=0.566400, test/loss=1.980422, test/num_examples=10000, total_duration=42399.894722, train/accuracy=0.783681, train/loss=0.931478, validation/accuracy=0.693120, validation/loss=1.327667, validation/num_examples=50000
I0127 06:48:56.026224 139865769342720 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.60463285446167, loss=2.331043004989624
I0127 06:49:29.867186 139865760950016 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.716431617736816, loss=2.3478407859802246
I0127 06:50:03.727478 139865769342720 logging_writer.py:48] [120700] global_step=120700, grad_norm=4.871013641357422, loss=2.3237695693969727
I0127 06:50:37.537437 139865760950016 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.460170269012451, loss=2.3433189392089844
I0127 06:51:11.390016 139865769342720 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.621758460998535, loss=2.2591257095336914
I0127 06:51:45.349551 139865760950016 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.918972492218018, loss=2.413175344467163
I0127 06:52:19.197204 139865769342720 logging_writer.py:48] [121100] global_step=121100, grad_norm=4.220986843109131, loss=2.280149459838867
I0127 06:52:53.052030 139865760950016 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.866786479949951, loss=2.2373898029327393
I0127 06:53:26.890954 139865769342720 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.462069511413574, loss=2.3048458099365234
I0127 06:54:00.732899 139865760950016 logging_writer.py:48] [121400] global_step=121400, grad_norm=6.074312210083008, loss=2.3229684829711914
I0127 06:54:34.615993 139865769342720 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.528855800628662, loss=2.33109450340271
I0127 06:55:08.438536 139865760950016 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.492793560028076, loss=2.362297773361206
I0127 06:55:42.278178 139865769342720 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.603373050689697, loss=2.315850019454956
I0127 06:56:16.142226 139865760950016 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.533961296081543, loss=2.3882248401641846
I0127 06:56:49.985391 139865769342720 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.40147590637207, loss=2.2838075160980225
I0127 06:57:00.968674 140027215431488 spec.py:321] Evaluating on the training split.
I0127 06:57:07.163056 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 06:57:15.894666 140027215431488 spec.py:349] Evaluating on the test split.
I0127 06:57:18.212450 140027215431488 submission_runner.py:408] Time since start: 42927.18s, 	Step: 121934, 	{'train/accuracy': 0.7770846486091614, 'train/loss': 0.960997998714447, 'validation/accuracy': 0.7005199790000916, 'validation/loss': 1.2985070943832397, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 1.9412811994552612, 'test/num_examples': 10000, 'score': 41370.45637798309, 'total_duration': 42927.18236398697, 'accumulated_submission_time': 41370.45637798309, 'accumulated_eval_time': 1548.710284948349, 'accumulated_logging_time': 3.8285911083221436}
I0127 06:57:18.253042 139862724310784 logging_writer.py:48] [121934] accumulated_eval_time=1548.710285, accumulated_logging_time=3.828591, accumulated_submission_time=41370.456378, global_step=121934, preemption_count=0, score=41370.456378, test/accuracy=0.576700, test/loss=1.941281, test/num_examples=10000, total_duration=42927.182364, train/accuracy=0.777085, train/loss=0.960998, validation/accuracy=0.700520, validation/loss=1.298507, validation/num_examples=50000
I0127 06:57:40.947516 139863663834880 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.823914527893066, loss=2.3190882205963135
I0127 06:58:14.852738 139862724310784 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.954368591308594, loss=2.3779213428497314
I0127 06:58:48.694513 139863663834880 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.761645317077637, loss=2.292246103286743
I0127 06:59:22.513568 139862724310784 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.591047763824463, loss=2.3480355739593506
I0127 06:59:56.339870 139863663834880 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.05152702331543, loss=2.2960965633392334
I0127 07:00:30.186852 139862724310784 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.600072860717773, loss=2.3324828147888184
I0127 07:01:04.071121 139863663834880 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.812873840332031, loss=2.247058629989624
I0127 07:01:37.911048 139862724310784 logging_writer.py:48] [122700] global_step=122700, grad_norm=4.496954917907715, loss=2.350510358810425
I0127 07:02:11.758523 139863663834880 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.678561210632324, loss=2.2981607913970947
I0127 07:02:45.606077 139862724310784 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.715779781341553, loss=2.3589038848876953
I0127 07:03:19.467762 139863663834880 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.977549076080322, loss=2.328693389892578
I0127 07:03:53.313148 139862724310784 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.6077985763549805, loss=2.2138655185699463
I0127 07:04:27.325151 139863663834880 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.730992794036865, loss=2.3117690086364746
I0127 07:05:01.151449 139862724310784 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.21634578704834, loss=2.362445116043091
I0127 07:05:34.982429 139863663834880 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.696400165557861, loss=2.3873698711395264
I0127 07:05:48.350902 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:05:54.470104 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:06:03.187632 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:06:05.429989 140027215431488 submission_runner.py:408] Time since start: 43454.40s, 	Step: 123441, 	{'train/accuracy': 0.7704280614852905, 'train/loss': 0.9899856448173523, 'validation/accuracy': 0.6974999904632568, 'validation/loss': 1.3206807374954224, 'validation/num_examples': 50000, 'test/accuracy': 0.5712000131607056, 'test/loss': 1.9497255086898804, 'test/num_examples': 10000, 'score': 41880.491114616394, 'total_duration': 43454.39989852905, 'accumulated_submission_time': 41880.491114616394, 'accumulated_eval_time': 1565.7893166542053, 'accumulated_logging_time': 3.87968111038208}
I0127 07:06:05.468383 139865769342720 logging_writer.py:48] [123441] accumulated_eval_time=1565.789317, accumulated_logging_time=3.879681, accumulated_submission_time=41880.491115, global_step=123441, preemption_count=0, score=41880.491115, test/accuracy=0.571200, test/loss=1.949726, test/num_examples=10000, total_duration=43454.399899, train/accuracy=0.770428, train/loss=0.989986, validation/accuracy=0.697500, validation/loss=1.320681, validation/num_examples=50000
I0127 07:06:25.782292 139866163582720 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.46215295791626, loss=2.3132736682891846
I0127 07:06:59.623615 139865769342720 logging_writer.py:48] [123600] global_step=123600, grad_norm=4.984944820404053, loss=2.232753276824951
I0127 07:07:33.446376 139866163582720 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.943942546844482, loss=2.337617874145508
I0127 07:08:07.318779 139865769342720 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.410526752471924, loss=2.2355170249938965
I0127 07:08:41.129715 139866163582720 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.465073585510254, loss=2.3661048412323
I0127 07:09:14.964960 139865769342720 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.200328350067139, loss=2.345268726348877
I0127 07:09:48.857331 139866163582720 logging_writer.py:48] [124100] global_step=124100, grad_norm=4.652879238128662, loss=2.3417458534240723
I0127 07:10:22.816427 139865769342720 logging_writer.py:48] [124200] global_step=124200, grad_norm=4.901318550109863, loss=2.368161201477051
I0127 07:10:56.688106 139866163582720 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.149022579193115, loss=2.1800284385681152
I0127 07:11:30.564798 139865769342720 logging_writer.py:48] [124400] global_step=124400, grad_norm=4.975851058959961, loss=2.401412010192871
I0127 07:12:04.402644 139866163582720 logging_writer.py:48] [124500] global_step=124500, grad_norm=4.606880187988281, loss=2.387164831161499
I0127 07:12:38.240921 139865769342720 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.536657333374023, loss=2.384758234024048
I0127 07:13:12.110443 139866163582720 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.130061626434326, loss=2.285942316055298
I0127 07:13:45.979963 139865769342720 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.047517776489258, loss=2.310513973236084
I0127 07:14:19.813176 139866163582720 logging_writer.py:48] [124900] global_step=124900, grad_norm=4.950357437133789, loss=2.368964672088623
I0127 07:14:35.526466 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:14:41.626177 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:14:50.307650 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:14:52.545169 140027215431488 submission_runner.py:408] Time since start: 43981.52s, 	Step: 124948, 	{'train/accuracy': 0.7798947691917419, 'train/loss': 0.9331660270690918, 'validation/accuracy': 0.7085399627685547, 'validation/loss': 1.2580126523971558, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.8990615606307983, 'test/num_examples': 10000, 'score': 42390.486533641815, 'total_duration': 43981.515066862106, 'accumulated_submission_time': 42390.486533641815, 'accumulated_eval_time': 1582.8079690933228, 'accumulated_logging_time': 3.928217649459839}
I0127 07:14:52.586578 139863663834880 logging_writer.py:48] [124948] accumulated_eval_time=1582.807969, accumulated_logging_time=3.928218, accumulated_submission_time=42390.486534, global_step=124948, preemption_count=0, score=42390.486534, test/accuracy=0.580500, test/loss=1.899062, test/num_examples=10000, total_duration=43981.515067, train/accuracy=0.779895, train/loss=0.933166, validation/accuracy=0.708540, validation/loss=1.258013, validation/num_examples=50000
I0127 07:15:10.558413 139865760950016 logging_writer.py:48] [125000] global_step=125000, grad_norm=4.91955041885376, loss=2.363187551498413
I0127 07:15:44.382992 139863663834880 logging_writer.py:48] [125100] global_step=125100, grad_norm=4.718467712402344, loss=2.3075687885284424
I0127 07:16:18.286144 139865760950016 logging_writer.py:48] [125200] global_step=125200, grad_norm=4.996308326721191, loss=2.263573169708252
I0127 07:16:52.144634 139863663834880 logging_writer.py:48] [125300] global_step=125300, grad_norm=4.843837261199951, loss=2.3487842082977295
I0127 07:17:26.024179 139865760950016 logging_writer.py:48] [125400] global_step=125400, grad_norm=4.628003120422363, loss=2.304185628890991
I0127 07:17:59.850780 139863663834880 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.45797061920166, loss=2.352491855621338
I0127 07:18:33.713233 139865760950016 logging_writer.py:48] [125600] global_step=125600, grad_norm=6.454068183898926, loss=2.3307974338531494
I0127 07:19:07.529255 139863663834880 logging_writer.py:48] [125700] global_step=125700, grad_norm=4.85280704498291, loss=2.2985761165618896
I0127 07:19:41.367683 139865760950016 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.330218315124512, loss=2.3730170726776123
I0127 07:20:15.220276 139863663834880 logging_writer.py:48] [125900] global_step=125900, grad_norm=4.540157318115234, loss=2.1836841106414795
I0127 07:20:49.061079 139865760950016 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.349269866943359, loss=2.326589584350586
I0127 07:21:22.920128 139863663834880 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.243071556091309, loss=2.1860785484313965
I0127 07:21:56.793271 139865760950016 logging_writer.py:48] [126200] global_step=126200, grad_norm=4.442412853240967, loss=2.279428005218506
I0127 07:22:30.709977 139863663834880 logging_writer.py:48] [126300] global_step=126300, grad_norm=6.142633438110352, loss=2.3596551418304443
I0127 07:23:04.508330 139865760950016 logging_writer.py:48] [126400] global_step=126400, grad_norm=4.587696552276611, loss=2.236464500427246
I0127 07:23:22.592281 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:23:28.742148 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:23:37.227855 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:23:39.536649 140027215431488 submission_runner.py:408] Time since start: 44508.51s, 	Step: 126455, 	{'train/accuracy': 0.7784597873687744, 'train/loss': 0.9620999693870544, 'validation/accuracy': 0.7042999863624573, 'validation/loss': 1.2834198474884033, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 1.9199188947677612, 'test/num_examples': 10000, 'score': 42900.42901682854, 'total_duration': 44508.50655436516, 'accumulated_submission_time': 42900.42901682854, 'accumulated_eval_time': 1599.7522914409637, 'accumulated_logging_time': 3.979759454727173}
I0127 07:23:39.578286 139866180368128 logging_writer.py:48] [126455] accumulated_eval_time=1599.752291, accumulated_logging_time=3.979759, accumulated_submission_time=42900.429017, global_step=126455, preemption_count=0, score=42900.429017, test/accuracy=0.583400, test/loss=1.919919, test/num_examples=10000, total_duration=44508.506554, train/accuracy=0.778460, train/loss=0.962100, validation/accuracy=0.704300, validation/loss=1.283420, validation/num_examples=50000
I0127 07:23:55.164705 139866188760832 logging_writer.py:48] [126500] global_step=126500, grad_norm=4.7762627601623535, loss=2.3054909706115723
I0127 07:24:29.004240 139866180368128 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.070712566375732, loss=2.265744924545288
I0127 07:25:02.875898 139866188760832 logging_writer.py:48] [126700] global_step=126700, grad_norm=4.817110061645508, loss=2.3003437519073486
I0127 07:25:36.759298 139866180368128 logging_writer.py:48] [126800] global_step=126800, grad_norm=4.7638726234436035, loss=2.3436944484710693
I0127 07:26:10.608097 139866188760832 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.048692226409912, loss=2.2367043495178223
I0127 07:26:44.463611 139866180368128 logging_writer.py:48] [127000] global_step=127000, grad_norm=4.889726161956787, loss=2.2838871479034424
I0127 07:27:18.341461 139866188760832 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.172702789306641, loss=2.311899185180664
I0127 07:27:52.426481 139866180368128 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.145213603973389, loss=2.262960910797119
I0127 07:28:26.309211 139866188760832 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.13832950592041, loss=2.286672830581665
I0127 07:29:00.211768 139866180368128 logging_writer.py:48] [127400] global_step=127400, grad_norm=4.801390171051025, loss=2.3799962997436523
I0127 07:29:34.110188 139866188760832 logging_writer.py:48] [127500] global_step=127500, grad_norm=6.332266807556152, loss=2.3441596031188965
I0127 07:30:07.969359 139866180368128 logging_writer.py:48] [127600] global_step=127600, grad_norm=4.8142499923706055, loss=2.2919952869415283
I0127 07:30:41.800638 139866188760832 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.494740009307861, loss=2.276052236557007
I0127 07:31:15.671149 139866180368128 logging_writer.py:48] [127800] global_step=127800, grad_norm=4.525177478790283, loss=2.2365458011627197
I0127 07:31:49.539830 139866188760832 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.486933708190918, loss=2.3445708751678467
I0127 07:32:09.626430 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:32:15.790313 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:32:24.300967 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:32:26.621312 140027215431488 submission_runner.py:408] Time since start: 45035.59s, 	Step: 127961, 	{'train/accuracy': 0.7736966013908386, 'train/loss': 0.9492216110229492, 'validation/accuracy': 0.7021399736404419, 'validation/loss': 1.2651011943817139, 'validation/num_examples': 50000, 'test/accuracy': 0.5796000361442566, 'test/loss': 1.9083911180496216, 'test/num_examples': 10000, 'score': 43410.41619229317, 'total_duration': 45035.59116268158, 'accumulated_submission_time': 43410.41619229317, 'accumulated_eval_time': 1616.7470650672913, 'accumulated_logging_time': 4.031066417694092}
I0127 07:32:26.685986 139862724310784 logging_writer.py:48] [127961] accumulated_eval_time=1616.747065, accumulated_logging_time=4.031066, accumulated_submission_time=43410.416192, global_step=127961, preemption_count=0, score=43410.416192, test/accuracy=0.579600, test/loss=1.908391, test/num_examples=10000, total_duration=45035.591163, train/accuracy=0.773697, train/loss=0.949222, validation/accuracy=0.702140, validation/loss=1.265101, validation/num_examples=50000
I0127 07:32:40.242539 139863663834880 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.44097375869751, loss=2.3106141090393066
I0127 07:33:14.047242 139862724310784 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.140560626983643, loss=2.311474561691284
I0127 07:33:47.911950 139863663834880 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.164179801940918, loss=2.2467269897460938
I0127 07:34:21.704228 139862724310784 logging_writer.py:48] [128300] global_step=128300, grad_norm=4.711574554443359, loss=2.235321044921875
I0127 07:34:55.599920 139863663834880 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.76411771774292, loss=2.2809700965881348
I0127 07:35:29.450078 139862724310784 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.527943134307861, loss=2.3046584129333496
I0127 07:36:03.318423 139863663834880 logging_writer.py:48] [128600] global_step=128600, grad_norm=4.818833827972412, loss=2.253051519393921
I0127 07:36:37.126431 139862724310784 logging_writer.py:48] [128700] global_step=128700, grad_norm=4.835756778717041, loss=2.1978602409362793
I0127 07:37:10.978582 139863663834880 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.2526750564575195, loss=2.2789034843444824
I0127 07:37:44.818168 139862724310784 logging_writer.py:48] [128900] global_step=128900, grad_norm=4.9716267585754395, loss=2.2150018215179443
I0127 07:38:18.644809 139863663834880 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.104176998138428, loss=2.2026610374450684
I0127 07:38:52.495696 139862724310784 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.0488104820251465, loss=2.208139419555664
I0127 07:39:26.356389 139863663834880 logging_writer.py:48] [129200] global_step=129200, grad_norm=4.805052757263184, loss=2.287632703781128
I0127 07:40:00.211832 139862724310784 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.2153191566467285, loss=2.3052866458892822
I0127 07:40:34.032621 139863663834880 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.352670192718506, loss=2.1547887325286865
I0127 07:40:56.921896 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:41:03.128917 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:41:11.882188 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:41:14.179141 140027215431488 submission_runner.py:408] Time since start: 45563.15s, 	Step: 129469, 	{'train/accuracy': 0.805683970451355, 'train/loss': 0.8402982354164124, 'validation/accuracy': 0.7069199681282043, 'validation/loss': 1.2687928676605225, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.9307630062103271, 'test/num_examples': 10000, 'score': 43920.58583164215, 'total_duration': 45563.149040699005, 'accumulated_submission_time': 43920.58583164215, 'accumulated_eval_time': 1634.0042452812195, 'accumulated_logging_time': 4.109456300735474}
I0127 07:41:14.220242 139862724310784 logging_writer.py:48] [129469] accumulated_eval_time=1634.004245, accumulated_logging_time=4.109456, accumulated_submission_time=43920.585832, global_step=129469, preemption_count=0, score=43920.585832, test/accuracy=0.578800, test/loss=1.930763, test/num_examples=10000, total_duration=45563.149041, train/accuracy=0.805684, train/loss=0.840298, validation/accuracy=0.706920, validation/loss=1.268793, validation/num_examples=50000
I0127 07:41:25.025160 139863663834880 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.09826135635376, loss=2.2951483726501465
I0127 07:41:58.831799 139862724310784 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.006812572479248, loss=2.328697681427002
I0127 07:42:32.636137 139863663834880 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.067075252532959, loss=2.25797700881958
I0127 07:43:06.465725 139862724310784 logging_writer.py:48] [129800] global_step=129800, grad_norm=4.830924034118652, loss=2.1609599590301514
I0127 07:43:40.305352 139863663834880 logging_writer.py:48] [129900] global_step=129900, grad_norm=4.872844219207764, loss=2.250080108642578
I0127 07:44:14.146122 139862724310784 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.285110950469971, loss=2.256039619445801
I0127 07:44:48.010607 139863663834880 logging_writer.py:48] [130100] global_step=130100, grad_norm=4.7346062660217285, loss=2.2058944702148438
I0127 07:45:21.858657 139862724310784 logging_writer.py:48] [130200] global_step=130200, grad_norm=4.7620954513549805, loss=2.149386167526245
I0127 07:45:55.693191 139863663834880 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.058635234832764, loss=2.22597336769104
I0127 07:46:29.541929 139862724310784 logging_writer.py:48] [130400] global_step=130400, grad_norm=4.893857002258301, loss=2.1911065578460693
I0127 07:47:03.474516 139863663834880 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.069990158081055, loss=2.177882194519043
I0127 07:47:37.300045 139862724310784 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.717136859893799, loss=2.2636561393737793
I0127 07:48:11.117593 139863663834880 logging_writer.py:48] [130700] global_step=130700, grad_norm=4.90120267868042, loss=2.2264838218688965
I0127 07:48:45.003926 139862724310784 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.74061393737793, loss=2.3599414825439453
I0127 07:49:18.840316 139863663834880 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.151900291442871, loss=2.3146731853485107
I0127 07:49:44.361090 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:49:50.565823 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:49:59.119350 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:50:01.435335 140027215431488 submission_runner.py:408] Time since start: 46090.41s, 	Step: 130977, 	{'train/accuracy': 0.7959781289100647, 'train/loss': 0.8902884125709534, 'validation/accuracy': 0.7091799974441528, 'validation/loss': 1.2596995830535889, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 1.9029459953308105, 'test/num_examples': 10000, 'score': 44430.66488528252, 'total_duration': 46090.4052464962, 'accumulated_submission_time': 44430.66488528252, 'accumulated_eval_time': 1651.078492641449, 'accumulated_logging_time': 4.160379648208618}
I0127 07:50:01.475545 139862724310784 logging_writer.py:48] [130977] accumulated_eval_time=1651.078493, accumulated_logging_time=4.160380, accumulated_submission_time=44430.664885, global_step=130977, preemption_count=0, score=44430.664885, test/accuracy=0.583600, test/loss=1.902946, test/num_examples=10000, total_duration=46090.405246, train/accuracy=0.795978, train/loss=0.890288, validation/accuracy=0.709180, validation/loss=1.259700, validation/num_examples=50000
I0127 07:50:09.616503 139865760950016 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.354248523712158, loss=2.237053394317627
I0127 07:50:43.434372 139862724310784 logging_writer.py:48] [131100] global_step=131100, grad_norm=4.99163818359375, loss=2.20390248298645
I0127 07:51:17.266379 139865760950016 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.065651893615723, loss=2.196690797805786
I0127 07:51:51.113801 139862724310784 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.127346992492676, loss=2.25130033493042
I0127 07:52:24.996187 139865760950016 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.370028495788574, loss=2.3200321197509766
I0127 07:52:58.918440 139862724310784 logging_writer.py:48] [131500] global_step=131500, grad_norm=4.930123329162598, loss=2.189993381500244
I0127 07:53:32.793369 139865760950016 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.158142566680908, loss=2.2488152980804443
I0127 07:54:06.664510 139862724310784 logging_writer.py:48] [131700] global_step=131700, grad_norm=4.86875581741333, loss=2.2387516498565674
I0127 07:54:40.503451 139865760950016 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.066161632537842, loss=2.1661362648010254
I0127 07:55:14.351671 139862724310784 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.430007457733154, loss=2.145156145095825
I0127 07:55:48.205375 139865760950016 logging_writer.py:48] [132000] global_step=132000, grad_norm=4.807567596435547, loss=2.201882839202881
I0127 07:56:22.085863 139862724310784 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.300631523132324, loss=2.178569793701172
I0127 07:56:55.939086 139865760950016 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.209580421447754, loss=2.226276159286499
I0127 07:57:29.765983 139862724310784 logging_writer.py:48] [132300] global_step=132300, grad_norm=6.411537170410156, loss=2.2780208587646484
I0127 07:58:03.593667 139865760950016 logging_writer.py:48] [132400] global_step=132400, grad_norm=4.9756574630737305, loss=2.201716899871826
I0127 07:58:31.477777 140027215431488 spec.py:321] Evaluating on the training split.
I0127 07:58:37.575806 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 07:58:46.257817 140027215431488 spec.py:349] Evaluating on the test split.
I0127 07:58:48.545349 140027215431488 submission_runner.py:408] Time since start: 46617.52s, 	Step: 132484, 	{'train/accuracy': 0.7964564561843872, 'train/loss': 0.8781556487083435, 'validation/accuracy': 0.7150799632072449, 'validation/loss': 1.2385895252227783, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.8773962259292603, 'test/num_examples': 10000, 'score': 44940.60585641861, 'total_duration': 46617.5152451992, 'accumulated_submission_time': 44940.60585641861, 'accumulated_eval_time': 1668.14599943161, 'accumulated_logging_time': 4.21102237701416}
I0127 07:58:48.585692 139862724310784 logging_writer.py:48] [132484] accumulated_eval_time=1668.145999, accumulated_logging_time=4.211022, accumulated_submission_time=44940.605856, global_step=132484, preemption_count=0, score=44940.605856, test/accuracy=0.587100, test/loss=1.877396, test/num_examples=10000, total_duration=46617.515245, train/accuracy=0.796456, train/loss=0.878156, validation/accuracy=0.715080, validation/loss=1.238590, validation/num_examples=50000
I0127 07:58:54.355536 139863663834880 logging_writer.py:48] [132500] global_step=132500, grad_norm=4.938316822052002, loss=2.251066207885742
I0127 07:59:28.223203 139862724310784 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.335793495178223, loss=2.2357730865478516
I0127 08:00:02.057289 139863663834880 logging_writer.py:48] [132700] global_step=132700, grad_norm=4.845849514007568, loss=2.2611806392669678
I0127 08:00:35.890024 139862724310784 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.398420810699463, loss=2.270404100418091
I0127 08:01:09.716057 139863663834880 logging_writer.py:48] [132900] global_step=132900, grad_norm=4.866293907165527, loss=2.2236218452453613
I0127 08:01:43.589785 139862724310784 logging_writer.py:48] [133000] global_step=133000, grad_norm=4.811748027801514, loss=2.2287304401397705
I0127 08:02:17.448658 139863663834880 logging_writer.py:48] [133100] global_step=133100, grad_norm=4.912465572357178, loss=2.2463059425354004
I0127 08:02:51.300210 139862724310784 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.350512504577637, loss=2.1361732482910156
I0127 08:03:25.164839 139863663834880 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.581313610076904, loss=2.2937793731689453
I0127 08:03:59.011820 139862724310784 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.71044397354126, loss=2.1982359886169434
I0127 08:04:32.875538 139863663834880 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.20955228805542, loss=2.2639875411987305
I0127 08:05:06.729934 139862724310784 logging_writer.py:48] [133600] global_step=133600, grad_norm=4.902764797210693, loss=2.1191012859344482
I0127 08:05:40.698810 139863663834880 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.455436706542969, loss=2.2410125732421875
I0127 08:06:14.580098 139862724310784 logging_writer.py:48] [133800] global_step=133800, grad_norm=4.903392791748047, loss=2.1867759227752686
I0127 08:06:48.450649 139863663834880 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.557121753692627, loss=2.181985855102539
I0127 08:07:18.712978 140027215431488 spec.py:321] Evaluating on the training split.
I0127 08:07:24.928570 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 08:07:33.540813 140027215431488 spec.py:349] Evaluating on the test split.
I0127 08:07:35.851855 140027215431488 submission_runner.py:408] Time since start: 47144.82s, 	Step: 133991, 	{'train/accuracy': 0.8026745915412903, 'train/loss': 0.8502547144889832, 'validation/accuracy': 0.7177799940109253, 'validation/loss': 1.2106214761734009, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.840102195739746, 'test/num_examples': 10000, 'score': 45450.668227910995, 'total_duration': 47144.82175087929, 'accumulated_submission_time': 45450.668227910995, 'accumulated_eval_time': 1685.284812450409, 'accumulated_logging_time': 4.261689901351929}
I0127 08:07:35.895101 139866171975424 logging_writer.py:48] [133991] accumulated_eval_time=1685.284812, accumulated_logging_time=4.261690, accumulated_submission_time=45450.668228, global_step=133991, preemption_count=0, score=45450.668228, test/accuracy=0.593200, test/loss=1.840102, test/num_examples=10000, total_duration=47144.821751, train/accuracy=0.802675, train/loss=0.850255, validation/accuracy=0.717780, validation/loss=1.210621, validation/num_examples=50000
I0127 08:07:39.289262 139866180368128 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.429648399353027, loss=2.206622838973999
I0127 08:08:13.155645 139866171975424 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.123225212097168, loss=2.245906114578247
I0127 08:08:46.998726 139866180368128 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.268555164337158, loss=2.1990392208099365
I0127 08:09:20.820818 139866171975424 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.10165548324585, loss=2.207315683364868
I0127 08:09:54.669042 139866180368128 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.778445243835449, loss=2.2377097606658936
I0127 08:10:28.505486 139866171975424 logging_writer.py:48] [134500] global_step=134500, grad_norm=4.950173854827881, loss=2.1935477256774902
I0127 08:11:02.407561 139866180368128 logging_writer.py:48] [134600] global_step=134600, grad_norm=4.91262149810791, loss=2.236232280731201
I0127 08:11:36.382201 139866171975424 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.015354156494141, loss=2.2443690299987793
I0127 08:12:10.255433 139866180368128 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.386186599731445, loss=2.260809898376465
I0127 08:12:44.101040 139866171975424 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.287631034851074, loss=2.2231106758117676
I0127 08:13:17.972323 139866180368128 logging_writer.py:48] [135000] global_step=135000, grad_norm=5.144795894622803, loss=2.257784366607666
I0127 08:13:51.861027 139866171975424 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.120177268981934, loss=2.2392282485961914
I0127 08:14:25.724668 139866180368128 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.763822555541992, loss=2.225590467453003
I0127 08:14:59.573585 139866171975424 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.51777982711792, loss=2.1614484786987305
I0127 08:15:33.396333 139866180368128 logging_writer.py:48] [135400] global_step=135400, grad_norm=5.358593463897705, loss=2.182638168334961
I0127 08:16:06.037265 140027215431488 spec.py:321] Evaluating on the training split.
I0127 08:16:12.276947 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 08:16:20.784745 140027215431488 spec.py:349] Evaluating on the test split.
I0127 08:16:23.157992 140027215431488 submission_runner.py:408] Time since start: 47672.13s, 	Step: 135498, 	{'train/accuracy': 0.7995057106018066, 'train/loss': 0.8691868782043457, 'validation/accuracy': 0.7166799902915955, 'validation/loss': 1.2189016342163086, 'validation/num_examples': 50000, 'test/accuracy': 0.5916000008583069, 'test/loss': 1.859628438949585, 'test/num_examples': 10000, 'score': 45960.74799871445, 'total_duration': 47672.127898454666, 'accumulated_submission_time': 45960.74799871445, 'accumulated_eval_time': 1702.4054865837097, 'accumulated_logging_time': 4.315122365951538}
I0127 08:16:23.202637 139862724310784 logging_writer.py:48] [135498] accumulated_eval_time=1702.405487, accumulated_logging_time=4.315122, accumulated_submission_time=45960.747999, global_step=135498, preemption_count=0, score=45960.747999, test/accuracy=0.591600, test/loss=1.859628, test/num_examples=10000, total_duration=47672.127898, train/accuracy=0.799506, train/loss=0.869187, validation/accuracy=0.716680, validation/loss=1.218902, validation/num_examples=50000
I0127 08:16:24.225950 139863663834880 logging_writer.py:48] [135500] global_step=135500, grad_norm=4.971582889556885, loss=2.1874091625213623
I0127 08:16:58.033583 139862724310784 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.387641906738281, loss=2.18467116355896
I0127 08:17:31.872198 139863663834880 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.651017189025879, loss=2.1827216148376465
I0127 08:18:05.799266 139862724310784 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.2697954177856445, loss=2.193937063217163
I0127 08:18:39.625517 139863663834880 logging_writer.py:48] [135900] global_step=135900, grad_norm=6.7870073318481445, loss=2.2607550621032715
I0127 08:19:13.480273 139862724310784 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.666066646575928, loss=2.2583186626434326
I0127 08:19:47.355147 139863663834880 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.436567783355713, loss=2.2046985626220703
I0127 08:20:21.195796 139862724310784 logging_writer.py:48] [136200] global_step=136200, grad_norm=5.269227027893066, loss=2.160884380340576
I0127 08:20:55.024903 139863663834880 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.4578022956848145, loss=2.2189934253692627
I0127 08:21:28.883888 139862724310784 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.586698532104492, loss=2.2250308990478516
I0127 08:22:02.745373 139863663834880 logging_writer.py:48] [136500] global_step=136500, grad_norm=5.602440357208252, loss=2.293562650680542
I0127 08:22:36.543270 139862724310784 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.428241729736328, loss=2.278832197189331
I0127 08:23:10.413869 139863663834880 logging_writer.py:48] [136700] global_step=136700, grad_norm=5.94897985458374, loss=2.2080001831054688
I0127 08:23:44.369764 139862724310784 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.4568328857421875, loss=2.2675869464874268
I0127 08:24:18.212213 139863663834880 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.7346343994140625, loss=2.201643705368042
I0127 08:24:52.051859 139862724310784 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.9844512939453125, loss=2.123049259185791
I0127 08:24:53.207144 140027215431488 spec.py:321] Evaluating on the training split.
I0127 08:24:59.355331 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 08:25:07.872006 140027215431488 spec.py:349] Evaluating on the test split.
I0127 08:25:10.141182 140027215431488 submission_runner.py:408] Time since start: 48199.11s, 	Step: 137005, 	{'train/accuracy': 0.7967952489852905, 'train/loss': 0.862323522567749, 'validation/accuracy': 0.7177000045776367, 'validation/loss': 1.2082816362380981, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.8467576503753662, 'test/num_examples': 10000, 'score': 46470.69027876854, 'total_duration': 48199.111078739166, 'accumulated_submission_time': 46470.69027876854, 'accumulated_eval_time': 1719.3394558429718, 'accumulated_logging_time': 4.369931697845459}
I0127 08:25:10.181375 139862724310784 logging_writer.py:48] [137005] accumulated_eval_time=1719.339456, accumulated_logging_time=4.369932, accumulated_submission_time=46470.690279, global_step=137005, preemption_count=0, score=46470.690279, test/accuracy=0.595700, test/loss=1.846758, test/num_examples=10000, total_duration=48199.111079, train/accuracy=0.796795, train/loss=0.862324, validation/accuracy=0.717700, validation/loss=1.208282, validation/num_examples=50000
I0127 08:25:42.655764 139866163582720 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.791255950927734, loss=2.167323350906372
I0127 08:26:16.492623 139862724310784 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.071053504943848, loss=2.1048524379730225
I0127 08:26:50.344049 139866163582720 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.482628345489502, loss=2.205531120300293
I0127 08:27:24.219640 139862724310784 logging_writer.py:48] [137400] global_step=137400, grad_norm=5.572259902954102, loss=2.214254379272461
I0127 08:27:58.063028 139866163582720 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.6933112144470215, loss=2.175485372543335
I0127 08:28:31.893995 139862724310784 logging_writer.py:48] [137600] global_step=137600, grad_norm=6.0440802574157715, loss=2.2620956897735596
I0127 08:29:05.733801 139866163582720 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.754510879516602, loss=2.197007656097412
I0127 08:29:39.591771 139862724310784 logging_writer.py:48] [137800] global_step=137800, grad_norm=5.439497470855713, loss=2.1141529083251953
I0127 08:30:13.504914 139866163582720 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.907006740570068, loss=2.21696138381958
I0127 08:30:47.362250 139862724310784 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.911951541900635, loss=2.2286369800567627
I0127 08:31:21.267957 139866163582720 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.508023738861084, loss=2.1607413291931152
I0127 08:31:55.101677 139862724310784 logging_writer.py:48] [138200] global_step=138200, grad_norm=5.882730484008789, loss=2.2728285789489746
I0127 08:32:28.951534 139866163582720 logging_writer.py:48] [138300] global_step=138300, grad_norm=4.897353649139404, loss=2.119168758392334
I0127 08:33:02.816654 139862724310784 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.158779621124268, loss=2.258411407470703
I0127 08:33:36.716080 139866163582720 logging_writer.py:48] [138500] global_step=138500, grad_norm=5.483451843261719, loss=2.1861515045166016
I0127 08:33:40.241740 140027215431488 spec.py:321] Evaluating on the training split.
I0127 08:33:46.324283 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 08:33:54.981056 140027215431488 spec.py:349] Evaluating on the test split.
I0127 08:33:57.264893 140027215431488 submission_runner.py:408] Time since start: 48726.23s, 	Step: 138512, 	{'train/accuracy': 0.8315330147743225, 'train/loss': 0.7364321947097778, 'validation/accuracy': 0.7225199937820435, 'validation/loss': 1.1886651515960693, 'validation/num_examples': 50000, 'test/accuracy': 0.5956000089645386, 'test/loss': 1.8367879390716553, 'test/num_examples': 10000, 'score': 46980.68811249733, 'total_duration': 48726.23480153084, 'accumulated_submission_time': 46980.68811249733, 'accumulated_eval_time': 1736.362550497055, 'accumulated_logging_time': 4.420541524887085}
I0127 08:33:57.305567 139865769342720 logging_writer.py:48] [138512] accumulated_eval_time=1736.362550, accumulated_logging_time=4.420542, accumulated_submission_time=46980.688112, global_step=138512, preemption_count=0, score=46980.688112, test/accuracy=0.595600, test/loss=1.836788, test/num_examples=10000, total_duration=48726.234802, train/accuracy=0.831533, train/loss=0.736432, validation/accuracy=0.722520, validation/loss=1.188665, validation/num_examples=50000
I0127 08:34:27.418589 139866180368128 logging_writer.py:48] [138600] global_step=138600, grad_norm=6.1963982582092285, loss=2.102200746536255
I0127 08:35:01.298755 139865769342720 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.020134925842285, loss=2.2058467864990234
I0127 08:35:35.160569 139866180368128 logging_writer.py:48] [138800] global_step=138800, grad_norm=5.473957061767578, loss=2.204296588897705
I0127 08:36:09.066696 139865769342720 logging_writer.py:48] [138900] global_step=138900, grad_norm=5.847062110900879, loss=2.142200231552124
I0127 08:36:42.938227 139866180368128 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.7464118003845215, loss=2.2100491523742676
I0127 08:37:16.792845 139865769342720 logging_writer.py:48] [139100] global_step=139100, grad_norm=5.863412857055664, loss=2.1501364707946777
I0127 08:37:50.640854 139866180368128 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.559092998504639, loss=2.133681535720825
I0127 08:38:24.486763 139865769342720 logging_writer.py:48] [139300] global_step=139300, grad_norm=5.42888069152832, loss=2.092951774597168
I0127 08:38:58.344935 139866180368128 logging_writer.py:48] [139400] global_step=139400, grad_norm=5.584092617034912, loss=2.1985316276550293
I0127 08:39:32.222685 139865769342720 logging_writer.py:48] [139500] global_step=139500, grad_norm=5.543734073638916, loss=2.1279542446136475
I0127 08:40:06.060589 139866180368128 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.555548667907715, loss=2.178868293762207
I0127 08:40:39.888242 139865769342720 logging_writer.py:48] [139700] global_step=139700, grad_norm=5.439547538757324, loss=2.153339385986328
I0127 08:41:13.783691 139866180368128 logging_writer.py:48] [139800] global_step=139800, grad_norm=5.98248815536499, loss=2.202087879180908
I0127 08:41:47.646065 139865769342720 logging_writer.py:48] [139900] global_step=139900, grad_norm=5.605173110961914, loss=2.1558425426483154
I0127 08:42:21.565268 139866180368128 logging_writer.py:48] [140000] global_step=140000, grad_norm=5.360896587371826, loss=2.0836057662963867
I0127 08:42:27.459442 140027215431488 spec.py:321] Evaluating on the training split.
I0127 08:42:33.546530 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 08:42:42.217862 140027215431488 spec.py:349] Evaluating on the test split.
I0127 08:42:44.496768 140027215431488 submission_runner.py:408] Time since start: 49253.47s, 	Step: 140019, 	{'train/accuracy': 0.8246771097183228, 'train/loss': 0.7462592720985413, 'validation/accuracy': 0.729919970035553, 'validation/loss': 1.1638081073760986, 'validation/num_examples': 50000, 'test/accuracy': 0.6030000448226929, 'test/loss': 1.794895052909851, 'test/num_examples': 10000, 'score': 47490.778540849686, 'total_duration': 49253.46666812897, 'accumulated_submission_time': 47490.778540849686, 'accumulated_eval_time': 1753.3998112678528, 'accumulated_logging_time': 4.472010850906372}
I0127 08:42:44.537776 139865760950016 logging_writer.py:48] [140019] accumulated_eval_time=1753.399811, accumulated_logging_time=4.472011, accumulated_submission_time=47490.778541, global_step=140019, preemption_count=0, score=47490.778541, test/accuracy=0.603000, test/loss=1.794895, test/num_examples=10000, total_duration=49253.466668, train/accuracy=0.824677, train/loss=0.746259, validation/accuracy=0.729920, validation/loss=1.163808, validation/num_examples=50000
I0127 08:43:12.296818 139865769342720 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.314785957336426, loss=2.121051549911499
I0127 08:43:46.131506 139865760950016 logging_writer.py:48] [140200] global_step=140200, grad_norm=6.267621994018555, loss=2.1197993755340576
I0127 08:44:19.972029 139865769342720 logging_writer.py:48] [140300] global_step=140300, grad_norm=5.773942470550537, loss=2.14947772026062
I0127 08:44:53.809715 139865760950016 logging_writer.py:48] [140400] global_step=140400, grad_norm=5.842085838317871, loss=2.228550910949707
I0127 08:45:27.663868 139865769342720 logging_writer.py:48] [140500] global_step=140500, grad_norm=5.877852439880371, loss=2.176400661468506
I0127 08:46:01.507210 139865760950016 logging_writer.py:48] [140600] global_step=140600, grad_norm=6.366138935089111, loss=2.1865172386169434
I0127 08:46:35.378455 139865769342720 logging_writer.py:48] [140700] global_step=140700, grad_norm=5.494359970092773, loss=2.085055351257324
I0127 08:47:09.221978 139865760950016 logging_writer.py:48] [140800] global_step=140800, grad_norm=5.897281169891357, loss=2.1946582794189453
I0127 08:47:43.091117 139865769342720 logging_writer.py:48] [140900] global_step=140900, grad_norm=5.473011016845703, loss=2.128488063812256
I0127 08:48:16.948465 139865760950016 logging_writer.py:48] [141000] global_step=141000, grad_norm=5.771556854248047, loss=2.1106321811676025
I0127 08:48:50.891026 139865769342720 logging_writer.py:48] [141100] global_step=141100, grad_norm=5.750139236450195, loss=2.2166693210601807
I0127 08:49:24.743617 139865760950016 logging_writer.py:48] [141200] global_step=141200, grad_norm=5.778118133544922, loss=2.113158941268921
I0127 08:49:58.569269 139865769342720 logging_writer.py:48] [141300] global_step=141300, grad_norm=5.621290683746338, loss=2.1464500427246094
I0127 08:50:32.451154 139865760950016 logging_writer.py:48] [141400] global_step=141400, grad_norm=5.680062294006348, loss=2.103641986846924
I0127 08:51:06.281486 139865769342720 logging_writer.py:48] [141500] global_step=141500, grad_norm=6.018141746520996, loss=2.199215888977051
I0127 08:51:14.556730 140027215431488 spec.py:321] Evaluating on the training split.
I0127 08:51:20.640119 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 08:51:29.365221 140027215431488 spec.py:349] Evaluating on the test split.
I0127 08:51:31.664269 140027215431488 submission_runner.py:408] Time since start: 49780.63s, 	Step: 141526, 	{'train/accuracy': 0.8252949714660645, 'train/loss': 0.7593898177146912, 'validation/accuracy': 0.7310400009155273, 'validation/loss': 1.1624298095703125, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.7896476984024048, 'test/num_examples': 10000, 'score': 48000.73310112953, 'total_duration': 49780.63418030739, 'accumulated_submission_time': 48000.73310112953, 'accumulated_eval_time': 1770.5072956085205, 'accumulated_logging_time': 4.523514747619629}
I0127 08:51:31.706622 139862715918080 logging_writer.py:48] [141526] accumulated_eval_time=1770.507296, accumulated_logging_time=4.523515, accumulated_submission_time=48000.733101, global_step=141526, preemption_count=0, score=48000.733101, test/accuracy=0.604400, test/loss=1.789648, test/num_examples=10000, total_duration=49780.634180, train/accuracy=0.825295, train/loss=0.759390, validation/accuracy=0.731040, validation/loss=1.162430, validation/num_examples=50000
I0127 08:51:57.089400 139862724310784 logging_writer.py:48] [141600] global_step=141600, grad_norm=5.449049472808838, loss=2.1787970066070557
I0127 08:52:30.948732 139862715918080 logging_writer.py:48] [141700] global_step=141700, grad_norm=6.350382328033447, loss=2.1429097652435303
I0127 08:53:04.781656 139862724310784 logging_writer.py:48] [141800] global_step=141800, grad_norm=5.646125316619873, loss=2.1734719276428223
I0127 08:53:38.653888 139862715918080 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.211516380310059, loss=2.142937660217285
I0127 08:54:12.520833 139862724310784 logging_writer.py:48] [142000] global_step=142000, grad_norm=5.732097148895264, loss=2.16989803314209
I0127 08:54:46.540938 139862715918080 logging_writer.py:48] [142100] global_step=142100, grad_norm=5.66509485244751, loss=2.100998640060425
I0127 08:55:20.372335 139862724310784 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.014621257781982, loss=2.1013572216033936
I0127 08:55:54.236005 139862715918080 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.684213161468506, loss=2.104525566101074
I0127 08:56:28.098220 139862724310784 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.945616245269775, loss=2.1435322761535645
I0127 08:57:01.973034 139862715918080 logging_writer.py:48] [142500] global_step=142500, grad_norm=5.785500526428223, loss=2.0813381671905518
I0127 08:57:35.828414 139862724310784 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.214545249938965, loss=2.150844097137451
I0127 08:58:09.681600 139862715918080 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.1992669105529785, loss=2.1872146129608154
I0127 08:58:43.567011 139862724310784 logging_writer.py:48] [142800] global_step=142800, grad_norm=6.011213302612305, loss=2.2100462913513184
I0127 08:59:17.434563 139862715918080 logging_writer.py:48] [142900] global_step=142900, grad_norm=5.7977728843688965, loss=2.172975778579712
I0127 08:59:51.275238 139862724310784 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.5019917488098145, loss=2.135197162628174
I0127 09:00:01.930494 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:00:08.096773 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:00:16.841073 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:00:19.095576 140027215431488 submission_runner.py:408] Time since start: 50308.07s, 	Step: 143033, 	{'train/accuracy': 0.8251753449440002, 'train/loss': 0.7437108159065247, 'validation/accuracy': 0.7314800024032593, 'validation/loss': 1.141937017440796, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.773678183555603, 'test/num_examples': 10000, 'score': 48510.89326953888, 'total_duration': 50308.06536364555, 'accumulated_submission_time': 48510.89326953888, 'accumulated_eval_time': 1787.6722025871277, 'accumulated_logging_time': 4.576416730880737}
I0127 09:00:19.139732 139863663834880 logging_writer.py:48] [143033] accumulated_eval_time=1787.672203, accumulated_logging_time=4.576417, accumulated_submission_time=48510.893270, global_step=143033, preemption_count=0, score=48510.893270, test/accuracy=0.610500, test/loss=1.773678, test/num_examples=10000, total_duration=50308.065364, train/accuracy=0.825175, train/loss=0.743711, validation/accuracy=0.731480, validation/loss=1.141937, validation/num_examples=50000
I0127 09:00:42.280481 139866180368128 logging_writer.py:48] [143100] global_step=143100, grad_norm=5.293080806732178, loss=2.1122450828552246
I0127 09:01:16.169986 139863663834880 logging_writer.py:48] [143200] global_step=143200, grad_norm=5.583183765411377, loss=2.119954824447632
I0127 09:01:49.993575 139866180368128 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.103283405303955, loss=2.1380012035369873
I0127 09:02:23.858074 139863663834880 logging_writer.py:48] [143400] global_step=143400, grad_norm=5.799102783203125, loss=2.1612842082977295
I0127 09:02:57.734120 139866180368128 logging_writer.py:48] [143500] global_step=143500, grad_norm=6.171610355377197, loss=2.107243537902832
I0127 09:03:31.589376 139863663834880 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.392241954803467, loss=2.1533570289611816
I0127 09:04:05.462388 139866180368128 logging_writer.py:48] [143700] global_step=143700, grad_norm=5.944647312164307, loss=2.1584267616271973
I0127 09:04:39.358886 139863663834880 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.1359028816223145, loss=2.1269781589508057
I0127 09:05:13.200793 139866180368128 logging_writer.py:48] [143900] global_step=143900, grad_norm=5.431831359863281, loss=2.059465169906616
I0127 09:05:47.055695 139863663834880 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.403013229370117, loss=2.1145644187927246
I0127 09:06:20.960737 139866180368128 logging_writer.py:48] [144100] global_step=144100, grad_norm=5.877352714538574, loss=2.163543224334717
I0127 09:06:54.902586 139863663834880 logging_writer.py:48] [144200] global_step=144200, grad_norm=5.976665019989014, loss=2.0775578022003174
I0127 09:07:28.771158 139866180368128 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.33427619934082, loss=2.095566749572754
I0127 09:08:02.635540 139863663834880 logging_writer.py:48] [144400] global_step=144400, grad_norm=6.123645305633545, loss=2.226003646850586
I0127 09:08:36.496872 139866180368128 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.436158657073975, loss=2.1072418689727783
I0127 09:08:49.182856 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:08:55.287467 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:09:03.926868 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:09:06.233994 140027215431488 submission_runner.py:408] Time since start: 50835.20s, 	Step: 144539, 	{'train/accuracy': 0.8236008882522583, 'train/loss': 0.7736658453941345, 'validation/accuracy': 0.7317599654197693, 'validation/loss': 1.1646684408187866, 'validation/num_examples': 50000, 'test/accuracy': 0.6085000038146973, 'test/loss': 1.8027182817459106, 'test/num_examples': 10000, 'score': 49020.87477731705, 'total_duration': 50835.20391178131, 'accumulated_submission_time': 49020.87477731705, 'accumulated_eval_time': 1804.7232937812805, 'accumulated_logging_time': 4.630538702011108}
I0127 09:09:06.274876 139862715918080 logging_writer.py:48] [144539] accumulated_eval_time=1804.723294, accumulated_logging_time=4.630539, accumulated_submission_time=49020.874777, global_step=144539, preemption_count=0, score=49020.874777, test/accuracy=0.608500, test/loss=1.802718, test/num_examples=10000, total_duration=50835.203912, train/accuracy=0.823601, train/loss=0.773666, validation/accuracy=0.731760, validation/loss=1.164668, validation/num_examples=50000
I0127 09:09:27.252439 139862724310784 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.221780300140381, loss=2.1121950149536133
I0127 09:10:01.109724 139862715918080 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.567285060882568, loss=2.1620121002197266
I0127 09:10:35.010299 139862724310784 logging_writer.py:48] [144800] global_step=144800, grad_norm=5.99051570892334, loss=2.0747427940368652
I0127 09:11:08.863986 139862715918080 logging_writer.py:48] [144900] global_step=144900, grad_norm=5.704227924346924, loss=2.1703901290893555
I0127 09:11:42.733930 139862724310784 logging_writer.py:48] [145000] global_step=145000, grad_norm=6.7265729904174805, loss=2.126671314239502
I0127 09:12:16.599517 139862715918080 logging_writer.py:48] [145100] global_step=145100, grad_norm=5.970819473266602, loss=2.1196584701538086
I0127 09:12:50.449826 139862724310784 logging_writer.py:48] [145200] global_step=145200, grad_norm=5.821820259094238, loss=2.1439833641052246
I0127 09:13:24.382972 139862715918080 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.236480236053467, loss=2.1933159828186035
I0127 09:13:58.245441 139862724310784 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.102451324462891, loss=2.0939762592315674
I0127 09:14:32.095232 139862715918080 logging_writer.py:48] [145500] global_step=145500, grad_norm=5.662099838256836, loss=2.018339157104492
I0127 09:15:05.959015 139862724310784 logging_writer.py:48] [145600] global_step=145600, grad_norm=5.963345527648926, loss=2.095259666442871
I0127 09:15:39.816720 139862715918080 logging_writer.py:48] [145700] global_step=145700, grad_norm=5.495424270629883, loss=2.068312883377075
I0127 09:16:13.653291 139862724310784 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.103025436401367, loss=2.0688095092773438
I0127 09:16:47.527772 139862715918080 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.1122212409973145, loss=2.1187334060668945
I0127 09:17:21.393494 139862724310784 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.841028690338135, loss=2.149061918258667
I0127 09:17:36.440849 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:17:42.644242 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:17:51.170883 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:17:53.392959 140027215431488 submission_runner.py:408] Time since start: 51362.36s, 	Step: 146046, 	{'train/accuracy': 0.8278858065605164, 'train/loss': 0.7407735586166382, 'validation/accuracy': 0.7351599931716919, 'validation/loss': 1.1394431591033936, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.7666029930114746, 'test/num_examples': 10000, 'score': 49530.97762846947, 'total_duration': 51362.362865924835, 'accumulated_submission_time': 49530.97762846947, 'accumulated_eval_time': 1821.6753494739532, 'accumulated_logging_time': 4.683336019515991}
I0127 09:17:53.434362 139866171975424 logging_writer.py:48] [146046] accumulated_eval_time=1821.675349, accumulated_logging_time=4.683336, accumulated_submission_time=49530.977628, global_step=146046, preemption_count=0, score=49530.977628, test/accuracy=0.615600, test/loss=1.766603, test/num_examples=10000, total_duration=51362.362866, train/accuracy=0.827886, train/loss=0.740774, validation/accuracy=0.735160, validation/loss=1.139443, validation/num_examples=50000
I0127 09:18:12.020036 139866180368128 logging_writer.py:48] [146100] global_step=146100, grad_norm=5.698104381561279, loss=2.0310823917388916
I0127 09:18:45.901824 139866171975424 logging_writer.py:48] [146200] global_step=146200, grad_norm=5.995507717132568, loss=2.150041341781616
I0127 09:19:19.925401 139866180368128 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.37895393371582, loss=2.125887393951416
I0127 09:19:53.785376 139866171975424 logging_writer.py:48] [146400] global_step=146400, grad_norm=5.668296813964844, loss=2.102224826812744
I0127 09:20:27.637784 139866180368128 logging_writer.py:48] [146500] global_step=146500, grad_norm=5.966693878173828, loss=2.050781726837158
I0127 09:21:01.543832 139866171975424 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.085592269897461, loss=2.1251420974731445
I0127 09:21:35.380904 139866180368128 logging_writer.py:48] [146700] global_step=146700, grad_norm=5.662940979003906, loss=2.0608081817626953
I0127 09:22:09.214417 139866171975424 logging_writer.py:48] [146800] global_step=146800, grad_norm=6.184531211853027, loss=1.993934154510498
I0127 09:22:43.113703 139866180368128 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.252784729003906, loss=2.1215879917144775
I0127 09:23:17.000906 139866171975424 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.228484153747559, loss=2.0809903144836426
I0127 09:23:50.837303 139866180368128 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.177969455718994, loss=2.0752956867218018
I0127 09:24:24.719127 139866171975424 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.118741035461426, loss=2.061896324157715
I0127 09:24:58.572929 139866180368128 logging_writer.py:48] [147300] global_step=147300, grad_norm=6.116470813751221, loss=2.1566314697265625
I0127 09:25:32.499895 139866171975424 logging_writer.py:48] [147400] global_step=147400, grad_norm=5.789523601531982, loss=2.0714893341064453
I0127 09:26:06.380895 139866180368128 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.266502857208252, loss=2.0868890285491943
I0127 09:26:23.445113 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:26:29.594584 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:26:38.066683 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:26:40.348737 140027215431488 submission_runner.py:408] Time since start: 51889.32s, 	Step: 147552, 	{'train/accuracy': 0.85843825340271, 'train/loss': 0.6291110515594482, 'validation/accuracy': 0.7379999756813049, 'validation/loss': 1.130968689918518, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.7525743246078491, 'test/num_examples': 10000, 'score': 50040.926671266556, 'total_duration': 51889.318643569946, 'accumulated_submission_time': 50040.926671266556, 'accumulated_eval_time': 1838.5789158344269, 'accumulated_logging_time': 4.734616041183472}
I0127 09:26:40.393569 139865760950016 logging_writer.py:48] [147552] accumulated_eval_time=1838.578916, accumulated_logging_time=4.734616, accumulated_submission_time=50040.926671, global_step=147552, preemption_count=0, score=50040.926671, test/accuracy=0.615300, test/loss=1.752574, test/num_examples=10000, total_duration=51889.318644, train/accuracy=0.858438, train/loss=0.629111, validation/accuracy=0.738000, validation/loss=1.130969, validation/num_examples=50000
I0127 09:26:56.989270 139865769342720 logging_writer.py:48] [147600] global_step=147600, grad_norm=5.735433101654053, loss=1.985053300857544
I0127 09:27:30.791743 139865760950016 logging_writer.py:48] [147700] global_step=147700, grad_norm=5.912214279174805, loss=2.0499165058135986
I0127 09:28:04.666760 139865769342720 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.151424407958984, loss=2.089412212371826
I0127 09:28:38.477061 139865760950016 logging_writer.py:48] [147900] global_step=147900, grad_norm=5.794623374938965, loss=2.028346061706543
I0127 09:29:12.330489 139865769342720 logging_writer.py:48] [148000] global_step=148000, grad_norm=6.005403995513916, loss=2.0899648666381836
I0127 09:29:46.215974 139865760950016 logging_writer.py:48] [148100] global_step=148100, grad_norm=6.261980056762695, loss=2.0425519943237305
I0127 09:30:20.116080 139865769342720 logging_writer.py:48] [148200] global_step=148200, grad_norm=6.357576847076416, loss=2.1612417697906494
I0127 09:30:54.000857 139865760950016 logging_writer.py:48] [148300] global_step=148300, grad_norm=5.745316028594971, loss=2.0368170738220215
I0127 09:31:28.044865 139865769342720 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.130844593048096, loss=2.0640220642089844
I0127 09:32:01.879220 139865760950016 logging_writer.py:48] [148500] global_step=148500, grad_norm=6.881777763366699, loss=2.1182172298431396
I0127 09:32:35.722409 139865769342720 logging_writer.py:48] [148600] global_step=148600, grad_norm=6.431677341461182, loss=2.151578664779663
I0127 09:33:09.612783 139865760950016 logging_writer.py:48] [148700] global_step=148700, grad_norm=7.051863193511963, loss=2.028944730758667
I0127 09:33:43.502792 139865769342720 logging_writer.py:48] [148800] global_step=148800, grad_norm=7.22551155090332, loss=2.110389232635498
I0127 09:34:17.370211 139865760950016 logging_writer.py:48] [148900] global_step=148900, grad_norm=6.053542613983154, loss=2.0662670135498047
I0127 09:34:51.250530 139865769342720 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.26566219329834, loss=2.041684150695801
I0127 09:35:10.677758 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:35:16.891281 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:35:25.516133 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:35:27.813631 140027215431488 submission_runner.py:408] Time since start: 52416.78s, 	Step: 149059, 	{'train/accuracy': 0.8477559089660645, 'train/loss': 0.667382538318634, 'validation/accuracy': 0.7384999990463257, 'validation/loss': 1.130212664604187, 'validation/num_examples': 50000, 'test/accuracy': 0.6147000193595886, 'test/loss': 1.7563661336898804, 'test/num_examples': 10000, 'score': 50551.148307561874, 'total_duration': 52416.78352403641, 'accumulated_submission_time': 50551.148307561874, 'accumulated_eval_time': 1855.7147312164307, 'accumulated_logging_time': 4.789153575897217}
I0127 09:35:27.858457 139862724310784 logging_writer.py:48] [149059] accumulated_eval_time=1855.714731, accumulated_logging_time=4.789154, accumulated_submission_time=50551.148308, global_step=149059, preemption_count=0, score=50551.148308, test/accuracy=0.614700, test/loss=1.756366, test/num_examples=10000, total_duration=52416.783524, train/accuracy=0.847756, train/loss=0.667383, validation/accuracy=0.738500, validation/loss=1.130213, validation/num_examples=50000
I0127 09:35:42.104914 139863663834880 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.607133388519287, loss=2.08266544342041
I0127 09:36:15.949339 139862724310784 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.119167804718018, loss=2.119931221008301
I0127 09:36:49.784873 139863663834880 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.440091133117676, loss=2.069155216217041
I0127 09:37:23.617711 139862724310784 logging_writer.py:48] [149400] global_step=149400, grad_norm=6.5599164962768555, loss=2.144308567047119
I0127 09:37:57.575762 139863663834880 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.464209079742432, loss=2.0998027324676514
I0127 09:38:31.429076 139862724310784 logging_writer.py:48] [149600] global_step=149600, grad_norm=6.349423408508301, loss=2.0869970321655273
I0127 09:39:05.269515 139863663834880 logging_writer.py:48] [149700] global_step=149700, grad_norm=6.684591770172119, loss=2.0405170917510986
I0127 09:39:39.134121 139862724310784 logging_writer.py:48] [149800] global_step=149800, grad_norm=6.476626873016357, loss=2.1555020809173584
I0127 09:40:13.016506 139863663834880 logging_writer.py:48] [149900] global_step=149900, grad_norm=5.874592304229736, loss=2.0524253845214844
I0127 09:40:46.857576 139862724310784 logging_writer.py:48] [150000] global_step=150000, grad_norm=6.2265119552612305, loss=2.041191577911377
I0127 09:41:20.683478 139863663834880 logging_writer.py:48] [150100] global_step=150100, grad_norm=6.502058982849121, loss=2.0561625957489014
I0127 09:41:54.518657 139862724310784 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.046688079833984, loss=1.9741411209106445
I0127 09:42:28.393618 139863663834880 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.302248477935791, loss=2.1099953651428223
I0127 09:43:02.284813 139862724310784 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.21336030960083, loss=2.0845439434051514
I0127 09:43:36.290069 139863663834880 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.448851108551025, loss=2.0471811294555664
I0127 09:43:58.104727 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:44:04.461183 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:44:13.181586 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:44:15.500201 140027215431488 submission_runner.py:408] Time since start: 52944.47s, 	Step: 150566, 	{'train/accuracy': 0.8537547588348389, 'train/loss': 0.6530286073684692, 'validation/accuracy': 0.7407999634742737, 'validation/loss': 1.108092188835144, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.735827088356018, 'test/num_examples': 10000, 'score': 51061.33330345154, 'total_duration': 52944.4701063633, 'accumulated_submission_time': 51061.33330345154, 'accumulated_eval_time': 1873.110155582428, 'accumulated_logging_time': 4.844248533248901}
I0127 09:44:15.544835 139862715918080 logging_writer.py:48] [150566] accumulated_eval_time=1873.110156, accumulated_logging_time=4.844249, accumulated_submission_time=51061.333303, global_step=150566, preemption_count=0, score=51061.333303, test/accuracy=0.620500, test/loss=1.735827, test/num_examples=10000, total_duration=52944.470106, train/accuracy=0.853755, train/loss=0.653029, validation/accuracy=0.740800, validation/loss=1.108092, validation/num_examples=50000
I0127 09:44:27.389577 139862724310784 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.386922836303711, loss=1.9952350854873657
I0127 09:45:01.220673 139862715918080 logging_writer.py:48] [150700] global_step=150700, grad_norm=5.789715766906738, loss=1.996225118637085
I0127 09:45:35.105814 139862724310784 logging_writer.py:48] [150800] global_step=150800, grad_norm=5.999391555786133, loss=2.1389880180358887
I0127 09:46:08.959764 139862715918080 logging_writer.py:48] [150900] global_step=150900, grad_norm=6.224044322967529, loss=2.035900592803955
I0127 09:46:42.803484 139862724310784 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.313229084014893, loss=2.0494227409362793
I0127 09:47:16.634919 139862715918080 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.097987174987793, loss=2.0066771507263184
I0127 09:47:50.481557 139862724310784 logging_writer.py:48] [151200] global_step=151200, grad_norm=5.870060443878174, loss=1.9892045259475708
I0127 09:48:24.323009 139862715918080 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.856912612915039, loss=2.052868604660034
I0127 09:48:58.209671 139862724310784 logging_writer.py:48] [151400] global_step=151400, grad_norm=5.594512939453125, loss=1.9484877586364746
I0127 09:49:32.067385 139862715918080 logging_writer.py:48] [151500] global_step=151500, grad_norm=6.723106861114502, loss=2.0058512687683105
I0127 09:50:06.017470 139862724310784 logging_writer.py:48] [151600] global_step=151600, grad_norm=6.165412425994873, loss=2.00783634185791
I0127 09:50:39.869696 139862715918080 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.1174445152282715, loss=2.065864086151123
I0127 09:51:13.731219 139862724310784 logging_writer.py:48] [151800] global_step=151800, grad_norm=6.5227437019348145, loss=1.9865734577178955
I0127 09:51:47.605859 139862715918080 logging_writer.py:48] [151900] global_step=151900, grad_norm=6.48394250869751, loss=2.042616128921509
I0127 09:52:21.446087 139862724310784 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.172813892364502, loss=1.9679583311080933
I0127 09:52:45.619992 140027215431488 spec.py:321] Evaluating on the training split.
I0127 09:52:52.454020 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 09:53:01.152147 140027215431488 spec.py:349] Evaluating on the test split.
I0127 09:53:03.499374 140027215431488 submission_runner.py:408] Time since start: 53472.47s, 	Step: 152073, 	{'train/accuracy': 0.8474569320678711, 'train/loss': 0.6855957508087158, 'validation/accuracy': 0.743939995765686, 'validation/loss': 1.1150567531585693, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.7458469867706299, 'test/num_examples': 10000, 'score': 51571.34389591217, 'total_duration': 53472.46926546097, 'accumulated_submission_time': 51571.34389591217, 'accumulated_eval_time': 1890.9894676208496, 'accumulated_logging_time': 4.900099754333496}
I0127 09:53:03.541103 139862724310784 logging_writer.py:48] [152073] accumulated_eval_time=1890.989468, accumulated_logging_time=4.900100, accumulated_submission_time=51571.343896, global_step=152073, preemption_count=0, score=51571.343896, test/accuracy=0.621300, test/loss=1.745847, test/num_examples=10000, total_duration=53472.469265, train/accuracy=0.847457, train/loss=0.685596, validation/accuracy=0.743940, validation/loss=1.115057, validation/num_examples=50000
I0127 09:53:13.036193 139863663834880 logging_writer.py:48] [152100] global_step=152100, grad_norm=6.355421543121338, loss=1.992101788520813
I0127 09:53:46.877714 139862724310784 logging_writer.py:48] [152200] global_step=152200, grad_norm=6.839290142059326, loss=1.9962553977966309
I0127 09:54:20.735920 139863663834880 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.7696990966796875, loss=2.012725591659546
I0127 09:54:54.604437 139862724310784 logging_writer.py:48] [152400] global_step=152400, grad_norm=6.557376384735107, loss=1.9882889986038208
I0127 09:55:28.465786 139863663834880 logging_writer.py:48] [152500] global_step=152500, grad_norm=6.370758056640625, loss=2.0793721675872803
I0127 09:56:02.405566 139862724310784 logging_writer.py:48] [152600] global_step=152600, grad_norm=6.0987772941589355, loss=2.061552047729492
I0127 09:56:36.274238 139863663834880 logging_writer.py:48] [152700] global_step=152700, grad_norm=6.847731590270996, loss=2.112070083618164
I0127 09:57:10.126321 139862724310784 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.450806617736816, loss=2.076613426208496
I0127 09:57:43.971604 139863663834880 logging_writer.py:48] [152900] global_step=152900, grad_norm=6.841739177703857, loss=2.018768310546875
I0127 09:58:17.857298 139862724310784 logging_writer.py:48] [153000] global_step=153000, grad_norm=6.718295574188232, loss=2.0023281574249268
I0127 09:58:51.711270 139863663834880 logging_writer.py:48] [153100] global_step=153100, grad_norm=6.690323352813721, loss=1.9583253860473633
I0127 09:59:25.555829 139862724310784 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.252839088439941, loss=2.003084182739258
I0127 09:59:59.411493 139863663834880 logging_writer.py:48] [153300] global_step=153300, grad_norm=5.855103969573975, loss=1.9404048919677734
I0127 10:00:33.289503 139862724310784 logging_writer.py:48] [153400] global_step=153400, grad_norm=6.81868839263916, loss=2.00107741355896
I0127 10:01:07.142212 139863663834880 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.582442760467529, loss=2.082535743713379
I0127 10:01:33.685913 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:01:39.808417 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:01:48.577565 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:01:50.817523 140027215431488 submission_runner.py:408] Time since start: 53999.79s, 	Step: 153580, 	{'train/accuracy': 0.8515625, 'train/loss': 0.6506571769714355, 'validation/accuracy': 0.7438799738883972, 'validation/loss': 1.100218653678894, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.7106108665466309, 'test/num_examples': 10000, 'score': 52081.42667245865, 'total_duration': 53999.787418842316, 'accumulated_submission_time': 52081.42667245865, 'accumulated_eval_time': 1908.1210148334503, 'accumulated_logging_time': 4.9520440101623535}
I0127 10:01:50.857766 139866180368128 logging_writer.py:48] [153580] accumulated_eval_time=1908.121015, accumulated_logging_time=4.952044, accumulated_submission_time=52081.426672, global_step=153580, preemption_count=0, score=52081.426672, test/accuracy=0.622700, test/loss=1.710611, test/num_examples=10000, total_duration=53999.787419, train/accuracy=0.851562, train/loss=0.650657, validation/accuracy=0.743880, validation/loss=1.100219, validation/num_examples=50000
I0127 10:01:57.966142 139866188760832 logging_writer.py:48] [153600] global_step=153600, grad_norm=6.154376029968262, loss=2.010769844055176
I0127 10:02:31.901753 139866180368128 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.656628131866455, loss=1.9686431884765625
I0127 10:03:05.721697 139866188760832 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.662173271179199, loss=1.972699761390686
I0127 10:03:39.576240 139866180368128 logging_writer.py:48] [153900] global_step=153900, grad_norm=6.208014011383057, loss=1.983649730682373
I0127 10:04:13.446827 139866188760832 logging_writer.py:48] [154000] global_step=154000, grad_norm=6.823049545288086, loss=2.0423879623413086
I0127 10:04:47.303896 139866180368128 logging_writer.py:48] [154100] global_step=154100, grad_norm=6.639667987823486, loss=1.9425039291381836
I0127 10:05:21.154884 139866188760832 logging_writer.py:48] [154200] global_step=154200, grad_norm=6.830641269683838, loss=2.0745937824249268
I0127 10:05:55.038128 139866180368128 logging_writer.py:48] [154300] global_step=154300, grad_norm=6.285861015319824, loss=1.9205721616744995
I0127 10:06:28.904290 139866188760832 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.114960670471191, loss=1.9607356786727905
I0127 10:07:02.742504 139866180368128 logging_writer.py:48] [154500] global_step=154500, grad_norm=6.61312198638916, loss=1.9899144172668457
I0127 10:07:36.610243 139866188760832 logging_writer.py:48] [154600] global_step=154600, grad_norm=7.005329608917236, loss=2.0049288272857666
I0127 10:08:10.507242 139866180368128 logging_writer.py:48] [154700] global_step=154700, grad_norm=6.9968132972717285, loss=1.9704546928405762
I0127 10:08:44.442374 139866188760832 logging_writer.py:48] [154800] global_step=154800, grad_norm=7.353189468383789, loss=1.9920728206634521
I0127 10:09:18.313746 139866180368128 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.200626850128174, loss=2.018446207046509
I0127 10:09:52.153810 139866188760832 logging_writer.py:48] [155000] global_step=155000, grad_norm=6.909071445465088, loss=2.0599279403686523
I0127 10:10:21.064219 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:10:27.165488 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:10:35.887039 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:10:38.145175 140027215431488 submission_runner.py:408] Time since start: 54527.12s, 	Step: 155087, 	{'train/accuracy': 0.8556082248687744, 'train/loss': 0.6351298093795776, 'validation/accuracy': 0.7490400075912476, 'validation/loss': 1.0883169174194336, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.720607876777649, 'test/num_examples': 10000, 'score': 52591.56684565544, 'total_duration': 54527.11508727074, 'accumulated_submission_time': 52591.56684565544, 'accumulated_eval_time': 1925.2019228935242, 'accumulated_logging_time': 5.0060014724731445}
I0127 10:10:38.191648 139862724310784 logging_writer.py:48] [155087] accumulated_eval_time=1925.201923, accumulated_logging_time=5.006001, accumulated_submission_time=52591.566846, global_step=155087, preemption_count=0, score=52591.566846, test/accuracy=0.625800, test/loss=1.720608, test/num_examples=10000, total_duration=54527.115087, train/accuracy=0.855608, train/loss=0.635130, validation/accuracy=0.749040, validation/loss=1.088317, validation/num_examples=50000
I0127 10:10:42.923898 139863663834880 logging_writer.py:48] [155100] global_step=155100, grad_norm=6.309215545654297, loss=2.0113134384155273
I0127 10:11:16.777320 139862724310784 logging_writer.py:48] [155200] global_step=155200, grad_norm=7.051795482635498, loss=2.0568647384643555
I0127 10:11:50.613979 139863663834880 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.437629222869873, loss=2.004220724105835
I0127 10:12:24.508323 139862724310784 logging_writer.py:48] [155400] global_step=155400, grad_norm=6.954528331756592, loss=2.0504584312438965
I0127 10:12:58.352478 139863663834880 logging_writer.py:48] [155500] global_step=155500, grad_norm=6.3627166748046875, loss=1.9475436210632324
I0127 10:13:32.203610 139862724310784 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.163673400878906, loss=2.017362356185913
I0127 10:14:06.090896 139863663834880 logging_writer.py:48] [155700] global_step=155700, grad_norm=7.296175003051758, loss=2.0339205265045166
I0127 10:14:40.008903 139862724310784 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.885651588439941, loss=2.040778875350952
I0127 10:15:13.884526 139863663834880 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.167302131652832, loss=2.0618693828582764
I0127 10:15:47.731738 139862724310784 logging_writer.py:48] [156000] global_step=156000, grad_norm=6.067741870880127, loss=1.929539680480957
I0127 10:16:21.590149 139863663834880 logging_writer.py:48] [156100] global_step=156100, grad_norm=6.844066143035889, loss=1.959454894065857
I0127 10:16:55.426663 139862724310784 logging_writer.py:48] [156200] global_step=156200, grad_norm=6.398294448852539, loss=2.0870118141174316
I0127 10:17:29.276795 139863663834880 logging_writer.py:48] [156300] global_step=156300, grad_norm=6.659240245819092, loss=1.9205172061920166
I0127 10:18:03.153871 139862724310784 logging_writer.py:48] [156400] global_step=156400, grad_norm=6.479896545410156, loss=1.9554357528686523
I0127 10:18:37.000529 139863663834880 logging_writer.py:48] [156500] global_step=156500, grad_norm=6.331070899963379, loss=1.988205909729004
I0127 10:19:08.297906 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:19:14.480949 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:19:22.944466 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:19:25.247315 140027215431488 submission_runner.py:408] Time since start: 55054.22s, 	Step: 156594, 	{'train/accuracy': 0.8785076141357422, 'train/loss': 0.5570769309997559, 'validation/accuracy': 0.7483800053596497, 'validation/loss': 1.0872067213058472, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.7072957754135132, 'test/num_examples': 10000, 'score': 53101.60917067528, 'total_duration': 55054.2172267437, 'accumulated_submission_time': 53101.60917067528, 'accumulated_eval_time': 1942.1512784957886, 'accumulated_logging_time': 5.063514232635498}
I0127 10:19:25.292466 139862724310784 logging_writer.py:48] [156594] accumulated_eval_time=1942.151278, accumulated_logging_time=5.063514, accumulated_submission_time=53101.609171, global_step=156594, preemption_count=0, score=53101.609171, test/accuracy=0.626000, test/loss=1.707296, test/num_examples=10000, total_duration=55054.217227, train/accuracy=0.878508, train/loss=0.557077, validation/accuracy=0.748380, validation/loss=1.087207, validation/num_examples=50000
I0127 10:19:27.671087 139866163582720 logging_writer.py:48] [156600] global_step=156600, grad_norm=6.502504825592041, loss=1.8993268013000488
I0127 10:20:01.510309 139862724310784 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.47597599029541, loss=1.9355483055114746
I0127 10:20:35.436398 139866163582720 logging_writer.py:48] [156800] global_step=156800, grad_norm=6.457411289215088, loss=1.958337664604187
I0127 10:21:09.237495 139862724310784 logging_writer.py:48] [156900] global_step=156900, grad_norm=6.367684364318848, loss=1.8932284116744995
I0127 10:21:43.082178 139866163582720 logging_writer.py:48] [157000] global_step=157000, grad_norm=6.939793586730957, loss=1.9134103059768677
I0127 10:22:16.946915 139862724310784 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.057222366333008, loss=1.9311072826385498
I0127 10:22:50.790451 139866163582720 logging_writer.py:48] [157200] global_step=157200, grad_norm=6.730985641479492, loss=1.9997516870498657
I0127 10:23:24.643648 139862724310784 logging_writer.py:48] [157300] global_step=157300, grad_norm=7.026076316833496, loss=1.974429726600647
I0127 10:23:58.470832 139866163582720 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.046473979949951, loss=2.0237200260162354
I0127 10:24:32.309157 139862724310784 logging_writer.py:48] [157500] global_step=157500, grad_norm=7.099985599517822, loss=1.947715401649475
I0127 10:25:06.132871 139866163582720 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.831763744354248, loss=2.0027260780334473
I0127 10:25:39.991206 139862724310784 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.3292951583862305, loss=1.9924381971359253
I0127 10:26:13.820878 139866163582720 logging_writer.py:48] [157800] global_step=157800, grad_norm=7.007185935974121, loss=2.032518148422241
I0127 10:26:47.740194 139862724310784 logging_writer.py:48] [157900] global_step=157900, grad_norm=7.757556438446045, loss=1.9925780296325684
I0127 10:27:21.572943 139866163582720 logging_writer.py:48] [158000] global_step=158000, grad_norm=6.898694038391113, loss=1.9654767513275146
I0127 10:27:55.436207 139862724310784 logging_writer.py:48] [158100] global_step=158100, grad_norm=7.410150527954102, loss=1.9523775577545166
I0127 10:27:55.443742 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:28:01.599771 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:28:10.347621 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:28:12.603265 140027215431488 submission_runner.py:408] Time since start: 55581.57s, 	Step: 158101, 	{'train/accuracy': 0.8756377100944519, 'train/loss': 0.5695292949676514, 'validation/accuracy': 0.7520399689674377, 'validation/loss': 1.07098388671875, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.6892168521881104, 'test/num_examples': 10000, 'score': 53611.69621825218, 'total_duration': 55581.5731716156, 'accumulated_submission_time': 53611.69621825218, 'accumulated_eval_time': 1959.310719013214, 'accumulated_logging_time': 5.121617794036865}
I0127 10:28:12.648266 139865760950016 logging_writer.py:48] [158101] accumulated_eval_time=1959.310719, accumulated_logging_time=5.121618, accumulated_submission_time=53611.696218, global_step=158101, preemption_count=0, score=53611.696218, test/accuracy=0.628600, test/loss=1.689217, test/num_examples=10000, total_duration=55581.573172, train/accuracy=0.875638, train/loss=0.569529, validation/accuracy=0.752040, validation/loss=1.070984, validation/num_examples=50000
I0127 10:28:46.544261 139865769342720 logging_writer.py:48] [158200] global_step=158200, grad_norm=7.350618362426758, loss=1.9219894409179688
I0127 10:29:20.392844 139865760950016 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.04477596282959, loss=1.9223697185516357
I0127 10:29:54.244528 139865769342720 logging_writer.py:48] [158400] global_step=158400, grad_norm=6.375256538391113, loss=1.9006474018096924
I0127 10:30:28.128506 139865760950016 logging_writer.py:48] [158500] global_step=158500, grad_norm=6.783838748931885, loss=2.0123891830444336
I0127 10:31:01.969198 139865769342720 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.0193281173706055, loss=1.936518907546997
I0127 10:31:35.817527 139865760950016 logging_writer.py:48] [158700] global_step=158700, grad_norm=6.841166019439697, loss=1.975907802581787
I0127 10:32:09.685986 139865769342720 logging_writer.py:48] [158800] global_step=158800, grad_norm=6.671118259429932, loss=1.939018726348877
I0127 10:32:43.585391 139865760950016 logging_writer.py:48] [158900] global_step=158900, grad_norm=6.688521862030029, loss=1.9610140323638916
I0127 10:33:17.509243 139865769342720 logging_writer.py:48] [159000] global_step=159000, grad_norm=6.435457229614258, loss=1.902489423751831
I0127 10:33:51.385935 139865760950016 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.015312194824219, loss=1.9544897079467773
I0127 10:34:25.230414 139865769342720 logging_writer.py:48] [159200] global_step=159200, grad_norm=6.685580253601074, loss=1.9309377670288086
I0127 10:34:59.107823 139865760950016 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.699298858642578, loss=1.9274195432662964
I0127 10:35:32.954714 139865769342720 logging_writer.py:48] [159400] global_step=159400, grad_norm=6.726437568664551, loss=1.9475737810134888
I0127 10:36:06.826548 139865760950016 logging_writer.py:48] [159500] global_step=159500, grad_norm=6.971913814544678, loss=1.8760442733764648
I0127 10:36:40.704972 139865769342720 logging_writer.py:48] [159600] global_step=159600, grad_norm=7.284438610076904, loss=2.0040252208709717
I0127 10:36:42.879876 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:36:48.969752 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:36:57.540730 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:36:59.928781 140027215431488 submission_runner.py:408] Time since start: 56108.90s, 	Step: 159608, 	{'train/accuracy': 0.8757772445678711, 'train/loss': 0.5535916686058044, 'validation/accuracy': 0.754539966583252, 'validation/loss': 1.05316162109375, 'validation/num_examples': 50000, 'test/accuracy': 0.6337000131607056, 'test/loss': 1.6755914688110352, 'test/num_examples': 10000, 'score': 54121.863714933395, 'total_duration': 56108.89868927002, 'accumulated_submission_time': 54121.863714933395, 'accumulated_eval_time': 1976.359569311142, 'accumulated_logging_time': 5.178009748458862}
I0127 10:36:59.972809 139862724310784 logging_writer.py:48] [159608] accumulated_eval_time=1976.359569, accumulated_logging_time=5.178010, accumulated_submission_time=54121.863715, global_step=159608, preemption_count=0, score=54121.863715, test/accuracy=0.633700, test/loss=1.675591, test/num_examples=10000, total_duration=56108.898689, train/accuracy=0.875777, train/loss=0.553592, validation/accuracy=0.754540, validation/loss=1.053162, validation/num_examples=50000
I0127 10:37:31.437672 139863663834880 logging_writer.py:48] [159700] global_step=159700, grad_norm=6.785531997680664, loss=1.9088468551635742
I0127 10:38:05.262760 139862724310784 logging_writer.py:48] [159800] global_step=159800, grad_norm=7.235732078552246, loss=1.9254673719406128
I0127 10:38:39.124972 139863663834880 logging_writer.py:48] [159900] global_step=159900, grad_norm=6.932591438293457, loss=1.962082862854004
I0127 10:39:12.993240 139862724310784 logging_writer.py:48] [160000] global_step=160000, grad_norm=6.550472736358643, loss=1.8474851846694946
I0127 10:39:46.876889 139863663834880 logging_writer.py:48] [160100] global_step=160100, grad_norm=6.792340278625488, loss=1.8809001445770264
I0127 10:40:20.700640 139862724310784 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.062295436859131, loss=1.9379298686981201
I0127 10:40:54.581353 139863663834880 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.4110589027404785, loss=1.8940397500991821
I0127 10:41:28.430067 139862724310784 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.636548042297363, loss=1.9376063346862793
I0127 10:42:02.249959 139863663834880 logging_writer.py:48] [160500] global_step=160500, grad_norm=7.158724308013916, loss=1.9410145282745361
I0127 10:42:36.124568 139862724310784 logging_writer.py:48] [160600] global_step=160600, grad_norm=6.9588541984558105, loss=1.8523939847946167
I0127 10:43:10.002783 139863663834880 logging_writer.py:48] [160700] global_step=160700, grad_norm=7.259177207946777, loss=2.005073070526123
I0127 10:43:43.845336 139862724310784 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.360054969787598, loss=1.973921298980713
I0127 10:44:17.704692 139863663834880 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.3284783363342285, loss=1.9372299909591675
I0127 10:44:51.572307 139862724310784 logging_writer.py:48] [161000] global_step=161000, grad_norm=6.841175079345703, loss=1.9393432140350342
I0127 10:45:25.485987 139863663834880 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.012499809265137, loss=2.0052859783172607
I0127 10:45:30.030287 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:45:36.179058 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:45:44.870852 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:45:47.200796 140027215431488 submission_runner.py:408] Time since start: 56636.17s, 	Step: 161115, 	{'train/accuracy': 0.8797831535339355, 'train/loss': 0.5523720979690552, 'validation/accuracy': 0.7566799521446228, 'validation/loss': 1.0592304468154907, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.6685220003128052, 'test/num_examples': 10000, 'score': 54631.85998415947, 'total_duration': 56636.17068815231, 'accumulated_submission_time': 54631.85998415947, 'accumulated_eval_time': 1993.5300033092499, 'accumulated_logging_time': 5.232148885726929}
I0127 10:45:47.266693 139862724310784 logging_writer.py:48] [161115] accumulated_eval_time=1993.530003, accumulated_logging_time=5.232149, accumulated_submission_time=54631.859984, global_step=161115, preemption_count=0, score=54631.859984, test/accuracy=0.638100, test/loss=1.668522, test/num_examples=10000, total_duration=56636.170688, train/accuracy=0.879783, train/loss=0.552372, validation/accuracy=0.756680, validation/loss=1.059230, validation/num_examples=50000
I0127 10:46:16.376648 139866163582720 logging_writer.py:48] [161200] global_step=161200, grad_norm=6.898536205291748, loss=1.9211578369140625
I0127 10:46:50.242202 139862724310784 logging_writer.py:48] [161300] global_step=161300, grad_norm=6.336434364318848, loss=1.9455642700195312
I0127 10:47:24.088166 139866163582720 logging_writer.py:48] [161400] global_step=161400, grad_norm=7.123413562774658, loss=1.9134646654129028
I0127 10:47:57.928971 139862724310784 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.279188632965088, loss=1.913618803024292
I0127 10:48:31.777319 139866163582720 logging_writer.py:48] [161600] global_step=161600, grad_norm=7.333906650543213, loss=1.9423911571502686
I0127 10:49:05.629952 139862724310784 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.867095470428467, loss=1.861368179321289
I0127 10:49:39.498209 139866163582720 logging_writer.py:48] [161800] global_step=161800, grad_norm=7.162847995758057, loss=1.914697289466858
I0127 10:50:13.357227 139862724310784 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.098607540130615, loss=1.9047088623046875
I0127 10:50:47.219316 139866163582720 logging_writer.py:48] [162000] global_step=162000, grad_norm=6.7397050857543945, loss=1.9389015436172485
I0127 10:51:21.133762 139862724310784 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.386915683746338, loss=1.953537940979004
I0127 10:51:54.982691 139866163582720 logging_writer.py:48] [162200] global_step=162200, grad_norm=6.781285762786865, loss=1.9176321029663086
I0127 10:52:28.838429 139862724310784 logging_writer.py:48] [162300] global_step=162300, grad_norm=7.455318927764893, loss=2.0176401138305664
I0127 10:53:02.707498 139866163582720 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.184060573577881, loss=1.9954451322555542
I0127 10:53:36.567603 139862724310784 logging_writer.py:48] [162500] global_step=162500, grad_norm=6.9813055992126465, loss=1.9154908657073975
I0127 10:54:10.444448 139866163582720 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.565899848937988, loss=1.8838306665420532
I0127 10:54:17.360382 140027215431488 spec.py:321] Evaluating on the training split.
I0127 10:54:23.517513 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 10:54:32.132324 140027215431488 spec.py:349] Evaluating on the test split.
I0127 10:54:34.469272 140027215431488 submission_runner.py:408] Time since start: 57163.44s, 	Step: 162622, 	{'train/accuracy': 0.8821747303009033, 'train/loss': 0.5450007915496826, 'validation/accuracy': 0.758080005645752, 'validation/loss': 1.0532749891281128, 'validation/num_examples': 50000, 'test/accuracy': 0.6375000476837158, 'test/loss': 1.6655832529067993, 'test/num_examples': 10000, 'score': 55141.891810655594, 'total_duration': 57163.43918633461, 'accumulated_submission_time': 55141.891810655594, 'accumulated_eval_time': 2010.6388430595398, 'accumulated_logging_time': 5.307616949081421}
I0127 10:54:34.517281 139862724310784 logging_writer.py:48] [162622] accumulated_eval_time=2010.638843, accumulated_logging_time=5.307617, accumulated_submission_time=55141.891811, global_step=162622, preemption_count=0, score=55141.891811, test/accuracy=0.637500, test/loss=1.665583, test/num_examples=10000, total_duration=57163.439186, train/accuracy=0.882175, train/loss=0.545001, validation/accuracy=0.758080, validation/loss=1.053275, validation/num_examples=50000
I0127 10:55:01.249675 139865769342720 logging_writer.py:48] [162700] global_step=162700, grad_norm=7.37913179397583, loss=1.9924813508987427
I0127 10:55:35.096513 139862724310784 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.451003551483154, loss=1.864840030670166
I0127 10:56:08.920187 139865769342720 logging_writer.py:48] [162900] global_step=162900, grad_norm=7.466280460357666, loss=1.941907525062561
I0127 10:56:42.786854 139862724310784 logging_writer.py:48] [163000] global_step=163000, grad_norm=6.835957050323486, loss=1.9024282693862915
I0127 10:57:16.668882 139865769342720 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.079223155975342, loss=1.8683524131774902
I0127 10:57:50.560629 139862724310784 logging_writer.py:48] [163200] global_step=163200, grad_norm=7.030488014221191, loss=1.9088093042373657
I0127 10:58:24.445981 139865769342720 logging_writer.py:48] [163300] global_step=163300, grad_norm=7.684035778045654, loss=1.8788800239562988
I0127 10:58:58.308555 139862724310784 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.092250347137451, loss=1.9189629554748535
I0127 10:59:32.154217 139865769342720 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.466948986053467, loss=1.9528632164001465
I0127 11:00:06.030130 139862724310784 logging_writer.py:48] [163600] global_step=163600, grad_norm=7.465574741363525, loss=1.9176089763641357
I0127 11:00:39.913069 139865769342720 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.504765510559082, loss=1.8961623907089233
I0127 11:01:13.774733 139862724310784 logging_writer.py:48] [163800] global_step=163800, grad_norm=7.079099655151367, loss=1.881298303604126
I0127 11:01:47.665329 139865769342720 logging_writer.py:48] [163900] global_step=163900, grad_norm=7.3149919509887695, loss=1.9228485822677612
I0127 11:02:21.511321 139862724310784 logging_writer.py:48] [164000] global_step=164000, grad_norm=7.364346027374268, loss=1.8813607692718506
I0127 11:02:55.361676 139865769342720 logging_writer.py:48] [164100] global_step=164100, grad_norm=7.782863140106201, loss=1.8811818361282349
I0127 11:03:04.630229 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:03:10.738570 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:03:19.285789 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:03:21.605360 140027215431488 submission_runner.py:408] Time since start: 57690.58s, 	Step: 164129, 	{'train/accuracy': 0.8889508843421936, 'train/loss': 0.5121031999588013, 'validation/accuracy': 0.761139988899231, 'validation/loss': 1.0397335290908813, 'validation/num_examples': 50000, 'test/accuracy': 0.6361000537872314, 'test/loss': 1.6553908586502075, 'test/num_examples': 10000, 'score': 55651.94191503525, 'total_duration': 57690.5752518177, 'accumulated_submission_time': 55651.94191503525, 'accumulated_eval_time': 2027.6138999462128, 'accumulated_logging_time': 5.36580753326416}
I0127 11:03:21.649178 139862715918080 logging_writer.py:48] [164129] accumulated_eval_time=2027.613900, accumulated_logging_time=5.365808, accumulated_submission_time=55651.941915, global_step=164129, preemption_count=0, score=55651.941915, test/accuracy=0.636100, test/loss=1.655391, test/num_examples=10000, total_duration=57690.575252, train/accuracy=0.888951, train/loss=0.512103, validation/accuracy=0.761140, validation/loss=1.039734, validation/num_examples=50000
I0127 11:03:46.087530 139865760950016 logging_writer.py:48] [164200] global_step=164200, grad_norm=6.9130024909973145, loss=1.8561912775039673
I0127 11:04:19.922887 139862715918080 logging_writer.py:48] [164300] global_step=164300, grad_norm=7.395480155944824, loss=1.8738282918930054
I0127 11:04:53.762160 139865760950016 logging_writer.py:48] [164400] global_step=164400, grad_norm=7.530448913574219, loss=1.9170453548431396
I0127 11:05:27.624800 139862715918080 logging_writer.py:48] [164500] global_step=164500, grad_norm=7.680050373077393, loss=1.9187871217727661
I0127 11:06:01.435696 139865760950016 logging_writer.py:48] [164600] global_step=164600, grad_norm=7.777883529663086, loss=1.8651477098464966
I0127 11:06:35.301232 139862715918080 logging_writer.py:48] [164700] global_step=164700, grad_norm=6.709955215454102, loss=1.8610875606536865
I0127 11:07:09.169149 139865760950016 logging_writer.py:48] [164800] global_step=164800, grad_norm=7.107539176940918, loss=1.8869571685791016
I0127 11:07:43.054566 139862715918080 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.101217269897461, loss=1.8672285079956055
I0127 11:08:16.925171 139865760950016 logging_writer.py:48] [165000] global_step=165000, grad_norm=6.8086066246032715, loss=1.878661036491394
I0127 11:08:50.792604 139862715918080 logging_writer.py:48] [165100] global_step=165100, grad_norm=7.315682411193848, loss=1.9078460931777954
I0127 11:09:24.631252 139865760950016 logging_writer.py:48] [165200] global_step=165200, grad_norm=6.904261112213135, loss=1.83170485496521
I0127 11:09:58.637459 139862715918080 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.548198699951172, loss=1.8949337005615234
I0127 11:10:32.479059 139865760950016 logging_writer.py:48] [165400] global_step=165400, grad_norm=7.8479204177856445, loss=1.9076319932937622
I0127 11:11:06.355098 139862715918080 logging_writer.py:48] [165500] global_step=165500, grad_norm=7.772735118865967, loss=1.8659496307373047
I0127 11:11:40.197878 139865760950016 logging_writer.py:48] [165600] global_step=165600, grad_norm=6.809788227081299, loss=1.8568546772003174
I0127 11:11:51.833089 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:11:57.994781 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:12:06.768852 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:12:09.103733 140027215431488 submission_runner.py:408] Time since start: 58218.07s, 	Step: 165636, 	{'train/accuracy': 0.9054129123687744, 'train/loss': 0.462770015001297, 'validation/accuracy': 0.7623400092124939, 'validation/loss': 1.0329078435897827, 'validation/num_examples': 50000, 'test/accuracy': 0.6397000551223755, 'test/loss': 1.6543211936950684, 'test/num_examples': 10000, 'score': 56162.06087732315, 'total_duration': 58218.07364296913, 'accumulated_submission_time': 56162.06087732315, 'accumulated_eval_time': 2044.8844938278198, 'accumulated_logging_time': 5.420394659042358}
I0127 11:12:09.156369 139865769342720 logging_writer.py:48] [165636] accumulated_eval_time=2044.884494, accumulated_logging_time=5.420395, accumulated_submission_time=56162.060877, global_step=165636, preemption_count=0, score=56162.060877, test/accuracy=0.639700, test/loss=1.654321, test/num_examples=10000, total_duration=58218.073643, train/accuracy=0.905413, train/loss=0.462770, validation/accuracy=0.762340, validation/loss=1.032908, validation/num_examples=50000
I0127 11:12:31.170197 139866180368128 logging_writer.py:48] [165700] global_step=165700, grad_norm=7.176159381866455, loss=1.8927873373031616
I0127 11:13:04.998081 139865769342720 logging_writer.py:48] [165800] global_step=165800, grad_norm=7.1720452308654785, loss=1.895853877067566
I0127 11:13:38.820366 139866180368128 logging_writer.py:48] [165900] global_step=165900, grad_norm=6.401613712310791, loss=1.8191052675247192
I0127 11:14:12.653659 139865769342720 logging_writer.py:48] [166000] global_step=166000, grad_norm=7.984753131866455, loss=1.83091139793396
I0127 11:14:46.534831 139866180368128 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.27679443359375, loss=1.8113080263137817
I0127 11:15:20.413592 139865769342720 logging_writer.py:48] [166200] global_step=166200, grad_norm=7.720240116119385, loss=1.8760546445846558
I0127 11:15:54.260413 139866180368128 logging_writer.py:48] [166300] global_step=166300, grad_norm=7.2649149894714355, loss=1.8684128522872925
I0127 11:16:28.203034 139865769342720 logging_writer.py:48] [166400] global_step=166400, grad_norm=8.369587898254395, loss=1.8956042528152466
I0127 11:17:02.040678 139866180368128 logging_writer.py:48] [166500] global_step=166500, grad_norm=6.86296272277832, loss=1.8286383152008057
I0127 11:17:35.885872 139865769342720 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.841956615447998, loss=1.8635671138763428
I0127 11:18:09.780894 139866180368128 logging_writer.py:48] [166700] global_step=166700, grad_norm=7.941960334777832, loss=1.8688706159591675
I0127 11:18:43.635435 139865769342720 logging_writer.py:48] [166800] global_step=166800, grad_norm=7.930981636047363, loss=1.8279223442077637
I0127 11:19:17.490513 139866180368128 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.017560958862305, loss=1.812089204788208
I0127 11:19:51.379722 139865769342720 logging_writer.py:48] [167000] global_step=167000, grad_norm=7.207729816436768, loss=1.8463664054870605
I0127 11:20:25.243277 139866180368128 logging_writer.py:48] [167100] global_step=167100, grad_norm=7.470431804656982, loss=1.881901502609253
I0127 11:20:39.250568 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:20:45.357244 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:20:54.046921 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:20:56.405834 140027215431488 submission_runner.py:408] Time since start: 58745.38s, 	Step: 167143, 	{'train/accuracy': 0.9016661047935486, 'train/loss': 0.47373443841934204, 'validation/accuracy': 0.7623800039291382, 'validation/loss': 1.0327937602996826, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.6520830392837524, 'test/num_examples': 10000, 'score': 56672.09117293358, 'total_duration': 58745.37574315071, 'accumulated_submission_time': 56672.09117293358, 'accumulated_eval_time': 2062.039719581604, 'accumulated_logging_time': 5.4839723110198975}
I0127 11:20:56.452199 139865760950016 logging_writer.py:48] [167143] accumulated_eval_time=2062.039720, accumulated_logging_time=5.483972, accumulated_submission_time=56672.091173, global_step=167143, preemption_count=0, score=56672.091173, test/accuracy=0.640700, test/loss=1.652083, test/num_examples=10000, total_duration=58745.375743, train/accuracy=0.901666, train/loss=0.473734, validation/accuracy=0.762380, validation/loss=1.032794, validation/num_examples=50000
I0127 11:21:16.062016 139866163582720 logging_writer.py:48] [167200] global_step=167200, grad_norm=7.249580383300781, loss=1.8568909168243408
I0127 11:21:49.896344 139865760950016 logging_writer.py:48] [167300] global_step=167300, grad_norm=7.3447113037109375, loss=1.848658800125122
I0127 11:22:23.796277 139866163582720 logging_writer.py:48] [167400] global_step=167400, grad_norm=7.686575889587402, loss=1.8432025909423828
I0127 11:22:57.663724 139865760950016 logging_writer.py:48] [167500] global_step=167500, grad_norm=7.403618812561035, loss=1.8800737857818604
I0127 11:23:31.523151 139866163582720 logging_writer.py:48] [167600] global_step=167600, grad_norm=7.342072486877441, loss=1.815321445465088
I0127 11:24:05.357643 139865760950016 logging_writer.py:48] [167700] global_step=167700, grad_norm=8.2183198928833, loss=1.88381028175354
I0127 11:24:39.185456 139866163582720 logging_writer.py:48] [167800] global_step=167800, grad_norm=7.517274856567383, loss=1.8378922939300537
I0127 11:25:13.035529 139865760950016 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.433737754821777, loss=1.8263174295425415
I0127 11:25:46.874450 139866163582720 logging_writer.py:48] [168000] global_step=168000, grad_norm=7.147822380065918, loss=1.8268283605575562
I0127 11:26:20.741950 139865760950016 logging_writer.py:48] [168100] global_step=168100, grad_norm=8.119915008544922, loss=1.827534556388855
I0127 11:26:54.609812 139866163582720 logging_writer.py:48] [168200] global_step=168200, grad_norm=7.589674949645996, loss=1.8357080221176147
I0127 11:27:28.454700 139865760950016 logging_writer.py:48] [168300] global_step=168300, grad_norm=6.92483377456665, loss=1.7753384113311768
I0127 11:28:02.319134 139866163582720 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.192931175231934, loss=1.8519401550292969
I0127 11:28:36.217861 139865760950016 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.294120788574219, loss=1.8183417320251465
I0127 11:29:10.094861 139866163582720 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.121895790100098, loss=1.885219693183899
I0127 11:29:26.461181 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:29:32.694352 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:29:41.434509 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:29:43.724049 140027215431488 submission_runner.py:408] Time since start: 59272.69s, 	Step: 168650, 	{'train/accuracy': 0.9009087681770325, 'train/loss': 0.4697478115558624, 'validation/accuracy': 0.766319990158081, 'validation/loss': 1.0211455821990967, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.6334751844406128, 'test/num_examples': 10000, 'score': 57182.03576087952, 'total_duration': 59272.69396233559, 'accumulated_submission_time': 57182.03576087952, 'accumulated_eval_time': 2079.302550792694, 'accumulated_logging_time': 5.541658639907837}
I0127 11:29:43.770428 139862724310784 logging_writer.py:48] [168650] accumulated_eval_time=2079.302551, accumulated_logging_time=5.541659, accumulated_submission_time=57182.035761, global_step=168650, preemption_count=0, score=57182.035761, test/accuracy=0.640300, test/loss=1.633475, test/num_examples=10000, total_duration=59272.693962, train/accuracy=0.900909, train/loss=0.469748, validation/accuracy=0.766320, validation/loss=1.021146, validation/num_examples=50000
I0127 11:30:01.039346 139863663834880 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.313300132751465, loss=1.8678834438323975
I0127 11:30:34.866308 139862724310784 logging_writer.py:48] [168800] global_step=168800, grad_norm=8.327078819274902, loss=1.9191967248916626
I0127 11:31:08.712878 139863663834880 logging_writer.py:48] [168900] global_step=168900, grad_norm=7.238955020904541, loss=1.8354190587997437
I0127 11:31:42.592368 139862724310784 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.63167142868042, loss=1.8567681312561035
I0127 11:32:16.423225 139863663834880 logging_writer.py:48] [169100] global_step=169100, grad_norm=7.626979351043701, loss=1.7660672664642334
I0127 11:32:50.292190 139862724310784 logging_writer.py:48] [169200] global_step=169200, grad_norm=7.67396879196167, loss=1.7971519231796265
I0127 11:33:24.154051 139863663834880 logging_writer.py:48] [169300] global_step=169300, grad_norm=7.44998025894165, loss=1.8551862239837646
I0127 11:33:58.007660 139862724310784 logging_writer.py:48] [169400] global_step=169400, grad_norm=7.760925769805908, loss=1.9304022789001465
I0127 11:34:31.949812 139863663834880 logging_writer.py:48] [169500] global_step=169500, grad_norm=7.820918083190918, loss=1.8388491868972778
I0127 11:35:05.810148 139862724310784 logging_writer.py:48] [169600] global_step=169600, grad_norm=7.50603723526001, loss=1.8640291690826416
I0127 11:35:39.688252 139863663834880 logging_writer.py:48] [169700] global_step=169700, grad_norm=7.615835189819336, loss=1.8640044927597046
I0127 11:36:13.554746 139862724310784 logging_writer.py:48] [169800] global_step=169800, grad_norm=8.216328620910645, loss=1.7838594913482666
I0127 11:36:47.413495 139863663834880 logging_writer.py:48] [169900] global_step=169900, grad_norm=8.06643009185791, loss=1.902787446975708
I0127 11:37:21.231151 139862724310784 logging_writer.py:48] [170000] global_step=170000, grad_norm=7.575637340545654, loss=1.8803372383117676
I0127 11:37:55.084759 139863663834880 logging_writer.py:48] [170100] global_step=170100, grad_norm=7.289003849029541, loss=1.8102208375930786
I0127 11:38:13.853876 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:38:19.993921 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:38:28.650796 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:38:30.941192 140027215431488 submission_runner.py:408] Time since start: 59799.91s, 	Step: 170157, 	{'train/accuracy': 0.9041772484779358, 'train/loss': 0.45963263511657715, 'validation/accuracy': 0.7662799954414368, 'validation/loss': 1.0119706392288208, 'validation/num_examples': 50000, 'test/accuracy': 0.6470000147819519, 'test/loss': 1.6323281526565552, 'test/num_examples': 10000, 'score': 57692.05851483345, 'total_duration': 59799.91110467911, 'accumulated_submission_time': 57692.05851483345, 'accumulated_eval_time': 2096.3898191452026, 'accumulated_logging_time': 5.597667694091797}
I0127 11:38:30.991133 139862724310784 logging_writer.py:48] [170157] accumulated_eval_time=2096.389819, accumulated_logging_time=5.597668, accumulated_submission_time=57692.058515, global_step=170157, preemption_count=0, score=57692.058515, test/accuracy=0.647000, test/loss=1.632328, test/num_examples=10000, total_duration=59799.911105, train/accuracy=0.904177, train/loss=0.459633, validation/accuracy=0.766280, validation/loss=1.011971, validation/num_examples=50000
I0127 11:38:45.905675 139866163582720 logging_writer.py:48] [170200] global_step=170200, grad_norm=7.651513576507568, loss=1.8329335451126099
I0127 11:39:19.734358 139862724310784 logging_writer.py:48] [170300] global_step=170300, grad_norm=8.127008438110352, loss=1.8655040264129639
I0127 11:39:53.558051 139866163582720 logging_writer.py:48] [170400] global_step=170400, grad_norm=7.675755023956299, loss=1.8029811382293701
I0127 11:40:27.459417 139862724310784 logging_writer.py:48] [170500] global_step=170500, grad_norm=7.952578544616699, loss=1.8407566547393799
I0127 11:41:01.327821 139866163582720 logging_writer.py:48] [170600] global_step=170600, grad_norm=7.891813278198242, loss=1.893884539604187
I0127 11:41:35.206049 139862724310784 logging_writer.py:48] [170700] global_step=170700, grad_norm=7.525399684906006, loss=1.8127964735031128
I0127 11:42:09.092154 139866163582720 logging_writer.py:48] [170800] global_step=170800, grad_norm=7.816531181335449, loss=1.8071544170379639
I0127 11:42:42.928205 139862724310784 logging_writer.py:48] [170900] global_step=170900, grad_norm=8.4955415725708, loss=1.8085570335388184
I0127 11:43:16.772226 139866163582720 logging_writer.py:48] [171000] global_step=171000, grad_norm=6.897472381591797, loss=1.7861406803131104
I0127 11:43:50.607052 139862724310784 logging_writer.py:48] [171100] global_step=171100, grad_norm=8.205575942993164, loss=1.8499091863632202
I0127 11:44:24.500303 139866163582720 logging_writer.py:48] [171200] global_step=171200, grad_norm=7.507088661193848, loss=1.7771331071853638
I0127 11:44:58.353045 139862724310784 logging_writer.py:48] [171300] global_step=171300, grad_norm=7.614011764526367, loss=1.8472366333007812
I0127 11:45:32.197712 139866163582720 logging_writer.py:48] [171400] global_step=171400, grad_norm=7.827949047088623, loss=1.7375590801239014
I0127 11:46:06.028187 139862724310784 logging_writer.py:48] [171500] global_step=171500, grad_norm=7.592975616455078, loss=1.8124589920043945
I0127 11:46:39.957726 139866163582720 logging_writer.py:48] [171600] global_step=171600, grad_norm=8.146112442016602, loss=1.7965065240859985
I0127 11:47:01.096602 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:47:07.285328 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:47:15.838871 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:47:18.043880 140027215431488 submission_runner.py:408] Time since start: 60327.01s, 	Step: 171664, 	{'train/accuracy': 0.9072065949440002, 'train/loss': 0.4597722291946411, 'validation/accuracy': 0.7687399983406067, 'validation/loss': 1.0186874866485596, 'validation/num_examples': 50000, 'test/accuracy': 0.6431000232696533, 'test/loss': 1.6356762647628784, 'test/num_examples': 10000, 'score': 58202.10117435455, 'total_duration': 60327.013768196106, 'accumulated_submission_time': 58202.10117435455, 'accumulated_eval_time': 2113.3370258808136, 'accumulated_logging_time': 5.657930612564087}
I0127 11:47:18.092923 139865760950016 logging_writer.py:48] [171664] accumulated_eval_time=2113.337026, accumulated_logging_time=5.657931, accumulated_submission_time=58202.101174, global_step=171664, preemption_count=0, score=58202.101174, test/accuracy=0.643100, test/loss=1.635676, test/num_examples=10000, total_duration=60327.013768, train/accuracy=0.907207, train/loss=0.459772, validation/accuracy=0.768740, validation/loss=1.018687, validation/num_examples=50000
I0127 11:47:30.616194 139865769342720 logging_writer.py:48] [171700] global_step=171700, grad_norm=7.424726486206055, loss=1.846604585647583
I0127 11:48:04.444681 139865760950016 logging_writer.py:48] [171800] global_step=171800, grad_norm=7.111301422119141, loss=1.759950041770935
I0127 11:48:38.255349 139865769342720 logging_writer.py:48] [171900] global_step=171900, grad_norm=7.632604122161865, loss=1.8368232250213623
I0127 11:49:12.113173 139865760950016 logging_writer.py:48] [172000] global_step=172000, grad_norm=6.800439834594727, loss=1.806032657623291
I0127 11:49:45.936205 139865769342720 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.080137252807617, loss=1.8269541263580322
I0127 11:50:19.808337 139865760950016 logging_writer.py:48] [172200] global_step=172200, grad_norm=8.439558982849121, loss=1.863429069519043
I0127 11:50:53.655053 139865769342720 logging_writer.py:48] [172300] global_step=172300, grad_norm=7.506766319274902, loss=1.7996125221252441
I0127 11:51:27.498275 139865760950016 logging_writer.py:48] [172400] global_step=172400, grad_norm=8.354584693908691, loss=1.7513751983642578
I0127 11:52:01.377224 139865769342720 logging_writer.py:48] [172500] global_step=172500, grad_norm=8.051199913024902, loss=1.8438535928726196
I0127 11:52:35.255485 139865760950016 logging_writer.py:48] [172600] global_step=172600, grad_norm=7.480410099029541, loss=1.7824206352233887
I0127 11:53:09.161930 139865769342720 logging_writer.py:48] [172700] global_step=172700, grad_norm=6.994478702545166, loss=1.7950763702392578
I0127 11:53:43.044544 139865760950016 logging_writer.py:48] [172800] global_step=172800, grad_norm=8.283864974975586, loss=1.783334732055664
I0127 11:54:16.904883 139865769342720 logging_writer.py:48] [172900] global_step=172900, grad_norm=7.533457279205322, loss=1.85744047164917
I0127 11:54:50.782557 139865760950016 logging_writer.py:48] [173000] global_step=173000, grad_norm=6.9178338050842285, loss=1.751251459121704
I0127 11:55:24.611729 139865769342720 logging_writer.py:48] [173100] global_step=173100, grad_norm=7.478061199188232, loss=1.8311396837234497
I0127 11:55:48.105347 140027215431488 spec.py:321] Evaluating on the training split.
I0127 11:55:54.233809 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 11:56:02.781702 140027215431488 spec.py:349] Evaluating on the test split.
I0127 11:56:05.082789 140027215431488 submission_runner.py:408] Time since start: 60854.05s, 	Step: 173171, 	{'train/accuracy': 0.9073860049247742, 'train/loss': 0.44713687896728516, 'validation/accuracy': 0.7688800096511841, 'validation/loss': 1.0100610256195068, 'validation/num_examples': 50000, 'test/accuracy': 0.6465000510215759, 'test/loss': 1.6286207437515259, 'test/num_examples': 10000, 'score': 58712.05089068413, 'total_duration': 60854.05264925957, 'accumulated_submission_time': 58712.05089068413, 'accumulated_eval_time': 2130.31436419487, 'accumulated_logging_time': 5.717501878738403}
I0127 11:56:05.130071 139862715918080 logging_writer.py:48] [173171] accumulated_eval_time=2130.314364, accumulated_logging_time=5.717502, accumulated_submission_time=58712.050891, global_step=173171, preemption_count=0, score=58712.050891, test/accuracy=0.646500, test/loss=1.628621, test/num_examples=10000, total_duration=60854.052649, train/accuracy=0.907386, train/loss=0.447137, validation/accuracy=0.768880, validation/loss=1.010061, validation/num_examples=50000
I0127 11:56:15.308633 139863663834880 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.101677894592285, loss=1.8130030632019043
I0127 11:56:49.145179 139862715918080 logging_writer.py:48] [173300] global_step=173300, grad_norm=7.2556562423706055, loss=1.8809682130813599
I0127 11:57:22.984009 139863663834880 logging_writer.py:48] [173400] global_step=173400, grad_norm=7.436684608459473, loss=1.7258894443511963
I0127 11:57:56.813357 139862715918080 logging_writer.py:48] [173500] global_step=173500, grad_norm=7.690854549407959, loss=1.819068193435669
I0127 11:58:30.643432 139863663834880 logging_writer.py:48] [173600] global_step=173600, grad_norm=8.852355003356934, loss=1.8435742855072021
I0127 11:59:04.583016 139862715918080 logging_writer.py:48] [173700] global_step=173700, grad_norm=7.550540924072266, loss=1.826979637145996
I0127 11:59:38.428087 139863663834880 logging_writer.py:48] [173800] global_step=173800, grad_norm=7.442717552185059, loss=1.8708202838897705
I0127 12:00:12.256757 139862715918080 logging_writer.py:48] [173900] global_step=173900, grad_norm=7.826145648956299, loss=1.828228235244751
I0127 12:00:46.095983 139863663834880 logging_writer.py:48] [174000] global_step=174000, grad_norm=7.256808757781982, loss=1.7817033529281616
I0127 12:01:19.962689 139862715918080 logging_writer.py:48] [174100] global_step=174100, grad_norm=7.8850531578063965, loss=1.8150296211242676
I0127 12:01:53.834384 139863663834880 logging_writer.py:48] [174200] global_step=174200, grad_norm=7.947686672210693, loss=1.8437974452972412
I0127 12:02:27.666170 139862715918080 logging_writer.py:48] [174300] global_step=174300, grad_norm=7.132909774780273, loss=1.831789255142212
I0127 12:03:01.513492 139863663834880 logging_writer.py:48] [174400] global_step=174400, grad_norm=7.269231796264648, loss=1.8068602085113525
I0127 12:03:35.392986 139862715918080 logging_writer.py:48] [174500] global_step=174500, grad_norm=8.005043983459473, loss=1.8348453044891357
I0127 12:04:09.248346 139863663834880 logging_writer.py:48] [174600] global_step=174600, grad_norm=7.833400249481201, loss=1.8460456132888794
I0127 12:04:35.111631 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:04:41.265244 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:04:49.988530 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:04:52.301105 140027215431488 submission_runner.py:408] Time since start: 61381.27s, 	Step: 174678, 	{'train/accuracy': 0.9171914458274841, 'train/loss': 0.4137333035469055, 'validation/accuracy': 0.7696399688720703, 'validation/loss': 1.0041489601135254, 'validation/num_examples': 50000, 'test/accuracy': 0.6478000283241272, 'test/loss': 1.612573504447937, 'test/num_examples': 10000, 'score': 59221.97033691406, 'total_duration': 61381.271008491516, 'accumulated_submission_time': 59221.97033691406, 'accumulated_eval_time': 2147.5038130283356, 'accumulated_logging_time': 5.7750890254974365}
I0127 12:04:52.344991 139862724310784 logging_writer.py:48] [174678] accumulated_eval_time=2147.503813, accumulated_logging_time=5.775089, accumulated_submission_time=59221.970337, global_step=174678, preemption_count=0, score=59221.970337, test/accuracy=0.647800, test/loss=1.612574, test/num_examples=10000, total_duration=61381.271008, train/accuracy=0.917191, train/loss=0.413733, validation/accuracy=0.769640, validation/loss=1.004149, validation/num_examples=50000
I0127 12:05:00.141064 139866171975424 logging_writer.py:48] [174700] global_step=174700, grad_norm=7.79167366027832, loss=1.858169674873352
I0127 12:05:34.106466 139862724310784 logging_writer.py:48] [174800] global_step=174800, grad_norm=7.582456588745117, loss=1.7810269594192505
I0127 12:06:07.980021 139866171975424 logging_writer.py:48] [174900] global_step=174900, grad_norm=8.112665176391602, loss=1.8300039768218994
I0127 12:06:41.821247 139862724310784 logging_writer.py:48] [175000] global_step=175000, grad_norm=8.534842491149902, loss=1.8424828052520752
I0127 12:07:15.673063 139866171975424 logging_writer.py:48] [175100] global_step=175100, grad_norm=7.3248610496521, loss=1.8028137683868408
I0127 12:07:49.563741 139862724310784 logging_writer.py:48] [175200] global_step=175200, grad_norm=8.66672420501709, loss=1.8297499418258667
I0127 12:08:23.419050 139866171975424 logging_writer.py:48] [175300] global_step=175300, grad_norm=7.828617572784424, loss=1.8457841873168945
I0127 12:08:57.276882 139862724310784 logging_writer.py:48] [175400] global_step=175400, grad_norm=7.845604419708252, loss=1.7755714654922485
I0127 12:09:31.157151 139866171975424 logging_writer.py:48] [175500] global_step=175500, grad_norm=8.092936515808105, loss=1.8409830331802368
I0127 12:10:04.995811 139862724310784 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.418506622314453, loss=1.7794779539108276
I0127 12:10:38.835116 139866171975424 logging_writer.py:48] [175700] global_step=175700, grad_norm=7.877880573272705, loss=1.8198070526123047
I0127 12:11:12.827415 139862724310784 logging_writer.py:48] [175800] global_step=175800, grad_norm=8.052079200744629, loss=1.8083641529083252
I0127 12:11:46.688400 139866171975424 logging_writer.py:48] [175900] global_step=175900, grad_norm=8.40233325958252, loss=1.9237418174743652
I0127 12:12:20.581860 139862724310784 logging_writer.py:48] [176000] global_step=176000, grad_norm=7.895627021789551, loss=1.8361146450042725
I0127 12:12:54.418141 139866171975424 logging_writer.py:48] [176100] global_step=176100, grad_norm=7.34443473815918, loss=1.7869799137115479
I0127 12:13:22.332642 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:13:28.497837 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:13:37.025590 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:13:39.279158 140027215431488 submission_runner.py:408] Time since start: 61908.25s, 	Step: 176184, 	{'train/accuracy': 0.9174505472183228, 'train/loss': 0.415025532245636, 'validation/accuracy': 0.7699399590492249, 'validation/loss': 1.0015920400619507, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.610582947731018, 'test/num_examples': 10000, 'score': 59731.89277672768, 'total_duration': 61908.24905347824, 'accumulated_submission_time': 59731.89277672768, 'accumulated_eval_time': 2164.4502770900726, 'accumulated_logging_time': 5.829644203186035}
I0127 12:13:39.326084 139866180368128 logging_writer.py:48] [176184] accumulated_eval_time=2164.450277, accumulated_logging_time=5.829644, accumulated_submission_time=59731.892777, global_step=176184, preemption_count=0, score=59731.892777, test/accuracy=0.649200, test/loss=1.610583, test/num_examples=10000, total_duration=61908.249053, train/accuracy=0.917451, train/loss=0.415026, validation/accuracy=0.769940, validation/loss=1.001592, validation/num_examples=50000
I0127 12:13:45.128014 139866188760832 logging_writer.py:48] [176200] global_step=176200, grad_norm=7.782580375671387, loss=1.8174915313720703
I0127 12:14:18.961175 139866180368128 logging_writer.py:48] [176300] global_step=176300, grad_norm=7.6014204025268555, loss=1.8296869993209839
I0127 12:14:52.801233 139866188760832 logging_writer.py:48] [176400] global_step=176400, grad_norm=7.753140449523926, loss=1.8203728199005127
I0127 12:15:26.671001 139866180368128 logging_writer.py:48] [176500] global_step=176500, grad_norm=8.788129806518555, loss=1.8118114471435547
I0127 12:16:00.564733 139866188760832 logging_writer.py:48] [176600] global_step=176600, grad_norm=7.793694972991943, loss=1.7386513948440552
I0127 12:16:34.411689 139866180368128 logging_writer.py:48] [176700] global_step=176700, grad_norm=7.672242641448975, loss=1.8434700965881348
I0127 12:17:08.257326 139866188760832 logging_writer.py:48] [176800] global_step=176800, grad_norm=8.37617301940918, loss=1.8209809064865112
I0127 12:17:42.206968 139866180368128 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.341740608215332, loss=1.7891703844070435
I0127 12:18:16.053070 139866188760832 logging_writer.py:48] [177000] global_step=177000, grad_norm=8.478952407836914, loss=1.8114739656448364
I0127 12:18:49.895120 139866180368128 logging_writer.py:48] [177100] global_step=177100, grad_norm=8.575404167175293, loss=1.8752965927124023
I0127 12:19:23.770626 139866188760832 logging_writer.py:48] [177200] global_step=177200, grad_norm=9.676200866699219, loss=1.773813247680664
I0127 12:19:57.879464 139866180368128 logging_writer.py:48] [177300] global_step=177300, grad_norm=7.553778648376465, loss=1.7851734161376953
I0127 12:20:31.744062 139866188760832 logging_writer.py:48] [177400] global_step=177400, grad_norm=7.639868259429932, loss=1.7607098817825317
I0127 12:21:05.590915 139866180368128 logging_writer.py:48] [177500] global_step=177500, grad_norm=8.119231224060059, loss=1.8625272512435913
I0127 12:21:39.429668 139866188760832 logging_writer.py:48] [177600] global_step=177600, grad_norm=8.057419776916504, loss=1.776751160621643
I0127 12:22:09.378897 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:22:15.533959 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:22:24.074653 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:22:26.459326 140027215431488 submission_runner.py:408] Time since start: 62435.43s, 	Step: 177690, 	{'train/accuracy': 0.9185068607330322, 'train/loss': 0.4077030122280121, 'validation/accuracy': 0.7705599665641785, 'validation/loss': 0.9990577697753906, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.609252691268921, 'test/num_examples': 10000, 'score': 60241.88081359863, 'total_duration': 62435.42911171913, 'accumulated_submission_time': 60241.88081359863, 'accumulated_eval_time': 2181.530555009842, 'accumulated_logging_time': 5.88846755027771}
I0127 12:22:26.509600 139863663834880 logging_writer.py:48] [177690] accumulated_eval_time=2181.530555, accumulated_logging_time=5.888468, accumulated_submission_time=60241.880814, global_step=177690, preemption_count=0, score=60241.880814, test/accuracy=0.651500, test/loss=1.609253, test/num_examples=10000, total_duration=62435.429112, train/accuracy=0.918507, train/loss=0.407703, validation/accuracy=0.770560, validation/loss=0.999058, validation/num_examples=50000
I0127 12:22:30.232364 139865760950016 logging_writer.py:48] [177700] global_step=177700, grad_norm=8.778806686401367, loss=1.820223331451416
I0127 12:23:04.097979 139863663834880 logging_writer.py:48] [177800] global_step=177800, grad_norm=8.292068481445312, loss=1.8306130170822144
I0127 12:23:38.022399 139865760950016 logging_writer.py:48] [177900] global_step=177900, grad_norm=8.071972846984863, loss=1.85516357421875
I0127 12:24:11.896591 139863663834880 logging_writer.py:48] [178000] global_step=178000, grad_norm=7.378826141357422, loss=1.7870615720748901
I0127 12:24:45.788836 139865760950016 logging_writer.py:48] [178100] global_step=178100, grad_norm=7.851386547088623, loss=1.7602571249008179
I0127 12:25:19.652431 139863663834880 logging_writer.py:48] [178200] global_step=178200, grad_norm=7.8047099113464355, loss=1.7863354682922363
I0127 12:25:53.536538 139865760950016 logging_writer.py:48] [178300] global_step=178300, grad_norm=7.9902849197387695, loss=1.758402943611145
I0127 12:26:27.397009 139863663834880 logging_writer.py:48] [178400] global_step=178400, grad_norm=6.905909538269043, loss=1.7402820587158203
I0127 12:27:01.270025 139865760950016 logging_writer.py:48] [178500] global_step=178500, grad_norm=7.852462291717529, loss=1.7653400897979736
I0127 12:27:35.173990 139863663834880 logging_writer.py:48] [178600] global_step=178600, grad_norm=8.616009712219238, loss=1.869949221611023
I0127 12:28:09.007091 139865760950016 logging_writer.py:48] [178700] global_step=178700, grad_norm=7.305097579956055, loss=1.7650532722473145
I0127 12:28:42.881782 139863663834880 logging_writer.py:48] [178800] global_step=178800, grad_norm=8.027289390563965, loss=1.771915078163147
I0127 12:29:16.757231 139865760950016 logging_writer.py:48] [178900] global_step=178900, grad_norm=8.857301712036133, loss=1.790419101715088
I0127 12:29:50.693177 139863663834880 logging_writer.py:48] [179000] global_step=179000, grad_norm=7.315598011016846, loss=1.7669659852981567
I0127 12:30:24.586597 139865760950016 logging_writer.py:48] [179100] global_step=179100, grad_norm=7.200780391693115, loss=1.8125383853912354
I0127 12:30:56.550224 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:31:02.687801 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:31:11.194179 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:31:13.572462 140027215431488 submission_runner.py:408] Time since start: 62962.54s, 	Step: 179196, 	{'train/accuracy': 0.9178292155265808, 'train/loss': 0.41300442814826965, 'validation/accuracy': 0.7719599604606628, 'validation/loss': 0.9963983297348022, 'validation/num_examples': 50000, 'test/accuracy': 0.6491000056266785, 'test/loss': 1.6055240631103516, 'test/num_examples': 10000, 'score': 60751.85917687416, 'total_duration': 62962.54237771034, 'accumulated_submission_time': 60751.85917687416, 'accumulated_eval_time': 2198.5527551174164, 'accumulated_logging_time': 5.948404788970947}
I0127 12:31:13.621544 139866171975424 logging_writer.py:48] [179196] accumulated_eval_time=2198.552755, accumulated_logging_time=5.948405, accumulated_submission_time=60751.859177, global_step=179196, preemption_count=0, score=60751.859177, test/accuracy=0.649100, test/loss=1.605524, test/num_examples=10000, total_duration=62962.542378, train/accuracy=0.917829, train/loss=0.413004, validation/accuracy=0.771960, validation/loss=0.996398, validation/num_examples=50000
I0127 12:31:15.325436 139866180368128 logging_writer.py:48] [179200] global_step=179200, grad_norm=8.257444381713867, loss=1.7480112314224243
I0127 12:31:49.157621 139866171975424 logging_writer.py:48] [179300] global_step=179300, grad_norm=8.12043571472168, loss=1.7776460647583008
I0127 12:32:23.011360 139866180368128 logging_writer.py:48] [179400] global_step=179400, grad_norm=7.788121223449707, loss=1.7324378490447998
I0127 12:32:56.868455 139866171975424 logging_writer.py:48] [179500] global_step=179500, grad_norm=8.45512866973877, loss=1.7759177684783936
I0127 12:33:30.692400 139866180368128 logging_writer.py:48] [179600] global_step=179600, grad_norm=7.686715602874756, loss=1.764509677886963
I0127 12:34:04.571458 139866171975424 logging_writer.py:48] [179700] global_step=179700, grad_norm=7.741358757019043, loss=1.7664721012115479
I0127 12:34:38.412946 139866180368128 logging_writer.py:48] [179800] global_step=179800, grad_norm=7.437921047210693, loss=1.7665777206420898
I0127 12:35:12.287158 139866171975424 logging_writer.py:48] [179900] global_step=179900, grad_norm=7.78389310836792, loss=1.735906720161438
I0127 12:35:46.123018 139866180368128 logging_writer.py:48] [180000] global_step=180000, grad_norm=7.513465881347656, loss=1.7134730815887451
I0127 12:36:20.050964 139866171975424 logging_writer.py:48] [180100] global_step=180100, grad_norm=8.485821723937988, loss=1.8093674182891846
I0127 12:36:53.945835 139866180368128 logging_writer.py:48] [180200] global_step=180200, grad_norm=8.887789726257324, loss=1.733796238899231
I0127 12:37:27.798078 139866171975424 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.285094261169434, loss=1.7570083141326904
I0127 12:38:01.653746 139866180368128 logging_writer.py:48] [180400] global_step=180400, grad_norm=8.08233642578125, loss=1.796927809715271
I0127 12:38:35.541175 139866171975424 logging_writer.py:48] [180500] global_step=180500, grad_norm=7.793805122375488, loss=1.7592577934265137
I0127 12:39:09.389569 139866180368128 logging_writer.py:48] [180600] global_step=180600, grad_norm=7.150359153747559, loss=1.7280374765396118
I0127 12:39:43.234213 139866171975424 logging_writer.py:48] [180700] global_step=180700, grad_norm=8.383071899414062, loss=1.8417012691497803
I0127 12:39:43.719103 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:39:49.993923 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:39:58.701957 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:40:01.077518 140027215431488 submission_runner.py:408] Time since start: 63490.05s, 	Step: 180703, 	{'train/accuracy': 0.9183274507522583, 'train/loss': 0.40757840871810913, 'validation/accuracy': 0.7716599702835083, 'validation/loss': 0.9949711561203003, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6056536436080933, 'test/num_examples': 10000, 'score': 61261.88948059082, 'total_duration': 63490.04742026329, 'accumulated_submission_time': 61261.88948059082, 'accumulated_eval_time': 2215.9111063480377, 'accumulated_logging_time': 6.012499570846558}
I0127 12:40:01.126048 139862724310784 logging_writer.py:48] [180703] accumulated_eval_time=2215.911106, accumulated_logging_time=6.012500, accumulated_submission_time=61261.889481, global_step=180703, preemption_count=0, score=61261.889481, test/accuracy=0.650900, test/loss=1.605654, test/num_examples=10000, total_duration=63490.047420, train/accuracy=0.918327, train/loss=0.407578, validation/accuracy=0.771660, validation/loss=0.994971, validation/num_examples=50000
I0127 12:40:34.319877 139863663834880 logging_writer.py:48] [180800] global_step=180800, grad_norm=8.681708335876465, loss=1.8037471771240234
I0127 12:41:08.170945 139862724310784 logging_writer.py:48] [180900] global_step=180900, grad_norm=7.537574291229248, loss=1.7447848320007324
I0127 12:41:42.023657 139863663834880 logging_writer.py:48] [181000] global_step=181000, grad_norm=6.907009601593018, loss=1.7179750204086304
I0127 12:42:15.967513 139862724310784 logging_writer.py:48] [181100] global_step=181100, grad_norm=7.541903972625732, loss=1.7933350801467896
I0127 12:42:49.828597 139863663834880 logging_writer.py:48] [181200] global_step=181200, grad_norm=7.89138650894165, loss=1.7492995262145996
I0127 12:43:23.642906 139862724310784 logging_writer.py:48] [181300] global_step=181300, grad_norm=7.359926700592041, loss=1.7218056917190552
I0127 12:43:57.520640 139863663834880 logging_writer.py:48] [181400] global_step=181400, grad_norm=8.394439697265625, loss=1.7538954019546509
I0127 12:44:31.391246 139862724310784 logging_writer.py:48] [181500] global_step=181500, grad_norm=7.8906474113464355, loss=1.7111512422561646
I0127 12:45:05.239834 139863663834880 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.594673156738281, loss=1.8136101961135864
I0127 12:45:39.082090 139862724310784 logging_writer.py:48] [181700] global_step=181700, grad_norm=7.420718193054199, loss=1.7932486534118652
I0127 12:46:12.957759 139863663834880 logging_writer.py:48] [181800] global_step=181800, grad_norm=8.05180835723877, loss=1.839159369468689
I0127 12:46:46.838498 139862724310784 logging_writer.py:48] [181900] global_step=181900, grad_norm=7.870815753936768, loss=1.8074978590011597
I0127 12:47:20.717670 139863663834880 logging_writer.py:48] [182000] global_step=182000, grad_norm=8.233057022094727, loss=1.7866787910461426
I0127 12:47:54.558188 139862724310784 logging_writer.py:48] [182100] global_step=182100, grad_norm=8.365349769592285, loss=1.797536849975586
I0127 12:48:28.453403 139863663834880 logging_writer.py:48] [182200] global_step=182200, grad_norm=8.534713745117188, loss=1.8255189657211304
I0127 12:48:31.319201 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:48:37.423943 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:48:45.946937 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:48:48.223161 140027215431488 submission_runner.py:408] Time since start: 64017.19s, 	Step: 182210, 	{'train/accuracy': 0.9197026491165161, 'train/loss': 0.4040387272834778, 'validation/accuracy': 0.772159993648529, 'validation/loss': 0.9936366081237793, 'validation/num_examples': 50000, 'test/accuracy': 0.6494000554084778, 'test/loss': 1.6046305894851685, 'test/num_examples': 10000, 'score': 61772.02049660683, 'total_duration': 64017.193064928055, 'accumulated_submission_time': 61772.02049660683, 'accumulated_eval_time': 2232.815001964569, 'accumulated_logging_time': 6.070794105529785}
I0127 12:48:48.269091 139862724310784 logging_writer.py:48] [182210] accumulated_eval_time=2232.815002, accumulated_logging_time=6.070794, accumulated_submission_time=61772.020497, global_step=182210, preemption_count=0, score=61772.020497, test/accuracy=0.649400, test/loss=1.604631, test/num_examples=10000, total_duration=64017.193065, train/accuracy=0.919703, train/loss=0.404039, validation/accuracy=0.772160, validation/loss=0.993637, validation/num_examples=50000
I0127 12:49:19.058653 139863663834880 logging_writer.py:48] [182300] global_step=182300, grad_norm=7.480447769165039, loss=1.7724894285202026
I0127 12:49:52.937273 139862724310784 logging_writer.py:48] [182400] global_step=182400, grad_norm=7.919524669647217, loss=1.8243495225906372
I0127 12:50:26.797167 139863663834880 logging_writer.py:48] [182500] global_step=182500, grad_norm=7.731332778930664, loss=1.777350902557373
I0127 12:51:00.652589 139862724310784 logging_writer.py:48] [182600] global_step=182600, grad_norm=7.643359661102295, loss=1.794853687286377
I0127 12:51:34.514497 139863663834880 logging_writer.py:48] [182700] global_step=182700, grad_norm=7.625133991241455, loss=1.7997643947601318
I0127 12:52:08.382713 139862724310784 logging_writer.py:48] [182800] global_step=182800, grad_norm=8.56424617767334, loss=1.7670060396194458
I0127 12:52:42.234003 139863663834880 logging_writer.py:48] [182900] global_step=182900, grad_norm=7.577479362487793, loss=1.8074947595596313
I0127 12:53:16.097485 139862724310784 logging_writer.py:48] [183000] global_step=183000, grad_norm=8.065646171569824, loss=1.8212223052978516
I0127 12:53:49.987853 139863663834880 logging_writer.py:48] [183100] global_step=183100, grad_norm=7.387770652770996, loss=1.7865043878555298
I0127 12:54:23.894408 139862724310784 logging_writer.py:48] [183200] global_step=183200, grad_norm=7.284475803375244, loss=1.7077890634536743
I0127 12:54:57.781564 139863663834880 logging_writer.py:48] [183300] global_step=183300, grad_norm=8.151302337646484, loss=1.8459453582763672
I0127 12:55:31.640566 139862724310784 logging_writer.py:48] [183400] global_step=183400, grad_norm=8.102354049682617, loss=1.7896106243133545
I0127 12:56:05.473037 139863663834880 logging_writer.py:48] [183500] global_step=183500, grad_norm=8.019737243652344, loss=1.8002725839614868
I0127 12:56:39.326864 139862724310784 logging_writer.py:48] [183600] global_step=183600, grad_norm=8.3170747756958, loss=1.8920836448669434
I0127 12:57:13.208028 139863663834880 logging_writer.py:48] [183700] global_step=183700, grad_norm=7.503269195556641, loss=1.7136950492858887
I0127 12:57:18.422258 140027215431488 spec.py:321] Evaluating on the training split.
I0127 12:57:24.696129 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 12:57:33.033807 140027215431488 spec.py:349] Evaluating on the test split.
I0127 12:57:35.398603 140027215431488 submission_runner.py:408] Time since start: 64544.37s, 	Step: 183717, 	{'train/accuracy': 0.9208585619926453, 'train/loss': 0.3966313898563385, 'validation/accuracy': 0.7724399566650391, 'validation/loss': 0.9899981021881104, 'validation/num_examples': 50000, 'test/accuracy': 0.6511000394821167, 'test/loss': 1.6006096601486206, 'test/num_examples': 10000, 'score': 62282.11263537407, 'total_duration': 64544.36849927902, 'accumulated_submission_time': 62282.11263537407, 'accumulated_eval_time': 2249.791279554367, 'accumulated_logging_time': 6.127057075500488}
I0127 12:57:35.444777 139862715918080 logging_writer.py:48] [183717] accumulated_eval_time=2249.791280, accumulated_logging_time=6.127057, accumulated_submission_time=62282.112635, global_step=183717, preemption_count=0, score=62282.112635, test/accuracy=0.651100, test/loss=1.600610, test/num_examples=10000, total_duration=64544.368499, train/accuracy=0.920859, train/loss=0.396631, validation/accuracy=0.772440, validation/loss=0.989998, validation/num_examples=50000
I0127 12:58:03.921553 139862724310784 logging_writer.py:48] [183800] global_step=183800, grad_norm=8.217607498168945, loss=1.7550970315933228
I0127 12:58:37.799949 139862715918080 logging_writer.py:48] [183900] global_step=183900, grad_norm=7.719407558441162, loss=1.8332043886184692
I0127 12:59:11.644637 139862724310784 logging_writer.py:48] [184000] global_step=184000, grad_norm=7.594435214996338, loss=1.8306394815444946
I0127 12:59:45.510876 139862715918080 logging_writer.py:48] [184100] global_step=184100, grad_norm=7.798542499542236, loss=1.8137463331222534
I0127 13:00:19.355294 139862724310784 logging_writer.py:48] [184200] global_step=184200, grad_norm=8.577703475952148, loss=1.8193433284759521
I0127 13:00:53.284111 139862715918080 logging_writer.py:48] [184300] global_step=184300, grad_norm=7.531520366668701, loss=1.7821221351623535
I0127 13:01:27.141020 139862724310784 logging_writer.py:48] [184400] global_step=184400, grad_norm=7.495146751403809, loss=1.7679544687271118
I0127 13:02:01.034124 139862715918080 logging_writer.py:48] [184500] global_step=184500, grad_norm=7.863758087158203, loss=1.8295626640319824
I0127 13:02:34.883081 139862724310784 logging_writer.py:48] [184600] global_step=184600, grad_norm=7.4417595863342285, loss=1.7203369140625
I0127 13:03:08.768395 139862715918080 logging_writer.py:48] [184700] global_step=184700, grad_norm=7.894876956939697, loss=1.7873209714889526
I0127 13:03:42.633413 139862724310784 logging_writer.py:48] [184800] global_step=184800, grad_norm=7.131530284881592, loss=1.7462937831878662
I0127 13:04:16.488207 139862715918080 logging_writer.py:48] [184900] global_step=184900, grad_norm=7.615345001220703, loss=1.7752093076705933
I0127 13:04:50.370860 139862724310784 logging_writer.py:48] [185000] global_step=185000, grad_norm=8.115126609802246, loss=1.780961036682129
I0127 13:05:24.246595 139862715918080 logging_writer.py:48] [185100] global_step=185100, grad_norm=8.081978797912598, loss=1.7970011234283447
I0127 13:05:58.102513 139862724310784 logging_writer.py:48] [185200] global_step=185200, grad_norm=8.113615036010742, loss=1.7922921180725098
I0127 13:06:05.698044 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:06:11.889104 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:06:20.504840 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:06:22.847347 140027215431488 submission_runner.py:408] Time since start: 65071.82s, 	Step: 185224, 	{'train/accuracy': 0.9212771058082581, 'train/loss': 0.39945515990257263, 'validation/accuracy': 0.7727199792861938, 'validation/loss': 0.9911830425262451, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6002691984176636, 'test/num_examples': 10000, 'score': 62792.30425167084, 'total_duration': 65071.81726360321, 'accumulated_submission_time': 62792.30425167084, 'accumulated_eval_time': 2266.940537214279, 'accumulated_logging_time': 6.183432102203369}
I0127 13:06:22.900490 139863663834880 logging_writer.py:48] [185224] accumulated_eval_time=2266.940537, accumulated_logging_time=6.183432, accumulated_submission_time=62792.304252, global_step=185224, preemption_count=0, score=62792.304252, test/accuracy=0.650900, test/loss=1.600269, test/num_examples=10000, total_duration=65071.817264, train/accuracy=0.921277, train/loss=0.399455, validation/accuracy=0.772720, validation/loss=0.991183, validation/num_examples=50000
I0127 13:06:49.113038 139865760950016 logging_writer.py:48] [185300] global_step=185300, grad_norm=7.283963680267334, loss=1.786447525024414
I0127 13:07:23.006582 139863663834880 logging_writer.py:48] [185400] global_step=185400, grad_norm=7.767775058746338, loss=1.7679494619369507
I0127 13:07:56.861699 139865760950016 logging_writer.py:48] [185500] global_step=185500, grad_norm=8.151037216186523, loss=1.7579220533370972
I0127 13:08:30.725343 139863663834880 logging_writer.py:48] [185600] global_step=185600, grad_norm=8.176569938659668, loss=1.8417694568634033
I0127 13:09:04.582400 139865760950016 logging_writer.py:48] [185700] global_step=185700, grad_norm=7.639308452606201, loss=1.7608351707458496
I0127 13:09:38.466484 139863663834880 logging_writer.py:48] [185800] global_step=185800, grad_norm=7.5410308837890625, loss=1.7681902647018433
I0127 13:09:58.976550 139865760950016 logging_writer.py:48] [185862] global_step=185862, preemption_count=0, score=63008.314156
I0127 13:09:59.493633 140027215431488 checkpoints.py:490] Saving checkpoint at step: 185862
I0127 13:10:00.792264 140027215431488 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1/checkpoint_185862
I0127 13:10:00.821012 140027215431488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_1/checkpoint_185862.
I0127 13:10:01.728728 140027215431488 submission_runner.py:583] Tuning trial 1/5
I0127 13:10:01.729003 140027215431488 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0127 13:10:01.733523 140027215431488 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007573341717943549, 'train/loss': 6.911828994750977, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 55.74225401878357, 'total_duration': 93.10303163528442, 'accumulated_submission_time': 55.74225401878357, 'accumulated_eval_time': 37.360684394836426, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1500, {'train/accuracy': 0.07481664419174194, 'train/loss': 5.330939769744873, 'validation/accuracy': 0.06864000111818314, 'validation/loss': 5.407910346984863, 'validation/num_examples': 50000, 'test/accuracy': 0.048100002110004425, 'test/loss': 5.63258171081543, 'test/num_examples': 10000, 'score': 565.8360676765442, 'total_duration': 621.1860675811768, 'accumulated_submission_time': 565.8360676765442, 'accumulated_eval_time': 55.27424716949463, 'accumulated_logging_time': 0.026404380798339844, 'global_step': 1500, 'preemption_count': 0}), (2997, {'train/accuracy': 0.18203921616077423, 'train/loss': 4.255775451660156, 'validation/accuracy': 0.16411998867988586, 'validation/loss': 4.366084098815918, 'validation/num_examples': 50000, 'test/accuracy': 0.12210000306367874, 'test/loss': 4.7546820640563965, 'test/num_examples': 10000, 'score': 1075.7997715473175, 'total_duration': 1149.57959151268, 'accumulated_submission_time': 1075.7997715473175, 'accumulated_eval_time': 73.62649869918823, 'accumulated_logging_time': 0.052556514739990234, 'global_step': 2997, 'preemption_count': 0}), (4495, {'train/accuracy': 0.28310346603393555, 'train/loss': 3.511129856109619, 'validation/accuracy': 0.25633999705314636, 'validation/loss': 3.6584579944610596, 'validation/num_examples': 50000, 'test/accuracy': 0.18560001254081726, 'test/loss': 4.202426910400391, 'test/num_examples': 10000, 'score': 1585.9234120845795, 'total_duration': 1677.6729209423065, 'accumulated_submission_time': 1585.9234120845795, 'accumulated_eval_time': 91.51947140693665, 'accumulated_logging_time': 0.07996487617492676, 'global_step': 4495, 'preemption_count': 0}), (5993, {'train/accuracy': 0.37711256742477417, 'train/loss': 2.901236057281494, 'validation/accuracy': 0.3469599783420563, 'validation/loss': 3.06270432472229, 'validation/num_examples': 50000, 'test/accuracy': 0.25780001282691956, 'test/loss': 3.6963891983032227, 'test/num_examples': 10000, 'score': 2095.9836592674255, 'total_duration': 2205.7152502536774, 'accumulated_submission_time': 2095.9836592674255, 'accumulated_eval_time': 109.42569613456726, 'accumulated_logging_time': 0.10534834861755371, 'global_step': 5993, 'preemption_count': 0}), (7491, {'train/accuracy': 0.437220960855484, 'train/loss': 2.591935873031616, 'validation/accuracy': 0.4038199782371521, 'validation/loss': 2.7630527019500732, 'validation/num_examples': 50000, 'test/accuracy': 0.3150000274181366, 'test/loss': 3.4180727005004883, 'test/num_examples': 10000, 'score': 2605.98605966568, 'total_duration': 2733.749913215637, 'accumulated_submission_time': 2605.98605966568, 'accumulated_eval_time': 127.3788537979126, 'accumulated_logging_time': 0.13325881958007812, 'global_step': 7491, 'preemption_count': 0}), (8990, {'train/accuracy': 0.4954759180545807, 'train/loss': 2.285950183868408, 'validation/accuracy': 0.4610599875450134, 'validation/loss': 2.458083391189575, 'validation/num_examples': 50000, 'test/accuracy': 0.35450002551078796, 'test/loss': 3.1170003414154053, 'test/num_examples': 10000, 'score': 3116.0023124217987, 'total_duration': 3262.0570573806763, 'accumulated_submission_time': 3116.0023124217987, 'accumulated_eval_time': 145.59150886535645, 'accumulated_logging_time': 0.16101622581481934, 'global_step': 8990, 'preemption_count': 0}), (10489, {'train/accuracy': 0.5600087642669678, 'train/loss': 1.9418503046035767, 'validation/accuracy': 0.49837997555732727, 'validation/loss': 2.2500064373016357, 'validation/num_examples': 50000, 'test/accuracy': 0.3785000145435333, 'test/loss': 2.9464919567108154, 'test/num_examples': 10000, 'score': 3626.0225105285645, 'total_duration': 3794.081609249115, 'accumulated_submission_time': 3626.0225105285645, 'accumulated_eval_time': 167.5162229537964, 'accumulated_logging_time': 0.18806004524230957, 'global_step': 10489, 'preemption_count': 0}), (11990, {'train/accuracy': 0.5775271058082581, 'train/loss': 1.8790894746780396, 'validation/accuracy': 0.5301399827003479, 'validation/loss': 2.1053919792175293, 'validation/num_examples': 50000, 'test/accuracy': 0.406900018453598, 'test/loss': 2.8000824451446533, 'test/num_examples': 10000, 'score': 4136.248651504517, 'total_duration': 4322.467486619949, 'accumulated_submission_time': 4136.248651504517, 'accumulated_eval_time': 185.59751963615417, 'accumulated_logging_time': 0.21561455726623535, 'global_step': 11990, 'preemption_count': 0}), (13491, {'train/accuracy': 0.5838648080825806, 'train/loss': 1.8349441289901733, 'validation/accuracy': 0.5408999919891357, 'validation/loss': 2.0403449535369873, 'validation/num_examples': 50000, 'test/accuracy': 0.42420002818107605, 'test/loss': 2.7253708839416504, 'test/num_examples': 10000, 'score': 4646.346125364304, 'total_duration': 4850.625118970871, 'accumulated_submission_time': 4646.346125364304, 'accumulated_eval_time': 203.5793845653534, 'accumulated_logging_time': 0.24520635604858398, 'global_step': 13491, 'preemption_count': 0}), (14992, {'train/accuracy': 0.5969586968421936, 'train/loss': 1.7865535020828247, 'validation/accuracy': 0.5550999641418457, 'validation/loss': 1.9808624982833862, 'validation/num_examples': 50000, 'test/accuracy': 0.43540000915527344, 'test/loss': 2.653869152069092, 'test/num_examples': 10000, 'score': 5156.277529478073, 'total_duration': 5378.708662033081, 'accumulated_submission_time': 5156.277529478073, 'accumulated_eval_time': 221.64778184890747, 'accumulated_logging_time': 0.2781078815460205, 'global_step': 14992, 'preemption_count': 0}), (16495, {'train/accuracy': 0.5969586968421936, 'train/loss': 1.7959184646606445, 'validation/accuracy': 0.5592600107192993, 'validation/loss': 1.978746771812439, 'validation/num_examples': 50000, 'test/accuracy': 0.4358000159263611, 'test/loss': 2.657799005508423, 'test/num_examples': 10000, 'score': 5666.510581970215, 'total_duration': 5907.21281003952, 'accumulated_submission_time': 5666.510581970215, 'accumulated_eval_time': 239.83745956420898, 'accumulated_logging_time': 0.3100862503051758, 'global_step': 16495, 'preemption_count': 0}), (17998, {'train/accuracy': 0.5971181392669678, 'train/loss': 1.7574492692947388, 'validation/accuracy': 0.5580799579620361, 'validation/loss': 1.9484416246414185, 'validation/num_examples': 50000, 'test/accuracy': 0.4360000193119049, 'test/loss': 2.6286208629608154, 'test/num_examples': 10000, 'score': 6176.487642765045, 'total_duration': 6436.447620630264, 'accumulated_submission_time': 6176.487642765045, 'accumulated_eval_time': 259.01236724853516, 'accumulated_logging_time': 0.342235803604126, 'global_step': 17998, 'preemption_count': 0}), (19501, {'train/accuracy': 0.6540975570678711, 'train/loss': 1.5059301853179932, 'validation/accuracy': 0.5709599852561951, 'validation/loss': 1.8825407028198242, 'validation/num_examples': 50000, 'test/accuracy': 0.44700002670288086, 'test/loss': 2.5772900581359863, 'test/num_examples': 10000, 'score': 6686.56877040863, 'total_duration': 6965.261632204056, 'accumulated_submission_time': 6686.56877040863, 'accumulated_eval_time': 277.65295243263245, 'accumulated_logging_time': 0.3832590579986572, 'global_step': 19501, 'preemption_count': 0}), (21004, {'train/accuracy': 0.6272919178009033, 'train/loss': 1.6106610298156738, 'validation/accuracy': 0.5689799785614014, 'validation/loss': 1.8841341733932495, 'validation/num_examples': 50000, 'test/accuracy': 0.4496000111103058, 'test/loss': 2.5562584400177, 'test/num_examples': 10000, 'score': 7196.738107442856, 'total_duration': 7497.752900362015, 'accumulated_submission_time': 7196.738107442856, 'accumulated_eval_time': 299.892765045166, 'accumulated_logging_time': 0.415421724319458, 'global_step': 21004, 'preemption_count': 0}), (22508, {'train/accuracy': 0.6285873651504517, 'train/loss': 1.606850266456604, 'validation/accuracy': 0.5818799734115601, 'validation/loss': 1.833842158317566, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4872632026672363, 'test/num_examples': 10000, 'score': 7706.86282658577, 'total_duration': 8029.213074207306, 'accumulated_submission_time': 7706.86282658577, 'accumulated_eval_time': 321.13594365119934, 'accumulated_logging_time': 0.4556243419647217, 'global_step': 22508, 'preemption_count': 0}), (24012, {'train/accuracy': 0.6264548897743225, 'train/loss': 1.6274893283843994, 'validation/accuracy': 0.5815799832344055, 'validation/loss': 1.8446393013000488, 'validation/num_examples': 50000, 'test/accuracy': 0.4627000093460083, 'test/loss': 2.5038511753082275, 'test/num_examples': 10000, 'score': 8216.786780834198, 'total_duration': 8561.249808549881, 'accumulated_submission_time': 8216.786780834198, 'accumulated_eval_time': 343.1451985836029, 'accumulated_logging_time': 0.5067691802978516, 'global_step': 24012, 'preemption_count': 0}), (25516, {'train/accuracy': 0.631277859210968, 'train/loss': 1.6501520872116089, 'validation/accuracy': 0.5895199775695801, 'validation/loss': 1.8403079509735107, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.4981985092163086, 'test/num_examples': 10000, 'score': 8726.874703884125, 'total_duration': 9093.15644454956, 'accumulated_submission_time': 8726.874703884125, 'accumulated_eval_time': 364.8770282268524, 'accumulated_logging_time': 0.5437021255493164, 'global_step': 25516, 'preemption_count': 0}), (27021, {'train/accuracy': 0.6287069320678711, 'train/loss': 1.6057114601135254, 'validation/accuracy': 0.5907599925994873, 'validation/loss': 1.7863342761993408, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.4951579570770264, 'test/num_examples': 10000, 'score': 9237.083882570267, 'total_duration': 9625.399262428284, 'accumulated_submission_time': 9237.083882570267, 'accumulated_eval_time': 386.82509112358093, 'accumulated_logging_time': 0.5786194801330566, 'global_step': 27021, 'preemption_count': 0}), (28524, {'train/accuracy': 0.683992326259613, 'train/loss': 1.3645371198654175, 'validation/accuracy': 0.597000002861023, 'validation/loss': 1.757430911064148, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.4163501262664795, 'test/num_examples': 10000, 'score': 9746.664439201355, 'total_duration': 10157.818138360977, 'accumulated_submission_time': 9746.664439201355, 'accumulated_eval_time': 409.05973839759827, 'accumulated_logging_time': 1.1308567523956299, 'global_step': 28524, 'preemption_count': 0}), (30028, {'train/accuracy': 0.6511877775192261, 'train/loss': 1.5284302234649658, 'validation/accuracy': 0.5967199802398682, 'validation/loss': 1.7889021635055542, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.4623258113861084, 'test/num_examples': 10000, 'score': 10256.616863250732, 'total_duration': 10690.812278747559, 'accumulated_submission_time': 10256.616863250732, 'accumulated_eval_time': 432.0158112049103, 'accumulated_logging_time': 1.1649293899536133, 'global_step': 30028, 'preemption_count': 0}), (31532, {'train/accuracy': 0.6303810477256775, 'train/loss': 1.6111574172973633, 'validation/accuracy': 0.5847600102424622, 'validation/loss': 1.82930588722229, 'validation/num_examples': 50000, 'test/accuracy': 0.4620000123977661, 'test/loss': 2.532831907272339, 'test/num_examples': 10000, 'score': 10766.595863342285, 'total_duration': 11225.963517665863, 'accumulated_submission_time': 10766.595863342285, 'accumulated_eval_time': 457.0925896167755, 'accumulated_logging_time': 1.210730791091919, 'global_step': 31532, 'preemption_count': 0}), (33037, {'train/accuracy': 0.649832546710968, 'train/loss': 1.4842784404754639, 'validation/accuracy': 0.6013000011444092, 'validation/loss': 1.7228082418441772, 'validation/num_examples': 50000, 'test/accuracy': 0.48250001668930054, 'test/loss': 2.3736181259155273, 'test/num_examples': 10000, 'score': 11276.758833408356, 'total_duration': 11759.353006362915, 'accumulated_submission_time': 11276.758833408356, 'accumulated_eval_time': 480.23394536972046, 'accumulated_logging_time': 1.2460718154907227, 'global_step': 33037, 'preemption_count': 0}), (34542, {'train/accuracy': 0.6471221446990967, 'train/loss': 1.524101734161377, 'validation/accuracy': 0.602180004119873, 'validation/loss': 1.7275532484054565, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3868050575256348, 'test/num_examples': 10000, 'score': 11786.717982053757, 'total_duration': 12292.46637749672, 'accumulated_submission_time': 11786.717982053757, 'accumulated_eval_time': 503.3027272224426, 'accumulated_logging_time': 1.2807552814483643, 'global_step': 34542, 'preemption_count': 0}), (36047, {'train/accuracy': 0.6511877775192261, 'train/loss': 1.5404627323150635, 'validation/accuracy': 0.6087799668312073, 'validation/loss': 1.7445299625396729, 'validation/num_examples': 50000, 'test/accuracy': 0.48510003089904785, 'test/loss': 2.3932998180389404, 'test/num_examples': 10000, 'score': 12296.627568244934, 'total_duration': 12825.880003213882, 'accumulated_submission_time': 12296.627568244934, 'accumulated_eval_time': 526.7157201766968, 'accumulated_logging_time': 1.3211784362792969, 'global_step': 36047, 'preemption_count': 0}), (37552, {'train/accuracy': 0.6801658272743225, 'train/loss': 1.3729093074798584, 'validation/accuracy': 0.5968199968338013, 'validation/loss': 1.7451149225234985, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.4134533405303955, 'test/num_examples': 10000, 'score': 12806.58596277237, 'total_duration': 13357.748576164246, 'accumulated_submission_time': 12806.58596277237, 'accumulated_eval_time': 548.5404841899872, 'accumulated_logging_time': 1.3562448024749756, 'global_step': 37552, 'preemption_count': 0}), (39057, {'train/accuracy': 0.6672711968421936, 'train/loss': 1.4364635944366455, 'validation/accuracy': 0.604640007019043, 'validation/loss': 1.7295138835906982, 'validation/num_examples': 50000, 'test/accuracy': 0.48180001974105835, 'test/loss': 2.3832080364227295, 'test/num_examples': 10000, 'score': 13316.641247034073, 'total_duration': 13888.769673347473, 'accumulated_submission_time': 13316.641247034073, 'accumulated_eval_time': 569.4229211807251, 'accumulated_logging_time': 1.3880341053009033, 'global_step': 39057, 'preemption_count': 0}), (40562, {'train/accuracy': 0.6653180718421936, 'train/loss': 1.4576128721237183, 'validation/accuracy': 0.6128999590873718, 'validation/loss': 1.690018653869629, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.349595308303833, 'test/num_examples': 10000, 'score': 13826.662028312683, 'total_duration': 14419.336757183075, 'accumulated_submission_time': 13826.662028312683, 'accumulated_eval_time': 589.8896474838257, 'accumulated_logging_time': 1.4172828197479248, 'global_step': 40562, 'preemption_count': 0}), (42067, {'train/accuracy': 0.6569873690605164, 'train/loss': 1.500503659248352, 'validation/accuracy': 0.608460009098053, 'validation/loss': 1.7169100046157837, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.381596803665161, 'test/num_examples': 10000, 'score': 14336.611745595932, 'total_duration': 14949.84656405449, 'accumulated_submission_time': 14336.611745595932, 'accumulated_eval_time': 610.3627202510834, 'accumulated_logging_time': 1.4521598815917969, 'global_step': 42067, 'preemption_count': 0}), (43573, {'train/accuracy': 0.6497528553009033, 'train/loss': 1.5108873844146729, 'validation/accuracy': 0.6085599660873413, 'validation/loss': 1.701366662979126, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.3547487258911133, 'test/num_examples': 10000, 'score': 14846.805841684341, 'total_duration': 15482.147728919983, 'accumulated_submission_time': 14846.805841684341, 'accumulated_eval_time': 632.3841454982758, 'accumulated_logging_time': 1.4866185188293457, 'global_step': 43573, 'preemption_count': 0}), (45079, {'train/accuracy': 0.6457469463348389, 'train/loss': 1.5189896821975708, 'validation/accuracy': 0.6038999557495117, 'validation/loss': 1.7213490009307861, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.438140869140625, 'test/num_examples': 10000, 'score': 15357.030562639236, 'total_duration': 16011.586278438568, 'accumulated_submission_time': 15357.030562639236, 'accumulated_eval_time': 651.5147202014923, 'accumulated_logging_time': 1.5189547538757324, 'global_step': 45079, 'preemption_count': 0}), (46585, {'train/accuracy': 0.6690250039100647, 'train/loss': 1.4443752765655518, 'validation/accuracy': 0.603119969367981, 'validation/loss': 1.738900899887085, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.3925979137420654, 'test/num_examples': 10000, 'score': 15867.12254691124, 'total_duration': 16542.223130464554, 'accumulated_submission_time': 15867.12254691124, 'accumulated_eval_time': 671.9697597026825, 'accumulated_logging_time': 1.5562007427215576, 'global_step': 46585, 'preemption_count': 0}), (48091, {'train/accuracy': 0.6783920526504517, 'train/loss': 1.3911480903625488, 'validation/accuracy': 0.6177600026130676, 'validation/loss': 1.6825976371765137, 'validation/num_examples': 50000, 'test/accuracy': 0.49320003390312195, 'test/loss': 2.372044324874878, 'test/num_examples': 10000, 'score': 16377.347776174545, 'total_duration': 17072.506830453873, 'accumulated_submission_time': 16377.347776174545, 'accumulated_eval_time': 691.9411878585815, 'accumulated_logging_time': 1.5927386283874512, 'global_step': 48091, 'preemption_count': 0}), (49597, {'train/accuracy': 0.6721141338348389, 'train/loss': 1.4001306295394897, 'validation/accuracy': 0.6206600069999695, 'validation/loss': 1.6346431970596313, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.282940149307251, 'test/num_examples': 10000, 'score': 16887.48326063156, 'total_duration': 17605.0581843853, 'accumulated_submission_time': 16887.48326063156, 'accumulated_eval_time': 714.2665731906891, 'accumulated_logging_time': 1.6319713592529297, 'global_step': 49597, 'preemption_count': 0}), (51104, {'train/accuracy': 0.6506895422935486, 'train/loss': 1.5444890260696411, 'validation/accuracy': 0.6065199971199036, 'validation/loss': 1.7607401609420776, 'validation/num_examples': 50000, 'test/accuracy': 0.48190003633499146, 'test/loss': 2.446437358856201, 'test/num_examples': 10000, 'score': 17397.699914216995, 'total_duration': 18136.358857870102, 'accumulated_submission_time': 17397.699914216995, 'accumulated_eval_time': 735.2593734264374, 'accumulated_logging_time': 1.6691064834594727, 'global_step': 51104, 'preemption_count': 0}), (52610, {'train/accuracy': 0.6633649468421936, 'train/loss': 1.458517074584961, 'validation/accuracy': 0.6189799904823303, 'validation/loss': 1.6728732585906982, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.3692007064819336, 'test/num_examples': 10000, 'score': 17907.79274368286, 'total_duration': 18665.110434770584, 'accumulated_submission_time': 17907.79274368286, 'accumulated_eval_time': 753.8350718021393, 'accumulated_logging_time': 1.702005386352539, 'global_step': 52610, 'preemption_count': 0}), (54117, {'train/accuracy': 0.652363657951355, 'train/loss': 1.5211600065231323, 'validation/accuracy': 0.6086999773979187, 'validation/loss': 1.7170997858047485, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.394115447998047, 'test/num_examples': 10000, 'score': 18417.992211580276, 'total_duration': 19192.784980773926, 'accumulated_submission_time': 18417.992211580276, 'accumulated_eval_time': 771.2206614017487, 'accumulated_logging_time': 1.7383487224578857, 'global_step': 54117, 'preemption_count': 0}), (55623, {'train/accuracy': 0.6726123690605164, 'train/loss': 1.4373608827590942, 'validation/accuracy': 0.6170600056648254, 'validation/loss': 1.6829478740692139, 'validation/num_examples': 50000, 'test/accuracy': 0.4896000325679779, 'test/loss': 2.3452506065368652, 'test/num_examples': 10000, 'score': 18927.954924583435, 'total_duration': 19720.387036561966, 'accumulated_submission_time': 18927.954924583435, 'accumulated_eval_time': 788.768620967865, 'accumulated_logging_time': 1.7768800258636475, 'global_step': 55623, 'preemption_count': 0}), (57129, {'train/accuracy': 0.6888552308082581, 'train/loss': 1.3610378503799438, 'validation/accuracy': 0.6221599578857422, 'validation/loss': 1.6592100858688354, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.3328607082366943, 'test/num_examples': 10000, 'score': 19438.130825281143, 'total_duration': 20247.76454782486, 'accumulated_submission_time': 19438.130825281143, 'accumulated_eval_time': 805.8777091503143, 'accumulated_logging_time': 1.8174619674682617, 'global_step': 57129, 'preemption_count': 0}), (58636, {'train/accuracy': 0.6800262928009033, 'train/loss': 1.3760676383972168, 'validation/accuracy': 0.620959997177124, 'validation/loss': 1.6456332206726074, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.325051784515381, 'test/num_examples': 10000, 'score': 19948.24530482292, 'total_duration': 20775.291144371033, 'accumulated_submission_time': 19948.24530482292, 'accumulated_eval_time': 823.1925563812256, 'accumulated_logging_time': 1.8636653423309326, 'global_step': 58636, 'preemption_count': 0}), (60143, {'train/accuracy': 0.6727718114852905, 'train/loss': 1.4089183807373047, 'validation/accuracy': 0.6265400052070618, 'validation/loss': 1.6337388753890991, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.290309190750122, 'test/num_examples': 10000, 'score': 20458.460973262787, 'total_duration': 21302.928270578384, 'accumulated_submission_time': 20458.460973262787, 'accumulated_eval_time': 840.5147912502289, 'accumulated_logging_time': 1.9102842807769775, 'global_step': 60143, 'preemption_count': 0}), (61651, {'train/accuracy': 0.6753029227256775, 'train/loss': 1.3835617303848267, 'validation/accuracy': 0.6267799735069275, 'validation/loss': 1.610917329788208, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.281240224838257, 'test/num_examples': 10000, 'score': 20968.66392183304, 'total_duration': 21831.363361120224, 'accumulated_submission_time': 20968.66392183304, 'accumulated_eval_time': 858.6574947834015, 'accumulated_logging_time': 1.9475953578948975, 'global_step': 61651, 'preemption_count': 0}), (63158, {'train/accuracy': 0.6793287396430969, 'train/loss': 1.402336597442627, 'validation/accuracy': 0.631060004234314, 'validation/loss': 1.6216193437576294, 'validation/num_examples': 50000, 'test/accuracy': 0.5006000399589539, 'test/loss': 2.285511016845703, 'test/num_examples': 10000, 'score': 21478.79666543007, 'total_duration': 22359.277554273605, 'accumulated_submission_time': 21478.79666543007, 'accumulated_eval_time': 876.3508095741272, 'accumulated_logging_time': 1.9852290153503418, 'global_step': 63158, 'preemption_count': 0}), (64665, {'train/accuracy': 0.6847297549247742, 'train/loss': 1.3605037927627563, 'validation/accuracy': 0.6293999552726746, 'validation/loss': 1.600814700126648, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2789101600646973, 'test/num_examples': 10000, 'score': 21988.747662067413, 'total_duration': 22887.225957870483, 'accumulated_submission_time': 21988.747662067413, 'accumulated_eval_time': 894.2573552131653, 'accumulated_logging_time': 2.025351047515869, 'global_step': 64665, 'preemption_count': 0}), (66172, {'train/accuracy': 0.7006337642669678, 'train/loss': 1.267736554145813, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.588563084602356, 'validation/num_examples': 50000, 'test/accuracy': 0.5067000389099121, 'test/loss': 2.267800807952881, 'test/num_examples': 10000, 'score': 22498.691545009613, 'total_duration': 23414.779324054718, 'accumulated_submission_time': 22498.691545009613, 'accumulated_eval_time': 911.7745883464813, 'accumulated_logging_time': 2.06522798538208, 'global_step': 66172, 'preemption_count': 0}), (67679, {'train/accuracy': 0.6891342401504517, 'train/loss': 1.355064868927002, 'validation/accuracy': 0.6316999793052673, 'validation/loss': 1.623766541481018, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.2622320652008057, 'test/num_examples': 10000, 'score': 23008.63331103325, 'total_duration': 23942.103929281235, 'accumulated_submission_time': 23008.63331103325, 'accumulated_eval_time': 929.0643086433411, 'accumulated_logging_time': 2.106299638748169, 'global_step': 67679, 'preemption_count': 0}), (69186, {'train/accuracy': 0.6904296875, 'train/loss': 1.3263139724731445, 'validation/accuracy': 0.6378600001335144, 'validation/loss': 1.573560118675232, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.2438392639160156, 'test/num_examples': 10000, 'score': 23518.742659330368, 'total_duration': 24469.36714053154, 'accumulated_submission_time': 23518.742659330368, 'accumulated_eval_time': 946.1260778903961, 'accumulated_logging_time': 2.1450648307800293, 'global_step': 69186, 'preemption_count': 0}), (70693, {'train/accuracy': 0.6878587007522583, 'train/loss': 1.3525885343551636, 'validation/accuracy': 0.6403399705886841, 'validation/loss': 1.573731780052185, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2697505950927734, 'test/num_examples': 10000, 'score': 24028.91832447052, 'total_duration': 24997.529990196228, 'accumulated_submission_time': 24028.91832447052, 'accumulated_eval_time': 964.0205476284027, 'accumulated_logging_time': 2.1862633228302, 'global_step': 70693, 'preemption_count': 0}), (72200, {'train/accuracy': 0.6857063174247742, 'train/loss': 1.3495975732803345, 'validation/accuracy': 0.6386199593544006, 'validation/loss': 1.5710564851760864, 'validation/num_examples': 50000, 'test/accuracy': 0.5123000144958496, 'test/loss': 2.2388885021209717, 'test/num_examples': 10000, 'score': 24538.978356838226, 'total_duration': 25524.813975811005, 'accumulated_submission_time': 24538.978356838226, 'accumulated_eval_time': 981.154138803482, 'accumulated_logging_time': 2.2252821922302246, 'global_step': 72200, 'preemption_count': 0}), (73707, {'train/accuracy': 0.6907086968421936, 'train/loss': 1.3185800313949585, 'validation/accuracy': 0.6348400115966797, 'validation/loss': 1.5660574436187744, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.202910900115967, 'test/num_examples': 10000, 'score': 25049.043762922287, 'total_duration': 26051.94483280182, 'accumulated_submission_time': 25049.043762922287, 'accumulated_eval_time': 998.1271076202393, 'accumulated_logging_time': 2.2647223472595215, 'global_step': 73707, 'preemption_count': 0}), (75215, {'train/accuracy': 0.7098612785339355, 'train/loss': 1.2410625219345093, 'validation/accuracy': 0.6350199580192566, 'validation/loss': 1.5747349262237549, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.2694790363311768, 'test/num_examples': 10000, 'score': 25559.250710248947, 'total_duration': 26580.20965051651, 'accumulated_submission_time': 25559.250710248947, 'accumulated_eval_time': 1016.0934247970581, 'accumulated_logging_time': 2.3037054538726807, 'global_step': 75215, 'preemption_count': 0}), (76722, {'train/accuracy': 0.7016701102256775, 'train/loss': 1.2833776473999023, 'validation/accuracy': 0.6454399824142456, 'validation/loss': 1.553180456161499, 'validation/num_examples': 50000, 'test/accuracy': 0.5156000256538391, 'test/loss': 2.2022013664245605, 'test/num_examples': 10000, 'score': 26069.155710458755, 'total_duration': 27107.455953359604, 'accumulated_submission_time': 26069.155710458755, 'accumulated_eval_time': 1033.3393914699554, 'accumulated_logging_time': 2.347717046737671, 'global_step': 76722, 'preemption_count': 0}), (78229, {'train/accuracy': 0.7018494606018066, 'train/loss': 1.2792024612426758, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.5287810564041138, 'validation/num_examples': 50000, 'test/accuracy': 0.5181000232696533, 'test/loss': 2.1945981979370117, 'test/num_examples': 10000, 'score': 26579.27547645569, 'total_duration': 27634.679057359695, 'accumulated_submission_time': 26579.27547645569, 'accumulated_eval_time': 1050.3499314785004, 'accumulated_logging_time': 2.3884060382843018, 'global_step': 78229, 'preemption_count': 0}), (79736, {'train/accuracy': 0.702168345451355, 'train/loss': 1.2795171737670898, 'validation/accuracy': 0.6496399641036987, 'validation/loss': 1.5087718963623047, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.1807165145874023, 'test/num_examples': 10000, 'score': 27089.258211374283, 'total_duration': 28161.826422214508, 'accumulated_submission_time': 27089.258211374283, 'accumulated_eval_time': 1067.4200563430786, 'accumulated_logging_time': 2.4313290119171143, 'global_step': 79736, 'preemption_count': 0}), (81243, {'train/accuracy': 0.6947743892669678, 'train/loss': 1.3432766199111938, 'validation/accuracy': 0.6473199725151062, 'validation/loss': 1.5561820268630981, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.222846031188965, 'test/num_examples': 10000, 'score': 27599.380972385406, 'total_duration': 28689.146060228348, 'accumulated_submission_time': 27599.380972385406, 'accumulated_eval_time': 1084.517686367035, 'accumulated_logging_time': 2.4775948524475098, 'global_step': 81243, 'preemption_count': 0}), (82750, {'train/accuracy': 0.6988998651504517, 'train/loss': 1.2671300172805786, 'validation/accuracy': 0.6509999632835388, 'validation/loss': 1.4982835054397583, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.15671443939209, 'test/num_examples': 10000, 'score': 28109.40303182602, 'total_duration': 29216.481212615967, 'accumulated_submission_time': 28109.40303182602, 'accumulated_eval_time': 1101.7360954284668, 'accumulated_logging_time': 2.5183119773864746, 'global_step': 82750, 'preemption_count': 0}), (84258, {'train/accuracy': 0.7250677347183228, 'train/loss': 1.181086778640747, 'validation/accuracy': 0.653499960899353, 'validation/loss': 1.501522421836853, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.192289113998413, 'test/num_examples': 10000, 'score': 28619.604907035828, 'total_duration': 29744.09475159645, 'accumulated_submission_time': 28619.604907035828, 'accumulated_eval_time': 1119.0488619804382, 'accumulated_logging_time': 2.564819812774658, 'global_step': 84258, 'preemption_count': 0}), (85765, {'train/accuracy': 0.7072703838348389, 'train/loss': 1.2629165649414062, 'validation/accuracy': 0.644540011882782, 'validation/loss': 1.5403581857681274, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.1950738430023193, 'test/num_examples': 10000, 'score': 29129.81632399559, 'total_duration': 30271.58340406418, 'accumulated_submission_time': 29129.81632399559, 'accumulated_eval_time': 1136.2286508083344, 'accumulated_logging_time': 2.6103806495666504, 'global_step': 85765, 'preemption_count': 0}), (87272, {'train/accuracy': 0.7074697017669678, 'train/loss': 1.2444090843200684, 'validation/accuracy': 0.6504600048065186, 'validation/loss': 1.5148764848709106, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.167389154434204, 'test/num_examples': 10000, 'score': 29639.82198214531, 'total_duration': 30799.211097955704, 'accumulated_submission_time': 29639.82198214531, 'accumulated_eval_time': 1153.7549715042114, 'accumulated_logging_time': 2.653380870819092, 'global_step': 87272, 'preemption_count': 0}), (88780, {'train/accuracy': 0.702566921710968, 'train/loss': 1.2763031721115112, 'validation/accuracy': 0.6502799987792969, 'validation/loss': 1.5065932273864746, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.174659013748169, 'test/num_examples': 10000, 'score': 30149.93711233139, 'total_duration': 31326.533321619034, 'accumulated_submission_time': 30149.93711233139, 'accumulated_eval_time': 1170.8653423786163, 'accumulated_logging_time': 2.6973180770874023, 'global_step': 88780, 'preemption_count': 0}), (90287, {'train/accuracy': 0.7116549611091614, 'train/loss': 1.246743083000183, 'validation/accuracy': 0.6574999690055847, 'validation/loss': 1.4754823446273804, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.1418609619140625, 'test/num_examples': 10000, 'score': 30659.994156122208, 'total_duration': 31853.742823123932, 'accumulated_submission_time': 30659.994156122208, 'accumulated_eval_time': 1187.9223272800446, 'accumulated_logging_time': 2.740720748901367, 'global_step': 90287, 'preemption_count': 0}), (91793, {'train/accuracy': 0.7194674611091614, 'train/loss': 1.1982098817825317, 'validation/accuracy': 0.6654799580574036, 'validation/loss': 1.4386276006698608, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.08927321434021, 'test/num_examples': 10000, 'score': 31169.86101269722, 'total_duration': 32380.824385404587, 'accumulated_submission_time': 31169.86101269722, 'accumulated_eval_time': 1204.966181755066, 'accumulated_logging_time': 2.859598398208618, 'global_step': 91793, 'preemption_count': 0}), (93300, {'train/accuracy': 0.722676157951355, 'train/loss': 1.201149821281433, 'validation/accuracy': 0.6536999940872192, 'validation/loss': 1.5257678031921387, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.1804299354553223, 'test/num_examples': 10000, 'score': 31679.799534082413, 'total_duration': 32907.94575691223, 'accumulated_submission_time': 31679.799534082413, 'accumulated_eval_time': 1222.0526728630066, 'accumulated_logging_time': 2.903446912765503, 'global_step': 93300, 'preemption_count': 0}), (94807, {'train/accuracy': 0.7135283946990967, 'train/loss': 1.2340788841247559, 'validation/accuracy': 0.650439977645874, 'validation/loss': 1.527204155921936, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.177957057952881, 'test/num_examples': 10000, 'score': 32189.77259206772, 'total_duration': 33435.2316634655, 'accumulated_submission_time': 32189.77259206772, 'accumulated_eval_time': 1239.2692143917084, 'accumulated_logging_time': 2.946500778198242, 'global_step': 94807, 'preemption_count': 0}), (96315, {'train/accuracy': 0.7221978306770325, 'train/loss': 1.189970850944519, 'validation/accuracy': 0.6604399681091309, 'validation/loss': 1.467153549194336, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.1280345916748047, 'test/num_examples': 10000, 'score': 32699.811596155167, 'total_duration': 33962.59027004242, 'accumulated_submission_time': 32699.811596155167, 'accumulated_eval_time': 1256.4928257465363, 'accumulated_logging_time': 2.9893760681152344, 'global_step': 96315, 'preemption_count': 0}), (97822, {'train/accuracy': 0.725027859210968, 'train/loss': 1.1888576745986938, 'validation/accuracy': 0.668999969959259, 'validation/loss': 1.4445313215255737, 'validation/num_examples': 50000, 'test/accuracy': 0.5423000454902649, 'test/loss': 2.084092617034912, 'test/num_examples': 10000, 'score': 33209.789820194244, 'total_duration': 34490.15003442764, 'accumulated_submission_time': 33209.789820194244, 'accumulated_eval_time': 1273.975423336029, 'accumulated_logging_time': 3.0374257564544678, 'global_step': 97822, 'preemption_count': 0}), (99329, {'train/accuracy': 0.7132493257522583, 'train/loss': 1.2412854433059692, 'validation/accuracy': 0.6611199975013733, 'validation/loss': 1.480249047279358, 'validation/num_examples': 50000, 'test/accuracy': 0.5276000499725342, 'test/loss': 2.150489568710327, 'test/num_examples': 10000, 'score': 33719.99088358879, 'total_duration': 35017.532859802246, 'accumulated_submission_time': 33719.99088358879, 'accumulated_eval_time': 1291.060376405716, 'accumulated_logging_time': 3.081228733062744, 'global_step': 99329, 'preemption_count': 0}), (100836, {'train/accuracy': 0.7284757494926453, 'train/loss': 1.1622774600982666, 'validation/accuracy': 0.6675999760627747, 'validation/loss': 1.4319740533828735, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.0938448905944824, 'test/num_examples': 10000, 'score': 34229.93264293671, 'total_duration': 35544.59036445618, 'accumulated_submission_time': 34229.93264293671, 'accumulated_eval_time': 1308.0780620574951, 'accumulated_logging_time': 3.127570152282715, 'global_step': 100836, 'preemption_count': 0}), (102343, {'train/accuracy': 0.7571149468421936, 'train/loss': 1.0458910465240479, 'validation/accuracy': 0.675879955291748, 'validation/loss': 1.4058371782302856, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.0450947284698486, 'test/num_examples': 10000, 'score': 34739.89816379547, 'total_duration': 36071.67170572281, 'accumulated_submission_time': 34739.89816379547, 'accumulated_eval_time': 1325.0967199802399, 'accumulated_logging_time': 3.1725218296051025, 'global_step': 102343, 'preemption_count': 0}), (103850, {'train/accuracy': 0.7410514950752258, 'train/loss': 1.109355092048645, 'validation/accuracy': 0.6710000038146973, 'validation/loss': 1.4239152669906616, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.0953757762908936, 'test/num_examples': 10000, 'score': 35250.05817198753, 'total_duration': 36599.079026699066, 'accumulated_submission_time': 35250.05817198753, 'accumulated_eval_time': 1342.2405395507812, 'accumulated_logging_time': 3.224278211593628, 'global_step': 103850, 'preemption_count': 0}), (105356, {'train/accuracy': 0.7430245280265808, 'train/loss': 1.116188883781433, 'validation/accuracy': 0.6763399839401245, 'validation/loss': 1.4128817319869995, 'validation/num_examples': 50000, 'test/accuracy': 0.5460000038146973, 'test/loss': 2.064647674560547, 'test/num_examples': 10000, 'score': 35759.976840257645, 'total_duration': 37126.32803606987, 'accumulated_submission_time': 35759.976840257645, 'accumulated_eval_time': 1359.4712007045746, 'accumulated_logging_time': 3.270002841949463, 'global_step': 105356, 'preemption_count': 0}), (106863, {'train/accuracy': 0.7424864172935486, 'train/loss': 1.1154100894927979, 'validation/accuracy': 0.6801799535751343, 'validation/loss': 1.388283610343933, 'validation/num_examples': 50000, 'test/accuracy': 0.5570999979972839, 'test/loss': 2.026524066925049, 'test/num_examples': 10000, 'score': 36269.989028692245, 'total_duration': 37653.339567899704, 'accumulated_submission_time': 36269.989028692245, 'accumulated_eval_time': 1376.36949300766, 'accumulated_logging_time': 3.318311929702759, 'global_step': 106863, 'preemption_count': 0}), (108370, {'train/accuracy': 0.7401745915412903, 'train/loss': 1.10150146484375, 'validation/accuracy': 0.6816999912261963, 'validation/loss': 1.3689839839935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.0306382179260254, 'test/num_examples': 10000, 'score': 36780.031766176224, 'total_duration': 38180.82426738739, 'accumulated_submission_time': 36780.031766176224, 'accumulated_eval_time': 1393.7109084129333, 'accumulated_logging_time': 3.367356061935425, 'global_step': 108370, 'preemption_count': 0}), (109877, {'train/accuracy': 0.7429846525192261, 'train/loss': 1.0977813005447388, 'validation/accuracy': 0.6827200055122375, 'validation/loss': 1.3649893999099731, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.0068023204803467, 'test/num_examples': 10000, 'score': 37290.11735486984, 'total_duration': 38708.2563290596, 'accumulated_submission_time': 37290.11735486984, 'accumulated_eval_time': 1410.958943605423, 'accumulated_logging_time': 3.413281202316284, 'global_step': 109877, 'preemption_count': 0}), (111385, {'train/accuracy': 0.7664620280265808, 'train/loss': 1.0001466274261475, 'validation/accuracy': 0.6792399883270264, 'validation/loss': 1.3854286670684814, 'validation/num_examples': 50000, 'test/accuracy': 0.551300048828125, 'test/loss': 2.0405306816101074, 'test/num_examples': 10000, 'score': 37800.23961639404, 'total_duration': 39235.68991231918, 'accumulated_submission_time': 37800.23961639404, 'accumulated_eval_time': 1428.169054031372, 'accumulated_logging_time': 3.4625115394592285, 'global_step': 111385, 'preemption_count': 0}), (112892, {'train/accuracy': 0.7581512928009033, 'train/loss': 1.031977891921997, 'validation/accuracy': 0.6830799579620361, 'validation/loss': 1.3659889698028564, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 2.0102922916412354, 'test/num_examples': 10000, 'score': 38310.37893438339, 'total_duration': 39763.20695757866, 'accumulated_submission_time': 38310.37893438339, 'accumulated_eval_time': 1445.4499547481537, 'accumulated_logging_time': 3.506695508956909, 'global_step': 112892, 'preemption_count': 0}), (114399, {'train/accuracy': 0.7623365521430969, 'train/loss': 1.0210669040679932, 'validation/accuracy': 0.6910600066184998, 'validation/loss': 1.3425350189208984, 'validation/num_examples': 50000, 'test/accuracy': 0.5693000555038452, 'test/loss': 1.9787520170211792, 'test/num_examples': 10000, 'score': 38820.30667281151, 'total_duration': 40290.31508851051, 'accumulated_submission_time': 38820.30667281151, 'accumulated_eval_time': 1462.527009487152, 'accumulated_logging_time': 3.55802059173584, 'global_step': 114399, 'preemption_count': 0}), (115906, {'train/accuracy': 0.7527702450752258, 'train/loss': 1.0518150329589844, 'validation/accuracy': 0.6881200075149536, 'validation/loss': 1.3431140184402466, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.0154571533203125, 'test/num_examples': 10000, 'score': 39330.32674264908, 'total_duration': 40817.67096376419, 'accumulated_submission_time': 39330.32674264908, 'accumulated_eval_time': 1479.7441306114197, 'accumulated_logging_time': 3.622938871383667, 'global_step': 115906, 'preemption_count': 0}), (117413, {'train/accuracy': 0.7529894709587097, 'train/loss': 1.057737946510315, 'validation/accuracy': 0.6910399794578552, 'validation/loss': 1.3415244817733765, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.969575047492981, 'test/num_examples': 10000, 'score': 39840.43624377251, 'total_duration': 41345.04013109207, 'accumulated_submission_time': 39840.43624377251, 'accumulated_eval_time': 1496.8971843719482, 'accumulated_logging_time': 3.6775968074798584, 'global_step': 117413, 'preemption_count': 0}), (118920, {'train/accuracy': 0.7530691623687744, 'train/loss': 1.0459004640579224, 'validation/accuracy': 0.6911999583244324, 'validation/loss': 1.3308132886886597, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9982550144195557, 'test/num_examples': 10000, 'score': 40350.61412215233, 'total_duration': 41872.54306650162, 'accumulated_submission_time': 40350.61412215233, 'accumulated_eval_time': 1514.1224842071533, 'accumulated_logging_time': 3.7258994579315186, 'global_step': 118920, 'preemption_count': 0}), (120427, {'train/accuracy': 0.7836814522743225, 'train/loss': 0.9314776659011841, 'validation/accuracy': 0.693120002746582, 'validation/loss': 1.3276666402816772, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 1.9804224967956543, 'test/num_examples': 10000, 'score': 40860.520089387894, 'total_duration': 42399.89472198486, 'accumulated_submission_time': 40860.520089387894, 'accumulated_eval_time': 1531.4665586948395, 'accumulated_logging_time': 3.774169683456421, 'global_step': 120427, 'preemption_count': 0}), (121934, {'train/accuracy': 0.7770846486091614, 'train/loss': 0.960997998714447, 'validation/accuracy': 0.7005199790000916, 'validation/loss': 1.2985070943832397, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 1.9412811994552612, 'test/num_examples': 10000, 'score': 41370.45637798309, 'total_duration': 42927.18236398697, 'accumulated_submission_time': 41370.45637798309, 'accumulated_eval_time': 1548.710284948349, 'accumulated_logging_time': 3.8285911083221436, 'global_step': 121934, 'preemption_count': 0}), (123441, {'train/accuracy': 0.7704280614852905, 'train/loss': 0.9899856448173523, 'validation/accuracy': 0.6974999904632568, 'validation/loss': 1.3206807374954224, 'validation/num_examples': 50000, 'test/accuracy': 0.5712000131607056, 'test/loss': 1.9497255086898804, 'test/num_examples': 10000, 'score': 41880.491114616394, 'total_duration': 43454.39989852905, 'accumulated_submission_time': 41880.491114616394, 'accumulated_eval_time': 1565.7893166542053, 'accumulated_logging_time': 3.87968111038208, 'global_step': 123441, 'preemption_count': 0}), (124948, {'train/accuracy': 0.7798947691917419, 'train/loss': 0.9331660270690918, 'validation/accuracy': 0.7085399627685547, 'validation/loss': 1.2580126523971558, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.8990615606307983, 'test/num_examples': 10000, 'score': 42390.486533641815, 'total_duration': 43981.515066862106, 'accumulated_submission_time': 42390.486533641815, 'accumulated_eval_time': 1582.8079690933228, 'accumulated_logging_time': 3.928217649459839, 'global_step': 124948, 'preemption_count': 0}), (126455, {'train/accuracy': 0.7784597873687744, 'train/loss': 0.9620999693870544, 'validation/accuracy': 0.7042999863624573, 'validation/loss': 1.2834198474884033, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 1.9199188947677612, 'test/num_examples': 10000, 'score': 42900.42901682854, 'total_duration': 44508.50655436516, 'accumulated_submission_time': 42900.42901682854, 'accumulated_eval_time': 1599.7522914409637, 'accumulated_logging_time': 3.979759454727173, 'global_step': 126455, 'preemption_count': 0}), (127961, {'train/accuracy': 0.7736966013908386, 'train/loss': 0.9492216110229492, 'validation/accuracy': 0.7021399736404419, 'validation/loss': 1.2651011943817139, 'validation/num_examples': 50000, 'test/accuracy': 0.5796000361442566, 'test/loss': 1.9083911180496216, 'test/num_examples': 10000, 'score': 43410.41619229317, 'total_duration': 45035.59116268158, 'accumulated_submission_time': 43410.41619229317, 'accumulated_eval_time': 1616.7470650672913, 'accumulated_logging_time': 4.031066417694092, 'global_step': 127961, 'preemption_count': 0}), (129469, {'train/accuracy': 0.805683970451355, 'train/loss': 0.8402982354164124, 'validation/accuracy': 0.7069199681282043, 'validation/loss': 1.2687928676605225, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.9307630062103271, 'test/num_examples': 10000, 'score': 43920.58583164215, 'total_duration': 45563.149040699005, 'accumulated_submission_time': 43920.58583164215, 'accumulated_eval_time': 1634.0042452812195, 'accumulated_logging_time': 4.109456300735474, 'global_step': 129469, 'preemption_count': 0}), (130977, {'train/accuracy': 0.7959781289100647, 'train/loss': 0.8902884125709534, 'validation/accuracy': 0.7091799974441528, 'validation/loss': 1.2596995830535889, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 1.9029459953308105, 'test/num_examples': 10000, 'score': 44430.66488528252, 'total_duration': 46090.4052464962, 'accumulated_submission_time': 44430.66488528252, 'accumulated_eval_time': 1651.078492641449, 'accumulated_logging_time': 4.160379648208618, 'global_step': 130977, 'preemption_count': 0}), (132484, {'train/accuracy': 0.7964564561843872, 'train/loss': 0.8781556487083435, 'validation/accuracy': 0.7150799632072449, 'validation/loss': 1.2385895252227783, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.8773962259292603, 'test/num_examples': 10000, 'score': 44940.60585641861, 'total_duration': 46617.5152451992, 'accumulated_submission_time': 44940.60585641861, 'accumulated_eval_time': 1668.14599943161, 'accumulated_logging_time': 4.21102237701416, 'global_step': 132484, 'preemption_count': 0}), (133991, {'train/accuracy': 0.8026745915412903, 'train/loss': 0.8502547144889832, 'validation/accuracy': 0.7177799940109253, 'validation/loss': 1.2106214761734009, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.840102195739746, 'test/num_examples': 10000, 'score': 45450.668227910995, 'total_duration': 47144.82175087929, 'accumulated_submission_time': 45450.668227910995, 'accumulated_eval_time': 1685.284812450409, 'accumulated_logging_time': 4.261689901351929, 'global_step': 133991, 'preemption_count': 0}), (135498, {'train/accuracy': 0.7995057106018066, 'train/loss': 0.8691868782043457, 'validation/accuracy': 0.7166799902915955, 'validation/loss': 1.2189016342163086, 'validation/num_examples': 50000, 'test/accuracy': 0.5916000008583069, 'test/loss': 1.859628438949585, 'test/num_examples': 10000, 'score': 45960.74799871445, 'total_duration': 47672.127898454666, 'accumulated_submission_time': 45960.74799871445, 'accumulated_eval_time': 1702.4054865837097, 'accumulated_logging_time': 4.315122365951538, 'global_step': 135498, 'preemption_count': 0}), (137005, {'train/accuracy': 0.7967952489852905, 'train/loss': 0.862323522567749, 'validation/accuracy': 0.7177000045776367, 'validation/loss': 1.2082816362380981, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.8467576503753662, 'test/num_examples': 10000, 'score': 46470.69027876854, 'total_duration': 48199.111078739166, 'accumulated_submission_time': 46470.69027876854, 'accumulated_eval_time': 1719.3394558429718, 'accumulated_logging_time': 4.369931697845459, 'global_step': 137005, 'preemption_count': 0}), (138512, {'train/accuracy': 0.8315330147743225, 'train/loss': 0.7364321947097778, 'validation/accuracy': 0.7225199937820435, 'validation/loss': 1.1886651515960693, 'validation/num_examples': 50000, 'test/accuracy': 0.5956000089645386, 'test/loss': 1.8367879390716553, 'test/num_examples': 10000, 'score': 46980.68811249733, 'total_duration': 48726.23480153084, 'accumulated_submission_time': 46980.68811249733, 'accumulated_eval_time': 1736.362550497055, 'accumulated_logging_time': 4.420541524887085, 'global_step': 138512, 'preemption_count': 0}), (140019, {'train/accuracy': 0.8246771097183228, 'train/loss': 0.7462592720985413, 'validation/accuracy': 0.729919970035553, 'validation/loss': 1.1638081073760986, 'validation/num_examples': 50000, 'test/accuracy': 0.6030000448226929, 'test/loss': 1.794895052909851, 'test/num_examples': 10000, 'score': 47490.778540849686, 'total_duration': 49253.46666812897, 'accumulated_submission_time': 47490.778540849686, 'accumulated_eval_time': 1753.3998112678528, 'accumulated_logging_time': 4.472010850906372, 'global_step': 140019, 'preemption_count': 0}), (141526, {'train/accuracy': 0.8252949714660645, 'train/loss': 0.7593898177146912, 'validation/accuracy': 0.7310400009155273, 'validation/loss': 1.1624298095703125, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.7896476984024048, 'test/num_examples': 10000, 'score': 48000.73310112953, 'total_duration': 49780.63418030739, 'accumulated_submission_time': 48000.73310112953, 'accumulated_eval_time': 1770.5072956085205, 'accumulated_logging_time': 4.523514747619629, 'global_step': 141526, 'preemption_count': 0}), (143033, {'train/accuracy': 0.8251753449440002, 'train/loss': 0.7437108159065247, 'validation/accuracy': 0.7314800024032593, 'validation/loss': 1.141937017440796, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.773678183555603, 'test/num_examples': 10000, 'score': 48510.89326953888, 'total_duration': 50308.06536364555, 'accumulated_submission_time': 48510.89326953888, 'accumulated_eval_time': 1787.6722025871277, 'accumulated_logging_time': 4.576416730880737, 'global_step': 143033, 'preemption_count': 0}), (144539, {'train/accuracy': 0.8236008882522583, 'train/loss': 0.7736658453941345, 'validation/accuracy': 0.7317599654197693, 'validation/loss': 1.1646684408187866, 'validation/num_examples': 50000, 'test/accuracy': 0.6085000038146973, 'test/loss': 1.8027182817459106, 'test/num_examples': 10000, 'score': 49020.87477731705, 'total_duration': 50835.20391178131, 'accumulated_submission_time': 49020.87477731705, 'accumulated_eval_time': 1804.7232937812805, 'accumulated_logging_time': 4.630538702011108, 'global_step': 144539, 'preemption_count': 0}), (146046, {'train/accuracy': 0.8278858065605164, 'train/loss': 0.7407735586166382, 'validation/accuracy': 0.7351599931716919, 'validation/loss': 1.1394431591033936, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.7666029930114746, 'test/num_examples': 10000, 'score': 49530.97762846947, 'total_duration': 51362.362865924835, 'accumulated_submission_time': 49530.97762846947, 'accumulated_eval_time': 1821.6753494739532, 'accumulated_logging_time': 4.683336019515991, 'global_step': 146046, 'preemption_count': 0}), (147552, {'train/accuracy': 0.85843825340271, 'train/loss': 0.6291110515594482, 'validation/accuracy': 0.7379999756813049, 'validation/loss': 1.130968689918518, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.7525743246078491, 'test/num_examples': 10000, 'score': 50040.926671266556, 'total_duration': 51889.318643569946, 'accumulated_submission_time': 50040.926671266556, 'accumulated_eval_time': 1838.5789158344269, 'accumulated_logging_time': 4.734616041183472, 'global_step': 147552, 'preemption_count': 0}), (149059, {'train/accuracy': 0.8477559089660645, 'train/loss': 0.667382538318634, 'validation/accuracy': 0.7384999990463257, 'validation/loss': 1.130212664604187, 'validation/num_examples': 50000, 'test/accuracy': 0.6147000193595886, 'test/loss': 1.7563661336898804, 'test/num_examples': 10000, 'score': 50551.148307561874, 'total_duration': 52416.78352403641, 'accumulated_submission_time': 50551.148307561874, 'accumulated_eval_time': 1855.7147312164307, 'accumulated_logging_time': 4.789153575897217, 'global_step': 149059, 'preemption_count': 0}), (150566, {'train/accuracy': 0.8537547588348389, 'train/loss': 0.6530286073684692, 'validation/accuracy': 0.7407999634742737, 'validation/loss': 1.108092188835144, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.735827088356018, 'test/num_examples': 10000, 'score': 51061.33330345154, 'total_duration': 52944.4701063633, 'accumulated_submission_time': 51061.33330345154, 'accumulated_eval_time': 1873.110155582428, 'accumulated_logging_time': 4.844248533248901, 'global_step': 150566, 'preemption_count': 0}), (152073, {'train/accuracy': 0.8474569320678711, 'train/loss': 0.6855957508087158, 'validation/accuracy': 0.743939995765686, 'validation/loss': 1.1150567531585693, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.7458469867706299, 'test/num_examples': 10000, 'score': 51571.34389591217, 'total_duration': 53472.46926546097, 'accumulated_submission_time': 51571.34389591217, 'accumulated_eval_time': 1890.9894676208496, 'accumulated_logging_time': 4.900099754333496, 'global_step': 152073, 'preemption_count': 0}), (153580, {'train/accuracy': 0.8515625, 'train/loss': 0.6506571769714355, 'validation/accuracy': 0.7438799738883972, 'validation/loss': 1.100218653678894, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.7106108665466309, 'test/num_examples': 10000, 'score': 52081.42667245865, 'total_duration': 53999.787418842316, 'accumulated_submission_time': 52081.42667245865, 'accumulated_eval_time': 1908.1210148334503, 'accumulated_logging_time': 4.9520440101623535, 'global_step': 153580, 'preemption_count': 0}), (155087, {'train/accuracy': 0.8556082248687744, 'train/loss': 0.6351298093795776, 'validation/accuracy': 0.7490400075912476, 'validation/loss': 1.0883169174194336, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.720607876777649, 'test/num_examples': 10000, 'score': 52591.56684565544, 'total_duration': 54527.11508727074, 'accumulated_submission_time': 52591.56684565544, 'accumulated_eval_time': 1925.2019228935242, 'accumulated_logging_time': 5.0060014724731445, 'global_step': 155087, 'preemption_count': 0}), (156594, {'train/accuracy': 0.8785076141357422, 'train/loss': 0.5570769309997559, 'validation/accuracy': 0.7483800053596497, 'validation/loss': 1.0872067213058472, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.7072957754135132, 'test/num_examples': 10000, 'score': 53101.60917067528, 'total_duration': 55054.2172267437, 'accumulated_submission_time': 53101.60917067528, 'accumulated_eval_time': 1942.1512784957886, 'accumulated_logging_time': 5.063514232635498, 'global_step': 156594, 'preemption_count': 0}), (158101, {'train/accuracy': 0.8756377100944519, 'train/loss': 0.5695292949676514, 'validation/accuracy': 0.7520399689674377, 'validation/loss': 1.07098388671875, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.6892168521881104, 'test/num_examples': 10000, 'score': 53611.69621825218, 'total_duration': 55581.5731716156, 'accumulated_submission_time': 53611.69621825218, 'accumulated_eval_time': 1959.310719013214, 'accumulated_logging_time': 5.121617794036865, 'global_step': 158101, 'preemption_count': 0}), (159608, {'train/accuracy': 0.8757772445678711, 'train/loss': 0.5535916686058044, 'validation/accuracy': 0.754539966583252, 'validation/loss': 1.05316162109375, 'validation/num_examples': 50000, 'test/accuracy': 0.6337000131607056, 'test/loss': 1.6755914688110352, 'test/num_examples': 10000, 'score': 54121.863714933395, 'total_duration': 56108.89868927002, 'accumulated_submission_time': 54121.863714933395, 'accumulated_eval_time': 1976.359569311142, 'accumulated_logging_time': 5.178009748458862, 'global_step': 159608, 'preemption_count': 0}), (161115, {'train/accuracy': 0.8797831535339355, 'train/loss': 0.5523720979690552, 'validation/accuracy': 0.7566799521446228, 'validation/loss': 1.0592304468154907, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.6685220003128052, 'test/num_examples': 10000, 'score': 54631.85998415947, 'total_duration': 56636.17068815231, 'accumulated_submission_time': 54631.85998415947, 'accumulated_eval_time': 1993.5300033092499, 'accumulated_logging_time': 5.232148885726929, 'global_step': 161115, 'preemption_count': 0}), (162622, {'train/accuracy': 0.8821747303009033, 'train/loss': 0.5450007915496826, 'validation/accuracy': 0.758080005645752, 'validation/loss': 1.0532749891281128, 'validation/num_examples': 50000, 'test/accuracy': 0.6375000476837158, 'test/loss': 1.6655832529067993, 'test/num_examples': 10000, 'score': 55141.891810655594, 'total_duration': 57163.43918633461, 'accumulated_submission_time': 55141.891810655594, 'accumulated_eval_time': 2010.6388430595398, 'accumulated_logging_time': 5.307616949081421, 'global_step': 162622, 'preemption_count': 0}), (164129, {'train/accuracy': 0.8889508843421936, 'train/loss': 0.5121031999588013, 'validation/accuracy': 0.761139988899231, 'validation/loss': 1.0397335290908813, 'validation/num_examples': 50000, 'test/accuracy': 0.6361000537872314, 'test/loss': 1.6553908586502075, 'test/num_examples': 10000, 'score': 55651.94191503525, 'total_duration': 57690.5752518177, 'accumulated_submission_time': 55651.94191503525, 'accumulated_eval_time': 2027.6138999462128, 'accumulated_logging_time': 5.36580753326416, 'global_step': 164129, 'preemption_count': 0}), (165636, {'train/accuracy': 0.9054129123687744, 'train/loss': 0.462770015001297, 'validation/accuracy': 0.7623400092124939, 'validation/loss': 1.0329078435897827, 'validation/num_examples': 50000, 'test/accuracy': 0.6397000551223755, 'test/loss': 1.6543211936950684, 'test/num_examples': 10000, 'score': 56162.06087732315, 'total_duration': 58218.07364296913, 'accumulated_submission_time': 56162.06087732315, 'accumulated_eval_time': 2044.8844938278198, 'accumulated_logging_time': 5.420394659042358, 'global_step': 165636, 'preemption_count': 0}), (167143, {'train/accuracy': 0.9016661047935486, 'train/loss': 0.47373443841934204, 'validation/accuracy': 0.7623800039291382, 'validation/loss': 1.0327937602996826, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.6520830392837524, 'test/num_examples': 10000, 'score': 56672.09117293358, 'total_duration': 58745.37574315071, 'accumulated_submission_time': 56672.09117293358, 'accumulated_eval_time': 2062.039719581604, 'accumulated_logging_time': 5.4839723110198975, 'global_step': 167143, 'preemption_count': 0}), (168650, {'train/accuracy': 0.9009087681770325, 'train/loss': 0.4697478115558624, 'validation/accuracy': 0.766319990158081, 'validation/loss': 1.0211455821990967, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.6334751844406128, 'test/num_examples': 10000, 'score': 57182.03576087952, 'total_duration': 59272.69396233559, 'accumulated_submission_time': 57182.03576087952, 'accumulated_eval_time': 2079.302550792694, 'accumulated_logging_time': 5.541658639907837, 'global_step': 168650, 'preemption_count': 0}), (170157, {'train/accuracy': 0.9041772484779358, 'train/loss': 0.45963263511657715, 'validation/accuracy': 0.7662799954414368, 'validation/loss': 1.0119706392288208, 'validation/num_examples': 50000, 'test/accuracy': 0.6470000147819519, 'test/loss': 1.6323281526565552, 'test/num_examples': 10000, 'score': 57692.05851483345, 'total_duration': 59799.91110467911, 'accumulated_submission_time': 57692.05851483345, 'accumulated_eval_time': 2096.3898191452026, 'accumulated_logging_time': 5.597667694091797, 'global_step': 170157, 'preemption_count': 0}), (171664, {'train/accuracy': 0.9072065949440002, 'train/loss': 0.4597722291946411, 'validation/accuracy': 0.7687399983406067, 'validation/loss': 1.0186874866485596, 'validation/num_examples': 50000, 'test/accuracy': 0.6431000232696533, 'test/loss': 1.6356762647628784, 'test/num_examples': 10000, 'score': 58202.10117435455, 'total_duration': 60327.013768196106, 'accumulated_submission_time': 58202.10117435455, 'accumulated_eval_time': 2113.3370258808136, 'accumulated_logging_time': 5.657930612564087, 'global_step': 171664, 'preemption_count': 0}), (173171, {'train/accuracy': 0.9073860049247742, 'train/loss': 0.44713687896728516, 'validation/accuracy': 0.7688800096511841, 'validation/loss': 1.0100610256195068, 'validation/num_examples': 50000, 'test/accuracy': 0.6465000510215759, 'test/loss': 1.6286207437515259, 'test/num_examples': 10000, 'score': 58712.05089068413, 'total_duration': 60854.05264925957, 'accumulated_submission_time': 58712.05089068413, 'accumulated_eval_time': 2130.31436419487, 'accumulated_logging_time': 5.717501878738403, 'global_step': 173171, 'preemption_count': 0}), (174678, {'train/accuracy': 0.9171914458274841, 'train/loss': 0.4137333035469055, 'validation/accuracy': 0.7696399688720703, 'validation/loss': 1.0041489601135254, 'validation/num_examples': 50000, 'test/accuracy': 0.6478000283241272, 'test/loss': 1.612573504447937, 'test/num_examples': 10000, 'score': 59221.97033691406, 'total_duration': 61381.271008491516, 'accumulated_submission_time': 59221.97033691406, 'accumulated_eval_time': 2147.5038130283356, 'accumulated_logging_time': 5.7750890254974365, 'global_step': 174678, 'preemption_count': 0}), (176184, {'train/accuracy': 0.9174505472183228, 'train/loss': 0.415025532245636, 'validation/accuracy': 0.7699399590492249, 'validation/loss': 1.0015920400619507, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.610582947731018, 'test/num_examples': 10000, 'score': 59731.89277672768, 'total_duration': 61908.24905347824, 'accumulated_submission_time': 59731.89277672768, 'accumulated_eval_time': 2164.4502770900726, 'accumulated_logging_time': 5.829644203186035, 'global_step': 176184, 'preemption_count': 0}), (177690, {'train/accuracy': 0.9185068607330322, 'train/loss': 0.4077030122280121, 'validation/accuracy': 0.7705599665641785, 'validation/loss': 0.9990577697753906, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.609252691268921, 'test/num_examples': 10000, 'score': 60241.88081359863, 'total_duration': 62435.42911171913, 'accumulated_submission_time': 60241.88081359863, 'accumulated_eval_time': 2181.530555009842, 'accumulated_logging_time': 5.88846755027771, 'global_step': 177690, 'preemption_count': 0}), (179196, {'train/accuracy': 0.9178292155265808, 'train/loss': 0.41300442814826965, 'validation/accuracy': 0.7719599604606628, 'validation/loss': 0.9963983297348022, 'validation/num_examples': 50000, 'test/accuracy': 0.6491000056266785, 'test/loss': 1.6055240631103516, 'test/num_examples': 10000, 'score': 60751.85917687416, 'total_duration': 62962.54237771034, 'accumulated_submission_time': 60751.85917687416, 'accumulated_eval_time': 2198.5527551174164, 'accumulated_logging_time': 5.948404788970947, 'global_step': 179196, 'preemption_count': 0}), (180703, {'train/accuracy': 0.9183274507522583, 'train/loss': 0.40757840871810913, 'validation/accuracy': 0.7716599702835083, 'validation/loss': 0.9949711561203003, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6056536436080933, 'test/num_examples': 10000, 'score': 61261.88948059082, 'total_duration': 63490.04742026329, 'accumulated_submission_time': 61261.88948059082, 'accumulated_eval_time': 2215.9111063480377, 'accumulated_logging_time': 6.012499570846558, 'global_step': 180703, 'preemption_count': 0}), (182210, {'train/accuracy': 0.9197026491165161, 'train/loss': 0.4040387272834778, 'validation/accuracy': 0.772159993648529, 'validation/loss': 0.9936366081237793, 'validation/num_examples': 50000, 'test/accuracy': 0.6494000554084778, 'test/loss': 1.6046305894851685, 'test/num_examples': 10000, 'score': 61772.02049660683, 'total_duration': 64017.193064928055, 'accumulated_submission_time': 61772.02049660683, 'accumulated_eval_time': 2232.815001964569, 'accumulated_logging_time': 6.070794105529785, 'global_step': 182210, 'preemption_count': 0}), (183717, {'train/accuracy': 0.9208585619926453, 'train/loss': 0.3966313898563385, 'validation/accuracy': 0.7724399566650391, 'validation/loss': 0.9899981021881104, 'validation/num_examples': 50000, 'test/accuracy': 0.6511000394821167, 'test/loss': 1.6006096601486206, 'test/num_examples': 10000, 'score': 62282.11263537407, 'total_duration': 64544.36849927902, 'accumulated_submission_time': 62282.11263537407, 'accumulated_eval_time': 2249.791279554367, 'accumulated_logging_time': 6.127057075500488, 'global_step': 183717, 'preemption_count': 0}), (185224, {'train/accuracy': 0.9212771058082581, 'train/loss': 0.39945515990257263, 'validation/accuracy': 0.7727199792861938, 'validation/loss': 0.9911830425262451, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6002691984176636, 'test/num_examples': 10000, 'score': 62792.30425167084, 'total_duration': 65071.81726360321, 'accumulated_submission_time': 62792.30425167084, 'accumulated_eval_time': 2266.940537214279, 'accumulated_logging_time': 6.183432102203369, 'global_step': 185224, 'preemption_count': 0})], 'global_step': 185862}
I0127 13:10:01.733869 140027215431488 submission_runner.py:586] Timing: 63008.31415557861
I0127 13:10:01.733940 140027215431488 submission_runner.py:588] Total number of evals: 124
I0127 13:10:01.733990 140027215431488 submission_runner.py:589] ====================
I0127 13:10:01.734042 140027215431488 submission_runner.py:542] Using RNG seed 3078694106
I0127 13:10:01.735713 140027215431488 submission_runner.py:551] --- Tuning run 2/5 ---
I0127 13:10:01.735831 140027215431488 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2.
I0127 13:10:01.741613 140027215431488 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2/hparams.json.
I0127 13:10:01.743145 140027215431488 submission_runner.py:206] Initializing dataset.
I0127 13:10:01.754131 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 13:10:01.764760 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 13:10:01.973902 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 13:10:02.284636 140027215431488 submission_runner.py:213] Initializing model.
I0127 13:10:08.395523 140027215431488 submission_runner.py:255] Initializing optimizer.
I0127 13:10:08.798733 140027215431488 submission_runner.py:262] Initializing metrics bundle.
I0127 13:10:08.798966 140027215431488 submission_runner.py:280] Initializing checkpoint and logger.
I0127 13:10:08.886462 140027215431488 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0127 13:10:08.886702 140027215431488 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0127 13:10:20.145053 140027215431488 logger_utils.py:220] Unable to record git information. Continuing without it.
I0127 13:10:30.769227 140027215431488 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2/flags_0.json.
I0127 13:10:30.781261 140027215431488 submission_runner.py:314] Starting training loop.
I0127 13:11:04.210258 139865760950016 logging_writer.py:48] [0] global_step=0, grad_norm=0.5300779342651367, loss=6.918390274047852
I0127 13:11:04.220481 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:11:10.460599 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:11:19.470608 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:11:21.967624 140027215431488 submission_runner.py:408] Time since start: 51.19s, 	Step: 1, 	{'train/accuracy': 0.0006576849264092743, 'train/loss': 6.912638187408447, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 33.4391393661499, 'total_duration': 51.186286211013794, 'accumulated_submission_time': 33.4391393661499, 'accumulated_eval_time': 17.747056245803833, 'accumulated_logging_time': 0}
I0127 13:11:21.977914 139865257662208 logging_writer.py:48] [1] accumulated_eval_time=17.747056, accumulated_logging_time=0, accumulated_submission_time=33.439139, global_step=1, preemption_count=0, score=33.439139, test/accuracy=0.001300, test/loss=6.911728, test/num_examples=10000, total_duration=51.186286, train/accuracy=0.000658, train/loss=6.912638, validation/accuracy=0.000680, validation/loss=6.912051, validation/num_examples=50000
I0127 13:11:55.968017 139865266054912 logging_writer.py:48] [100] global_step=100, grad_norm=0.5243191123008728, loss=6.9022216796875
I0127 13:12:29.982565 139865257662208 logging_writer.py:48] [200] global_step=200, grad_norm=0.5339565873146057, loss=6.85552978515625
I0127 13:13:04.025768 139865266054912 logging_writer.py:48] [300] global_step=300, grad_norm=0.5776817798614502, loss=6.770439624786377
I0127 13:13:38.075989 139865257662208 logging_writer.py:48] [400] global_step=400, grad_norm=0.6367026567459106, loss=6.671178817749023
I0127 13:14:12.115808 139865266054912 logging_writer.py:48] [500] global_step=500, grad_norm=0.6628660559654236, loss=6.595005989074707
I0127 13:14:46.177725 139865257662208 logging_writer.py:48] [600] global_step=600, grad_norm=0.7074207663536072, loss=6.512227535247803
I0127 13:15:20.249834 139865266054912 logging_writer.py:48] [700] global_step=700, grad_norm=0.8528745174407959, loss=6.469108581542969
I0127 13:15:54.302925 139865257662208 logging_writer.py:48] [800] global_step=800, grad_norm=1.2188752889633179, loss=6.388007164001465
I0127 13:16:28.362951 139865266054912 logging_writer.py:48] [900] global_step=900, grad_norm=1.8466860055923462, loss=6.286403656005859
I0127 13:17:02.490336 139865257662208 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.817282199859619, loss=6.20329475402832
I0127 13:17:36.527235 139865266054912 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.8448063135147095, loss=6.183078289031982
I0127 13:18:10.593203 139865257662208 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.5183876752853394, loss=6.07139778137207
I0127 13:18:44.661483 139865266054912 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.4006056785583496, loss=6.038412094116211
I0127 13:19:18.715812 139865257662208 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.6544429063796997, loss=5.952714920043945
I0127 13:19:52.262413 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:19:58.465118 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:20:07.406244 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:20:09.862455 140027215431488 submission_runner.py:408] Time since start: 579.08s, 	Step: 1500, 	{'train/accuracy': 0.0859375, 'train/loss': 5.226979732513428, 'validation/accuracy': 0.07903999835252762, 'validation/loss': 5.291569709777832, 'validation/num_examples': 50000, 'test/accuracy': 0.057100001722574234, 'test/loss': 5.528391361236572, 'test/num_examples': 10000, 'score': 543.6612985134125, 'total_duration': 579.0811305046082, 'accumulated_submission_time': 543.6612985134125, 'accumulated_eval_time': 35.347055435180664, 'accumulated_logging_time': 0.019211769104003906}
I0127 13:20:09.886530 139865257662208 logging_writer.py:48] [1500] accumulated_eval_time=35.347055, accumulated_logging_time=0.019212, accumulated_submission_time=543.661299, global_step=1500, preemption_count=0, score=543.661299, test/accuracy=0.057100, test/loss=5.528391, test/num_examples=10000, total_duration=579.081131, train/accuracy=0.085938, train/loss=5.226980, validation/accuracy=0.079040, validation/loss=5.291570, validation/num_examples=50000
I0127 13:20:10.248706 139865266054912 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.959661602973938, loss=5.9481353759765625
I0127 13:20:44.285118 139865257662208 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.3317549228668213, loss=5.816254615783691
I0127 13:21:18.314673 139865266054912 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8907005786895752, loss=5.765367031097412
I0127 13:21:52.334598 139865257662208 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.838040828704834, loss=5.765199184417725
I0127 13:22:26.373131 139865266054912 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.6439666748046875, loss=5.7672343254089355
I0127 13:23:00.455650 139865257662208 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.3509368896484375, loss=5.650681495666504
I0127 13:23:34.569968 139865266054912 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.612297773361206, loss=5.580434799194336
I0127 13:24:08.657007 139865257662208 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.013200521469116, loss=5.589503765106201
I0127 13:24:42.702855 139865266054912 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.0934865474700928, loss=5.533934116363525
I0127 13:25:16.753628 139865257662208 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.9833619594573975, loss=5.486428737640381
I0127 13:25:50.815472 139865266054912 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.7079050540924072, loss=5.440854072570801
I0127 13:26:24.887152 139865257662208 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.6773831844329834, loss=5.357600688934326
I0127 13:26:58.963328 139865266054912 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.135329246520996, loss=5.476648807525635
I0127 13:27:33.038591 139865257662208 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.6910669803619385, loss=5.3183465003967285
I0127 13:28:07.099231 139865266054912 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.265791893005371, loss=5.285224914550781
I0127 13:28:39.936531 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:28:46.183612 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:28:55.115669 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:28:57.628893 140027215431488 submission_runner.py:408] Time since start: 1106.85s, 	Step: 2998, 	{'train/accuracy': 0.1965680718421936, 'train/loss': 4.198776721954346, 'validation/accuracy': 0.17921999096870422, 'validation/loss': 4.319735050201416, 'validation/num_examples': 50000, 'test/accuracy': 0.12939999997615814, 'test/loss': 4.724494457244873, 'test/num_examples': 10000, 'score': 1053.6502876281738, 'total_duration': 1106.8475799560547, 'accumulated_submission_time': 1053.6502876281738, 'accumulated_eval_time': 53.0393807888031, 'accumulated_logging_time': 0.0525212287902832}
I0127 13:28:57.646497 139865266054912 logging_writer.py:48] [2998] accumulated_eval_time=53.039381, accumulated_logging_time=0.052521, accumulated_submission_time=1053.650288, global_step=2998, preemption_count=0, score=1053.650288, test/accuracy=0.129400, test/loss=4.724494, test/num_examples=10000, total_duration=1106.847580, train/accuracy=0.196568, train/loss=4.198777, validation/accuracy=0.179220, validation/loss=4.319735, validation/num_examples=50000
I0127 13:28:58.667101 139865769342720 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.1579699516296387, loss=5.22659158706665
I0127 13:29:32.813030 139865266054912 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.96059513092041, loss=5.1831254959106445
I0127 13:30:06.854610 139865769342720 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.979343891143799, loss=5.167162895202637
I0127 13:30:40.948044 139865266054912 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.5053460597991943, loss=5.210640907287598
I0127 13:31:14.993973 139865769342720 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.933586597442627, loss=5.1316914558410645
I0127 13:31:49.058374 139865266054912 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.5064034461975098, loss=5.096317291259766
I0127 13:32:23.118220 139865769342720 logging_writer.py:48] [3600] global_step=3600, grad_norm=8.03072452545166, loss=5.085534572601318
I0127 13:32:57.212419 139865266054912 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.628176689147949, loss=4.940948963165283
I0127 13:33:31.292129 139865769342720 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.0432794094085693, loss=4.969366550445557
I0127 13:34:05.362563 139865266054912 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.5225281715393066, loss=4.908683776855469
I0127 13:34:39.458938 139865769342720 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.295248508453369, loss=4.906159400939941
I0127 13:35:13.526799 139865266054912 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.8743185997009277, loss=4.830804824829102
I0127 13:35:47.647296 139865769342720 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.828084945678711, loss=4.707897186279297
I0127 13:36:21.700846 139865266054912 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.3892831802368164, loss=4.782503604888916
I0127 13:36:55.796728 139865769342720 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.408740758895874, loss=4.8064374923706055
I0127 13:37:27.937035 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:37:34.983753 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:37:43.675282 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:37:46.089522 140027215431488 submission_runner.py:408] Time since start: 1635.31s, 	Step: 4496, 	{'train/accuracy': 0.3117426633834839, 'train/loss': 3.379237413406372, 'validation/accuracy': 0.28421998023986816, 'validation/loss': 3.520533323287964, 'validation/num_examples': 50000, 'test/accuracy': 0.20760001242160797, 'test/loss': 4.096458911895752, 'test/num_examples': 10000, 'score': 1563.8795804977417, 'total_duration': 1635.308201789856, 'accumulated_submission_time': 1563.8795804977417, 'accumulated_eval_time': 71.19184017181396, 'accumulated_logging_time': 0.07930827140808105}
I0127 13:37:46.107388 139866163582720 logging_writer.py:48] [4496] accumulated_eval_time=71.191840, accumulated_logging_time=0.079308, accumulated_submission_time=1563.879580, global_step=4496, preemption_count=0, score=1563.879580, test/accuracy=0.207600, test/loss=4.096459, test/num_examples=10000, total_duration=1635.308202, train/accuracy=0.311743, train/loss=3.379237, validation/accuracy=0.284220, validation/loss=3.520533, validation/num_examples=50000
I0127 13:37:47.816629 139866171975424 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.2733795642852783, loss=4.798628330230713
I0127 13:38:21.869523 139866163582720 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.0479979515075684, loss=4.673756122589111
I0127 13:38:55.952257 139866171975424 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.984020233154297, loss=4.702731609344482
I0127 13:39:30.016326 139866163582720 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.3900909423828125, loss=4.736274242401123
I0127 13:40:04.072707 139866171975424 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.903599739074707, loss=4.603694438934326
I0127 13:40:38.143674 139866163582720 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.717613697052002, loss=4.701837539672852
I0127 13:41:12.206353 139866171975424 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.8054358959198, loss=4.5487871170043945
I0127 13:41:46.266446 139866163582720 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.031942844390869, loss=4.680272102355957
I0127 13:42:20.421788 139866171975424 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.373605728149414, loss=4.451186180114746
I0127 13:42:54.506800 139866163582720 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.9522151947021484, loss=4.550703525543213
I0127 13:43:28.561135 139866171975424 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.538578748703003, loss=4.387358665466309
I0127 13:44:02.611772 139866163582720 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.128976821899414, loss=4.38817024230957
I0127 13:44:36.689586 139866171975424 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.88319730758667, loss=4.3631744384765625
I0127 13:45:10.769633 139866163582720 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.977499008178711, loss=4.3480939865112305
I0127 13:45:44.838848 139866171975424 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.9817323684692383, loss=4.430764198303223
I0127 13:46:16.302266 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:46:22.487575 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:46:31.358196 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:46:33.852986 140027215431488 submission_runner.py:408] Time since start: 2163.07s, 	Step: 5994, 	{'train/accuracy': 0.3806600570678711, 'train/loss': 2.963285207748413, 'validation/accuracy': 0.3558799922466278, 'validation/loss': 3.0989913940429688, 'validation/num_examples': 50000, 'test/accuracy': 0.2656000256538391, 'test/loss': 3.724989652633667, 'test/num_examples': 10000, 'score': 2074.014079093933, 'total_duration': 2163.0716738700867, 'accumulated_submission_time': 2074.014079093933, 'accumulated_eval_time': 88.74252462387085, 'accumulated_logging_time': 0.10614848136901855}
I0127 13:46:33.871069 139865266054912 logging_writer.py:48] [5994] accumulated_eval_time=88.742525, accumulated_logging_time=0.106148, accumulated_submission_time=2074.014079, global_step=5994, preemption_count=0, score=2074.014079, test/accuracy=0.265600, test/loss=3.724990, test/num_examples=10000, total_duration=2163.071674, train/accuracy=0.380660, train/loss=2.963285, validation/accuracy=0.355880, validation/loss=3.098991, validation/num_examples=50000
I0127 13:46:36.246576 139865274447616 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.783310651779175, loss=4.277338981628418
I0127 13:47:10.272346 139865266054912 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.5388829708099365, loss=4.314688205718994
I0127 13:47:44.291921 139865274447616 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.090261936187744, loss=4.4028215408325195
I0127 13:48:18.445333 139865266054912 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.7518670558929443, loss=4.3215789794921875
I0127 13:48:52.490353 139865274447616 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.191521167755127, loss=4.305473327636719
I0127 13:49:26.544332 139865266054912 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.4104928970336914, loss=4.255900859832764
I0127 13:50:00.581813 139865274447616 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.7659027576446533, loss=4.165308475494385
I0127 13:50:34.632421 139865266054912 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.812657117843628, loss=4.199741363525391
I0127 13:51:08.698706 139865274447616 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.137596607208252, loss=4.18941593170166
I0127 13:51:42.756747 139865266054912 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.1851115226745605, loss=4.195342540740967
I0127 13:52:16.788958 139865274447616 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.721723794937134, loss=4.236139297485352
I0127 13:52:50.846387 139865266054912 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.6995785236358643, loss=4.140105247497559
I0127 13:53:24.888382 139865274447616 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.9650845527648926, loss=4.103418350219727
I0127 13:53:58.919883 139865266054912 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.5640435218811035, loss=4.131466388702393
I0127 13:54:33.032456 139865274447616 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.965294361114502, loss=4.06135892868042
I0127 13:55:04.133617 140027215431488 spec.py:321] Evaluating on the training split.
I0127 13:55:10.307141 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 13:55:19.120208 140027215431488 spec.py:349] Evaluating on the test split.
I0127 13:55:21.605281 140027215431488 submission_runner.py:408] Time since start: 2690.82s, 	Step: 7493, 	{'train/accuracy': 0.46181440353393555, 'train/loss': 2.6370625495910645, 'validation/accuracy': 0.4315599799156189, 'validation/loss': 2.7958452701568604, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.3746931552886963, 'test/num_examples': 10000, 'score': 2584.21363902092, 'total_duration': 2690.823953151703, 'accumulated_submission_time': 2584.21363902092, 'accumulated_eval_time': 106.2141387462616, 'accumulated_logging_time': 0.1336226463317871}
I0127 13:55:21.623988 139865274447616 logging_writer.py:48] [7493] accumulated_eval_time=106.214139, accumulated_logging_time=0.133623, accumulated_submission_time=2584.213639, global_step=7493, preemption_count=0, score=2584.213639, test/accuracy=0.331500, test/loss=3.374693, test/num_examples=10000, total_duration=2690.823953, train/accuracy=0.461814, train/loss=2.637063, validation/accuracy=0.431560, validation/loss=2.795845, validation/num_examples=50000
I0127 13:55:24.369486 139866163582720 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8715444803237915, loss=4.156772613525391
I0127 13:55:58.385790 139865274447616 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.973327398300171, loss=4.0305633544921875
I0127 13:56:32.370685 139866163582720 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.203333616256714, loss=4.05238151550293
I0127 13:57:06.379839 139865274447616 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.2437291145324707, loss=4.049473285675049
I0127 13:57:40.416658 139866163582720 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.060523748397827, loss=4.087780952453613
I0127 13:58:14.443791 139865274447616 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.6862921714782715, loss=4.031502723693848
I0127 13:58:48.470125 139866163582720 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.975359559059143, loss=4.06905460357666
I0127 13:59:22.521519 139865274447616 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.169049024581909, loss=3.969695568084717
I0127 13:59:56.561754 139866163582720 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5104799270629883, loss=3.986131429672241
I0127 14:00:30.654938 139865274447616 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.3460593223571777, loss=4.009416580200195
I0127 14:01:04.691872 139866163582720 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.099159002304077, loss=4.0937700271606445
I0127 14:01:38.713836 139865274447616 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.820676326751709, loss=3.9934957027435303
I0127 14:02:12.735612 139866163582720 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.623004913330078, loss=3.9717936515808105
I0127 14:02:46.751206 139865274447616 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.75394868850708, loss=3.9955759048461914
I0127 14:03:20.753307 139866163582720 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.9644407033920288, loss=3.8600869178771973
I0127 14:03:51.850237 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:03:58.004190 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:04:06.697567 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:04:09.131537 140027215431488 submission_runner.py:408] Time since start: 3218.35s, 	Step: 8993, 	{'train/accuracy': 0.5307716727256775, 'train/loss': 2.255420207977295, 'validation/accuracy': 0.47283998131752014, 'validation/loss': 2.5475475788116455, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.1709978580474854, 'test/num_examples': 10000, 'score': 3094.3763308525085, 'total_duration': 3218.3501625061035, 'accumulated_submission_time': 3094.3763308525085, 'accumulated_eval_time': 123.49534726142883, 'accumulated_logging_time': 0.1627497673034668}
I0127 14:04:09.150921 139865769342720 logging_writer.py:48] [8993] accumulated_eval_time=123.495347, accumulated_logging_time=0.162750, accumulated_submission_time=3094.376331, global_step=8993, preemption_count=0, score=3094.376331, test/accuracy=0.361300, test/loss=3.170998, test/num_examples=10000, total_duration=3218.350163, train/accuracy=0.530772, train/loss=2.255420, validation/accuracy=0.472840, validation/loss=2.547548, validation/num_examples=50000
I0127 14:04:11.881619 139866188760832 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.251332998275757, loss=4.093277454376221
I0127 14:04:45.876544 139865769342720 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.982479453086853, loss=3.8803467750549316
I0127 14:05:19.873264 139866188760832 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.40441632270813, loss=3.977853536605835
I0127 14:05:53.880636 139865769342720 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.363640546798706, loss=3.9205541610717773
I0127 14:06:27.916970 139866188760832 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.815075635910034, loss=3.8734776973724365
I0127 14:07:02.016416 139865769342720 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.4761908054351807, loss=3.903775930404663
I0127 14:07:36.056913 139866188760832 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.383840799331665, loss=3.822542428970337
I0127 14:08:10.061935 139865769342720 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8742512464523315, loss=3.879289388656616
I0127 14:08:44.073225 139866188760832 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.7513550519943237, loss=3.8089122772216797
I0127 14:09:18.082310 139865769342720 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.125103235244751, loss=3.8441109657287598
I0127 14:09:52.097702 139866188760832 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8864250183105469, loss=3.8010079860687256
I0127 14:10:26.100481 139865769342720 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.2774081230163574, loss=3.8862929344177246
I0127 14:11:00.100955 139866188760832 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.8159290552139282, loss=3.800708770751953
I0127 14:11:34.114624 139865769342720 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.6595321893692017, loss=3.8560380935668945
I0127 14:12:08.092800 139866188760832 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.4554481506347656, loss=3.7768237590789795
I0127 14:12:39.174612 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:12:45.390660 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:12:54.363844 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:12:56.779794 140027215431488 submission_runner.py:408] Time since start: 3746.00s, 	Step: 10493, 	{'train/accuracy': 0.5577766299247742, 'train/loss': 2.1650500297546387, 'validation/accuracy': 0.5104199647903442, 'validation/loss': 2.3915514945983887, 'validation/num_examples': 50000, 'test/accuracy': 0.38860002160072327, 'test/loss': 3.049933433532715, 'test/num_examples': 10000, 'score': 3604.3356716632843, 'total_duration': 3745.998475790024, 'accumulated_submission_time': 3604.3356716632843, 'accumulated_eval_time': 141.10048961639404, 'accumulated_logging_time': 0.19440197944641113}
I0127 14:12:56.799666 139866171975424 logging_writer.py:48] [10493] accumulated_eval_time=141.100490, accumulated_logging_time=0.194402, accumulated_submission_time=3604.335672, global_step=10493, preemption_count=0, score=3604.335672, test/accuracy=0.388600, test/loss=3.049933, test/num_examples=10000, total_duration=3745.998476, train/accuracy=0.557777, train/loss=2.165050, validation/accuracy=0.510420, validation/loss=2.391551, validation/num_examples=50000
I0127 14:12:59.532628 139866180368128 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.84625244140625, loss=3.8330512046813965
I0127 14:13:33.620385 139866171975424 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.6056920289993286, loss=3.7426068782806396
I0127 14:14:07.615791 139866180368128 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.6848807334899902, loss=3.74649715423584
I0127 14:14:41.666797 139866171975424 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.0135750770568848, loss=3.73227858543396
I0127 14:15:15.635052 139866180368128 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.7307389974594116, loss=3.8317151069641113
I0127 14:15:49.647893 139866171975424 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9209074974060059, loss=3.8175876140594482
I0127 14:16:23.640799 139866180368128 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.70702064037323, loss=3.6735928058624268
I0127 14:16:57.663443 139866171975424 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.4434592723846436, loss=3.7231671810150146
I0127 14:17:31.646884 139866180368128 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.7541216611862183, loss=3.6885294914245605
I0127 14:18:05.650249 139866171975424 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.741866946220398, loss=3.7071733474731445
I0127 14:18:39.647604 139866180368128 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.3299214839935303, loss=3.7481279373168945
I0127 14:19:13.847841 139866171975424 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.80256986618042, loss=3.773294448852539
I0127 14:19:47.837591 139866180368128 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.7776256799697876, loss=3.703380584716797
I0127 14:20:21.851884 139866171975424 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.46213698387146, loss=3.7810757160186768
I0127 14:20:55.855923 139866180368128 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.3874132633209229, loss=3.7407453060150146
I0127 14:21:26.930783 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:21:33.202074 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:21:42.021354 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:21:44.600859 140027215431488 submission_runner.py:408] Time since start: 4273.82s, 	Step: 11993, 	{'train/accuracy': 0.6017418503761292, 'train/loss': 1.9433568716049194, 'validation/accuracy': 0.5548200011253357, 'validation/loss': 2.1569015979766846, 'validation/num_examples': 50000, 'test/accuracy': 0.4271000325679779, 'test/loss': 2.8042361736297607, 'test/num_examples': 10000, 'score': 4114.402346134186, 'total_duration': 4273.819544792175, 'accumulated_submission_time': 4114.402346134186, 'accumulated_eval_time': 158.77054691314697, 'accumulated_logging_time': 0.22590017318725586}
I0127 14:21:44.620788 139865266054912 logging_writer.py:48] [11993] accumulated_eval_time=158.770547, accumulated_logging_time=0.225900, accumulated_submission_time=4114.402346, global_step=11993, preemption_count=0, score=4114.402346, test/accuracy=0.427100, test/loss=2.804236, test/num_examples=10000, total_duration=4273.819545, train/accuracy=0.601742, train/loss=1.943357, validation/accuracy=0.554820, validation/loss=2.156902, validation/num_examples=50000
I0127 14:21:47.343753 139865274447616 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.189279556274414, loss=3.656520128250122
I0127 14:22:21.331579 139865266054912 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.844885230064392, loss=3.612301826477051
I0127 14:22:55.318968 139865274447616 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.492637276649475, loss=3.6459736824035645
I0127 14:23:29.305076 139865266054912 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.6965644359588623, loss=3.682189464569092
I0127 14:24:03.296412 139865274447616 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.66074538230896, loss=3.6275343894958496
I0127 14:24:37.292361 139865266054912 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.495323419570923, loss=3.63682222366333
I0127 14:25:11.285707 139865274447616 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.7321548461914062, loss=3.6873111724853516
I0127 14:25:45.393916 139865266054912 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.7656084299087524, loss=3.623706817626953
I0127 14:26:19.402229 139865274447616 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.8037772178649902, loss=3.6265246868133545
I0127 14:26:53.388370 139865266054912 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.4403154850006104, loss=3.588890314102173
I0127 14:27:27.365102 139865274447616 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.5297610759735107, loss=3.5683679580688477
I0127 14:28:01.351511 139865266054912 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.479582667350769, loss=3.607245683670044
I0127 14:28:35.367861 139865274447616 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.6811413764953613, loss=3.64089298248291
I0127 14:29:09.350901 139865266054912 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.1070995330810547, loss=3.6464905738830566
I0127 14:29:43.352799 139865274447616 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.9007209539413452, loss=3.631727933883667
I0127 14:30:14.756273 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:30:21.019318 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:30:29.594033 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:30:32.079833 140027215431488 submission_runner.py:408] Time since start: 4801.30s, 	Step: 13494, 	{'train/accuracy': 0.6224888563156128, 'train/loss': 1.774627685546875, 'validation/accuracy': 0.5730400085449219, 'validation/loss': 2.0085930824279785, 'validation/num_examples': 50000, 'test/accuracy': 0.45100003480911255, 'test/loss': 2.655189275741577, 'test/num_examples': 10000, 'score': 4624.474926710129, 'total_duration': 4801.298516511917, 'accumulated_submission_time': 4624.474926710129, 'accumulated_eval_time': 176.094083070755, 'accumulated_logging_time': 0.25512123107910156}
I0127 14:30:32.100636 139865257662208 logging_writer.py:48] [13494] accumulated_eval_time=176.094083, accumulated_logging_time=0.255121, accumulated_submission_time=4624.474927, global_step=13494, preemption_count=0, score=4624.474927, test/accuracy=0.451000, test/loss=2.655189, test/num_examples=10000, total_duration=4801.298517, train/accuracy=0.622489, train/loss=1.774628, validation/accuracy=0.573040, validation/loss=2.008593, validation/num_examples=50000
I0127 14:30:34.479532 139866163582720 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.926206111907959, loss=3.6114485263824463
I0127 14:31:08.470670 139865257662208 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.5360682010650635, loss=3.562244415283203
I0127 14:31:42.440706 139866163582720 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.554212212562561, loss=3.5136990547180176
I0127 14:32:16.490967 139865257662208 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.8319225311279297, loss=3.5766444206237793
I0127 14:32:50.498294 139866163582720 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4362283945083618, loss=3.5468287467956543
I0127 14:33:24.493145 139865257662208 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6659568548202515, loss=3.5295348167419434
I0127 14:33:58.482619 139866163582720 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.747521162033081, loss=3.5691723823547363
I0127 14:34:32.491869 139865257662208 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.065918445587158, loss=3.5170252323150635
I0127 14:35:06.473204 139866163582720 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.4880906343460083, loss=3.6058642864227295
I0127 14:35:40.490323 139865257662208 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4771082401275635, loss=3.5463547706604004
I0127 14:36:14.475033 139866163582720 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4894921779632568, loss=3.520411491394043
I0127 14:36:48.466499 139865257662208 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.5999901294708252, loss=3.520022392272949
I0127 14:37:22.450441 139866163582720 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.3843801021575928, loss=3.463959217071533
I0127 14:37:56.444877 139865257662208 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.9912605285644531, loss=3.5738894939422607
I0127 14:38:30.486916 139866163582720 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.4332629442214966, loss=3.5040552616119385
I0127 14:39:02.259297 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:39:08.547977 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:39:17.519135 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:39:20.078085 140027215431488 submission_runner.py:408] Time since start: 5329.30s, 	Step: 14995, 	{'train/accuracy': 0.6300023794174194, 'train/loss': 1.7994438409805298, 'validation/accuracy': 0.5817999839782715, 'validation/loss': 2.0272626876831055, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.6749420166015625, 'test/num_examples': 10000, 'score': 5134.568528413773, 'total_duration': 5329.296462774277, 'accumulated_submission_time': 5134.568528413773, 'accumulated_eval_time': 193.91254234313965, 'accumulated_logging_time': 0.2859010696411133}
I0127 14:39:20.097061 139865274447616 logging_writer.py:48] [14995] accumulated_eval_time=193.912542, accumulated_logging_time=0.285901, accumulated_submission_time=5134.568528, global_step=14995, preemption_count=0, score=5134.568528, test/accuracy=0.456900, test/loss=2.674942, test/num_examples=10000, total_duration=5329.296463, train/accuracy=0.630002, train/loss=1.799444, validation/accuracy=0.581800, validation/loss=2.027263, validation/num_examples=50000
I0127 14:39:22.127898 139865760950016 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.464027762413025, loss=3.4881439208984375
I0127 14:39:56.053462 139865274447616 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.7255364656448364, loss=3.6187973022460938
I0127 14:40:29.987303 139865760950016 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.6509265899658203, loss=3.5663418769836426
I0127 14:41:03.980711 139865274447616 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.1351008415222168, loss=3.5531692504882812
I0127 14:41:37.946441 139865760950016 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.0108261108398438, loss=3.564537525177002
I0127 14:42:11.896693 139865274447616 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.2679301500320435, loss=3.561504602432251
I0127 14:42:45.850668 139865760950016 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.6017749309539795, loss=3.5306639671325684
I0127 14:43:19.814041 139865274447616 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.439102292060852, loss=3.451463460922241
I0127 14:43:53.788827 139865760950016 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.734608769416809, loss=3.4651894569396973
I0127 14:44:27.791465 139865274447616 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.2241194248199463, loss=3.4821324348449707
I0127 14:45:01.753588 139865760950016 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.4430419206619263, loss=3.501587390899658
I0127 14:45:35.728779 139865274447616 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.4494479894638062, loss=3.4880166053771973
I0127 14:46:09.701009 139865760950016 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.7944750785827637, loss=3.51279878616333
I0127 14:46:43.651592 139865274447616 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.7955409288406372, loss=3.4748661518096924
I0127 14:47:17.614977 139865760950016 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.589371919631958, loss=3.588827610015869
I0127 14:47:50.365666 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:47:56.537172 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:48:05.441204 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:48:07.873055 140027215431488 submission_runner.py:408] Time since start: 5857.09s, 	Step: 16498, 	{'train/accuracy': 0.6436144709587097, 'train/loss': 1.6748861074447632, 'validation/accuracy': 0.5989199876785278, 'validation/loss': 1.8900413513183594, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.5429165363311768, 'test/num_examples': 10000, 'score': 5644.774896144867, 'total_duration': 5857.091723680496, 'accumulated_submission_time': 5644.774896144867, 'accumulated_eval_time': 211.41987991333008, 'accumulated_logging_time': 0.31459641456604004}
I0127 14:48:07.894757 139866180368128 logging_writer.py:48] [16498] accumulated_eval_time=211.419880, accumulated_logging_time=0.314596, accumulated_submission_time=5644.774896, global_step=16498, preemption_count=0, score=5644.774896, test/accuracy=0.473500, test/loss=2.542917, test/num_examples=10000, total_duration=5857.091724, train/accuracy=0.643614, train/loss=1.674886, validation/accuracy=0.598920, validation/loss=1.890041, validation/num_examples=50000
I0127 14:48:08.918188 139866188760832 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.3203967809677124, loss=3.4310669898986816
I0127 14:48:42.882838 139866180368128 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.1914167404174805, loss=3.536330223083496
I0127 14:49:16.814486 139866188760832 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.6549789905548096, loss=3.460073947906494
I0127 14:49:50.768928 139866180368128 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.031796932220459, loss=3.404435634613037
I0127 14:50:24.722456 139866188760832 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.4981098175048828, loss=3.5074961185455322
I0127 14:50:58.729510 139866180368128 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.6441831588745117, loss=3.45416522026062
I0127 14:51:32.696193 139866188760832 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.3473820686340332, loss=3.4026129245758057
I0127 14:52:06.679902 139866180368128 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.6363905668258667, loss=3.499967575073242
I0127 14:52:40.634110 139866188760832 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.6395018100738525, loss=3.428873062133789
I0127 14:53:14.613950 139866180368128 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.4629666805267334, loss=3.448240041732788
I0127 14:53:48.588041 139866188760832 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.706171989440918, loss=3.464916944503784
I0127 14:54:22.539665 139866180368128 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.518593430519104, loss=3.4982075691223145
I0127 14:54:56.502894 139866188760832 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.4969204664230347, loss=3.4235994815826416
I0127 14:55:30.454535 139866180368128 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.4762022495269775, loss=3.541314125061035
I0127 14:56:04.417733 139866188760832 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.2350925207138062, loss=3.367133617401123
I0127 14:56:38.407837 139866180368128 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.6328473091125488, loss=3.479475736618042
I0127 14:56:38.416523 140027215431488 spec.py:321] Evaluating on the training split.
I0127 14:56:44.611553 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 14:56:53.488557 140027215431488 spec.py:349] Evaluating on the test split.
I0127 14:56:55.869155 140027215431488 submission_runner.py:408] Time since start: 6385.09s, 	Step: 18001, 	{'train/accuracy': 0.7017298936843872, 'train/loss': 1.4178893566131592, 'validation/accuracy': 0.6070199608802795, 'validation/loss': 1.8295985460281372, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.4914474487304688, 'test/num_examples': 10000, 'score': 6155.2353079319, 'total_duration': 6385.0878365039825, 'accumulated_submission_time': 6155.2353079319, 'accumulated_eval_time': 228.8724648952484, 'accumulated_logging_time': 0.3461291790008545}
I0127 14:56:55.890964 139865760950016 logging_writer.py:48] [18001] accumulated_eval_time=228.872465, accumulated_logging_time=0.346129, accumulated_submission_time=6155.235308, global_step=18001, preemption_count=0, score=6155.235308, test/accuracy=0.479700, test/loss=2.491447, test/num_examples=10000, total_duration=6385.087837, train/accuracy=0.701730, train/loss=1.417889, validation/accuracy=0.607020, validation/loss=1.829599, validation/num_examples=50000
I0127 14:57:29.974907 139865769342720 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.407893419265747, loss=3.4484686851501465
I0127 14:58:03.898679 139865760950016 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.8181356191635132, loss=3.4372830390930176
I0127 14:58:37.872773 139865769342720 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.832348346710205, loss=3.4802029132843018
I0127 14:59:11.862518 139865760950016 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.7247886657714844, loss=3.359511137008667
I0127 14:59:45.823940 139865769342720 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.17536199092865, loss=3.4382097721099854
I0127 15:00:19.783183 139865760950016 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.4335534572601318, loss=3.4115538597106934
I0127 15:00:53.761445 139865769342720 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0753273963928223, loss=3.422968626022339
I0127 15:01:27.710125 139865760950016 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.6997528076171875, loss=3.4389684200286865
I0127 15:02:01.690268 139865769342720 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.590854525566101, loss=3.3985238075256348
I0127 15:02:35.642904 139865760950016 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.4269139766693115, loss=3.4108381271362305
I0127 15:03:09.677053 139865769342720 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5875850915908813, loss=3.4054341316223145
I0127 15:03:43.608120 139865760950016 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7775001525878906, loss=3.4415202140808105
I0127 15:04:17.568157 139865769342720 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.8958560228347778, loss=3.4490997791290283
I0127 15:04:51.546101 139865760950016 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.32442045211792, loss=3.4072940349578857
I0127 15:05:25.530566 139865769342720 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.2091134786605835, loss=3.340524911880493
I0127 15:05:26.008081 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:05:32.259061 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:05:40.945301 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:05:43.402000 140027215431488 submission_runner.py:408] Time since start: 6912.62s, 	Step: 19503, 	{'train/accuracy': 0.6970065236091614, 'train/loss': 1.4243632555007935, 'validation/accuracy': 0.6238399744033813, 'validation/loss': 1.7519325017929077, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.404106855392456, 'test/num_examples': 10000, 'score': 6665.291936635971, 'total_duration': 6912.620675325394, 'accumulated_submission_time': 6665.291936635971, 'accumulated_eval_time': 246.26633167266846, 'accumulated_logging_time': 0.3774294853210449}
I0127 15:05:43.423453 139865274447616 logging_writer.py:48] [19503] accumulated_eval_time=246.266332, accumulated_logging_time=0.377429, accumulated_submission_time=6665.291937, global_step=19503, preemption_count=0, score=6665.291937, test/accuracy=0.496800, test/loss=2.404107, test/num_examples=10000, total_duration=6912.620675, train/accuracy=0.697007, train/loss=1.424363, validation/accuracy=0.623840, validation/loss=1.751933, validation/num_examples=50000
I0127 15:06:16.696713 139865760950016 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.408521294593811, loss=3.360982656478882
I0127 15:06:50.668777 139865274447616 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.2671185731887817, loss=3.383514642715454
I0127 15:07:24.615139 139865760950016 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.7346365451812744, loss=3.3407039642333984
I0127 15:07:58.570479 139865274447616 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.1929768323898315, loss=3.4916181564331055
I0127 15:08:32.539498 139865760950016 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.520685076713562, loss=3.4005038738250732
I0127 15:09:06.506902 139865274447616 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.461134672164917, loss=3.37064266204834
I0127 15:09:40.527251 139865760950016 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1820093393325806, loss=3.3986480236053467
I0127 15:10:14.491614 139865274447616 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.778120756149292, loss=3.4212257862091064
I0127 15:10:48.481852 139865760950016 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.455979585647583, loss=3.4865059852600098
I0127 15:11:22.470295 139865274447616 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.3851606845855713, loss=3.4446139335632324
I0127 15:11:56.438427 139865760950016 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.868345856666565, loss=3.455677032470703
I0127 15:12:30.394479 139865274447616 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.5009790658950806, loss=3.3905625343322754
I0127 15:13:04.374216 139865760950016 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.3624184131622314, loss=3.3966808319091797
I0127 15:13:38.331802 139865274447616 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.5869061946868896, loss=3.3663570880889893
I0127 15:14:12.310830 139865760950016 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.7370398044586182, loss=3.3986682891845703
I0127 15:14:13.479193 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:14:19.752946 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:14:28.723806 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:14:31.721154 140027215431488 submission_runner.py:408] Time since start: 7440.94s, 	Step: 21005, 	{'train/accuracy': 0.6917251348495483, 'train/loss': 1.4819576740264893, 'validation/accuracy': 0.6297399997711182, 'validation/loss': 1.7615430355072021, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.432694673538208, 'test/num_examples': 10000, 'score': 7175.285215139389, 'total_duration': 7440.939846038818, 'accumulated_submission_time': 7175.285215139389, 'accumulated_eval_time': 264.5082576274872, 'accumulated_logging_time': 0.4087963104248047}
I0127 15:14:31.743354 139866180368128 logging_writer.py:48] [21005] accumulated_eval_time=264.508258, accumulated_logging_time=0.408796, accumulated_submission_time=7175.285215, global_step=21005, preemption_count=0, score=7175.285215, test/accuracy=0.495900, test/loss=2.432695, test/num_examples=10000, total_duration=7440.939846, train/accuracy=0.691725, train/loss=1.481958, validation/accuracy=0.629740, validation/loss=1.761543, validation/num_examples=50000
I0127 15:15:04.312453 139866188760832 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.3548682928085327, loss=3.375643730163574
I0127 15:15:38.230681 139866180368128 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.4068126678466797, loss=3.371943235397339
I0127 15:16:12.355297 139866188760832 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.056826591491699, loss=3.3534936904907227
I0127 15:16:46.344569 139866180368128 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6169342994689941, loss=3.381478786468506
I0127 15:17:20.294580 139866188760832 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.5043524503707886, loss=3.401853084564209
I0127 15:17:54.250456 139866180368128 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.4274921417236328, loss=3.3700318336486816
I0127 15:18:28.223268 139866188760832 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.389331579208374, loss=3.29329514503479
I0127 15:19:02.174111 139866180368128 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.1395741701126099, loss=3.3227202892303467
I0127 15:19:36.139972 139866188760832 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.3824087381362915, loss=3.314143180847168
I0127 15:20:10.149357 139866180368128 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.3744641542434692, loss=3.36167049407959
I0127 15:20:44.122245 139866188760832 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3448172807693481, loss=3.4306821823120117
I0127 15:21:18.083451 139866180368128 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.4297646284103394, loss=3.358037233352661
I0127 15:21:52.047479 139866188760832 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.4749953746795654, loss=3.286766529083252
I0127 15:22:26.143524 139866180368128 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.3365387916564941, loss=3.359713077545166
I0127 15:23:00.104233 139866188760832 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.5295180082321167, loss=3.3090627193450928
I0127 15:23:01.949225 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:23:08.102373 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:23:16.749058 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:23:19.220291 140027215431488 submission_runner.py:408] Time since start: 7968.44s, 	Step: 22507, 	{'train/accuracy': 0.6911072731018066, 'train/loss': 1.4709314107894897, 'validation/accuracy': 0.629539966583252, 'validation/loss': 1.7492947578430176, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.40031361579895, 'test/num_examples': 10000, 'score': 7685.427667379379, 'total_duration': 7968.438979625702, 'accumulated_submission_time': 7685.427667379379, 'accumulated_eval_time': 281.779283285141, 'accumulated_logging_time': 0.4405534267425537}
I0127 15:23:19.242474 139865257662208 logging_writer.py:48] [22507] accumulated_eval_time=281.779283, accumulated_logging_time=0.440553, accumulated_submission_time=7685.427667, global_step=22507, preemption_count=0, score=7685.427667, test/accuracy=0.503200, test/loss=2.400314, test/num_examples=10000, total_duration=7968.438980, train/accuracy=0.691107, train/loss=1.470931, validation/accuracy=0.629540, validation/loss=1.749295, validation/num_examples=50000
I0127 15:23:51.145344 139865266054912 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.4319353103637695, loss=3.3461947441101074
I0127 15:24:25.093753 139865257662208 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.164867877960205, loss=3.32462477684021
I0127 15:24:59.053983 139865266054912 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.3235464096069336, loss=3.3462841510772705
I0127 15:25:33.028050 139865257662208 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0470530986785889, loss=3.319387674331665
I0127 15:26:06.980704 139865266054912 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.3472718000411987, loss=3.301405429840088
I0127 15:26:40.948271 139865257662208 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.2961214780807495, loss=3.335448741912842
I0127 15:27:14.911286 139865266054912 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.4982378482818604, loss=3.335463762283325
I0127 15:27:48.882532 139865257662208 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.1521527767181396, loss=3.404559373855591
I0127 15:28:22.974246 139865266054912 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.1275434494018555, loss=3.345951557159424
I0127 15:28:56.920352 139865257662208 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.3855535984039307, loss=3.4105358123779297
I0127 15:29:30.891995 139865266054912 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.190352439880371, loss=3.3418900966644287
I0127 15:30:04.867120 139865257662208 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.2109791040420532, loss=3.34993839263916
I0127 15:30:38.833214 139865266054912 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.8472504615783691, loss=3.3843448162078857
I0127 15:31:12.802718 139865257662208 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.2917914390563965, loss=3.2984228134155273
I0127 15:31:46.737285 139865266054912 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2702786922454834, loss=3.3305952548980713
I0127 15:31:49.273920 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:31:55.590501 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:32:04.594505 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:32:07.075801 140027215431488 submission_runner.py:408] Time since start: 8496.29s, 	Step: 24009, 	{'train/accuracy': 0.6873604655265808, 'train/loss': 1.4837753772735596, 'validation/accuracy': 0.6243799924850464, 'validation/loss': 1.7636795043945312, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.3973007202148438, 'test/num_examples': 10000, 'score': 8195.398663759232, 'total_duration': 8496.294484138489, 'accumulated_submission_time': 8195.398663759232, 'accumulated_eval_time': 299.5811333656311, 'accumulated_logging_time': 0.4718921184539795}
I0127 15:32:07.097846 139865274447616 logging_writer.py:48] [24009] accumulated_eval_time=299.581133, accumulated_logging_time=0.471892, accumulated_submission_time=8195.398664, global_step=24009, preemption_count=0, score=8195.398664, test/accuracy=0.502900, test/loss=2.397301, test/num_examples=10000, total_duration=8496.294484, train/accuracy=0.687360, train/loss=1.483775, validation/accuracy=0.624380, validation/loss=1.763680, validation/num_examples=50000
I0127 15:32:38.335566 139866163582720 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.2397985458374023, loss=3.3142852783203125
I0127 15:33:12.280336 139865274447616 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1655051708221436, loss=3.2760651111602783
I0127 15:33:46.229403 139866163582720 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.4394323825836182, loss=3.254589319229126
I0127 15:34:20.193279 139865274447616 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.2630243301391602, loss=3.3107378482818604
I0127 15:34:54.239946 139866163582720 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3213918209075928, loss=3.333721876144409
I0127 15:35:28.193042 139865274447616 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.3998308181762695, loss=3.3724217414855957
I0127 15:36:02.163214 139866163582720 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.6036626100540161, loss=3.293025493621826
I0127 15:36:36.133796 139865274447616 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.3366869688034058, loss=3.3006749153137207
I0127 15:37:10.084238 139866163582720 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.7019025087356567, loss=3.3317012786865234
I0127 15:37:44.051952 139865274447616 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.3341504335403442, loss=3.284714937210083
I0127 15:38:18.016722 139866163582720 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.4722468852996826, loss=3.2425942420959473
I0127 15:38:51.991131 139865274447616 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.4198943376541138, loss=3.30145263671875
I0127 15:39:25.966059 139866163582720 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.2722653150558472, loss=3.295344352722168
I0127 15:39:59.933832 139865274447616 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.5619912147521973, loss=3.356590986251831
I0127 15:40:33.859186 139866163582720 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1255035400390625, loss=3.2808139324188232
I0127 15:40:37.388011 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:40:43.840497 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:40:52.552935 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:40:55.045085 140027215431488 submission_runner.py:408] Time since start: 9024.26s, 	Step: 25512, 	{'train/accuracy': 0.6941366195678711, 'train/loss': 1.4621155261993408, 'validation/accuracy': 0.6352399587631226, 'validation/loss': 1.7313721179962158, 'validation/num_examples': 50000, 'test/accuracy': 0.5123000144958496, 'test/loss': 2.378972291946411, 'test/num_examples': 10000, 'score': 8705.627409219742, 'total_duration': 9024.263773679733, 'accumulated_submission_time': 8705.627409219742, 'accumulated_eval_time': 317.2381706237793, 'accumulated_logging_time': 0.5033385753631592}
I0127 15:40:55.070007 139865266054912 logging_writer.py:48] [25512] accumulated_eval_time=317.238171, accumulated_logging_time=0.503339, accumulated_submission_time=8705.627409, global_step=25512, preemption_count=0, score=8705.627409, test/accuracy=0.512300, test/loss=2.378972, test/num_examples=10000, total_duration=9024.263774, train/accuracy=0.694137, train/loss=1.462116, validation/accuracy=0.635240, validation/loss=1.731372, validation/num_examples=50000
I0127 15:41:25.268830 139865760950016 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.5762684345245361, loss=3.2395520210266113
I0127 15:41:59.199748 139865266054912 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.2346162796020508, loss=3.322570562362671
I0127 15:42:33.114017 139865760950016 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.2743664979934692, loss=3.290781021118164
I0127 15:43:07.079776 139865266054912 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2896877527236938, loss=3.3799214363098145
I0127 15:43:41.057147 139865760950016 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.4015872478485107, loss=3.2701938152313232
I0127 15:44:15.019103 139865266054912 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.2446579933166504, loss=3.2981929779052734
I0127 15:44:48.961879 139865760950016 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.5849841833114624, loss=3.3184144496917725
I0127 15:45:22.924030 139865266054912 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6097770929336548, loss=3.268124580383301
I0127 15:45:56.891915 139865760950016 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3154228925704956, loss=3.223337173461914
I0127 15:46:30.866837 139865266054912 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.338228702545166, loss=3.2646772861480713
I0127 15:47:04.899837 139865760950016 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.4149359464645386, loss=3.242685556411743
I0127 15:47:38.852777 139865266054912 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.4780827760696411, loss=3.304093360900879
I0127 15:48:12.829372 139865760950016 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.4593431949615479, loss=3.3555984497070312
I0127 15:48:46.774897 139865266054912 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.3389374017715454, loss=3.283641815185547
I0127 15:49:20.746262 139865760950016 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.4291539192199707, loss=3.2741265296936035
I0127 15:49:25.286826 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:49:31.458964 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:49:40.239784 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:49:42.729423 140027215431488 submission_runner.py:408] Time since start: 9551.95s, 	Step: 27015, 	{'train/accuracy': 0.7335180044174194, 'train/loss': 1.286011815071106, 'validation/accuracy': 0.6350799798965454, 'validation/loss': 1.7067891359329224, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.38372540473938, 'test/num_examples': 10000, 'score': 9215.783554315567, 'total_duration': 9551.9481112957, 'accumulated_submission_time': 9215.783554315567, 'accumulated_eval_time': 334.68074440956116, 'accumulated_logging_time': 0.5375471115112305}
I0127 15:49:42.751850 139865266054912 logging_writer.py:48] [27015] accumulated_eval_time=334.680744, accumulated_logging_time=0.537547, accumulated_submission_time=9215.783554, global_step=27015, preemption_count=0, score=9215.783554, test/accuracy=0.502800, test/loss=2.383725, test/num_examples=10000, total_duration=9551.948111, train/accuracy=0.733518, train/loss=1.286012, validation/accuracy=0.635080, validation/loss=1.706789, validation/num_examples=50000
I0127 15:50:11.935170 139865274447616 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2697581052780151, loss=3.224334716796875
I0127 15:50:45.891703 139865266054912 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.3099526166915894, loss=3.25683331489563
I0127 15:51:19.837748 139865274447616 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.4614994525909424, loss=3.215121269226074
I0127 15:51:53.804915 139865266054912 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.5654151439666748, loss=3.3103065490722656
I0127 15:52:27.759434 139865274447616 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.311962604522705, loss=3.2527036666870117
I0127 15:53:01.674469 139865266054912 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.3055055141448975, loss=3.278419017791748
I0127 15:53:35.707897 139865274447616 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.5011414289474487, loss=3.2831690311431885
I0127 15:54:09.642214 139865266054912 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.3502180576324463, loss=3.2642486095428467
I0127 15:54:43.581551 139865274447616 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.3620669841766357, loss=3.2565810680389404
I0127 15:55:17.559031 139865266054912 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.3280009031295776, loss=3.286318778991699
I0127 15:55:51.511979 139865274447616 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3788704872131348, loss=3.294234037399292
I0127 15:56:25.447004 139865266054912 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.6018620729446411, loss=3.2654755115509033
I0127 15:56:59.378395 139865274447616 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.5069776773452759, loss=3.2601804733276367
I0127 15:57:33.344478 139865266054912 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.4372633695602417, loss=3.280398368835449
I0127 15:58:07.320973 139865274447616 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.422018051147461, loss=3.2035858631134033
I0127 15:58:12.891731 140027215431488 spec.py:321] Evaluating on the training split.
I0127 15:58:19.102005 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 15:58:28.024155 140027215431488 spec.py:349] Evaluating on the test split.
I0127 15:58:31.081319 140027215431488 submission_runner.py:408] Time since start: 10080.30s, 	Step: 28518, 	{'train/accuracy': 0.7210817933082581, 'train/loss': 1.3177582025527954, 'validation/accuracy': 0.6410999894142151, 'validation/loss': 1.6783519983291626, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.341330051422119, 'test/num_examples': 10000, 'score': 9725.861224412918, 'total_duration': 10080.300015211105, 'accumulated_submission_time': 9725.861224412918, 'accumulated_eval_time': 352.87031412124634, 'accumulated_logging_time': 0.5690395832061768}
I0127 15:58:31.101518 139865274447616 logging_writer.py:48] [28518] accumulated_eval_time=352.870314, accumulated_logging_time=0.569040, accumulated_submission_time=9725.861224, global_step=28518, preemption_count=0, score=9725.861224, test/accuracy=0.512600, test/loss=2.341330, test/num_examples=10000, total_duration=10080.300015, train/accuracy=0.721082, train/loss=1.317758, validation/accuracy=0.641100, validation/loss=1.678352, validation/num_examples=50000
I0127 15:58:59.256917 139865769342720 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.3775618076324463, loss=3.270754814147949
I0127 15:59:33.162953 139865274447616 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.334501028060913, loss=3.291792392730713
I0127 16:00:07.152047 139865769342720 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.4579092264175415, loss=3.2440288066864014
I0127 16:00:41.093059 139865274447616 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6591148376464844, loss=3.201794147491455
I0127 16:01:15.043030 139865769342720 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.4353667497634888, loss=3.2680206298828125
I0127 16:01:49.002673 139865274447616 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.230827808380127, loss=3.304137706756592
I0127 16:02:22.947678 139865769342720 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.7787688970565796, loss=3.303788661956787
I0127 16:02:56.887229 139865274447616 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.49594247341156, loss=3.333030939102173
I0127 16:03:30.844439 139865769342720 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.5580874681472778, loss=3.3059756755828857
I0127 16:04:04.802341 139865274447616 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.4095823764801025, loss=3.2718610763549805
I0127 16:04:38.741705 139865769342720 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.4364283084869385, loss=3.340726613998413
I0127 16:05:12.666168 139865274447616 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.2185497283935547, loss=3.260054349899292
I0127 16:05:46.679188 139865769342720 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.5991339683532715, loss=3.3378100395202637
I0127 16:06:20.604342 139865274447616 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.5701220035552979, loss=3.2447216510772705
I0127 16:06:54.547734 139865769342720 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.1381328105926514, loss=3.2698240280151367
I0127 16:07:01.133238 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:07:07.518215 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 16:07:16.375409 140027215431488 spec.py:349] Evaluating on the test split.
I0127 16:07:18.871993 140027215431488 submission_runner.py:408] Time since start: 10608.09s, 	Step: 30021, 	{'train/accuracy': 0.7088049650192261, 'train/loss': 1.3988709449768066, 'validation/accuracy': 0.6373199820518494, 'validation/loss': 1.7182626724243164, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.397118330001831, 'test/num_examples': 10000, 'score': 10235.831578016281, 'total_duration': 10608.090680122375, 'accumulated_submission_time': 10235.831578016281, 'accumulated_eval_time': 370.6090452671051, 'accumulated_logging_time': 0.5975942611694336}
I0127 16:07:18.895024 139865266054912 logging_writer.py:48] [30021] accumulated_eval_time=370.609045, accumulated_logging_time=0.597594, accumulated_submission_time=10235.831578, global_step=30021, preemption_count=0, score=10235.831578, test/accuracy=0.508900, test/loss=2.397118, test/num_examples=10000, total_duration=10608.090680, train/accuracy=0.708805, train/loss=1.398871, validation/accuracy=0.637320, validation/loss=1.718263, validation/num_examples=50000
I0127 16:07:46.050705 139865760950016 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.4208239316940308, loss=3.266470193862915
I0127 16:08:19.947363 139865266054912 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.6391688585281372, loss=3.2994046211242676
I0127 16:08:53.913919 139865760950016 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.496153473854065, loss=3.2933268547058105
I0127 16:09:27.848542 139865266054912 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.4034054279327393, loss=3.2953102588653564
I0127 16:10:01.780256 139865760950016 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3829782009124756, loss=3.3422303199768066
I0127 16:10:35.681859 139865266054912 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.387959361076355, loss=3.233872890472412
I0127 16:11:09.632704 139865760950016 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.4291578531265259, loss=3.250919818878174
I0127 16:11:43.579555 139865266054912 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.4822986125946045, loss=3.318922996520996
I0127 16:12:17.574379 139865760950016 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.1925429105758667, loss=3.277947425842285
I0127 16:12:51.480018 139865266054912 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.3248423337936401, loss=3.2437639236450195
I0127 16:13:25.433727 139865760950016 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.4542746543884277, loss=3.2461700439453125
I0127 16:13:59.356202 139865266054912 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.2760523557662964, loss=3.2542619705200195
I0127 16:14:33.276622 139865760950016 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.3281842470169067, loss=3.287034273147583
I0127 16:15:07.239314 139865266054912 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.5319172143936157, loss=3.284867286682129
I0127 16:15:41.171255 139865760950016 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.2634354829788208, loss=3.2193763256073
I0127 16:15:49.103230 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:15:55.375022 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 16:16:04.267764 140027215431488 spec.py:349] Evaluating on the test split.
I0127 16:16:06.784841 140027215431488 submission_runner.py:408] Time since start: 11136.00s, 	Step: 31525, 	{'train/accuracy': 0.7114556431770325, 'train/loss': 1.361906886100769, 'validation/accuracy': 0.646340012550354, 'validation/loss': 1.6596877574920654, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.325451374053955, 'test/num_examples': 10000, 'score': 10745.978585481644, 'total_duration': 11136.003517150879, 'accumulated_submission_time': 10745.978585481644, 'accumulated_eval_time': 388.2906057834625, 'accumulated_logging_time': 0.6299974918365479}
I0127 16:16:06.812129 139866163582720 logging_writer.py:48] [31525] accumulated_eval_time=388.290606, accumulated_logging_time=0.629997, accumulated_submission_time=10745.978585, global_step=31525, preemption_count=0, score=10745.978585, test/accuracy=0.511900, test/loss=2.325451, test/num_examples=10000, total_duration=11136.003517, train/accuracy=0.711456, train/loss=1.361907, validation/accuracy=0.646340, validation/loss=1.659688, validation/num_examples=50000
I0127 16:16:32.603060 139866171975424 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3354604244232178, loss=3.232347249984741
I0127 16:17:06.988691 139866163582720 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.3086683750152588, loss=3.2192535400390625
I0127 16:17:40.946895 139866171975424 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.372901439666748, loss=3.2653918266296387
I0127 16:18:14.901350 139866163582720 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.3312212228775024, loss=3.208442211151123
I0127 16:18:48.922027 139866171975424 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.4394549131393433, loss=3.29231595993042
I0127 16:19:22.820919 139866163582720 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.3842532634735107, loss=3.1954751014709473
I0127 16:19:56.780062 139866171975424 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3931243419647217, loss=3.207784414291382
I0127 16:20:30.729159 139866163582720 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.5431684255599976, loss=3.1610231399536133
I0127 16:21:04.648123 139866171975424 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.2569373846054077, loss=3.234992504119873
I0127 16:21:38.564330 139866163582720 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.398129940032959, loss=3.2852060794830322
I0127 16:22:12.503213 139866171975424 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.4351710081100464, loss=3.261110782623291
I0127 16:22:46.454520 139866163582720 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.6148827075958252, loss=3.189488172531128
I0127 16:23:20.376537 139866171975424 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.7862707376480103, loss=3.2798454761505127
I0127 16:23:54.309645 139866163582720 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.4285248517990112, loss=3.1979243755340576
I0127 16:24:28.321526 139866171975424 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.463706135749817, loss=3.2077016830444336
I0127 16:24:36.957498 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:24:43.348708 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 16:24:52.337693 140027215431488 spec.py:349] Evaluating on the test split.
I0127 16:24:54.738176 140027215431488 submission_runner.py:408] Time since start: 11663.96s, 	Step: 33027, 	{'train/accuracy': 0.7103396058082581, 'train/loss': 1.374114990234375, 'validation/accuracy': 0.6455199718475342, 'validation/loss': 1.6690250635147095, 'validation/num_examples': 50000, 'test/accuracy': 0.5130000114440918, 'test/loss': 2.3377459049224854, 'test/num_examples': 10000, 'score': 11256.05967617035, 'total_duration': 11663.95685338974, 'accumulated_submission_time': 11256.05967617035, 'accumulated_eval_time': 406.07124972343445, 'accumulated_logging_time': 0.669827938079834}
I0127 16:24:54.763000 139865760950016 logging_writer.py:48] [33027] accumulated_eval_time=406.071250, accumulated_logging_time=0.669828, accumulated_submission_time=11256.059676, global_step=33027, preemption_count=0, score=11256.059676, test/accuracy=0.513000, test/loss=2.337746, test/num_examples=10000, total_duration=11663.956853, train/accuracy=0.710340, train/loss=1.374115, validation/accuracy=0.645520, validation/loss=1.669025, validation/num_examples=50000
I0127 16:25:19.836944 139865769342720 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.506980061531067, loss=3.20346736907959
I0127 16:25:53.741184 139865760950016 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.536860704421997, loss=3.156770944595337
I0127 16:26:27.644858 139865769342720 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.4415009021759033, loss=3.253594398498535
I0127 16:27:01.573010 139865760950016 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.370948076248169, loss=3.2473390102386475
I0127 16:27:35.470886 139865769342720 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.288838505744934, loss=3.2216899394989014
I0127 16:28:09.404241 139865760950016 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.2863564491271973, loss=3.208777666091919
I0127 16:28:43.306638 139865769342720 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.3264687061309814, loss=3.219566583633423
I0127 16:29:17.250655 139865760950016 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.4910179376602173, loss=3.357820749282837
I0127 16:29:51.157924 139865769342720 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.5182465314865112, loss=3.2355096340179443
I0127 16:30:25.077913 139865760950016 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.3837982416152954, loss=3.280895471572876
I0127 16:30:59.061261 139865769342720 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.4963185787200928, loss=3.254124879837036
I0127 16:31:32.984673 139865760950016 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.5226589441299438, loss=3.2733752727508545
I0127 16:32:06.892718 139865769342720 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.4298300743103027, loss=3.201807737350464
I0127 16:32:40.805417 139865760950016 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.4915385246276855, loss=3.2013731002807617
I0127 16:33:14.713804 139865769342720 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.3084149360656738, loss=3.200580358505249
I0127 16:33:25.012069 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:33:31.310814 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 16:33:39.913313 140027215431488 spec.py:349] Evaluating on the test split.
I0127 16:33:42.436370 140027215431488 submission_runner.py:408] Time since start: 12191.66s, 	Step: 34532, 	{'train/accuracy': 0.7158003449440002, 'train/loss': 1.3178354501724243, 'validation/accuracy': 0.6526600122451782, 'validation/loss': 1.6064244508743286, 'validation/num_examples': 50000, 'test/accuracy': 0.5246000289916992, 'test/loss': 2.271498680114746, 'test/num_examples': 10000, 'score': 11766.244701385498, 'total_duration': 12191.655038356781, 'accumulated_submission_time': 11766.244701385498, 'accumulated_eval_time': 423.49549770355225, 'accumulated_logging_time': 0.7041542530059814}
I0127 16:33:42.464257 139866171975424 logging_writer.py:48] [34532] accumulated_eval_time=423.495498, accumulated_logging_time=0.704154, accumulated_submission_time=11766.244701, global_step=34532, preemption_count=0, score=11766.244701, test/accuracy=0.524600, test/loss=2.271499, test/num_examples=10000, total_duration=12191.655038, train/accuracy=0.715800, train/loss=1.317835, validation/accuracy=0.652660, validation/loss=1.606424, validation/num_examples=50000
I0127 16:34:05.880270 139866180368128 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.4249604940414429, loss=3.2011513710021973
I0127 16:34:39.777955 139866171975424 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.5314687490463257, loss=3.221914052963257
I0127 16:35:13.719804 139866180368128 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.4341647624969482, loss=3.1642191410064697
I0127 16:35:47.643976 139866171975424 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.3569456338882446, loss=3.169766426086426
I0127 16:36:21.566736 139866180368128 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.4893944263458252, loss=3.2198662757873535
I0127 16:36:55.505138 139866171975424 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.3456096649169922, loss=3.042160987854004
I0127 16:37:29.522940 139866180368128 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.2822632789611816, loss=3.1713197231292725
I0127 16:38:03.445830 139866171975424 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.5784920454025269, loss=3.245762586593628
I0127 16:38:37.368946 139866180368128 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.430701732635498, loss=3.1882598400115967
I0127 16:39:11.322865 139866171975424 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5978312492370605, loss=3.2427539825439453
I0127 16:39:45.219759 139866180368128 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2478128671646118, loss=3.197617530822754
I0127 16:40:19.163225 139866171975424 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.4438118934631348, loss=3.210984706878662
I0127 16:40:53.094604 139866180368128 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.4394193887710571, loss=3.2475576400756836
I0127 16:41:26.988093 139866171975424 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.4600828886032104, loss=3.2066497802734375
I0127 16:42:00.939225 139866180368128 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6038298606872559, loss=3.2049028873443604
I0127 16:42:12.617969 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:42:18.823909 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 16:42:27.810550 140027215431488 spec.py:349] Evaluating on the test split.
I0127 16:42:30.357311 140027215431488 submission_runner.py:408] Time since start: 12719.58s, 	Step: 36036, 	{'train/accuracy': 0.7166773080825806, 'train/loss': 1.3349188566207886, 'validation/accuracy': 0.6400600075721741, 'validation/loss': 1.6809086799621582, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.3779425621032715, 'test/num_examples': 10000, 'score': 12276.337631940842, 'total_duration': 12719.575999498367, 'accumulated_submission_time': 12276.337631940842, 'accumulated_eval_time': 441.2348201274872, 'accumulated_logging_time': 0.7414686679840088}
I0127 16:42:30.378837 139865760950016 logging_writer.py:48] [36036] accumulated_eval_time=441.234820, accumulated_logging_time=0.741469, accumulated_submission_time=12276.337632, global_step=36036, preemption_count=0, score=12276.337632, test/accuracy=0.508500, test/loss=2.377943, test/num_examples=10000, total_duration=12719.575999, train/accuracy=0.716677, train/loss=1.334919, validation/accuracy=0.640060, validation/loss=1.680909, validation/num_examples=50000
I0127 16:42:52.406374 139865769342720 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.712026596069336, loss=3.276355743408203
I0127 16:43:26.401105 139865760950016 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.4178475141525269, loss=3.187201738357544
I0127 16:44:00.343045 139865769342720 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.297547698020935, loss=3.188227653503418
I0127 16:44:34.232507 139865760950016 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.5324515104293823, loss=3.233893394470215
I0127 16:45:08.159806 139865769342720 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.3539657592773438, loss=3.190314531326294
I0127 16:45:42.135101 139865760950016 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.5291836261749268, loss=3.204697847366333
I0127 16:46:16.039537 139865769342720 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5024760961532593, loss=3.1835408210754395
I0127 16:46:49.989193 139865760950016 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.507235050201416, loss=3.22996187210083
I0127 16:47:23.948843 139865769342720 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.4377293586730957, loss=3.173062801361084
I0127 16:47:57.859957 139865760950016 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.3972524404525757, loss=3.297067165374756
I0127 16:48:31.790188 139865769342720 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.5000967979431152, loss=3.2496402263641357
I0127 16:49:05.745844 139865760950016 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.537368655204773, loss=3.2072553634643555
I0127 16:49:39.753797 139865769342720 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.7431806325912476, loss=3.2362141609191895
I0127 16:50:13.645668 139865760950016 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.6454650163650513, loss=3.2437777519226074
I0127 16:50:47.582293 139865769342720 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.4567539691925049, loss=3.2277510166168213
I0127 16:51:00.590839 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:51:06.898505 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 16:51:15.621674 140027215431488 spec.py:349] Evaluating on the test split.
I0127 16:51:18.083353 140027215431488 submission_runner.py:408] Time since start: 13247.30s, 	Step: 37540, 	{'train/accuracy': 0.7401347160339355, 'train/loss': 1.2507890462875366, 'validation/accuracy': 0.6541999578475952, 'validation/loss': 1.622602939605713, 'validation/num_examples': 50000, 'test/accuracy': 0.5275000333786011, 'test/loss': 2.2785587310791016, 'test/num_examples': 10000, 'score': 12786.48780798912, 'total_duration': 13247.302038192749, 'accumulated_submission_time': 12786.48780798912, 'accumulated_eval_time': 458.72729420661926, 'accumulated_logging_time': 0.7722766399383545}
I0127 16:51:18.110073 139865760950016 logging_writer.py:48] [37540] accumulated_eval_time=458.727294, accumulated_logging_time=0.772277, accumulated_submission_time=12786.487808, global_step=37540, preemption_count=0, score=12786.487808, test/accuracy=0.527500, test/loss=2.278559, test/num_examples=10000, total_duration=13247.302038, train/accuracy=0.740135, train/loss=1.250789, validation/accuracy=0.654200, validation/loss=1.622603, validation/num_examples=50000
I0127 16:51:38.818688 139866171975424 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.4472486972808838, loss=3.2015819549560547
I0127 16:52:12.740298 139865760950016 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6289275884628296, loss=3.2077853679656982
I0127 16:52:46.687205 139866171975424 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.437452793121338, loss=3.237464427947998
I0127 16:53:20.610450 139865760950016 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5675256252288818, loss=3.207512378692627
I0127 16:53:54.544389 139866171975424 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.373882532119751, loss=3.1736679077148438
I0127 16:54:28.498866 139865760950016 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.435621738433838, loss=3.182664394378662
I0127 16:55:02.421083 139866171975424 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.6434376239776611, loss=3.1902246475219727
I0127 16:55:36.345824 139865760950016 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.5172992944717407, loss=3.12345552444458
I0127 16:56:10.417256 139866171975424 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.4642242193222046, loss=3.161705493927002
I0127 16:56:44.340131 139865760950016 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.767172932624817, loss=3.169328212738037
I0127 16:57:18.270782 139866171975424 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.4848088026046753, loss=3.185453414916992
I0127 16:57:52.198845 139865760950016 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.5390422344207764, loss=3.2401633262634277
I0127 16:58:26.139110 139866171975424 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.756946086883545, loss=3.2718703746795654
I0127 16:59:00.029284 139865760950016 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.3761838674545288, loss=3.1431093215942383
I0127 16:59:33.969302 139866171975424 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.5026379823684692, loss=3.274507999420166
I0127 16:59:48.391586 140027215431488 spec.py:321] Evaluating on the training split.
I0127 16:59:54.777835 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:00:03.479118 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:00:05.999597 140027215431488 submission_runner.py:408] Time since start: 13775.22s, 	Step: 39044, 	{'train/accuracy': 0.7385004758834839, 'train/loss': 1.2336959838867188, 'validation/accuracy': 0.6571800112724304, 'validation/loss': 1.5852231979370117, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.2626054286956787, 'test/num_examples': 10000, 'score': 13296.705997228622, 'total_duration': 13775.218259096146, 'accumulated_submission_time': 13296.705997228622, 'accumulated_eval_time': 476.3352725505829, 'accumulated_logging_time': 0.80930495262146}
I0127 17:00:06.022813 139865274447616 logging_writer.py:48] [39044] accumulated_eval_time=476.335273, accumulated_logging_time=0.809305, accumulated_submission_time=13296.705997, global_step=39044, preemption_count=0, score=13296.705997, test/accuracy=0.523100, test/loss=2.262605, test/num_examples=10000, total_duration=13775.218259, train/accuracy=0.738500, train/loss=1.233696, validation/accuracy=0.657180, validation/loss=1.585223, validation/num_examples=50000
I0127 17:00:25.371304 139865760950016 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.5130789279937744, loss=3.2307376861572266
I0127 17:00:59.285667 139865274447616 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.321757435798645, loss=3.1560895442962646
I0127 17:01:33.217549 139865760950016 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.6580028533935547, loss=3.24766206741333
I0127 17:02:07.243819 139865274447616 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.4716989994049072, loss=3.122148036956787
I0127 17:02:41.180521 139865760950016 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5815114974975586, loss=3.2018678188323975
I0127 17:03:15.093145 139865274447616 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7200413942337036, loss=3.14933705329895
I0127 17:03:49.050356 139865760950016 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7136121988296509, loss=3.2041049003601074
I0127 17:04:23.007243 139865274447616 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.8315269947052002, loss=3.1853950023651123
I0127 17:04:56.943633 139865760950016 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.5478166341781616, loss=3.1885032653808594
I0127 17:05:30.880118 139865274447616 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.5023400783538818, loss=3.165663957595825
I0127 17:06:04.825031 139865760950016 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.4656933546066284, loss=3.157313346862793
I0127 17:06:38.725091 139865274447616 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.573142170906067, loss=3.1881518363952637
I0127 17:07:12.658759 139865760950016 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.4625548124313354, loss=3.16510272026062
I0127 17:07:46.564871 139865274447616 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.6558259725570679, loss=3.246492862701416
I0127 17:08:20.599725 139865760950016 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.6854958534240723, loss=3.164503812789917
I0127 17:08:36.003716 140027215431488 spec.py:321] Evaluating on the training split.
I0127 17:08:42.329190 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:08:51.101940 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:08:53.506940 140027215431488 submission_runner.py:408] Time since start: 14302.73s, 	Step: 40547, 	{'train/accuracy': 0.7364476919174194, 'train/loss': 1.2696105241775513, 'validation/accuracy': 0.6611799597740173, 'validation/loss': 1.5964609384536743, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.2442195415496826, 'test/num_examples': 10000, 'score': 13806.621383428574, 'total_duration': 14302.725590705872, 'accumulated_submission_time': 13806.621383428574, 'accumulated_eval_time': 493.83841919898987, 'accumulated_logging_time': 0.8440403938293457}
I0127 17:08:53.540958 139866171975424 logging_writer.py:48] [40547] accumulated_eval_time=493.838419, accumulated_logging_time=0.844040, accumulated_submission_time=13806.621383, global_step=40547, preemption_count=0, score=13806.621383, test/accuracy=0.536200, test/loss=2.244220, test/num_examples=10000, total_duration=14302.725591, train/accuracy=0.736448, train/loss=1.269611, validation/accuracy=0.661180, validation/loss=1.596461, validation/num_examples=50000
I0127 17:09:11.830828 139866180368128 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.645894169807434, loss=3.1808176040649414
I0127 17:09:45.740859 139866171975424 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.4265375137329102, loss=3.222724437713623
I0127 17:10:19.663739 139866180368128 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6140937805175781, loss=3.223607063293457
I0127 17:10:53.584160 139866171975424 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7026432752609253, loss=3.2256176471710205
I0127 17:11:27.546888 139866180368128 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.5312800407409668, loss=3.129157066345215
I0127 17:12:01.445749 139866171975424 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.5352357625961304, loss=3.2213034629821777
I0127 17:12:35.388439 139866180368128 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5081675052642822, loss=3.165039300918579
I0127 17:13:09.328560 139866171975424 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.4570181369781494, loss=3.1838369369506836
I0127 17:13:43.237784 139866180368128 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7343560457229614, loss=3.2877371311187744
I0127 17:14:17.179378 139866171975424 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.5432684421539307, loss=3.143183946609497
I0127 17:14:51.157607 139866180368128 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.6275267601013184, loss=3.218928337097168
I0127 17:15:25.099022 139866171975424 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.6779078245162964, loss=3.2281384468078613
I0127 17:15:59.028773 139866180368128 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.634711503982544, loss=3.131571054458618
I0127 17:16:32.964216 139866171975424 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.6835626363754272, loss=3.265122175216675
I0127 17:17:06.868298 139866180368128 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.4661880731582642, loss=3.211885929107666
I0127 17:17:23.630876 140027215431488 spec.py:321] Evaluating on the training split.
I0127 17:17:30.474729 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:17:39.377575 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:17:41.848061 140027215431488 submission_runner.py:408] Time since start: 14831.07s, 	Step: 42051, 	{'train/accuracy': 0.7135283946990967, 'train/loss': 1.3360618352890015, 'validation/accuracy': 0.6412599682807922, 'validation/loss': 1.6634626388549805, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.3140530586242676, 'test/num_examples': 10000, 'score': 14316.647776842117, 'total_duration': 14831.066751718521, 'accumulated_submission_time': 14316.647776842117, 'accumulated_eval_time': 512.0555701255798, 'accumulated_logging_time': 0.8885290622711182}
I0127 17:17:41.872629 139865266054912 logging_writer.py:48] [42051] accumulated_eval_time=512.055570, accumulated_logging_time=0.888529, accumulated_submission_time=14316.647777, global_step=42051, preemption_count=0, score=14316.647777, test/accuracy=0.521100, test/loss=2.314053, test/num_examples=10000, total_duration=14831.066752, train/accuracy=0.713528, train/loss=1.336062, validation/accuracy=0.641260, validation/loss=1.663463, validation/num_examples=50000
I0127 17:17:58.836824 139865274447616 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.675321340560913, loss=3.179039239883423
I0127 17:18:32.753731 139865266054912 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7480378150939941, loss=3.2853617668151855
I0127 17:19:06.687180 139865274447616 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.581780195236206, loss=3.14461088180542
I0127 17:19:40.603654 139865266054912 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.6197527647018433, loss=3.200028419494629
I0127 17:20:14.499829 139865274447616 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.507820725440979, loss=3.192713737487793
I0127 17:20:48.598152 139865266054912 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.652637243270874, loss=3.178208112716675
I0127 17:21:22.522709 139865274447616 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.9329249858856201, loss=3.181419849395752
I0127 17:21:56.473800 139865266054912 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5970706939697266, loss=3.1768126487731934
I0127 17:22:30.365618 139865274447616 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.6290324926376343, loss=3.2151408195495605
I0127 17:23:04.308847 139865266054912 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.7762118577957153, loss=3.140080451965332
I0127 17:23:38.267150 139865274447616 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5690162181854248, loss=3.168407440185547
I0127 17:24:12.199399 139865266054912 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7204391956329346, loss=3.1170578002929688
I0127 17:24:46.110110 139865274447616 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.539409875869751, loss=3.1324973106384277
I0127 17:25:20.045219 139865266054912 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.692712426185608, loss=3.267759084701538
I0127 17:25:53.938476 139865274447616 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.608185887336731, loss=3.1912384033203125
I0127 17:26:12.056477 140027215431488 spec.py:321] Evaluating on the training split.
I0127 17:26:18.282311 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:26:26.785074 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:26:29.301928 140027215431488 submission_runner.py:408] Time since start: 15358.52s, 	Step: 43555, 	{'train/accuracy': 0.7233737111091614, 'train/loss': 1.2983133792877197, 'validation/accuracy': 0.6625799536705017, 'validation/loss': 1.5901869535446167, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.240588426589966, 'test/num_examples': 10000, 'score': 14826.77017712593, 'total_duration': 15358.520558834076, 'accumulated_submission_time': 14826.77017712593, 'accumulated_eval_time': 529.3009285926819, 'accumulated_logging_time': 0.9220795631408691}
I0127 17:26:29.341394 139865760950016 logging_writer.py:48] [43555] accumulated_eval_time=529.300929, accumulated_logging_time=0.922080, accumulated_submission_time=14826.770177, global_step=43555, preemption_count=0, score=14826.770177, test/accuracy=0.536200, test/loss=2.240588, test/num_examples=10000, total_duration=15358.520559, train/accuracy=0.723374, train/loss=1.298313, validation/accuracy=0.662580, validation/loss=1.590187, validation/num_examples=50000
I0127 17:26:44.936695 139866163582720 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.4654020071029663, loss=3.1149957180023193
I0127 17:27:18.975689 139865760950016 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.7951502799987793, loss=3.180997848510742
I0127 17:27:52.866957 139866163582720 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.765527367591858, loss=3.2597432136535645
I0127 17:28:26.811134 139865760950016 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7904752492904663, loss=3.24776291847229
I0127 17:29:00.732359 139866163582720 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.525551438331604, loss=3.229987382888794
I0127 17:29:34.662614 139865760950016 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.46613609790802, loss=3.2439427375793457
I0127 17:30:08.627914 139866163582720 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.5734177827835083, loss=3.1591925621032715
I0127 17:30:42.542345 139865760950016 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.5790728330612183, loss=3.1735246181488037
I0127 17:31:16.476577 139866163582720 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.6595863103866577, loss=3.1862916946411133
I0127 17:31:50.411769 139865760950016 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.6722253561019897, loss=3.1917898654937744
I0127 17:32:24.297962 139866163582720 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8871393203735352, loss=3.1915156841278076
I0127 17:32:58.241655 139865760950016 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.76137375831604, loss=3.1322755813598633
I0127 17:33:32.377851 139866163582720 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.5751311779022217, loss=3.1693150997161865
I0127 17:34:06.291919 139865760950016 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.5666215419769287, loss=3.1544013023376465
I0127 17:34:40.238827 139866163582720 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.7521454095840454, loss=3.200597047805786
I0127 17:34:59.379432 140027215431488 spec.py:321] Evaluating on the training split.
I0127 17:35:05.719076 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:35:14.747521 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:35:17.213350 140027215431488 submission_runner.py:408] Time since start: 15886.43s, 	Step: 45058, 	{'train/accuracy': 0.7239118218421936, 'train/loss': 1.3282990455627441, 'validation/accuracy': 0.6556199789047241, 'validation/loss': 1.6299936771392822, 'validation/num_examples': 50000, 'test/accuracy': 0.5277000069618225, 'test/loss': 2.2814555168151855, 'test/num_examples': 10000, 'score': 15336.741206169128, 'total_duration': 15886.432034015656, 'accumulated_submission_time': 15336.741206169128, 'accumulated_eval_time': 547.1348049640656, 'accumulated_logging_time': 0.9746749401092529}
I0127 17:35:17.241323 139865257662208 logging_writer.py:48] [45058] accumulated_eval_time=547.134805, accumulated_logging_time=0.974675, accumulated_submission_time=15336.741206, global_step=45058, preemption_count=0, score=15336.741206, test/accuracy=0.527700, test/loss=2.281456, test/num_examples=10000, total_duration=15886.432034, train/accuracy=0.723912, train/loss=1.328299, validation/accuracy=0.655620, validation/loss=1.629994, validation/num_examples=50000
I0127 17:35:31.828600 139865266054912 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.7136242389678955, loss=3.2379305362701416
I0127 17:36:05.742715 139865257662208 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.6864523887634277, loss=3.189129114151001
I0127 17:36:39.636085 139865266054912 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.5897592306137085, loss=3.197063684463501
I0127 17:37:13.574498 139865257662208 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.7747215032577515, loss=3.230093479156494
I0127 17:37:47.491116 139865266054912 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.6106175184249878, loss=3.1871678829193115
I0127 17:38:21.422407 139865257662208 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.564658522605896, loss=3.152613878250122
I0127 17:38:55.344869 139865266054912 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.6464369297027588, loss=3.1468846797943115
I0127 17:39:29.249829 139865257662208 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.6850323677062988, loss=3.223083257675171
I0127 17:40:03.291448 139865266054912 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.6512715816497803, loss=3.0963220596313477
I0127 17:40:37.186998 139865257662208 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.758702278137207, loss=3.216230869293213
I0127 17:41:11.113287 139865266054912 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7140053510665894, loss=3.153826951980591
I0127 17:41:45.018640 139865257662208 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.698947548866272, loss=3.3116614818573
I0127 17:42:18.937113 139865266054912 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.7669035196304321, loss=3.1951887607574463
I0127 17:42:52.871458 139865257662208 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.7012321949005127, loss=3.125255823135376
I0127 17:43:26.767193 139865266054912 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.6718933582305908, loss=3.2240118980407715
I0127 17:43:47.285671 140027215431488 spec.py:321] Evaluating on the training split.
I0127 17:43:53.554481 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:44:02.343199 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:44:04.883781 140027215431488 submission_runner.py:408] Time since start: 16414.10s, 	Step: 46562, 	{'train/accuracy': 0.7540656924247742, 'train/loss': 1.198965311050415, 'validation/accuracy': 0.6650199890136719, 'validation/loss': 1.5895963907241821, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.2207345962524414, 'test/num_examples': 10000, 'score': 15846.720610380173, 'total_duration': 16414.10246682167, 'accumulated_submission_time': 15846.720610380173, 'accumulated_eval_time': 564.7328906059265, 'accumulated_logging_time': 1.0138413906097412}
I0127 17:44:04.913784 139865257662208 logging_writer.py:48] [46562] accumulated_eval_time=564.732891, accumulated_logging_time=1.013841, accumulated_submission_time=15846.720610, global_step=46562, preemption_count=0, score=15846.720610, test/accuracy=0.538300, test/loss=2.220735, test/num_examples=10000, total_duration=16414.102467, train/accuracy=0.754066, train/loss=1.198965, validation/accuracy=0.665020, validation/loss=1.589596, validation/num_examples=50000
I0127 17:44:18.149902 139865266054912 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7492557764053345, loss=3.1991450786590576
I0127 17:44:52.041126 139865257662208 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.625746488571167, loss=3.1811723709106445
I0127 17:45:25.962375 139865266054912 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.6999733448028564, loss=3.149036169052124
I0127 17:46:00.013795 139865257662208 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.9280610084533691, loss=3.171096086502075
I0127 17:46:33.952055 139865266054912 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.7083812952041626, loss=3.190171957015991
I0127 17:47:07.860762 139865257662208 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7818834781646729, loss=3.202805757522583
I0127 17:47:41.796797 139865266054912 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.7755491733551025, loss=3.155050039291382
I0127 17:48:15.734025 139865257662208 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8898135423660278, loss=3.155879020690918
I0127 17:48:49.638203 139865266054912 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.6490795612335205, loss=3.1719071865081787
I0127 17:49:23.575112 139865257662208 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.679850459098816, loss=3.106950283050537
I0127 17:49:57.497555 139865266054912 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.8410444259643555, loss=3.17092227935791
I0127 17:50:31.394014 139865257662208 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.951354742050171, loss=3.2189178466796875
I0127 17:51:05.322976 139865266054912 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.783739447593689, loss=3.2544851303100586
I0127 17:51:39.254612 139865257662208 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7152349948883057, loss=3.20750093460083
I0127 17:52:13.363790 139865266054912 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.643943190574646, loss=3.1620845794677734
I0127 17:52:35.221275 140027215431488 spec.py:321] Evaluating on the training split.
I0127 17:52:41.503715 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 17:52:50.255924 140027215431488 spec.py:349] Evaluating on the test split.
I0127 17:52:52.767007 140027215431488 submission_runner.py:408] Time since start: 16941.99s, 	Step: 48066, 	{'train/accuracy': 0.7438217401504517, 'train/loss': 1.2626017332077026, 'validation/accuracy': 0.6648600101470947, 'validation/loss': 1.6124789714813232, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.236502170562744, 'test/num_examples': 10000, 'score': 16356.965524196625, 'total_duration': 16941.98569583893, 'accumulated_submission_time': 16356.965524196625, 'accumulated_eval_time': 582.2785995006561, 'accumulated_logging_time': 1.0533804893493652}
I0127 17:52:52.792996 139866171975424 logging_writer.py:48] [48066] accumulated_eval_time=582.278600, accumulated_logging_time=1.053380, accumulated_submission_time=16356.965524, global_step=48066, preemption_count=0, score=16356.965524, test/accuracy=0.540900, test/loss=2.236502, test/num_examples=10000, total_duration=16941.985696, train/accuracy=0.743822, train/loss=1.262602, validation/accuracy=0.664860, validation/loss=1.612479, validation/num_examples=50000
I0127 17:53:04.673549 139866180368128 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.6813242435455322, loss=3.242915153503418
I0127 17:53:38.549065 139866171975424 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.6412057876586914, loss=3.0921761989593506
I0127 17:54:12.454666 139866180368128 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.746065378189087, loss=3.175095796585083
I0127 17:54:46.310557 139866171975424 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.9319531917572021, loss=3.17622447013855
I0127 17:55:20.226633 139866180368128 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.7750548124313354, loss=3.0688419342041016
I0127 17:55:54.111083 139866171975424 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.6979823112487793, loss=3.188441753387451
I0127 17:56:28.048321 139866180368128 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.7065883874893188, loss=3.0564656257629395
I0127 17:57:01.940151 139866171975424 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.6457860469818115, loss=3.141868829727173
I0127 17:57:35.842301 139866180368128 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.778057336807251, loss=3.101830244064331
I0127 17:58:09.759570 139866171975424 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.8146183490753174, loss=3.223541259765625
I0127 17:58:43.766072 139866180368128 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.7435870170593262, loss=3.2185556888580322
I0127 17:59:17.713943 139866171975424 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.93787682056427, loss=3.0875842571258545
I0127 17:59:51.631160 139866180368128 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.799142837524414, loss=3.1760144233703613
I0127 18:00:25.555659 139866171975424 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7370247840881348, loss=3.0865495204925537
I0127 18:00:59.463145 139866180368128 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8367809057235718, loss=3.262497901916504
I0127 18:01:22.999312 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:01:29.263751 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:01:38.141694 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:01:40.641078 140027215431488 submission_runner.py:408] Time since start: 17469.86s, 	Step: 49571, 	{'train/accuracy': 0.7395169138908386, 'train/loss': 1.25126314163208, 'validation/accuracy': 0.6665399670600891, 'validation/loss': 1.5785037279129028, 'validation/num_examples': 50000, 'test/accuracy': 0.5455999970436096, 'test/loss': 2.187873601913452, 'test/num_examples': 10000, 'score': 16867.10874891281, 'total_duration': 17469.85976576805, 'accumulated_submission_time': 16867.10874891281, 'accumulated_eval_time': 599.9203262329102, 'accumulated_logging_time': 1.0888376235961914}
I0127 18:01:40.667297 139865266054912 logging_writer.py:48] [49571] accumulated_eval_time=599.920326, accumulated_logging_time=1.088838, accumulated_submission_time=16867.108749, global_step=49571, preemption_count=0, score=16867.108749, test/accuracy=0.545600, test/loss=2.187874, test/num_examples=10000, total_duration=17469.859766, train/accuracy=0.739517, train/loss=1.251263, validation/accuracy=0.666540, validation/loss=1.578504, validation/num_examples=50000
I0127 18:01:50.843287 139865760950016 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.7275094985961914, loss=3.1126527786254883
I0127 18:02:24.761224 139865266054912 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.7628194093704224, loss=3.18196177482605
I0127 18:02:58.647710 139865760950016 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.9590877294540405, loss=3.1242096424102783
I0127 18:03:32.567554 139865266054912 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.028991222381592, loss=3.1892614364624023
I0127 18:04:06.524696 139865760950016 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7480039596557617, loss=3.1168696880340576
I0127 18:04:40.519976 139865266054912 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.5898158550262451, loss=3.1555886268615723
I0127 18:05:14.436403 139865760950016 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.8379948139190674, loss=3.239232301712036
I0127 18:05:48.370001 139865266054912 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.745361328125, loss=3.1650164127349854
I0127 18:06:22.271272 139865760950016 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.8996020555496216, loss=3.1319751739501953
I0127 18:06:56.193265 139865266054912 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.7576167583465576, loss=3.168765068054199
I0127 18:07:30.144181 139865760950016 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.6922911405563354, loss=3.1220383644104004
I0127 18:08:04.059859 139865266054912 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.8884670734405518, loss=3.1544907093048096
I0127 18:08:38.006069 139865760950016 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.7787202596664429, loss=3.163191318511963
I0127 18:09:11.939964 139865266054912 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8903213739395142, loss=3.051934242248535
I0127 18:09:45.867999 139865760950016 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.79916250705719, loss=3.131671190261841
I0127 18:10:10.784663 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:10:17.038768 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:10:26.020427 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:10:28.461769 140027215431488 submission_runner.py:408] Time since start: 17997.68s, 	Step: 51075, 	{'train/accuracy': 0.7398756146430969, 'train/loss': 1.2330973148345947, 'validation/accuracy': 0.666700005531311, 'validation/loss': 1.5564137697219849, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.2007992267608643, 'test/num_examples': 10000, 'score': 17377.16103053093, 'total_duration': 17997.680426597595, 'accumulated_submission_time': 17377.16103053093, 'accumulated_eval_time': 617.5973706245422, 'accumulated_logging_time': 1.126636266708374}
I0127 18:10:28.486953 139865266054912 logging_writer.py:48] [51075] accumulated_eval_time=617.597371, accumulated_logging_time=1.126636, accumulated_submission_time=17377.161031, global_step=51075, preemption_count=0, score=17377.161031, test/accuracy=0.538700, test/loss=2.200799, test/num_examples=10000, total_duration=17997.680427, train/accuracy=0.739876, train/loss=1.233097, validation/accuracy=0.666700, validation/loss=1.556414, validation/num_examples=50000
I0127 18:10:37.282870 139866171975424 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.96554696559906, loss=3.137847423553467
I0127 18:11:11.255958 139865266054912 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.710715413093567, loss=3.1010022163391113
I0127 18:11:45.166304 139866171975424 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.883180022239685, loss=3.245271682739258
I0127 18:12:19.085672 139865266054912 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7506794929504395, loss=3.1751272678375244
I0127 18:12:53.014304 139866171975424 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8584131002426147, loss=3.2558951377868652
I0127 18:13:26.975899 139865266054912 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.969342827796936, loss=3.185819387435913
I0127 18:14:00.889492 139866171975424 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8702723979949951, loss=3.122304677963257
I0127 18:14:34.831253 139865266054912 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8935623168945312, loss=3.1206247806549072
I0127 18:15:08.791374 139866171975424 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.6538004875183105, loss=3.1530356407165527
I0127 18:15:42.701311 139865266054912 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.7501881122589111, loss=3.138442277908325
I0127 18:16:16.637336 139866171975424 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.2234857082366943, loss=3.140164852142334
I0127 18:16:50.563424 139865266054912 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7482281923294067, loss=3.057849407196045
I0127 18:17:24.662831 139866171975424 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.765029788017273, loss=3.080000638961792
I0127 18:17:58.610593 139865266054912 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.702433466911316, loss=3.1114187240600586
I0127 18:18:32.562966 139866171975424 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7700175046920776, loss=3.131303310394287
I0127 18:18:58.500048 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:19:04.894864 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:19:13.468951 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:19:15.985347 140027215431488 submission_runner.py:408] Time since start: 18525.20s, 	Step: 52578, 	{'train/accuracy': 0.7335976958274841, 'train/loss': 1.282623529434204, 'validation/accuracy': 0.6645999550819397, 'validation/loss': 1.590146541595459, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.245861291885376, 'test/num_examples': 10000, 'score': 17887.110765457153, 'total_duration': 18525.204036712646, 'accumulated_submission_time': 17887.110765457153, 'accumulated_eval_time': 635.0826358795166, 'accumulated_logging_time': 1.1614611148834229}
I0127 18:19:16.011440 139865266054912 logging_writer.py:48] [52578] accumulated_eval_time=635.082636, accumulated_logging_time=1.161461, accumulated_submission_time=17887.110765, global_step=52578, preemption_count=0, score=17887.110765, test/accuracy=0.534700, test/loss=2.245861, test/num_examples=10000, total_duration=18525.204037, train/accuracy=0.733598, train/loss=1.282624, validation/accuracy=0.664600, validation/loss=1.590147, validation/num_examples=50000
I0127 18:19:23.782552 139865274447616 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.7515205144882202, loss=3.1285693645477295
I0127 18:19:57.703829 139865266054912 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.746337890625, loss=3.156083822250366
I0127 18:20:31.578295 139865274447616 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8712749481201172, loss=3.1448535919189453
I0127 18:21:05.507371 139865266054912 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.9680821895599365, loss=3.1136209964752197
I0127 18:21:39.426206 139865274447616 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8762900829315186, loss=3.153963804244995
I0127 18:22:13.363581 139865266054912 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.839756727218628, loss=3.103893518447876
I0127 18:22:47.305781 139865274447616 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.746787190437317, loss=3.1949753761291504
I0127 18:23:21.247607 139865266054912 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.9530932903289795, loss=3.184335231781006
I0127 18:23:55.283493 139865274447616 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8399213552474976, loss=3.1134896278381348
I0127 18:24:29.216086 139865266054912 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.8663841485977173, loss=3.193788766860962
I0127 18:25:03.098932 139865274447616 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.791961669921875, loss=3.1725382804870605
I0127 18:25:37.042794 139865266054912 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.7981011867523193, loss=3.1615207195281982
I0127 18:26:10.997225 139865274447616 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.9112482070922852, loss=3.18040132522583
I0127 18:26:44.923840 139865266054912 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.744596004486084, loss=3.123422384262085
I0127 18:27:18.852662 139865274447616 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8941349983215332, loss=3.088557004928589
I0127 18:27:46.164219 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:27:52.374358 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:28:01.315335 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:28:03.862914 140027215431488 submission_runner.py:408] Time since start: 19053.08s, 	Step: 54082, 	{'train/accuracy': 0.7420878410339355, 'train/loss': 1.2159703969955444, 'validation/accuracy': 0.6678999662399292, 'validation/loss': 1.5432895421981812, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.1653194427490234, 'test/num_examples': 10000, 'score': 18397.201429128647, 'total_duration': 19053.081587553024, 'accumulated_submission_time': 18397.201429128647, 'accumulated_eval_time': 652.7812848091125, 'accumulated_logging_time': 1.1971261501312256}
I0127 18:28:03.892825 139865266054912 logging_writer.py:48] [54082] accumulated_eval_time=652.781285, accumulated_logging_time=1.197126, accumulated_submission_time=18397.201429, global_step=54082, preemption_count=0, score=18397.201429, test/accuracy=0.545400, test/loss=2.165319, test/num_examples=10000, total_duration=19053.081588, train/accuracy=0.742088, train/loss=1.215970, validation/accuracy=0.667900, validation/loss=1.543290, validation/num_examples=50000
I0127 18:28:10.373261 139865760950016 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.7435358762741089, loss=3.1166679859161377
I0127 18:28:44.253548 139865266054912 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.701354742050171, loss=3.136464834213257
I0127 18:29:18.178399 139865760950016 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.6899412870407104, loss=3.1418986320495605
I0127 18:29:52.157116 139865266054912 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.975355625152588, loss=3.114428758621216
I0127 18:30:26.064534 139865760950016 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9609957933425903, loss=3.093632221221924
I0127 18:31:00.036854 139865266054912 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.8253228664398193, loss=3.1138997077941895
I0127 18:31:33.974784 139865760950016 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.883784294128418, loss=3.0955302715301514
I0127 18:32:07.883446 139865266054912 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8560051918029785, loss=3.062589168548584
I0127 18:32:41.824838 139865760950016 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.927406668663025, loss=3.1197094917297363
I0127 18:33:15.737600 139865266054912 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.6460567712783813, loss=3.0687763690948486
I0127 18:33:49.655093 139865760950016 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8042869567871094, loss=3.1135647296905518
I0127 18:34:23.606684 139865266054912 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.0144989490509033, loss=3.116086006164551
I0127 18:34:57.502215 139865760950016 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.9341046810150146, loss=3.1766715049743652
I0127 18:35:31.437498 139865266054912 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.779708981513977, loss=3.1017873287200928
I0127 18:36:05.554950 139865760950016 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.8993452787399292, loss=3.062014102935791
I0127 18:36:34.177232 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:36:40.358033 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:36:48.953605 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:36:51.480198 140027215431488 submission_runner.py:408] Time since start: 19580.70s, 	Step: 55586, 	{'train/accuracy': 0.7388392686843872, 'train/loss': 1.2988405227661133, 'validation/accuracy': 0.6522200107574463, 'validation/loss': 1.6849249601364136, 'validation/num_examples': 50000, 'test/accuracy': 0.5271000266075134, 'test/loss': 2.3522109985351562, 'test/num_examples': 10000, 'score': 18907.42284011841, 'total_duration': 19580.69886445999, 'accumulated_submission_time': 18907.42284011841, 'accumulated_eval_time': 670.0841941833496, 'accumulated_logging_time': 1.2361524105072021}
I0127 18:36:51.509985 139866180368128 logging_writer.py:48] [55586] accumulated_eval_time=670.084194, accumulated_logging_time=1.236152, accumulated_submission_time=18907.422840, global_step=55586, preemption_count=0, score=18907.422840, test/accuracy=0.527100, test/loss=2.352211, test/num_examples=10000, total_duration=19580.698864, train/accuracy=0.738839, train/loss=1.298841, validation/accuracy=0.652220, validation/loss=1.684925, validation/num_examples=50000
I0127 18:36:56.593827 139866188760832 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.1471335887908936, loss=3.1386866569519043
I0127 18:37:30.491993 139866180368128 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.7925488948822021, loss=3.0982446670532227
I0127 18:38:04.383708 139866188760832 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.9649890661239624, loss=3.134645700454712
I0127 18:38:38.299285 139866180368128 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.8767127990722656, loss=3.1503796577453613
I0127 18:39:12.215038 139866188760832 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7620538473129272, loss=3.111032485961914
I0127 18:39:46.135946 139866180368128 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.802875280380249, loss=3.163071632385254
I0127 18:40:20.059144 139866188760832 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8332037925720215, loss=3.1089251041412354
I0127 18:40:53.985479 139866180368128 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.9530082941055298, loss=3.1315150260925293
I0127 18:41:27.923433 139866188760832 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.9623702764511108, loss=3.081239938735962
I0127 18:42:01.839194 139866180368128 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.9214283227920532, loss=3.1086103916168213
I0127 18:42:35.915062 139866188760832 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8918403387069702, loss=3.11326265335083
I0127 18:43:09.842134 139866180368128 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9939820766448975, loss=3.1594715118408203
I0127 18:43:43.774029 139866188760832 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.7475258111953735, loss=3.090219020843506
I0127 18:44:17.739412 139866180368128 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8989062309265137, loss=3.08267879486084
I0127 18:44:51.673629 139866188760832 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.855047345161438, loss=3.12373685836792
I0127 18:45:21.680151 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:45:27.914455 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:45:36.494946 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:45:38.989311 140027215431488 submission_runner.py:408] Time since start: 20108.21s, 	Step: 57090, 	{'train/accuracy': 0.7461535334587097, 'train/loss': 1.222550630569458, 'validation/accuracy': 0.6632999777793884, 'validation/loss': 1.5833584070205688, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.2467947006225586, 'test/num_examples': 10000, 'score': 19417.531020641327, 'total_duration': 20108.20799922943, 'accumulated_submission_time': 19417.531020641327, 'accumulated_eval_time': 687.3933174610138, 'accumulated_logging_time': 1.2751078605651855}
I0127 18:45:39.016998 139865760950016 logging_writer.py:48] [57090] accumulated_eval_time=687.393317, accumulated_logging_time=1.275108, accumulated_submission_time=19417.531021, global_step=57090, preemption_count=0, score=19417.531021, test/accuracy=0.531300, test/loss=2.246795, test/num_examples=10000, total_duration=20108.207999, train/accuracy=0.746154, train/loss=1.222551, validation/accuracy=0.663300, validation/loss=1.583358, validation/num_examples=50000
I0127 18:45:42.756600 139865769342720 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.9649021625518799, loss=3.1657915115356445
I0127 18:46:16.675505 139865760950016 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.079413414001465, loss=3.1454482078552246
I0127 18:46:50.565329 139865769342720 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.8416721820831299, loss=3.080057144165039
I0127 18:47:24.495334 139865760950016 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.885663390159607, loss=3.1309561729431152
I0127 18:47:58.421861 139865769342720 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.954088807106018, loss=3.094086170196533
I0127 18:48:32.446627 139865760950016 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.8799798488616943, loss=3.140617609024048
I0127 18:49:06.386219 139865769342720 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.8261038064956665, loss=3.092966318130493
I0127 18:49:40.296555 139865760950016 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8307785987854004, loss=3.101790428161621
I0127 18:50:14.218113 139865769342720 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.920465350151062, loss=3.0333926677703857
I0127 18:50:48.116849 139865760950016 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.8902240991592407, loss=3.1605429649353027
I0127 18:51:22.040241 139865769342720 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.8615680932998657, loss=3.182945489883423
I0127 18:51:55.971181 139865760950016 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.982350468635559, loss=3.1584079265594482
I0127 18:52:29.866494 139865769342720 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.965085506439209, loss=3.1120975017547607
I0127 18:53:03.782420 139865760950016 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9013195037841797, loss=3.123411178588867
I0127 18:53:37.687994 139865769342720 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9085453748703003, loss=3.122591257095337
I0127 18:54:09.075639 140027215431488 spec.py:321] Evaluating on the training split.
I0127 18:54:15.481216 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 18:54:23.979264 140027215431488 spec.py:349] Evaluating on the test split.
I0127 18:54:26.482120 140027215431488 submission_runner.py:408] Time since start: 20635.70s, 	Step: 58594, 	{'train/accuracy': 0.750418484210968, 'train/loss': 1.2128936052322388, 'validation/accuracy': 0.6708599925041199, 'validation/loss': 1.5625097751617432, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.2097489833831787, 'test/num_examples': 10000, 'score': 19927.526131629944, 'total_duration': 20635.700806617737, 'accumulated_submission_time': 19927.526131629944, 'accumulated_eval_time': 704.799777507782, 'accumulated_logging_time': 1.3132286071777344}
I0127 18:54:26.514406 139866180368128 logging_writer.py:48] [58594] accumulated_eval_time=704.799778, accumulated_logging_time=1.313229, accumulated_submission_time=19927.526132, global_step=58594, preemption_count=0, score=19927.526132, test/accuracy=0.546300, test/loss=2.209749, test/num_examples=10000, total_duration=20635.700807, train/accuracy=0.750418, train/loss=1.212894, validation/accuracy=0.670860, validation/loss=1.562510, validation/num_examples=50000
I0127 18:54:28.903226 139866188760832 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9397742748260498, loss=3.0139670372009277
I0127 18:55:02.962806 139866180368128 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.095970869064331, loss=3.1051528453826904
I0127 18:55:36.882618 139866188760832 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8953474760055542, loss=3.07466459274292
I0127 18:56:10.773859 139866180368128 logging_writer.py:48] [58900] global_step=58900, grad_norm=2.048083543777466, loss=3.1175377368927
I0127 18:56:44.719642 139866188760832 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.7852022647857666, loss=3.0728750228881836
I0127 18:57:18.663035 139866180368128 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.8928000926971436, loss=3.0967447757720947
I0127 18:57:52.575981 139866188760832 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.8685178756713867, loss=3.0869505405426025
I0127 18:58:26.495859 139866180368128 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.907639741897583, loss=3.0847785472869873
I0127 18:59:00.380049 139866188760832 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.9407051801681519, loss=3.1488873958587646
I0127 18:59:34.304316 139866180368128 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.011369466781616, loss=3.082364320755005
I0127 19:00:08.243729 139866188760832 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.0835118293762207, loss=3.1682660579681396
I0127 19:00:42.153768 139866180368128 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.126295804977417, loss=3.080031394958496
I0127 19:01:16.163606 139866188760832 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.7949217557907104, loss=3.076599597930908
I0127 19:01:50.088055 139866180368128 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.883946418762207, loss=3.084477424621582
I0127 19:02:24.006297 139866188760832 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.9391882419586182, loss=3.0741896629333496
I0127 19:02:56.703244 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:03:02.989172 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:03:11.602435 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:03:14.078849 140027215431488 submission_runner.py:408] Time since start: 21163.30s, 	Step: 60098, 	{'train/accuracy': 0.7451171875, 'train/loss': 1.1998909711837769, 'validation/accuracy': 0.6719799637794495, 'validation/loss': 1.529384970664978, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.1742067337036133, 'test/num_examples': 10000, 'score': 20437.654178380966, 'total_duration': 21163.29753088951, 'accumulated_submission_time': 20437.654178380966, 'accumulated_eval_time': 722.1753432750702, 'accumulated_logging_time': 1.3547327518463135}
I0127 19:03:14.111505 139865266054912 logging_writer.py:48] [60098] accumulated_eval_time=722.175343, accumulated_logging_time=1.354733, accumulated_submission_time=20437.654178, global_step=60098, preemption_count=0, score=20437.654178, test/accuracy=0.551700, test/loss=2.174207, test/num_examples=10000, total_duration=21163.297531, train/accuracy=0.745117, train/loss=1.199891, validation/accuracy=0.671980, validation/loss=1.529385, validation/num_examples=50000
I0127 19:03:15.156488 139865274447616 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.0220818519592285, loss=3.0725598335266113
I0127 19:03:49.061333 139865266054912 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.9027531147003174, loss=3.1162092685699463
I0127 19:04:22.961704 139865274447616 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.0468554496765137, loss=3.1645641326904297
I0127 19:04:56.871738 139865266054912 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9054641723632812, loss=3.1548917293548584
I0127 19:05:30.802451 139865274447616 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8120677471160889, loss=3.0719451904296875
I0127 19:06:04.706945 139865266054912 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9483200311660767, loss=3.18924880027771
I0127 19:06:38.629345 139865274447616 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.98297917842865, loss=3.058328151702881
I0127 19:07:12.593297 139865266054912 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.9185994863510132, loss=3.098548650741577
I0127 19:07:46.532661 139865274447616 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.0303761959075928, loss=3.2624974250793457
I0127 19:08:20.469702 139865266054912 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.9625535011291504, loss=3.2118654251098633
I0127 19:08:54.437132 139865274447616 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.9375025033950806, loss=3.18251633644104
I0127 19:09:28.376563 139865266054912 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.006373405456543, loss=3.1250476837158203
I0127 19:10:02.282105 139865274447616 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9132968187332153, loss=3.129271984100342
I0127 19:10:36.216137 139865266054912 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.9837404489517212, loss=3.085606336593628
I0127 19:11:10.148711 139865274447616 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9384617805480957, loss=3.079371690750122
I0127 19:11:44.087328 139865266054912 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.9196393489837646, loss=3.0820913314819336
I0127 19:11:44.094871 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:11:50.280380 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:11:59.063004 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:12:01.580740 140027215431488 submission_runner.py:408] Time since start: 21690.80s, 	Step: 61601, 	{'train/accuracy': 0.7535673975944519, 'train/loss': 1.186854362487793, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5134427547454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 2.1582529544830322, 'test/num_examples': 10000, 'score': 20947.574555158615, 'total_duration': 21690.799419879913, 'accumulated_submission_time': 20947.574555158615, 'accumulated_eval_time': 739.661140203476, 'accumulated_logging_time': 1.3968374729156494}
I0127 19:12:01.618040 139866163582720 logging_writer.py:48] [61601] accumulated_eval_time=739.661140, accumulated_logging_time=1.396837, accumulated_submission_time=20947.574555, global_step=61601, preemption_count=0, score=20947.574555, test/accuracy=0.552400, test/loss=2.158253, test/num_examples=10000, total_duration=21690.799420, train/accuracy=0.753567, train/loss=1.186854, validation/accuracy=0.680160, validation/loss=1.513443, validation/num_examples=50000
I0127 19:12:35.528108 139866171975424 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.837339997291565, loss=3.0442991256713867
I0127 19:13:09.427039 139866163582720 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.948789358139038, loss=3.124782085418701
I0127 19:13:43.413059 139866171975424 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.906291127204895, loss=3.1169822216033936
I0127 19:14:17.344167 139866163582720 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9651821851730347, loss=3.0235326290130615
I0127 19:14:51.267510 139866171975424 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.9135768413543701, loss=3.0967507362365723
I0127 19:15:25.170421 139866163582720 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.042006731033325, loss=3.057358503341675
I0127 19:15:59.098095 139866171975424 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.840524435043335, loss=3.120304822921753
I0127 19:16:33.028500 139866163582720 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.0654635429382324, loss=3.109330415725708
I0127 19:17:06.977225 139866171975424 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9853609800338745, loss=3.085602045059204
I0127 19:17:40.856842 139866163582720 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.018805980682373, loss=3.1037847995758057
I0127 19:18:14.789808 139866171975424 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.8731505870819092, loss=3.0822689533233643
I0127 19:18:48.686089 139866163582720 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.9943650960922241, loss=3.131450891494751
I0127 19:19:22.609755 139866171975424 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.1244940757751465, loss=3.158905029296875
I0127 19:19:56.587905 139866163582720 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.852853536605835, loss=3.038078784942627
I0127 19:20:30.503623 139866171975424 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.160085916519165, loss=3.221536636352539
I0127 19:20:31.678126 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:20:37.994642 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:20:46.576168 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:20:49.079061 140027215431488 submission_runner.py:408] Time since start: 22218.30s, 	Step: 63105, 	{'train/accuracy': 0.7455755472183228, 'train/loss': 1.2469745874404907, 'validation/accuracy': 0.6726199984550476, 'validation/loss': 1.5651296377182007, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.2219254970550537, 'test/num_examples': 10000, 'score': 21457.5686917305, 'total_duration': 22218.297739744186, 'accumulated_submission_time': 21457.5686917305, 'accumulated_eval_time': 757.0620410442352, 'accumulated_logging_time': 1.448808193206787}
I0127 19:20:49.107696 139865760950016 logging_writer.py:48] [63105] accumulated_eval_time=757.062041, accumulated_logging_time=1.448808, accumulated_submission_time=21457.568692, global_step=63105, preemption_count=0, score=21457.568692, test/accuracy=0.541100, test/loss=2.221925, test/num_examples=10000, total_duration=22218.297740, train/accuracy=0.745576, train/loss=1.246975, validation/accuracy=0.672620, validation/loss=1.565130, validation/num_examples=50000
I0127 19:21:21.676623 139865769342720 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.9435101747512817, loss=3.150928020477295
I0127 19:21:55.571612 139865760950016 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.7923530340194702, loss=3.0552566051483154
I0127 19:22:29.481716 139865769342720 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.9950803518295288, loss=3.0792758464813232
I0127 19:23:03.380161 139865760950016 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.026721715927124, loss=3.1162402629852295
I0127 19:23:37.323345 139865769342720 logging_writer.py:48] [63600] global_step=63600, grad_norm=2.084702253341675, loss=3.1470165252685547
I0127 19:24:11.222264 139865760950016 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.004631757736206, loss=3.13922119140625
I0127 19:24:45.144022 139865769342720 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.0908074378967285, loss=3.065746307373047
I0127 19:25:19.048808 139865760950016 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9818636178970337, loss=3.111574172973633
I0127 19:25:52.997260 139865769342720 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.8907102346420288, loss=3.0193307399749756
I0127 19:26:26.983225 139865760950016 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.041940927505493, loss=3.088087320327759
I0127 19:27:00.903350 139865769342720 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.898296594619751, loss=3.0039806365966797
I0127 19:27:34.798964 139865760950016 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.832234501838684, loss=3.0540003776550293
I0127 19:28:08.735023 139865769342720 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.815799355506897, loss=3.0526764392852783
I0127 19:28:42.654248 139865760950016 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.000643491744995, loss=3.133563995361328
I0127 19:29:16.567985 139865769342720 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.982047438621521, loss=3.0693745613098145
I0127 19:29:19.081939 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:29:25.330159 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:29:34.188757 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:29:36.705545 140027215431488 submission_runner.py:408] Time since start: 22745.92s, 	Step: 64609, 	{'train/accuracy': 0.7794363498687744, 'train/loss': 1.0946069955825806, 'validation/accuracy': 0.6756399869918823, 'validation/loss': 1.5386929512023926, 'validation/num_examples': 50000, 'test/accuracy': 0.5548000335693359, 'test/loss': 2.164292335510254, 'test/num_examples': 10000, 'score': 21967.481950998306, 'total_duration': 22745.924226760864, 'accumulated_submission_time': 21967.481950998306, 'accumulated_eval_time': 774.6855986118317, 'accumulated_logging_time': 1.487745761871338}
I0127 19:29:36.734515 139866180368128 logging_writer.py:48] [64609] accumulated_eval_time=774.685599, accumulated_logging_time=1.487746, accumulated_submission_time=21967.481951, global_step=64609, preemption_count=0, score=21967.481951, test/accuracy=0.554800, test/loss=2.164292, test/num_examples=10000, total_duration=22745.924227, train/accuracy=0.779436, train/loss=1.094607, validation/accuracy=0.675640, validation/loss=1.538693, validation/num_examples=50000
I0127 19:30:07.921423 139866188760832 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9853569269180298, loss=3.091703414916992
I0127 19:30:41.822449 139866180368128 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.166445732116699, loss=3.061685562133789
I0127 19:31:15.736999 139866188760832 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.06765079498291, loss=3.0864102840423584
I0127 19:31:49.645439 139866180368128 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.0914876461029053, loss=3.0803208351135254
I0127 19:32:23.647020 139866188760832 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.20165753364563, loss=3.149468421936035
I0127 19:32:57.568601 139866180368128 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.0547564029693604, loss=3.072965621948242
I0127 19:33:31.480122 139866188760832 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.181593179702759, loss=3.1856045722961426
I0127 19:34:05.405654 139866180368128 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.0700888633728027, loss=3.1460535526275635
I0127 19:34:39.333445 139866188760832 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.065903902053833, loss=3.1281073093414307
I0127 19:35:13.250948 139866180368128 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.9188232421875, loss=3.0735254287719727
I0127 19:35:47.157834 139866188760832 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.9435838460922241, loss=3.089526891708374
I0127 19:36:21.063626 139866180368128 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.045210838317871, loss=3.054267168045044
I0127 19:36:54.978744 139866188760832 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.9961122274398804, loss=3.068892002105713
I0127 19:37:28.901615 139866180368128 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.9596443176269531, loss=3.088031530380249
I0127 19:38:02.801140 139866188760832 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9177424907684326, loss=3.016516923904419
I0127 19:38:07.023998 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:38:13.255405 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:38:22.131915 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:38:24.927325 140027215431488 submission_runner.py:408] Time since start: 23274.15s, 	Step: 66114, 	{'train/accuracy': 0.7683154940605164, 'train/loss': 1.1131125688552856, 'validation/accuracy': 0.6814999580383301, 'validation/loss': 1.4990878105163574, 'validation/num_examples': 50000, 'test/accuracy': 0.5603000521659851, 'test/loss': 2.1476669311523438, 'test/num_examples': 10000, 'score': 22477.707414150238, 'total_duration': 23274.14600086212, 'accumulated_submission_time': 22477.707414150238, 'accumulated_eval_time': 792.5888900756836, 'accumulated_logging_time': 1.5288665294647217}
I0127 19:38:24.963297 139865274447616 logging_writer.py:48] [66114] accumulated_eval_time=792.588890, accumulated_logging_time=1.528867, accumulated_submission_time=22477.707414, global_step=66114, preemption_count=0, score=22477.707414, test/accuracy=0.560300, test/loss=2.147667, test/num_examples=10000, total_duration=23274.146001, train/accuracy=0.768315, train/loss=1.113113, validation/accuracy=0.681500, validation/loss=1.499088, validation/num_examples=50000
I0127 19:38:54.432494 139865760950016 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.9951521158218384, loss=3.106858253479004
I0127 19:39:28.348372 139865274447616 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.1583051681518555, loss=3.0996663570404053
I0127 19:40:02.216127 139865760950016 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9917323589324951, loss=3.0850465297698975
I0127 19:40:36.137019 139865274447616 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.9407435655593872, loss=3.0920791625976562
I0127 19:41:10.019170 139865760950016 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.015166997909546, loss=3.060056686401367
I0127 19:41:43.946326 139865274447616 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.9602686166763306, loss=3.1472229957580566
I0127 19:42:17.871482 139865760950016 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.2062315940856934, loss=3.080906867980957
I0127 19:42:51.797190 139865274447616 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.129836082458496, loss=3.116394281387329
I0127 19:43:25.753532 139865760950016 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.1467807292938232, loss=3.062063217163086
I0127 19:43:59.667366 139865274447616 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.031646251678467, loss=3.1278648376464844
I0127 19:44:33.579565 139865760950016 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.0320208072662354, loss=3.0575835704803467
I0127 19:45:07.563949 139865274447616 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.194871664047241, loss=3.0070581436157227
I0127 19:45:41.513011 139865760950016 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.0493311882019043, loss=3.0374767780303955
I0127 19:46:15.405427 139865274447616 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.0400729179382324, loss=3.040025234222412
I0127 19:46:49.367411 139865760950016 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.016967535018921, loss=3.10402512550354
I0127 19:46:54.928297 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:47:01.243377 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:47:10.133547 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:47:12.649291 140027215431488 submission_runner.py:408] Time since start: 23801.87s, 	Step: 67618, 	{'train/accuracy': 0.7588488459587097, 'train/loss': 1.1663302183151245, 'validation/accuracy': 0.6769199967384338, 'validation/loss': 1.5283010005950928, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.1961100101470947, 'test/num_examples': 10000, 'score': 22987.609982013702, 'total_duration': 23801.867978334427, 'accumulated_submission_time': 22987.609982013702, 'accumulated_eval_time': 810.3098471164703, 'accumulated_logging_time': 1.5757479667663574}
I0127 19:47:12.682184 139865266054912 logging_writer.py:48] [67618] accumulated_eval_time=810.309847, accumulated_logging_time=1.575748, accumulated_submission_time=22987.609982, global_step=67618, preemption_count=0, score=22987.609982, test/accuracy=0.541200, test/loss=2.196110, test/num_examples=10000, total_duration=23801.867978, train/accuracy=0.758849, train/loss=1.166330, validation/accuracy=0.676920, validation/loss=1.528301, validation/num_examples=50000
I0127 19:47:40.792715 139866171975424 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.1459851264953613, loss=3.0866191387176514
I0127 19:48:14.704571 139865266054912 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.06643009185791, loss=3.0725526809692383
I0127 19:48:48.613462 139866171975424 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.0803515911102295, loss=3.0573763847351074
I0127 19:49:22.552873 139865266054912 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.1253206729888916, loss=3.120018482208252
I0127 19:49:56.480235 139866171975424 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.072638511657715, loss=3.0805845260620117
I0127 19:50:30.389541 139865266054912 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.9811502695083618, loss=3.056042194366455
I0127 19:51:04.383761 139866171975424 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.0575308799743652, loss=3.034034252166748
I0127 19:51:38.339960 139865266054912 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.2213621139526367, loss=3.0734782218933105
I0127 19:52:12.289348 139866171975424 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.292862892150879, loss=3.0760741233825684
I0127 19:52:46.197665 139865266054912 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.29038405418396, loss=3.0079033374786377
I0127 19:53:20.142026 139866171975424 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.1153793334960938, loss=3.0629043579101562
I0127 19:53:54.034817 139865266054912 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0611748695373535, loss=3.0313258171081543
I0127 19:54:27.978848 139866171975424 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.9760334491729736, loss=3.146078586578369
I0127 19:55:01.860319 139865266054912 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.9001965522766113, loss=3.0234909057617188
I0127 19:55:35.799879 139866171975424 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.249223470687866, loss=3.1586434841156006
I0127 19:55:42.723010 140027215431488 spec.py:321] Evaluating on the training split.
I0127 19:55:48.894201 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 19:55:57.739639 140027215431488 spec.py:349] Evaluating on the test split.
I0127 19:56:00.206481 140027215431488 submission_runner.py:408] Time since start: 24329.43s, 	Step: 69122, 	{'train/accuracy': 0.7570351958274841, 'train/loss': 1.1902135610580444, 'validation/accuracy': 0.6778199672698975, 'validation/loss': 1.5349388122558594, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.1934776306152344, 'test/num_examples': 10000, 'score': 23497.588663101196, 'total_duration': 24329.425166130066, 'accumulated_submission_time': 23497.588663101196, 'accumulated_eval_time': 827.7932825088501, 'accumulated_logging_time': 1.6187632083892822}
I0127 19:56:00.239088 139865769342720 logging_writer.py:48] [69122] accumulated_eval_time=827.793283, accumulated_logging_time=1.618763, accumulated_submission_time=23497.588663, global_step=69122, preemption_count=0, score=23497.588663, test/accuracy=0.552000, test/loss=2.193478, test/num_examples=10000, total_duration=24329.425166, train/accuracy=0.757035, train/loss=1.190214, validation/accuracy=0.677820, validation/loss=1.534939, validation/num_examples=50000
I0127 19:56:27.042546 139866163582720 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.0734291076660156, loss=3.0345921516418457
I0127 19:57:00.930747 139865769342720 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.1461589336395264, loss=3.015550374984741
I0127 19:57:34.886664 139866163582720 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.9788113832473755, loss=3.011111259460449
I0127 19:58:08.793337 139865769342720 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.1866588592529297, loss=3.0904176235198975
I0127 19:58:42.710967 139866163582720 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.2005536556243896, loss=2.980454683303833
I0127 19:59:16.626438 139865769342720 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.0289204120635986, loss=3.0530691146850586
I0127 19:59:50.545022 139866163582720 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.4030978679656982, loss=3.0215277671813965
I0127 20:00:24.460160 139865769342720 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.087346076965332, loss=3.067274808883667
I0127 20:00:58.355993 139866163582720 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.0411105155944824, loss=3.0388569831848145
I0127 20:01:32.262840 139865769342720 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0553948879241943, loss=3.0668869018554688
I0127 20:02:06.159129 139866163582720 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.9127393960952759, loss=3.0057241916656494
I0127 20:02:40.060378 139865769342720 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.1699554920196533, loss=3.030294895172119
I0127 20:03:13.968589 139866163582720 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.1681864261627197, loss=3.0508956909179688
I0127 20:03:47.964578 139865769342720 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.054945468902588, loss=3.0255320072174072
I0127 20:04:21.876287 139866163582720 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.089769124984741, loss=3.0953361988067627
I0127 20:04:30.482068 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:04:36.700506 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:04:45.520848 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:04:47.983280 140027215431488 submission_runner.py:408] Time since start: 24857.20s, 	Step: 70627, 	{'train/accuracy': 0.7458944320678711, 'train/loss': 1.2468349933624268, 'validation/accuracy': 0.668999969959259, 'validation/loss': 1.5783573389053345, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.213312864303589, 'test/num_examples': 10000, 'score': 24007.770708084106, 'total_duration': 24857.20196413994, 'accumulated_submission_time': 24007.770708084106, 'accumulated_eval_time': 845.2944579124451, 'accumulated_logging_time': 1.6605701446533203}
I0127 20:04:48.016420 139866171975424 logging_writer.py:48] [70627] accumulated_eval_time=845.294458, accumulated_logging_time=1.660570, accumulated_submission_time=24007.770708, global_step=70627, preemption_count=0, score=24007.770708, test/accuracy=0.546700, test/loss=2.213313, test/num_examples=10000, total_duration=24857.201964, train/accuracy=0.745894, train/loss=1.246835, validation/accuracy=0.669000, validation/loss=1.578357, validation/num_examples=50000
I0127 20:05:13.123773 139866180368128 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.944357991218567, loss=3.027543544769287
I0127 20:05:47.017678 139866171975424 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.030526876449585, loss=3.0251028537750244
I0127 20:06:20.959036 139866180368128 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.3061187267303467, loss=3.08548641204834
I0127 20:06:54.836762 139866171975424 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.035059690475464, loss=3.0716280937194824
I0127 20:07:28.771136 139866180368128 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.2436954975128174, loss=3.1855101585388184
I0127 20:08:02.708446 139866171975424 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.9981610774993896, loss=3.1346099376678467
I0127 20:08:36.622373 139866180368128 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.233811855316162, loss=3.043030023574829
I0127 20:09:10.588540 139866171975424 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.9356580972671509, loss=3.03285551071167
I0127 20:09:44.674075 139866180368128 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.1195902824401855, loss=3.069042205810547
I0127 20:10:18.573891 139866171975424 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.147386074066162, loss=3.093911647796631
I0127 20:10:52.518489 139866180368128 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.017857789993286, loss=3.0455732345581055
I0127 20:11:26.422022 139866171975424 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.159494400024414, loss=3.117780923843384
I0127 20:12:00.345825 139866180368128 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.1580896377563477, loss=3.0514822006225586
I0127 20:12:34.235181 139866171975424 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.235656976699829, loss=3.1405444145202637
I0127 20:13:08.171769 139866180368128 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.051168203353882, loss=3.0632436275482178
I0127 20:13:18.152835 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:13:24.358984 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:13:33.276682 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:13:35.755689 140027215431488 submission_runner.py:408] Time since start: 25384.97s, 	Step: 72131, 	{'train/accuracy': 0.7466716766357422, 'train/loss': 1.2096285820007324, 'validation/accuracy': 0.6700999736785889, 'validation/loss': 1.548133373260498, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.200495719909668, 'test/num_examples': 10000, 'score': 24517.843168497086, 'total_duration': 25384.974372386932, 'accumulated_submission_time': 24517.843168497086, 'accumulated_eval_time': 862.8972687721252, 'accumulated_logging_time': 1.7031033039093018}
I0127 20:13:35.790125 139865760950016 logging_writer.py:48] [72131] accumulated_eval_time=862.897269, accumulated_logging_time=1.703103, accumulated_submission_time=24517.843168, global_step=72131, preemption_count=0, score=24517.843168, test/accuracy=0.552500, test/loss=2.200496, test/num_examples=10000, total_duration=25384.974372, train/accuracy=0.746672, train/loss=1.209629, validation/accuracy=0.670100, validation/loss=1.548133, validation/num_examples=50000
I0127 20:13:59.502484 139865769342720 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.127164125442505, loss=3.0024571418762207
I0127 20:14:33.410627 139865760950016 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.037475347518921, loss=3.085115909576416
I0127 20:15:07.290454 139865769342720 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.1560308933258057, loss=3.0812342166900635
I0127 20:15:41.190513 139865760950016 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.20943546295166, loss=3.141653537750244
I0127 20:16:15.226994 139865769342720 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.0231001377105713, loss=3.1288352012634277
I0127 20:16:49.138979 139865760950016 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.281420946121216, loss=3.028843641281128
I0127 20:17:23.059820 139865769342720 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.137953996658325, loss=3.0721096992492676
I0127 20:17:56.989231 139865760950016 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.64440655708313, loss=3.0431978702545166
I0127 20:18:30.886015 139865769342720 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.0727763175964355, loss=3.0634968280792236
I0127 20:19:04.809516 139865760950016 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.001987934112549, loss=3.0798661708831787
I0127 20:19:38.703781 139865769342720 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.0707645416259766, loss=3.0539040565490723
I0127 20:20:12.646791 139865760950016 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.111693859100342, loss=2.9963395595550537
I0127 20:20:46.507647 139865769342720 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.173548936843872, loss=3.098750114440918
I0127 20:21:20.416160 139865760950016 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.1099841594696045, loss=3.0807247161865234
I0127 20:21:54.283468 139865769342720 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.1756742000579834, loss=3.0075297355651855
I0127 20:22:06.024743 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:22:12.207664 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:22:21.132061 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:22:23.636169 140027215431488 submission_runner.py:408] Time since start: 25912.85s, 	Step: 73636, 	{'train/accuracy': 0.7932278513908386, 'train/loss': 1.0378259420394897, 'validation/accuracy': 0.6830999851226807, 'validation/loss': 1.5034915208816528, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.1635515689849854, 'test/num_examples': 10000, 'score': 25028.014729499817, 'total_duration': 25912.854856967926, 'accumulated_submission_time': 25028.014729499817, 'accumulated_eval_time': 880.508659362793, 'accumulated_logging_time': 1.7472789287567139}
I0127 20:22:23.671477 139865274447616 logging_writer.py:48] [73636] accumulated_eval_time=880.508659, accumulated_logging_time=1.747279, accumulated_submission_time=25028.014729, global_step=73636, preemption_count=0, score=25028.014729, test/accuracy=0.556600, test/loss=2.163552, test/num_examples=10000, total_duration=25912.854857, train/accuracy=0.793228, train/loss=1.037826, validation/accuracy=0.683100, validation/loss=1.503492, validation/num_examples=50000
I0127 20:22:45.692826 139866171975424 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0921525955200195, loss=3.054297924041748
I0127 20:23:19.604394 139865274447616 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.1205406188964844, loss=3.1128368377685547
I0127 20:23:53.498497 139866171975424 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.0719096660614014, loss=3.0662198066711426
I0127 20:24:27.412869 139865274447616 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9951066970825195, loss=2.9861409664154053
I0127 20:25:01.305522 139866171975424 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0466830730438232, loss=3.052866220474243
I0127 20:25:35.216183 139865274447616 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.1925785541534424, loss=3.0161185264587402
I0127 20:26:09.147439 139866171975424 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.0902931690216064, loss=3.082314968109131
I0127 20:26:43.054514 139865274447616 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.12631893157959, loss=3.116201400756836
I0127 20:27:16.969204 139866171975424 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.16804575920105, loss=3.059384346008301
I0127 20:27:50.883783 139865274447616 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.0606632232666016, loss=3.0640335083007812
I0127 20:28:24.879498 139866171975424 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.1881613731384277, loss=3.0774245262145996
I0127 20:28:58.802872 139865274447616 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.9843775033950806, loss=2.995267868041992
I0127 20:29:32.732444 139866171975424 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.1158695220947266, loss=3.0706963539123535
I0127 20:30:06.642490 139865274447616 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.3440613746643066, loss=3.1228063106536865
I0127 20:30:40.568944 139866171975424 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.1984522342681885, loss=3.059784173965454
I0127 20:30:53.919270 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:31:00.173691 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:31:08.923808 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:31:11.427182 140027215431488 submission_runner.py:408] Time since start: 26440.65s, 	Step: 75141, 	{'train/accuracy': 0.7795758843421936, 'train/loss': 1.0734102725982666, 'validation/accuracy': 0.6831799745559692, 'validation/loss': 1.4843811988830566, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 2.1242897510528564, 'test/num_examples': 10000, 'score': 25538.201239585876, 'total_duration': 26440.645871162415, 'accumulated_submission_time': 25538.201239585876, 'accumulated_eval_time': 898.0165367126465, 'accumulated_logging_time': 1.791778802871704}
I0127 20:31:11.458260 139866163582720 logging_writer.py:48] [75141] accumulated_eval_time=898.016537, accumulated_logging_time=1.791779, accumulated_submission_time=25538.201240, global_step=75141, preemption_count=0, score=25538.201240, test/accuracy=0.561500, test/loss=2.124290, test/num_examples=10000, total_duration=26440.645871, train/accuracy=0.779576, train/loss=1.073410, validation/accuracy=0.683180, validation/loss=1.484381, validation/num_examples=50000
I0127 20:31:31.775919 139866171975424 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.1170101165771484, loss=3.0217599868774414
I0127 20:32:05.673550 139866163582720 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1439578533172607, loss=3.0019912719726562
I0127 20:32:39.557871 139866171975424 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.0884549617767334, loss=2.9713470935821533
I0127 20:33:13.470127 139866163582720 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.195706605911255, loss=3.0687808990478516
I0127 20:33:47.406948 139866171975424 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.213292360305786, loss=3.0432136058807373
I0127 20:34:21.301410 139866163582720 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.1831676959991455, loss=3.0492093563079834
I0127 20:34:55.423404 139866171975424 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.1646997928619385, loss=3.0521650314331055
I0127 20:35:29.310781 139866163582720 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.132993221282959, loss=3.094111442565918
I0127 20:36:03.195733 139866171975424 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.337796449661255, loss=3.01495099067688
I0127 20:36:37.110813 139866163582720 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.2946908473968506, loss=3.0145442485809326
I0127 20:37:10.976458 139866171975424 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.1600778102874756, loss=2.9776880741119385
I0127 20:37:44.898129 139866163582720 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1421148777008057, loss=2.9689877033233643
I0127 20:38:18.772978 139866171975424 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.1541764736175537, loss=3.08453106880188
I0127 20:38:52.684561 139866163582720 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.2903380393981934, loss=3.0241622924804688
I0127 20:39:26.567843 139866171975424 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.3161678314208984, loss=3.0819051265716553
I0127 20:39:41.627229 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:39:47.937435 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:39:56.821302 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:39:59.280746 140027215431488 submission_runner.py:408] Time since start: 26968.50s, 	Step: 76646, 	{'train/accuracy': 0.7704480290412903, 'train/loss': 1.1027264595031738, 'validation/accuracy': 0.6850999593734741, 'validation/loss': 1.478226661682129, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.119495391845703, 'test/num_examples': 10000, 'score': 26048.3084192276, 'total_duration': 26968.49898505211, 'accumulated_submission_time': 26048.3084192276, 'accumulated_eval_time': 915.6695799827576, 'accumulated_logging_time': 1.8321490287780762}
I0127 20:39:59.312467 139865760950016 logging_writer.py:48] [76646] accumulated_eval_time=915.669580, accumulated_logging_time=1.832149, accumulated_submission_time=26048.308419, global_step=76646, preemption_count=0, score=26048.308419, test/accuracy=0.556900, test/loss=2.119495, test/num_examples=10000, total_duration=26968.498985, train/accuracy=0.770448, train/loss=1.102726, validation/accuracy=0.685100, validation/loss=1.478227, validation/num_examples=50000
I0127 20:40:17.957130 139865769342720 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.064103603363037, loss=3.1406986713409424
I0127 20:40:51.943815 139865760950016 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.2258987426757812, loss=3.034461498260498
I0127 20:41:25.857244 139865769342720 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.2176008224487305, loss=3.031324863433838
I0127 20:41:59.717546 139865760950016 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1838552951812744, loss=3.003460645675659
I0127 20:42:33.643650 139865769342720 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.268557071685791, loss=3.0452260971069336
I0127 20:43:07.517002 139865760950016 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.2050163745880127, loss=3.008272171020508
I0127 20:43:41.447082 139865769342720 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.048100709915161, loss=3.064724922180176
I0127 20:44:15.319703 139865760950016 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.201075315475464, loss=3.0754032135009766
I0127 20:44:49.238978 139865769342720 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.2238104343414307, loss=3.0609970092773438
I0127 20:45:23.120706 139865760950016 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.0651352405548096, loss=3.035097122192383
I0127 20:45:57.045359 139865769342720 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.2481040954589844, loss=3.028170585632324
I0127 20:46:30.927871 139865760950016 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.3637564182281494, loss=3.050577163696289
I0127 20:47:04.893045 139865769342720 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.0756657123565674, loss=2.979949474334717
I0127 20:47:38.812938 139865760950016 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.096482038497925, loss=2.9991202354431152
I0127 20:48:12.705647 139865769342720 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.3322501182556152, loss=2.9925167560577393
I0127 20:48:29.498465 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:48:35.694787 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:48:44.258805 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:48:46.715453 140027215431488 submission_runner.py:408] Time since start: 27495.93s, 	Step: 78151, 	{'train/accuracy': 0.7723413705825806, 'train/loss': 1.126224398612976, 'validation/accuracy': 0.6868199706077576, 'validation/loss': 1.4868748188018799, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 2.1145002841949463, 'test/num_examples': 10000, 'score': 26558.43163084984, 'total_duration': 27495.934143304825, 'accumulated_submission_time': 26558.43163084984, 'accumulated_eval_time': 932.8865323066711, 'accumulated_logging_time': 1.8738563060760498}
I0127 20:48:46.749336 139865266054912 logging_writer.py:48] [78151] accumulated_eval_time=932.886532, accumulated_logging_time=1.873856, accumulated_submission_time=26558.431631, global_step=78151, preemption_count=0, score=26558.431631, test/accuracy=0.566400, test/loss=2.114500, test/num_examples=10000, total_duration=27495.934143, train/accuracy=0.772341, train/loss=1.126224, validation/accuracy=0.686820, validation/loss=1.486875, validation/num_examples=50000
I0127 20:49:03.699905 139865274447616 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.109009265899658, loss=3.0482091903686523
I0127 20:49:37.576036 139865266054912 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.10540509223938, loss=3.022810220718384
I0127 20:50:11.477835 139865274447616 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.251603126525879, loss=3.0363292694091797
I0127 20:50:45.381812 139865266054912 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.103182792663574, loss=2.991830825805664
I0127 20:51:19.298935 139865274447616 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.415022373199463, loss=3.0549445152282715
I0127 20:51:53.212060 139865266054912 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.163069248199463, loss=3.054534912109375
I0127 20:52:27.145851 139865274447616 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.1948599815368652, loss=2.956990957260132
I0127 20:53:01.054813 139865266054912 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.1174099445343018, loss=2.9976015090942383
I0127 20:53:35.032569 139865274447616 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.1577720642089844, loss=3.0656137466430664
I0127 20:54:08.957267 139865266054912 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.1751394271850586, loss=3.0394458770751953
I0127 20:54:42.868120 139865274447616 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.2509357929229736, loss=3.075111150741577
I0127 20:55:16.782693 139865266054912 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.307422161102295, loss=3.0001280307769775
I0127 20:55:50.690156 139865274447616 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.233604669570923, loss=3.049631118774414
I0127 20:56:24.590208 139865266054912 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.210386276245117, loss=3.127474069595337
I0127 20:56:58.498490 139865274447616 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.2265899181365967, loss=3.0319879055023193
I0127 20:57:16.942795 140027215431488 spec.py:321] Evaluating on the training split.
I0127 20:57:23.215743 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 20:57:32.188783 140027215431488 spec.py:349] Evaluating on the test split.
I0127 20:57:34.637124 140027215431488 submission_runner.py:408] Time since start: 28023.86s, 	Step: 79656, 	{'train/accuracy': 0.7729591727256775, 'train/loss': 1.1393262147903442, 'validation/accuracy': 0.6887999773025513, 'validation/loss': 1.5029897689819336, 'validation/num_examples': 50000, 'test/accuracy': 0.5633000135421753, 'test/loss': 2.1523826122283936, 'test/num_examples': 10000, 'score': 27068.563962221146, 'total_duration': 28023.855808973312, 'accumulated_submission_time': 27068.563962221146, 'accumulated_eval_time': 950.5808329582214, 'accumulated_logging_time': 1.9172337055206299}
I0127 20:57:34.672810 139865760950016 logging_writer.py:48] [79656] accumulated_eval_time=950.580833, accumulated_logging_time=1.917234, accumulated_submission_time=27068.563962, global_step=79656, preemption_count=0, score=27068.563962, test/accuracy=0.563300, test/loss=2.152383, test/num_examples=10000, total_duration=28023.855809, train/accuracy=0.772959, train/loss=1.139326, validation/accuracy=0.688800, validation/loss=1.502990, validation/num_examples=50000
I0127 20:57:49.927247 139865769342720 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.198265790939331, loss=3.0000526905059814
I0127 20:58:23.814408 139865760950016 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.1478824615478516, loss=2.980787992477417
I0127 20:58:57.697451 139865769342720 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.350905656814575, loss=3.071401357650757
I0127 20:59:31.605434 139865760950016 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.283158779144287, loss=3.004741907119751
I0127 21:00:05.593941 139865769342720 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.240389585494995, loss=3.0570788383483887
I0127 21:00:39.481058 139865760950016 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.1576943397521973, loss=3.0291261672973633
I0127 21:01:13.394763 139865769342720 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.2759788036346436, loss=3.085014581680298
I0127 21:01:47.291275 139865760950016 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.3367536067962646, loss=3.013017177581787
I0127 21:02:21.192936 139865769342720 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.268101930618286, loss=3.0113325119018555
I0127 21:02:55.132448 139865760950016 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.125296115875244, loss=3.027228355407715
I0127 21:03:29.028656 139865769342720 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.167682647705078, loss=3.028918504714966
I0127 21:04:02.978693 139865760950016 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.3159122467041016, loss=3.034614086151123
I0127 21:04:36.860893 139865769342720 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.147095203399658, loss=2.9412739276885986
I0127 21:05:10.816506 139865760950016 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.31406831741333, loss=3.0015416145324707
I0127 21:05:44.793158 139865769342720 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1428823471069336, loss=3.009960174560547
I0127 21:06:04.938260 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:06:11.937689 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:06:20.879889 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:06:23.334668 140027215431488 submission_runner.py:408] Time since start: 28552.55s, 	Step: 81161, 	{'train/accuracy': 0.7640505433082581, 'train/loss': 1.1171501874923706, 'validation/accuracy': 0.682379961013794, 'validation/loss': 1.4879083633422852, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.143951177597046, 'test/num_examples': 10000, 'score': 27578.765528678894, 'total_duration': 28552.553351163864, 'accumulated_submission_time': 27578.765528678894, 'accumulated_eval_time': 968.9772000312805, 'accumulated_logging_time': 1.9648942947387695}
I0127 21:06:23.367818 139865274447616 logging_writer.py:48] [81161] accumulated_eval_time=968.977200, accumulated_logging_time=1.964894, accumulated_submission_time=27578.765529, global_step=81161, preemption_count=0, score=27578.765529, test/accuracy=0.553300, test/loss=2.143951, test/num_examples=10000, total_duration=28552.553351, train/accuracy=0.764051, train/loss=1.117150, validation/accuracy=0.682380, validation/loss=1.487908, validation/num_examples=50000
I0127 21:06:36.933847 139866163582720 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.1034884452819824, loss=2.958960771560669
I0127 21:07:10.840829 139865274447616 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.3561575412750244, loss=3.020031213760376
I0127 21:07:44.704508 139866163582720 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.2617242336273193, loss=3.0040411949157715
I0127 21:08:18.595246 139865274447616 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.494398355484009, loss=3.141390562057495
I0127 21:08:52.502592 139866163582720 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.234886407852173, loss=3.011918783187866
I0127 21:09:26.443716 139865274447616 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.1008517742156982, loss=3.048867702484131
I0127 21:10:00.301507 139866163582720 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.269509792327881, loss=3.0275254249572754
I0127 21:10:34.593100 139865274447616 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.2259559631347656, loss=2.979186773300171
I0127 21:11:08.513937 139866163582720 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.2164058685302734, loss=3.016003131866455
I0127 21:11:42.425738 139865274447616 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.30425763130188, loss=2.9834718704223633
I0127 21:12:16.445984 139866163582720 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.394897222518921, loss=2.997074604034424
I0127 21:12:50.370827 139865274447616 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.215785503387451, loss=3.0229525566101074
I0127 21:13:24.260615 139866163582720 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.1026771068573, loss=3.004220962524414
I0127 21:13:58.154561 139865274447616 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.133047580718994, loss=3.0245771408081055
I0127 21:14:32.085515 139866163582720 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.373739719390869, loss=3.0843098163604736
I0127 21:14:53.598961 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:14:59.808928 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:15:08.499675 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:15:11.081679 140027215431488 submission_runner.py:408] Time since start: 29080.30s, 	Step: 82665, 	{'train/accuracy': 0.8116629123687744, 'train/loss': 0.9491603970527649, 'validation/accuracy': 0.6913999915122986, 'validation/loss': 1.4522778987884521, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 2.0855112075805664, 'test/num_examples': 10000, 'score': 28088.932448387146, 'total_duration': 29080.300355911255, 'accumulated_submission_time': 28088.932448387146, 'accumulated_eval_time': 986.45987200737, 'accumulated_logging_time': 2.0094380378723145}
I0127 21:15:11.117796 139865760950016 logging_writer.py:48] [82665] accumulated_eval_time=986.459872, accumulated_logging_time=2.009438, accumulated_submission_time=28088.932448, global_step=82665, preemption_count=0, score=28088.932448, test/accuracy=0.569000, test/loss=2.085511, test/num_examples=10000, total_duration=29080.300356, train/accuracy=0.811663, train/loss=0.949160, validation/accuracy=0.691400, validation/loss=1.452278, validation/num_examples=50000
I0127 21:15:23.304022 139865769342720 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.4438891410827637, loss=3.0641143321990967
I0127 21:15:57.208377 139865760950016 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.162451982498169, loss=2.969465494155884
I0127 21:16:31.093522 139865769342720 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.3127546310424805, loss=3.052478790283203
I0127 21:17:05.015932 139865760950016 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.2084896564483643, loss=2.957019805908203
I0127 21:17:38.929788 139865769342720 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.3725993633270264, loss=3.0444705486297607
I0127 21:18:12.846000 139865760950016 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.0896248817443848, loss=2.9263970851898193
I0127 21:18:46.800735 139865769342720 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.163891077041626, loss=3.0257620811462402
I0127 21:19:20.709282 139865760950016 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.2336318492889404, loss=3.0277099609375
I0127 21:19:54.623747 139865769342720 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2261877059936523, loss=3.0085268020629883
I0127 21:20:28.555884 139865760950016 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.196296453475952, loss=3.0294480323791504
I0127 21:21:02.468613 139865769342720 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.463750123977661, loss=3.0599205493927
I0127 21:21:36.368412 139865760950016 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.154149055480957, loss=3.077598810195923
I0127 21:22:10.321746 139865769342720 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.492617607116699, loss=3.051734209060669
I0127 21:22:44.213454 139865760950016 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.296804904937744, loss=3.0690104961395264
I0127 21:23:18.118946 139865769342720 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.613776206970215, loss=3.032179117202759
I0127 21:23:41.330003 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:23:47.497939 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:23:56.316830 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:23:58.852359 140027215431488 submission_runner.py:408] Time since start: 29608.07s, 	Step: 84170, 	{'train/accuracy': 0.786152720451355, 'train/loss': 1.03113853931427, 'validation/accuracy': 0.6920199990272522, 'validation/loss': 1.4382027387619019, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 2.106266975402832, 'test/num_examples': 10000, 'score': 28599.081391334534, 'total_duration': 29608.071023464203, 'accumulated_submission_time': 28599.081391334534, 'accumulated_eval_time': 1003.9821727275848, 'accumulated_logging_time': 2.0565803050994873}
I0127 21:23:58.885729 139865266054912 logging_writer.py:48] [84170] accumulated_eval_time=1003.982173, accumulated_logging_time=2.056580, accumulated_submission_time=28599.081391, global_step=84170, preemption_count=0, score=28599.081391, test/accuracy=0.563800, test/loss=2.106267, test/num_examples=10000, total_duration=29608.071023, train/accuracy=0.786153, train/loss=1.031139, validation/accuracy=0.692020, validation/loss=1.438203, validation/num_examples=50000
I0127 21:24:09.425733 139865274447616 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.3318064212799072, loss=3.069946765899658
I0127 21:24:43.405737 139865266054912 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.13045334815979, loss=2.981826066970825
I0127 21:25:17.275650 139865274447616 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.304675340652466, loss=3.02347469329834
I0127 21:25:51.174938 139865266054912 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.164379119873047, loss=2.9961090087890625
I0127 21:26:25.070946 139865274447616 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.3069136142730713, loss=3.0322632789611816
I0127 21:26:58.957869 139865266054912 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.23193097114563, loss=2.945046901702881
I0127 21:27:32.888709 139865274447616 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.2751662731170654, loss=2.9327573776245117
I0127 21:28:06.814124 139865266054912 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.5058341026306152, loss=3.1076693534851074
I0127 21:28:40.696220 139865274447616 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.4851865768432617, loss=3.0228219032287598
I0127 21:29:14.627662 139865266054912 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.331799268722534, loss=3.067534923553467
I0127 21:29:48.499507 139865274447616 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.2224037647247314, loss=3.0563271045684814
I0127 21:30:22.430843 139865266054912 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.248760461807251, loss=2.9738454818725586
I0127 21:30:56.433897 139865274447616 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.5318679809570312, loss=3.036017656326294
I0127 21:31:30.327116 139865266054912 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.156620979309082, loss=2.982095241546631
I0127 21:32:04.254245 139865274447616 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.143570899963379, loss=3.0128939151763916
I0127 21:32:29.141393 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:32:35.346739 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:32:43.944685 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:32:46.453257 140027215431488 submission_runner.py:408] Time since start: 30135.67s, 	Step: 85675, 	{'train/accuracy': 0.7844387888908386, 'train/loss': 1.050876259803772, 'validation/accuracy': 0.6937800049781799, 'validation/loss': 1.4405685663223267, 'validation/num_examples': 50000, 'test/accuracy': 0.5645000338554382, 'test/loss': 2.099759101867676, 'test/num_examples': 10000, 'score': 29109.275886297226, 'total_duration': 30135.671944856644, 'accumulated_submission_time': 29109.275886297226, 'accumulated_eval_time': 1021.2940018177032, 'accumulated_logging_time': 2.0996451377868652}
I0127 21:32:46.484752 139866163582720 logging_writer.py:48] [85675] accumulated_eval_time=1021.294002, accumulated_logging_time=2.099645, accumulated_submission_time=29109.275886, global_step=85675, preemption_count=0, score=29109.275886, test/accuracy=0.564500, test/loss=2.099759, test/num_examples=10000, total_duration=30135.671945, train/accuracy=0.784439, train/loss=1.050876, validation/accuracy=0.693780, validation/loss=1.440569, validation/num_examples=50000
I0127 21:32:55.328741 139866171975424 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.319995641708374, loss=3.0756030082702637
I0127 21:33:29.187475 139866163582720 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.3921961784362793, loss=3.036043167114258
I0127 21:34:03.069032 139866171975424 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.4514529705047607, loss=3.0454471111297607
I0127 21:34:36.985076 139866163582720 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.4174342155456543, loss=3.0686821937561035
I0127 21:35:10.902672 139866171975424 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.3284244537353516, loss=2.9883227348327637
I0127 21:35:44.777656 139866163582720 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.3059301376342773, loss=2.963283061981201
I0127 21:36:18.679307 139866171975424 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.487712860107422, loss=3.0806171894073486
I0127 21:36:52.601335 139866163582720 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.2738285064697266, loss=2.9453675746917725
I0127 21:37:26.549786 139866171975424 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.194247245788574, loss=2.9339914321899414
I0127 21:38:00.473863 139866163582720 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.2277214527130127, loss=3.0111968517303467
I0127 21:38:34.384269 139866171975424 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3681743144989014, loss=3.057133436203003
I0127 21:39:08.332179 139866163582720 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.4466631412506104, loss=3.0505874156951904
I0127 21:39:42.255103 139866171975424 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.1794354915618896, loss=2.991806983947754
I0127 21:40:16.179297 139866163582720 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.505413293838501, loss=2.9658493995666504
I0127 21:40:50.101652 139866171975424 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.3320388793945312, loss=3.0029661655426025
I0127 21:41:16.699371 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:41:22.906685 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:41:31.651228 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:41:34.194453 140027215431488 submission_runner.py:408] Time since start: 30663.41s, 	Step: 87180, 	{'train/accuracy': 0.7828842401504517, 'train/loss': 1.0602805614471436, 'validation/accuracy': 0.6989799737930298, 'validation/loss': 1.430122971534729, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 2.0682175159454346, 'test/num_examples': 10000, 'score': 29619.428687810898, 'total_duration': 30663.413135290146, 'accumulated_submission_time': 29619.428687810898, 'accumulated_eval_time': 1038.789042711258, 'accumulated_logging_time': 2.1402103900909424}
I0127 21:41:34.224862 139865274447616 logging_writer.py:48] [87180] accumulated_eval_time=1038.789043, accumulated_logging_time=2.140210, accumulated_submission_time=29619.428688, global_step=87180, preemption_count=0, score=29619.428688, test/accuracy=0.571700, test/loss=2.068218, test/num_examples=10000, total_duration=30663.413135, train/accuracy=0.782884, train/loss=1.060281, validation/accuracy=0.698980, validation/loss=1.430123, validation/num_examples=50000
I0127 21:41:41.333030 139865760950016 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.392550230026245, loss=3.0323548316955566
I0127 21:42:15.201242 139865274447616 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2325220108032227, loss=2.930394411087036
I0127 21:42:49.102966 139865760950016 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.282972812652588, loss=2.980013608932495
I0127 21:43:23.094037 139865274447616 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.404741048812866, loss=3.013012647628784
I0127 21:43:56.999343 139865760950016 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.369746208190918, loss=2.9692111015319824
I0127 21:44:30.922933 139865274447616 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.3384220600128174, loss=2.972839593887329
I0127 21:45:04.829731 139865760950016 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.4753944873809814, loss=3.003429889678955
I0127 21:45:38.746234 139865274447616 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.343183755874634, loss=2.974454402923584
I0127 21:46:12.661971 139865760950016 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.365182638168335, loss=2.943363666534424
I0127 21:46:46.574727 139865274447616 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3904054164886475, loss=3.022264003753662
I0127 21:47:20.508161 139865760950016 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3058323860168457, loss=2.9552903175354004
I0127 21:47:54.413891 139865274447616 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.330242156982422, loss=2.988382339477539
I0127 21:48:28.334240 139865760950016 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.5542962551116943, loss=2.998952627182007
I0127 21:49:02.247747 139865274447616 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.5518882274627686, loss=2.9569878578186035
I0127 21:49:36.317598 139865760950016 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.395869493484497, loss=2.9986658096313477
I0127 21:50:04.278964 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:50:10.495252 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:50:19.322826 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:50:21.867480 140027215431488 submission_runner.py:408] Time since start: 31191.09s, 	Step: 88684, 	{'train/accuracy': 0.7784797549247742, 'train/loss': 1.0503365993499756, 'validation/accuracy': 0.6977999806404114, 'validation/loss': 1.412257194519043, 'validation/num_examples': 50000, 'test/accuracy': 0.565500020980835, 'test/loss': 2.0685789585113525, 'test/num_examples': 10000, 'score': 30129.41860818863, 'total_duration': 31191.086156368256, 'accumulated_submission_time': 30129.41860818863, 'accumulated_eval_time': 1056.377511024475, 'accumulated_logging_time': 2.181400775909424}
I0127 21:50:21.904210 139865274447616 logging_writer.py:48] [88684] accumulated_eval_time=1056.377511, accumulated_logging_time=2.181401, accumulated_submission_time=30129.418608, global_step=88684, preemption_count=0, score=30129.418608, test/accuracy=0.565500, test/loss=2.068579, test/num_examples=10000, total_duration=31191.086156, train/accuracy=0.778480, train/loss=1.050337, validation/accuracy=0.697800, validation/loss=1.412257, validation/num_examples=50000
I0127 21:50:27.663936 139866171975424 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.3200573921203613, loss=2.9124057292938232
I0127 21:51:01.592818 139865274447616 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.263028383255005, loss=2.9610939025878906
I0127 21:51:35.460695 139866171975424 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.3173458576202393, loss=2.998002529144287
I0127 21:52:09.399296 139865274447616 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.4865174293518066, loss=2.979278802871704
I0127 21:52:43.289838 139866171975424 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3387811183929443, loss=2.9549973011016846
I0127 21:53:17.224979 139865274447616 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.379484176635742, loss=2.8825855255126953
I0127 21:53:51.116373 139866171975424 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.246311902999878, loss=2.9864048957824707
I0127 21:54:25.036983 139865274447616 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.524900197982788, loss=2.965789556503296
I0127 21:54:58.913930 139866171975424 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.228193759918213, loss=2.9637861251831055
I0127 21:55:32.871356 139865274447616 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2531940937042236, loss=3.008127450942993
I0127 21:56:06.866383 139866171975424 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.25657057762146, loss=2.8922696113586426
I0127 21:56:40.772377 139865274447616 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.5683319568634033, loss=3.05330228805542
I0127 21:57:14.688183 139866171975424 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.4243900775909424, loss=3.0641984939575195
I0127 21:57:48.629989 139865274447616 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.534456729888916, loss=2.996084213256836
I0127 21:58:22.562357 139866171975424 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.4473893642425537, loss=2.9734601974487305
I0127 21:58:52.207338 140027215431488 spec.py:321] Evaluating on the training split.
I0127 21:58:58.519989 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 21:59:07.208603 140027215431488 spec.py:349] Evaluating on the test split.
I0127 21:59:09.717636 140027215431488 submission_runner.py:408] Time since start: 31718.94s, 	Step: 90189, 	{'train/accuracy': 0.7796755433082581, 'train/loss': 1.052192211151123, 'validation/accuracy': 0.6933599710464478, 'validation/loss': 1.4358712434768677, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 2.074946641921997, 'test/num_examples': 10000, 'score': 30639.66088938713, 'total_duration': 31718.936313152313, 'accumulated_submission_time': 30639.66088938713, 'accumulated_eval_time': 1073.8877835273743, 'accumulated_logging_time': 2.227370262145996}
I0127 21:59:09.751615 139865266054912 logging_writer.py:48] [90189] accumulated_eval_time=1073.887784, accumulated_logging_time=2.227370, accumulated_submission_time=30639.660889, global_step=90189, preemption_count=0, score=30639.660889, test/accuracy=0.567300, test/loss=2.074947, test/num_examples=10000, total_duration=31718.936313, train/accuracy=0.779676, train/loss=1.052192, validation/accuracy=0.693360, validation/loss=1.435871, validation/num_examples=50000
I0127 21:59:13.821481 139865274447616 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.467918872833252, loss=2.969817638397217
I0127 21:59:47.727496 139865266054912 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.327638626098633, loss=3.0116543769836426
I0127 22:00:21.617294 139865274447616 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.3383986949920654, loss=3.044903516769409
I0127 22:00:55.519190 139865266054912 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.3431949615478516, loss=2.975433349609375
I0127 22:01:29.449463 139865274447616 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.4786577224731445, loss=3.0431277751922607
I0127 22:02:03.435014 139865266054912 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.3731281757354736, loss=2.9164187908172607
I0127 22:02:37.325870 139865274447616 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.4877333641052246, loss=2.9552958011627197
I0127 22:03:11.246519 139865266054912 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.2386422157287598, loss=2.974015474319458
I0127 22:03:45.136630 139865274447616 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.4012110233306885, loss=2.9577853679656982
I0127 22:04:19.057132 139865266054912 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.415665864944458, loss=2.945678949356079
I0127 22:04:52.982923 139865274447616 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.336118221282959, loss=2.957728147506714
I0127 22:05:26.900596 139865266054912 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.396474838256836, loss=2.9708571434020996
I0127 22:06:00.807125 139865274447616 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.4211347103118896, loss=2.9084889888763428
I0127 22:06:34.749875 139865266054912 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.4534575939178467, loss=2.966062307357788
I0127 22:07:08.638262 139865274447616 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.5308494567871094, loss=3.0031466484069824
I0127 22:07:40.000514 140027215431488 spec.py:321] Evaluating on the training split.
I0127 22:07:46.220807 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 22:07:54.885320 140027215431488 spec.py:349] Evaluating on the test split.
I0127 22:07:57.382212 140027215431488 submission_runner.py:408] Time since start: 32246.60s, 	Step: 91694, 	{'train/accuracy': 0.8287826776504517, 'train/loss': 0.869025707244873, 'validation/accuracy': 0.7048199772834778, 'validation/loss': 1.3970078229904175, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 2.055137872695923, 'test/num_examples': 10000, 'score': 31149.845452308655, 'total_duration': 32246.600895643234, 'accumulated_submission_time': 31149.845452308655, 'accumulated_eval_time': 1091.269455909729, 'accumulated_logging_time': 2.272991180419922}
I0127 22:07:57.419973 139866163582720 logging_writer.py:48] [91694] accumulated_eval_time=1091.269456, accumulated_logging_time=2.272991, accumulated_submission_time=31149.845452, global_step=91694, preemption_count=0, score=31149.845452, test/accuracy=0.575600, test/loss=2.055138, test/num_examples=10000, total_duration=32246.600896, train/accuracy=0.828783, train/loss=0.869026, validation/accuracy=0.704820, validation/loss=1.397008, validation/num_examples=50000
I0127 22:07:59.805217 139866171975424 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.5038743019104004, loss=3.0061278343200684
I0127 22:08:33.734782 139866163582720 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.5364181995391846, loss=3.012735366821289
I0127 22:09:07.617135 139866171975424 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.6480801105499268, loss=2.9314181804656982
I0127 22:09:41.535659 139866163582720 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.310822010040283, loss=2.987046957015991
I0127 22:10:15.439078 139866171975424 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.3596558570861816, loss=2.964587926864624
I0127 22:10:49.361971 139866163582720 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.4073636531829834, loss=2.9591784477233887
I0127 22:11:23.262185 139866171975424 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.34427809715271, loss=2.9492344856262207
I0127 22:11:57.176883 139866163582720 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.511319875717163, loss=2.964238166809082
I0127 22:12:31.075591 139866171975424 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4201419353485107, loss=3.022368907928467
I0127 22:13:05.002041 139866163582720 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.48561429977417, loss=2.942723512649536
I0127 22:13:38.900879 139866171975424 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.6552064418792725, loss=2.9693636894226074
I0127 22:14:12.808006 139866163582720 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.5731725692749023, loss=3.0790796279907227
I0127 22:14:46.937217 139866171975424 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.489337205886841, loss=2.991330623626709
I0127 22:15:20.833908 139866163582720 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.4734435081481934, loss=2.9706575870513916
I0127 22:15:54.726872 139866171975424 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.477915048599243, loss=2.961787223815918
I0127 22:16:27.409102 140027215431488 spec.py:321] Evaluating on the training split.
I0127 22:16:33.621358 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 22:16:42.228947 140027215431488 spec.py:349] Evaluating on the test split.
I0127 22:16:44.827604 140027215431488 submission_runner.py:408] Time since start: 32774.05s, 	Step: 93198, 	{'train/accuracy': 0.7990872263908386, 'train/loss': 0.9855494499206543, 'validation/accuracy': 0.6990999579429626, 'validation/loss': 1.429892659187317, 'validation/num_examples': 50000, 'test/accuracy': 0.5735000371932983, 'test/loss': 2.089207172393799, 'test/num_examples': 10000, 'score': 31659.772423505783, 'total_duration': 32774.04626703262, 'accumulated_submission_time': 31659.772423505783, 'accumulated_eval_time': 1108.6878995895386, 'accumulated_logging_time': 2.321443557739258}
I0127 22:16:44.864877 139865257662208 logging_writer.py:48] [93198] accumulated_eval_time=1108.687900, accumulated_logging_time=2.321444, accumulated_submission_time=31659.772424, global_step=93198, preemption_count=0, score=31659.772424, test/accuracy=0.573500, test/loss=2.089207, test/num_examples=10000, total_duration=32774.046267, train/accuracy=0.799087, train/loss=0.985549, validation/accuracy=0.699100, validation/loss=1.429893, validation/num_examples=50000
I0127 22:16:45.894063 139865266054912 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.486215829849243, loss=3.0098674297332764
I0127 22:17:19.779222 139865257662208 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.5845258235931396, loss=2.9861831665039062
I0127 22:17:53.675967 139865266054912 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.5079758167266846, loss=2.943033218383789
I0127 22:18:27.600947 139865257662208 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.4863266944885254, loss=2.97383451461792
I0127 22:19:01.499343 139865266054912 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.374908924102783, loss=2.9483559131622314
I0127 22:19:35.451327 139865257662208 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.6054680347442627, loss=3.1124351024627686
I0127 22:20:09.408277 139865266054912 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.5171940326690674, loss=2.9851021766662598
I0127 22:20:43.303853 139865257662208 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.3984687328338623, loss=2.951889753341675
I0127 22:21:17.336198 139865266054912 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.5255823135375977, loss=3.0018420219421387
I0127 22:21:51.287696 139865257662208 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.4435808658599854, loss=2.935093402862549
I0127 22:22:25.242632 139865266054912 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.4169633388519287, loss=2.9873220920562744
I0127 22:22:59.149096 139865257662208 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.472212791442871, loss=3.1053073406219482
I0127 22:23:33.106046 139865266054912 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.383043050765991, loss=2.9270708560943604
I0127 22:24:07.013341 139865257662208 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.2828524112701416, loss=3.0410687923431396
I0127 22:24:40.912763 139865266054912 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.336608648300171, loss=2.929274559020996
I0127 22:25:14.857186 139865257662208 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.781651496887207, loss=3.0142407417297363
I0127 22:25:14.866485 140027215431488 spec.py:321] Evaluating on the training split.
I0127 22:25:21.148446 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 22:25:29.865532 140027215431488 spec.py:349] Evaluating on the test split.
I0127 22:25:32.388060 140027215431488 submission_runner.py:408] Time since start: 33301.61s, 	Step: 94701, 	{'train/accuracy': 0.7864317297935486, 'train/loss': 1.0102328062057495, 'validation/accuracy': 0.6943399906158447, 'validation/loss': 1.414360761642456, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 2.086948871612549, 'test/num_examples': 10000, 'score': 32169.710786104202, 'total_duration': 33301.60674357414, 'accumulated_submission_time': 32169.710786104202, 'accumulated_eval_time': 1126.209413766861, 'accumulated_logging_time': 2.367856502532959}
I0127 22:25:32.423780 139865257662208 logging_writer.py:48] [94701] accumulated_eval_time=1126.209414, accumulated_logging_time=2.367857, accumulated_submission_time=32169.710786, global_step=94701, preemption_count=0, score=32169.710786, test/accuracy=0.559700, test/loss=2.086949, test/num_examples=10000, total_duration=33301.606744, train/accuracy=0.786432, train/loss=1.010233, validation/accuracy=0.694340, validation/loss=1.414361, validation/num_examples=50000
I0127 22:26:06.292492 139866163582720 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.4826509952545166, loss=3.0132694244384766
I0127 22:26:40.177584 139865257662208 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.441716432571411, loss=3.037182092666626
I0127 22:27:14.169862 139866163582720 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.6298038959503174, loss=2.9839155673980713
I0127 22:27:48.094668 139865257662208 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.49758243560791, loss=2.8944599628448486
I0127 22:28:22.033653 139866163582720 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.5317602157592773, loss=2.929049491882324
I0127 22:28:55.932317 139865257662208 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.4044861793518066, loss=2.9406960010528564
I0127 22:29:29.875360 139866163582720 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.4087331295013428, loss=2.94795560836792
I0127 22:30:03.795475 139865257662208 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.4161794185638428, loss=2.9601998329162598
I0127 22:30:37.698044 139866163582720 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.448160409927368, loss=2.9026405811309814
I0127 22:31:11.613409 139865257662208 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.562016487121582, loss=2.966869831085205
I0127 22:31:45.522469 139866163582720 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.5465855598449707, loss=3.04400634765625
I0127 22:32:19.426915 139865257662208 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.5701630115509033, loss=2.933948040008545
I0127 22:32:53.361121 139866163582720 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.462740659713745, loss=2.949528694152832
I0127 22:33:27.352486 139865257662208 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.612335681915283, loss=2.9388182163238525
I0127 22:34:01.313493 139866163582720 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.8691277503967285, loss=2.944429636001587
I0127 22:34:02.483584 140027215431488 spec.py:321] Evaluating on the training split.
I0127 22:34:08.696399 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 22:34:17.556208 140027215431488 spec.py:349] Evaluating on the test split.
I0127 22:34:20.029068 140027215431488 submission_runner.py:408] Time since start: 33829.25s, 	Step: 96205, 	{'train/accuracy': 0.8004623651504517, 'train/loss': 1.0195797681808472, 'validation/accuracy': 0.7038599848747253, 'validation/loss': 1.428849697113037, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 2.0695486068725586, 'test/num_examples': 10000, 'score': 32679.708333969116, 'total_duration': 33829.24775338173, 'accumulated_submission_time': 32679.708333969116, 'accumulated_eval_time': 1143.754875421524, 'accumulated_logging_time': 2.413187265396118}
I0127 22:34:20.062887 139866171975424 logging_writer.py:48] [96205] accumulated_eval_time=1143.754875, accumulated_logging_time=2.413187, accumulated_submission_time=32679.708334, global_step=96205, preemption_count=0, score=32679.708334, test/accuracy=0.576800, test/loss=2.069549, test/num_examples=10000, total_duration=33829.247753, train/accuracy=0.800462, train/loss=1.019580, validation/accuracy=0.703860, validation/loss=1.428850, validation/num_examples=50000
I0127 22:34:52.606310 139866180368128 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.4528377056121826, loss=2.963275671005249
I0127 22:35:26.513527 139866171975424 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.5026798248291016, loss=2.9933884143829346
I0127 22:36:00.438031 139866180368128 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.407388925552368, loss=2.9805281162261963
I0127 22:36:34.357491 139866171975424 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.66072940826416, loss=2.9895243644714355
I0127 22:37:08.291693 139866180368128 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.4181125164031982, loss=2.9246904850006104
I0127 22:37:42.222218 139866171975424 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.415587902069092, loss=2.9781627655029297
I0127 22:38:16.146680 139866180368128 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.3395588397979736, loss=2.925215721130371
I0127 22:38:50.060388 139866171975424 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.5391111373901367, loss=2.902616500854492
I0127 22:39:23.967076 139866180368128 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3675537109375, loss=2.9855728149414062
I0127 22:39:58.046294 139866171975424 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.5257835388183594, loss=2.93803071975708
I0127 22:40:31.949580 139866180368128 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.470337152481079, loss=2.971850633621216
I0127 22:41:05.884614 139866171975424 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.6631386280059814, loss=2.95474910736084
I0127 22:41:39.799009 139866180368128 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.5359206199645996, loss=2.9251904487609863
I0127 22:42:13.695789 139866171975424 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.708472728729248, loss=2.998034715652466
I0127 22:42:47.560836 139866180368128 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.528644561767578, loss=2.9057259559631348
I0127 22:42:50.088063 140027215431488 spec.py:321] Evaluating on the training split.
I0127 22:42:56.226967 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 22:43:05.163880 140027215431488 spec.py:349] Evaluating on the test split.
I0127 22:43:07.647368 140027215431488 submission_runner.py:408] Time since start: 34356.87s, 	Step: 97709, 	{'train/accuracy': 0.7979910373687744, 'train/loss': 1.0076885223388672, 'validation/accuracy': 0.7076399922370911, 'validation/loss': 1.4024112224578857, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 2.061427354812622, 'test/num_examples': 10000, 'score': 33189.67002224922, 'total_duration': 34356.86605596542, 'accumulated_submission_time': 33189.67002224922, 'accumulated_eval_time': 1161.3141412734985, 'accumulated_logging_time': 2.4573397636413574}
I0127 22:43:07.681613 139865274447616 logging_writer.py:48] [97709] accumulated_eval_time=1161.314141, accumulated_logging_time=2.457340, accumulated_submission_time=33189.670022, global_step=97709, preemption_count=0, score=33189.670022, test/accuracy=0.575500, test/loss=2.061427, test/num_examples=10000, total_duration=34356.866056, train/accuracy=0.797991, train/loss=1.007689, validation/accuracy=0.707640, validation/loss=1.402411, validation/num_examples=50000
I0127 22:43:38.907232 139865760950016 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.651294708251953, loss=3.0081000328063965
I0127 22:44:12.786215 139865274447616 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.6104414463043213, loss=2.9235010147094727
I0127 22:44:46.702836 139865760950016 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.5080296993255615, loss=2.9310598373413086
I0127 22:45:20.594621 139865274447616 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.6617977619171143, loss=3.027900457382202
I0127 22:45:54.672090 139865760950016 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.478140115737915, loss=2.889070749282837
I0127 22:46:28.605699 139865274447616 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.507719039916992, loss=2.9263393878936768
I0127 22:47:02.540765 139865760950016 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.5176916122436523, loss=2.963749885559082
I0127 22:47:36.441758 139865274447616 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.541801929473877, loss=2.921510696411133
I0127 22:48:10.374927 139865760950016 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.512528657913208, loss=2.9818263053894043
I0127 22:48:44.278667 139865274447616 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.493746042251587, loss=2.950809955596924
I0127 22:49:18.200795 139865760950016 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.6432037353515625, loss=2.9850053787231445
I0127 22:49:52.112040 139865274447616 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.7080941200256348, loss=2.921802282333374
I0127 22:50:26.030723 139865760950016 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.5553407669067383, loss=2.998262405395508
I0127 22:50:59.951249 139865274447616 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.6809017658233643, loss=2.9423840045928955
I0127 22:51:33.832644 139865760950016 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.4777228832244873, loss=2.951059579849243
I0127 22:51:37.708173 140027215431488 spec.py:321] Evaluating on the training split.
I0127 22:51:43.951440 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 22:51:52.365881 140027215431488 spec.py:349] Evaluating on the test split.
I0127 22:51:54.847769 140027215431488 submission_runner.py:408] Time since start: 34884.07s, 	Step: 99213, 	{'train/accuracy': 0.7958186864852905, 'train/loss': 1.0074490308761597, 'validation/accuracy': 0.7049799561500549, 'validation/loss': 1.3988691568374634, 'validation/num_examples': 50000, 'test/accuracy': 0.5746999979019165, 'test/loss': 2.0553035736083984, 'test/num_examples': 10000, 'score': 33699.635172605515, 'total_duration': 34884.06643486023, 'accumulated_submission_time': 33699.635172605515, 'accumulated_eval_time': 1178.4537107944489, 'accumulated_logging_time': 2.5008058547973633}
I0127 22:51:54.883788 139866171975424 logging_writer.py:48] [99213] accumulated_eval_time=1178.453711, accumulated_logging_time=2.500806, accumulated_submission_time=33699.635173, global_step=99213, preemption_count=0, score=33699.635173, test/accuracy=0.574700, test/loss=2.055304, test/num_examples=10000, total_duration=34884.066435, train/accuracy=0.795819, train/loss=1.007449, validation/accuracy=0.704980, validation/loss=1.398869, validation/num_examples=50000
I0127 22:52:24.773198 139866180368128 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.628911256790161, loss=2.9352214336395264
I0127 22:52:58.677890 139866171975424 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.400399684906006, loss=2.9554591178894043
I0127 22:53:32.579811 139866180368128 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.420009136199951, loss=2.949558973312378
I0127 22:54:06.507380 139866171975424 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.651346206665039, loss=2.963381290435791
I0127 22:54:40.420849 139866180368128 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.575129747390747, loss=2.9438905715942383
I0127 22:55:14.358058 139866171975424 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.351346969604492, loss=2.8660831451416016
I0127 22:55:48.297188 139866180368128 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.4476306438446045, loss=2.952800989151001
I0127 22:56:22.167690 139866171975424 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.4349987506866455, loss=2.8359694480895996
I0127 22:56:56.104004 139866180368128 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.7186930179595947, loss=2.935853958129883
I0127 22:57:29.980558 139866171975424 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.6609930992126465, loss=2.9618544578552246
I0127 22:58:03.906997 139866180368128 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.4464504718780518, loss=2.936227321624756
I0127 22:58:37.930361 139866171975424 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.766763925552368, loss=2.9529736042022705
I0127 22:59:11.869946 139866180368128 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.6632373332977295, loss=2.8862147331237793
I0127 22:59:45.762206 139866171975424 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.3864123821258545, loss=2.867455244064331
I0127 23:00:19.706459 139866180368128 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.487502098083496, loss=2.9030871391296387
I0127 23:00:24.924175 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:00:31.194813 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:00:40.122206 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:00:42.639808 140027215431488 submission_runner.py:408] Time since start: 35411.86s, 	Step: 100717, 	{'train/accuracy': 0.8254942297935486, 'train/loss': 0.8944934010505676, 'validation/accuracy': 0.7102400064468384, 'validation/loss': 1.3767321109771729, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 2.0282983779907227, 'test/num_examples': 10000, 'score': 34209.61276984215, 'total_duration': 35411.85849046707, 'accumulated_submission_time': 34209.61276984215, 'accumulated_eval_time': 1196.1693103313446, 'accumulated_logging_time': 2.546966552734375}
I0127 23:00:42.674966 139865274447616 logging_writer.py:48] [100717] accumulated_eval_time=1196.169310, accumulated_logging_time=2.546967, accumulated_submission_time=34209.612770, global_step=100717, preemption_count=0, score=34209.612770, test/accuracy=0.579800, test/loss=2.028298, test/num_examples=10000, total_duration=35411.858490, train/accuracy=0.825494, train/loss=0.894493, validation/accuracy=0.710240, validation/loss=1.376732, validation/num_examples=50000
I0127 23:01:11.163365 139865760950016 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.5752134323120117, loss=2.8946714401245117
I0127 23:01:45.050241 139865274447616 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.5181641578674316, loss=2.959599256515503
I0127 23:02:18.963106 139865760950016 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.556769847869873, loss=2.9168686866760254
I0127 23:02:52.830389 139865274447616 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.7103755474090576, loss=2.952709913253784
I0127 23:03:26.745705 139865760950016 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.6321730613708496, loss=2.87896728515625
I0127 23:04:00.636394 139865274447616 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.6864192485809326, loss=2.935413360595703
I0127 23:04:34.561005 139865760950016 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.783993721008301, loss=2.8921701908111572
I0127 23:05:08.531814 139865274447616 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.5839712619781494, loss=2.952800989151001
I0127 23:05:42.435835 139865760950016 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.4890739917755127, loss=2.9162421226501465
I0127 23:06:16.367434 139865274447616 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.712926149368286, loss=3.002041816711426
I0127 23:06:50.298727 139865760950016 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.524190664291382, loss=2.8492071628570557
I0127 23:07:24.219551 139865274447616 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.7833573818206787, loss=2.9434473514556885
I0127 23:07:58.126282 139865760950016 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.7331314086914062, loss=2.925874710083008
I0127 23:08:32.020704 139865274447616 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.574418783187866, loss=2.942379951477051
I0127 23:09:05.939927 139865760950016 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.7000584602355957, loss=2.941178560256958
I0127 23:09:12.880596 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:09:19.194369 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:09:27.668763 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:09:30.198112 140027215431488 submission_runner.py:408] Time since start: 35939.42s, 	Step: 102222, 	{'train/accuracy': 0.8199936151504517, 'train/loss': 0.9147710800170898, 'validation/accuracy': 0.7091999650001526, 'validation/loss': 1.3878830671310425, 'validation/num_examples': 50000, 'test/accuracy': 0.5827000141143799, 'test/loss': 2.037583112716675, 'test/num_examples': 10000, 'score': 34719.75500845909, 'total_duration': 35939.416763305664, 'accumulated_submission_time': 34719.75500845909, 'accumulated_eval_time': 1213.4867506027222, 'accumulated_logging_time': 2.5913703441619873}
I0127 23:09:30.249178 139866171975424 logging_writer.py:48] [102222] accumulated_eval_time=1213.486751, accumulated_logging_time=2.591370, accumulated_submission_time=34719.755008, global_step=102222, preemption_count=0, score=34719.755008, test/accuracy=0.582700, test/loss=2.037583, test/num_examples=10000, total_duration=35939.416763, train/accuracy=0.819994, train/loss=0.914771, validation/accuracy=0.709200, validation/loss=1.387883, validation/num_examples=50000
I0127 23:09:57.024739 139866180368128 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.495924711227417, loss=2.8971426486968994
I0127 23:10:30.901746 139866171975424 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.501692533493042, loss=2.8770852088928223
I0127 23:11:04.897916 139866180368128 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.4630515575408936, loss=2.874396800994873
I0127 23:11:38.821394 139866171975424 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.578768253326416, loss=2.8911943435668945
I0127 23:12:12.757448 139866180368128 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.6358346939086914, loss=2.9015560150146484
I0127 23:12:46.661625 139866171975424 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.7420425415039062, loss=2.9142699241638184
I0127 23:13:20.588608 139866180368128 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.526965379714966, loss=2.8794362545013428
I0127 23:13:54.521958 139866171975424 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.744716167449951, loss=2.9185781478881836
I0127 23:14:28.419718 139866180368128 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.4499282836914062, loss=2.868849039077759
I0127 23:15:02.365080 139866171975424 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.576246738433838, loss=2.8918707370758057
I0127 23:15:36.286545 139866180368128 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.954725503921509, loss=2.9442138671875
I0127 23:16:10.226053 139866171975424 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.6550159454345703, loss=2.945559024810791
I0127 23:16:44.134427 139866180368128 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.595921277999878, loss=2.916837215423584
I0127 23:17:18.112836 139866171975424 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.600689649581909, loss=2.9483797550201416
I0127 23:17:52.034760 139866180368128 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.589326858520508, loss=2.8538804054260254
I0127 23:18:00.364728 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:18:06.698458 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:18:15.381163 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:18:17.793814 140027215431488 submission_runner.py:408] Time since start: 36467.01s, 	Step: 103726, 	{'train/accuracy': 0.8092314600944519, 'train/loss': 0.9412323236465454, 'validation/accuracy': 0.7082799673080444, 'validation/loss': 1.3787775039672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 2.0294604301452637, 'test/num_examples': 10000, 'score': 35229.8086771965, 'total_duration': 36467.01249408722, 'accumulated_submission_time': 35229.8086771965, 'accumulated_eval_time': 1230.9158039093018, 'accumulated_logging_time': 2.652010679244995}
I0127 23:18:17.829823 139865760950016 logging_writer.py:48] [103726] accumulated_eval_time=1230.915804, accumulated_logging_time=2.652011, accumulated_submission_time=35229.808677, global_step=103726, preemption_count=0, score=35229.808677, test/accuracy=0.580900, test/loss=2.029460, test/num_examples=10000, total_duration=36467.012494, train/accuracy=0.809231, train/loss=0.941232, validation/accuracy=0.708280, validation/loss=1.378778, validation/num_examples=50000
I0127 23:18:43.260587 139865769342720 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.4444239139556885, loss=2.866241455078125
I0127 23:19:17.153021 139865760950016 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.594766616821289, loss=2.880220413208008
I0127 23:19:51.097364 139865769342720 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.6694650650024414, loss=2.8666343688964844
I0127 23:20:24.990567 139865760950016 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.5961813926696777, loss=2.9007012844085693
I0127 23:20:58.895224 139865769342720 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.809950590133667, loss=2.9343018531799316
I0127 23:21:32.815314 139865760950016 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.523085594177246, loss=2.928194046020508
I0127 23:22:06.698747 139865769342720 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.7652931213378906, loss=2.9361445903778076
I0127 23:22:40.606570 139865760950016 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.7788634300231934, loss=2.9133541584014893
I0127 23:23:14.512627 139865769342720 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.706374406814575, loss=2.9490363597869873
I0127 23:23:48.491266 139865760950016 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.5582141876220703, loss=2.904078483581543
I0127 23:24:22.448040 139865769342720 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.5944859981536865, loss=2.9137680530548096
I0127 23:24:56.335650 139865760950016 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.7303383350372314, loss=2.951202392578125
I0127 23:25:30.260158 139865769342720 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.5735220909118652, loss=2.8366496562957764
I0127 23:26:04.148468 139865760950016 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.5442190170288086, loss=2.8544130325317383
I0127 23:26:38.070585 139865769342720 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.791435718536377, loss=2.9792511463165283
I0127 23:26:48.034599 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:26:54.299839 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:27:03.034441 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:27:05.470460 140027215431488 submission_runner.py:408] Time since start: 36994.69s, 	Step: 105231, 	{'train/accuracy': 0.810546875, 'train/loss': 0.9459798336029053, 'validation/accuracy': 0.7136399745941162, 'validation/loss': 1.372373104095459, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 2.013390302658081, 'test/num_examples': 10000, 'score': 35739.95236110687, 'total_duration': 36994.68914103508, 'accumulated_submission_time': 35739.95236110687, 'accumulated_eval_time': 1248.3516201972961, 'accumulated_logging_time': 2.697594404220581}
I0127 23:27:05.508738 139866180368128 logging_writer.py:48] [105231] accumulated_eval_time=1248.351620, accumulated_logging_time=2.697594, accumulated_submission_time=35739.952361, global_step=105231, preemption_count=0, score=35739.952361, test/accuracy=0.587900, test/loss=2.013390, test/num_examples=10000, total_duration=36994.689141, train/accuracy=0.810547, train/loss=0.945980, validation/accuracy=0.713640, validation/loss=1.372373, validation/num_examples=50000
I0127 23:27:29.231667 139866188760832 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.476966619491577, loss=2.886166572570801
I0127 23:28:03.142548 139866180368128 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.6795034408569336, loss=2.919829845428467
I0127 23:28:37.047043 139866188760832 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.6598992347717285, loss=2.9251837730407715
I0127 23:29:10.968478 139866180368128 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.8165934085845947, loss=2.9161200523376465
I0127 23:29:44.874784 139866188760832 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.590101957321167, loss=2.92158842086792
I0127 23:30:18.873103 139866180368128 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.5987508296966553, loss=2.9030165672302246
I0127 23:30:52.821494 139866188760832 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.4557018280029297, loss=2.8549063205718994
I0127 23:31:26.745842 139866180368128 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.659330368041992, loss=2.9358677864074707
I0127 23:32:00.661393 139866188760832 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.543328046798706, loss=2.887479782104492
I0127 23:32:34.584339 139866180368128 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.7993435859680176, loss=2.8530330657958984
I0127 23:33:08.468514 139866188760832 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.6592843532562256, loss=2.85827374458313
I0127 23:33:42.387942 139866180368128 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.829340696334839, loss=2.959235668182373
I0127 23:34:16.292743 139866188760832 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.606586217880249, loss=2.9480583667755127
I0127 23:34:50.208526 139866180368128 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.8193790912628174, loss=2.9203083515167236
I0127 23:35:24.098047 139866188760832 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.740750312805176, loss=2.982020616531372
I0127 23:35:35.782367 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:35:42.019863 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:35:50.586095 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:35:53.128072 140027215431488 submission_runner.py:408] Time since start: 37522.35s, 	Step: 106736, 	{'train/accuracy': 0.8152502775192261, 'train/loss': 0.9214245080947876, 'validation/accuracy': 0.7125799655914307, 'validation/loss': 1.3635106086730957, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.9973350763320923, 'test/num_examples': 10000, 'score': 36250.164659023285, 'total_duration': 37522.34675478935, 'accumulated_submission_time': 36250.164659023285, 'accumulated_eval_time': 1265.6972844600677, 'accumulated_logging_time': 2.745483875274658}
I0127 23:35:53.167960 139865274447616 logging_writer.py:48] [106736] accumulated_eval_time=1265.697284, accumulated_logging_time=2.745484, accumulated_submission_time=36250.164659, global_step=106736, preemption_count=0, score=36250.164659, test/accuracy=0.592400, test/loss=1.997335, test/num_examples=10000, total_duration=37522.346755, train/accuracy=0.815250, train/loss=0.921425, validation/accuracy=0.712580, validation/loss=1.363511, validation/num_examples=50000
I0127 23:36:15.309027 139865760950016 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.664449453353882, loss=2.866849422454834
I0127 23:36:49.193754 139865274447616 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.668652296066284, loss=2.9022271633148193
I0127 23:37:23.115102 139865760950016 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.6957409381866455, loss=2.889770984649658
I0127 23:37:57.033106 139865274447616 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.7974870204925537, loss=2.895890951156616
I0127 23:38:30.972245 139865760950016 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.6258246898651123, loss=2.880214214324951
I0127 23:39:04.861313 139865274447616 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.6036429405212402, loss=2.829063892364502
I0127 23:39:38.781236 139865760950016 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.717313051223755, loss=2.9118459224700928
I0127 23:40:12.670670 139865274447616 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.707223653793335, loss=2.825625419616699
I0127 23:40:46.601670 139865760950016 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.665067434310913, loss=2.8648524284362793
I0127 23:41:20.537502 139865274447616 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.820753574371338, loss=2.9575564861297607
I0127 23:41:54.418732 139865760950016 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.7033588886260986, loss=2.915038585662842
I0127 23:42:28.401471 139865274447616 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.8115592002868652, loss=2.9270949363708496
I0127 23:43:02.349968 139865760950016 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.767817497253418, loss=2.935164451599121
I0127 23:43:36.246007 139865274447616 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.6782078742980957, loss=2.882167339324951
I0127 23:44:10.168930 139865760950016 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.738039255142212, loss=2.900876998901367
I0127 23:44:23.179558 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:44:29.468189 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:44:38.187318 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:44:40.595527 140027215431488 submission_runner.py:408] Time since start: 38049.81s, 	Step: 108240, 	{'train/accuracy': 0.8124202489852905, 'train/loss': 0.9675102829933167, 'validation/accuracy': 0.7174199819564819, 'validation/loss': 1.3825063705444336, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 2.006366014480591, 'test/num_examples': 10000, 'score': 36760.113042354584, 'total_duration': 38049.814202070236, 'accumulated_submission_time': 36760.113042354584, 'accumulated_eval_time': 1283.1132173538208, 'accumulated_logging_time': 2.795067071914673}
I0127 23:44:40.630884 139865274447616 logging_writer.py:48] [108240] accumulated_eval_time=1283.113217, accumulated_logging_time=2.795067, accumulated_submission_time=36760.113042, global_step=108240, preemption_count=0, score=36760.113042, test/accuracy=0.596000, test/loss=2.006366, test/num_examples=10000, total_duration=38049.814202, train/accuracy=0.812420, train/loss=0.967510, validation/accuracy=0.717420, validation/loss=1.382506, validation/num_examples=50000
I0127 23:45:01.321188 139866171975424 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.81925368309021, loss=2.875758647918701
I0127 23:45:35.233585 139865274447616 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.7069830894470215, loss=2.924548864364624
I0127 23:46:09.127511 139866171975424 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.5778682231903076, loss=2.8839638233184814
I0127 23:46:43.042106 139865274447616 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.697093963623047, loss=2.9174232482910156
I0127 23:47:16.942322 139866171975424 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.663062334060669, loss=2.8959195613861084
I0127 23:47:50.881930 139865274447616 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.730027198791504, loss=2.8914244174957275
I0127 23:48:24.774057 139866171975424 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.6634938716888428, loss=2.862558126449585
I0127 23:48:58.862380 139865274447616 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.7936480045318604, loss=2.909801959991455
I0127 23:49:32.766357 139866171975424 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.846778392791748, loss=2.9470324516296387
I0127 23:50:06.657218 139865274447616 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.796637773513794, loss=2.8744680881500244
I0127 23:50:40.592716 139866171975424 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.748445510864258, loss=2.9219963550567627
I0127 23:51:14.458631 139865274447616 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.7176413536071777, loss=2.8664793968200684
I0127 23:51:48.383160 139866171975424 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.54407000541687, loss=2.8862414360046387
I0127 23:52:22.286193 139865274447616 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.8182930946350098, loss=2.95137619972229
I0127 23:52:56.183791 139866171975424 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.8095130920410156, loss=2.9132542610168457
I0127 23:53:10.886976 140027215431488 spec.py:321] Evaluating on the training split.
I0127 23:53:17.149562 140027215431488 spec.py:333] Evaluating on the validation split.
I0127 23:53:25.982902 140027215431488 spec.py:349] Evaluating on the test split.
I0127 23:53:28.470580 140027215431488 submission_runner.py:408] Time since start: 38577.69s, 	Step: 109745, 	{'train/accuracy': 0.8219866156578064, 'train/loss': 0.869674026966095, 'validation/accuracy': 0.7159799933433533, 'validation/loss': 1.3264567852020264, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.9520862102508545, 'test/num_examples': 10000, 'score': 37270.30475926399, 'total_duration': 38577.689265728, 'accumulated_submission_time': 37270.30475926399, 'accumulated_eval_time': 1300.696792602539, 'accumulated_logging_time': 2.8408803939819336}
I0127 23:53:28.507217 139865760950016 logging_writer.py:48] [109745] accumulated_eval_time=1300.696793, accumulated_logging_time=2.840880, accumulated_submission_time=37270.304759, global_step=109745, preemption_count=0, score=37270.304759, test/accuracy=0.590600, test/loss=1.952086, test/num_examples=10000, total_duration=38577.689266, train/accuracy=0.821987, train/loss=0.869674, validation/accuracy=0.715980, validation/loss=1.326457, validation/num_examples=50000
I0127 23:53:47.500371 139865769342720 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.823559522628784, loss=2.8993239402770996
I0127 23:54:21.385655 139865760950016 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.865614175796509, loss=2.888815402984619
I0127 23:54:55.476515 139865769342720 logging_writer.py:48] [110000] global_step=110000, grad_norm=3.1827802658081055, loss=2.902303457260132
I0127 23:55:29.389917 139865760950016 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.8817014694213867, loss=2.888205051422119
I0127 23:56:03.317182 139865769342720 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.7789769172668457, loss=2.886418342590332
I0127 23:56:37.233074 139865760950016 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.699054479598999, loss=2.7893104553222656
I0127 23:57:11.157660 139865769342720 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.7860958576202393, loss=2.90094256401062
I0127 23:57:45.083516 139865760950016 logging_writer.py:48] [110500] global_step=110500, grad_norm=3.0477747917175293, loss=2.891267776489258
I0127 23:58:18.993638 139865769342720 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.7854979038238525, loss=2.8396780490875244
I0127 23:58:52.955792 139865760950016 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.7326464653015137, loss=2.9559433460235596
I0127 23:59:26.881773 139865769342720 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.7710301876068115, loss=2.8776228427886963
I0128 00:00:00.807803 139865760950016 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.8745245933532715, loss=2.93613600730896
I0128 00:00:34.735082 139865769342720 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.6883316040039062, loss=2.8297135829925537
I0128 00:01:08.769122 139865760950016 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.5656819343566895, loss=2.8112378120422363
I0128 00:01:42.707665 139865769342720 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.805231809616089, loss=2.888550281524658
I0128 00:01:58.796960 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:02:05.125385 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:02:14.028903 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:02:16.446579 140027215431488 submission_runner.py:408] Time since start: 39105.67s, 	Step: 111249, 	{'train/accuracy': 0.8328284025192261, 'train/loss': 0.8718942999839783, 'validation/accuracy': 0.7133600115776062, 'validation/loss': 1.374558448791504, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 2.0042407512664795, 'test/num_examples': 10000, 'score': 37780.53218817711, 'total_duration': 39105.6652610302, 'accumulated_submission_time': 37780.53218817711, 'accumulated_eval_time': 1318.3463683128357, 'accumulated_logging_time': 2.886791706085205}
I0128 00:02:16.483620 139865257662208 logging_writer.py:48] [111249] accumulated_eval_time=1318.346368, accumulated_logging_time=2.886792, accumulated_submission_time=37780.532188, global_step=111249, preemption_count=0, score=37780.532188, test/accuracy=0.589900, test/loss=2.004241, test/num_examples=10000, total_duration=39105.665261, train/accuracy=0.832828, train/loss=0.871894, validation/accuracy=0.713360, validation/loss=1.374558, validation/num_examples=50000
I0128 00:02:34.104811 139866163582720 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.8416900634765625, loss=2.832657814025879
I0128 00:03:08.002410 139865257662208 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.834651470184326, loss=2.8858046531677246
I0128 00:03:41.884034 139866163582720 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.954392671585083, loss=2.870839834213257
I0128 00:04:15.823959 139865257662208 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.7624733448028564, loss=2.8652477264404297
I0128 00:04:49.735430 139866163582720 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.835926055908203, loss=2.8350164890289307
I0128 00:05:23.665089 139865257662208 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.843764305114746, loss=2.947040319442749
I0128 00:05:57.565987 139866163582720 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.85892653465271, loss=2.8640027046203613
I0128 00:06:31.499164 139865257662208 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.9405667781829834, loss=2.8850085735321045
I0128 00:07:05.399901 139866163582720 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.7802670001983643, loss=2.8465490341186523
I0128 00:07:39.390584 139865257662208 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.792275905609131, loss=2.911848545074463
I0128 00:08:13.313066 139866163582720 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.740811347961426, loss=2.882783889770508
I0128 00:08:47.256846 139865257662208 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.846240520477295, loss=2.850126028060913
I0128 00:09:21.191003 139866163582720 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.589287042617798, loss=2.8279597759246826
I0128 00:09:55.096061 139865257662208 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.889970302581787, loss=2.888134002685547
I0128 00:10:28.986852 139866163582720 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.8382668495178223, loss=2.878396987915039
I0128 00:10:46.773322 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:10:53.050825 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:11:01.909892 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:11:04.400189 140027215431488 submission_runner.py:408] Time since start: 39633.62s, 	Step: 112754, 	{'train/accuracy': 0.829121470451355, 'train/loss': 0.8686312437057495, 'validation/accuracy': 0.7148799896240234, 'validation/loss': 1.34848952293396, 'validation/num_examples': 50000, 'test/accuracy': 0.5905000567436218, 'test/loss': 2.003549337387085, 'test/num_examples': 10000, 'score': 38290.755179166794, 'total_duration': 39633.61887669563, 'accumulated_submission_time': 38290.755179166794, 'accumulated_eval_time': 1335.973201751709, 'accumulated_logging_time': 2.9371728897094727}
I0128 00:11:04.442335 139865266054912 logging_writer.py:48] [112754] accumulated_eval_time=1335.973202, accumulated_logging_time=2.937173, accumulated_submission_time=38290.755179, global_step=112754, preemption_count=0, score=38290.755179, test/accuracy=0.590500, test/loss=2.003549, test/num_examples=10000, total_duration=39633.618877, train/accuracy=0.829121, train/loss=0.868631, validation/accuracy=0.714880, validation/loss=1.348490, validation/num_examples=50000
I0128 00:11:20.372769 139865769342720 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.762035369873047, loss=2.8390276432037354
I0128 00:11:54.276520 139865266054912 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.8528189659118652, loss=2.892549514770508
I0128 00:12:28.163265 139865769342720 logging_writer.py:48] [113000] global_step=113000, grad_norm=3.0328099727630615, loss=2.9158644676208496
I0128 00:13:02.068214 139865266054912 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.8306806087493896, loss=2.8148486614227295
I0128 00:13:36.026262 139865769342720 logging_writer.py:48] [113200] global_step=113200, grad_norm=3.040367841720581, loss=2.8597464561462402
I0128 00:14:09.938588 139865266054912 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.7875072956085205, loss=2.8919878005981445
I0128 00:14:43.859886 139865769342720 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.704164981842041, loss=2.791571855545044
I0128 00:15:17.750377 139865266054912 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.830821990966797, loss=2.8626163005828857
I0128 00:15:51.683531 139865769342720 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8002519607543945, loss=2.860018253326416
I0128 00:16:25.592795 139865266054912 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.826582193374634, loss=2.8171486854553223
I0128 00:16:59.507503 139865769342720 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.9343342781066895, loss=2.8599395751953125
I0128 00:17:33.425129 139865266054912 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.815669059753418, loss=2.834345579147339
I0128 00:18:07.333120 139865769342720 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.7344019412994385, loss=2.825277328491211
I0128 00:18:41.264623 139865266054912 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.9281764030456543, loss=2.869936227798462
I0128 00:19:15.146098 139865769342720 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.7944514751434326, loss=2.9245433807373047
I0128 00:19:34.621674 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:19:40.837531 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:19:49.530867 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:19:52.431694 140027215431488 submission_runner.py:408] Time since start: 40161.65s, 	Step: 114259, 	{'train/accuracy': 0.8265106678009033, 'train/loss': 0.8988329768180847, 'validation/accuracy': 0.7185800075531006, 'validation/loss': 1.36379075050354, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.997839331626892, 'test/num_examples': 10000, 'score': 38800.871701955795, 'total_duration': 40161.65034651756, 'accumulated_submission_time': 38800.871701955795, 'accumulated_eval_time': 1353.7831537723541, 'accumulated_logging_time': 2.9890265464782715}
I0128 00:19:52.471165 139865760950016 logging_writer.py:48] [114259] accumulated_eval_time=1353.783154, accumulated_logging_time=2.989027, accumulated_submission_time=38800.871702, global_step=114259, preemption_count=0, score=38800.871702, test/accuracy=0.597100, test/loss=1.997839, test/num_examples=10000, total_duration=40161.650347, train/accuracy=0.826511, train/loss=0.898833, validation/accuracy=0.718580, validation/loss=1.363791, validation/num_examples=50000
I0128 00:20:06.710255 139866163582720 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.720172166824341, loss=2.8997116088867188
I0128 00:20:40.613798 139865760950016 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.899007797241211, loss=2.832153558731079
I0128 00:21:14.481019 139866163582720 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.7840816974639893, loss=2.804065227508545
I0128 00:21:48.397624 139865760950016 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.901350975036621, loss=2.917520523071289
I0128 00:22:22.274604 139866163582720 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.738360643386841, loss=2.841829299926758
I0128 00:22:56.206693 139865760950016 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.5870344638824463, loss=2.8095593452453613
I0128 00:23:30.094460 139866163582720 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.7271740436553955, loss=2.7975432872772217
I0128 00:24:04.008370 139865760950016 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.9835190773010254, loss=2.857006788253784
I0128 00:24:37.902076 139866163582720 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.8254072666168213, loss=2.8393805027008057
I0128 00:25:11.828719 139865760950016 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.8758628368377686, loss=2.860555648803711
I0128 00:25:45.700048 139866163582720 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.7029154300689697, loss=2.7546164989471436
I0128 00:26:19.692728 139865760950016 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.086787462234497, loss=2.9090723991394043
I0128 00:26:53.553576 139866163582720 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.8969762325286865, loss=2.8403713703155518
I0128 00:27:27.469155 139865760950016 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.727444648742676, loss=2.840108633041382
I0128 00:28:01.347192 139866163582720 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.9698705673217773, loss=2.924065113067627
I0128 00:28:22.497681 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:28:28.740168 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:28:37.343502 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:28:39.811043 140027215431488 submission_runner.py:408] Time since start: 40689.03s, 	Step: 115764, 	{'train/accuracy': 0.8270089030265808, 'train/loss': 0.9045054316520691, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.3632978200912476, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 2.0048916339874268, 'test/num_examples': 10000, 'score': 39310.83686709404, 'total_duration': 40689.02972865105, 'accumulated_submission_time': 39310.83686709404, 'accumulated_eval_time': 1371.0964777469635, 'accumulated_logging_time': 3.0382204055786133}
I0128 00:28:39.850216 139865760950016 logging_writer.py:48] [115764] accumulated_eval_time=1371.096478, accumulated_logging_time=3.038220, accumulated_submission_time=39310.836867, global_step=115764, preemption_count=0, score=39310.836867, test/accuracy=0.593900, test/loss=2.004892, test/num_examples=10000, total_duration=40689.029729, train/accuracy=0.827009, train/loss=0.904505, validation/accuracy=0.722820, validation/loss=1.363298, validation/num_examples=50000
I0128 00:28:52.391286 139865769342720 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.8822591304779053, loss=2.815171241760254
I0128 00:29:26.246265 139865760950016 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.916919469833374, loss=2.904608726501465
I0128 00:30:00.160628 139865769342720 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.8105101585388184, loss=2.8105149269104004
I0128 00:30:34.039859 139865760950016 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.9014861583709717, loss=2.921152114868164
I0128 00:31:07.939701 139865769342720 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.8175930976867676, loss=2.875476837158203
I0128 00:31:41.872446 139865760950016 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.9115958213806152, loss=2.8180458545684814
I0128 00:32:15.779094 139865769342720 logging_writer.py:48] [116400] global_step=116400, grad_norm=3.0520546436309814, loss=2.8500709533691406
I0128 00:32:49.751423 139865760950016 logging_writer.py:48] [116500] global_step=116500, grad_norm=3.03889536857605, loss=2.796820878982544
I0128 00:33:23.674779 139865769342720 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.790304660797119, loss=2.8419127464294434
I0128 00:33:57.573267 139865760950016 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.9419007301330566, loss=2.891474723815918
I0128 00:34:31.499378 139865769342720 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.773461103439331, loss=2.819732189178467
I0128 00:35:05.393418 139865760950016 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.8201279640197754, loss=2.8609910011291504
I0128 00:35:39.309488 139865769342720 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.721160411834717, loss=2.7532389163970947
I0128 00:36:13.196332 139865760950016 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.9917988777160645, loss=2.8325724601745605
I0128 00:36:47.111146 139865769342720 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.739161491394043, loss=2.816518545150757
I0128 00:37:09.967449 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:37:16.255613 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:37:25.127472 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:37:27.510000 140027215431488 submission_runner.py:408] Time since start: 41216.73s, 	Step: 117269, 	{'train/accuracy': 0.8316724896430969, 'train/loss': 0.8688330054283142, 'validation/accuracy': 0.7247799634933472, 'validation/loss': 1.3182138204574585, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.9583295583724976, 'test/num_examples': 10000, 'score': 39820.8876388073, 'total_duration': 41216.72868323326, 'accumulated_submission_time': 39820.8876388073, 'accumulated_eval_time': 1388.6389904022217, 'accumulated_logging_time': 3.0894229412078857}
I0128 00:37:27.552718 139865760950016 logging_writer.py:48] [117269] accumulated_eval_time=1388.638990, accumulated_logging_time=3.089423, accumulated_submission_time=39820.887639, global_step=117269, preemption_count=0, score=39820.887639, test/accuracy=0.601400, test/loss=1.958330, test/num_examples=10000, total_duration=41216.728683, train/accuracy=0.831672, train/loss=0.868833, validation/accuracy=0.724780, validation/loss=1.318214, validation/num_examples=50000
I0128 00:37:38.414070 139865769342720 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.0398712158203125, loss=2.872741937637329
I0128 00:38:12.325719 139865760950016 logging_writer.py:48] [117400] global_step=117400, grad_norm=3.036428928375244, loss=2.8467626571655273
I0128 00:38:46.257268 139865769342720 logging_writer.py:48] [117500] global_step=117500, grad_norm=3.010652542114258, loss=2.8427774906158447
I0128 00:39:20.170677 139865760950016 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.8328795433044434, loss=2.8572402000427246
I0128 00:39:54.065719 139865769342720 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.1707916259765625, loss=2.823314666748047
I0128 00:40:27.997333 139865760950016 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.9268686771392822, loss=2.944371223449707
I0128 00:41:01.894946 139865769342720 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.9215545654296875, loss=2.877589225769043
I0128 00:41:35.817877 139865760950016 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.040956497192383, loss=2.884446144104004
I0128 00:42:09.725659 139865769342720 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.9034337997436523, loss=2.883157730102539
I0128 00:42:43.616204 139865760950016 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8759491443634033, loss=2.872248649597168
I0128 00:43:17.504274 139865769342720 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.009856939315796, loss=2.863636016845703
I0128 00:43:51.426655 139865760950016 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.9096643924713135, loss=2.7906877994537354
I0128 00:44:25.342078 139865769342720 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.8503520488739014, loss=2.853320598602295
I0128 00:44:59.282608 139865760950016 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.1624057292938232, loss=2.8185274600982666
I0128 00:45:33.220337 139865769342720 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9220123291015625, loss=2.8418076038360596
I0128 00:45:57.780794 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:46:04.756726 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:46:13.317623 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:46:15.742841 140027215431488 submission_runner.py:408] Time since start: 41744.96s, 	Step: 118774, 	{'train/accuracy': 0.8371332883834839, 'train/loss': 0.8562281131744385, 'validation/accuracy': 0.725659966468811, 'validation/loss': 1.3277863264083862, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.9448498487472534, 'test/num_examples': 10000, 'score': 40331.050414800644, 'total_duration': 41744.96152448654, 'accumulated_submission_time': 40331.050414800644, 'accumulated_eval_time': 1406.6010098457336, 'accumulated_logging_time': 3.143878936767578}
I0128 00:46:15.782272 139865257662208 logging_writer.py:48] [118774] accumulated_eval_time=1406.601010, accumulated_logging_time=3.143879, accumulated_submission_time=40331.050415, global_step=118774, preemption_count=0, score=40331.050415, test/accuracy=0.607900, test/loss=1.944850, test/num_examples=10000, total_duration=41744.961524, train/accuracy=0.837133, train/loss=0.856228, validation/accuracy=0.725660, validation/loss=1.327786, validation/num_examples=50000
I0128 00:46:24.924970 139865266054912 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.0198752880096436, loss=2.7980566024780273
I0128 00:46:58.837830 139865257662208 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.924729585647583, loss=2.832319736480713
I0128 00:47:32.712995 139865266054912 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.190190076828003, loss=2.8383405208587646
I0128 00:48:06.639251 139865257662208 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.787051200866699, loss=2.853743553161621
I0128 00:48:40.571231 139865266054912 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.9127964973449707, loss=2.783217668533325
I0128 00:49:14.475702 139865257662208 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.8037328720092773, loss=2.8452706336975098
I0128 00:49:48.373674 139865266054912 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.861034870147705, loss=2.839338541030884
I0128 00:50:22.265972 139865257662208 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.975346803665161, loss=2.866363763809204
I0128 00:50:56.179674 139865266054912 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.0380361080169678, loss=2.8491828441619873
I0128 00:51:30.258316 139865257662208 logging_writer.py:48] [119700] global_step=119700, grad_norm=3.073349952697754, loss=2.821688413619995
I0128 00:52:04.175126 139865266054912 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.894244432449341, loss=2.9062161445617676
I0128 00:52:38.080078 139865257662208 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.989283800125122, loss=2.8197925090789795
I0128 00:53:11.984409 139865266054912 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.8281986713409424, loss=2.807155132293701
I0128 00:53:45.880916 139865257662208 logging_writer.py:48] [120100] global_step=120100, grad_norm=3.197902202606201, loss=2.8064732551574707
I0128 00:54:19.801409 139865266054912 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9429166316986084, loss=2.852674722671509
I0128 00:54:46.053837 140027215431488 spec.py:321] Evaluating on the training split.
I0128 00:54:52.298458 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 00:55:00.964233 140027215431488 spec.py:349] Evaluating on the test split.
I0128 00:55:03.539582 140027215431488 submission_runner.py:408] Time since start: 42272.76s, 	Step: 120279, 	{'train/accuracy': 0.8495694994926453, 'train/loss': 0.8090940117835999, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.3468101024627686, 'validation/num_examples': 50000, 'test/accuracy': 0.5987000465393066, 'test/loss': 1.9820739030838013, 'test/num_examples': 10000, 'score': 40841.25688076019, 'total_duration': 42272.75825166702, 'accumulated_submission_time': 40841.25688076019, 'accumulated_eval_time': 1424.0867023468018, 'accumulated_logging_time': 3.195374011993408}
I0128 00:55:03.581085 139866171975424 logging_writer.py:48] [120279] accumulated_eval_time=1424.086702, accumulated_logging_time=3.195374, accumulated_submission_time=40841.256881, global_step=120279, preemption_count=0, score=40841.256881, test/accuracy=0.598700, test/loss=1.982074, test/num_examples=10000, total_duration=42272.758252, train/accuracy=0.849569, train/loss=0.809094, validation/accuracy=0.720580, validation/loss=1.346810, validation/num_examples=50000
I0128 00:55:11.045528 139866180368128 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.1398725509643555, loss=2.835850238800049
I0128 00:55:44.927243 139866171975424 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.948643207550049, loss=2.7878494262695312
I0128 00:56:18.840839 139866180368128 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.0541460514068604, loss=2.817575454711914
I0128 00:56:52.730648 139866171975424 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.985218048095703, loss=2.855527877807617
I0128 00:57:26.812939 139866180368128 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.864454507827759, loss=2.7887039184570312
I0128 00:58:00.704512 139866171975424 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.006199598312378, loss=2.794797420501709
I0128 00:58:34.657110 139866180368128 logging_writer.py:48] [120900] global_step=120900, grad_norm=3.02968168258667, loss=2.735921621322632
I0128 00:59:08.567088 139866171975424 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.0664210319519043, loss=2.852583408355713
I0128 00:59:42.519722 139866180368128 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.8446762561798096, loss=2.7926247119903564
I0128 01:00:16.442393 139866171975424 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.901587963104248, loss=2.7206125259399414
I0128 01:00:50.351541 139866180368128 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.8005645275115967, loss=2.821071147918701
I0128 01:01:24.279867 139866171975424 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.1880338191986084, loss=2.781142473220825
I0128 01:01:58.163472 139866180368128 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.8999500274658203, loss=2.808314085006714
I0128 01:02:32.099155 139866171975424 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.9262471199035645, loss=2.8142495155334473
I0128 01:03:05.985428 139866180368128 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.9635891914367676, loss=2.7846598625183105
I0128 01:03:33.790373 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:03:39.928133 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:03:48.737107 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:03:51.231634 140027215431488 submission_runner.py:408] Time since start: 42800.45s, 	Step: 121783, 	{'train/accuracy': 0.8489118218421936, 'train/loss': 0.7738045454025269, 'validation/accuracy': 0.7283599972724915, 'validation/loss': 1.285322904586792, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.9310460090637207, 'test/num_examples': 10000, 'score': 41351.40269494057, 'total_duration': 42800.45030093193, 'accumulated_submission_time': 41351.40269494057, 'accumulated_eval_time': 1441.527908563614, 'accumulated_logging_time': 3.246919870376587}
I0128 01:03:51.269577 139865257662208 logging_writer.py:48] [121783] accumulated_eval_time=1441.527909, accumulated_logging_time=3.246920, accumulated_submission_time=41351.402695, global_step=121783, preemption_count=0, score=41351.402695, test/accuracy=0.605000, test/loss=1.931046, test/num_examples=10000, total_duration=42800.450301, train/accuracy=0.848912, train/loss=0.773805, validation/accuracy=0.728360, validation/loss=1.285323, validation/num_examples=50000
I0128 01:03:57.407355 139865266054912 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.0907835960388184, loss=2.8306121826171875
I0128 01:04:31.269286 139865257662208 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.697613000869751, loss=2.737112522125244
I0128 01:05:05.165537 139865266054912 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.9563066959381104, loss=2.814657688140869
I0128 01:05:39.069241 139865257662208 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.0598576068878174, loss=2.8410487174987793
I0128 01:06:12.965584 139865266054912 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.0358529090881348, loss=2.8097238540649414
I0128 01:06:46.859748 139865257662208 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.1107211112976074, loss=2.834071159362793
I0128 01:07:20.767366 139865266054912 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.949497938156128, loss=2.7763454914093018
I0128 01:07:54.669771 139865257662208 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.0533370971679688, loss=2.8325958251953125
I0128 01:08:28.590427 139865266054912 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.12610125541687, loss=2.796489715576172
I0128 01:09:02.488592 139865257662208 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.051609754562378, loss=2.812713146209717
I0128 01:09:36.422857 139865266054912 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.9968488216400146, loss=2.798553466796875
I0128 01:10:10.444277 139865257662208 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.919682025909424, loss=2.8259997367858887
I0128 01:10:44.391288 139865266054912 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.9790289402008057, loss=2.8020126819610596
I0128 01:11:18.265172 139865257662208 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.9204676151275635, loss=2.7318735122680664
I0128 01:11:52.185095 139865266054912 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.1585886478424072, loss=2.819180965423584
I0128 01:12:21.467741 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:12:27.662010 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:12:36.505401 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:12:38.986939 140027215431488 submission_runner.py:408] Time since start: 43328.21s, 	Step: 123288, 	{'train/accuracy': 0.8492506146430969, 'train/loss': 0.7951707243919373, 'validation/accuracy': 0.7295199632644653, 'validation/loss': 1.2949326038360596, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.9248424768447876, 'test/num_examples': 10000, 'score': 41861.53848028183, 'total_duration': 43328.20562505722, 'accumulated_submission_time': 41861.53848028183, 'accumulated_eval_time': 1459.0470685958862, 'accumulated_logging_time': 3.294360637664795}
I0128 01:12:39.027924 139865266054912 logging_writer.py:48] [123288] accumulated_eval_time=1459.047069, accumulated_logging_time=3.294361, accumulated_submission_time=41861.538480, global_step=123288, preemption_count=0, score=41861.538480, test/accuracy=0.604400, test/loss=1.924842, test/num_examples=10000, total_duration=43328.205625, train/accuracy=0.849251, train/loss=0.795171, validation/accuracy=0.729520, validation/loss=1.294933, validation/num_examples=50000
I0128 01:12:43.452550 139865274447616 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.1264939308166504, loss=2.795300006866455
I0128 01:13:17.327870 139865266054912 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.223172426223755, loss=2.8715853691101074
I0128 01:13:51.257814 139865274447616 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.1600823402404785, loss=2.8003668785095215
I0128 01:14:25.128896 139865266054912 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.1657207012176514, loss=2.751406192779541
I0128 01:14:59.085458 139865274447616 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.8931326866149902, loss=2.816793441772461
I0128 01:15:32.988519 139865266054912 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.018251657485962, loss=2.7743067741394043
I0128 01:16:06.897401 139865274447616 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.274188280105591, loss=2.848013401031494
I0128 01:16:40.860813 139865266054912 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.9531803131103516, loss=2.787234306335449
I0128 01:17:14.785258 139865274447616 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.0829951763153076, loss=2.815277576446533
I0128 01:17:48.703203 139865266054912 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.0898075103759766, loss=2.797224521636963
I0128 01:18:22.619141 139865274447616 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.912670612335205, loss=2.680445671081543
I0128 01:18:56.524954 139865266054912 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.1043338775634766, loss=2.8407950401306152
I0128 01:19:30.453684 139865274447616 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.108841896057129, loss=2.847531795501709
I0128 01:20:04.369862 139865266054912 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.1526126861572266, loss=2.888190269470215
I0128 01:20:38.295427 139865274447616 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.0674760341644287, loss=2.7764806747436523
I0128 01:21:09.294540 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:21:15.521860 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:21:24.244696 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:21:26.750296 140027215431488 submission_runner.py:408] Time since start: 43855.97s, 	Step: 124793, 	{'train/accuracy': 0.8462810516357422, 'train/loss': 0.8076683878898621, 'validation/accuracy': 0.7290399670600891, 'validation/loss': 1.3021578788757324, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.9222787618637085, 'test/num_examples': 10000, 'score': 42371.742612838745, 'total_duration': 43855.968982219696, 'accumulated_submission_time': 42371.742612838745, 'accumulated_eval_time': 1476.5027883052826, 'accumulated_logging_time': 3.3449392318725586}
I0128 01:21:26.792690 139866163582720 logging_writer.py:48] [124793] accumulated_eval_time=1476.502788, accumulated_logging_time=3.344939, accumulated_submission_time=42371.742613, global_step=124793, preemption_count=0, score=42371.742613, test/accuracy=0.604400, test/loss=1.922279, test/num_examples=10000, total_duration=43855.968982, train/accuracy=0.846281, train/loss=0.807668, validation/accuracy=0.729040, validation/loss=1.302158, validation/num_examples=50000
I0128 01:21:29.510153 139866180368128 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.3465521335601807, loss=2.7728912830352783
I0128 01:22:03.380508 139866163582720 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.166177988052368, loss=2.8394575119018555
I0128 01:22:37.371687 139866180368128 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.0841522216796875, loss=2.8134663105010986
I0128 01:23:11.267146 139866163582720 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.255427122116089, loss=2.84865403175354
I0128 01:23:45.190476 139866180368128 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.220909357070923, loss=2.786477565765381
I0128 01:24:19.068418 139866163582720 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.4653050899505615, loss=2.8042829036712646
I0128 01:24:52.982785 139866180368128 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.033074140548706, loss=2.7598583698272705
I0128 01:25:26.872798 139866163582720 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.4193084239959717, loss=2.8224196434020996
I0128 01:26:00.808303 139866180368128 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.412109613418579, loss=2.8208749294281006
I0128 01:26:34.744467 139866163582720 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.1798713207244873, loss=2.781855821609497
I0128 01:27:08.657297 139866180368128 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.28806471824646, loss=2.853729009628296
I0128 01:27:42.583645 139866163582720 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.9301867485046387, loss=2.7014214992523193
I0128 01:28:16.520230 139866180368128 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.3770792484283447, loss=2.7877821922302246
I0128 01:28:50.514379 139866163582720 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.829423666000366, loss=2.6903927326202393
I0128 01:29:24.407856 139866180368128 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.0839288234710693, loss=2.780388832092285
I0128 01:29:56.788042 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:30:03.101409 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:30:11.917474 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:30:14.429216 140027215431488 submission_runner.py:408] Time since start: 44383.65s, 	Step: 126297, 	{'train/accuracy': 0.8422752022743225, 'train/loss': 0.7997431755065918, 'validation/accuracy': 0.7246800065040588, 'validation/loss': 1.2923554182052612, 'validation/num_examples': 50000, 'test/accuracy': 0.596500039100647, 'test/loss': 1.9440224170684814, 'test/num_examples': 10000, 'score': 42881.6758556366, 'total_duration': 44383.64790058136, 'accumulated_submission_time': 42881.6758556366, 'accumulated_eval_time': 1494.1439380645752, 'accumulated_logging_time': 3.396597146987915}
I0128 01:30:14.468297 139865760950016 logging_writer.py:48] [126297] accumulated_eval_time=1494.143938, accumulated_logging_time=3.396597, accumulated_submission_time=42881.675856, global_step=126297, preemption_count=0, score=42881.675856, test/accuracy=0.596500, test/loss=1.944022, test/num_examples=10000, total_duration=44383.647901, train/accuracy=0.842275, train/loss=0.799743, validation/accuracy=0.724680, validation/loss=1.292355, validation/num_examples=50000
I0128 01:30:15.849300 139865769342720 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.1186532974243164, loss=2.824840784072876
I0128 01:30:49.730551 139865760950016 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.042269706726074, loss=2.732853651046753
I0128 01:31:23.619108 139865769342720 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.2484192848205566, loss=2.8131728172302246
I0128 01:31:57.527266 139865760950016 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.1302905082702637, loss=2.7807297706604004
I0128 01:32:31.437788 139865769342720 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.1720046997070312, loss=2.7699637413024902
I0128 01:33:05.369448 139865760950016 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.1722352504730225, loss=2.8132200241088867
I0128 01:33:39.257501 139865769342720 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.142634391784668, loss=2.7372589111328125
I0128 01:34:13.191898 139865760950016 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.3716890811920166, loss=2.756103515625
I0128 01:34:47.089384 139865769342720 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.367915153503418, loss=2.78600811958313
I0128 01:35:21.044938 139865760950016 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.197528839111328, loss=2.773405075073242
I0128 01:35:54.958619 139865769342720 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.3973772525787354, loss=2.75529408454895
I0128 01:36:28.850389 139865760950016 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.286710500717163, loss=2.8388490676879883
I0128 01:37:02.760105 139865769342720 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.285050392150879, loss=2.836591958999634
I0128 01:37:36.660115 139865760950016 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.223342180252075, loss=2.771937370300293
I0128 01:38:10.577076 139865769342720 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.1041274070739746, loss=2.7895450592041016
I0128 01:38:44.504205 139865760950016 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.1286966800689697, loss=2.752601385116577
I0128 01:38:44.511776 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:38:50.711379 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:38:59.300151 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:39:01.848560 140027215431488 submission_runner.py:408] Time since start: 44911.07s, 	Step: 127801, 	{'train/accuracy': 0.8557876348495483, 'train/loss': 0.7706797122955322, 'validation/accuracy': 0.7314199805259705, 'validation/loss': 1.2801084518432617, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8942350149154663, 'test/num_examples': 10000, 'score': 43391.6541454792, 'total_duration': 44911.067249298096, 'accumulated_submission_time': 43391.6541454792, 'accumulated_eval_time': 1511.4806642532349, 'accumulated_logging_time': 3.4473884105682373}
I0128 01:39:01.890331 139865274447616 logging_writer.py:48] [127801] accumulated_eval_time=1511.480664, accumulated_logging_time=3.447388, accumulated_submission_time=43391.654145, global_step=127801, preemption_count=0, score=43391.654145, test/accuracy=0.613800, test/loss=1.894235, test/num_examples=10000, total_duration=44911.067249, train/accuracy=0.855788, train/loss=0.770680, validation/accuracy=0.731420, validation/loss=1.280108, validation/num_examples=50000
I0128 01:39:35.772139 139866180368128 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.2801434993743896, loss=2.786078691482544
I0128 01:40:09.682864 139865274447616 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.4059512615203857, loss=2.794952869415283
I0128 01:40:43.589558 139866180368128 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.248711585998535, loss=2.8304271697998047
I0128 01:41:17.609752 139865274447616 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.1562061309814453, loss=2.7445435523986816
I0128 01:41:51.510397 139866180368128 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.034665822982788, loss=2.750948667526245
I0128 01:42:25.433843 139865274447616 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.213322639465332, loss=2.754504680633545
I0128 01:42:59.317068 139866180368128 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.2273457050323486, loss=2.7865333557128906
I0128 01:43:33.237921 139865274447616 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.234600782394409, loss=2.7406346797943115
I0128 01:44:07.150046 139866180368128 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.1138484477996826, loss=2.7462594509124756
I0128 01:44:41.069753 139865274447616 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.266751527786255, loss=2.7653756141662598
I0128 01:45:14.967983 139866180368128 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.139631986618042, loss=2.748237133026123
I0128 01:45:48.884300 139865274447616 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.0821027755737305, loss=2.7136144638061523
I0128 01:46:22.774137 139866180368128 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.2359848022460938, loss=2.7337515354156494
I0128 01:46:56.691707 139865274447616 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.275775194168091, loss=2.7800614833831787
I0128 01:47:30.807722 139866180368128 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.3389933109283447, loss=2.8082268238067627
I0128 01:47:31.988898 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:47:38.250256 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:47:46.985541 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:47:49.456826 140027215431488 submission_runner.py:408] Time since start: 45438.68s, 	Step: 129305, 	{'train/accuracy': 0.8703563213348389, 'train/loss': 0.682815670967102, 'validation/accuracy': 0.7312600016593933, 'validation/loss': 1.259967565536499, 'validation/num_examples': 50000, 'test/accuracy': 0.6118000149726868, 'test/loss': 1.8826649188995361, 'test/num_examples': 10000, 'score': 43901.69043469429, 'total_duration': 45438.67550635338, 'accumulated_submission_time': 43901.69043469429, 'accumulated_eval_time': 1528.9485657215118, 'accumulated_logging_time': 3.4982686042785645}
I0128 01:47:49.499053 139865266054912 logging_writer.py:48] [129305] accumulated_eval_time=1528.948566, accumulated_logging_time=3.498269, accumulated_submission_time=43901.690435, global_step=129305, preemption_count=0, score=43901.690435, test/accuracy=0.611800, test/loss=1.882665, test/num_examples=10000, total_duration=45438.675506, train/accuracy=0.870356, train/loss=0.682816, validation/accuracy=0.731260, validation/loss=1.259968, validation/num_examples=50000
I0128 01:48:22.015650 139865274447616 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.2687594890594482, loss=2.679786443710327
I0128 01:48:55.927937 139865266054912 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.3626325130462646, loss=2.7874207496643066
I0128 01:49:29.826381 139865274447616 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.1360394954681396, loss=2.8263983726501465
I0128 01:50:03.768127 139865266054912 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.4243757724761963, loss=2.7830324172973633
I0128 01:50:37.694772 139865274447616 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.518726110458374, loss=2.6952595710754395
I0128 01:51:11.613760 139865266054912 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.146327018737793, loss=2.7438478469848633
I0128 01:51:45.540172 139865274447616 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.300518751144409, loss=2.7812469005584717
I0128 01:52:19.484007 139865266054912 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.4597465991973877, loss=2.74222731590271
I0128 01:52:53.362811 139865274447616 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.198711395263672, loss=2.6963329315185547
I0128 01:53:27.306086 139865266054912 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.0680930614471436, loss=2.752310276031494
I0128 01:54:01.274857 139865274447616 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.129826307296753, loss=2.703416109085083
I0128 01:54:35.142632 139865266054912 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.2519218921661377, loss=2.692603588104248
I0128 01:55:09.064530 139865274447616 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.1409285068511963, loss=2.7538657188415527
I0128 01:55:42.945502 139865266054912 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.2339231967926025, loss=2.7501673698425293
I0128 01:56:16.858034 139865274447616 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.424150228500366, loss=2.8551669120788574
I0128 01:56:19.705412 140027215431488 spec.py:321] Evaluating on the training split.
I0128 01:56:25.901345 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 01:56:34.517388 140027215431488 spec.py:349] Evaluating on the test split.
I0128 01:56:37.017534 140027215431488 submission_runner.py:408] Time since start: 45966.24s, 	Step: 130810, 	{'train/accuracy': 0.869559109210968, 'train/loss': 0.7021733522415161, 'validation/accuracy': 0.7360599637031555, 'validation/loss': 1.262072205543518, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.8829753398895264, 'test/num_examples': 10000, 'score': 44411.83263254166, 'total_duration': 45966.23621058464, 'accumulated_submission_time': 44411.83263254166, 'accumulated_eval_time': 1546.2606403827667, 'accumulated_logging_time': 3.5511486530303955}
I0128 01:56:37.058347 139866171975424 logging_writer.py:48] [130810] accumulated_eval_time=1546.260640, accumulated_logging_time=3.551149, accumulated_submission_time=44411.832633, global_step=130810, preemption_count=0, score=44411.832633, test/accuracy=0.610500, test/loss=1.882975, test/num_examples=10000, total_duration=45966.236211, train/accuracy=0.869559, train/loss=0.702173, validation/accuracy=0.736060, validation/loss=1.262072, validation/num_examples=50000
I0128 01:57:07.934030 139866180368128 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.3347034454345703, loss=2.773791790008545
I0128 01:57:41.758936 139866171975424 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.285510540008545, loss=2.721902847290039
I0128 01:58:15.658483 139866180368128 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.345989465713501, loss=2.7427568435668945
I0128 01:58:49.579077 139866171975424 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.05869722366333, loss=2.713921546936035
I0128 01:59:23.502533 139866180368128 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.41213059425354, loss=2.757080316543579
I0128 01:59:57.504043 139866171975424 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.337789535522461, loss=2.768144130706787
I0128 02:00:31.449780 139866180368128 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.2936978340148926, loss=2.7199599742889404
I0128 02:01:05.363303 139866171975424 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.2039198875427246, loss=2.792304515838623
I0128 02:01:39.277166 139866180368128 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.190700054168701, loss=2.7478768825531006
I0128 02:02:13.220535 139866171975424 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.0781426429748535, loss=2.6583807468414307
I0128 02:02:47.131291 139866180368128 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.0631072521209717, loss=2.6715054512023926
I0128 02:03:21.396000 139866171975424 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.3429620265960693, loss=2.710646629333496
I0128 02:03:55.312571 139866180368128 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.2598533630371094, loss=2.694823741912842
I0128 02:04:29.221819 139866171975424 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.250171422958374, loss=2.7640764713287354
I0128 02:05:03.152494 139866180368128 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.293884754180908, loss=2.7645812034606934
I0128 02:05:07.022446 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:05:13.249001 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:05:21.749449 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:05:24.296652 140027215431488 submission_runner.py:408] Time since start: 46493.52s, 	Step: 132313, 	{'train/accuracy': 0.8615074753761292, 'train/loss': 0.731268048286438, 'validation/accuracy': 0.7324999570846558, 'validation/loss': 1.2648930549621582, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.890793800354004, 'test/num_examples': 10000, 'score': 44921.73423457146, 'total_duration': 46493.515330314636, 'accumulated_submission_time': 44921.73423457146, 'accumulated_eval_time': 1563.5348060131073, 'accumulated_logging_time': 3.6022284030914307}
I0128 02:05:24.337562 139865760950016 logging_writer.py:48] [132313] accumulated_eval_time=1563.534806, accumulated_logging_time=3.602228, accumulated_submission_time=44921.734235, global_step=132313, preemption_count=0, score=44921.734235, test/accuracy=0.610800, test/loss=1.890794, test/num_examples=10000, total_duration=46493.515330, train/accuracy=0.861507, train/loss=0.731268, validation/accuracy=0.732500, validation/loss=1.264893, validation/num_examples=50000
I0128 02:05:54.152893 139865769342720 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.255734443664551, loss=2.7374510765075684
I0128 02:06:28.109176 139865760950016 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.55055832862854, loss=2.7628183364868164
I0128 02:07:02.006628 139865769342720 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.3740925788879395, loss=2.749070405960083
I0128 02:07:35.945826 139865760950016 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.328564167022705, loss=2.7330923080444336
I0128 02:08:09.861612 139865769342720 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2321434020996094, loss=2.7445387840270996
I0128 02:08:43.751147 139865760950016 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.339770555496216, loss=2.742652416229248
I0128 02:09:17.661667 139865769342720 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.113504409790039, loss=2.736480474472046
I0128 02:09:51.583536 139865760950016 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.2027580738067627, loss=2.752756357192993
I0128 02:10:25.478711 139865769342720 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.1437485218048096, loss=2.692923069000244
I0128 02:10:59.411246 139865760950016 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.5484776496887207, loss=2.7989139556884766
I0128 02:11:33.335877 139865769342720 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.4450438022613525, loss=2.729753255844116
I0128 02:12:07.240689 139865760950016 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.308248996734619, loss=2.759011745452881
I0128 02:12:41.342694 139865769342720 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.2037711143493652, loss=2.65792179107666
I0128 02:13:15.232909 139865760950016 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.311901569366455, loss=2.7593541145324707
I0128 02:13:49.151988 139865769342720 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.3108794689178467, loss=2.6709582805633545
I0128 02:13:54.381617 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:14:00.592085 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:14:09.457871 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:14:11.976073 140027215431488 submission_runner.py:408] Time since start: 47021.19s, 	Step: 133817, 	{'train/accuracy': 0.8706154227256775, 'train/loss': 0.7510040998458862, 'validation/accuracy': 0.7382999658584595, 'validation/loss': 1.2810701131820679, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.9176183938980103, 'test/num_examples': 10000, 'score': 45431.71617555618, 'total_duration': 47021.19475483894, 'accumulated_submission_time': 45431.71617555618, 'accumulated_eval_time': 1581.1292176246643, 'accumulated_logging_time': 3.6521716117858887}
I0128 02:14:12.017337 139866171975424 logging_writer.py:48] [133817] accumulated_eval_time=1581.129218, accumulated_logging_time=3.652172, accumulated_submission_time=45431.716176, global_step=133817, preemption_count=0, score=45431.716176, test/accuracy=0.615900, test/loss=1.917618, test/num_examples=10000, total_duration=47021.194755, train/accuracy=0.870615, train/loss=0.751004, validation/accuracy=0.738300, validation/loss=1.281070, validation/num_examples=50000
I0128 02:14:40.480950 139866180368128 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.1678667068481445, loss=2.697361707687378
I0128 02:15:14.392505 139866171975424 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.409487724304199, loss=2.7271833419799805
I0128 02:15:48.250165 139866180368128 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.382925510406494, loss=2.756218671798706
I0128 02:16:22.173879 139866171975424 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.587144136428833, loss=2.7232582569122314
I0128 02:16:56.049885 139866180368128 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.065115451812744, loss=2.712531089782715
I0128 02:17:29.983397 139866171975424 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.2389044761657715, loss=2.7355289459228516
I0128 02:18:03.849097 139866180368128 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.1030216217041016, loss=2.711219549179077
I0128 02:18:37.871403 139866171975424 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3866260051727295, loss=2.6750359535217285
I0128 02:19:11.805427 139866180368128 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.347795009613037, loss=2.781097173690796
I0128 02:19:45.716456 139866171975424 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.5738418102264404, loss=2.766866445541382
I0128 02:20:19.610570 139866180368128 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.4655349254608154, loss=2.7405972480773926
I0128 02:20:53.516454 139866171975424 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.3505098819732666, loss=2.76554274559021
I0128 02:21:27.415991 139866180368128 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.3707644939422607, loss=2.7588460445404053
I0128 02:22:01.351125 139866171975424 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.30554461479187, loss=2.723825454711914
I0128 02:22:35.255230 139866180368128 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.2936718463897705, loss=2.6702067852020264
I0128 02:22:42.183011 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:22:48.395427 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:22:57.245793 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:22:59.704862 140027215431488 submission_runner.py:408] Time since start: 47548.92s, 	Step: 135322, 	{'train/accuracy': 0.8668088316917419, 'train/loss': 0.6989973187446594, 'validation/accuracy': 0.7389199733734131, 'validation/loss': 1.2317496538162231, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.8706231117248535, 'test/num_examples': 10000, 'score': 45941.8173930645, 'total_duration': 47548.92354273796, 'accumulated_submission_time': 45941.8173930645, 'accumulated_eval_time': 1598.651022195816, 'accumulated_logging_time': 3.7039690017700195}
I0128 02:22:59.749294 139865274447616 logging_writer.py:48] [135322] accumulated_eval_time=1598.651022, accumulated_logging_time=3.703969, accumulated_submission_time=45941.817393, global_step=135322, preemption_count=0, score=45941.817393, test/accuracy=0.614800, test/loss=1.870623, test/num_examples=10000, total_duration=47548.923543, train/accuracy=0.866809, train/loss=0.698997, validation/accuracy=0.738920, validation/loss=1.231750, validation/num_examples=50000
I0128 02:23:26.522890 139865760950016 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.4340312480926514, loss=2.680793523788452
I0128 02:24:00.406627 139865274447616 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.2964842319488525, loss=2.699239730834961
I0128 02:24:34.302496 139865760950016 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.0637624263763428, loss=2.6806516647338867
I0128 02:25:08.315449 139865274447616 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.6933748722076416, loss=2.6865687370300293
I0128 02:25:42.220147 139865760950016 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.286806344985962, loss=2.7172024250030518
I0128 02:26:16.133783 139865274447616 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.579313278198242, loss=2.7672348022460938
I0128 02:26:50.022122 139865760950016 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.4692795276641846, loss=2.7501397132873535
I0128 02:27:23.932066 139865274447616 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.192410945892334, loss=2.7300915718078613
I0128 02:27:57.860915 139865760950016 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.392202615737915, loss=2.7125885486602783
I0128 02:28:31.790151 139865274447616 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.8918051719665527, loss=2.7591934204101562
I0128 02:29:05.721386 139865760950016 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.305542469024658, loss=2.7254862785339355
I0128 02:29:39.625168 139865274447616 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.67289400100708, loss=2.809387445449829
I0128 02:30:13.540007 139865760950016 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.3042659759521484, loss=2.783536672592163
I0128 02:30:47.429273 139865274447616 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.358450412750244, loss=2.7243402004241943
I0128 02:31:21.429970 139865760950016 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.4557740688323975, loss=2.7559754848480225
I0128 02:31:29.721261 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:31:35.935708 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:31:44.604676 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:31:47.093472 140027215431488 submission_runner.py:408] Time since start: 48076.31s, 	Step: 136826, 	{'train/accuracy': 0.8732461333274841, 'train/loss': 0.7015025019645691, 'validation/accuracy': 0.7404599785804749, 'validation/loss': 1.2484326362609863, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.886087417602539, 'test/num_examples': 10000, 'score': 46451.72684264183, 'total_duration': 48076.31215286255, 'accumulated_submission_time': 46451.72684264183, 'accumulated_eval_time': 1616.0231895446777, 'accumulated_logging_time': 3.759093761444092}
I0128 02:31:47.136746 139866163582720 logging_writer.py:48] [136826] accumulated_eval_time=1616.023190, accumulated_logging_time=3.759094, accumulated_submission_time=46451.726843, global_step=136826, preemption_count=0, score=46451.726843, test/accuracy=0.617200, test/loss=1.886087, test/num_examples=10000, total_duration=48076.312153, train/accuracy=0.873246, train/loss=0.701503, validation/accuracy=0.740460, validation/loss=1.248433, validation/num_examples=50000
I0128 02:32:12.542810 139866171975424 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.428633451461792, loss=2.7305359840393066
I0128 02:32:46.415608 139866163582720 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.2809627056121826, loss=2.63480281829834
I0128 02:33:20.322806 139866171975424 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.23400616645813, loss=2.6729660034179688
I0128 02:33:54.186853 139866163582720 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.2418487071990967, loss=2.6562044620513916
I0128 02:34:28.123672 139866171975424 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.333225965499878, loss=2.6991963386535645
I0128 02:35:01.981183 139866163582720 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.5999176502227783, loss=2.7350502014160156
I0128 02:35:35.918466 139866171975424 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.4163501262664795, loss=2.6973092555999756
I0128 02:36:09.804791 139866163582720 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.479682445526123, loss=2.7789242267608643
I0128 02:36:43.741591 139866171975424 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.464628219604492, loss=2.7135660648345947
I0128 02:37:17.640219 139866163582720 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.5014967918395996, loss=2.6746859550476074
I0128 02:37:51.598626 139866171975424 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.511822462081909, loss=2.765866994857788
I0128 02:38:25.490783 139866163582720 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.5785000324249268, loss=2.77310848236084
I0128 02:38:59.355788 139866171975424 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.5259909629821777, loss=2.704162120819092
I0128 02:39:33.268113 139866163582720 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.436781644821167, loss=2.745615243911743
I0128 02:40:07.142929 139866171975424 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.298203229904175, loss=2.6564671993255615
I0128 02:40:17.135786 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:40:23.334445 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:40:32.035164 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:40:34.578885 140027215431488 submission_runner.py:408] Time since start: 48603.80s, 	Step: 138331, 	{'train/accuracy': 0.8901665806770325, 'train/loss': 0.6423742771148682, 'validation/accuracy': 0.7400799989700317, 'validation/loss': 1.2568217515945435, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.88319993019104, 'test/num_examples': 10000, 'score': 46961.66208958626, 'total_duration': 48603.79757499695, 'accumulated_submission_time': 46961.66208958626, 'accumulated_eval_time': 1633.4662518501282, 'accumulated_logging_time': 3.812281608581543}
I0128 02:40:34.624063 139865266054912 logging_writer.py:48] [138331] accumulated_eval_time=1633.466252, accumulated_logging_time=3.812282, accumulated_submission_time=46961.662090, global_step=138331, preemption_count=0, score=46961.662090, test/accuracy=0.616500, test/loss=1.883200, test/num_examples=10000, total_duration=48603.797575, train/accuracy=0.890167, train/loss=0.642374, validation/accuracy=0.740080, validation/loss=1.256822, validation/num_examples=50000
I0128 02:40:58.342764 139865760950016 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.622328996658325, loss=2.747654676437378
I0128 02:41:32.196752 139865266054912 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.646352529525757, loss=2.6995034217834473
I0128 02:42:06.108185 139865760950016 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.594188928604126, loss=2.682283878326416
I0128 02:42:40.011988 139865266054912 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.358778476715088, loss=2.7350993156433105
I0128 02:43:13.924914 139865760950016 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.5835533142089844, loss=2.725395441055298
I0128 02:43:47.874757 139865266054912 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.2962429523468018, loss=2.6744437217712402
I0128 02:44:21.783574 139865760950016 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.4930756092071533, loss=2.7135214805603027
I0128 02:44:55.732570 139865266054912 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.58001708984375, loss=2.7032582759857178
I0128 02:45:29.633590 139865760950016 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.2689380645751953, loss=2.6922879219055176
I0128 02:46:03.586584 139865266054912 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.3996522426605225, loss=2.6730010509490967
I0128 02:46:37.509330 139865760950016 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.4864394664764404, loss=2.762481212615967
I0128 02:47:11.412748 139865266054912 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.4236438274383545, loss=2.6669363975524902
I0128 02:47:45.353930 139865760950016 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.5038506984710693, loss=2.6908960342407227
I0128 02:48:19.243408 139865266054912 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.5808238983154297, loss=2.726863384246826
I0128 02:48:53.175843 139865760950016 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.4218926429748535, loss=2.734560012817383
I0128 02:49:04.828178 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:49:11.033268 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:49:19.690715 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:49:22.084689 140027215431488 submission_runner.py:408] Time since start: 49131.30s, 	Step: 139836, 	{'train/accuracy': 0.8897680044174194, 'train/loss': 0.627855658531189, 'validation/accuracy': 0.74235999584198, 'validation/loss': 1.2312299013137817, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.8507357835769653, 'test/num_examples': 10000, 'score': 47471.80485224724, 'total_duration': 49131.303370952606, 'accumulated_submission_time': 47471.80485224724, 'accumulated_eval_time': 1650.7227218151093, 'accumulated_logging_time': 3.866595983505249}
I0128 02:49:22.130841 139865266054912 logging_writer.py:48] [139836] accumulated_eval_time=1650.722722, accumulated_logging_time=3.866596, accumulated_submission_time=47471.804852, global_step=139836, preemption_count=0, score=47471.804852, test/accuracy=0.622900, test/loss=1.850736, test/num_examples=10000, total_duration=49131.303371, train/accuracy=0.889768, train/loss=0.627856, validation/accuracy=0.742360, validation/loss=1.231230, validation/num_examples=50000
I0128 02:49:44.153138 139865274447616 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.2961714267730713, loss=2.6822926998138428
I0128 02:50:18.078379 139865266054912 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.5012612342834473, loss=2.6318249702453613
I0128 02:50:51.982521 139865274447616 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.5170247554779053, loss=2.6746835708618164
I0128 02:51:25.888070 139865266054912 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.625788450241089, loss=2.6824395656585693
I0128 02:51:59.794555 139865274447616 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.3745038509368896, loss=2.6861305236816406
I0128 02:52:33.682404 139865266054912 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.3718230724334717, loss=2.709949254989624
I0128 02:53:07.581150 139865274447616 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.646648406982422, loss=2.7104203701019287
I0128 02:53:41.506040 139865266054912 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.5682053565979004, loss=2.711576461791992
I0128 02:54:15.415736 139865274447616 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.4824585914611816, loss=2.6348705291748047
I0128 02:54:49.330069 139865266054912 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.448834180831909, loss=2.713059663772583
I0128 02:55:23.238433 139865274447616 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.5574071407318115, loss=2.670370578765869
I0128 02:55:57.152621 139865266054912 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.806529998779297, loss=2.6554157733917236
I0128 02:56:31.159273 139865274447616 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.7148029804229736, loss=2.74123477935791
I0128 02:57:05.064990 139865266054912 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.292612314224243, loss=2.636556386947632
I0128 02:57:38.995079 139865274447616 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.292797088623047, loss=2.67454195022583
I0128 02:57:52.382409 140027215431488 spec.py:321] Evaluating on the training split.
I0128 02:57:58.616962 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 02:58:07.421063 140027215431488 spec.py:349] Evaluating on the test split.
I0128 02:58:09.882243 140027215431488 submission_runner.py:408] Time since start: 49659.10s, 	Step: 141341, 	{'train/accuracy': 0.886738657951355, 'train/loss': 0.6396247744560242, 'validation/accuracy': 0.745639979839325, 'validation/loss': 1.2290503978729248, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.8512567281723022, 'test/num_examples': 10000, 'score': 47981.99515795708, 'total_duration': 49659.100927591324, 'accumulated_submission_time': 47981.99515795708, 'accumulated_eval_time': 1668.2225155830383, 'accumulated_logging_time': 3.9220056533813477}
I0128 02:58:09.933017 139866180368128 logging_writer.py:48] [141341] accumulated_eval_time=1668.222516, accumulated_logging_time=3.922006, accumulated_submission_time=47981.995158, global_step=141341, preemption_count=0, score=47981.995158, test/accuracy=0.623700, test/loss=1.851257, test/num_examples=10000, total_duration=49659.100928, train/accuracy=0.886739, train/loss=0.639625, validation/accuracy=0.745640, validation/loss=1.229050, validation/num_examples=50000
I0128 02:58:30.273178 139866188760832 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.5039820671081543, loss=2.691251516342163
I0128 02:59:04.141928 139866180368128 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.3752403259277344, loss=2.739532947540283
I0128 02:59:38.040833 139866188760832 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.336937665939331, loss=2.7390811443328857
I0128 03:00:11.932139 139866180368128 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.6337428092956543, loss=2.6794285774230957
I0128 03:00:45.878450 139866188760832 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.5840096473693848, loss=2.692396402359009
I0128 03:01:19.753216 139866180368128 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.512089729309082, loss=2.7118630409240723
I0128 03:01:53.674605 139866188760832 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.7193660736083984, loss=2.7083206176757812
I0128 03:02:27.656132 139866180368128 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.4336965084075928, loss=2.6467065811157227
I0128 03:03:01.564093 139866188760832 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.4985756874084473, loss=2.668956756591797
I0128 03:03:35.482975 139866180368128 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.3764748573303223, loss=2.623466730117798
I0128 03:04:09.401396 139866188760832 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.471683979034424, loss=2.679994583129883
I0128 03:04:43.296047 139866180368128 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.6608009338378906, loss=2.6261935234069824
I0128 03:05:17.217063 139866188760832 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6135475635528564, loss=2.679995536804199
I0128 03:05:51.135426 139866180368128 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.7975118160247803, loss=2.707172393798828
I0128 03:06:25.054756 139866188760832 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.6851155757904053, loss=2.744591236114502
I0128 03:06:40.108902 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:06:46.265004 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:06:54.803749 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:06:57.300299 140027215431488 submission_runner.py:408] Time since start: 50186.52s, 	Step: 142846, 	{'train/accuracy': 0.8891502022743225, 'train/loss': 0.6449418067932129, 'validation/accuracy': 0.7464199662208557, 'validation/loss': 1.2233476638793945, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.851295828819275, 'test/num_examples': 10000, 'score': 48492.108885765076, 'total_duration': 50186.51897478104, 'accumulated_submission_time': 48492.108885765076, 'accumulated_eval_time': 1685.4138662815094, 'accumulated_logging_time': 3.9819884300231934}
I0128 03:06:57.341952 139865257662208 logging_writer.py:48] [142846] accumulated_eval_time=1685.413866, accumulated_logging_time=3.981988, accumulated_submission_time=48492.108886, global_step=142846, preemption_count=0, score=48492.108886, test/accuracy=0.623400, test/loss=1.851296, test/num_examples=10000, total_duration=50186.518975, train/accuracy=0.889150, train/loss=0.644942, validation/accuracy=0.746420, validation/loss=1.223348, validation/num_examples=50000
I0128 03:07:15.965872 139865266054912 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.5466442108154297, loss=2.7131264209747314
I0128 03:07:49.854891 139865257662208 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.6450698375701904, loss=2.687926769256592
I0128 03:08:23.764876 139865266054912 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.566437244415283, loss=2.6788926124572754
I0128 03:08:57.723962 139865257662208 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.6311750411987305, loss=2.6896936893463135
I0128 03:09:31.624204 139865266054912 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.628904342651367, loss=2.693610429763794
I0128 03:10:05.534567 139865257662208 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.5916426181793213, loss=2.727344512939453
I0128 03:10:39.442785 139865266054912 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.7269911766052246, loss=2.661144256591797
I0128 03:11:13.378535 139865257662208 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.9381983280181885, loss=2.7417256832122803
I0128 03:11:47.265324 139865266054912 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.5287368297576904, loss=2.69683837890625
I0128 03:12:21.190397 139865257662208 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.527315616607666, loss=2.6489524841308594
I0128 03:12:55.092745 139865266054912 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.3764772415161133, loss=2.6101036071777344
I0128 03:13:29.000828 139865257662208 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.7549386024475098, loss=2.6737060546875
I0128 03:14:02.929715 139865266054912 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.7972726821899414, loss=2.7270922660827637
I0128 03:14:36.821705 139865257662208 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.346184730529785, loss=2.613612651824951
I0128 03:15:10.844568 139865266054912 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.655219316482544, loss=2.661234140396118
I0128 03:15:27.599650 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:15:33.811324 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:15:42.365364 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:15:44.863716 140027215431488 submission_runner.py:408] Time since start: 50714.08s, 	Step: 144351, 	{'train/accuracy': 0.8878347873687744, 'train/loss': 0.636131763458252, 'validation/accuracy': 0.7448999881744385, 'validation/loss': 1.2222989797592163, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.849866271018982, 'test/num_examples': 10000, 'score': 49002.305592775345, 'total_duration': 50714.08239650726, 'accumulated_submission_time': 49002.305592775345, 'accumulated_eval_time': 1702.677888393402, 'accumulated_logging_time': 4.033036470413208}
I0128 03:15:44.905174 139865266054912 logging_writer.py:48] [144351] accumulated_eval_time=1702.677888, accumulated_logging_time=4.033036, accumulated_submission_time=49002.305593, global_step=144351, preemption_count=0, score=49002.305593, test/accuracy=0.627900, test/loss=1.849866, test/num_examples=10000, total_duration=50714.082397, train/accuracy=0.887835, train/loss=0.636132, validation/accuracy=0.744900, validation/loss=1.222299, validation/num_examples=50000
I0128 03:16:01.866642 139866171975424 logging_writer.py:48] [144400] global_step=144400, grad_norm=4.023321628570557, loss=2.768991470336914
I0128 03:16:35.737999 139865266054912 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.705336332321167, loss=2.660654306411743
I0128 03:17:09.616541 139866171975424 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.9288835525512695, loss=2.6623241901397705
I0128 03:17:43.514270 139865266054912 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.648898124694824, loss=2.6743314266204834
I0128 03:18:17.396347 139866171975424 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.6300032138824463, loss=2.6598052978515625
I0128 03:18:51.321189 139865266054912 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.8684420585632324, loss=2.705864667892456
I0128 03:19:25.212162 139866171975424 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.6034443378448486, loss=2.669093608856201
I0128 03:19:59.123199 139865266054912 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.561248540878296, loss=2.647465229034424
I0128 03:20:33.036457 139866171975424 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.605954647064209, loss=2.66910719871521
I0128 03:21:07.046456 139865266054912 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.6514172554016113, loss=2.7425570487976074
I0128 03:21:40.948024 139866171975424 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.45332407951355, loss=2.6733481884002686
I0128 03:22:14.875347 139865266054912 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.3722925186157227, loss=2.622868537902832
I0128 03:22:48.783190 139866171975424 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.622947931289673, loss=2.694575071334839
I0128 03:23:22.698590 139865266054912 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.5734431743621826, loss=2.65802264213562
I0128 03:23:56.605098 139866171975424 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.6694352626800537, loss=2.6257171630859375
I0128 03:24:15.056148 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:24:21.218244 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:24:29.975027 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:24:32.418416 140027215431488 submission_runner.py:408] Time since start: 51241.64s, 	Step: 145856, 	{'train/accuracy': 0.8903858065605164, 'train/loss': 0.6295886039733887, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.2230639457702637, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.8473961353302002, 'test/num_examples': 10000, 'score': 49512.394984960556, 'total_duration': 51241.637093544006, 'accumulated_submission_time': 49512.394984960556, 'accumulated_eval_time': 1720.0401091575623, 'accumulated_logging_time': 4.083997964859009}
I0128 03:24:32.465743 139865274447616 logging_writer.py:48] [145856] accumulated_eval_time=1720.040109, accumulated_logging_time=4.083998, accumulated_submission_time=49512.394985, global_step=145856, preemption_count=0, score=49512.394985, test/accuracy=0.626100, test/loss=1.847396, test/num_examples=10000, total_duration=51241.637094, train/accuracy=0.890386, train/loss=0.629589, validation/accuracy=0.745700, validation/loss=1.223064, validation/num_examples=50000
I0128 03:24:47.739888 139865760950016 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.5482003688812256, loss=2.6627297401428223
I0128 03:25:21.627398 139865274447616 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.894862651824951, loss=2.6745810508728027
I0128 03:25:55.509648 139865760950016 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.8235673904418945, loss=2.6524462699890137
I0128 03:26:29.430952 139865274447616 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.6521573066711426, loss=2.6651477813720703
I0128 03:27:03.324075 139865760950016 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.548635244369507, loss=2.6657447814941406
I0128 03:27:37.459969 139865274447616 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.5724947452545166, loss=2.662250518798828
I0128 03:28:11.384884 139865760950016 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.690446615219116, loss=2.6344010829925537
I0128 03:28:45.307453 139865274447616 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.5593457221984863, loss=2.6647608280181885
I0128 03:29:19.195038 139865760950016 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.66849684715271, loss=2.64076566696167
I0128 03:29:53.109046 139865274447616 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.4223380088806152, loss=2.622786521911621
I0128 03:30:27.008362 139865760950016 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.8558874130249023, loss=2.674574375152588
I0128 03:31:00.914962 139865274447616 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.850893974304199, loss=2.637385129928589
I0128 03:31:34.814515 139865760950016 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.69326114654541, loss=2.630457639694214
I0128 03:32:08.707581 139865274447616 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.7615954875946045, loss=2.611952304840088
I0128 03:32:42.631055 139865760950016 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.6763916015625, loss=2.721069812774658
I0128 03:33:02.427861 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:33:08.679980 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:33:17.423915 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:33:19.890393 140027215431488 submission_runner.py:408] Time since start: 51769.11s, 	Step: 147360, 	{'train/accuracy': 0.9123684167861938, 'train/loss': 0.5474082827568054, 'validation/accuracy': 0.7480799555778503, 'validation/loss': 1.2101439237594604, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8346625566482544, 'test/num_examples': 10000, 'score': 50022.29506134987, 'total_duration': 51769.10906100273, 'accumulated_submission_time': 50022.29506134987, 'accumulated_eval_time': 1737.5025906562805, 'accumulated_logging_time': 4.140517711639404}
I0128 03:33:19.953714 139865266054912 logging_writer.py:48] [147360] accumulated_eval_time=1737.502591, accumulated_logging_time=4.140518, accumulated_submission_time=50022.295061, global_step=147360, preemption_count=0, score=50022.295061, test/accuracy=0.631000, test/loss=1.834663, test/num_examples=10000, total_duration=51769.109061, train/accuracy=0.912368, train/loss=0.547408, validation/accuracy=0.748080, validation/loss=1.210144, validation/num_examples=50000
I0128 03:33:33.885006 139865274447616 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.4081709384918213, loss=2.642779588699341
I0128 03:34:07.844358 139865266054912 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.547610282897949, loss=2.6568872928619385
I0128 03:34:41.722908 139865274447616 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.594273090362549, loss=2.556605577468872
I0128 03:35:15.657283 139865266054912 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.8035011291503906, loss=2.6296465396881104
I0128 03:35:49.581059 139865274447616 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.568758964538574, loss=2.6436166763305664
I0128 03:36:23.483268 139865266054912 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.6583902835845947, loss=2.604480266571045
I0128 03:36:57.413291 139865274447616 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.6661620140075684, loss=2.634448289871216
I0128 03:37:31.294871 139865266054912 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.467684030532837, loss=2.6036887168884277
I0128 03:38:05.234157 139865274447616 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.8151040077209473, loss=2.7164313793182373
I0128 03:38:39.155649 139865266054912 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.765479326248169, loss=2.6297121047973633
I0128 03:39:13.064112 139865274447616 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.8005142211914062, loss=2.6501917839050293
I0128 03:39:47.026005 139865266054912 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.870344877243042, loss=2.6642942428588867
I0128 03:40:21.091836 139865274447616 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.9446353912353516, loss=2.726203680038452
I0128 03:40:55.030441 139865266054912 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.673121690750122, loss=2.6355559825897217
I0128 03:41:28.944931 139865274447616 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.6583900451660156, loss=2.676297664642334
I0128 03:41:50.086021 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:41:56.382817 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:42:04.948114 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:42:07.492331 140027215431488 submission_runner.py:408] Time since start: 52296.71s, 	Step: 148864, 	{'train/accuracy': 0.9058912396430969, 'train/loss': 0.5808451175689697, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.2156970500946045, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8480470180511475, 'test/num_examples': 10000, 'score': 50532.36496210098, 'total_duration': 52296.71101593971, 'accumulated_submission_time': 50532.36496210098, 'accumulated_eval_time': 1754.9088788032532, 'accumulated_logging_time': 4.21375036239624}
I0128 03:42:07.535743 139865266054912 logging_writer.py:48] [148864] accumulated_eval_time=1754.908879, accumulated_logging_time=4.213750, accumulated_submission_time=50532.364962, global_step=148864, preemption_count=0, score=50532.364962, test/accuracy=0.629000, test/loss=1.848047, test/num_examples=10000, total_duration=52296.711016, train/accuracy=0.905891, train/loss=0.580845, validation/accuracy=0.750320, validation/loss=1.215697, validation/num_examples=50000
I0128 03:42:20.104786 139866163582720 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.865537166595459, loss=2.63401460647583
I0128 03:42:53.990026 139865266054912 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.51576828956604, loss=2.604735851287842
I0128 03:43:27.899514 139866163582720 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.638381004333496, loss=2.6163992881774902
I0128 03:44:01.804127 139865266054912 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.07722282409668, loss=2.6999568939208984
I0128 03:44:35.714493 139866163582720 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.6519384384155273, loss=2.6461896896362305
I0128 03:45:09.641435 139865266054912 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.606415271759033, loss=2.678061008453369
I0128 03:45:43.557592 139866163582720 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.8189499378204346, loss=2.650439977645874
I0128 03:46:17.572411 139865266054912 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.863874673843384, loss=2.6940932273864746
I0128 03:46:51.511241 139866163582720 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.8692967891693115, loss=2.6331562995910645
I0128 03:47:25.417222 139865266054912 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.847677230834961, loss=2.7000558376312256
I0128 03:47:59.337988 139866163582720 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.570539951324463, loss=2.6405887603759766
I0128 03:48:33.251584 139865266054912 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.6101269721984863, loss=2.60963773727417
I0128 03:49:07.188836 139866163582720 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.8934402465820312, loss=2.653289318084717
I0128 03:49:41.112530 139865266054912 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.6150386333465576, loss=2.5815513134002686
I0128 03:50:14.991443 139866163582720 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.7588188648223877, loss=2.6501834392547607
I0128 03:50:37.506963 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:50:44.201837 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:50:52.901960 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:50:55.439895 140027215431488 submission_runner.py:408] Time since start: 52824.66s, 	Step: 150368, 	{'train/accuracy': 0.9048748016357422, 'train/loss': 0.5630061030387878, 'validation/accuracy': 0.7488600015640259, 'validation/loss': 1.1980425119400024, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.820862054824829, 'test/num_examples': 10000, 'score': 51042.27467918396, 'total_duration': 52824.65858411789, 'accumulated_submission_time': 51042.27467918396, 'accumulated_eval_time': 1772.8417789936066, 'accumulated_logging_time': 4.267144680023193}
I0128 03:50:55.486348 139865257662208 logging_writer.py:48] [150368] accumulated_eval_time=1772.841779, accumulated_logging_time=4.267145, accumulated_submission_time=51042.274679, global_step=150368, preemption_count=0, score=51042.274679, test/accuracy=0.628600, test/loss=1.820862, test/num_examples=10000, total_duration=52824.658584, train/accuracy=0.904875, train/loss=0.563006, validation/accuracy=0.748860, validation/loss=1.198043, validation/num_examples=50000
I0128 03:51:06.654578 139865266054912 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.907393217086792, loss=2.638629913330078
I0128 03:51:40.552856 139865257662208 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.7749829292297363, loss=2.6462888717651367
I0128 03:52:14.446944 139865266054912 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.696901559829712, loss=2.5829591751098633
I0128 03:52:48.430263 139865257662208 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.753948450088501, loss=2.6057136058807373
I0128 03:53:22.302553 139865266054912 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.935539960861206, loss=2.7059032917022705
I0128 03:53:56.226958 139865257662208 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.8102219104766846, loss=2.6178154945373535
I0128 03:54:30.109544 139865266054912 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.7706708908081055, loss=2.6421566009521484
I0128 03:55:04.053319 139865257662208 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.7955663204193115, loss=2.590947389602661
I0128 03:55:37.955194 139865266054912 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.7404561042785645, loss=2.6152243614196777
I0128 03:56:11.885045 139865257662208 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.9423863887786865, loss=2.6463558673858643
I0128 03:56:45.774299 139865266054912 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.5910110473632812, loss=2.5376343727111816
I0128 03:57:19.701899 139865257662208 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.868194818496704, loss=2.5911526679992676
I0128 03:57:53.575399 139865266054912 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.6841230392456055, loss=2.596656084060669
I0128 03:58:27.520352 139865257662208 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.7350497245788574, loss=2.672807455062866
I0128 03:59:01.480514 139865266054912 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.735325813293457, loss=2.6014628410339355
I0128 03:59:25.688238 140027215431488 spec.py:321] Evaluating on the training split.
I0128 03:59:31.924279 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 03:59:40.585443 140027215431488 spec.py:349] Evaluating on the test split.
I0128 03:59:43.104997 140027215431488 submission_runner.py:408] Time since start: 53352.32s, 	Step: 151873, 	{'train/accuracy': 0.9094387292861938, 'train/loss': 0.5636129379272461, 'validation/accuracy': 0.7510600090026855, 'validation/loss': 1.2021054029464722, 'validation/num_examples': 50000, 'test/accuracy': 0.6339000463485718, 'test/loss': 1.816255807876587, 'test/num_examples': 10000, 'score': 51552.41488814354, 'total_duration': 53352.32368469238, 'accumulated_submission_time': 51552.41488814354, 'accumulated_eval_time': 1790.2585053443909, 'accumulated_logging_time': 4.323015451431274}
I0128 03:59:43.148524 139866180368128 logging_writer.py:48] [151873] accumulated_eval_time=1790.258505, accumulated_logging_time=4.323015, accumulated_submission_time=51552.414888, global_step=151873, preemption_count=0, score=51552.414888, test/accuracy=0.633900, test/loss=1.816256, test/num_examples=10000, total_duration=53352.323685, train/accuracy=0.909439, train/loss=0.563613, validation/accuracy=0.751060, validation/loss=1.202105, validation/num_examples=50000
I0128 03:59:52.649812 139866188760832 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.635446786880493, loss=2.624680995941162
I0128 04:00:26.559056 139866180368128 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.921478509902954, loss=2.579596757888794
I0128 04:01:00.432172 139866188760832 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.5694119930267334, loss=2.5721585750579834
I0128 04:01:34.345956 139866180368128 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.5566413402557373, loss=2.5886900424957275
I0128 04:02:08.241144 139866188760832 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.009823799133301, loss=2.61741042137146
I0128 04:02:42.189037 139866180368128 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.8171072006225586, loss=2.5695621967315674
I0128 04:03:16.076908 139866188760832 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.8826987743377686, loss=2.6496360301971436
I0128 04:03:50.008485 139866180368128 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.053455829620361, loss=2.652008056640625
I0128 04:04:23.876752 139866188760832 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.8732714653015137, loss=2.685235023498535
I0128 04:04:57.841668 139866180368128 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.7822024822235107, loss=2.6442410945892334
I0128 04:05:31.762449 139866188760832 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.980790138244629, loss=2.614330768585205
I0128 04:06:05.683385 139866180368128 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.977006435394287, loss=2.575223207473755
I0128 04:06:39.594724 139866188760832 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.5747742652893066, loss=2.5766115188598633
I0128 04:07:13.541332 139866180368128 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.804536819458008, loss=2.6060385704040527
I0128 04:07:47.484347 139866188760832 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.7258853912353516, loss=2.5712783336639404
I0128 04:08:13.396631 140027215431488 spec.py:321] Evaluating on the training split.
I0128 04:08:19.571514 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 04:08:28.310273 140027215431488 spec.py:349] Evaluating on the test split.
I0128 04:08:30.902231 140027215431488 submission_runner.py:408] Time since start: 53880.12s, 	Step: 153378, 	{'train/accuracy': 0.9103156924247742, 'train/loss': 0.563623309135437, 'validation/accuracy': 0.7524799704551697, 'validation/loss': 1.2048771381378174, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.829545259475708, 'test/num_examples': 10000, 'score': 52062.60071802139, 'total_duration': 53880.12092804909, 'accumulated_submission_time': 52062.60071802139, 'accumulated_eval_time': 1807.7640812397003, 'accumulated_logging_time': 4.375983238220215}
I0128 04:08:30.943503 139865266054912 logging_writer.py:48] [153378] accumulated_eval_time=1807.764081, accumulated_logging_time=4.375983, accumulated_submission_time=52062.600718, global_step=153378, preemption_count=0, score=52062.600718, test/accuracy=0.630500, test/loss=1.829545, test/num_examples=10000, total_duration=53880.120928, train/accuracy=0.910316, train/loss=0.563623, validation/accuracy=0.752480, validation/loss=1.204877, validation/num_examples=50000
I0128 04:08:38.762294 139865760950016 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.050808906555176, loss=2.642735481262207
I0128 04:09:12.631657 139865266054912 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.997692584991455, loss=2.642026901245117
I0128 04:09:46.497312 139865760950016 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.012125015258789, loss=2.6324455738067627
I0128 04:10:20.393038 139865266054912 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.7840609550476074, loss=2.586773157119751
I0128 04:10:54.294074 139865760950016 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.7267229557037354, loss=2.575279712677002
I0128 04:11:28.377609 139865266054912 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.808264970779419, loss=2.583962917327881
I0128 04:12:02.299907 139865760950016 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.9203803539276123, loss=2.6209850311279297
I0128 04:12:36.210630 139865266054912 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.8712234497070312, loss=2.5686938762664795
I0128 04:13:10.143383 139865760950016 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.9470374584198, loss=2.6700692176818848
I0128 04:13:44.066889 139865266054912 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.664137601852417, loss=2.557267189025879
I0128 04:14:17.969865 139865760950016 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.746081829071045, loss=2.5852999687194824
I0128 04:14:51.891580 139865266054912 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.9148104190826416, loss=2.596322536468506
I0128 04:15:25.782776 139865760950016 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.691136121749878, loss=2.6164801120758057
I0128 04:15:59.698077 139865266054912 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.926190137863159, loss=2.5853426456451416
I0128 04:16:33.624065 139865760950016 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.8668458461761475, loss=2.5892574787139893
I0128 04:17:01.236517 140027215431488 spec.py:321] Evaluating on the training split.
I0128 04:17:07.432613 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 04:17:16.348194 140027215431488 spec.py:349] Evaluating on the test split.
I0128 04:17:18.736959 140027215431488 submission_runner.py:408] Time since start: 54407.96s, 	Step: 154883, 	{'train/accuracy': 0.9117307066917419, 'train/loss': 0.5565671324729919, 'validation/accuracy': 0.752020001411438, 'validation/loss': 1.2094497680664062, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.829664707183838, 'test/num_examples': 10000, 'score': 52572.8312189579, 'total_duration': 54407.95564079285, 'accumulated_submission_time': 52572.8312189579, 'accumulated_eval_time': 1825.2644836902618, 'accumulated_logging_time': 4.426474571228027}
I0128 04:17:18.782646 139866171975424 logging_writer.py:48] [154883] accumulated_eval_time=1825.264484, accumulated_logging_time=4.426475, accumulated_submission_time=52572.831219, global_step=154883, preemption_count=0, score=52572.831219, test/accuracy=0.634100, test/loss=1.829665, test/num_examples=10000, total_duration=54407.955641, train/accuracy=0.911731, train/loss=0.556567, validation/accuracy=0.752020, validation/loss=1.209450, validation/num_examples=50000
I0128 04:17:24.883218 139866180368128 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.735095977783203, loss=2.5935003757476807
I0128 04:17:58.940593 139866171975424 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.8334670066833496, loss=2.644702911376953
I0128 04:18:32.800179 139866180368128 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.782748222351074, loss=2.6360273361206055
I0128 04:19:06.690470 139866171975424 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.408846378326416, loss=2.6492950916290283
I0128 04:19:40.602360 139866180368128 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.5963425636291504, loss=2.6079070568084717
I0128 04:20:14.496448 139866171975424 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.067263126373291, loss=2.6638355255126953
I0128 04:20:48.416553 139866180368128 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.7019224166870117, loss=2.592484474182129
I0128 04:21:22.340360 139866171975424 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.96024227142334, loss=2.614966869354248
I0128 04:21:56.219893 139866180368128 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.8217813968658447, loss=2.6363327503204346
I0128 04:22:30.153254 139866171975424 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.740236282348633, loss=2.6048004627227783
I0128 04:23:04.076814 139866180368128 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.9223108291625977, loss=2.6516852378845215
I0128 04:23:37.996229 139866171975424 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.8071324825286865, loss=2.555143117904663
I0128 04:24:12.004612 139866180368128 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.592665910720825, loss=2.6013216972351074
I0128 04:24:45.906967 139866171975424 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.774495840072632, loss=2.6599483489990234
I0128 04:25:19.811285 139866180368128 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.7549562454223633, loss=2.585853099822998
I0128 04:25:48.797479 140027215431488 spec.py:321] Evaluating on the training split.
I0128 04:25:54.939287 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 04:26:04.014659 140027215431488 spec.py:349] Evaluating on the test split.
I0128 04:26:06.561693 140027215431488 submission_runner.py:408] Time since start: 54935.78s, 	Step: 156387, 	{'train/accuracy': 0.9270368218421936, 'train/loss': 0.5088555812835693, 'validation/accuracy': 0.7543599605560303, 'validation/loss': 1.2003954648971558, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.8163809776306152, 'test/num_examples': 10000, 'score': 53082.78435873985, 'total_duration': 54935.78036189079, 'accumulated_submission_time': 53082.78435873985, 'accumulated_eval_time': 1843.0286478996277, 'accumulated_logging_time': 4.481486082077026}
I0128 04:26:06.609525 139865274447616 logging_writer.py:48] [156387] accumulated_eval_time=1843.028648, accumulated_logging_time=4.481486, accumulated_submission_time=53082.784359, global_step=156387, preemption_count=0, score=53082.784359, test/accuracy=0.636700, test/loss=1.816381, test/num_examples=10000, total_duration=54935.780362, train/accuracy=0.927037, train/loss=0.508856, validation/accuracy=0.754360, validation/loss=1.200395, validation/num_examples=50000
I0128 04:26:11.360532 139865760950016 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.7139787673950195, loss=2.5695431232452393
I0128 04:26:45.228625 139865274447616 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.758824586868286, loss=2.6242287158966064
I0128 04:27:19.135675 139865760950016 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.686558723449707, loss=2.5362837314605713
I0128 04:27:53.012743 139865274447616 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.963036060333252, loss=2.5909650325775146
I0128 04:28:26.918452 139865760950016 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.91538143157959, loss=2.5971148014068604
I0128 04:29:00.811196 139865274447616 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.746291160583496, loss=2.5432260036468506
I0128 04:29:34.693106 139865760950016 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.858184814453125, loss=2.534339427947998
I0128 04:30:08.661021 139865274447616 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.788045644760132, loss=2.557486057281494
I0128 04:30:42.589060 139865760950016 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.214847564697266, loss=2.649820566177368
I0128 04:31:16.483893 139865274447616 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.7175488471984863, loss=2.5800623893737793
I0128 04:31:50.401528 139865760950016 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.048166751861572, loss=2.6204848289489746
I0128 04:32:24.346830 139865274447616 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.011643409729004, loss=2.5847535133361816
I0128 04:32:58.268105 139865760950016 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.315316200256348, loss=2.6094107627868652
I0128 04:33:32.182362 139865274447616 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.8042571544647217, loss=2.5709636211395264
I0128 04:34:06.093478 139865760950016 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.9286584854125977, loss=2.6303720474243164
I0128 04:34:36.765218 140027215431488 spec.py:321] Evaluating on the training split.
I0128 04:34:43.501663 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 04:34:52.611134 140027215431488 spec.py:349] Evaluating on the test split.
I0128 04:34:55.123670 140027215431488 submission_runner.py:408] Time since start: 55464.34s, 	Step: 157892, 	{'train/accuracy': 0.9226323366165161, 'train/loss': 0.5026780366897583, 'validation/accuracy': 0.7541999816894531, 'validation/loss': 1.1845531463623047, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.8125512599945068, 'test/num_examples': 10000, 'score': 53592.87838935852, 'total_duration': 55464.342358112335, 'accumulated_submission_time': 53592.87838935852, 'accumulated_eval_time': 1861.3870635032654, 'accumulated_logging_time': 4.539382696151733}
I0128 04:34:55.172301 139866171975424 logging_writer.py:48] [157892] accumulated_eval_time=1861.387064, accumulated_logging_time=4.539383, accumulated_submission_time=53592.878389, global_step=157892, preemption_count=0, score=53592.878389, test/accuracy=0.632600, test/loss=1.812551, test/num_examples=10000, total_duration=55464.342358, train/accuracy=0.922632, train/loss=0.502678, validation/accuracy=0.754200, validation/loss=1.184553, validation/num_examples=50000
I0128 04:34:58.233201 139866180368128 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.261435031890869, loss=2.5864744186401367
I0128 04:35:32.126959 139866171975424 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.982149362564087, loss=2.598034620285034
I0128 04:36:05.991159 139866180368128 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.121345520019531, loss=2.5849506855010986
I0128 04:36:40.080152 139866171975424 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.9579553604125977, loss=2.5744097232818604
I0128 04:37:13.997910 139866180368128 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.683445930480957, loss=2.5639736652374268
I0128 04:37:47.934931 139866171975424 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.8816957473754883, loss=2.54080867767334
I0128 04:38:21.852267 139866180368128 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.026844024658203, loss=2.6321234703063965
I0128 04:38:55.752194 139866171975424 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.164649963378906, loss=2.576371908187866
I0128 04:39:29.663432 139866180368128 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.9541049003601074, loss=2.6245431900024414
I0128 04:40:03.573456 139866171975424 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.8287956714630127, loss=2.558018684387207
I0128 04:40:37.499141 139866180368128 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.050241470336914, loss=2.6042490005493164
I0128 04:41:11.404025 139866171975424 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.891031265258789, loss=2.560579538345337
I0128 04:41:45.306498 139866180368128 logging_writer.py:48] [159100] global_step=159100, grad_norm=3.9243791103363037, loss=2.581144332885742
I0128 04:42:19.220049 139866171975424 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.9870429039001465, loss=2.555767774581909
I0128 04:42:53.308244 139866180368128 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.022857666015625, loss=2.5440480709075928
I0128 04:43:25.343960 140027215431488 spec.py:321] Evaluating on the training split.
I0128 04:43:31.466741 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 04:43:40.281285 140027215431488 spec.py:349] Evaluating on the test split.
I0128 04:43:42.759948 140027215431488 submission_runner.py:408] Time since start: 55991.98s, 	Step: 159396, 	{'train/accuracy': 0.9216158986091614, 'train/loss': 0.5223816633224487, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.198951244354248, 'validation/num_examples': 50000, 'test/accuracy': 0.64000004529953, 'test/loss': 1.8155686855316162, 'test/num_examples': 10000, 'score': 54102.98790359497, 'total_duration': 55991.9786362648, 'accumulated_submission_time': 54102.98790359497, 'accumulated_eval_time': 1878.803019285202, 'accumulated_logging_time': 4.597425699234009}
I0128 04:43:42.804127 139865257662208 logging_writer.py:48] [159396] accumulated_eval_time=1878.803019, accumulated_logging_time=4.597426, accumulated_submission_time=54102.987904, global_step=159396, preemption_count=0, score=54102.987904, test/accuracy=0.640000, test/loss=1.815569, test/num_examples=10000, total_duration=55991.978636, train/accuracy=0.921616, train/loss=0.522382, validation/accuracy=0.755860, validation/loss=1.198951, validation/num_examples=50000
I0128 04:43:44.504034 139865266054912 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.9183003902435303, loss=2.564379930496216
I0128 04:44:18.397538 139865257662208 logging_writer.py:48] [159500] global_step=159500, grad_norm=3.9738526344299316, loss=2.5227108001708984
I0128 04:44:52.263986 139865266054912 logging_writer.py:48] [159600] global_step=159600, grad_norm=3.767033815383911, loss=2.605515956878662
I0128 04:45:26.194789 139865257662208 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.447871208190918, loss=2.5701682567596436
I0128 04:46:00.090738 139865266054912 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.726682662963867, loss=2.5824625492095947
I0128 04:46:34.029433 139865257662208 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.8507165908813477, loss=2.564601421356201
I0128 04:47:07.928501 139865266054912 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.018393516540527, loss=2.5334525108337402
I0128 04:47:41.864727 139865257662208 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.933720350265503, loss=2.5435705184936523
I0128 04:48:15.753243 139865266054912 logging_writer.py:48] [160200] global_step=160200, grad_norm=3.654768466949463, loss=2.553820848464966
I0128 04:48:49.661569 139865257662208 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.6998889446258545, loss=2.524792432785034
I0128 04:49:23.745738 139865266054912 logging_writer.py:48] [160400] global_step=160400, grad_norm=3.7971580028533936, loss=2.5318782329559326
I0128 04:49:57.638873 139865257662208 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.08646297454834, loss=2.5890696048736572
I0128 04:50:31.549818 139865266054912 logging_writer.py:48] [160600] global_step=160600, grad_norm=3.6749114990234375, loss=2.5090789794921875
I0128 04:51:05.466947 139865257662208 logging_writer.py:48] [160700] global_step=160700, grad_norm=3.929344892501831, loss=2.642199993133545
I0128 04:51:39.375159 139865266054912 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.04679012298584, loss=2.6363015174865723
I0128 04:52:12.763769 140027215431488 spec.py:321] Evaluating on the training split.
I0128 04:52:19.039764 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 04:52:27.673736 140027215431488 spec.py:349] Evaluating on the test split.
I0128 04:52:30.279126 140027215431488 submission_runner.py:408] Time since start: 56519.50s, 	Step: 160900, 	{'train/accuracy': 0.9227519035339355, 'train/loss': 0.5238286256790161, 'validation/accuracy': 0.7569599747657776, 'validation/loss': 1.1966742277145386, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.8186490535736084, 'test/num_examples': 10000, 'score': 54612.884423971176, 'total_duration': 56519.49779844284, 'accumulated_submission_time': 54612.884423971176, 'accumulated_eval_time': 1896.3183450698853, 'accumulated_logging_time': 4.652040481567383}
I0128 04:52:30.335099 139865266054912 logging_writer.py:48] [160900] accumulated_eval_time=1896.318345, accumulated_logging_time=4.652040, accumulated_submission_time=54612.884424, global_step=160900, preemption_count=0, score=54612.884424, test/accuracy=0.638300, test/loss=1.818649, test/num_examples=10000, total_duration=56519.497798, train/accuracy=0.922752, train/loss=0.523829, validation/accuracy=0.756960, validation/loss=1.196674, validation/num_examples=50000
I0128 04:52:30.708054 139865769342720 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.860680103302002, loss=2.5701286792755127
I0128 04:53:04.613936 139865266054912 logging_writer.py:48] [161000] global_step=161000, grad_norm=3.9465692043304443, loss=2.5814645290374756
I0128 04:53:38.500198 139865769342720 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.3108811378479, loss=2.661522150039673
I0128 04:54:12.411727 139865266054912 logging_writer.py:48] [161200] global_step=161200, grad_norm=3.9194176197052, loss=2.5676145553588867
I0128 04:54:46.277158 139865769342720 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.8448667526245117, loss=2.576103448867798
I0128 04:55:20.301436 139865266054912 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.8915252685546875, loss=2.556087017059326
I0128 04:55:54.196703 139865769342720 logging_writer.py:48] [161500] global_step=161500, grad_norm=3.987859010696411, loss=2.6083149909973145
I0128 04:56:28.084313 139865266054912 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.080267429351807, loss=2.5994389057159424
I0128 04:57:02.018559 139865769342720 logging_writer.py:48] [161700] global_step=161700, grad_norm=3.7898366451263428, loss=2.535933017730713
I0128 04:57:35.953499 139865266054912 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.17943811416626, loss=2.5573031902313232
I0128 04:58:09.815353 139865769342720 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.055576801300049, loss=2.567594051361084
I0128 04:58:43.746552 139865266054912 logging_writer.py:48] [162000] global_step=162000, grad_norm=3.884876251220703, loss=2.5676674842834473
I0128 04:59:17.607215 139865769342720 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.9237053394317627, loss=2.5989480018615723
I0128 04:59:51.519855 139865266054912 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.7251007556915283, loss=2.5400550365448
I0128 05:00:25.411482 139865769342720 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.17347526550293, loss=2.6632533073425293
I0128 05:00:59.331073 139865266054912 logging_writer.py:48] [162400] global_step=162400, grad_norm=3.9835925102233887, loss=2.628040313720703
I0128 05:01:00.490280 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:01:06.634949 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:01:15.514349 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:01:17.989550 140027215431488 submission_runner.py:408] Time since start: 57047.21s, 	Step: 162405, 	{'train/accuracy': 0.9255221486091614, 'train/loss': 0.5173733234405518, 'validation/accuracy': 0.7567399740219116, 'validation/loss': 1.1949071884155273, 'validation/num_examples': 50000, 'test/accuracy': 0.6376000046730042, 'test/loss': 1.8151713609695435, 'test/num_examples': 10000, 'score': 55122.97531867027, 'total_duration': 57047.208235025406, 'accumulated_submission_time': 55122.97531867027, 'accumulated_eval_time': 1913.817587852478, 'accumulated_logging_time': 4.720116376876831}
I0128 05:01:18.038972 139865266054912 logging_writer.py:48] [162405] accumulated_eval_time=1913.817588, accumulated_logging_time=4.720116, accumulated_submission_time=55122.975319, global_step=162405, preemption_count=0, score=55122.975319, test/accuracy=0.637600, test/loss=1.815171, test/num_examples=10000, total_duration=57047.208235, train/accuracy=0.925522, train/loss=0.517373, validation/accuracy=0.756740, validation/loss=1.194907, validation/num_examples=50000
I0128 05:01:50.655576 139866171975424 logging_writer.py:48] [162500] global_step=162500, grad_norm=3.91815185546875, loss=2.5766375064849854
I0128 05:02:24.554601 139865266054912 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.201352596282959, loss=2.576505661010742
I0128 05:02:58.426153 139866171975424 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.011056423187256, loss=2.629554271697998
I0128 05:03:32.340135 139865266054912 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.5785603523254395, loss=2.5300443172454834
I0128 05:04:06.226481 139866171975424 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.160084247589111, loss=2.5617594718933105
I0128 05:04:40.152916 139865266054912 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.020742893218994, loss=2.5683860778808594
I0128 05:05:14.038948 139866171975424 logging_writer.py:48] [163100] global_step=163100, grad_norm=3.9978461265563965, loss=2.536802053451538
I0128 05:05:47.974819 139865266054912 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.090826988220215, loss=2.567110061645508
I0128 05:06:21.901684 139866171975424 logging_writer.py:48] [163300] global_step=163300, grad_norm=3.8215250968933105, loss=2.5544633865356445
I0128 05:06:55.798033 139865266054912 logging_writer.py:48] [163400] global_step=163400, grad_norm=3.7676138877868652, loss=2.5721349716186523
I0128 05:07:29.723139 139866171975424 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.0254740715026855, loss=2.6181938648223877
I0128 05:08:03.716519 139865266054912 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.190961837768555, loss=2.5965824127197266
I0128 05:08:37.591046 139866171975424 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.995452880859375, loss=2.549205780029297
I0128 05:09:11.508296 139865266054912 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.9405412673950195, loss=2.558286428451538
I0128 05:09:45.437915 139866171975424 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.814466714859009, loss=2.569641590118408
I0128 05:09:48.286080 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:09:54.434273 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:10:03.423358 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:10:05.918158 140027215431488 submission_runner.py:408] Time since start: 57575.14s, 	Step: 163910, 	{'train/accuracy': 0.9263990521430969, 'train/loss': 0.5073674917221069, 'validation/accuracy': 0.7562999725341797, 'validation/loss': 1.1926721334457397, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.8144419193267822, 'test/num_examples': 10000, 'score': 55633.16078686714, 'total_duration': 57575.13672566414, 'accumulated_submission_time': 55633.16078686714, 'accumulated_eval_time': 1931.4495086669922, 'accumulated_logging_time': 4.778955459594727}
I0128 05:10:05.963686 139865266054912 logging_writer.py:48] [163910] accumulated_eval_time=1931.449509, accumulated_logging_time=4.778955, accumulated_submission_time=55633.160787, global_step=163910, preemption_count=0, score=55633.160787, test/accuracy=0.638100, test/loss=1.814442, test/num_examples=10000, total_duration=57575.136726, train/accuracy=0.926399, train/loss=0.507367, validation/accuracy=0.756300, validation/loss=1.192672, validation/num_examples=50000
I0128 05:10:36.769233 139865769342720 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.8382341861724854, loss=2.565054416656494
I0128 05:11:10.625878 139865266054912 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.982957601547241, loss=2.56234073638916
I0128 05:11:44.553747 139865769342720 logging_writer.py:48] [164200] global_step=164200, grad_norm=3.8959758281707764, loss=2.5504937171936035
I0128 05:12:18.488162 139865266054912 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.9618074893951416, loss=2.5704638957977295
I0128 05:12:52.387677 139865769342720 logging_writer.py:48] [164400] global_step=164400, grad_norm=3.9454526901245117, loss=2.5959599018096924
I0128 05:13:26.311080 139865266054912 logging_writer.py:48] [164500] global_step=164500, grad_norm=3.7263808250427246, loss=2.5621345043182373
I0128 05:14:00.195416 139865769342720 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.023009300231934, loss=2.5495738983154297
I0128 05:14:34.295056 139865266054912 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.0147786140441895, loss=2.5379600524902344
I0128 05:15:08.189691 139865769342720 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.304225921630859, loss=2.568422555923462
I0128 05:15:42.112382 139865266054912 logging_writer.py:48] [164900] global_step=164900, grad_norm=3.846491813659668, loss=2.5458216667175293
I0128 05:16:16.035514 139865769342720 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.494838237762451, loss=2.5447254180908203
I0128 05:16:49.922670 139865266054912 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.0097270011901855, loss=2.5812478065490723
I0128 05:17:23.818891 139865769342720 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.046850204467773, loss=2.523522138595581
I0128 05:17:57.719346 139865266054912 logging_writer.py:48] [165300] global_step=165300, grad_norm=3.982769250869751, loss=2.5528485774993896
I0128 05:18:31.625478 139865769342720 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.017971992492676, loss=2.5653841495513916
I0128 05:18:36.186701 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:18:42.307844 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:18:51.164670 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:18:53.582248 140027215431488 submission_runner.py:408] Time since start: 58102.80s, 	Step: 165415, 	{'train/accuracy': 0.93558669090271, 'train/loss': 0.4720862805843353, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.1844284534454346, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.8039889335632324, 'test/num_examples': 10000, 'score': 56143.32125544548, 'total_duration': 58102.80092835426, 'accumulated_submission_time': 56143.32125544548, 'accumulated_eval_time': 1948.845008611679, 'accumulated_logging_time': 4.834153413772583}
I0128 05:18:53.632778 139866171975424 logging_writer.py:48] [165415] accumulated_eval_time=1948.845009, accumulated_logging_time=4.834153, accumulated_submission_time=56143.321255, global_step=165415, preemption_count=0, score=56143.321255, test/accuracy=0.637000, test/loss=1.803989, test/num_examples=10000, total_duration=58102.800928, train/accuracy=0.935587, train/loss=0.472086, validation/accuracy=0.757040, validation/loss=1.184428, validation/num_examples=50000
I0128 05:19:22.782575 139866180368128 logging_writer.py:48] [165500] global_step=165500, grad_norm=3.9695749282836914, loss=2.5579867362976074
I0128 05:19:56.683451 139866171975424 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.979846954345703, loss=2.548273801803589
I0128 05:20:30.696216 139866180368128 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.907318353652954, loss=2.5524637699127197
I0128 05:21:04.614166 139866171975424 logging_writer.py:48] [165800] global_step=165800, grad_norm=3.783931255340576, loss=2.544269323348999
I0128 05:21:38.506636 139866180368128 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.961074113845825, loss=2.504476547241211
I0128 05:22:12.434952 139866171975424 logging_writer.py:48] [166000] global_step=166000, grad_norm=3.889800548553467, loss=2.4887640476226807
I0128 05:22:46.372491 139866180368128 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.953530788421631, loss=2.485678195953369
I0128 05:23:20.267809 139866171975424 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.9296109676361084, loss=2.5566132068634033
I0128 05:23:54.225720 139866180368128 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.151726245880127, loss=2.5568642616271973
I0128 05:24:28.125206 139866171975424 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.156294345855713, loss=2.548487424850464
I0128 05:25:02.048229 139866180368128 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.747601270675659, loss=2.508528470993042
I0128 05:25:35.985768 139866171975424 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.061933517456055, loss=2.5416133403778076
I0128 05:26:09.889376 139866180368128 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.091070652008057, loss=2.576444387435913
I0128 05:26:43.844080 139866171975424 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.042382717132568, loss=2.5217509269714355
I0128 05:27:17.776434 139866180368128 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.1247053146362305, loss=2.514878273010254
I0128 05:27:23.691422 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:27:29.811360 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:27:38.545305 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:27:41.022833 140027215431488 submission_runner.py:408] Time since start: 58630.24s, 	Step: 166919, 	{'train/accuracy': 0.9346898794174194, 'train/loss': 0.47572416067123413, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.1815327405929565, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.7958463430404663, 'test/num_examples': 10000, 'score': 56653.3142747879, 'total_duration': 58630.24149942398, 'accumulated_submission_time': 56653.3142747879, 'accumulated_eval_time': 1966.1763620376587, 'accumulated_logging_time': 4.896468162536621}
I0128 05:27:41.070556 139865257662208 logging_writer.py:48] [166919] accumulated_eval_time=1966.176362, accumulated_logging_time=4.896468, accumulated_submission_time=56653.314275, global_step=166919, preemption_count=0, score=56653.314275, test/accuracy=0.640500, test/loss=1.795846, test/num_examples=10000, total_duration=58630.241499, train/accuracy=0.934690, train/loss=0.475724, validation/accuracy=0.759880, validation/loss=1.181533, validation/num_examples=50000
I0128 05:28:08.860405 139865266054912 logging_writer.py:48] [167000] global_step=167000, grad_norm=3.9634532928466797, loss=2.5110814571380615
I0128 05:28:42.771851 139865257662208 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.010716438293457, loss=2.5511536598205566
I0128 05:29:16.640917 139865266054912 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.452412128448486, loss=2.580702781677246
I0128 05:29:50.563548 139865257662208 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.676382303237915, loss=2.5484044551849365
I0128 05:30:24.473754 139865266054912 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.97074294090271, loss=2.527827739715576
I0128 05:30:58.384891 139865257662208 logging_writer.py:48] [167500] global_step=167500, grad_norm=3.7994139194488525, loss=2.534193992614746
I0128 05:31:32.277743 139865266054912 logging_writer.py:48] [167600] global_step=167600, grad_norm=3.7184340953826904, loss=2.5143842697143555
I0128 05:32:06.198161 139865257662208 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.347668170928955, loss=2.5669825077056885
I0128 05:32:40.078759 139865266054912 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.9396331310272217, loss=2.526904582977295
I0128 05:33:14.106158 139865257662208 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.17985725402832, loss=2.5347325801849365
I0128 05:33:48.019063 139865266054912 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.094235420227051, loss=2.5105443000793457
I0128 05:34:21.942571 139865257662208 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.09187650680542, loss=2.5379767417907715
I0128 05:34:55.838531 139865266054912 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.975433111190796, loss=2.519531011581421
I0128 05:35:29.761402 139865257662208 logging_writer.py:48] [168300] global_step=168300, grad_norm=3.6753146648406982, loss=2.4892892837524414
I0128 05:36:03.677186 139865266054912 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.704681396484375, loss=2.569692373275757
I0128 05:36:11.288693 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:36:17.424901 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:36:26.086581 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:36:28.579949 140027215431488 submission_runner.py:408] Time since start: 59157.80s, 	Step: 168424, 	{'train/accuracy': 0.9356863498687744, 'train/loss': 0.46207720041275024, 'validation/accuracy': 0.7594199776649475, 'validation/loss': 1.1747729778289795, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.7989342212677002, 'test/num_examples': 10000, 'score': 57163.469307899475, 'total_duration': 59157.79861664772, 'accumulated_submission_time': 57163.469307899475, 'accumulated_eval_time': 1983.467565536499, 'accumulated_logging_time': 4.954056978225708}
I0128 05:36:28.630652 139865266054912 logging_writer.py:48] [168424] accumulated_eval_time=1983.467566, accumulated_logging_time=4.954057, accumulated_submission_time=57163.469308, global_step=168424, preemption_count=0, score=57163.469308, test/accuracy=0.640200, test/loss=1.798934, test/num_examples=10000, total_duration=59157.798617, train/accuracy=0.935686, train/loss=0.462077, validation/accuracy=0.759420, validation/loss=1.174773, validation/num_examples=50000
I0128 05:36:54.735839 139866171975424 logging_writer.py:48] [168500] global_step=168500, grad_norm=3.8126144409179688, loss=2.5197157859802246
I0128 05:37:28.602766 139865266054912 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.066556930541992, loss=2.5557782649993896
I0128 05:38:02.534059 139866171975424 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.083718776702881, loss=2.5474324226379395
I0128 05:38:36.417765 139865266054912 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.3029351234436035, loss=2.5794997215270996
I0128 05:39:10.405025 139866171975424 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.8376622200012207, loss=2.5190813541412354
I0128 05:39:44.362199 139865266054912 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.795886516571045, loss=2.544419288635254
I0128 05:40:18.266598 139866171975424 logging_writer.py:48] [169100] global_step=169100, grad_norm=3.795943260192871, loss=2.4685609340667725
I0128 05:40:52.198312 139865266054912 logging_writer.py:48] [169200] global_step=169200, grad_norm=3.9363739490509033, loss=2.5177202224731445
I0128 05:41:26.136212 139866171975424 logging_writer.py:48] [169300] global_step=169300, grad_norm=3.8969531059265137, loss=2.5321011543273926
I0128 05:42:00.041159 139865266054912 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.342879772186279, loss=2.623732566833496
I0128 05:42:33.963502 139866171975424 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.150388240814209, loss=2.5228612422943115
I0128 05:43:07.853342 139865266054912 logging_writer.py:48] [169600] global_step=169600, grad_norm=3.9870591163635254, loss=2.549912214279175
I0128 05:43:41.775382 139866171975424 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.12119197845459, loss=2.5611677169799805
I0128 05:44:15.650471 139865266054912 logging_writer.py:48] [169800] global_step=169800, grad_norm=3.7348263263702393, loss=2.5036239624023438
I0128 05:44:49.585767 139866171975424 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.243319511413574, loss=2.607072591781616
I0128 05:44:58.884608 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:45:05.000545 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:45:13.939482 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:45:16.408481 140027215431488 submission_runner.py:408] Time since start: 59685.63s, 	Step: 169929, 	{'train/accuracy': 0.9359853267669678, 'train/loss': 0.4732051193714142, 'validation/accuracy': 0.7616399526596069, 'validation/loss': 1.1783033609390259, 'validation/num_examples': 50000, 'test/accuracy': 0.6430000066757202, 'test/loss': 1.7972735166549683, 'test/num_examples': 10000, 'score': 57673.66196155548, 'total_duration': 59685.62716174126, 'accumulated_submission_time': 57673.66196155548, 'accumulated_eval_time': 2000.991406917572, 'accumulated_logging_time': 5.0141987800598145}
I0128 05:45:16.459242 139865266054912 logging_writer.py:48] [169929] accumulated_eval_time=2000.991407, accumulated_logging_time=5.014199, accumulated_submission_time=57673.661962, global_step=169929, preemption_count=0, score=57673.661962, test/accuracy=0.643000, test/loss=1.797274, test/num_examples=10000, total_duration=59685.627162, train/accuracy=0.935985, train/loss=0.473205, validation/accuracy=0.761640, validation/loss=1.178303, validation/num_examples=50000
I0128 05:45:40.921674 139865274447616 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.024861812591553, loss=2.5617427825927734
I0128 05:46:14.860675 139865266054912 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.3835577964782715, loss=2.5055251121520996
I0128 05:46:48.747542 139865274447616 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.197388172149658, loss=2.5313048362731934
I0128 05:47:22.681108 139865266054912 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.342895984649658, loss=2.568337917327881
I0128 05:47:56.583520 139865274447616 logging_writer.py:48] [170400] global_step=170400, grad_norm=3.9488563537597656, loss=2.536039352416992
I0128 05:48:30.509084 139865266054912 logging_writer.py:48] [170500] global_step=170500, grad_norm=3.9783079624176025, loss=2.548192262649536
I0128 05:49:04.402414 139865274447616 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.103409767150879, loss=2.566917896270752
I0128 05:49:38.323102 139865266054912 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.043911933898926, loss=2.520565986633301
I0128 05:50:12.223416 139865274447616 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.094449043273926, loss=2.5182814598083496
I0128 05:50:46.150757 139865266054912 logging_writer.py:48] [170900] global_step=170900, grad_norm=3.8987085819244385, loss=2.519559144973755
I0128 05:51:20.033853 139865274447616 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.190369129180908, loss=2.4992945194244385
I0128 05:51:54.019778 139865266054912 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.157052516937256, loss=2.53605055809021
I0128 05:52:27.940945 139865274447616 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.991542339324951, loss=2.5024561882019043
I0128 05:53:01.808215 139865266054912 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.140524387359619, loss=2.564275026321411
I0128 05:53:35.697114 139865274447616 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.070676803588867, loss=2.470754384994507
I0128 05:53:46.714089 140027215431488 spec.py:321] Evaluating on the training split.
I0128 05:53:52.846704 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 05:54:01.633366 140027215431488 spec.py:349] Evaluating on the test split.
I0128 05:54:04.125711 140027215431488 submission_runner.py:408] Time since start: 60213.34s, 	Step: 171434, 	{'train/accuracy': 0.9338527917861938, 'train/loss': 0.47603392601013184, 'validation/accuracy': 0.7612999677658081, 'validation/loss': 1.17757248878479, 'validation/num_examples': 50000, 'test/accuracy': 0.6459000110626221, 'test/loss': 1.798004150390625, 'test/num_examples': 10000, 'score': 58183.85387516022, 'total_duration': 60213.344397068024, 'accumulated_submission_time': 58183.85387516022, 'accumulated_eval_time': 2018.4029893875122, 'accumulated_logging_time': 5.076361656188965}
I0128 05:54:04.171881 139866171975424 logging_writer.py:48] [171434] accumulated_eval_time=2018.402989, accumulated_logging_time=5.076362, accumulated_submission_time=58183.853875, global_step=171434, preemption_count=0, score=58183.853875, test/accuracy=0.645900, test/loss=1.798004, test/num_examples=10000, total_duration=60213.344397, train/accuracy=0.933853, train/loss=0.476034, validation/accuracy=0.761300, validation/loss=1.177572, validation/num_examples=50000
I0128 05:54:26.912883 139866180368128 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.085636615753174, loss=2.520773410797119
I0128 05:55:00.812227 139866171975424 logging_writer.py:48] [171600] global_step=171600, grad_norm=3.977875232696533, loss=2.5014944076538086
I0128 05:55:34.736901 139866180368128 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.110850811004639, loss=2.541487693786621
I0128 05:56:08.639626 139866171975424 logging_writer.py:48] [171800] global_step=171800, grad_norm=3.7633659839630127, loss=2.4627058506011963
I0128 05:56:42.571775 139866180368128 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.3422722816467285, loss=2.5333046913146973
I0128 05:57:16.469178 139866171975424 logging_writer.py:48] [172000] global_step=172000, grad_norm=3.935668706893921, loss=2.5344560146331787
I0128 05:57:50.401245 139866180368128 logging_writer.py:48] [172100] global_step=172100, grad_norm=3.8288185596466064, loss=2.5339818000793457
I0128 05:58:24.454291 139866171975424 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.3881449699401855, loss=2.5747716426849365
I0128 05:58:58.388504 139866180368128 logging_writer.py:48] [172300] global_step=172300, grad_norm=3.8978044986724854, loss=2.5232698917388916
I0128 05:59:32.326787 139866171975424 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.365691184997559, loss=2.5141005516052246
I0128 06:00:06.253108 139866180368128 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.170956611633301, loss=2.538134813308716
I0128 06:00:40.163611 139866171975424 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.069789886474609, loss=2.524266004562378
I0128 06:01:14.098532 139866180368128 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.151821613311768, loss=2.508331537246704
I0128 06:01:47.995457 139866171975424 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.156878471374512, loss=2.494917392730713
I0128 06:02:21.923686 139866180368128 logging_writer.py:48] [172900] global_step=172900, grad_norm=3.9694571495056152, loss=2.5371227264404297
I0128 06:02:34.301493 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:02:40.402952 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:02:49.117938 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:02:51.701343 140027215431488 submission_runner.py:408] Time since start: 60740.92s, 	Step: 172938, 	{'train/accuracy': 0.9358657598495483, 'train/loss': 0.4723958671092987, 'validation/accuracy': 0.7612400054931641, 'validation/loss': 1.1811754703521729, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.8015018701553345, 'test/num_examples': 10000, 'score': 58693.92332792282, 'total_duration': 60740.91999530792, 'accumulated_submission_time': 58693.92332792282, 'accumulated_eval_time': 2035.8027634620667, 'accumulated_logging_time': 5.131362676620483}
I0128 06:02:51.748884 139865760950016 logging_writer.py:48] [172938] accumulated_eval_time=2035.802763, accumulated_logging_time=5.131363, accumulated_submission_time=58693.923328, global_step=172938, preemption_count=0, score=58693.923328, test/accuracy=0.642100, test/loss=1.801502, test/num_examples=10000, total_duration=60740.919995, train/accuracy=0.935866, train/loss=0.472396, validation/accuracy=0.761240, validation/loss=1.181175, validation/num_examples=50000
I0128 06:03:13.115008 139865769342720 logging_writer.py:48] [173000] global_step=173000, grad_norm=3.9131643772125244, loss=2.4703526496887207
I0128 06:03:47.023968 139865760950016 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.064149379730225, loss=2.5295677185058594
I0128 06:04:21.028945 139865769342720 logging_writer.py:48] [173200] global_step=173200, grad_norm=3.956516742706299, loss=2.5371227264404297
I0128 06:04:54.934984 139865760950016 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.210657596588135, loss=2.5728814601898193
I0128 06:05:28.879393 139865769342720 logging_writer.py:48] [173400] global_step=173400, grad_norm=3.954374074935913, loss=2.4504518508911133
I0128 06:06:02.808765 139865760950016 logging_writer.py:48] [173500] global_step=173500, grad_norm=3.985147476196289, loss=2.5120208263397217
I0128 06:06:36.738065 139865769342720 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.1527910232543945, loss=2.543497085571289
I0128 06:07:10.681755 139865760950016 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.001997470855713, loss=2.538191080093384
I0128 06:07:44.574755 139865769342720 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.071163177490234, loss=2.571164608001709
I0128 06:08:18.491143 139865760950016 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.125061511993408, loss=2.567943572998047
I0128 06:08:52.370351 139865769342720 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.363936424255371, loss=2.5203909873962402
I0128 06:09:26.286952 139865760950016 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.121476650238037, loss=2.5489399433135986
I0128 06:10:00.189433 139865769342720 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.1606245040893555, loss=2.5410759449005127
I0128 06:10:34.296137 139865760950016 logging_writer.py:48] [174300] global_step=174300, grad_norm=3.8020288944244385, loss=2.5391435623168945
I0128 06:11:08.213299 139865769342720 logging_writer.py:48] [174400] global_step=174400, grad_norm=3.6199989318847656, loss=2.5010035037994385
I0128 06:11:21.911120 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:11:28.008914 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:11:36.967220 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:11:39.464756 140027215431488 submission_runner.py:408] Time since start: 61268.68s, 	Step: 174442, 	{'train/accuracy': 0.938875138759613, 'train/loss': 0.4575260877609253, 'validation/accuracy': 0.7615000009536743, 'validation/loss': 1.1745693683624268, 'validation/num_examples': 50000, 'test/accuracy': 0.6446000337600708, 'test/loss': 1.7955400943756104, 'test/num_examples': 10000, 'score': 59204.02347588539, 'total_duration': 61268.683440208435, 'accumulated_submission_time': 59204.02347588539, 'accumulated_eval_time': 2053.356356859207, 'accumulated_logging_time': 5.18861722946167}
I0128 06:11:39.513752 139865274447616 logging_writer.py:48] [174442] accumulated_eval_time=2053.356357, accumulated_logging_time=5.188617, accumulated_submission_time=59204.023476, global_step=174442, preemption_count=0, score=59204.023476, test/accuracy=0.644600, test/loss=1.795540, test/num_examples=10000, total_duration=61268.683440, train/accuracy=0.938875, train/loss=0.457526, validation/accuracy=0.761500, validation/loss=1.174569, validation/num_examples=50000
I0128 06:11:59.523319 139865760950016 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.058335781097412, loss=2.5205745697021484
I0128 06:12:33.391609 139865274447616 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.142593860626221, loss=2.5758349895477295
I0128 06:13:07.300419 139865760950016 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.246926784515381, loss=2.5705394744873047
I0128 06:13:41.190183 139865274447616 logging_writer.py:48] [174800] global_step=174800, grad_norm=3.871776580810547, loss=2.5085761547088623
I0128 06:14:15.083061 139865760950016 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.229373455047607, loss=2.5634469985961914
I0128 06:14:48.975013 139865274447616 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.069930076599121, loss=2.559252977371216
I0128 06:15:22.911428 139865760950016 logging_writer.py:48] [175100] global_step=175100, grad_norm=3.8558740615844727, loss=2.545383930206299
I0128 06:15:56.823216 139865274447616 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.128386497497559, loss=2.540398359298706
I0128 06:16:30.711972 139865760950016 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.1051812171936035, loss=2.570824146270752
I0128 06:17:04.752398 139865274447616 logging_writer.py:48] [175400] global_step=175400, grad_norm=3.89654278755188, loss=2.4728329181671143
I0128 06:17:38.681370 139865760950016 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.014023303985596, loss=2.5555787086486816
I0128 06:18:12.589883 139865274447616 logging_writer.py:48] [175600] global_step=175600, grad_norm=3.9877240657806396, loss=2.4977664947509766
I0128 06:18:46.493043 139865760950016 logging_writer.py:48] [175700] global_step=175700, grad_norm=3.953542470932007, loss=2.529738187789917
I0128 06:19:20.382219 139865274447616 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.101996898651123, loss=2.524400472640991
I0128 06:19:54.321357 139865760950016 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.031380653381348, loss=2.5831756591796875
I0128 06:20:09.705536 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:20:15.908855 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:20:24.767515 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:20:27.298679 140027215431488 submission_runner.py:408] Time since start: 61796.52s, 	Step: 175947, 	{'train/accuracy': 0.9404296875, 'train/loss': 0.45358148217201233, 'validation/accuracy': 0.7617799639701843, 'validation/loss': 1.1750295162200928, 'validation/num_examples': 50000, 'test/accuracy': 0.6451000571250916, 'test/loss': 1.7953126430511475, 'test/num_examples': 10000, 'score': 59714.15299129486, 'total_duration': 61796.51736474037, 'accumulated_submission_time': 59714.15299129486, 'accumulated_eval_time': 2070.9494745731354, 'accumulated_logging_time': 5.2475292682647705}
I0128 06:20:27.345655 139865769342720 logging_writer.py:48] [175947] accumulated_eval_time=2070.949475, accumulated_logging_time=5.247529, accumulated_submission_time=59714.152991, global_step=175947, preemption_count=0, score=59714.152991, test/accuracy=0.645100, test/loss=1.795313, test/num_examples=10000, total_duration=61796.517365, train/accuracy=0.940430, train/loss=0.453581, validation/accuracy=0.761780, validation/loss=1.175030, validation/num_examples=50000
I0128 06:20:45.667441 139866163582720 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.122122764587402, loss=2.538205146789551
I0128 06:21:19.560038 139865769342720 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.234347343444824, loss=2.5334320068359375
I0128 06:21:53.488048 139866163582720 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.106603145599365, loss=2.5501158237457275
I0128 06:22:27.406594 139865769342720 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.004727363586426, loss=2.541145086288452
I0128 06:23:01.308705 139866163582720 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.192451477050781, loss=2.565438747406006
I0128 06:23:35.316927 139865769342720 logging_writer.py:48] [176500] global_step=176500, grad_norm=3.966743230819702, loss=2.5187036991119385
I0128 06:24:09.256097 139866163582720 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.272285461425781, loss=2.511734962463379
I0128 06:24:43.188798 139865769342720 logging_writer.py:48] [176700] global_step=176700, grad_norm=3.9986469745635986, loss=2.5606980323791504
I0128 06:25:17.091919 139866163582720 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.266446113586426, loss=2.5422537326812744
I0128 06:25:51.029274 139865769342720 logging_writer.py:48] [176900] global_step=176900, grad_norm=3.9298086166381836, loss=2.546351432800293
I0128 06:26:24.917603 139866163582720 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.183681011199951, loss=2.530836582183838
I0128 06:26:58.845218 139865769342720 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.409519195556641, loss=2.5587196350097656
I0128 06:27:32.749337 139866163582720 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.04817533493042, loss=2.4821085929870605
I0128 06:28:06.674154 139865769342720 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.24920654296875, loss=2.5328071117401123
I0128 06:28:40.570529 139866163582720 logging_writer.py:48] [177400] global_step=177400, grad_norm=3.974032163619995, loss=2.493987560272217
I0128 06:28:57.331851 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:29:03.471414 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:29:12.473866 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:29:15.068760 140027215431488 submission_runner.py:408] Time since start: 62324.29s, 	Step: 177451, 	{'train/accuracy': 0.9409877061843872, 'train/loss': 0.45723941922187805, 'validation/accuracy': 0.762179970741272, 'validation/loss': 1.175032138824463, 'validation/num_examples': 50000, 'test/accuracy': 0.647100031375885, 'test/loss': 1.7913333177566528, 'test/num_examples': 10000, 'score': 60224.07753944397, 'total_duration': 62324.28744530678, 'accumulated_submission_time': 60224.07753944397, 'accumulated_eval_time': 2088.686345100403, 'accumulated_logging_time': 5.303653001785278}
I0128 06:29:15.119987 139865266054912 logging_writer.py:48] [177451] accumulated_eval_time=2088.686345, accumulated_logging_time=5.303653, accumulated_submission_time=60224.077539, global_step=177451, preemption_count=0, score=60224.077539, test/accuracy=0.647100, test/loss=1.791333, test/num_examples=10000, total_duration=62324.287445, train/accuracy=0.940988, train/loss=0.457239, validation/accuracy=0.762180, validation/loss=1.175032, validation/num_examples=50000
I0128 06:29:32.119330 139865274447616 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.312572002410889, loss=2.559004306793213
I0128 06:30:06.052425 139865266054912 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.555723190307617, loss=2.5119853019714355
I0128 06:30:39.951329 139865274447616 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.402693271636963, loss=2.5617151260375977
I0128 06:31:13.907418 139865266054912 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.2766523361206055, loss=2.5557425022125244
I0128 06:31:47.843814 139865274447616 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.181822299957275, loss=2.5914838314056396
I0128 06:32:21.768430 139865266054912 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.005548000335693, loss=2.5163414478302
I0128 06:32:55.701701 139865274447616 logging_writer.py:48] [178100] global_step=178100, grad_norm=3.9769301414489746, loss=2.533205032348633
I0128 06:33:29.607949 139865266054912 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.184995651245117, loss=2.5281832218170166
I0128 06:34:03.533467 139865274447616 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.025136470794678, loss=2.5050346851348877
I0128 06:34:37.428742 139865266054912 logging_writer.py:48] [178400] global_step=178400, grad_norm=3.7961392402648926, loss=2.4900739192962646
I0128 06:35:11.354023 139865274447616 logging_writer.py:48] [178500] global_step=178500, grad_norm=3.9696097373962402, loss=2.5162851810455322
I0128 06:35:45.340205 139865266054912 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.07423734664917, loss=2.5951027870178223
I0128 06:36:19.277170 139865274447616 logging_writer.py:48] [178700] global_step=178700, grad_norm=3.9773380756378174, loss=2.5254952907562256
I0128 06:36:53.191725 139865266054912 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.033175468444824, loss=2.4989781379699707
I0128 06:37:27.127711 139865274447616 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.313387393951416, loss=2.5178608894348145
I0128 06:37:45.240117 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:37:51.363420 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:38:00.111393 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:38:02.587152 140027215431488 submission_runner.py:408] Time since start: 62851.81s, 	Step: 178955, 	{'train/accuracy': 0.9386160373687744, 'train/loss': 0.4601021409034729, 'validation/accuracy': 0.7621399760246277, 'validation/loss': 1.1740968227386475, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7911723852157593, 'test/num_examples': 10000, 'score': 60734.13635802269, 'total_duration': 62851.805837869644, 'accumulated_submission_time': 60734.13635802269, 'accumulated_eval_time': 2106.0333411693573, 'accumulated_logging_time': 5.3640947341918945}
I0128 06:38:02.636009 139865266054912 logging_writer.py:48] [178955] accumulated_eval_time=2106.033341, accumulated_logging_time=5.364095, accumulated_submission_time=60734.136358, global_step=178955, preemption_count=0, score=60734.136358, test/accuracy=0.645200, test/loss=1.791172, test/num_examples=10000, total_duration=62851.805838, train/accuracy=0.938616, train/loss=0.460102, validation/accuracy=0.762140, validation/loss=1.174097, validation/num_examples=50000
I0128 06:38:18.220328 139865769342720 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.032121181488037, loss=2.4868922233581543
I0128 06:38:52.118329 139865266054912 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.000840663909912, loss=2.5605223178863525
I0128 06:39:26.031910 139865769342720 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.0831780433654785, loss=2.483832836151123
I0128 06:39:59.944735 139865266054912 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.030910968780518, loss=2.494257926940918
I0128 06:40:33.894621 139865769342720 logging_writer.py:48] [179400] global_step=179400, grad_norm=3.8014681339263916, loss=2.481855869293213
I0128 06:41:07.812052 139865266054912 logging_writer.py:48] [179500] global_step=179500, grad_norm=3.9299869537353516, loss=2.5200812816619873
I0128 06:41:41.737446 139865769342720 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.2871994972229, loss=2.5074801445007324
I0128 06:42:15.836222 139865266054912 logging_writer.py:48] [179700] global_step=179700, grad_norm=3.8387722969055176, loss=2.4783949851989746
I0128 06:42:49.743665 139865769342720 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.104310989379883, loss=2.5133841037750244
I0128 06:43:23.692949 139865266054912 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.238130569458008, loss=2.4892075061798096
I0128 06:43:57.608735 139865769342720 logging_writer.py:48] [180000] global_step=180000, grad_norm=3.666008472442627, loss=2.480461835861206
I0128 06:44:31.486135 139865266054912 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.455053329467773, loss=2.5360097885131836
I0128 06:45:05.401647 139865769342720 logging_writer.py:48] [180200] global_step=180200, grad_norm=3.9984054565429688, loss=2.47472882270813
I0128 06:45:39.302687 139865266054912 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.249074935913086, loss=2.501962423324585
I0128 06:46:13.258851 139865769342720 logging_writer.py:48] [180400] global_step=180400, grad_norm=3.8668267726898193, loss=2.5233774185180664
I0128 06:46:32.753359 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:46:38.903850 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:46:47.591910 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:46:50.100681 140027215431488 submission_runner.py:408] Time since start: 63379.32s, 	Step: 180459, 	{'train/accuracy': 0.9403499364852905, 'train/loss': 0.45583006739616394, 'validation/accuracy': 0.7623599767684937, 'validation/loss': 1.1769626140594482, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.7931833267211914, 'test/num_examples': 10000, 'score': 61244.19420695305, 'total_duration': 63379.31936812401, 'accumulated_submission_time': 61244.19420695305, 'accumulated_eval_time': 2123.3806269168854, 'accumulated_logging_time': 5.421769380569458}
I0128 06:46:50.152819 139866180368128 logging_writer.py:48] [180459] accumulated_eval_time=2123.380627, accumulated_logging_time=5.421769, accumulated_submission_time=61244.194207, global_step=180459, preemption_count=0, score=61244.194207, test/accuracy=0.644300, test/loss=1.793183, test/num_examples=10000, total_duration=63379.319368, train/accuracy=0.940350, train/loss=0.455830, validation/accuracy=0.762360, validation/loss=1.176963, validation/num_examples=50000
I0128 06:47:04.407061 139866188760832 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.208906173706055, loss=2.500816583633423
I0128 06:47:38.297087 139866180368128 logging_writer.py:48] [180600] global_step=180600, grad_norm=3.977787494659424, loss=2.4915261268615723
I0128 06:48:12.323638 139866188760832 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.404897689819336, loss=2.556675434112549
I0128 06:48:46.214164 139866180368128 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.079458236694336, loss=2.5348002910614014
I0128 06:49:20.153931 139866188760832 logging_writer.py:48] [180900] global_step=180900, grad_norm=3.850334405899048, loss=2.466958999633789
I0128 06:49:54.047781 139866180368128 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.034214496612549, loss=2.500300884246826
I0128 06:50:27.983613 139866188760832 logging_writer.py:48] [181100] global_step=181100, grad_norm=3.8786978721618652, loss=2.546980142593384
I0128 06:51:01.868846 139866180368128 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.310165882110596, loss=2.4980735778808594
I0128 06:51:35.802459 139866188760832 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.032121181488037, loss=2.504633903503418
I0128 06:52:09.728536 139866180368128 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.385073184967041, loss=2.516880750656128
I0128 06:52:43.664746 139866188760832 logging_writer.py:48] [181500] global_step=181500, grad_norm=3.791670083999634, loss=2.452817440032959
I0128 06:53:17.608517 139866180368128 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.35734748840332, loss=2.549891233444214
I0128 06:53:51.548199 139866188760832 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.138976097106934, loss=2.529865264892578
I0128 06:54:25.520514 139866180368128 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.373277187347412, loss=2.5600640773773193
I0128 06:54:59.461612 139866188760832 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.046435356140137, loss=2.5344274044036865
I0128 06:55:20.294121 140027215431488 spec.py:321] Evaluating on the training split.
I0128 06:55:26.367102 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 06:55:35.350193 140027215431488 spec.py:349] Evaluating on the test split.
I0128 06:55:37.850211 140027215431488 submission_runner.py:408] Time since start: 63907.07s, 	Step: 181963, 	{'train/accuracy': 0.9390943646430969, 'train/loss': 0.45994701981544495, 'validation/accuracy': 0.7626799941062927, 'validation/loss': 1.1733717918395996, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.7920598983764648, 'test/num_examples': 10000, 'score': 61754.27343964577, 'total_duration': 63907.06888461113, 'accumulated_submission_time': 61754.27343964577, 'accumulated_eval_time': 2140.9366660118103, 'accumulated_logging_time': 5.483500957489014}
I0128 06:55:37.898971 139865266054912 logging_writer.py:48] [181963] accumulated_eval_time=2140.936666, accumulated_logging_time=5.483501, accumulated_submission_time=61754.273440, global_step=181963, preemption_count=0, score=61754.273440, test/accuracy=0.644300, test/loss=1.792060, test/num_examples=10000, total_duration=63907.068885, train/accuracy=0.939094, train/loss=0.459947, validation/accuracy=0.762680, validation/loss=1.173372, validation/num_examples=50000
I0128 06:55:51.831319 139865274447616 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.234528064727783, loss=2.515956163406372
I0128 06:56:25.728266 139865266054912 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.189342498779297, loss=2.549931526184082
I0128 06:56:59.589057 139865274447616 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.535288333892822, loss=2.550659418106079
I0128 06:57:33.520166 139865266054912 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.019893646240234, loss=2.522970199584961
I0128 06:58:07.397182 139865274447616 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.449575901031494, loss=2.5243239402770996
I0128 06:58:41.301703 139865266054912 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.184834003448486, loss=2.561896562576294
I0128 06:59:15.210169 139865274447616 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.092658996582031, loss=2.5559349060058594
I0128 06:59:49.140693 139865266054912 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.169562339782715, loss=2.5391197204589844
I0128 07:00:23.042804 139865274447616 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.406161308288574, loss=2.518003225326538
I0128 07:00:57.012883 139865266054912 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.108050346374512, loss=2.5437121391296387
I0128 07:01:30.942874 139865274447616 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.210005760192871, loss=2.547152280807495
I0128 07:02:04.879210 139865266054912 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.27208948135376, loss=2.5596413612365723
I0128 07:02:38.767074 139865274447616 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.049361228942871, loss=2.4931750297546387
I0128 07:03:12.704995 139865266054912 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.271642684936523, loss=2.5626447200775146
I0128 07:03:46.605991 139865274447616 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.052628040313721, loss=2.57122802734375
I0128 07:04:08.110971 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:04:14.210557 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:04:23.141246 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:04:25.720731 140027215431488 submission_runner.py:408] Time since start: 64434.94s, 	Step: 183465, 	{'train/accuracy': 0.9401307106018066, 'train/loss': 0.45374590158462524, 'validation/accuracy': 0.7622999548912048, 'validation/loss': 1.1755332946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.7937980890274048, 'test/num_examples': 10000, 'score': 62263.354476451874, 'total_duration': 64434.93941235542, 'accumulated_submission_time': 62263.354476451874, 'accumulated_eval_time': 2158.5463812351227, 'accumulated_logging_time': 6.609484672546387}
I0128 07:04:25.773858 139866171975424 logging_writer.py:48] [183465] accumulated_eval_time=2158.546381, accumulated_logging_time=6.609485, accumulated_submission_time=62263.354476, global_step=183465, preemption_count=0, score=62263.354476, test/accuracy=0.644300, test/loss=1.793798, test/num_examples=10000, total_duration=64434.939412, train/accuracy=0.940131, train/loss=0.453746, validation/accuracy=0.762300, validation/loss=1.175533, validation/num_examples=50000
I0128 07:04:37.986329 139866180368128 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.110711574554443, loss=2.542327404022217
I0128 07:05:11.826747 139866171975424 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.250685691833496, loss=2.6052558422088623
I0128 07:05:45.736538 139866180368128 logging_writer.py:48] [183700] global_step=183700, grad_norm=3.9571373462677, loss=2.4421963691711426
I0128 07:06:19.650428 139866171975424 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.212470531463623, loss=2.5183749198913574
I0128 07:06:53.538838 139866180368128 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.401063442230225, loss=2.551844596862793
I0128 07:07:27.550446 139866171975424 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.2046427726745605, loss=2.5552029609680176
I0128 07:08:01.429891 139866180368128 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.085619926452637, loss=2.5593974590301514
I0128 07:08:35.359061 139866171975424 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.412939071655273, loss=2.5448315143585205
I0128 07:09:09.245699 139866180368128 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.005270957946777, loss=2.5161585807800293
I0128 07:09:43.172824 139866171975424 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.029067516326904, loss=2.5224568843841553
I0128 07:10:17.064228 139866180368128 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.060092926025391, loss=2.553502082824707
I0128 07:10:50.995389 139866171975424 logging_writer.py:48] [184600] global_step=184600, grad_norm=3.917402505874634, loss=2.4788267612457275
I0128 07:11:24.922345 139866180368128 logging_writer.py:48] [184700] global_step=184700, grad_norm=3.9261419773101807, loss=2.5234034061431885
I0128 07:11:58.826518 139866171975424 logging_writer.py:48] [184800] global_step=184800, grad_norm=3.8175156116485596, loss=2.479743480682373
I0128 07:12:32.762342 139866180368128 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.187741279602051, loss=2.5122270584106445
I0128 07:12:55.944707 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:13:02.059257 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:13:10.930956 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:13:13.417617 140027215431488 submission_runner.py:408] Time since start: 64962.64s, 	Step: 184970, 	{'train/accuracy': 0.9401506781578064, 'train/loss': 0.45149460434913635, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 1.1717240810394287, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7898361682891846, 'test/num_examples': 10000, 'score': 62773.463541030884, 'total_duration': 64962.63630104065, 'accumulated_submission_time': 62773.463541030884, 'accumulated_eval_time': 2176.019249677658, 'accumulated_logging_time': 6.671429634094238}
I0128 07:13:13.472854 139865760950016 logging_writer.py:48] [184970] accumulated_eval_time=2176.019250, accumulated_logging_time=6.671430, accumulated_submission_time=62773.463541, global_step=184970, preemption_count=0, score=62773.463541, test/accuracy=0.645200, test/loss=1.789836, test/num_examples=10000, total_duration=64962.636301, train/accuracy=0.940151, train/loss=0.451495, validation/accuracy=0.762580, validation/loss=1.171724, validation/num_examples=50000
I0128 07:13:24.137011 139865769342720 logging_writer.py:48] [185000] global_step=185000, grad_norm=3.980398416519165, loss=2.5123443603515625
I0128 07:13:58.013691 139865760950016 logging_writer.py:48] [185100] global_step=185100, grad_norm=3.9931023120880127, loss=2.5715928077697754
I0128 07:14:31.889538 139865769342720 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.3875932693481445, loss=2.523317813873291
I0128 07:15:05.782742 139865760950016 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.015665054321289, loss=2.5206875801086426
I0128 07:15:39.687988 139865769342720 logging_writer.py:48] [185400] global_step=185400, grad_norm=3.6435420513153076, loss=2.4754302501678467
I0128 07:16:13.582369 139865760950016 logging_writer.py:48] [185500] global_step=185500, grad_norm=3.966989517211914, loss=2.5144896507263184
I0128 07:16:47.468092 139865769342720 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.1581220626831055, loss=2.57352876663208
I0128 07:17:08.320064 139865760950016 logging_writer.py:48] [185663] global_step=185663, preemption_count=0, score=63008.241269
I0128 07:17:08.790592 140027215431488 checkpoints.py:490] Saving checkpoint at step: 185663
I0128 07:17:10.104640 140027215431488 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2/checkpoint_185663
I0128 07:17:10.129162 140027215431488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_2/checkpoint_185663.
I0128 07:17:10.888911 140027215431488 submission_runner.py:583] Tuning trial 2/5
I0128 07:17:10.889176 140027215431488 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0128 07:17:10.896057 140027215431488 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006576849264092743, 'train/loss': 6.912638187408447, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 33.4391393661499, 'total_duration': 51.186286211013794, 'accumulated_submission_time': 33.4391393661499, 'accumulated_eval_time': 17.747056245803833, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1500, {'train/accuracy': 0.0859375, 'train/loss': 5.226979732513428, 'validation/accuracy': 0.07903999835252762, 'validation/loss': 5.291569709777832, 'validation/num_examples': 50000, 'test/accuracy': 0.057100001722574234, 'test/loss': 5.528391361236572, 'test/num_examples': 10000, 'score': 543.6612985134125, 'total_duration': 579.0811305046082, 'accumulated_submission_time': 543.6612985134125, 'accumulated_eval_time': 35.347055435180664, 'accumulated_logging_time': 0.019211769104003906, 'global_step': 1500, 'preemption_count': 0}), (2998, {'train/accuracy': 0.1965680718421936, 'train/loss': 4.198776721954346, 'validation/accuracy': 0.17921999096870422, 'validation/loss': 4.319735050201416, 'validation/num_examples': 50000, 'test/accuracy': 0.12939999997615814, 'test/loss': 4.724494457244873, 'test/num_examples': 10000, 'score': 1053.6502876281738, 'total_duration': 1106.8475799560547, 'accumulated_submission_time': 1053.6502876281738, 'accumulated_eval_time': 53.0393807888031, 'accumulated_logging_time': 0.0525212287902832, 'global_step': 2998, 'preemption_count': 0}), (4496, {'train/accuracy': 0.3117426633834839, 'train/loss': 3.379237413406372, 'validation/accuracy': 0.28421998023986816, 'validation/loss': 3.520533323287964, 'validation/num_examples': 50000, 'test/accuracy': 0.20760001242160797, 'test/loss': 4.096458911895752, 'test/num_examples': 10000, 'score': 1563.8795804977417, 'total_duration': 1635.308201789856, 'accumulated_submission_time': 1563.8795804977417, 'accumulated_eval_time': 71.19184017181396, 'accumulated_logging_time': 0.07930827140808105, 'global_step': 4496, 'preemption_count': 0}), (5994, {'train/accuracy': 0.3806600570678711, 'train/loss': 2.963285207748413, 'validation/accuracy': 0.3558799922466278, 'validation/loss': 3.0989913940429688, 'validation/num_examples': 50000, 'test/accuracy': 0.2656000256538391, 'test/loss': 3.724989652633667, 'test/num_examples': 10000, 'score': 2074.014079093933, 'total_duration': 2163.0716738700867, 'accumulated_submission_time': 2074.014079093933, 'accumulated_eval_time': 88.74252462387085, 'accumulated_logging_time': 0.10614848136901855, 'global_step': 5994, 'preemption_count': 0}), (7493, {'train/accuracy': 0.46181440353393555, 'train/loss': 2.6370625495910645, 'validation/accuracy': 0.4315599799156189, 'validation/loss': 2.7958452701568604, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.3746931552886963, 'test/num_examples': 10000, 'score': 2584.21363902092, 'total_duration': 2690.823953151703, 'accumulated_submission_time': 2584.21363902092, 'accumulated_eval_time': 106.2141387462616, 'accumulated_logging_time': 0.1336226463317871, 'global_step': 7493, 'preemption_count': 0}), (8993, {'train/accuracy': 0.5307716727256775, 'train/loss': 2.255420207977295, 'validation/accuracy': 0.47283998131752014, 'validation/loss': 2.5475475788116455, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.1709978580474854, 'test/num_examples': 10000, 'score': 3094.3763308525085, 'total_duration': 3218.3501625061035, 'accumulated_submission_time': 3094.3763308525085, 'accumulated_eval_time': 123.49534726142883, 'accumulated_logging_time': 0.1627497673034668, 'global_step': 8993, 'preemption_count': 0}), (10493, {'train/accuracy': 0.5577766299247742, 'train/loss': 2.1650500297546387, 'validation/accuracy': 0.5104199647903442, 'validation/loss': 2.3915514945983887, 'validation/num_examples': 50000, 'test/accuracy': 0.38860002160072327, 'test/loss': 3.049933433532715, 'test/num_examples': 10000, 'score': 3604.3356716632843, 'total_duration': 3745.998475790024, 'accumulated_submission_time': 3604.3356716632843, 'accumulated_eval_time': 141.10048961639404, 'accumulated_logging_time': 0.19440197944641113, 'global_step': 10493, 'preemption_count': 0}), (11993, {'train/accuracy': 0.6017418503761292, 'train/loss': 1.9433568716049194, 'validation/accuracy': 0.5548200011253357, 'validation/loss': 2.1569015979766846, 'validation/num_examples': 50000, 'test/accuracy': 0.4271000325679779, 'test/loss': 2.8042361736297607, 'test/num_examples': 10000, 'score': 4114.402346134186, 'total_duration': 4273.819544792175, 'accumulated_submission_time': 4114.402346134186, 'accumulated_eval_time': 158.77054691314697, 'accumulated_logging_time': 0.22590017318725586, 'global_step': 11993, 'preemption_count': 0}), (13494, {'train/accuracy': 0.6224888563156128, 'train/loss': 1.774627685546875, 'validation/accuracy': 0.5730400085449219, 'validation/loss': 2.0085930824279785, 'validation/num_examples': 50000, 'test/accuracy': 0.45100003480911255, 'test/loss': 2.655189275741577, 'test/num_examples': 10000, 'score': 4624.474926710129, 'total_duration': 4801.298516511917, 'accumulated_submission_time': 4624.474926710129, 'accumulated_eval_time': 176.094083070755, 'accumulated_logging_time': 0.25512123107910156, 'global_step': 13494, 'preemption_count': 0}), (14995, {'train/accuracy': 0.6300023794174194, 'train/loss': 1.7994438409805298, 'validation/accuracy': 0.5817999839782715, 'validation/loss': 2.0272626876831055, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.6749420166015625, 'test/num_examples': 10000, 'score': 5134.568528413773, 'total_duration': 5329.296462774277, 'accumulated_submission_time': 5134.568528413773, 'accumulated_eval_time': 193.91254234313965, 'accumulated_logging_time': 0.2859010696411133, 'global_step': 14995, 'preemption_count': 0}), (16498, {'train/accuracy': 0.6436144709587097, 'train/loss': 1.6748861074447632, 'validation/accuracy': 0.5989199876785278, 'validation/loss': 1.8900413513183594, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.5429165363311768, 'test/num_examples': 10000, 'score': 5644.774896144867, 'total_duration': 5857.091723680496, 'accumulated_submission_time': 5644.774896144867, 'accumulated_eval_time': 211.41987991333008, 'accumulated_logging_time': 0.31459641456604004, 'global_step': 16498, 'preemption_count': 0}), (18001, {'train/accuracy': 0.7017298936843872, 'train/loss': 1.4178893566131592, 'validation/accuracy': 0.6070199608802795, 'validation/loss': 1.8295985460281372, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.4914474487304688, 'test/num_examples': 10000, 'score': 6155.2353079319, 'total_duration': 6385.0878365039825, 'accumulated_submission_time': 6155.2353079319, 'accumulated_eval_time': 228.8724648952484, 'accumulated_logging_time': 0.3461291790008545, 'global_step': 18001, 'preemption_count': 0}), (19503, {'train/accuracy': 0.6970065236091614, 'train/loss': 1.4243632555007935, 'validation/accuracy': 0.6238399744033813, 'validation/loss': 1.7519325017929077, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.404106855392456, 'test/num_examples': 10000, 'score': 6665.291936635971, 'total_duration': 6912.620675325394, 'accumulated_submission_time': 6665.291936635971, 'accumulated_eval_time': 246.26633167266846, 'accumulated_logging_time': 0.3774294853210449, 'global_step': 19503, 'preemption_count': 0}), (21005, {'train/accuracy': 0.6917251348495483, 'train/loss': 1.4819576740264893, 'validation/accuracy': 0.6297399997711182, 'validation/loss': 1.7615430355072021, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.432694673538208, 'test/num_examples': 10000, 'score': 7175.285215139389, 'total_duration': 7440.939846038818, 'accumulated_submission_time': 7175.285215139389, 'accumulated_eval_time': 264.5082576274872, 'accumulated_logging_time': 0.4087963104248047, 'global_step': 21005, 'preemption_count': 0}), (22507, {'train/accuracy': 0.6911072731018066, 'train/loss': 1.4709314107894897, 'validation/accuracy': 0.629539966583252, 'validation/loss': 1.7492947578430176, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.40031361579895, 'test/num_examples': 10000, 'score': 7685.427667379379, 'total_duration': 7968.438979625702, 'accumulated_submission_time': 7685.427667379379, 'accumulated_eval_time': 281.779283285141, 'accumulated_logging_time': 0.4405534267425537, 'global_step': 22507, 'preemption_count': 0}), (24009, {'train/accuracy': 0.6873604655265808, 'train/loss': 1.4837753772735596, 'validation/accuracy': 0.6243799924850464, 'validation/loss': 1.7636795043945312, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.3973007202148438, 'test/num_examples': 10000, 'score': 8195.398663759232, 'total_duration': 8496.294484138489, 'accumulated_submission_time': 8195.398663759232, 'accumulated_eval_time': 299.5811333656311, 'accumulated_logging_time': 0.4718921184539795, 'global_step': 24009, 'preemption_count': 0}), (25512, {'train/accuracy': 0.6941366195678711, 'train/loss': 1.4621155261993408, 'validation/accuracy': 0.6352399587631226, 'validation/loss': 1.7313721179962158, 'validation/num_examples': 50000, 'test/accuracy': 0.5123000144958496, 'test/loss': 2.378972291946411, 'test/num_examples': 10000, 'score': 8705.627409219742, 'total_duration': 9024.263773679733, 'accumulated_submission_time': 8705.627409219742, 'accumulated_eval_time': 317.2381706237793, 'accumulated_logging_time': 0.5033385753631592, 'global_step': 25512, 'preemption_count': 0}), (27015, {'train/accuracy': 0.7335180044174194, 'train/loss': 1.286011815071106, 'validation/accuracy': 0.6350799798965454, 'validation/loss': 1.7067891359329224, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.38372540473938, 'test/num_examples': 10000, 'score': 9215.783554315567, 'total_duration': 9551.9481112957, 'accumulated_submission_time': 9215.783554315567, 'accumulated_eval_time': 334.68074440956116, 'accumulated_logging_time': 0.5375471115112305, 'global_step': 27015, 'preemption_count': 0}), (28518, {'train/accuracy': 0.7210817933082581, 'train/loss': 1.3177582025527954, 'validation/accuracy': 0.6410999894142151, 'validation/loss': 1.6783519983291626, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.341330051422119, 'test/num_examples': 10000, 'score': 9725.861224412918, 'total_duration': 10080.300015211105, 'accumulated_submission_time': 9725.861224412918, 'accumulated_eval_time': 352.87031412124634, 'accumulated_logging_time': 0.5690395832061768, 'global_step': 28518, 'preemption_count': 0}), (30021, {'train/accuracy': 0.7088049650192261, 'train/loss': 1.3988709449768066, 'validation/accuracy': 0.6373199820518494, 'validation/loss': 1.7182626724243164, 'validation/num_examples': 50000, 'test/accuracy': 0.5089000463485718, 'test/loss': 2.397118330001831, 'test/num_examples': 10000, 'score': 10235.831578016281, 'total_duration': 10608.090680122375, 'accumulated_submission_time': 10235.831578016281, 'accumulated_eval_time': 370.6090452671051, 'accumulated_logging_time': 0.5975942611694336, 'global_step': 30021, 'preemption_count': 0}), (31525, {'train/accuracy': 0.7114556431770325, 'train/loss': 1.361906886100769, 'validation/accuracy': 0.646340012550354, 'validation/loss': 1.6596877574920654, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.325451374053955, 'test/num_examples': 10000, 'score': 10745.978585481644, 'total_duration': 11136.003517150879, 'accumulated_submission_time': 10745.978585481644, 'accumulated_eval_time': 388.2906057834625, 'accumulated_logging_time': 0.6299974918365479, 'global_step': 31525, 'preemption_count': 0}), (33027, {'train/accuracy': 0.7103396058082581, 'train/loss': 1.374114990234375, 'validation/accuracy': 0.6455199718475342, 'validation/loss': 1.6690250635147095, 'validation/num_examples': 50000, 'test/accuracy': 0.5130000114440918, 'test/loss': 2.3377459049224854, 'test/num_examples': 10000, 'score': 11256.05967617035, 'total_duration': 11663.95685338974, 'accumulated_submission_time': 11256.05967617035, 'accumulated_eval_time': 406.07124972343445, 'accumulated_logging_time': 0.669827938079834, 'global_step': 33027, 'preemption_count': 0}), (34532, {'train/accuracy': 0.7158003449440002, 'train/loss': 1.3178354501724243, 'validation/accuracy': 0.6526600122451782, 'validation/loss': 1.6064244508743286, 'validation/num_examples': 50000, 'test/accuracy': 0.5246000289916992, 'test/loss': 2.271498680114746, 'test/num_examples': 10000, 'score': 11766.244701385498, 'total_duration': 12191.655038356781, 'accumulated_submission_time': 11766.244701385498, 'accumulated_eval_time': 423.49549770355225, 'accumulated_logging_time': 0.7041542530059814, 'global_step': 34532, 'preemption_count': 0}), (36036, {'train/accuracy': 0.7166773080825806, 'train/loss': 1.3349188566207886, 'validation/accuracy': 0.6400600075721741, 'validation/loss': 1.6809086799621582, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.3779425621032715, 'test/num_examples': 10000, 'score': 12276.337631940842, 'total_duration': 12719.575999498367, 'accumulated_submission_time': 12276.337631940842, 'accumulated_eval_time': 441.2348201274872, 'accumulated_logging_time': 0.7414686679840088, 'global_step': 36036, 'preemption_count': 0}), (37540, {'train/accuracy': 0.7401347160339355, 'train/loss': 1.2507890462875366, 'validation/accuracy': 0.6541999578475952, 'validation/loss': 1.622602939605713, 'validation/num_examples': 50000, 'test/accuracy': 0.5275000333786011, 'test/loss': 2.2785587310791016, 'test/num_examples': 10000, 'score': 12786.48780798912, 'total_duration': 13247.302038192749, 'accumulated_submission_time': 12786.48780798912, 'accumulated_eval_time': 458.72729420661926, 'accumulated_logging_time': 0.7722766399383545, 'global_step': 37540, 'preemption_count': 0}), (39044, {'train/accuracy': 0.7385004758834839, 'train/loss': 1.2336959838867188, 'validation/accuracy': 0.6571800112724304, 'validation/loss': 1.5852231979370117, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.2626054286956787, 'test/num_examples': 10000, 'score': 13296.705997228622, 'total_duration': 13775.218259096146, 'accumulated_submission_time': 13296.705997228622, 'accumulated_eval_time': 476.3352725505829, 'accumulated_logging_time': 0.80930495262146, 'global_step': 39044, 'preemption_count': 0}), (40547, {'train/accuracy': 0.7364476919174194, 'train/loss': 1.2696105241775513, 'validation/accuracy': 0.6611799597740173, 'validation/loss': 1.5964609384536743, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.2442195415496826, 'test/num_examples': 10000, 'score': 13806.621383428574, 'total_duration': 14302.725590705872, 'accumulated_submission_time': 13806.621383428574, 'accumulated_eval_time': 493.83841919898987, 'accumulated_logging_time': 0.8440403938293457, 'global_step': 40547, 'preemption_count': 0}), (42051, {'train/accuracy': 0.7135283946990967, 'train/loss': 1.3360618352890015, 'validation/accuracy': 0.6412599682807922, 'validation/loss': 1.6634626388549805, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.3140530586242676, 'test/num_examples': 10000, 'score': 14316.647776842117, 'total_duration': 14831.066751718521, 'accumulated_submission_time': 14316.647776842117, 'accumulated_eval_time': 512.0555701255798, 'accumulated_logging_time': 0.8885290622711182, 'global_step': 42051, 'preemption_count': 0}), (43555, {'train/accuracy': 0.7233737111091614, 'train/loss': 1.2983133792877197, 'validation/accuracy': 0.6625799536705017, 'validation/loss': 1.5901869535446167, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.240588426589966, 'test/num_examples': 10000, 'score': 14826.77017712593, 'total_duration': 15358.520558834076, 'accumulated_submission_time': 14826.77017712593, 'accumulated_eval_time': 529.3009285926819, 'accumulated_logging_time': 0.9220795631408691, 'global_step': 43555, 'preemption_count': 0}), (45058, {'train/accuracy': 0.7239118218421936, 'train/loss': 1.3282990455627441, 'validation/accuracy': 0.6556199789047241, 'validation/loss': 1.6299936771392822, 'validation/num_examples': 50000, 'test/accuracy': 0.5277000069618225, 'test/loss': 2.2814555168151855, 'test/num_examples': 10000, 'score': 15336.741206169128, 'total_duration': 15886.432034015656, 'accumulated_submission_time': 15336.741206169128, 'accumulated_eval_time': 547.1348049640656, 'accumulated_logging_time': 0.9746749401092529, 'global_step': 45058, 'preemption_count': 0}), (46562, {'train/accuracy': 0.7540656924247742, 'train/loss': 1.198965311050415, 'validation/accuracy': 0.6650199890136719, 'validation/loss': 1.5895963907241821, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.2207345962524414, 'test/num_examples': 10000, 'score': 15846.720610380173, 'total_duration': 16414.10246682167, 'accumulated_submission_time': 15846.720610380173, 'accumulated_eval_time': 564.7328906059265, 'accumulated_logging_time': 1.0138413906097412, 'global_step': 46562, 'preemption_count': 0}), (48066, {'train/accuracy': 0.7438217401504517, 'train/loss': 1.2626017332077026, 'validation/accuracy': 0.6648600101470947, 'validation/loss': 1.6124789714813232, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.236502170562744, 'test/num_examples': 10000, 'score': 16356.965524196625, 'total_duration': 16941.98569583893, 'accumulated_submission_time': 16356.965524196625, 'accumulated_eval_time': 582.2785995006561, 'accumulated_logging_time': 1.0533804893493652, 'global_step': 48066, 'preemption_count': 0}), (49571, {'train/accuracy': 0.7395169138908386, 'train/loss': 1.25126314163208, 'validation/accuracy': 0.6665399670600891, 'validation/loss': 1.5785037279129028, 'validation/num_examples': 50000, 'test/accuracy': 0.5455999970436096, 'test/loss': 2.187873601913452, 'test/num_examples': 10000, 'score': 16867.10874891281, 'total_duration': 17469.85976576805, 'accumulated_submission_time': 16867.10874891281, 'accumulated_eval_time': 599.9203262329102, 'accumulated_logging_time': 1.0888376235961914, 'global_step': 49571, 'preemption_count': 0}), (51075, {'train/accuracy': 0.7398756146430969, 'train/loss': 1.2330973148345947, 'validation/accuracy': 0.666700005531311, 'validation/loss': 1.5564137697219849, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.2007992267608643, 'test/num_examples': 10000, 'score': 17377.16103053093, 'total_duration': 17997.680426597595, 'accumulated_submission_time': 17377.16103053093, 'accumulated_eval_time': 617.5973706245422, 'accumulated_logging_time': 1.126636266708374, 'global_step': 51075, 'preemption_count': 0}), (52578, {'train/accuracy': 0.7335976958274841, 'train/loss': 1.282623529434204, 'validation/accuracy': 0.6645999550819397, 'validation/loss': 1.590146541595459, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.245861291885376, 'test/num_examples': 10000, 'score': 17887.110765457153, 'total_duration': 18525.204036712646, 'accumulated_submission_time': 17887.110765457153, 'accumulated_eval_time': 635.0826358795166, 'accumulated_logging_time': 1.1614611148834229, 'global_step': 52578, 'preemption_count': 0}), (54082, {'train/accuracy': 0.7420878410339355, 'train/loss': 1.2159703969955444, 'validation/accuracy': 0.6678999662399292, 'validation/loss': 1.5432895421981812, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.1653194427490234, 'test/num_examples': 10000, 'score': 18397.201429128647, 'total_duration': 19053.081587553024, 'accumulated_submission_time': 18397.201429128647, 'accumulated_eval_time': 652.7812848091125, 'accumulated_logging_time': 1.1971261501312256, 'global_step': 54082, 'preemption_count': 0}), (55586, {'train/accuracy': 0.7388392686843872, 'train/loss': 1.2988405227661133, 'validation/accuracy': 0.6522200107574463, 'validation/loss': 1.6849249601364136, 'validation/num_examples': 50000, 'test/accuracy': 0.5271000266075134, 'test/loss': 2.3522109985351562, 'test/num_examples': 10000, 'score': 18907.42284011841, 'total_duration': 19580.69886445999, 'accumulated_submission_time': 18907.42284011841, 'accumulated_eval_time': 670.0841941833496, 'accumulated_logging_time': 1.2361524105072021, 'global_step': 55586, 'preemption_count': 0}), (57090, {'train/accuracy': 0.7461535334587097, 'train/loss': 1.222550630569458, 'validation/accuracy': 0.6632999777793884, 'validation/loss': 1.5833584070205688, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.2467947006225586, 'test/num_examples': 10000, 'score': 19417.531020641327, 'total_duration': 20108.20799922943, 'accumulated_submission_time': 19417.531020641327, 'accumulated_eval_time': 687.3933174610138, 'accumulated_logging_time': 1.2751078605651855, 'global_step': 57090, 'preemption_count': 0}), (58594, {'train/accuracy': 0.750418484210968, 'train/loss': 1.2128936052322388, 'validation/accuracy': 0.6708599925041199, 'validation/loss': 1.5625097751617432, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.2097489833831787, 'test/num_examples': 10000, 'score': 19927.526131629944, 'total_duration': 20635.700806617737, 'accumulated_submission_time': 19927.526131629944, 'accumulated_eval_time': 704.799777507782, 'accumulated_logging_time': 1.3132286071777344, 'global_step': 58594, 'preemption_count': 0}), (60098, {'train/accuracy': 0.7451171875, 'train/loss': 1.1998909711837769, 'validation/accuracy': 0.6719799637794495, 'validation/loss': 1.529384970664978, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.1742067337036133, 'test/num_examples': 10000, 'score': 20437.654178380966, 'total_duration': 21163.29753088951, 'accumulated_submission_time': 20437.654178380966, 'accumulated_eval_time': 722.1753432750702, 'accumulated_logging_time': 1.3547327518463135, 'global_step': 60098, 'preemption_count': 0}), (61601, {'train/accuracy': 0.7535673975944519, 'train/loss': 1.186854362487793, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5134427547454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 2.1582529544830322, 'test/num_examples': 10000, 'score': 20947.574555158615, 'total_duration': 21690.799419879913, 'accumulated_submission_time': 20947.574555158615, 'accumulated_eval_time': 739.661140203476, 'accumulated_logging_time': 1.3968374729156494, 'global_step': 61601, 'preemption_count': 0}), (63105, {'train/accuracy': 0.7455755472183228, 'train/loss': 1.2469745874404907, 'validation/accuracy': 0.6726199984550476, 'validation/loss': 1.5651296377182007, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.2219254970550537, 'test/num_examples': 10000, 'score': 21457.5686917305, 'total_duration': 22218.297739744186, 'accumulated_submission_time': 21457.5686917305, 'accumulated_eval_time': 757.0620410442352, 'accumulated_logging_time': 1.448808193206787, 'global_step': 63105, 'preemption_count': 0}), (64609, {'train/accuracy': 0.7794363498687744, 'train/loss': 1.0946069955825806, 'validation/accuracy': 0.6756399869918823, 'validation/loss': 1.5386929512023926, 'validation/num_examples': 50000, 'test/accuracy': 0.5548000335693359, 'test/loss': 2.164292335510254, 'test/num_examples': 10000, 'score': 21967.481950998306, 'total_duration': 22745.924226760864, 'accumulated_submission_time': 21967.481950998306, 'accumulated_eval_time': 774.6855986118317, 'accumulated_logging_time': 1.487745761871338, 'global_step': 64609, 'preemption_count': 0}), (66114, {'train/accuracy': 0.7683154940605164, 'train/loss': 1.1131125688552856, 'validation/accuracy': 0.6814999580383301, 'validation/loss': 1.4990878105163574, 'validation/num_examples': 50000, 'test/accuracy': 0.5603000521659851, 'test/loss': 2.1476669311523438, 'test/num_examples': 10000, 'score': 22477.707414150238, 'total_duration': 23274.14600086212, 'accumulated_submission_time': 22477.707414150238, 'accumulated_eval_time': 792.5888900756836, 'accumulated_logging_time': 1.5288665294647217, 'global_step': 66114, 'preemption_count': 0}), (67618, {'train/accuracy': 0.7588488459587097, 'train/loss': 1.1663302183151245, 'validation/accuracy': 0.6769199967384338, 'validation/loss': 1.5283010005950928, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.1961100101470947, 'test/num_examples': 10000, 'score': 22987.609982013702, 'total_duration': 23801.867978334427, 'accumulated_submission_time': 22987.609982013702, 'accumulated_eval_time': 810.3098471164703, 'accumulated_logging_time': 1.5757479667663574, 'global_step': 67618, 'preemption_count': 0}), (69122, {'train/accuracy': 0.7570351958274841, 'train/loss': 1.1902135610580444, 'validation/accuracy': 0.6778199672698975, 'validation/loss': 1.5349388122558594, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.1934776306152344, 'test/num_examples': 10000, 'score': 23497.588663101196, 'total_duration': 24329.425166130066, 'accumulated_submission_time': 23497.588663101196, 'accumulated_eval_time': 827.7932825088501, 'accumulated_logging_time': 1.6187632083892822, 'global_step': 69122, 'preemption_count': 0}), (70627, {'train/accuracy': 0.7458944320678711, 'train/loss': 1.2468349933624268, 'validation/accuracy': 0.668999969959259, 'validation/loss': 1.5783573389053345, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.213312864303589, 'test/num_examples': 10000, 'score': 24007.770708084106, 'total_duration': 24857.20196413994, 'accumulated_submission_time': 24007.770708084106, 'accumulated_eval_time': 845.2944579124451, 'accumulated_logging_time': 1.6605701446533203, 'global_step': 70627, 'preemption_count': 0}), (72131, {'train/accuracy': 0.7466716766357422, 'train/loss': 1.2096285820007324, 'validation/accuracy': 0.6700999736785889, 'validation/loss': 1.548133373260498, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.200495719909668, 'test/num_examples': 10000, 'score': 24517.843168497086, 'total_duration': 25384.974372386932, 'accumulated_submission_time': 24517.843168497086, 'accumulated_eval_time': 862.8972687721252, 'accumulated_logging_time': 1.7031033039093018, 'global_step': 72131, 'preemption_count': 0}), (73636, {'train/accuracy': 0.7932278513908386, 'train/loss': 1.0378259420394897, 'validation/accuracy': 0.6830999851226807, 'validation/loss': 1.5034915208816528, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.1635515689849854, 'test/num_examples': 10000, 'score': 25028.014729499817, 'total_duration': 25912.854856967926, 'accumulated_submission_time': 25028.014729499817, 'accumulated_eval_time': 880.508659362793, 'accumulated_logging_time': 1.7472789287567139, 'global_step': 73636, 'preemption_count': 0}), (75141, {'train/accuracy': 0.7795758843421936, 'train/loss': 1.0734102725982666, 'validation/accuracy': 0.6831799745559692, 'validation/loss': 1.4843811988830566, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 2.1242897510528564, 'test/num_examples': 10000, 'score': 25538.201239585876, 'total_duration': 26440.645871162415, 'accumulated_submission_time': 25538.201239585876, 'accumulated_eval_time': 898.0165367126465, 'accumulated_logging_time': 1.791778802871704, 'global_step': 75141, 'preemption_count': 0}), (76646, {'train/accuracy': 0.7704480290412903, 'train/loss': 1.1027264595031738, 'validation/accuracy': 0.6850999593734741, 'validation/loss': 1.478226661682129, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.119495391845703, 'test/num_examples': 10000, 'score': 26048.3084192276, 'total_duration': 26968.49898505211, 'accumulated_submission_time': 26048.3084192276, 'accumulated_eval_time': 915.6695799827576, 'accumulated_logging_time': 1.8321490287780762, 'global_step': 76646, 'preemption_count': 0}), (78151, {'train/accuracy': 0.7723413705825806, 'train/loss': 1.126224398612976, 'validation/accuracy': 0.6868199706077576, 'validation/loss': 1.4868748188018799, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 2.1145002841949463, 'test/num_examples': 10000, 'score': 26558.43163084984, 'total_duration': 27495.934143304825, 'accumulated_submission_time': 26558.43163084984, 'accumulated_eval_time': 932.8865323066711, 'accumulated_logging_time': 1.8738563060760498, 'global_step': 78151, 'preemption_count': 0}), (79656, {'train/accuracy': 0.7729591727256775, 'train/loss': 1.1393262147903442, 'validation/accuracy': 0.6887999773025513, 'validation/loss': 1.5029897689819336, 'validation/num_examples': 50000, 'test/accuracy': 0.5633000135421753, 'test/loss': 2.1523826122283936, 'test/num_examples': 10000, 'score': 27068.563962221146, 'total_duration': 28023.855808973312, 'accumulated_submission_time': 27068.563962221146, 'accumulated_eval_time': 950.5808329582214, 'accumulated_logging_time': 1.9172337055206299, 'global_step': 79656, 'preemption_count': 0}), (81161, {'train/accuracy': 0.7640505433082581, 'train/loss': 1.1171501874923706, 'validation/accuracy': 0.682379961013794, 'validation/loss': 1.4879083633422852, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.143951177597046, 'test/num_examples': 10000, 'score': 27578.765528678894, 'total_duration': 28552.553351163864, 'accumulated_submission_time': 27578.765528678894, 'accumulated_eval_time': 968.9772000312805, 'accumulated_logging_time': 1.9648942947387695, 'global_step': 81161, 'preemption_count': 0}), (82665, {'train/accuracy': 0.8116629123687744, 'train/loss': 0.9491603970527649, 'validation/accuracy': 0.6913999915122986, 'validation/loss': 1.4522778987884521, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 2.0855112075805664, 'test/num_examples': 10000, 'score': 28088.932448387146, 'total_duration': 29080.300355911255, 'accumulated_submission_time': 28088.932448387146, 'accumulated_eval_time': 986.45987200737, 'accumulated_logging_time': 2.0094380378723145, 'global_step': 82665, 'preemption_count': 0}), (84170, {'train/accuracy': 0.786152720451355, 'train/loss': 1.03113853931427, 'validation/accuracy': 0.6920199990272522, 'validation/loss': 1.4382027387619019, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 2.106266975402832, 'test/num_examples': 10000, 'score': 28599.081391334534, 'total_duration': 29608.071023464203, 'accumulated_submission_time': 28599.081391334534, 'accumulated_eval_time': 1003.9821727275848, 'accumulated_logging_time': 2.0565803050994873, 'global_step': 84170, 'preemption_count': 0}), (85675, {'train/accuracy': 0.7844387888908386, 'train/loss': 1.050876259803772, 'validation/accuracy': 0.6937800049781799, 'validation/loss': 1.4405685663223267, 'validation/num_examples': 50000, 'test/accuracy': 0.5645000338554382, 'test/loss': 2.099759101867676, 'test/num_examples': 10000, 'score': 29109.275886297226, 'total_duration': 30135.671944856644, 'accumulated_submission_time': 29109.275886297226, 'accumulated_eval_time': 1021.2940018177032, 'accumulated_logging_time': 2.0996451377868652, 'global_step': 85675, 'preemption_count': 0}), (87180, {'train/accuracy': 0.7828842401504517, 'train/loss': 1.0602805614471436, 'validation/accuracy': 0.6989799737930298, 'validation/loss': 1.430122971534729, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 2.0682175159454346, 'test/num_examples': 10000, 'score': 29619.428687810898, 'total_duration': 30663.413135290146, 'accumulated_submission_time': 29619.428687810898, 'accumulated_eval_time': 1038.789042711258, 'accumulated_logging_time': 2.1402103900909424, 'global_step': 87180, 'preemption_count': 0}), (88684, {'train/accuracy': 0.7784797549247742, 'train/loss': 1.0503365993499756, 'validation/accuracy': 0.6977999806404114, 'validation/loss': 1.412257194519043, 'validation/num_examples': 50000, 'test/accuracy': 0.565500020980835, 'test/loss': 2.0685789585113525, 'test/num_examples': 10000, 'score': 30129.41860818863, 'total_duration': 31191.086156368256, 'accumulated_submission_time': 30129.41860818863, 'accumulated_eval_time': 1056.377511024475, 'accumulated_logging_time': 2.181400775909424, 'global_step': 88684, 'preemption_count': 0}), (90189, {'train/accuracy': 0.7796755433082581, 'train/loss': 1.052192211151123, 'validation/accuracy': 0.6933599710464478, 'validation/loss': 1.4358712434768677, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 2.074946641921997, 'test/num_examples': 10000, 'score': 30639.66088938713, 'total_duration': 31718.936313152313, 'accumulated_submission_time': 30639.66088938713, 'accumulated_eval_time': 1073.8877835273743, 'accumulated_logging_time': 2.227370262145996, 'global_step': 90189, 'preemption_count': 0}), (91694, {'train/accuracy': 0.8287826776504517, 'train/loss': 0.869025707244873, 'validation/accuracy': 0.7048199772834778, 'validation/loss': 1.3970078229904175, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 2.055137872695923, 'test/num_examples': 10000, 'score': 31149.845452308655, 'total_duration': 32246.600895643234, 'accumulated_submission_time': 31149.845452308655, 'accumulated_eval_time': 1091.269455909729, 'accumulated_logging_time': 2.272991180419922, 'global_step': 91694, 'preemption_count': 0}), (93198, {'train/accuracy': 0.7990872263908386, 'train/loss': 0.9855494499206543, 'validation/accuracy': 0.6990999579429626, 'validation/loss': 1.429892659187317, 'validation/num_examples': 50000, 'test/accuracy': 0.5735000371932983, 'test/loss': 2.089207172393799, 'test/num_examples': 10000, 'score': 31659.772423505783, 'total_duration': 32774.04626703262, 'accumulated_submission_time': 31659.772423505783, 'accumulated_eval_time': 1108.6878995895386, 'accumulated_logging_time': 2.321443557739258, 'global_step': 93198, 'preemption_count': 0}), (94701, {'train/accuracy': 0.7864317297935486, 'train/loss': 1.0102328062057495, 'validation/accuracy': 0.6943399906158447, 'validation/loss': 1.414360761642456, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 2.086948871612549, 'test/num_examples': 10000, 'score': 32169.710786104202, 'total_duration': 33301.60674357414, 'accumulated_submission_time': 32169.710786104202, 'accumulated_eval_time': 1126.209413766861, 'accumulated_logging_time': 2.367856502532959, 'global_step': 94701, 'preemption_count': 0}), (96205, {'train/accuracy': 0.8004623651504517, 'train/loss': 1.0195797681808472, 'validation/accuracy': 0.7038599848747253, 'validation/loss': 1.428849697113037, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 2.0695486068725586, 'test/num_examples': 10000, 'score': 32679.708333969116, 'total_duration': 33829.24775338173, 'accumulated_submission_time': 32679.708333969116, 'accumulated_eval_time': 1143.754875421524, 'accumulated_logging_time': 2.413187265396118, 'global_step': 96205, 'preemption_count': 0}), (97709, {'train/accuracy': 0.7979910373687744, 'train/loss': 1.0076885223388672, 'validation/accuracy': 0.7076399922370911, 'validation/loss': 1.4024112224578857, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 2.061427354812622, 'test/num_examples': 10000, 'score': 33189.67002224922, 'total_duration': 34356.86605596542, 'accumulated_submission_time': 33189.67002224922, 'accumulated_eval_time': 1161.3141412734985, 'accumulated_logging_time': 2.4573397636413574, 'global_step': 97709, 'preemption_count': 0}), (99213, {'train/accuracy': 0.7958186864852905, 'train/loss': 1.0074490308761597, 'validation/accuracy': 0.7049799561500549, 'validation/loss': 1.3988691568374634, 'validation/num_examples': 50000, 'test/accuracy': 0.5746999979019165, 'test/loss': 2.0553035736083984, 'test/num_examples': 10000, 'score': 33699.635172605515, 'total_duration': 34884.06643486023, 'accumulated_submission_time': 33699.635172605515, 'accumulated_eval_time': 1178.4537107944489, 'accumulated_logging_time': 2.5008058547973633, 'global_step': 99213, 'preemption_count': 0}), (100717, {'train/accuracy': 0.8254942297935486, 'train/loss': 0.8944934010505676, 'validation/accuracy': 0.7102400064468384, 'validation/loss': 1.3767321109771729, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 2.0282983779907227, 'test/num_examples': 10000, 'score': 34209.61276984215, 'total_duration': 35411.85849046707, 'accumulated_submission_time': 34209.61276984215, 'accumulated_eval_time': 1196.1693103313446, 'accumulated_logging_time': 2.546966552734375, 'global_step': 100717, 'preemption_count': 0}), (102222, {'train/accuracy': 0.8199936151504517, 'train/loss': 0.9147710800170898, 'validation/accuracy': 0.7091999650001526, 'validation/loss': 1.3878830671310425, 'validation/num_examples': 50000, 'test/accuracy': 0.5827000141143799, 'test/loss': 2.037583112716675, 'test/num_examples': 10000, 'score': 34719.75500845909, 'total_duration': 35939.416763305664, 'accumulated_submission_time': 34719.75500845909, 'accumulated_eval_time': 1213.4867506027222, 'accumulated_logging_time': 2.5913703441619873, 'global_step': 102222, 'preemption_count': 0}), (103726, {'train/accuracy': 0.8092314600944519, 'train/loss': 0.9412323236465454, 'validation/accuracy': 0.7082799673080444, 'validation/loss': 1.3787775039672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 2.0294604301452637, 'test/num_examples': 10000, 'score': 35229.8086771965, 'total_duration': 36467.01249408722, 'accumulated_submission_time': 35229.8086771965, 'accumulated_eval_time': 1230.9158039093018, 'accumulated_logging_time': 2.652010679244995, 'global_step': 103726, 'preemption_count': 0}), (105231, {'train/accuracy': 0.810546875, 'train/loss': 0.9459798336029053, 'validation/accuracy': 0.7136399745941162, 'validation/loss': 1.372373104095459, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 2.013390302658081, 'test/num_examples': 10000, 'score': 35739.95236110687, 'total_duration': 36994.68914103508, 'accumulated_submission_time': 35739.95236110687, 'accumulated_eval_time': 1248.3516201972961, 'accumulated_logging_time': 2.697594404220581, 'global_step': 105231, 'preemption_count': 0}), (106736, {'train/accuracy': 0.8152502775192261, 'train/loss': 0.9214245080947876, 'validation/accuracy': 0.7125799655914307, 'validation/loss': 1.3635106086730957, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.9973350763320923, 'test/num_examples': 10000, 'score': 36250.164659023285, 'total_duration': 37522.34675478935, 'accumulated_submission_time': 36250.164659023285, 'accumulated_eval_time': 1265.6972844600677, 'accumulated_logging_time': 2.745483875274658, 'global_step': 106736, 'preemption_count': 0}), (108240, {'train/accuracy': 0.8124202489852905, 'train/loss': 0.9675102829933167, 'validation/accuracy': 0.7174199819564819, 'validation/loss': 1.3825063705444336, 'validation/num_examples': 50000, 'test/accuracy': 0.5960000157356262, 'test/loss': 2.006366014480591, 'test/num_examples': 10000, 'score': 36760.113042354584, 'total_duration': 38049.814202070236, 'accumulated_submission_time': 36760.113042354584, 'accumulated_eval_time': 1283.1132173538208, 'accumulated_logging_time': 2.795067071914673, 'global_step': 108240, 'preemption_count': 0}), (109745, {'train/accuracy': 0.8219866156578064, 'train/loss': 0.869674026966095, 'validation/accuracy': 0.7159799933433533, 'validation/loss': 1.3264567852020264, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.9520862102508545, 'test/num_examples': 10000, 'score': 37270.30475926399, 'total_duration': 38577.689265728, 'accumulated_submission_time': 37270.30475926399, 'accumulated_eval_time': 1300.696792602539, 'accumulated_logging_time': 2.8408803939819336, 'global_step': 109745, 'preemption_count': 0}), (111249, {'train/accuracy': 0.8328284025192261, 'train/loss': 0.8718942999839783, 'validation/accuracy': 0.7133600115776062, 'validation/loss': 1.374558448791504, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 2.0042407512664795, 'test/num_examples': 10000, 'score': 37780.53218817711, 'total_duration': 39105.6652610302, 'accumulated_submission_time': 37780.53218817711, 'accumulated_eval_time': 1318.3463683128357, 'accumulated_logging_time': 2.886791706085205, 'global_step': 111249, 'preemption_count': 0}), (112754, {'train/accuracy': 0.829121470451355, 'train/loss': 0.8686312437057495, 'validation/accuracy': 0.7148799896240234, 'validation/loss': 1.34848952293396, 'validation/num_examples': 50000, 'test/accuracy': 0.5905000567436218, 'test/loss': 2.003549337387085, 'test/num_examples': 10000, 'score': 38290.755179166794, 'total_duration': 39633.61887669563, 'accumulated_submission_time': 38290.755179166794, 'accumulated_eval_time': 1335.973201751709, 'accumulated_logging_time': 2.9371728897094727, 'global_step': 112754, 'preemption_count': 0}), (114259, {'train/accuracy': 0.8265106678009033, 'train/loss': 0.8988329768180847, 'validation/accuracy': 0.7185800075531006, 'validation/loss': 1.36379075050354, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.997839331626892, 'test/num_examples': 10000, 'score': 38800.871701955795, 'total_duration': 40161.65034651756, 'accumulated_submission_time': 38800.871701955795, 'accumulated_eval_time': 1353.7831537723541, 'accumulated_logging_time': 2.9890265464782715, 'global_step': 114259, 'preemption_count': 0}), (115764, {'train/accuracy': 0.8270089030265808, 'train/loss': 0.9045054316520691, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.3632978200912476, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 2.0048916339874268, 'test/num_examples': 10000, 'score': 39310.83686709404, 'total_duration': 40689.02972865105, 'accumulated_submission_time': 39310.83686709404, 'accumulated_eval_time': 1371.0964777469635, 'accumulated_logging_time': 3.0382204055786133, 'global_step': 115764, 'preemption_count': 0}), (117269, {'train/accuracy': 0.8316724896430969, 'train/loss': 0.8688330054283142, 'validation/accuracy': 0.7247799634933472, 'validation/loss': 1.3182138204574585, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.9583295583724976, 'test/num_examples': 10000, 'score': 39820.8876388073, 'total_duration': 41216.72868323326, 'accumulated_submission_time': 39820.8876388073, 'accumulated_eval_time': 1388.6389904022217, 'accumulated_logging_time': 3.0894229412078857, 'global_step': 117269, 'preemption_count': 0}), (118774, {'train/accuracy': 0.8371332883834839, 'train/loss': 0.8562281131744385, 'validation/accuracy': 0.725659966468811, 'validation/loss': 1.3277863264083862, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.9448498487472534, 'test/num_examples': 10000, 'score': 40331.050414800644, 'total_duration': 41744.96152448654, 'accumulated_submission_time': 40331.050414800644, 'accumulated_eval_time': 1406.6010098457336, 'accumulated_logging_time': 3.143878936767578, 'global_step': 118774, 'preemption_count': 0}), (120279, {'train/accuracy': 0.8495694994926453, 'train/loss': 0.8090940117835999, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.3468101024627686, 'validation/num_examples': 50000, 'test/accuracy': 0.5987000465393066, 'test/loss': 1.9820739030838013, 'test/num_examples': 10000, 'score': 40841.25688076019, 'total_duration': 42272.75825166702, 'accumulated_submission_time': 40841.25688076019, 'accumulated_eval_time': 1424.0867023468018, 'accumulated_logging_time': 3.195374011993408, 'global_step': 120279, 'preemption_count': 0}), (121783, {'train/accuracy': 0.8489118218421936, 'train/loss': 0.7738045454025269, 'validation/accuracy': 0.7283599972724915, 'validation/loss': 1.285322904586792, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.9310460090637207, 'test/num_examples': 10000, 'score': 41351.40269494057, 'total_duration': 42800.45030093193, 'accumulated_submission_time': 41351.40269494057, 'accumulated_eval_time': 1441.527908563614, 'accumulated_logging_time': 3.246919870376587, 'global_step': 121783, 'preemption_count': 0}), (123288, {'train/accuracy': 0.8492506146430969, 'train/loss': 0.7951707243919373, 'validation/accuracy': 0.7295199632644653, 'validation/loss': 1.2949326038360596, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.9248424768447876, 'test/num_examples': 10000, 'score': 41861.53848028183, 'total_duration': 43328.20562505722, 'accumulated_submission_time': 41861.53848028183, 'accumulated_eval_time': 1459.0470685958862, 'accumulated_logging_time': 3.294360637664795, 'global_step': 123288, 'preemption_count': 0}), (124793, {'train/accuracy': 0.8462810516357422, 'train/loss': 0.8076683878898621, 'validation/accuracy': 0.7290399670600891, 'validation/loss': 1.3021578788757324, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.9222787618637085, 'test/num_examples': 10000, 'score': 42371.742612838745, 'total_duration': 43855.968982219696, 'accumulated_submission_time': 42371.742612838745, 'accumulated_eval_time': 1476.5027883052826, 'accumulated_logging_time': 3.3449392318725586, 'global_step': 124793, 'preemption_count': 0}), (126297, {'train/accuracy': 0.8422752022743225, 'train/loss': 0.7997431755065918, 'validation/accuracy': 0.7246800065040588, 'validation/loss': 1.2923554182052612, 'validation/num_examples': 50000, 'test/accuracy': 0.596500039100647, 'test/loss': 1.9440224170684814, 'test/num_examples': 10000, 'score': 42881.6758556366, 'total_duration': 44383.64790058136, 'accumulated_submission_time': 42881.6758556366, 'accumulated_eval_time': 1494.1439380645752, 'accumulated_logging_time': 3.396597146987915, 'global_step': 126297, 'preemption_count': 0}), (127801, {'train/accuracy': 0.8557876348495483, 'train/loss': 0.7706797122955322, 'validation/accuracy': 0.7314199805259705, 'validation/loss': 1.2801084518432617, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8942350149154663, 'test/num_examples': 10000, 'score': 43391.6541454792, 'total_duration': 44911.067249298096, 'accumulated_submission_time': 43391.6541454792, 'accumulated_eval_time': 1511.4806642532349, 'accumulated_logging_time': 3.4473884105682373, 'global_step': 127801, 'preemption_count': 0}), (129305, {'train/accuracy': 0.8703563213348389, 'train/loss': 0.682815670967102, 'validation/accuracy': 0.7312600016593933, 'validation/loss': 1.259967565536499, 'validation/num_examples': 50000, 'test/accuracy': 0.6118000149726868, 'test/loss': 1.8826649188995361, 'test/num_examples': 10000, 'score': 43901.69043469429, 'total_duration': 45438.67550635338, 'accumulated_submission_time': 43901.69043469429, 'accumulated_eval_time': 1528.9485657215118, 'accumulated_logging_time': 3.4982686042785645, 'global_step': 129305, 'preemption_count': 0}), (130810, {'train/accuracy': 0.869559109210968, 'train/loss': 0.7021733522415161, 'validation/accuracy': 0.7360599637031555, 'validation/loss': 1.262072205543518, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.8829753398895264, 'test/num_examples': 10000, 'score': 44411.83263254166, 'total_duration': 45966.23621058464, 'accumulated_submission_time': 44411.83263254166, 'accumulated_eval_time': 1546.2606403827667, 'accumulated_logging_time': 3.5511486530303955, 'global_step': 130810, 'preemption_count': 0}), (132313, {'train/accuracy': 0.8615074753761292, 'train/loss': 0.731268048286438, 'validation/accuracy': 0.7324999570846558, 'validation/loss': 1.2648930549621582, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.890793800354004, 'test/num_examples': 10000, 'score': 44921.73423457146, 'total_duration': 46493.515330314636, 'accumulated_submission_time': 44921.73423457146, 'accumulated_eval_time': 1563.5348060131073, 'accumulated_logging_time': 3.6022284030914307, 'global_step': 132313, 'preemption_count': 0}), (133817, {'train/accuracy': 0.8706154227256775, 'train/loss': 0.7510040998458862, 'validation/accuracy': 0.7382999658584595, 'validation/loss': 1.2810701131820679, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.9176183938980103, 'test/num_examples': 10000, 'score': 45431.71617555618, 'total_duration': 47021.19475483894, 'accumulated_submission_time': 45431.71617555618, 'accumulated_eval_time': 1581.1292176246643, 'accumulated_logging_time': 3.6521716117858887, 'global_step': 133817, 'preemption_count': 0}), (135322, {'train/accuracy': 0.8668088316917419, 'train/loss': 0.6989973187446594, 'validation/accuracy': 0.7389199733734131, 'validation/loss': 1.2317496538162231, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.8706231117248535, 'test/num_examples': 10000, 'score': 45941.8173930645, 'total_duration': 47548.92354273796, 'accumulated_submission_time': 45941.8173930645, 'accumulated_eval_time': 1598.651022195816, 'accumulated_logging_time': 3.7039690017700195, 'global_step': 135322, 'preemption_count': 0}), (136826, {'train/accuracy': 0.8732461333274841, 'train/loss': 0.7015025019645691, 'validation/accuracy': 0.7404599785804749, 'validation/loss': 1.2484326362609863, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.886087417602539, 'test/num_examples': 10000, 'score': 46451.72684264183, 'total_duration': 48076.31215286255, 'accumulated_submission_time': 46451.72684264183, 'accumulated_eval_time': 1616.0231895446777, 'accumulated_logging_time': 3.759093761444092, 'global_step': 136826, 'preemption_count': 0}), (138331, {'train/accuracy': 0.8901665806770325, 'train/loss': 0.6423742771148682, 'validation/accuracy': 0.7400799989700317, 'validation/loss': 1.2568217515945435, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.88319993019104, 'test/num_examples': 10000, 'score': 46961.66208958626, 'total_duration': 48603.79757499695, 'accumulated_submission_time': 46961.66208958626, 'accumulated_eval_time': 1633.4662518501282, 'accumulated_logging_time': 3.812281608581543, 'global_step': 138331, 'preemption_count': 0}), (139836, {'train/accuracy': 0.8897680044174194, 'train/loss': 0.627855658531189, 'validation/accuracy': 0.74235999584198, 'validation/loss': 1.2312299013137817, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.8507357835769653, 'test/num_examples': 10000, 'score': 47471.80485224724, 'total_duration': 49131.303370952606, 'accumulated_submission_time': 47471.80485224724, 'accumulated_eval_time': 1650.7227218151093, 'accumulated_logging_time': 3.866595983505249, 'global_step': 139836, 'preemption_count': 0}), (141341, {'train/accuracy': 0.886738657951355, 'train/loss': 0.6396247744560242, 'validation/accuracy': 0.745639979839325, 'validation/loss': 1.2290503978729248, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.8512567281723022, 'test/num_examples': 10000, 'score': 47981.99515795708, 'total_duration': 49659.100927591324, 'accumulated_submission_time': 47981.99515795708, 'accumulated_eval_time': 1668.2225155830383, 'accumulated_logging_time': 3.9220056533813477, 'global_step': 141341, 'preemption_count': 0}), (142846, {'train/accuracy': 0.8891502022743225, 'train/loss': 0.6449418067932129, 'validation/accuracy': 0.7464199662208557, 'validation/loss': 1.2233476638793945, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.851295828819275, 'test/num_examples': 10000, 'score': 48492.108885765076, 'total_duration': 50186.51897478104, 'accumulated_submission_time': 48492.108885765076, 'accumulated_eval_time': 1685.4138662815094, 'accumulated_logging_time': 3.9819884300231934, 'global_step': 142846, 'preemption_count': 0}), (144351, {'train/accuracy': 0.8878347873687744, 'train/loss': 0.636131763458252, 'validation/accuracy': 0.7448999881744385, 'validation/loss': 1.2222989797592163, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.849866271018982, 'test/num_examples': 10000, 'score': 49002.305592775345, 'total_duration': 50714.08239650726, 'accumulated_submission_time': 49002.305592775345, 'accumulated_eval_time': 1702.677888393402, 'accumulated_logging_time': 4.033036470413208, 'global_step': 144351, 'preemption_count': 0}), (145856, {'train/accuracy': 0.8903858065605164, 'train/loss': 0.6295886039733887, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.2230639457702637, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.8473961353302002, 'test/num_examples': 10000, 'score': 49512.394984960556, 'total_duration': 51241.637093544006, 'accumulated_submission_time': 49512.394984960556, 'accumulated_eval_time': 1720.0401091575623, 'accumulated_logging_time': 4.083997964859009, 'global_step': 145856, 'preemption_count': 0}), (147360, {'train/accuracy': 0.9123684167861938, 'train/loss': 0.5474082827568054, 'validation/accuracy': 0.7480799555778503, 'validation/loss': 1.2101439237594604, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8346625566482544, 'test/num_examples': 10000, 'score': 50022.29506134987, 'total_duration': 51769.10906100273, 'accumulated_submission_time': 50022.29506134987, 'accumulated_eval_time': 1737.5025906562805, 'accumulated_logging_time': 4.140517711639404, 'global_step': 147360, 'preemption_count': 0}), (148864, {'train/accuracy': 0.9058912396430969, 'train/loss': 0.5808451175689697, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.2156970500946045, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8480470180511475, 'test/num_examples': 10000, 'score': 50532.36496210098, 'total_duration': 52296.71101593971, 'accumulated_submission_time': 50532.36496210098, 'accumulated_eval_time': 1754.9088788032532, 'accumulated_logging_time': 4.21375036239624, 'global_step': 148864, 'preemption_count': 0}), (150368, {'train/accuracy': 0.9048748016357422, 'train/loss': 0.5630061030387878, 'validation/accuracy': 0.7488600015640259, 'validation/loss': 1.1980425119400024, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.820862054824829, 'test/num_examples': 10000, 'score': 51042.27467918396, 'total_duration': 52824.65858411789, 'accumulated_submission_time': 51042.27467918396, 'accumulated_eval_time': 1772.8417789936066, 'accumulated_logging_time': 4.267144680023193, 'global_step': 150368, 'preemption_count': 0}), (151873, {'train/accuracy': 0.9094387292861938, 'train/loss': 0.5636129379272461, 'validation/accuracy': 0.7510600090026855, 'validation/loss': 1.2021054029464722, 'validation/num_examples': 50000, 'test/accuracy': 0.6339000463485718, 'test/loss': 1.816255807876587, 'test/num_examples': 10000, 'score': 51552.41488814354, 'total_duration': 53352.32368469238, 'accumulated_submission_time': 51552.41488814354, 'accumulated_eval_time': 1790.2585053443909, 'accumulated_logging_time': 4.323015451431274, 'global_step': 151873, 'preemption_count': 0}), (153378, {'train/accuracy': 0.9103156924247742, 'train/loss': 0.563623309135437, 'validation/accuracy': 0.7524799704551697, 'validation/loss': 1.2048771381378174, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.829545259475708, 'test/num_examples': 10000, 'score': 52062.60071802139, 'total_duration': 53880.12092804909, 'accumulated_submission_time': 52062.60071802139, 'accumulated_eval_time': 1807.7640812397003, 'accumulated_logging_time': 4.375983238220215, 'global_step': 153378, 'preemption_count': 0}), (154883, {'train/accuracy': 0.9117307066917419, 'train/loss': 0.5565671324729919, 'validation/accuracy': 0.752020001411438, 'validation/loss': 1.2094497680664062, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.829664707183838, 'test/num_examples': 10000, 'score': 52572.8312189579, 'total_duration': 54407.95564079285, 'accumulated_submission_time': 52572.8312189579, 'accumulated_eval_time': 1825.2644836902618, 'accumulated_logging_time': 4.426474571228027, 'global_step': 154883, 'preemption_count': 0}), (156387, {'train/accuracy': 0.9270368218421936, 'train/loss': 0.5088555812835693, 'validation/accuracy': 0.7543599605560303, 'validation/loss': 1.2003954648971558, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.8163809776306152, 'test/num_examples': 10000, 'score': 53082.78435873985, 'total_duration': 54935.78036189079, 'accumulated_submission_time': 53082.78435873985, 'accumulated_eval_time': 1843.0286478996277, 'accumulated_logging_time': 4.481486082077026, 'global_step': 156387, 'preemption_count': 0}), (157892, {'train/accuracy': 0.9226323366165161, 'train/loss': 0.5026780366897583, 'validation/accuracy': 0.7541999816894531, 'validation/loss': 1.1845531463623047, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.8125512599945068, 'test/num_examples': 10000, 'score': 53592.87838935852, 'total_duration': 55464.342358112335, 'accumulated_submission_time': 53592.87838935852, 'accumulated_eval_time': 1861.3870635032654, 'accumulated_logging_time': 4.539382696151733, 'global_step': 157892, 'preemption_count': 0}), (159396, {'train/accuracy': 0.9216158986091614, 'train/loss': 0.5223816633224487, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.198951244354248, 'validation/num_examples': 50000, 'test/accuracy': 0.64000004529953, 'test/loss': 1.8155686855316162, 'test/num_examples': 10000, 'score': 54102.98790359497, 'total_duration': 55991.9786362648, 'accumulated_submission_time': 54102.98790359497, 'accumulated_eval_time': 1878.803019285202, 'accumulated_logging_time': 4.597425699234009, 'global_step': 159396, 'preemption_count': 0}), (160900, {'train/accuracy': 0.9227519035339355, 'train/loss': 0.5238286256790161, 'validation/accuracy': 0.7569599747657776, 'validation/loss': 1.1966742277145386, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.8186490535736084, 'test/num_examples': 10000, 'score': 54612.884423971176, 'total_duration': 56519.49779844284, 'accumulated_submission_time': 54612.884423971176, 'accumulated_eval_time': 1896.3183450698853, 'accumulated_logging_time': 4.652040481567383, 'global_step': 160900, 'preemption_count': 0}), (162405, {'train/accuracy': 0.9255221486091614, 'train/loss': 0.5173733234405518, 'validation/accuracy': 0.7567399740219116, 'validation/loss': 1.1949071884155273, 'validation/num_examples': 50000, 'test/accuracy': 0.6376000046730042, 'test/loss': 1.8151713609695435, 'test/num_examples': 10000, 'score': 55122.97531867027, 'total_duration': 57047.208235025406, 'accumulated_submission_time': 55122.97531867027, 'accumulated_eval_time': 1913.817587852478, 'accumulated_logging_time': 4.720116376876831, 'global_step': 162405, 'preemption_count': 0}), (163910, {'train/accuracy': 0.9263990521430969, 'train/loss': 0.5073674917221069, 'validation/accuracy': 0.7562999725341797, 'validation/loss': 1.1926721334457397, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.8144419193267822, 'test/num_examples': 10000, 'score': 55633.16078686714, 'total_duration': 57575.13672566414, 'accumulated_submission_time': 55633.16078686714, 'accumulated_eval_time': 1931.4495086669922, 'accumulated_logging_time': 4.778955459594727, 'global_step': 163910, 'preemption_count': 0}), (165415, {'train/accuracy': 0.93558669090271, 'train/loss': 0.4720862805843353, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.1844284534454346, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.8039889335632324, 'test/num_examples': 10000, 'score': 56143.32125544548, 'total_duration': 58102.80092835426, 'accumulated_submission_time': 56143.32125544548, 'accumulated_eval_time': 1948.845008611679, 'accumulated_logging_time': 4.834153413772583, 'global_step': 165415, 'preemption_count': 0}), (166919, {'train/accuracy': 0.9346898794174194, 'train/loss': 0.47572416067123413, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.1815327405929565, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.7958463430404663, 'test/num_examples': 10000, 'score': 56653.3142747879, 'total_duration': 58630.24149942398, 'accumulated_submission_time': 56653.3142747879, 'accumulated_eval_time': 1966.1763620376587, 'accumulated_logging_time': 4.896468162536621, 'global_step': 166919, 'preemption_count': 0}), (168424, {'train/accuracy': 0.9356863498687744, 'train/loss': 0.46207720041275024, 'validation/accuracy': 0.7594199776649475, 'validation/loss': 1.1747729778289795, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.7989342212677002, 'test/num_examples': 10000, 'score': 57163.469307899475, 'total_duration': 59157.79861664772, 'accumulated_submission_time': 57163.469307899475, 'accumulated_eval_time': 1983.467565536499, 'accumulated_logging_time': 4.954056978225708, 'global_step': 168424, 'preemption_count': 0}), (169929, {'train/accuracy': 0.9359853267669678, 'train/loss': 0.4732051193714142, 'validation/accuracy': 0.7616399526596069, 'validation/loss': 1.1783033609390259, 'validation/num_examples': 50000, 'test/accuracy': 0.6430000066757202, 'test/loss': 1.7972735166549683, 'test/num_examples': 10000, 'score': 57673.66196155548, 'total_duration': 59685.62716174126, 'accumulated_submission_time': 57673.66196155548, 'accumulated_eval_time': 2000.991406917572, 'accumulated_logging_time': 5.0141987800598145, 'global_step': 169929, 'preemption_count': 0}), (171434, {'train/accuracy': 0.9338527917861938, 'train/loss': 0.47603392601013184, 'validation/accuracy': 0.7612999677658081, 'validation/loss': 1.17757248878479, 'validation/num_examples': 50000, 'test/accuracy': 0.6459000110626221, 'test/loss': 1.798004150390625, 'test/num_examples': 10000, 'score': 58183.85387516022, 'total_duration': 60213.344397068024, 'accumulated_submission_time': 58183.85387516022, 'accumulated_eval_time': 2018.4029893875122, 'accumulated_logging_time': 5.076361656188965, 'global_step': 171434, 'preemption_count': 0}), (172938, {'train/accuracy': 0.9358657598495483, 'train/loss': 0.4723958671092987, 'validation/accuracy': 0.7612400054931641, 'validation/loss': 1.1811754703521729, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.8015018701553345, 'test/num_examples': 10000, 'score': 58693.92332792282, 'total_duration': 60740.91999530792, 'accumulated_submission_time': 58693.92332792282, 'accumulated_eval_time': 2035.8027634620667, 'accumulated_logging_time': 5.131362676620483, 'global_step': 172938, 'preemption_count': 0}), (174442, {'train/accuracy': 0.938875138759613, 'train/loss': 0.4575260877609253, 'validation/accuracy': 0.7615000009536743, 'validation/loss': 1.1745693683624268, 'validation/num_examples': 50000, 'test/accuracy': 0.6446000337600708, 'test/loss': 1.7955400943756104, 'test/num_examples': 10000, 'score': 59204.02347588539, 'total_duration': 61268.683440208435, 'accumulated_submission_time': 59204.02347588539, 'accumulated_eval_time': 2053.356356859207, 'accumulated_logging_time': 5.18861722946167, 'global_step': 174442, 'preemption_count': 0}), (175947, {'train/accuracy': 0.9404296875, 'train/loss': 0.45358148217201233, 'validation/accuracy': 0.7617799639701843, 'validation/loss': 1.1750295162200928, 'validation/num_examples': 50000, 'test/accuracy': 0.6451000571250916, 'test/loss': 1.7953126430511475, 'test/num_examples': 10000, 'score': 59714.15299129486, 'total_duration': 61796.51736474037, 'accumulated_submission_time': 59714.15299129486, 'accumulated_eval_time': 2070.9494745731354, 'accumulated_logging_time': 5.2475292682647705, 'global_step': 175947, 'preemption_count': 0}), (177451, {'train/accuracy': 0.9409877061843872, 'train/loss': 0.45723941922187805, 'validation/accuracy': 0.762179970741272, 'validation/loss': 1.175032138824463, 'validation/num_examples': 50000, 'test/accuracy': 0.647100031375885, 'test/loss': 1.7913333177566528, 'test/num_examples': 10000, 'score': 60224.07753944397, 'total_duration': 62324.28744530678, 'accumulated_submission_time': 60224.07753944397, 'accumulated_eval_time': 2088.686345100403, 'accumulated_logging_time': 5.303653001785278, 'global_step': 177451, 'preemption_count': 0}), (178955, {'train/accuracy': 0.9386160373687744, 'train/loss': 0.4601021409034729, 'validation/accuracy': 0.7621399760246277, 'validation/loss': 1.1740968227386475, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7911723852157593, 'test/num_examples': 10000, 'score': 60734.13635802269, 'total_duration': 62851.805837869644, 'accumulated_submission_time': 60734.13635802269, 'accumulated_eval_time': 2106.0333411693573, 'accumulated_logging_time': 5.3640947341918945, 'global_step': 178955, 'preemption_count': 0}), (180459, {'train/accuracy': 0.9403499364852905, 'train/loss': 0.45583006739616394, 'validation/accuracy': 0.7623599767684937, 'validation/loss': 1.1769626140594482, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.7931833267211914, 'test/num_examples': 10000, 'score': 61244.19420695305, 'total_duration': 63379.31936812401, 'accumulated_submission_time': 61244.19420695305, 'accumulated_eval_time': 2123.3806269168854, 'accumulated_logging_time': 5.421769380569458, 'global_step': 180459, 'preemption_count': 0}), (181963, {'train/accuracy': 0.9390943646430969, 'train/loss': 0.45994701981544495, 'validation/accuracy': 0.7626799941062927, 'validation/loss': 1.1733717918395996, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.7920598983764648, 'test/num_examples': 10000, 'score': 61754.27343964577, 'total_duration': 63907.06888461113, 'accumulated_submission_time': 61754.27343964577, 'accumulated_eval_time': 2140.9366660118103, 'accumulated_logging_time': 5.483500957489014, 'global_step': 181963, 'preemption_count': 0}), (183465, {'train/accuracy': 0.9401307106018066, 'train/loss': 0.45374590158462524, 'validation/accuracy': 0.7622999548912048, 'validation/loss': 1.1755332946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.7937980890274048, 'test/num_examples': 10000, 'score': 62263.354476451874, 'total_duration': 64434.93941235542, 'accumulated_submission_time': 62263.354476451874, 'accumulated_eval_time': 2158.5463812351227, 'accumulated_logging_time': 6.609484672546387, 'global_step': 183465, 'preemption_count': 0}), (184970, {'train/accuracy': 0.9401506781578064, 'train/loss': 0.45149460434913635, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 1.1717240810394287, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7898361682891846, 'test/num_examples': 10000, 'score': 62773.463541030884, 'total_duration': 64962.63630104065, 'accumulated_submission_time': 62773.463541030884, 'accumulated_eval_time': 2176.019249677658, 'accumulated_logging_time': 6.671429634094238, 'global_step': 184970, 'preemption_count': 0})], 'global_step': 185663}
I0128 07:17:10.896363 140027215431488 submission_runner.py:586] Timing: 63008.241268634796
I0128 07:17:10.896453 140027215431488 submission_runner.py:588] Total number of evals: 124
I0128 07:17:10.896499 140027215431488 submission_runner.py:589] ====================
I0128 07:17:10.896548 140027215431488 submission_runner.py:542] Using RNG seed 3078694106
I0128 07:17:10.898309 140027215431488 submission_runner.py:551] --- Tuning run 3/5 ---
I0128 07:17:10.898461 140027215431488 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3.
I0128 07:17:10.899793 140027215431488 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3/hparams.json.
I0128 07:17:10.900643 140027215431488 submission_runner.py:206] Initializing dataset.
I0128 07:17:10.910151 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0128 07:17:10.921208 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0128 07:17:11.107743 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0128 07:17:11.356123 140027215431488 submission_runner.py:213] Initializing model.
I0128 07:17:17.779396 140027215431488 submission_runner.py:255] Initializing optimizer.
I0128 07:17:18.196935 140027215431488 submission_runner.py:262] Initializing metrics bundle.
I0128 07:17:18.197165 140027215431488 submission_runner.py:280] Initializing checkpoint and logger.
I0128 07:17:18.211482 140027215431488 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0128 07:17:18.211655 140027215431488 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0128 07:17:29.981860 140027215431488 logger_utils.py:220] Unable to record git information. Continuing without it.
I0128 07:17:41.286906 140027215431488 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3/flags_0.json.
I0128 07:17:41.293957 140027215431488 submission_runner.py:314] Starting training loop.
I0128 07:18:17.426965 139865760950016 logging_writer.py:48] [0] global_step=0, grad_norm=0.6595183610916138, loss=6.916248798370361
I0128 07:18:17.442355 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:18:23.757476 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:18:32.646878 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:18:35.227935 140027215431488 submission_runner.py:408] Time since start: 53.93s, 	Step: 1, 	{'train/accuracy': 0.0006776147638447583, 'train/loss': 6.912829399108887, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 36.148282289505005, 'total_duration': 53.93392467498779, 'accumulated_submission_time': 36.148282289505005, 'accumulated_eval_time': 17.785524606704712, 'accumulated_logging_time': 0}
I0128 07:18:35.238286 139865232500480 logging_writer.py:48] [1] accumulated_eval_time=17.785525, accumulated_logging_time=0, accumulated_submission_time=36.148282, global_step=1, preemption_count=0, score=36.148282, test/accuracy=0.001300, test/loss=6.911728, test/num_examples=10000, total_duration=53.933925, train/accuracy=0.000678, train/loss=6.912829, validation/accuracy=0.000680, validation/loss=6.912051, validation/num_examples=50000
I0128 07:19:09.198062 139865240893184 logging_writer.py:48] [100] global_step=100, grad_norm=0.6548985838890076, loss=6.903148174285889
I0128 07:19:43.217893 139865232500480 logging_writer.py:48] [200] global_step=200, grad_norm=0.6614153981208801, loss=6.857009410858154
I0128 07:20:17.259042 139865240893184 logging_writer.py:48] [300] global_step=300, grad_norm=0.6998633146286011, loss=6.7762956619262695
I0128 07:20:51.281979 139865232500480 logging_writer.py:48] [400] global_step=400, grad_norm=0.771919310092926, loss=6.65867280960083
I0128 07:21:25.327727 139865240893184 logging_writer.py:48] [500] global_step=500, grad_norm=0.8161630630493164, loss=6.548357009887695
I0128 07:21:59.384915 139865232500480 logging_writer.py:48] [600] global_step=600, grad_norm=0.8683827519416809, loss=6.441118240356445
I0128 07:22:33.448072 139865240893184 logging_writer.py:48] [700] global_step=700, grad_norm=0.8838940262794495, loss=6.377496242523193
I0128 07:23:07.514843 139865232500480 logging_writer.py:48] [800] global_step=800, grad_norm=1.0319281816482544, loss=6.282172203063965
I0128 07:23:41.658767 139865240893184 logging_writer.py:48] [900] global_step=900, grad_norm=2.688868761062622, loss=6.1094255447387695
I0128 07:24:15.704869 139865232500480 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9430872201919556, loss=5.97615909576416
I0128 07:24:49.734544 139865240893184 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.7313028573989868, loss=5.93140172958374
I0128 07:25:23.778332 139865232500480 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.9765058755874634, loss=5.772164344787598
I0128 07:25:57.834936 139865240893184 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.3373701572418213, loss=5.708698272705078
I0128 07:26:31.888630 139865232500480 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.551600694656372, loss=5.609274864196777
I0128 07:27:05.379454 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:27:11.705209 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:27:20.531886 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:27:23.060495 140027215431488 submission_runner.py:408] Time since start: 581.77s, 	Step: 1500, 	{'train/accuracy': 0.07029256969690323, 'train/loss': 5.318418025970459, 'validation/accuracy': 0.06721999496221542, 'validation/loss': 5.3727641105651855, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.6236677169799805, 'test/num_examples': 10000, 'score': 546.2268629074097, 'total_duration': 581.7664752006531, 'accumulated_submission_time': 546.2268629074097, 'accumulated_eval_time': 35.466519832611084, 'accumulated_logging_time': 0.0201263427734375}
I0128 07:27:23.081866 139866163582720 logging_writer.py:48] [1500] accumulated_eval_time=35.466520, accumulated_logging_time=0.020126, accumulated_submission_time=546.226863, global_step=1500, preemption_count=0, score=546.226863, test/accuracy=0.048500, test/loss=5.623668, test/num_examples=10000, total_duration=581.766475, train/accuracy=0.070293, train/loss=5.318418, validation/accuracy=0.067220, validation/loss=5.372764, validation/num_examples=50000
I0128 07:27:23.431291 139866171975424 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.590496778488159, loss=5.578326225280762
I0128 07:27:57.441374 139866163582720 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.7318856716156006, loss=5.376523971557617
I0128 07:28:31.477411 139866171975424 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.264232873916626, loss=5.295264720916748
I0128 07:29:05.516059 139866163582720 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7201926708221436, loss=5.296507358551025
I0128 07:29:39.639730 139866171975424 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.859125137329102, loss=5.28366756439209
I0128 07:30:13.703943 139866163582720 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.752840042114258, loss=5.120732307434082
I0128 07:30:47.743525 139866171975424 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.181734085083008, loss=5.022377014160156
I0128 07:31:21.803932 139866163582720 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.882146120071411, loss=5.010536193847656
I0128 07:31:55.883214 139866171975424 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.9569413661956787, loss=4.968424320220947
I0128 07:32:29.946317 139866163582720 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.435662269592285, loss=4.903342247009277
I0128 07:33:04.012932 139866171975424 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.426328182220459, loss=4.854336738586426
I0128 07:33:38.051713 139866163582720 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.744852542877197, loss=4.712309837341309
I0128 07:34:12.095035 139866171975424 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.80287504196167, loss=4.8928632736206055
I0128 07:34:46.149270 139866163582720 logging_writer.py:48] [2800] global_step=2800, grad_norm=9.131086349487305, loss=4.715357780456543
I0128 07:35:20.198630 139866171975424 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.388628005981445, loss=4.566572189331055
I0128 07:35:53.370615 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:35:59.709151 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:36:08.703011 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:36:11.290474 140027215431488 submission_runner.py:408] Time since start: 1110.00s, 	Step: 2999, 	{'train/accuracy': 0.17598053812980652, 'train/loss': 4.236908912658691, 'validation/accuracy': 0.1589599996805191, 'validation/loss': 4.364312648773193, 'validation/num_examples': 50000, 'test/accuracy': 0.11150000244379044, 'test/loss': 4.811929702758789, 'test/num_examples': 10000, 'score': 1056.4533824920654, 'total_duration': 1109.996464729309, 'accumulated_submission_time': 1056.4533824920654, 'accumulated_eval_time': 53.3863730430603, 'accumulated_logging_time': 0.050855398178100586}
I0128 07:36:11.308454 139865240893184 logging_writer.py:48] [2999] accumulated_eval_time=53.386373, accumulated_logging_time=0.050855, accumulated_submission_time=1056.453382, global_step=2999, preemption_count=0, score=1056.453382, test/accuracy=0.111500, test/loss=4.811930, test/num_examples=10000, total_duration=1109.996465, train/accuracy=0.175981, train/loss=4.236909, validation/accuracy=0.158960, validation/loss=4.364313, validation/num_examples=50000
I0128 07:36:12.004728 139865760950016 logging_writer.py:48] [3000] global_step=3000, grad_norm=7.119295120239258, loss=4.587161064147949
I0128 07:36:46.038797 139865240893184 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.333749771118164, loss=4.514320373535156
I0128 07:37:20.100423 139865760950016 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.750542163848877, loss=4.486837387084961
I0128 07:37:54.143299 139865240893184 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.151605129241943, loss=4.570734977722168
I0128 07:38:28.180104 139865760950016 logging_writer.py:48] [3400] global_step=3400, grad_norm=6.873904228210449, loss=4.412144660949707
I0128 07:39:02.260151 139865240893184 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.771461486816406, loss=4.372432231903076
I0128 07:39:36.335345 139865760950016 logging_writer.py:48] [3600] global_step=3600, grad_norm=9.803894996643066, loss=4.32999324798584
I0128 07:40:10.403801 139865240893184 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.693831920623779, loss=4.17305326461792
I0128 07:40:44.492644 139865760950016 logging_writer.py:48] [3800] global_step=3800, grad_norm=6.95440673828125, loss=4.188777446746826
I0128 07:41:18.554367 139865240893184 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.016517639160156, loss=4.117465019226074
I0128 07:41:52.641237 139865760950016 logging_writer.py:48] [4000] global_step=4000, grad_norm=5.111241817474365, loss=4.088887691497803
I0128 07:42:38.833032 139865240893184 logging_writer.py:48] [4100] global_step=4100, grad_norm=7.15620756149292, loss=3.988560676574707
I0128 07:43:24.731416 139865760950016 logging_writer.py:48] [4200] global_step=4200, grad_norm=11.540619850158691, loss=3.8602612018585205
I0128 07:43:58.775191 139865240893184 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.763027667999268, loss=3.9670867919921875
I0128 07:44:32.836378 139865760950016 logging_writer.py:48] [4400] global_step=4400, grad_norm=6.537289142608643, loss=3.920931816101074
I0128 07:44:41.486786 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:44:47.714570 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:44:56.783200 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:44:59.442146 140027215431488 submission_runner.py:408] Time since start: 1638.15s, 	Step: 4427, 	{'train/accuracy': 0.27032843232154846, 'train/loss': 3.506873846054077, 'validation/accuracy': 0.25033998489379883, 'validation/loss': 3.6432690620422363, 'validation/num_examples': 50000, 'test/accuracy': 0.17840000987052917, 'test/loss': 4.235896110534668, 'test/num_examples': 10000, 'score': 1566.5691194534302, 'total_duration': 1638.1481268405914, 'accumulated_submission_time': 1566.5691194534302, 'accumulated_eval_time': 71.34168648719788, 'accumulated_logging_time': 0.08032560348510742}
I0128 07:44:59.465064 139865232500480 logging_writer.py:48] [4427] accumulated_eval_time=71.341686, accumulated_logging_time=0.080326, accumulated_submission_time=1566.569119, global_step=4427, preemption_count=0, score=1566.569119, test/accuracy=0.178400, test/loss=4.235896, test/num_examples=10000, total_duration=1638.148127, train/accuracy=0.270328, train/loss=3.506874, validation/accuracy=0.250340, validation/loss=3.643269, validation/num_examples=50000
I0128 07:45:24.636476 139866163582720 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.342236042022705, loss=3.9652271270751953
I0128 07:45:58.672716 139865232500480 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.882500648498535, loss=3.8413960933685303
I0128 07:46:32.731428 139866163582720 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.864257335662842, loss=3.823047161102295
I0128 07:47:06.778306 139865232500480 logging_writer.py:48] [4800] global_step=4800, grad_norm=6.466907501220703, loss=3.9178690910339355
I0128 07:47:40.825316 139866163582720 logging_writer.py:48] [4900] global_step=4900, grad_norm=6.329927921295166, loss=3.730501651763916
I0128 07:48:14.871788 139865232500480 logging_writer.py:48] [5000] global_step=5000, grad_norm=7.271364212036133, loss=3.8422188758850098
I0128 07:48:48.951251 139866163582720 logging_writer.py:48] [5100] global_step=5100, grad_norm=7.0585103034973145, loss=3.6060242652893066
I0128 07:49:23.060470 139865232500480 logging_writer.py:48] [5200] global_step=5200, grad_norm=6.98061466217041, loss=3.7535018920898438
I0128 07:49:57.121895 139866163582720 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.928884983062744, loss=3.423288583755493
I0128 07:50:31.155428 139865232500480 logging_writer.py:48] [5400] global_step=5400, grad_norm=7.814945697784424, loss=3.617274522781372
I0128 07:51:05.223639 139866163582720 logging_writer.py:48] [5500] global_step=5500, grad_norm=9.506762504577637, loss=3.3899550437927246
I0128 07:51:39.292339 139865232500480 logging_writer.py:48] [5600] global_step=5600, grad_norm=11.508974075317383, loss=3.4387550354003906
I0128 07:52:13.339002 139866163582720 logging_writer.py:48] [5700] global_step=5700, grad_norm=6.7628703117370605, loss=3.3516671657562256
I0128 07:52:47.393228 139865232500480 logging_writer.py:48] [5800] global_step=5800, grad_norm=7.7831244468688965, loss=3.300842761993408
I0128 07:53:21.455069 139866163582720 logging_writer.py:48] [5900] global_step=5900, grad_norm=8.009225845336914, loss=3.4405007362365723
I0128 07:53:29.442638 140027215431488 spec.py:321] Evaluating on the training split.
I0128 07:53:35.665844 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 07:53:44.692515 140027215431488 spec.py:349] Evaluating on the test split.
I0128 07:53:47.316340 140027215431488 submission_runner.py:408] Time since start: 2166.02s, 	Step: 5925, 	{'train/accuracy': 0.3623844087123871, 'train/loss': 2.9141533374786377, 'validation/accuracy': 0.3374599814414978, 'validation/loss': 3.0710597038269043, 'validation/num_examples': 50000, 'test/accuracy': 0.2468000054359436, 'test/loss': 3.741469383239746, 'test/num_examples': 10000, 'score': 2076.4831607341766, 'total_duration': 2166.0223004817963, 'accumulated_submission_time': 2076.4831607341766, 'accumulated_eval_time': 89.21532106399536, 'accumulated_logging_time': 0.11384129524230957}
I0128 07:53:47.337374 139865769342720 logging_writer.py:48] [5925] accumulated_eval_time=89.215321, accumulated_logging_time=0.113841, accumulated_submission_time=2076.483161, global_step=5925, preemption_count=0, score=2076.483161, test/accuracy=0.246800, test/loss=3.741469, test/num_examples=10000, total_duration=2166.022300, train/accuracy=0.362384, train/loss=2.914153, validation/accuracy=0.337460, validation/loss=3.071060, validation/num_examples=50000
I0128 07:54:13.173778 139866180368128 logging_writer.py:48] [6000] global_step=6000, grad_norm=7.12575101852417, loss=3.2389118671417236
I0128 07:54:47.168250 139865769342720 logging_writer.py:48] [6100] global_step=6100, grad_norm=10.360274314880371, loss=3.330749273300171
I0128 07:55:21.211734 139866180368128 logging_writer.py:48] [6200] global_step=6200, grad_norm=7.912719249725342, loss=3.3801708221435547
I0128 07:55:55.328650 139865769342720 logging_writer.py:48] [6300] global_step=6300, grad_norm=8.74277400970459, loss=3.3153820037841797
I0128 07:56:29.375363 139866180368128 logging_writer.py:48] [6400] global_step=6400, grad_norm=8.393473625183105, loss=3.265995740890503
I0128 07:57:03.420593 139865769342720 logging_writer.py:48] [6500] global_step=6500, grad_norm=5.610376358032227, loss=3.207418918609619
I0128 07:57:37.480301 139866180368128 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.415726184844971, loss=3.0972912311553955
I0128 07:58:11.532833 139865769342720 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.9994611740112305, loss=3.1670587062835693
I0128 07:58:45.574298 139866180368128 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.317422866821289, loss=3.0758986473083496
I0128 07:59:19.576722 139865769342720 logging_writer.py:48] [6900] global_step=6900, grad_norm=5.429401397705078, loss=3.0836405754089355
I0128 07:59:53.643367 139866180368128 logging_writer.py:48] [7000] global_step=7000, grad_norm=8.147171974182129, loss=3.1964499950408936
I0128 08:00:27.663372 139865769342720 logging_writer.py:48] [7100] global_step=7100, grad_norm=8.30208683013916, loss=3.001744270324707
I0128 08:01:01.709666 139866180368128 logging_writer.py:48] [7200] global_step=7200, grad_norm=10.353019714355469, loss=2.9579522609710693
I0128 08:01:35.771858 139865769342720 logging_writer.py:48] [7300] global_step=7300, grad_norm=6.53902006149292, loss=3.026155471801758
I0128 08:02:09.903396 139866180368128 logging_writer.py:48] [7400] global_step=7400, grad_norm=7.501332759857178, loss=2.9069039821624756
I0128 08:02:17.563144 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:02:23.787075 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:02:32.789938 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:02:35.420043 140027215431488 submission_runner.py:408] Time since start: 2694.13s, 	Step: 7424, 	{'train/accuracy': 0.43327486515045166, 'train/loss': 2.47650408744812, 'validation/accuracy': 0.38857999444007874, 'validation/loss': 2.7649340629577637, 'validation/num_examples': 50000, 'test/accuracy': 0.289000004529953, 'test/loss': 3.5003814697265625, 'test/num_examples': 10000, 'score': 2586.6451222896576, 'total_duration': 2694.1260364055634, 'accumulated_submission_time': 2586.6451222896576, 'accumulated_eval_time': 107.07217979431152, 'accumulated_logging_time': 0.14467430114746094}
I0128 08:02:35.440147 139865224107776 logging_writer.py:48] [7424] accumulated_eval_time=107.072180, accumulated_logging_time=0.144674, accumulated_submission_time=2586.645122, global_step=7424, preemption_count=0, score=2586.645122, test/accuracy=0.289000, test/loss=3.500381, test/num_examples=10000, total_duration=2694.126036, train/accuracy=0.433275, train/loss=2.476504, validation/accuracy=0.388580, validation/loss=2.764934, validation/num_examples=50000
I0128 08:03:01.663236 139865232500480 logging_writer.py:48] [7500] global_step=7500, grad_norm=5.572835445404053, loss=3.0601234436035156
I0128 08:03:35.687364 139865224107776 logging_writer.py:48] [7600] global_step=7600, grad_norm=5.952956676483154, loss=2.8641693592071533
I0128 08:04:09.741266 139865232500480 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.782786846160889, loss=2.906578540802002
I0128 08:04:43.814093 139865224107776 logging_writer.py:48] [7800] global_step=7800, grad_norm=4.274563789367676, loss=2.875405788421631
I0128 08:05:17.838727 139865232500480 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.622923851013184, loss=2.903270721435547
I0128 08:05:51.880962 139865224107776 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.926905632019043, loss=2.8919730186462402
I0128 08:06:25.905764 139865232500480 logging_writer.py:48] [8100] global_step=8100, grad_norm=7.5763654708862305, loss=2.8474960327148438
I0128 08:06:59.945230 139865224107776 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.275371551513672, loss=2.7782137393951416
I0128 08:07:34.004732 139865232500480 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.798210859298706, loss=2.768242835998535
I0128 08:08:08.040777 139865224107776 logging_writer.py:48] [8400] global_step=8400, grad_norm=5.136956691741943, loss=2.7560908794403076
I0128 08:08:42.168704 139865232500480 logging_writer.py:48] [8500] global_step=8500, grad_norm=6.685697078704834, loss=2.9410035610198975
I0128 08:09:16.203113 139865224107776 logging_writer.py:48] [8600] global_step=8600, grad_norm=5.739943504333496, loss=2.825465202331543
I0128 08:09:50.218355 139865232500480 logging_writer.py:48] [8700] global_step=8700, grad_norm=4.636683464050293, loss=2.712751626968384
I0128 08:10:24.234687 139865224107776 logging_writer.py:48] [8800] global_step=8800, grad_norm=5.0005059242248535, loss=2.850341320037842
I0128 08:10:58.303544 139865232500480 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.917244911193848, loss=2.606637477874756
I0128 08:11:05.591920 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:11:12.607306 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:11:21.722882 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:11:24.380722 140027215431488 submission_runner.py:408] Time since start: 3223.09s, 	Step: 8923, 	{'train/accuracy': 0.5002790093421936, 'train/loss': 2.140385389328003, 'validation/accuracy': 0.4495999813079834, 'validation/loss': 2.4287495613098145, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.144484043121338, 'test/num_examples': 10000, 'score': 3096.7352623939514, 'total_duration': 3223.0867116451263, 'accumulated_submission_time': 3096.7352623939514, 'accumulated_eval_time': 125.86094260215759, 'accumulated_logging_time': 0.1740093231201172}
I0128 08:11:24.400140 139866171975424 logging_writer.py:48] [8923] accumulated_eval_time=125.860943, accumulated_logging_time=0.174009, accumulated_submission_time=3096.735262, global_step=8923, preemption_count=0, score=3096.735262, test/accuracy=0.342700, test/loss=3.144484, test/num_examples=10000, total_duration=3223.086712, train/accuracy=0.500279, train/loss=2.140385, validation/accuracy=0.449600, validation/loss=2.428750, validation/num_examples=50000
I0128 08:11:50.937219 139866180368128 logging_writer.py:48] [9000] global_step=9000, grad_norm=7.681654930114746, loss=2.868964910507202
I0128 08:12:24.964169 139866171975424 logging_writer.py:48] [9100] global_step=9100, grad_norm=6.587127685546875, loss=2.6505002975463867
I0128 08:12:58.998355 139866180368128 logging_writer.py:48] [9200] global_step=9200, grad_norm=7.900885581970215, loss=2.763538360595703
I0128 08:13:33.019974 139866171975424 logging_writer.py:48] [9300] global_step=9300, grad_norm=8.045018196105957, loss=2.697023868560791
I0128 08:14:07.017619 139866180368128 logging_writer.py:48] [9400] global_step=9400, grad_norm=7.388261795043945, loss=2.5832934379577637
I0128 08:14:41.061530 139866171975424 logging_writer.py:48] [9500] global_step=9500, grad_norm=8.541205406188965, loss=2.6411619186401367
I0128 08:15:15.160761 139866180368128 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.927602767944336, loss=2.5529937744140625
I0128 08:15:49.187199 139866171975424 logging_writer.py:48] [9700] global_step=9700, grad_norm=9.054901123046875, loss=2.591153383255005
I0128 08:16:23.205441 139866180368128 logging_writer.py:48] [9800] global_step=9800, grad_norm=7.057214260101318, loss=2.6020634174346924
I0128 08:16:57.218691 139866171975424 logging_writer.py:48] [9900] global_step=9900, grad_norm=6.095548629760742, loss=2.5889530181884766
I0128 08:17:31.230945 139866180368128 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.648242950439453, loss=2.587869167327881
I0128 08:18:05.262206 139866171975424 logging_writer.py:48] [10100] global_step=10100, grad_norm=8.821381568908691, loss=2.678520917892456
I0128 08:18:39.235305 139866180368128 logging_writer.py:48] [10200] global_step=10200, grad_norm=6.04986047744751, loss=2.56546950340271
I0128 08:19:13.259280 139866171975424 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.618968963623047, loss=2.571514129638672
I0128 08:19:47.258149 139866180368128 logging_writer.py:48] [10400] global_step=10400, grad_norm=7.180644989013672, loss=2.536527633666992
I0128 08:19:54.540445 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:20:00.776662 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:20:09.852934 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:20:12.421256 140027215431488 submission_runner.py:408] Time since start: 3751.13s, 	Step: 10423, 	{'train/accuracy': 0.5160634517669678, 'train/loss': 2.058394432067871, 'validation/accuracy': 0.4732999801635742, 'validation/loss': 2.3019864559173584, 'validation/num_examples': 50000, 'test/accuracy': 0.3750000298023224, 'test/loss': 2.9806602001190186, 'test/num_examples': 10000, 'score': 3606.813559293747, 'total_duration': 3751.127242565155, 'accumulated_submission_time': 3606.813559293747, 'accumulated_eval_time': 143.74170780181885, 'accumulated_logging_time': 0.20235371589660645}
I0128 08:20:12.441147 139866163582720 logging_writer.py:48] [10423] accumulated_eval_time=143.741708, accumulated_logging_time=0.202354, accumulated_submission_time=3606.813559, global_step=10423, preemption_count=0, score=3606.813559, test/accuracy=0.375000, test/loss=2.980660, test/num_examples=10000, total_duration=3751.127243, train/accuracy=0.516063, train/loss=2.058394, validation/accuracy=0.473300, validation/loss=2.301986, validation/num_examples=50000
I0128 08:20:38.971427 139866188760832 logging_writer.py:48] [10500] global_step=10500, grad_norm=8.18362808227539, loss=2.6141140460968018
I0128 08:21:12.954927 139866163582720 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.794674396514893, loss=2.482804775238037
I0128 08:21:47.157440 139866188760832 logging_writer.py:48] [10700] global_step=10700, grad_norm=7.165432929992676, loss=2.4468274116516113
I0128 08:22:21.163919 139866163582720 logging_writer.py:48] [10800] global_step=10800, grad_norm=9.237992286682129, loss=2.500884532928467
I0128 08:22:55.176700 139866188760832 logging_writer.py:48] [10900] global_step=10900, grad_norm=5.930416107177734, loss=2.671454429626465
I0128 08:23:29.181312 139866163582720 logging_writer.py:48] [11000] global_step=11000, grad_norm=5.531698226928711, loss=2.6232571601867676
I0128 08:24:03.188926 139866188760832 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.760447978973389, loss=2.3223254680633545
I0128 08:24:37.189349 139866163582720 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.7872796058654785, loss=2.4358227252960205
I0128 08:25:11.205479 139866188760832 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.446193218231201, loss=2.4062225818634033
I0128 08:25:45.232464 139866163582720 logging_writer.py:48] [11400] global_step=11400, grad_norm=5.979437351226807, loss=2.4543139934539795
I0128 08:26:19.223596 139866188760832 logging_writer.py:48] [11500] global_step=11500, grad_norm=7.979455471038818, loss=2.5618672370910645
I0128 08:26:53.226136 139866163582720 logging_writer.py:48] [11600] global_step=11600, grad_norm=6.1837968826293945, loss=2.4727578163146973
I0128 08:27:27.220433 139866188760832 logging_writer.py:48] [11700] global_step=11700, grad_norm=7.243011951446533, loss=2.4017958641052246
I0128 08:28:01.322951 139866163582720 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.795477390289307, loss=2.543365478515625
I0128 08:28:35.334130 139866188760832 logging_writer.py:48] [11900] global_step=11900, grad_norm=6.786667346954346, loss=2.48724627494812
I0128 08:28:42.610304 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:28:48.814501 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:28:57.840781 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:29:00.453085 140027215431488 submission_runner.py:408] Time since start: 4279.16s, 	Step: 11923, 	{'train/accuracy': 0.5541493892669678, 'train/loss': 1.873650074005127, 'validation/accuracy': 0.510159969329834, 'validation/loss': 2.0996718406677246, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.822295665740967, 'test/num_examples': 10000, 'score': 4116.920515537262, 'total_duration': 4279.158988952637, 'accumulated_submission_time': 4116.920515537262, 'accumulated_eval_time': 161.58436393737793, 'accumulated_logging_time': 0.23344969749450684}
I0128 08:29:00.472144 139865240893184 logging_writer.py:48] [11923] accumulated_eval_time=161.584364, accumulated_logging_time=0.233450, accumulated_submission_time=4116.920516, global_step=11923, preemption_count=0, score=4116.920516, test/accuracy=0.392100, test/loss=2.822296, test/num_examples=10000, total_duration=4279.158989, train/accuracy=0.554149, train/loss=1.873650, validation/accuracy=0.510160, validation/loss=2.099672, validation/num_examples=50000
I0128 08:29:26.962164 139865760950016 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.112403392791748, loss=2.3994784355163574
I0128 08:30:00.893027 139865240893184 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.388522624969482, loss=2.3017890453338623
I0128 08:30:34.865575 139865760950016 logging_writer.py:48] [12200] global_step=12200, grad_norm=7.476267337799072, loss=2.3997676372528076
I0128 08:31:08.850922 139865240893184 logging_writer.py:48] [12300] global_step=12300, grad_norm=5.173281192779541, loss=2.421581745147705
I0128 08:31:42.842459 139865760950016 logging_writer.py:48] [12400] global_step=12400, grad_norm=5.547464370727539, loss=2.3921687602996826
I0128 08:32:16.835129 139865240893184 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.812075138092041, loss=2.4094326496124268
I0128 08:32:50.825322 139865760950016 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.02572774887085, loss=2.4804842472076416
I0128 08:33:24.782565 139865240893184 logging_writer.py:48] [12700] global_step=12700, grad_norm=7.898565769195557, loss=2.3366029262542725
I0128 08:33:58.739809 139865760950016 logging_writer.py:48] [12800] global_step=12800, grad_norm=6.245863437652588, loss=2.3398149013519287
I0128 08:34:32.910453 139865240893184 logging_writer.py:48] [12900] global_step=12900, grad_norm=8.044458389282227, loss=2.3071417808532715
I0128 08:35:06.906493 139865760950016 logging_writer.py:48] [13000] global_step=13000, grad_norm=7.452441692352295, loss=2.2945401668548584
I0128 08:35:40.876599 139865240893184 logging_writer.py:48] [13100] global_step=13100, grad_norm=5.638093948364258, loss=2.320310592651367
I0128 08:36:14.849406 139865760950016 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.9166693687438965, loss=2.3888416290283203
I0128 08:36:48.817471 139865240893184 logging_writer.py:48] [13300] global_step=13300, grad_norm=5.7808451652526855, loss=2.415013313293457
I0128 08:37:22.805406 139865760950016 logging_writer.py:48] [13400] global_step=13400, grad_norm=8.011519432067871, loss=2.3650996685028076
I0128 08:37:30.756611 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:37:37.053606 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:37:45.818493 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:37:48.412583 140027215431488 submission_runner.py:408] Time since start: 4807.12s, 	Step: 13425, 	{'train/accuracy': 0.5691764950752258, 'train/loss': 1.8064440488815308, 'validation/accuracy': 0.527899980545044, 'validation/loss': 2.0273935794830322, 'validation/num_examples': 50000, 'test/accuracy': 0.40710002183914185, 'test/loss': 2.8018851280212402, 'test/num_examples': 10000, 'score': 4627.143044948578, 'total_duration': 4807.1185483932495, 'accumulated_submission_time': 4627.143044948578, 'accumulated_eval_time': 179.24028539657593, 'accumulated_logging_time': 0.2617185115814209}
I0128 08:37:48.431949 139865240893184 logging_writer.py:48] [13425] accumulated_eval_time=179.240285, accumulated_logging_time=0.261719, accumulated_submission_time=4627.143045, global_step=13425, preemption_count=0, score=4627.143045, test/accuracy=0.407100, test/loss=2.801885, test/num_examples=10000, total_duration=4807.118548, train/accuracy=0.569176, train/loss=1.806444, validation/accuracy=0.527900, validation/loss=2.027394, validation/num_examples=50000
I0128 08:38:14.274359 139865760950016 logging_writer.py:48] [13500] global_step=13500, grad_norm=8.489191055297852, loss=2.3173940181732178
I0128 08:38:48.265872 139865240893184 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.323391914367676, loss=2.2612011432647705
I0128 08:39:22.235937 139865760950016 logging_writer.py:48] [13700] global_step=13700, grad_norm=6.706024169921875, loss=2.178891181945801
I0128 08:39:56.166648 139865240893184 logging_writer.py:48] [13800] global_step=13800, grad_norm=9.976341247558594, loss=2.337118148803711
I0128 08:40:30.249849 139865760950016 logging_writer.py:48] [13900] global_step=13900, grad_norm=7.825917720794678, loss=2.2776575088500977
I0128 08:41:04.225620 139865240893184 logging_writer.py:48] [14000] global_step=14000, grad_norm=8.099493980407715, loss=2.2583281993865967
I0128 08:41:38.193426 139865760950016 logging_writer.py:48] [14100] global_step=14100, grad_norm=6.340783596038818, loss=2.2893564701080322
I0128 08:42:12.124007 139865240893184 logging_writer.py:48] [14200] global_step=14200, grad_norm=6.281923770904541, loss=2.2394793033599854
I0128 08:42:46.102745 139865760950016 logging_writer.py:48] [14300] global_step=14300, grad_norm=4.646188735961914, loss=2.293184757232666
I0128 08:43:20.116124 139865240893184 logging_writer.py:48] [14400] global_step=14400, grad_norm=9.675040245056152, loss=2.258859634399414
I0128 08:43:54.087383 139865760950016 logging_writer.py:48] [14500] global_step=14500, grad_norm=6.936895370483398, loss=2.2542715072631836
I0128 08:44:28.028043 139865240893184 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.550690174102783, loss=2.192922592163086
I0128 08:45:02.024284 139865760950016 logging_writer.py:48] [14700] global_step=14700, grad_norm=7.975142955780029, loss=2.169325828552246
I0128 08:45:35.976220 139865240893184 logging_writer.py:48] [14800] global_step=14800, grad_norm=7.503400802612305, loss=2.326421022415161
I0128 08:46:09.941279 139865760950016 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.491758346557617, loss=2.1844136714935303
I0128 08:46:18.571149 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:46:24.767820 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:46:33.547495 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:46:36.103091 140027215431488 submission_runner.py:408] Time since start: 5334.81s, 	Step: 14927, 	{'train/accuracy': 0.5782844424247742, 'train/loss': 1.7499090433120728, 'validation/accuracy': 0.5433799624443054, 'validation/loss': 1.9397135972976685, 'validation/num_examples': 50000, 'test/accuracy': 0.41850000619888306, 'test/loss': 2.6923470497131348, 'test/num_examples': 10000, 'score': 5137.219739675522, 'total_duration': 5334.8082802295685, 'accumulated_submission_time': 5137.219739675522, 'accumulated_eval_time': 196.7713873386383, 'accumulated_logging_time': 0.29082202911376953}
I0128 08:46:36.122761 139865224107776 logging_writer.py:48] [14927] accumulated_eval_time=196.771387, accumulated_logging_time=0.290822, accumulated_submission_time=5137.219740, global_step=14927, preemption_count=0, score=5137.219740, test/accuracy=0.418500, test/loss=2.692347, test/num_examples=10000, total_duration=5334.808280, train/accuracy=0.578284, train/loss=1.749909, validation/accuracy=0.543380, validation/loss=1.939714, validation/num_examples=50000
I0128 08:47:01.304392 139865232500480 logging_writer.py:48] [15000] global_step=15000, grad_norm=5.180137634277344, loss=2.182612180709839
I0128 08:47:35.259639 139865224107776 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.491204738616943, loss=2.319026231765747
I0128 08:48:09.231965 139865232500480 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.360241889953613, loss=2.297165870666504
I0128 08:48:43.165297 139865224107776 logging_writer.py:48] [15300] global_step=15300, grad_norm=7.504587650299072, loss=2.2944188117980957
I0128 08:49:17.114018 139865232500480 logging_writer.py:48] [15400] global_step=15400, grad_norm=5.217323303222656, loss=2.2937285900115967
I0128 08:49:51.089341 139865224107776 logging_writer.py:48] [15500] global_step=15500, grad_norm=5.380270957946777, loss=2.300269842147827
I0128 08:50:25.049818 139865232500480 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.439112663269043, loss=2.2815027236938477
I0128 08:50:58.954119 139865224107776 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.059688091278076, loss=2.1523232460021973
I0128 08:51:32.901153 139865232500480 logging_writer.py:48] [15800] global_step=15800, grad_norm=6.580164432525635, loss=2.1815338134765625
I0128 08:52:06.868126 139865224107776 logging_writer.py:48] [15900] global_step=15900, grad_norm=7.55858850479126, loss=2.2394659519195557
I0128 08:52:40.829145 139865232500480 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.9360287189483643, loss=2.2785768508911133
I0128 08:53:14.864286 139865224107776 logging_writer.py:48] [16100] global_step=16100, grad_norm=7.2075676918029785, loss=2.187314987182617
I0128 08:53:48.815080 139865232500480 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.872311592102051, loss=2.257891893386841
I0128 08:54:22.757894 139865224107776 logging_writer.py:48] [16300] global_step=16300, grad_norm=6.436122894287109, loss=2.1905527114868164
I0128 08:54:56.723893 139865232500480 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.444802761077881, loss=2.307807445526123
I0128 08:55:06.397105 140027215431488 spec.py:321] Evaluating on the training split.
I0128 08:55:12.614128 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 08:55:21.384314 140027215431488 spec.py:349] Evaluating on the test split.
I0128 08:55:23.993564 140027215431488 submission_runner.py:408] Time since start: 5862.70s, 	Step: 16430, 	{'train/accuracy': 0.5916174650192261, 'train/loss': 1.6916300058364868, 'validation/accuracy': 0.5521799921989441, 'validation/loss': 1.8945378065109253, 'validation/num_examples': 50000, 'test/accuracy': 0.4326000213623047, 'test/loss': 2.6046390533447266, 'test/num_examples': 10000, 'score': 5647.431823968887, 'total_duration': 5862.699553728104, 'accumulated_submission_time': 5647.431823968887, 'accumulated_eval_time': 214.36780762672424, 'accumulated_logging_time': 0.3212883472442627}
I0128 08:55:24.017446 139866171975424 logging_writer.py:48] [16430] accumulated_eval_time=214.367808, accumulated_logging_time=0.321288, accumulated_submission_time=5647.431824, global_step=16430, preemption_count=0, score=5647.431824, test/accuracy=0.432600, test/loss=2.604639, test/num_examples=10000, total_duration=5862.699554, train/accuracy=0.591617, train/loss=1.691630, validation/accuracy=0.552180, validation/loss=1.894538, validation/num_examples=50000
I0128 08:55:48.106590 139866180368128 logging_writer.py:48] [16500] global_step=16500, grad_norm=6.503280162811279, loss=2.2008373737335205
I0128 08:56:21.987477 139866171975424 logging_writer.py:48] [16600] global_step=16600, grad_norm=6.200750827789307, loss=2.259357213973999
I0128 08:56:55.939448 139866180368128 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.656588077545166, loss=2.1587510108947754
I0128 08:57:29.843595 139866171975424 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.112631320953369, loss=2.132411241531372
I0128 08:58:03.791158 139866180368128 logging_writer.py:48] [16900] global_step=16900, grad_norm=6.292519569396973, loss=2.2709920406341553
I0128 08:58:37.753789 139866171975424 logging_writer.py:48] [17000] global_step=17000, grad_norm=6.499133110046387, loss=2.160109043121338
I0128 08:59:11.686205 139866180368128 logging_writer.py:48] [17100] global_step=17100, grad_norm=7.5268778800964355, loss=2.1112895011901855
I0128 08:59:45.733145 139866171975424 logging_writer.py:48] [17200] global_step=17200, grad_norm=6.061757564544678, loss=2.1891326904296875
I0128 09:00:19.661909 139866180368128 logging_writer.py:48] [17300] global_step=17300, grad_norm=8.609403610229492, loss=2.1873326301574707
I0128 09:00:53.576662 139866171975424 logging_writer.py:48] [17400] global_step=17400, grad_norm=6.190088748931885, loss=2.152923107147217
I0128 09:01:27.518347 139866180368128 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.62484073638916, loss=2.1846377849578857
I0128 09:02:01.424554 139866171975424 logging_writer.py:48] [17600] global_step=17600, grad_norm=6.738749980926514, loss=2.2750349044799805
I0128 09:02:35.368603 139866180368128 logging_writer.py:48] [17700] global_step=17700, grad_norm=5.571839332580566, loss=2.188107490539551
I0128 09:03:09.256782 139866171975424 logging_writer.py:48] [17800] global_step=17800, grad_norm=6.092299938201904, loss=2.3574752807617188
I0128 09:03:43.209274 139866180368128 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.389042854309082, loss=2.0857315063476562
I0128 09:03:54.221493 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:04:00.392107 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:04:09.380234 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:04:12.006363 140027215431488 submission_runner.py:408] Time since start: 6390.71s, 	Step: 17934, 	{'train/accuracy': 0.6174266338348389, 'train/loss': 1.5416178703308105, 'validation/accuracy': 0.5548799633979797, 'validation/loss': 1.8838409185409546, 'validation/num_examples': 50000, 'test/accuracy': 0.4327000081539154, 'test/loss': 2.6345584392547607, 'test/num_examples': 10000, 'score': 6157.574437856674, 'total_duration': 6390.7123556137085, 'accumulated_submission_time': 6157.574437856674, 'accumulated_eval_time': 232.15265464782715, 'accumulated_logging_time': 0.3548550605773926}
I0128 09:04:12.027273 139865232500480 logging_writer.py:48] [17934] accumulated_eval_time=232.152655, accumulated_logging_time=0.354855, accumulated_submission_time=6157.574438, global_step=17934, preemption_count=0, score=6157.574438, test/accuracy=0.432700, test/loss=2.634558, test/num_examples=10000, total_duration=6390.712356, train/accuracy=0.617427, train/loss=1.541618, validation/accuracy=0.554880, validation/loss=1.883841, validation/num_examples=50000
I0128 09:04:34.729159 139865240893184 logging_writer.py:48] [18000] global_step=18000, grad_norm=5.292963981628418, loss=2.206517219543457
I0128 09:05:08.637749 139865232500480 logging_writer.py:48] [18100] global_step=18100, grad_norm=5.386981964111328, loss=2.2091572284698486
I0128 09:05:42.608534 139865240893184 logging_writer.py:48] [18200] global_step=18200, grad_norm=5.59299898147583, loss=2.262552499771118
I0128 09:06:16.585855 139865232500480 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.504637241363525, loss=2.1636369228363037
I0128 09:06:50.508783 139865240893184 logging_writer.py:48] [18400] global_step=18400, grad_norm=5.898974895477295, loss=2.122286081314087
I0128 09:07:24.441965 139865232500480 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.747413635253906, loss=2.1999454498291016
I0128 09:07:58.381238 139865240893184 logging_writer.py:48] [18600] global_step=18600, grad_norm=4.227626323699951, loss=2.2105042934417725
I0128 09:08:32.287439 139865232500480 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.60877799987793, loss=2.183137893676758
I0128 09:09:06.253721 139865240893184 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.419081926345825, loss=2.165200710296631
I0128 09:09:40.168793 139865232500480 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.1958627700805664, loss=2.1577818393707275
I0128 09:10:14.117430 139865240893184 logging_writer.py:48] [19000] global_step=19000, grad_norm=5.698378086090088, loss=2.1451303958892822
I0128 09:10:48.064054 139865232500480 logging_writer.py:48] [19100] global_step=19100, grad_norm=5.689671993255615, loss=2.1258997917175293
I0128 09:11:21.950466 139865240893184 logging_writer.py:48] [19200] global_step=19200, grad_norm=6.3092780113220215, loss=2.213106155395508
I0128 09:11:55.895148 139865232500480 logging_writer.py:48] [19300] global_step=19300, grad_norm=5.159098148345947, loss=2.186751127243042
I0128 09:12:29.911488 139865240893184 logging_writer.py:48] [19400] global_step=19400, grad_norm=5.315073013305664, loss=2.164525270462036
I0128 09:12:42.275094 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:12:48.500646 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:12:57.603988 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:13:00.198396 140027215431488 submission_runner.py:408] Time since start: 6918.90s, 	Step: 19438, 	{'train/accuracy': 0.6119060516357422, 'train/loss': 1.5986028909683228, 'validation/accuracy': 0.5607999563217163, 'validation/loss': 1.8535418510437012, 'validation/num_examples': 50000, 'test/accuracy': 0.43640002608299255, 'test/loss': 2.5843722820281982, 'test/num_examples': 10000, 'score': 6667.758923768997, 'total_duration': 6918.904389619827, 'accumulated_submission_time': 6667.758923768997, 'accumulated_eval_time': 250.0759198665619, 'accumulated_logging_time': 0.38729166984558105}
I0128 09:13:00.219921 139866180368128 logging_writer.py:48] [19438] accumulated_eval_time=250.075920, accumulated_logging_time=0.387292, accumulated_submission_time=6667.758924, global_step=19438, preemption_count=0, score=6667.758924, test/accuracy=0.436400, test/loss=2.584372, test/num_examples=10000, total_duration=6918.904390, train/accuracy=0.611906, train/loss=1.598603, validation/accuracy=0.560800, validation/loss=1.853542, validation/num_examples=50000
I0128 09:13:21.593673 139866188760832 logging_writer.py:48] [19500] global_step=19500, grad_norm=6.287070274353027, loss=2.0782439708709717
I0128 09:13:55.519159 139866180368128 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.574747085571289, loss=2.1237497329711914
I0128 09:14:29.428670 139866188760832 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.246490955352783, loss=2.1088309288024902
I0128 09:15:03.375131 139866180368128 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.730742931365967, loss=2.085784435272217
I0128 09:15:37.278100 139866188760832 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.751267433166504, loss=2.3354806900024414
I0128 09:16:11.200072 139866180368128 logging_writer.py:48] [20000] global_step=20000, grad_norm=5.426600456237793, loss=2.150702953338623
I0128 09:16:45.102121 139866188760832 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.655986785888672, loss=2.1449077129364014
I0128 09:17:19.039409 139866180368128 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.1415677070617676, loss=2.1388001441955566
I0128 09:17:52.942842 139866188760832 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.992859125137329, loss=2.160961151123047
I0128 09:18:26.856965 139866180368128 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.8356027603149414, loss=2.301347494125366
I0128 09:19:00.870626 139866188760832 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.829807758331299, loss=2.192296028137207
I0128 09:19:34.810882 139866180368128 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.616562366485596, loss=2.2551424503326416
I0128 09:20:08.715054 139866188760832 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.061774730682373, loss=2.0866355895996094
I0128 09:20:42.635423 139866180368128 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.555539131164551, loss=2.14908504486084
I0128 09:21:16.558565 139866188760832 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.715153217315674, loss=2.042710781097412
I0128 09:21:30.266303 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:21:36.452054 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:21:45.240516 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:21:47.803012 140027215431488 submission_runner.py:408] Time since start: 7446.51s, 	Step: 20942, 	{'train/accuracy': 0.6103515625, 'train/loss': 1.5754483938217163, 'validation/accuracy': 0.5648599863052368, 'validation/loss': 1.8282480239868164, 'validation/num_examples': 50000, 'test/accuracy': 0.45250001549720764, 'test/loss': 2.538072109222412, 'test/num_examples': 10000, 'score': 7177.744247436523, 'total_duration': 7446.508990764618, 'accumulated_submission_time': 7177.744247436523, 'accumulated_eval_time': 267.61257815361023, 'accumulated_logging_time': 0.4182553291320801}
I0128 09:21:47.824156 139865760950016 logging_writer.py:48] [20942] accumulated_eval_time=267.612578, accumulated_logging_time=0.418255, accumulated_submission_time=7177.744247, global_step=20942, preemption_count=0, score=7177.744247, test/accuracy=0.452500, test/loss=2.538072, test/num_examples=10000, total_duration=7446.508991, train/accuracy=0.610352, train/loss=1.575448, validation/accuracy=0.564860, validation/loss=1.828248, validation/num_examples=50000
I0128 09:22:07.820194 139865769342720 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.745651721954346, loss=2.11419939994812
I0128 09:22:41.727537 139865760950016 logging_writer.py:48] [21100] global_step=21100, grad_norm=4.243678092956543, loss=2.1144065856933594
I0128 09:23:15.605041 139865769342720 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.5396151542663574, loss=2.0437960624694824
I0128 09:23:49.499077 139865760950016 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.964680194854736, loss=2.0533182621002197
I0128 09:24:23.425239 139865769342720 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.79036021232605, loss=2.1250288486480713
I0128 09:24:57.497167 139865760950016 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.2464866638183594, loss=2.1744401454925537
I0128 09:25:31.429755 139865769342720 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.857598304748535, loss=2.148902177810669
I0128 09:26:05.334892 139865760950016 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.224971771240234, loss=2.0236918926239014
I0128 09:26:39.274343 139865769342720 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.5314905643463135, loss=2.0520622730255127
I0128 09:27:13.178097 139865760950016 logging_writer.py:48] [21900] global_step=21900, grad_norm=4.219043731689453, loss=2.0211362838745117
I0128 09:27:47.113727 139865769342720 logging_writer.py:48] [22000] global_step=22000, grad_norm=5.674564838409424, loss=2.1093857288360596
I0128 09:28:21.024396 139865760950016 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.083885669708252, loss=2.2248566150665283
I0128 09:28:54.953624 139865769342720 logging_writer.py:48] [22200] global_step=22200, grad_norm=4.036988735198975, loss=2.0264172554016113
I0128 09:29:28.836090 139865760950016 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.8193325996398926, loss=2.017625570297241
I0128 09:30:02.777360 139865769342720 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.022946357727051, loss=2.107966661453247
I0128 09:30:17.832657 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:30:24.012304 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:30:32.785872 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:30:35.435847 140027215431488 submission_runner.py:408] Time since start: 7974.14s, 	Step: 22446, 	{'train/accuracy': 0.6111288070678711, 'train/loss': 1.5875287055969238, 'validation/accuracy': 0.572219967842102, 'validation/loss': 1.8043186664581299, 'validation/num_examples': 50000, 'test/accuracy': 0.4520000219345093, 'test/loss': 2.550936698913574, 'test/num_examples': 10000, 'score': 7687.6893701553345, 'total_duration': 7974.141838550568, 'accumulated_submission_time': 7687.6893701553345, 'accumulated_eval_time': 285.21574664115906, 'accumulated_logging_time': 0.4493522644042969}
I0128 09:30:35.460847 139866180368128 logging_writer.py:48] [22446] accumulated_eval_time=285.215747, accumulated_logging_time=0.449352, accumulated_submission_time=7687.689370, global_step=22446, preemption_count=0, score=7687.689370, test/accuracy=0.452000, test/loss=2.550937, test/num_examples=10000, total_duration=7974.141839, train/accuracy=0.611129, train/loss=1.587529, validation/accuracy=0.572220, validation/loss=1.804319, validation/num_examples=50000
I0128 09:30:54.114202 139866188760832 logging_writer.py:48] [22500] global_step=22500, grad_norm=4.764047622680664, loss=2.027661085128784
I0128 09:31:28.104380 139866180368128 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.901348114013672, loss=2.0917129516601562
I0128 09:32:02.019160 139866188760832 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.6786961555480957, loss=2.069281578063965
I0128 09:32:35.943437 139866180368128 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.0556373596191406, loss=2.072539806365967
I0128 09:33:09.849266 139866188760832 logging_writer.py:48] [22900] global_step=22900, grad_norm=4.709283351898193, loss=2.036695957183838
I0128 09:33:43.767649 139866180368128 logging_writer.py:48] [23000] global_step=23000, grad_norm=4.231915473937988, loss=2.0071728229522705
I0128 09:34:17.698499 139866188760832 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.515528678894043, loss=2.069544792175293
I0128 09:34:51.636130 139866180368128 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.402894020080566, loss=2.0461411476135254
I0128 09:35:25.516055 139866188760832 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.03059196472168, loss=2.1982672214508057
I0128 09:35:59.490060 139866180368128 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.543189287185669, loss=2.1394503116607666
I0128 09:36:33.367676 139866188760832 logging_writer.py:48] [23500] global_step=23500, grad_norm=4.288889408111572, loss=2.1753273010253906
I0128 09:37:07.287047 139866180368128 logging_writer.py:48] [23600] global_step=23600, grad_norm=4.176719665527344, loss=2.096266269683838
I0128 09:37:41.276092 139866188760832 logging_writer.py:48] [23700] global_step=23700, grad_norm=4.054446697235107, loss=2.0939462184906006
I0128 09:38:15.181435 139866180368128 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.479607582092285, loss=2.156255006790161
I0128 09:38:49.103509 139866188760832 logging_writer.py:48] [23900] global_step=23900, grad_norm=4.190847873687744, loss=2.033574104309082
I0128 09:39:05.574462 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:39:11.929923 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:39:20.649823 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:39:23.252466 140027215431488 submission_runner.py:408] Time since start: 8501.96s, 	Step: 23950, 	{'train/accuracy': 0.6212531924247742, 'train/loss': 1.5448447465896606, 'validation/accuracy': 0.5806999802589417, 'validation/loss': 1.7550562620162964, 'validation/num_examples': 50000, 'test/accuracy': 0.4563000202178955, 'test/loss': 2.495838165283203, 'test/num_examples': 10000, 'score': 8197.739970445633, 'total_duration': 8501.958456516266, 'accumulated_submission_time': 8197.739970445633, 'accumulated_eval_time': 302.8937132358551, 'accumulated_logging_time': 0.4838879108428955}
I0128 09:39:23.273989 139865240893184 logging_writer.py:48] [23950] accumulated_eval_time=302.893713, accumulated_logging_time=0.483888, accumulated_submission_time=8197.739970, global_step=23950, preemption_count=0, score=8197.739970, test/accuracy=0.456300, test/loss=2.495838, test/num_examples=10000, total_duration=8501.958457, train/accuracy=0.621253, train/loss=1.544845, validation/accuracy=0.580700, validation/loss=1.755056, validation/num_examples=50000
I0128 09:39:40.569130 139865760950016 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.781546115875244, loss=2.0759332180023193
I0128 09:40:14.441874 139865240893184 logging_writer.py:48] [24100] global_step=24100, grad_norm=4.047170639038086, loss=2.0364041328430176
I0128 09:40:48.352164 139865760950016 logging_writer.py:48] [24200] global_step=24200, grad_norm=4.0731000900268555, loss=2.026416778564453
I0128 09:41:22.264063 139865240893184 logging_writer.py:48] [24300] global_step=24300, grad_norm=4.184864521026611, loss=2.0146055221557617
I0128 09:41:56.184015 139865760950016 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.5268449783325195, loss=2.039464235305786
I0128 09:42:30.075204 139865240893184 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.440291404724121, loss=2.066342830657959
I0128 09:43:04.009996 139865760950016 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.357879161834717, loss=2.0927579402923584
I0128 09:43:37.899837 139865240893184 logging_writer.py:48] [24700] global_step=24700, grad_norm=4.268261432647705, loss=1.991811990737915
I0128 09:44:11.904909 139865760950016 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.6223387718200684, loss=1.99299955368042
I0128 09:44:45.848461 139865240893184 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.613389015197754, loss=2.0734264850616455
I0128 09:45:19.768561 139865760950016 logging_writer.py:48] [25000] global_step=25000, grad_norm=4.504308223724365, loss=2.0316779613494873
I0128 09:45:53.687717 139865240893184 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.8604981899261475, loss=1.9932401180267334
I0128 09:46:27.602604 139865760950016 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.900604486465454, loss=2.0366625785827637
I0128 09:47:01.508273 139865240893184 logging_writer.py:48] [25300] global_step=25300, grad_norm=4.547307014465332, loss=1.9935020208358765
I0128 09:47:35.421166 139865760950016 logging_writer.py:48] [25400] global_step=25400, grad_norm=4.358711242675781, loss=2.0540177822113037
I0128 09:47:53.540597 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:47:59.739006 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:48:08.669087 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:48:11.279938 140027215431488 submission_runner.py:408] Time since start: 9029.99s, 	Step: 25455, 	{'train/accuracy': 0.6300023794174194, 'train/loss': 1.4953486919403076, 'validation/accuracy': 0.5855000019073486, 'validation/loss': 1.7113131284713745, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.4377589225769043, 'test/num_examples': 10000, 'score': 8707.943544864655, 'total_duration': 9029.98591208458, 'accumulated_submission_time': 8707.943544864655, 'accumulated_eval_time': 320.63299918174744, 'accumulated_logging_time': 0.5171177387237549}
I0128 09:48:11.305892 139866180368128 logging_writer.py:48] [25455] accumulated_eval_time=320.632999, accumulated_logging_time=0.517118, accumulated_submission_time=8707.943545, global_step=25455, preemption_count=0, score=8707.943545, test/accuracy=0.463800, test/loss=2.437759, test/num_examples=10000, total_duration=9029.985912, train/accuracy=0.630002, train/loss=1.495349, validation/accuracy=0.585500, validation/loss=1.711313, validation/num_examples=50000
I0128 09:48:26.900852 139866188760832 logging_writer.py:48] [25500] global_step=25500, grad_norm=4.052112579345703, loss=1.9653270244598389
I0128 09:49:00.725051 139866180368128 logging_writer.py:48] [25600] global_step=25600, grad_norm=4.057170391082764, loss=1.9464912414550781
I0128 09:49:34.584701 139866188760832 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.7653298377990723, loss=2.048999309539795
I0128 09:50:08.520923 139866180368128 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.1621012687683105, loss=2.00948166847229
I0128 09:50:42.569650 139866188760832 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.6954777240753174, loss=2.148446559906006
I0128 09:51:16.493979 139866180368128 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.9476497173309326, loss=2.01035213470459
I0128 09:51:50.408761 139866188760832 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.4685776233673096, loss=1.9668487310409546
I0128 09:52:24.346355 139866180368128 logging_writer.py:48] [26200] global_step=26200, grad_norm=4.094282627105713, loss=2.0493319034576416
I0128 09:52:58.229746 139866188760832 logging_writer.py:48] [26300] global_step=26300, grad_norm=4.015271186828613, loss=1.9517951011657715
I0128 09:53:32.167781 139866180368128 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.220454216003418, loss=1.9365220069885254
I0128 09:54:06.056356 139866188760832 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.804018974304199, loss=1.9595396518707275
I0128 09:54:39.980990 139866180368128 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.069697618484497, loss=1.9154083728790283
I0128 09:55:13.857374 139866188760832 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.8088436126708984, loss=1.997766137123108
I0128 09:55:47.770446 139866180368128 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.332154273986816, loss=2.095386028289795
I0128 09:56:21.694835 139866188760832 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.6781368255615234, loss=2.055422306060791
I0128 09:56:41.515202 140027215431488 spec.py:321] Evaluating on the training split.
I0128 09:56:47.697611 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 09:56:56.918803 140027215431488 spec.py:349] Evaluating on the test split.
I0128 09:56:59.523072 140027215431488 submission_runner.py:408] Time since start: 9558.23s, 	Step: 26960, 	{'train/accuracy': 0.6515864133834839, 'train/loss': 1.3849291801452637, 'validation/accuracy': 0.5827599763870239, 'validation/loss': 1.7557555437088013, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.4801058769226074, 'test/num_examples': 10000, 'score': 9218.090360164642, 'total_duration': 9558.22905421257, 'accumulated_submission_time': 9218.090360164642, 'accumulated_eval_time': 338.6408226490021, 'accumulated_logging_time': 0.5527477264404297}
I0128 09:56:59.546582 139865224107776 logging_writer.py:48] [26960] accumulated_eval_time=338.640823, accumulated_logging_time=0.552748, accumulated_submission_time=9218.090360, global_step=26960, preemption_count=0, score=9218.090360, test/accuracy=0.460600, test/loss=2.480106, test/num_examples=10000, total_duration=9558.229054, train/accuracy=0.651586, train/loss=1.384929, validation/accuracy=0.582760, validation/loss=1.755756, validation/num_examples=50000
I0128 09:57:13.456932 139865232500480 logging_writer.py:48] [27000] global_step=27000, grad_norm=4.57302713394165, loss=2.0251882076263428
I0128 09:57:47.311008 139865224107776 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.2718143463134766, loss=1.898301601409912
I0128 09:58:21.199823 139865232500480 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.233762741088867, loss=1.9148527383804321
I0128 09:58:55.092738 139865224107776 logging_writer.py:48] [27300] global_step=27300, grad_norm=5.26023530960083, loss=1.8678337335586548
I0128 09:59:29.006662 139865232500480 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.8791704177856445, loss=2.097303867340088
I0128 10:00:02.900249 139865224107776 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.7399070262908936, loss=1.969409465789795
I0128 10:00:36.818037 139865232500480 logging_writer.py:48] [27600] global_step=27600, grad_norm=4.034507751464844, loss=2.019209623336792
I0128 10:01:10.695786 139865224107776 logging_writer.py:48] [27700] global_step=27700, grad_norm=4.164548873901367, loss=1.936194658279419
I0128 10:01:44.628129 139865232500480 logging_writer.py:48] [27800] global_step=27800, grad_norm=4.3030853271484375, loss=1.9778496026992798
I0128 10:02:18.521177 139865224107776 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.7340590953826904, loss=1.9433962106704712
I0128 10:02:52.425456 139865232500480 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.5264837741851807, loss=1.9934313297271729
I0128 10:03:26.382498 139865224107776 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.44882869720459, loss=2.0437302589416504
I0128 10:04:00.314447 139865232500480 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.806575298309326, loss=2.0474913120269775
I0128 10:04:34.201801 139865224107776 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.988182544708252, loss=1.9823490381240845
I0128 10:05:08.123365 139865232500480 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.5263845920562744, loss=1.9634066820144653
I0128 10:05:29.621022 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:05:35.837127 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:05:44.513516 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:05:47.142053 140027215431488 submission_runner.py:408] Time since start: 10085.85s, 	Step: 28465, 	{'train/accuracy': 0.6382134556770325, 'train/loss': 1.4549899101257324, 'validation/accuracy': 0.5848399996757507, 'validation/loss': 1.7341421842575073, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.44994854927063, 'test/num_examples': 10000, 'score': 9728.102895259857, 'total_duration': 10085.848005533218, 'accumulated_submission_time': 9728.102895259857, 'accumulated_eval_time': 356.16178250312805, 'accumulated_logging_time': 0.5854485034942627}
I0128 10:05:47.164222 139866171975424 logging_writer.py:48] [28465] accumulated_eval_time=356.161783, accumulated_logging_time=0.585449, accumulated_submission_time=9728.102895, global_step=28465, preemption_count=0, score=9728.102895, test/accuracy=0.460600, test/loss=2.449949, test/num_examples=10000, total_duration=10085.848006, train/accuracy=0.638213, train/loss=1.454990, validation/accuracy=0.584840, validation/loss=1.734142, validation/num_examples=50000
I0128 10:05:59.374606 139866180368128 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.3604555130004883, loss=1.837126612663269
I0128 10:06:33.242266 139866171975424 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.7370238304138184, loss=2.025359869003296
I0128 10:07:07.092759 139866180368128 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.5889878273010254, loss=2.015390634536743
I0128 10:07:41.001633 139866171975424 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.7766880989074707, loss=1.9835131168365479
I0128 10:08:14.881516 139866180368128 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.3561618328094482, loss=1.8930079936981201
I0128 10:08:48.796575 139866171975424 logging_writer.py:48] [29000] global_step=29000, grad_norm=4.816336631774902, loss=1.9750139713287354
I0128 10:09:22.689040 139866180368128 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.396120071411133, loss=2.0613231658935547
I0128 10:09:56.671176 139866171975424 logging_writer.py:48] [29200] global_step=29200, grad_norm=4.782703399658203, loss=2.0518712997436523
I0128 10:10:30.525773 139866180368128 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.3844833374023438, loss=2.1497578620910645
I0128 10:11:04.430841 139866171975424 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.730224609375, loss=2.0949103832244873
I0128 10:11:38.328674 139866180368128 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.597991943359375, loss=1.9655365943908691
I0128 10:12:12.244061 139866171975424 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.9359593391418457, loss=2.0995237827301025
I0128 10:12:46.149652 139866180368128 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.5247604846954346, loss=1.969839334487915
I0128 10:13:20.046524 139866171975424 logging_writer.py:48] [29800] global_step=29800, grad_norm=4.243985176086426, loss=2.0586676597595215
I0128 10:13:53.927645 139866180368128 logging_writer.py:48] [29900] global_step=29900, grad_norm=4.456963062286377, loss=1.9526222944259644
I0128 10:14:17.446941 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:14:23.627202 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:14:32.451012 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:14:35.102312 140027215431488 submission_runner.py:408] Time since start: 10613.81s, 	Step: 29971, 	{'train/accuracy': 0.6387914419174194, 'train/loss': 1.4706834554672241, 'validation/accuracy': 0.5878399610519409, 'validation/loss': 1.7269160747528076, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.4534895420074463, 'test/num_examples': 10000, 'score': 10238.321905136108, 'total_duration': 10613.808283567429, 'accumulated_submission_time': 10238.321905136108, 'accumulated_eval_time': 373.8170976638794, 'accumulated_logging_time': 0.6196649074554443}
I0128 10:14:35.132216 139865760950016 logging_writer.py:48] [29971] accumulated_eval_time=373.817098, accumulated_logging_time=0.619665, accumulated_submission_time=10238.321905, global_step=29971, preemption_count=0, score=10238.321905, test/accuracy=0.467800, test/loss=2.453490, test/num_examples=10000, total_duration=10613.808284, train/accuracy=0.638791, train/loss=1.470683, validation/accuracy=0.587840, validation/loss=1.726916, validation/num_examples=50000
I0128 10:14:45.322959 139865769342720 logging_writer.py:48] [30000] global_step=30000, grad_norm=4.337329864501953, loss=1.9826854467391968
I0128 10:15:19.207328 139865760950016 logging_writer.py:48] [30100] global_step=30100, grad_norm=4.672706127166748, loss=1.9564342498779297
I0128 10:15:53.056669 139865769342720 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.7717623710632324, loss=1.9712355136871338
I0128 10:16:27.140312 139865760950016 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.2896370887756348, loss=2.002492904663086
I0128 10:17:01.000703 139865769342720 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.7317183017730713, loss=2.0432915687561035
I0128 10:17:34.917354 139865760950016 logging_writer.py:48] [30500] global_step=30500, grad_norm=4.410864353179932, loss=2.152573585510254
I0128 10:18:08.787709 139865769342720 logging_writer.py:48] [30600] global_step=30600, grad_norm=4.14517068862915, loss=1.946981430053711
I0128 10:18:42.650464 139865760950016 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.902688980102539, loss=1.9282761812210083
I0128 10:19:16.571228 139865769342720 logging_writer.py:48] [30800] global_step=30800, grad_norm=4.261112213134766, loss=2.055910348892212
I0128 10:19:50.447075 139865760950016 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.8490757942199707, loss=2.0522303581237793
I0128 10:20:24.334965 139865769342720 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.3837931156158447, loss=2.0206446647644043
I0128 10:20:58.234344 139865760950016 logging_writer.py:48] [31100] global_step=31100, grad_norm=4.13079833984375, loss=1.9711192846298218
I0128 10:21:32.148242 139865769342720 logging_writer.py:48] [31200] global_step=31200, grad_norm=4.173387050628662, loss=1.9863721132278442
I0128 10:22:06.047234 139865760950016 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.848452568054199, loss=2.0494120121002197
I0128 10:22:40.033819 139865769342720 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.4371817111968994, loss=1.914766550064087
I0128 10:23:05.258399 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:23:11.569798 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:23:20.368784 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:23:22.868007 140027215431488 submission_runner.py:408] Time since start: 11141.57s, 	Step: 31476, 	{'train/accuracy': 0.6308194994926453, 'train/loss': 1.480445384979248, 'validation/accuracy': 0.5906999707221985, 'validation/loss': 1.7181880474090576, 'validation/num_examples': 50000, 'test/accuracy': 0.46960002183914185, 'test/loss': 2.4204015731811523, 'test/num_examples': 10000, 'score': 10748.38038611412, 'total_duration': 11141.573992013931, 'accumulated_submission_time': 10748.38038611412, 'accumulated_eval_time': 391.4266884326935, 'accumulated_logging_time': 0.6621429920196533}
I0128 10:23:22.891773 139865240893184 logging_writer.py:48] [31476] accumulated_eval_time=391.426688, accumulated_logging_time=0.662143, accumulated_submission_time=10748.380386, global_step=31476, preemption_count=0, score=10748.380386, test/accuracy=0.469600, test/loss=2.420402, test/num_examples=10000, total_duration=11141.573992, train/accuracy=0.630819, train/loss=1.480445, validation/accuracy=0.590700, validation/loss=1.718188, validation/num_examples=50000
I0128 10:23:31.348041 139866171975424 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.4207849502563477, loss=1.9552139043807983
I0128 10:24:05.210937 139865240893184 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.4911792278289795, loss=1.9251251220703125
I0128 10:24:39.093420 139866171975424 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.387948751449585, loss=1.9154049158096313
I0128 10:25:12.929536 139865240893184 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.8097612857818604, loss=2.023098945617676
I0128 10:25:46.780281 139866171975424 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.7276129722595215, loss=1.9322664737701416
I0128 10:26:20.648522 139865240893184 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.7287137508392334, loss=2.015186309814453
I0128 10:26:54.539376 139866171975424 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.2748188972473145, loss=1.927003026008606
I0128 10:27:28.421312 139865240893184 logging_writer.py:48] [32200] global_step=32200, grad_norm=4.033280849456787, loss=1.8896057605743408
I0128 10:28:02.325270 139866171975424 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.260193347930908, loss=1.8812041282653809
I0128 10:28:36.177619 139865240893184 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.634033203125, loss=1.9543606042861938
I0128 10:29:10.211392 139866171975424 logging_writer.py:48] [32500] global_step=32500, grad_norm=4.167730808258057, loss=2.0250415802001953
I0128 10:29:44.134248 139865240893184 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.0759692192077637, loss=1.9770053625106812
I0128 10:30:18.029740 139866171975424 logging_writer.py:48] [32700] global_step=32700, grad_norm=4.084574222564697, loss=1.9274299144744873
I0128 10:30:51.947954 139865240893184 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.9599127769470215, loss=2.0767290592193604
I0128 10:31:25.835167 139866171975424 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.5385570526123047, loss=1.9489620923995972
I0128 10:31:53.099435 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:31:59.321928 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:32:08.106892 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:32:10.686277 140027215431488 submission_runner.py:408] Time since start: 11669.39s, 	Step: 32982, 	{'train/accuracy': 0.6412228941917419, 'train/loss': 1.4475443363189697, 'validation/accuracy': 0.598639965057373, 'validation/loss': 1.678407907485962, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.3977839946746826, 'test/num_examples': 10000, 'score': 11258.525603055954, 'total_duration': 11669.392268419266, 'accumulated_submission_time': 11258.525603055954, 'accumulated_eval_time': 409.01349687576294, 'accumulated_logging_time': 0.6950950622558594}
I0128 10:32:10.710373 139865232500480 logging_writer.py:48] [32982] accumulated_eval_time=409.013497, accumulated_logging_time=0.695095, accumulated_submission_time=11258.525603, global_step=32982, preemption_count=0, score=11258.525603, test/accuracy=0.475900, test/loss=2.397784, test/num_examples=10000, total_duration=11669.392268, train/accuracy=0.641223, train/loss=1.447544, validation/accuracy=0.598640, validation/loss=1.678408, validation/num_examples=50000
I0128 10:32:17.154789 139865760950016 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.6875483989715576, loss=1.9356443881988525
I0128 10:32:51.014663 139865232500480 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.765493631362915, loss=1.8994899988174438
I0128 10:33:24.883586 139865760950016 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.4757533073425293, loss=1.8318463563919067
I0128 10:33:58.800323 139865232500480 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.34226131439209, loss=1.9272304773330688
I0128 10:34:32.694503 139865760950016 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.3763391971588135, loss=1.9381557703018188
I0128 10:35:06.595131 139865232500480 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.8663601875305176, loss=1.9690841436386108
I0128 10:35:40.590438 139865760950016 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.5205845832824707, loss=1.9801958799362183
I0128 10:36:14.473177 139865232500480 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.8390543460845947, loss=1.9030585289001465
I0128 10:36:48.377739 139865760950016 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.972470283508301, loss=2.106858253479004
I0128 10:37:22.286659 139865232500480 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.5037150382995605, loss=1.944319725036621
I0128 10:37:56.158643 139865760950016 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.473731756210327, loss=2.0112807750701904
I0128 10:38:30.073303 139865232500480 logging_writer.py:48] [34100] global_step=34100, grad_norm=4.507531642913818, loss=1.9730545282363892
I0128 10:39:03.966545 139865760950016 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.7695538997650146, loss=1.9735108613967896
I0128 10:39:37.877303 139865232500480 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.8929219245910645, loss=1.97413969039917
I0128 10:40:11.764570 139865760950016 logging_writer.py:48] [34400] global_step=34400, grad_norm=4.522957801818848, loss=1.9507601261138916
I0128 10:40:40.713350 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:40:46.929492 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:40:55.575807 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:40:58.185932 140027215431488 submission_runner.py:408] Time since start: 12196.89s, 	Step: 34487, 	{'train/accuracy': 0.6408242583274841, 'train/loss': 1.449402093887329, 'validation/accuracy': 0.5999000072479248, 'validation/loss': 1.667544960975647, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.399554967880249, 'test/num_examples': 10000, 'score': 11768.464093208313, 'total_duration': 12196.891924381256, 'accumulated_submission_time': 11768.464093208313, 'accumulated_eval_time': 426.486044883728, 'accumulated_logging_time': 0.7312412261962891}
I0128 10:40:58.213603 139865224107776 logging_writer.py:48] [34487] accumulated_eval_time=426.486045, accumulated_logging_time=0.731241, accumulated_submission_time=11768.464093, global_step=34487, preemption_count=0, score=11768.464093, test/accuracy=0.475700, test/loss=2.399555, test/num_examples=10000, total_duration=12196.891924, train/accuracy=0.640824, train/loss=1.449402, validation/accuracy=0.599900, validation/loss=1.667545, validation/num_examples=50000
I0128 10:41:02.961596 139866163582720 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.2359437942504883, loss=1.956607460975647
I0128 10:41:36.984493 139865224107776 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.4009549617767334, loss=1.8547887802124023
I0128 10:42:10.852291 139866163582720 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.7180628776550293, loss=1.9631396532058716
I0128 10:42:44.746325 139865224107776 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.3697009086608887, loss=1.9033681154251099
I0128 10:43:18.591882 139866163582720 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.3006765842437744, loss=1.9423222541809082
I0128 10:43:52.496280 139865224107776 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.398512363433838, loss=1.9786943197250366
I0128 10:44:26.387701 139866163582720 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.4038431644439697, loss=1.6617474555969238
I0128 10:45:00.253488 139865224107776 logging_writer.py:48] [35200] global_step=35200, grad_norm=4.47282075881958, loss=1.9010722637176514
I0128 10:45:34.167241 139866163582720 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.595992088317871, loss=1.9371204376220703
I0128 10:46:08.021783 139865224107776 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.617976188659668, loss=1.8314456939697266
I0128 10:46:41.883461 139866163582720 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.9155609607696533, loss=1.950859546661377
I0128 10:47:15.748641 139865224107776 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.474515438079834, loss=1.8552830219268799
I0128 10:47:49.639389 139866163582720 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.162762403488159, loss=1.9133154153823853
I0128 10:48:23.654207 139865224107776 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.339874267578125, loss=1.9512734413146973
I0128 10:48:57.525654 139866163582720 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.631807804107666, loss=1.931700587272644
I0128 10:49:28.485422 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:49:34.914824 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:49:43.863006 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:49:46.545040 140027215431488 submission_runner.py:408] Time since start: 12725.25s, 	Step: 35993, 	{'train/accuracy': 0.668965220451355, 'train/loss': 1.3015565872192383, 'validation/accuracy': 0.5925999879837036, 'validation/loss': 1.6919569969177246, 'validation/num_examples': 50000, 'test/accuracy': 0.4733000099658966, 'test/loss': 2.408229112625122, 'test/num_examples': 10000, 'score': 12278.674576044083, 'total_duration': 12725.251027584076, 'accumulated_submission_time': 12278.674576044083, 'accumulated_eval_time': 444.545640707016, 'accumulated_logging_time': 0.7679879665374756}
I0128 10:49:46.569715 139865760950016 logging_writer.py:48] [35993] accumulated_eval_time=444.545641, accumulated_logging_time=0.767988, accumulated_submission_time=12278.674576, global_step=35993, preemption_count=0, score=12278.674576, test/accuracy=0.473300, test/loss=2.408229, test/num_examples=10000, total_duration=12725.251028, train/accuracy=0.668965, train/loss=1.301557, validation/accuracy=0.592600, validation/loss=1.691957, validation/num_examples=50000
I0128 10:49:49.300104 139865769342720 logging_writer.py:48] [36000] global_step=36000, grad_norm=5.103673934936523, loss=1.8966501951217651
I0128 10:50:23.105853 139865760950016 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.8497536182403564, loss=2.0284154415130615
I0128 10:50:56.962772 139865769342720 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.1838152408599854, loss=1.897438645362854
I0128 10:51:30.849742 139865760950016 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.826331377029419, loss=1.9594087600708008
I0128 10:52:04.684599 139865769342720 logging_writer.py:48] [36400] global_step=36400, grad_norm=4.358632564544678, loss=1.9341578483581543
I0128 10:52:38.549567 139865760950016 logging_writer.py:48] [36500] global_step=36500, grad_norm=4.3659210205078125, loss=1.9384336471557617
I0128 10:53:12.435373 139865769342720 logging_writer.py:48] [36600] global_step=36600, grad_norm=4.031869411468506, loss=1.9161525964736938
I0128 10:53:46.348891 139865760950016 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.6971728801727295, loss=1.8553074598312378
I0128 10:54:20.860466 139865769342720 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.4277231693267822, loss=1.9759631156921387
I0128 10:54:54.751064 139865760950016 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.2859885692596436, loss=1.9153401851654053
I0128 10:55:28.595479 139865769342720 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.5172946453094482, loss=2.061267137527466
I0128 10:56:02.462296 139865760950016 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.2024824619293213, loss=1.9963316917419434
I0128 10:56:36.312039 139865769342720 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.6149704456329346, loss=1.9465529918670654
I0128 10:57:10.200920 139865760950016 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.6203553676605225, loss=2.0267183780670166
I0128 10:57:44.075885 139865769342720 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.9505505561828613, loss=1.9796503782272339
I0128 10:58:16.727178 140027215431488 spec.py:321] Evaluating on the training split.
I0128 10:58:22.997077 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 10:58:31.803612 140027215431488 spec.py:349] Evaluating on the test split.
I0128 10:58:34.298912 140027215431488 submission_runner.py:408] Time since start: 13253.00s, 	Step: 37498, 	{'train/accuracy': 0.6574656963348389, 'train/loss': 1.356908917427063, 'validation/accuracy': 0.6019399762153625, 'validation/loss': 1.649037480354309, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.377382278442383, 'test/num_examples': 10000, 'score': 12788.770513534546, 'total_duration': 13253.0048995018, 'accumulated_submission_time': 12788.770513534546, 'accumulated_eval_time': 462.1173481941223, 'accumulated_logging_time': 0.802004337310791}
I0128 10:58:34.323040 139865240893184 logging_writer.py:48] [37498] accumulated_eval_time=462.117348, accumulated_logging_time=0.802004, accumulated_submission_time=12788.770514, global_step=37498, preemption_count=0, score=12788.770514, test/accuracy=0.480400, test/loss=2.377382, test/num_examples=10000, total_duration=13253.004900, train/accuracy=0.657466, train/loss=1.356909, validation/accuracy=0.601940, validation/loss=1.649037, validation/num_examples=50000
I0128 10:58:35.371369 139865760950016 logging_writer.py:48] [37500] global_step=37500, grad_norm=4.0347394943237305, loss=1.929116129875183
I0128 10:59:09.229665 139865240893184 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.7154836654663086, loss=1.9535623788833618
I0128 10:59:43.088323 139865760950016 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.907667636871338, loss=1.9279389381408691
I0128 11:00:16.949852 139865240893184 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.467355489730835, loss=1.995959997177124
I0128 11:00:50.885480 139865760950016 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.6776063442230225, loss=1.9167861938476562
I0128 11:01:24.754824 139865240893184 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.6051690578460693, loss=1.8740273714065552
I0128 11:01:58.678527 139865760950016 logging_writer.py:48] [38100] global_step=38100, grad_norm=4.869870185852051, loss=1.9749380350112915
I0128 11:02:32.546421 139865240893184 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.603687047958374, loss=1.8467926979064941
I0128 11:03:06.404739 139865760950016 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.1214020252227783, loss=1.8176705837249756
I0128 11:03:40.276825 139865240893184 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.4097046852111816, loss=1.8510377407073975
I0128 11:04:14.144618 139865760950016 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.842000961303711, loss=1.8729596138000488
I0128 11:04:48.039719 139865240893184 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.58282732963562, loss=1.8529702425003052
I0128 11:05:21.931467 139865760950016 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.5097954273223877, loss=1.9100432395935059
I0128 11:05:55.773739 139865240893184 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.6468939781188965, loss=2.017982006072998
I0128 11:06:29.634594 139865760950016 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.265172243118286, loss=1.8505916595458984
I0128 11:07:03.613146 139865240893184 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.378488063812256, loss=2.0098085403442383
I0128 11:07:04.441483 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:07:10.628250 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 11:07:19.503137 140027215431488 spec.py:349] Evaluating on the test split.
I0128 11:07:22.145314 140027215431488 submission_runner.py:408] Time since start: 13780.85s, 	Step: 39004, 	{'train/accuracy': 0.6500119566917419, 'train/loss': 1.40574049949646, 'validation/accuracy': 0.5988199710845947, 'validation/loss': 1.6665188074111938, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.388904571533203, 'test/num_examples': 10000, 'score': 13298.82503771782, 'total_duration': 13780.851271390915, 'accumulated_submission_time': 13298.82503771782, 'accumulated_eval_time': 479.8211030960083, 'accumulated_logging_time': 0.8365299701690674}
I0128 11:07:22.169660 139866163582720 logging_writer.py:48] [39004] accumulated_eval_time=479.821103, accumulated_logging_time=0.836530, accumulated_submission_time=13298.825038, global_step=39004, preemption_count=0, score=13298.825038, test/accuracy=0.472200, test/loss=2.388905, test/num_examples=10000, total_duration=13780.851271, train/accuracy=0.650012, train/loss=1.405740, validation/accuracy=0.598820, validation/loss=1.666519, validation/num_examples=50000
I0128 11:07:54.985451 139866171975424 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.704543113708496, loss=1.947153091430664
I0128 11:08:28.839778 139866163582720 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.763033390045166, loss=1.8698426485061646
I0128 11:09:02.747080 139866171975424 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.6789355278015137, loss=1.9832857847213745
I0128 11:09:36.607935 139866163582720 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.4628114700317383, loss=1.822811245918274
I0128 11:10:10.461161 139866171975424 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.375382900238037, loss=1.9294084310531616
I0128 11:10:44.356430 139866163582720 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.404233455657959, loss=1.8430894613265991
I0128 11:11:18.233513 139866171975424 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.6880531311035156, loss=1.9393279552459717
I0128 11:11:52.087690 139866163582720 logging_writer.py:48] [39800] global_step=39800, grad_norm=4.604757308959961, loss=1.9180724620819092
I0128 11:12:25.987337 139866171975424 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.628419399261475, loss=1.8678333759307861
I0128 11:12:59.875183 139866163582720 logging_writer.py:48] [40000] global_step=40000, grad_norm=4.087289333343506, loss=1.8531361818313599
I0128 11:13:33.760459 139866171975424 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.7682201862335205, loss=1.8807170391082764
I0128 11:14:07.657762 139866163582720 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.3557469844818115, loss=1.8806204795837402
I0128 11:14:41.533779 139866171975424 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.363072156906128, loss=1.879733681678772
I0128 11:15:15.423824 139866163582720 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.8646886348724365, loss=1.989495038986206
I0128 11:15:49.330632 139866171975424 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.9764726161956787, loss=1.9030556678771973
I0128 11:15:52.194075 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:15:58.513995 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 11:16:07.491050 140027215431488 spec.py:349] Evaluating on the test split.
I0128 11:16:10.215719 140027215431488 submission_runner.py:408] Time since start: 14308.92s, 	Step: 40510, 	{'train/accuracy': 0.6475805044174194, 'train/loss': 1.4063873291015625, 'validation/accuracy': 0.6014999747276306, 'validation/loss': 1.6467198133468628, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.3571691513061523, 'test/num_examples': 10000, 'score': 13808.78684091568, 'total_duration': 14308.921383857727, 'accumulated_submission_time': 13808.78684091568, 'accumulated_eval_time': 497.84239530563354, 'accumulated_logging_time': 0.8696949481964111}
I0128 11:16:10.245578 139865240893184 logging_writer.py:48] [40510] accumulated_eval_time=497.842395, accumulated_logging_time=0.869695, accumulated_submission_time=13808.786841, global_step=40510, preemption_count=0, score=13808.786841, test/accuracy=0.478000, test/loss=2.357169, test/num_examples=10000, total_duration=14308.921384, train/accuracy=0.647581, train/loss=1.406387, validation/accuracy=0.601500, validation/loss=1.646720, validation/num_examples=50000
I0128 11:16:41.026445 139865760950016 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.826284170150757, loss=1.914329171180725
I0128 11:17:14.852699 139865240893184 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.873002052307129, loss=1.9412291049957275
I0128 11:17:48.723103 139865760950016 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.5839576721191406, loss=1.9672306776046753
I0128 11:18:22.597270 139865240893184 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.4137039184570312, loss=2.0039782524108887
I0128 11:18:56.487945 139865760950016 logging_writer.py:48] [41000] global_step=41000, grad_norm=5.148126602172852, loss=1.8257415294647217
I0128 11:19:30.385147 139865240893184 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.9801297187805176, loss=2.030333995819092
I0128 11:20:04.282417 139865760950016 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.542616367340088, loss=1.938019871711731
I0128 11:20:38.171344 139865240893184 logging_writer.py:48] [41300] global_step=41300, grad_norm=4.219267845153809, loss=1.966204285621643
I0128 11:21:12.049045 139865760950016 logging_writer.py:48] [41400] global_step=41400, grad_norm=4.161106109619141, loss=2.0268683433532715
I0128 11:21:45.930156 139865240893184 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.3666305541992188, loss=1.8362040519714355
I0128 11:22:19.798592 139865760950016 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.373115301132202, loss=1.9498244524002075
I0128 11:22:53.658485 139865240893184 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.680145025253296, loss=1.984287977218628
I0128 11:23:27.533463 139865760950016 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.8112661838531494, loss=1.898478388786316
I0128 11:24:01.398352 139865240893184 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.3634839057922363, loss=1.9935826063156128
I0128 11:24:35.238643 139865760950016 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.5160584449768066, loss=1.924372673034668
I0128 11:24:40.450158 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:24:46.662331 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 11:24:55.282656 140027215431488 spec.py:349] Evaluating on the test split.
I0128 11:24:57.903653 140027215431488 submission_runner.py:408] Time since start: 14836.61s, 	Step: 42017, 	{'train/accuracy': 0.6520049571990967, 'train/loss': 1.4068289995193481, 'validation/accuracy': 0.6034199595451355, 'validation/loss': 1.6507673263549805, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.4058632850646973, 'test/num_examples': 10000, 'score': 14318.928505182266, 'total_duration': 14836.609621286392, 'accumulated_submission_time': 14318.928505182266, 'accumulated_eval_time': 515.2958283424377, 'accumulated_logging_time': 0.9089441299438477}
I0128 11:24:57.932392 139865232500480 logging_writer.py:48] [42017] accumulated_eval_time=515.295828, accumulated_logging_time=0.908944, accumulated_submission_time=14318.928505, global_step=42017, preemption_count=0, score=14318.928505, test/accuracy=0.480900, test/loss=2.405863, test/num_examples=10000, total_duration=14836.609621, train/accuracy=0.652005, train/loss=1.406829, validation/accuracy=0.603420, validation/loss=1.650767, validation/num_examples=50000
I0128 11:25:26.372257 139865240893184 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.5262258052825928, loss=1.9847499132156372
I0128 11:26:00.196883 139865232500480 logging_writer.py:48] [42200] global_step=42200, grad_norm=4.048000335693359, loss=2.0193939208984375
I0128 11:26:34.174579 139865240893184 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.5340442657470703, loss=1.8536490201950073
I0128 11:27:08.025079 139865232500480 logging_writer.py:48] [42400] global_step=42400, grad_norm=4.853724956512451, loss=1.9893450736999512
I0128 11:27:41.897523 139865240893184 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.877204656600952, loss=1.8633075952529907
I0128 11:28:15.788652 139865232500480 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.77327823638916, loss=1.879887342453003
I0128 11:28:49.662748 139865240893184 logging_writer.py:48] [42700] global_step=42700, grad_norm=4.3587751388549805, loss=1.9677702188491821
I0128 11:29:23.479936 139865232500480 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.8514411449432373, loss=1.9186813831329346
I0128 11:29:57.313705 139865240893184 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.785153865814209, loss=1.929243803024292
I0128 11:30:31.162177 139865232500480 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.7369956970214844, loss=1.8316785097122192
I0128 11:31:05.069005 139865240893184 logging_writer.py:48] [43100] global_step=43100, grad_norm=4.138291358947754, loss=1.874899983406067
I0128 11:31:38.909918 139865232500480 logging_writer.py:48] [43200] global_step=43200, grad_norm=4.171623706817627, loss=1.810233473777771
I0128 11:32:12.771913 139865240893184 logging_writer.py:48] [43300] global_step=43300, grad_norm=4.071080207824707, loss=1.79665207862854
I0128 11:32:46.728631 139865232500480 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.640324831008911, loss=2.0224318504333496
I0128 11:33:20.579725 139865240893184 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.631295919418335, loss=1.9738671779632568
I0128 11:33:28.162953 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:33:34.353602 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 11:33:43.186013 140027215431488 spec.py:349] Evaluating on the test split.
I0128 11:33:45.772072 140027215431488 submission_runner.py:408] Time since start: 15364.48s, 	Step: 43524, 	{'train/accuracy': 0.6497727632522583, 'train/loss': 1.3987377882003784, 'validation/accuracy': 0.6049000024795532, 'validation/loss': 1.6419485807418823, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.3409223556518555, 'test/num_examples': 10000, 'score': 14829.094542264938, 'total_duration': 15364.478039741516, 'accumulated_submission_time': 14829.094542264938, 'accumulated_eval_time': 532.904883146286, 'accumulated_logging_time': 0.9475512504577637}
I0128 11:33:45.797603 139865232500480 logging_writer.py:48] [43524] accumulated_eval_time=532.904883, accumulated_logging_time=0.947551, accumulated_submission_time=14829.094542, global_step=43524, preemption_count=0, score=14829.094542, test/accuracy=0.482000, test/loss=2.340922, test/num_examples=10000, total_duration=15364.478040, train/accuracy=0.649773, train/loss=1.398738, validation/accuracy=0.604900, validation/loss=1.641949, validation/num_examples=50000
I0128 11:34:11.857725 139866163582720 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.201188325881958, loss=1.8097069263458252
I0128 11:34:45.722892 139865232500480 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.1584572792053223, loss=1.9292088747024536
I0128 11:35:19.588868 139866163582720 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.2669432163238525, loss=2.0169453620910645
I0128 11:35:53.418978 139865232500480 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.6110479831695557, loss=2.0333521366119385
I0128 11:36:27.270220 139866163582720 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.2799675464630127, loss=1.962166666984558
I0128 11:37:01.176383 139865232500480 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.6883811950683594, loss=2.0020885467529297
I0128 11:37:35.024753 139866163582720 logging_writer.py:48] [44200] global_step=44200, grad_norm=4.357526779174805, loss=1.95212721824646
I0128 11:38:08.927803 139865232500480 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.675307273864746, loss=1.8896900415420532
I0128 11:38:42.792218 139866163582720 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.8424417972564697, loss=1.9814690351486206
I0128 11:39:16.796802 139865232500480 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.6176161766052246, loss=1.8663926124572754
I0128 11:39:50.631036 139866163582720 logging_writer.py:48] [44600] global_step=44600, grad_norm=4.713367462158203, loss=1.8938393592834473
I0128 11:40:24.493060 139865232500480 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.6071932315826416, loss=1.8207333087921143
I0128 11:40:58.376747 139866163582720 logging_writer.py:48] [44800] global_step=44800, grad_norm=4.340813159942627, loss=1.897487759590149
I0128 11:41:32.248599 139865232500480 logging_writer.py:48] [44900] global_step=44900, grad_norm=4.090590953826904, loss=1.883039116859436
I0128 11:42:06.083461 139866163582720 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.114465236663818, loss=1.9396517276763916
I0128 11:42:16.016791 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:42:22.224027 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 11:42:31.003329 140027215431488 spec.py:349] Evaluating on the test split.
I0128 11:42:33.626388 140027215431488 submission_runner.py:408] Time since start: 15892.33s, 	Step: 45031, 	{'train/accuracy': 0.6812220811843872, 'train/loss': 1.246896505355835, 'validation/accuracy': 0.6029199957847595, 'validation/loss': 1.6404823064804077, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3501029014587402, 'test/num_examples': 10000, 'score': 15339.251924276352, 'total_duration': 15892.332380533218, 'accumulated_submission_time': 15339.251924276352, 'accumulated_eval_time': 550.5144395828247, 'accumulated_logging_time': 0.9823529720306396}
I0128 11:42:33.651453 139865224107776 logging_writer.py:48] [45031] accumulated_eval_time=550.514440, accumulated_logging_time=0.982353, accumulated_submission_time=15339.251924, global_step=45031, preemption_count=0, score=15339.251924, test/accuracy=0.481700, test/loss=2.350103, test/num_examples=10000, total_duration=15892.332381, train/accuracy=0.681222, train/loss=1.246897, validation/accuracy=0.602920, validation/loss=1.640482, validation/num_examples=50000
I0128 11:42:57.355322 139865232500480 logging_writer.py:48] [45100] global_step=45100, grad_norm=4.2628326416015625, loss=1.9481439590454102
I0128 11:43:31.202198 139865224107776 logging_writer.py:48] [45200] global_step=45200, grad_norm=4.534142971038818, loss=1.9901976585388184
I0128 11:44:05.096506 139865232500480 logging_writer.py:48] [45300] global_step=45300, grad_norm=4.209662914276123, loss=1.8883135318756104
I0128 11:44:38.983164 139865224107776 logging_writer.py:48] [45400] global_step=45400, grad_norm=5.272035598754883, loss=1.9986577033996582
I0128 11:45:12.890454 139865232500480 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.959101438522339, loss=1.925394058227539
I0128 11:45:46.836273 139865224107776 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.8939077854156494, loss=1.888917088508606
I0128 11:46:20.727790 139865232500480 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.3380229473114014, loss=1.86855947971344
I0128 11:46:54.613972 139865224107776 logging_writer.py:48] [45800] global_step=45800, grad_norm=4.1770782470703125, loss=1.915378212928772
I0128 11:47:28.453401 139865232500480 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.149511814117432, loss=1.7473068237304688
I0128 11:48:02.306889 139865224107776 logging_writer.py:48] [46000] global_step=46000, grad_norm=4.2704362869262695, loss=1.9286242723464966
I0128 11:48:36.194448 139865232500480 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.912316083908081, loss=1.8969894647598267
I0128 11:49:10.058415 139865224107776 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.969545602798462, loss=2.106309652328491
I0128 11:49:43.910617 139865232500480 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.482665061950684, loss=1.9759025573730469
I0128 11:50:17.786397 139865224107776 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.626190423965454, loss=1.8604778051376343
I0128 11:50:51.669688 139865232500480 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.8713440895080566, loss=1.948380947113037
I0128 11:51:03.709540 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:51:09.941294 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 11:51:18.770416 140027215431488 spec.py:349] Evaluating on the test split.
I0128 11:51:21.355956 140027215431488 submission_runner.py:408] Time since start: 16420.06s, 	Step: 46537, 	{'train/accuracy': 0.6583425998687744, 'train/loss': 1.357541561126709, 'validation/accuracy': 0.600600004196167, 'validation/loss': 1.6586617231369019, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3764281272888184, 'test/num_examples': 10000, 'score': 15849.248585939407, 'total_duration': 16420.061936855316, 'accumulated_submission_time': 15849.248585939407, 'accumulated_eval_time': 568.1608099937439, 'accumulated_logging_time': 1.0166702270507812}
I0128 11:51:21.380844 139865240893184 logging_writer.py:48] [46537] accumulated_eval_time=568.160810, accumulated_logging_time=1.016670, accumulated_submission_time=15849.248586, global_step=46537, preemption_count=0, score=15849.248586, test/accuracy=0.479300, test/loss=2.376428, test/num_examples=10000, total_duration=16420.061937, train/accuracy=0.658343, train/loss=1.357542, validation/accuracy=0.600600, validation/loss=1.658662, validation/num_examples=50000
I0128 11:51:43.118319 139866163582720 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.7186107635498047, loss=1.92063307762146
I0128 11:52:16.969065 139865240893184 logging_writer.py:48] [46700] global_step=46700, grad_norm=4.042886257171631, loss=1.9216177463531494
I0128 11:52:50.812173 139866163582720 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.5582735538482666, loss=1.8092024326324463
I0128 11:53:24.650229 139865240893184 logging_writer.py:48] [46900] global_step=46900, grad_norm=4.0791754722595215, loss=1.9294421672821045
I0128 11:53:58.498631 139866163582720 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.5580365657806396, loss=1.8847399950027466
I0128 11:54:32.391622 139865240893184 logging_writer.py:48] [47100] global_step=47100, grad_norm=4.3057427406311035, loss=2.0210437774658203
I0128 11:55:06.250761 139866163582720 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.957796096801758, loss=1.8396090269088745
I0128 11:55:40.082586 139865240893184 logging_writer.py:48] [47300] global_step=47300, grad_norm=4.077635765075684, loss=1.9226438999176025
I0128 11:56:13.933711 139866163582720 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.7719104290008545, loss=1.9447484016418457
I0128 11:56:47.828949 139865240893184 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.4236791133880615, loss=1.860957145690918
I0128 11:57:21.691077 139866163582720 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.4820520877838135, loss=1.8913764953613281
I0128 11:57:55.555045 139865240893184 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.511348724365234, loss=1.956404685974121
I0128 11:58:29.466272 139866163582720 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.222157001495361, loss=2.022909641265869
I0128 11:59:03.322301 139865240893184 logging_writer.py:48] [47900] global_step=47900, grad_norm=4.333299160003662, loss=1.9686866998672485
I0128 11:59:37.171320 139866163582720 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.7246289253234863, loss=1.915073275566101
I0128 11:59:51.562355 140027215431488 spec.py:321] Evaluating on the training split.
I0128 11:59:58.633973 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:00:07.254717 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:00:09.892699 140027215431488 submission_runner.py:408] Time since start: 16948.60s, 	Step: 48044, 	{'train/accuracy': 0.6581233739852905, 'train/loss': 1.347723126411438, 'validation/accuracy': 0.6038599610328674, 'validation/loss': 1.6352239847183228, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.325078010559082, 'test/num_examples': 10000, 'score': 16359.366846084595, 'total_duration': 16948.598692417145, 'accumulated_submission_time': 16359.366846084595, 'accumulated_eval_time': 586.4911158084869, 'accumulated_logging_time': 1.051117181777954}
I0128 12:00:09.918330 139866171975424 logging_writer.py:48] [48044] accumulated_eval_time=586.491116, accumulated_logging_time=1.051117, accumulated_submission_time=16359.366846, global_step=48044, preemption_count=0, score=16359.366846, test/accuracy=0.488200, test/loss=2.325078, test/num_examples=10000, total_duration=16948.598692, train/accuracy=0.658123, train/loss=1.347723, validation/accuracy=0.603860, validation/loss=1.635224, validation/num_examples=50000
I0128 12:00:29.176369 139866180368128 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.9804747104644775, loss=2.0499930381774902
I0128 12:01:03.037309 139866171975424 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.6523213386535645, loss=1.726605772972107
I0128 12:01:36.855805 139866180368128 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.6251301765441895, loss=1.939536213874817
I0128 12:02:10.705640 139866171975424 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.99100399017334, loss=1.9187541007995605
I0128 12:02:44.537032 139866180368128 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.9168143272399902, loss=1.7304556369781494
I0128 12:03:18.366941 139866171975424 logging_writer.py:48] [48600] global_step=48600, grad_norm=4.060778617858887, loss=1.9350543022155762
I0128 12:03:52.259911 139866180368128 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.8449926376342773, loss=1.7432063817977905
I0128 12:04:26.296513 139866171975424 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.8259947299957275, loss=1.9003503322601318
I0128 12:05:00.155013 139866180368128 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.4710164070129395, loss=1.7985531091690063
I0128 12:05:34.037765 139866171975424 logging_writer.py:48] [49000] global_step=49000, grad_norm=4.055699348449707, loss=1.9851326942443848
I0128 12:06:07.880720 139866180368128 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.6333677768707275, loss=2.0098371505737305
I0128 12:06:41.719474 139866171975424 logging_writer.py:48] [49200] global_step=49200, grad_norm=4.680030345916748, loss=1.7835776805877686
I0128 12:07:15.591601 139866180368128 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.322918653488159, loss=1.8731242418289185
I0128 12:07:49.474864 139866171975424 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.874770164489746, loss=1.7916169166564941
I0128 12:08:23.333223 139866180368128 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.7281484603881836, loss=2.037419557571411
I0128 12:08:40.065868 140027215431488 spec.py:321] Evaluating on the training split.
I0128 12:08:46.306625 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:08:54.950399 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:08:57.562764 140027215431488 submission_runner.py:408] Time since start: 17476.27s, 	Step: 49551, 	{'train/accuracy': 0.6349250674247742, 'train/loss': 1.4702372550964355, 'validation/accuracy': 0.5914999842643738, 'validation/loss': 1.712287425994873, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.4408493041992188, 'test/num_examples': 10000, 'score': 16869.45350933075, 'total_duration': 17476.268760681152, 'accumulated_submission_time': 16869.45350933075, 'accumulated_eval_time': 603.9879791736603, 'accumulated_logging_time': 1.0860350131988525}
I0128 12:08:57.588307 139865240893184 logging_writer.py:48] [49551] accumulated_eval_time=603.987979, accumulated_logging_time=1.086035, accumulated_submission_time=16869.453509, global_step=49551, preemption_count=0, score=16869.453509, test/accuracy=0.469300, test/loss=2.440849, test/num_examples=10000, total_duration=17476.268761, train/accuracy=0.634925, train/loss=1.470237, validation/accuracy=0.591500, validation/loss=1.712287, validation/num_examples=50000
I0128 12:09:14.481528 139865760950016 logging_writer.py:48] [49600] global_step=49600, grad_norm=5.078232765197754, loss=1.8984583616256714
I0128 12:09:48.348648 139865240893184 logging_writer.py:48] [49700] global_step=49700, grad_norm=4.140148162841797, loss=1.9064948558807373
I0128 12:10:22.182044 139865760950016 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.587110757827759, loss=1.7688227891921997
I0128 12:10:56.196745 139865240893184 logging_writer.py:48] [49900] global_step=49900, grad_norm=4.047924518585205, loss=1.9402326345443726
I0128 12:11:30.058557 139865760950016 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.341057062149048, loss=1.8547561168670654
I0128 12:12:03.932408 139865240893184 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.242098093032837, loss=1.865351915359497
I0128 12:12:37.790540 139865760950016 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.7535698413848877, loss=1.9679733514785767
I0128 12:13:11.636857 139865240893184 logging_writer.py:48] [50300] global_step=50300, grad_norm=4.689464092254639, loss=1.8981049060821533
I0128 12:13:45.495723 139865760950016 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.5488758087158203, loss=1.8778622150421143
I0128 12:14:19.331505 139865240893184 logging_writer.py:48] [50500] global_step=50500, grad_norm=4.145661354064941, loss=1.8657381534576416
I0128 12:14:53.168869 139865760950016 logging_writer.py:48] [50600] global_step=50600, grad_norm=4.299923419952393, loss=1.800888180732727
I0128 12:15:27.027369 139865240893184 logging_writer.py:48] [50700] global_step=50700, grad_norm=4.071080684661865, loss=1.8832377195358276
I0128 12:16:00.850140 139865760950016 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.3436689376831055, loss=1.8842219114303589
I0128 12:16:34.687062 139865240893184 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.6036694049835205, loss=1.6847403049468994
I0128 12:17:08.753858 139865760950016 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.9404473304748535, loss=1.8244942426681519
I0128 12:17:27.872901 140027215431488 spec.py:321] Evaluating on the training split.
I0128 12:17:34.078502 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:17:42.605824 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:17:45.264772 140027215431488 submission_runner.py:408] Time since start: 18003.97s, 	Step: 51058, 	{'train/accuracy': 0.6541573405265808, 'train/loss': 1.3928905725479126, 'validation/accuracy': 0.6054399609565735, 'validation/loss': 1.634318232536316, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.329887628555298, 'test/num_examples': 10000, 'score': 17379.67597913742, 'total_duration': 18003.970739126205, 'accumulated_submission_time': 17379.67597913742, 'accumulated_eval_time': 621.3797891139984, 'accumulated_logging_time': 1.1208219528198242}
I0128 12:17:45.297158 139865224107776 logging_writer.py:48] [51058] accumulated_eval_time=621.379789, accumulated_logging_time=1.120822, accumulated_submission_time=17379.675979, global_step=51058, preemption_count=0, score=17379.675979, test/accuracy=0.483800, test/loss=2.329888, test/num_examples=10000, total_duration=18003.970739, train/accuracy=0.654157, train/loss=1.392891, validation/accuracy=0.605440, validation/loss=1.634318, validation/num_examples=50000
I0128 12:17:59.819065 139866163582720 logging_writer.py:48] [51100] global_step=51100, grad_norm=4.5992536544799805, loss=1.8122648000717163
I0128 12:18:33.617973 139865224107776 logging_writer.py:48] [51200] global_step=51200, grad_norm=4.397062301635742, loss=1.8849667310714722
I0128 12:19:07.433492 139866163582720 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.482163906097412, loss=2.009716272354126
I0128 12:19:41.280444 139865224107776 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.096327781677246, loss=1.9185841083526611
I0128 12:20:15.107589 139866163582720 logging_writer.py:48] [51500] global_step=51500, grad_norm=4.72976016998291, loss=2.086785316467285
I0128 12:20:48.938126 139865224107776 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.9089879989624023, loss=1.8570337295532227
I0128 12:21:22.786095 139866163582720 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.812284469604492, loss=1.870887279510498
I0128 12:21:56.663590 139865224107776 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.8078510761260986, loss=1.8510801792144775
I0128 12:22:30.513152 139866163582720 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.4782490730285645, loss=1.8838127851486206
I0128 12:23:04.359209 139865224107776 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.9791557788848877, loss=1.8474889993667603
I0128 12:23:38.363204 139866163582720 logging_writer.py:48] [52100] global_step=52100, grad_norm=4.198032855987549, loss=1.8446099758148193
I0128 12:24:12.205630 139865224107776 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.226900577545166, loss=1.7901240587234497
I0128 12:24:46.032472 139866163582720 logging_writer.py:48] [52300] global_step=52300, grad_norm=5.099822998046875, loss=1.8224124908447266
I0128 12:25:19.858085 139865224107776 logging_writer.py:48] [52400] global_step=52400, grad_norm=4.1520256996154785, loss=1.8228156566619873
I0128 12:25:53.707014 139866163582720 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.947380542755127, loss=1.8849284648895264
I0128 12:26:15.512700 140027215431488 spec.py:321] Evaluating on the training split.
I0128 12:26:21.776962 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:26:30.581528 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:26:33.205159 140027215431488 submission_runner.py:408] Time since start: 18531.91s, 	Step: 52566, 	{'train/accuracy': 0.6519849896430969, 'train/loss': 1.3961294889450073, 'validation/accuracy': 0.6064199805259705, 'validation/loss': 1.6361829042434692, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.343546152114868, 'test/num_examples': 10000, 'score': 17889.827553749084, 'total_duration': 18531.911148548126, 'accumulated_submission_time': 17889.827553749084, 'accumulated_eval_time': 639.0722250938416, 'accumulated_logging_time': 1.1622974872589111}
I0128 12:26:33.235113 139865240893184 logging_writer.py:48] [52566] accumulated_eval_time=639.072225, accumulated_logging_time=1.162297, accumulated_submission_time=17889.827554, global_step=52566, preemption_count=0, score=17889.827554, test/accuracy=0.484500, test/loss=2.343546, test/num_examples=10000, total_duration=18531.911149, train/accuracy=0.651985, train/loss=1.396129, validation/accuracy=0.606420, validation/loss=1.636183, validation/num_examples=50000
I0128 12:26:45.058685 139865760950016 logging_writer.py:48] [52600] global_step=52600, grad_norm=4.100100994110107, loss=1.8236924409866333
I0128 12:27:18.895865 139865240893184 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.723679780960083, loss=1.9317924976348877
I0128 12:27:52.717623 139865760950016 logging_writer.py:48] [52800] global_step=52800, grad_norm=4.6761040687561035, loss=1.8525762557983398
I0128 12:28:26.566041 139865240893184 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.53327751159668, loss=1.8569413423538208
I0128 12:29:00.421772 139865760950016 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.6532583236694336, loss=1.8601009845733643
I0128 12:29:34.270559 139865240893184 logging_writer.py:48] [53100] global_step=53100, grad_norm=4.240317344665527, loss=1.8310930728912354
I0128 12:30:08.206591 139865760950016 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.387671709060669, loss=1.90304434299469
I0128 12:30:42.032345 139865240893184 logging_writer.py:48] [53300] global_step=53300, grad_norm=4.167018890380859, loss=1.9376378059387207
I0128 12:31:15.881845 139865760950016 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.5086543560028076, loss=1.8716729879379272
I0128 12:31:49.727472 139865240893184 logging_writer.py:48] [53500] global_step=53500, grad_norm=4.763943195343018, loss=1.9518213272094727
I0128 12:32:23.580788 139865760950016 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.288897752761841, loss=1.9552066326141357
I0128 12:32:57.446979 139865240893184 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.5896012783050537, loss=1.9049453735351562
I0128 12:33:31.325932 139865760950016 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.388288736343384, loss=1.9456526041030884
I0128 12:34:05.122555 139865240893184 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.576158046722412, loss=1.88504159450531
I0128 12:34:38.993389 139865760950016 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.9606103897094727, loss=1.8145358562469482
I0128 12:35:03.480189 140027215431488 spec.py:321] Evaluating on the training split.
I0128 12:35:09.664316 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:35:18.324730 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:35:20.944461 140027215431488 submission_runner.py:408] Time since start: 19059.65s, 	Step: 54074, 	{'train/accuracy': 0.6962292790412903, 'train/loss': 1.2013616561889648, 'validation/accuracy': 0.6180799603462219, 'validation/loss': 1.5881798267364502, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.290510654449463, 'test/num_examples': 10000, 'score': 18400.011061429977, 'total_duration': 19059.650450468063, 'accumulated_submission_time': 18400.011061429977, 'accumulated_eval_time': 656.536458492279, 'accumulated_logging_time': 1.201387643814087}
I0128 12:35:20.976312 139865240893184 logging_writer.py:48] [54074] accumulated_eval_time=656.536458, accumulated_logging_time=1.201388, accumulated_submission_time=18400.011061, global_step=54074, preemption_count=0, score=18400.011061, test/accuracy=0.493700, test/loss=2.290511, test/num_examples=10000, total_duration=19059.650450, train/accuracy=0.696229, train/loss=1.201362, validation/accuracy=0.618080, validation/loss=1.588180, validation/num_examples=50000
I0128 12:35:30.123010 139866163582720 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.4159963130950928, loss=1.8383526802062988
I0128 12:36:03.964093 139865240893184 logging_writer.py:48] [54200] global_step=54200, grad_norm=4.258331298828125, loss=1.871085524559021
I0128 12:36:37.866327 139866163582720 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.5325417518615723, loss=1.909353494644165
I0128 12:37:11.703775 139865240893184 logging_writer.py:48] [54400] global_step=54400, grad_norm=4.163161277770996, loss=1.7939106225967407
I0128 12:37:45.623205 139866163582720 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.757624387741089, loss=1.8584941625595093
I0128 12:38:19.460002 139865240893184 logging_writer.py:48] [54600] global_step=54600, grad_norm=4.07198429107666, loss=1.830720067024231
I0128 12:38:53.318697 139866163582720 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.7197704315185547, loss=1.8518446683883667
I0128 12:39:27.182479 139865240893184 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.6201157569885254, loss=1.6997718811035156
I0128 12:40:01.058201 139866163582720 logging_writer.py:48] [54900] global_step=54900, grad_norm=4.090229034423828, loss=1.8512009382247925
I0128 12:40:34.910984 139865240893184 logging_writer.py:48] [55000] global_step=55000, grad_norm=4.183417797088623, loss=1.8003545999526978
I0128 12:41:08.754791 139866163582720 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.351979970932007, loss=1.8106549978256226
I0128 12:41:42.631775 139865240893184 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.7793030738830566, loss=1.8482698202133179
I0128 12:42:16.480151 139866163582720 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.6184675693511963, loss=1.9181385040283203
I0128 12:42:50.357595 139865240893184 logging_writer.py:48] [55400] global_step=55400, grad_norm=4.245108604431152, loss=1.80649995803833
I0128 12:43:24.234598 139866163582720 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.365431785583496, loss=1.7471915483474731
I0128 12:43:51.122245 140027215431488 spec.py:321] Evaluating on the training split.
I0128 12:43:57.509278 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:44:06.093422 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:44:08.705220 140027215431488 submission_runner.py:408] Time since start: 19587.41s, 	Step: 55581, 	{'train/accuracy': 0.680086076259613, 'train/loss': 1.259409785270691, 'validation/accuracy': 0.6174600124359131, 'validation/loss': 1.571677565574646, 'validation/num_examples': 50000, 'test/accuracy': 0.5017000436782837, 'test/loss': 2.253861904144287, 'test/num_examples': 10000, 'score': 18910.092315673828, 'total_duration': 19587.41103410721, 'accumulated_submission_time': 18910.092315673828, 'accumulated_eval_time': 674.1192197799683, 'accumulated_logging_time': 1.2447161674499512}
I0128 12:44:08.731480 139865769342720 logging_writer.py:48] [55581] accumulated_eval_time=674.119220, accumulated_logging_time=1.244716, accumulated_submission_time=18910.092316, global_step=55581, preemption_count=0, score=18910.092316, test/accuracy=0.501700, test/loss=2.253862, test/num_examples=10000, total_duration=19587.411034, train/accuracy=0.680086, train/loss=1.259410, validation/accuracy=0.617460, validation/loss=1.571678, validation/num_examples=50000
I0128 12:44:15.536956 139866180368128 logging_writer.py:48] [55600] global_step=55600, grad_norm=4.039318084716797, loss=1.80754554271698
I0128 12:44:49.361290 139865769342720 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.823744058609009, loss=1.7656617164611816
I0128 12:45:23.177704 139866180368128 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.6353001594543457, loss=1.8911221027374268
I0128 12:45:57.005807 139865769342720 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.7399258613586426, loss=1.9111063480377197
I0128 12:46:30.866404 139866180368128 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.86979079246521, loss=1.8566241264343262
I0128 12:47:04.696372 139865769342720 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.93973445892334, loss=1.9096654653549194
I0128 12:47:38.529859 139866180368128 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.8308401107788086, loss=1.8184199333190918
I0128 12:48:12.363772 139865769342720 logging_writer.py:48] [56300] global_step=56300, grad_norm=4.3579630851745605, loss=1.830140233039856
I0128 12:48:46.228204 139866180368128 logging_writer.py:48] [56400] global_step=56400, grad_norm=4.6726884841918945, loss=1.7718400955200195
I0128 12:49:20.157021 139865769342720 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.771444797515869, loss=1.8857632875442505
I0128 12:49:53.994934 139866180368128 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.639631748199463, loss=1.830855369567871
I0128 12:50:27.857584 139865769342720 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.448967218399048, loss=1.8794664144515991
I0128 12:51:01.707181 139866180368128 logging_writer.py:48] [56800] global_step=56800, grad_norm=4.5014567375183105, loss=1.7715723514556885
I0128 12:51:35.536376 139865769342720 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.9361531734466553, loss=1.8188599348068237
I0128 12:52:09.372787 139866180368128 logging_writer.py:48] [57000] global_step=57000, grad_norm=4.052403450012207, loss=1.8758833408355713
I0128 12:52:38.961384 140027215431488 spec.py:321] Evaluating on the training split.
I0128 12:52:45.184681 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 12:52:54.286650 140027215431488 spec.py:349] Evaluating on the test split.
I0128 12:52:56.872549 140027215431488 submission_runner.py:408] Time since start: 20115.58s, 	Step: 57089, 	{'train/accuracy': 0.6758211255073547, 'train/loss': 1.280925989151001, 'validation/accuracy': 0.6241399645805359, 'validation/loss': 1.5517821311950684, 'validation/num_examples': 50000, 'test/accuracy': 0.4967000186443329, 'test/loss': 2.2936458587646484, 'test/num_examples': 10000, 'score': 19420.2594435215, 'total_duration': 20115.578449726105, 'accumulated_submission_time': 19420.2594435215, 'accumulated_eval_time': 692.030259847641, 'accumulated_logging_time': 1.281590461730957}
I0128 12:52:56.900782 139865240893184 logging_writer.py:48] [57089] accumulated_eval_time=692.030260, accumulated_logging_time=1.281590, accumulated_submission_time=19420.259444, global_step=57089, preemption_count=0, score=19420.259444, test/accuracy=0.496700, test/loss=2.293646, test/num_examples=10000, total_duration=20115.578450, train/accuracy=0.675821, train/loss=1.280926, validation/accuracy=0.624140, validation/loss=1.551782, validation/num_examples=50000
I0128 12:53:00.962908 139865760950016 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.7800545692443848, loss=1.8971843719482422
I0128 12:53:34.770807 139865240893184 logging_writer.py:48] [57200] global_step=57200, grad_norm=4.037833213806152, loss=1.904099941253662
I0128 12:54:08.641497 139865760950016 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.477980852127075, loss=1.7788101434707642
I0128 12:54:42.460671 139865240893184 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.892965078353882, loss=1.9057613611221313
I0128 12:55:16.444520 139865760950016 logging_writer.py:48] [57500] global_step=57500, grad_norm=4.13860559463501, loss=1.8458888530731201
I0128 12:55:50.263315 139865240893184 logging_writer.py:48] [57600] global_step=57600, grad_norm=4.099462985992432, loss=1.8515310287475586
I0128 12:56:24.124027 139865760950016 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.745737314224243, loss=1.830716848373413
I0128 12:56:57.991548 139865240893184 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.6991724967956543, loss=1.8707817792892456
I0128 12:57:31.803553 139865760950016 logging_writer.py:48] [57900] global_step=57900, grad_norm=4.012762069702148, loss=1.687074065208435
I0128 12:58:05.680933 139865240893184 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.596569538116455, loss=1.8971415758132935
I0128 12:58:39.498521 139865760950016 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.606438159942627, loss=1.8958247900009155
I0128 12:59:13.351886 139865240893184 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.853318691253662, loss=1.8325371742248535
I0128 12:59:47.145116 139865760950016 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.9721624851226807, loss=1.8034477233886719
I0128 13:00:21.008955 139865240893184 logging_writer.py:48] [58400] global_step=58400, grad_norm=4.113237380981445, loss=1.8589545488357544
I0128 13:00:54.792945 139865760950016 logging_writer.py:48] [58500] global_step=58500, grad_norm=4.079477787017822, loss=1.861965298652649
I0128 13:01:26.910217 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:01:33.125201 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:01:42.101767 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:01:44.724005 140027215431488 submission_runner.py:408] Time since start: 20643.43s, 	Step: 58596, 	{'train/accuracy': 0.6629464030265808, 'train/loss': 1.3316421508789062, 'validation/accuracy': 0.6131399869918823, 'validation/loss': 1.5955449342727661, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.289567470550537, 'test/num_examples': 10000, 'score': 19930.20670747757, 'total_duration': 20643.429992198944, 'accumulated_submission_time': 19930.20670747757, 'accumulated_eval_time': 709.8440093994141, 'accumulated_logging_time': 1.3187220096588135}
I0128 13:01:44.755455 139866171975424 logging_writer.py:48] [58596] accumulated_eval_time=709.844009, accumulated_logging_time=1.318722, accumulated_submission_time=19930.206707, global_step=58596, preemption_count=0, score=19930.206707, test/accuracy=0.495900, test/loss=2.289567, test/num_examples=10000, total_duration=20643.429992, train/accuracy=0.662946, train/loss=1.331642, validation/accuracy=0.613140, validation/loss=1.595545, validation/num_examples=50000
I0128 13:01:46.447472 139866180368128 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.971147298812866, loss=1.7569642066955566
I0128 13:02:20.263796 139866171975424 logging_writer.py:48] [58700] global_step=58700, grad_norm=4.764652252197266, loss=1.833177089691162
I0128 13:02:54.081012 139866180368128 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.524425506591797, loss=1.8329862356185913
I0128 13:03:27.942834 139866171975424 logging_writer.py:48] [58900] global_step=58900, grad_norm=4.2107157707214355, loss=1.8507332801818848
I0128 13:04:01.782239 139866180368128 logging_writer.py:48] [59000] global_step=59000, grad_norm=4.25706148147583, loss=1.800464391708374
I0128 13:04:35.589303 139866171975424 logging_writer.py:48] [59100] global_step=59100, grad_norm=4.310486316680908, loss=1.821658730506897
I0128 13:05:09.415417 139866180368128 logging_writer.py:48] [59200] global_step=59200, grad_norm=4.585628509521484, loss=1.835695743560791
I0128 13:05:43.253163 139866171975424 logging_writer.py:48] [59300] global_step=59300, grad_norm=4.219875335693359, loss=1.7883577346801758
I0128 13:06:17.104263 139866180368128 logging_writer.py:48] [59400] global_step=59400, grad_norm=4.059365749359131, loss=1.9417710304260254
I0128 13:06:50.953404 139866171975424 logging_writer.py:48] [59500] global_step=59500, grad_norm=4.406976222991943, loss=1.790711760520935
I0128 13:07:24.806196 139866180368128 logging_writer.py:48] [59600] global_step=59600, grad_norm=4.178593158721924, loss=1.923095703125
I0128 13:07:58.781909 139866171975424 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.7103397846221924, loss=1.7940608263015747
I0128 13:08:32.606573 139866180368128 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.79486083984375, loss=1.7519614696502686
I0128 13:09:06.437148 139866171975424 logging_writer.py:48] [59900] global_step=59900, grad_norm=4.007377624511719, loss=1.8272254467010498
I0128 13:09:40.292734 139866180368128 logging_writer.py:48] [60000] global_step=60000, grad_norm=4.0840044021606445, loss=1.8391848802566528
I0128 13:10:14.143352 139866171975424 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.7662014961242676, loss=1.7233318090438843
I0128 13:10:14.960068 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:10:21.116390 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:10:30.088359 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:10:33.014579 140027215431488 submission_runner.py:408] Time since start: 21171.72s, 	Step: 60104, 	{'train/accuracy': 0.6681082248687744, 'train/loss': 1.3214025497436523, 'validation/accuracy': 0.6195399761199951, 'validation/loss': 1.5713294744491577, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.2615723609924316, 'test/num_examples': 10000, 'score': 20440.350209712982, 'total_duration': 21171.720563173294, 'accumulated_submission_time': 20440.350209712982, 'accumulated_eval_time': 727.8984732627869, 'accumulated_logging_time': 1.3597385883331299}
I0128 13:10:33.041395 139865760950016 logging_writer.py:48] [60104] accumulated_eval_time=727.898473, accumulated_logging_time=1.359739, accumulated_submission_time=20440.350210, global_step=60104, preemption_count=0, score=20440.350210, test/accuracy=0.499900, test/loss=2.261572, test/num_examples=10000, total_duration=21171.720563, train/accuracy=0.668108, train/loss=1.321403, validation/accuracy=0.619540, validation/loss=1.571329, validation/num_examples=50000
I0128 13:11:05.850823 139865769342720 logging_writer.py:48] [60200] global_step=60200, grad_norm=4.165617942810059, loss=1.8726376295089722
I0128 13:11:39.702269 139865760950016 logging_writer.py:48] [60300] global_step=60300, grad_norm=4.5008721351623535, loss=1.9195852279663086
I0128 13:12:13.522227 139865769342720 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.67618989944458, loss=1.9287126064300537
I0128 13:12:47.388277 139865760950016 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.848158359527588, loss=1.7535356283187866
I0128 13:13:21.217689 139865769342720 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.6149730682373047, loss=1.9379937648773193
I0128 13:13:55.052012 139865760950016 logging_writer.py:48] [60700] global_step=60700, grad_norm=4.1794819831848145, loss=1.7906852960586548
I0128 13:14:28.977790 139865769342720 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.8103384971618652, loss=1.7723989486694336
I0128 13:15:02.830194 139865760950016 logging_writer.py:48] [60900] global_step=60900, grad_norm=4.205408573150635, loss=2.045100212097168
I0128 13:15:36.672848 139865769342720 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.8893327713012695, loss=1.938767433166504
I0128 13:16:10.492040 139865760950016 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.453970193862915, loss=1.9837818145751953
I0128 13:16:44.345395 139865769342720 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.8700146675109863, loss=1.8554476499557495
I0128 13:17:18.206766 139865760950016 logging_writer.py:48] [61300] global_step=61300, grad_norm=4.125885963439941, loss=1.8738459348678589
I0128 13:17:52.029841 139865769342720 logging_writer.py:48] [61400] global_step=61400, grad_norm=4.811830997467041, loss=1.8195964097976685
I0128 13:18:25.876589 139865760950016 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.4082794189453125, loss=1.8581702709197998
I0128 13:18:59.702367 139865769342720 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.712785005569458, loss=1.7951446771621704
I0128 13:19:03.228195 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:19:09.450522 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:19:18.369981 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:19:20.894716 140027215431488 submission_runner.py:408] Time since start: 21699.60s, 	Step: 61612, 	{'train/accuracy': 0.6642019748687744, 'train/loss': 1.337678074836731, 'validation/accuracy': 0.6198999881744385, 'validation/loss': 1.577870488166809, 'validation/num_examples': 50000, 'test/accuracy': 0.49220001697540283, 'test/loss': 2.2768988609313965, 'test/num_examples': 10000, 'score': 20950.476552248, 'total_duration': 21699.60069823265, 'accumulated_submission_time': 20950.476552248, 'accumulated_eval_time': 745.5649440288544, 'accumulated_logging_time': 1.3957176208496094}
I0128 13:19:20.922695 139865224107776 logging_writer.py:48] [61612] accumulated_eval_time=745.564944, accumulated_logging_time=1.395718, accumulated_submission_time=20950.476552, global_step=61612, preemption_count=0, score=20950.476552, test/accuracy=0.492200, test/loss=2.276899, test/num_examples=10000, total_duration=21699.600698, train/accuracy=0.664202, train/loss=1.337678, validation/accuracy=0.619900, validation/loss=1.577870, validation/num_examples=50000
I0128 13:19:51.021550 139865232500480 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.5025031566619873, loss=1.7560367584228516
I0128 13:20:24.825863 139865224107776 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.468071222305298, loss=1.8157609701156616
I0128 13:20:58.844985 139865232500480 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.5457763671875, loss=1.8117268085479736
I0128 13:21:32.680921 139865224107776 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.6263062953948975, loss=1.733453392982483
I0128 13:22:06.518682 139865232500480 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.907755136489868, loss=1.823132038116455
I0128 13:22:40.373073 139865224107776 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.6977338790893555, loss=1.753868818283081
I0128 13:23:14.155584 139865232500480 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.4872312545776367, loss=1.8192822933197021
I0128 13:23:48.009554 139865224107776 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.9440462589263916, loss=1.9341726303100586
I0128 13:24:21.794886 139865232500480 logging_writer.py:48] [62500] global_step=62500, grad_norm=4.597421169281006, loss=1.8704631328582764
I0128 13:24:55.633457 139865224107776 logging_writer.py:48] [62600] global_step=62600, grad_norm=4.363708019256592, loss=1.8394250869750977
I0128 13:25:29.468639 139865232500480 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.8108911514282227, loss=1.8439286947250366
I0128 13:26:03.284466 139865224107776 logging_writer.py:48] [62800] global_step=62800, grad_norm=4.013186454772949, loss=1.8837430477142334
I0128 13:26:37.134129 139865232500480 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.5775256156921387, loss=1.8921464681625366
I0128 13:27:11.137234 139865224107776 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.716026782989502, loss=1.6816489696502686
I0128 13:27:44.984214 139865232500480 logging_writer.py:48] [63100] global_step=63100, grad_norm=5.139547824859619, loss=1.9736557006835938
I0128 13:27:51.234577 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:27:57.452086 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:28:06.180140 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:28:08.755205 140027215431488 submission_runner.py:408] Time since start: 22227.46s, 	Step: 63120, 	{'train/accuracy': 0.6990194320678711, 'train/loss': 1.1689326763153076, 'validation/accuracy': 0.6167399883270264, 'validation/loss': 1.5819714069366455, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.285916805267334, 'test/num_examples': 10000, 'score': 21460.725561141968, 'total_duration': 22227.461196899414, 'accumulated_submission_time': 21460.725561141968, 'accumulated_eval_time': 763.08553647995, 'accumulated_logging_time': 1.4341421127319336}
I0128 13:28:08.788940 139866163582720 logging_writer.py:48] [63120] accumulated_eval_time=763.085536, accumulated_logging_time=1.434142, accumulated_submission_time=21460.725561, global_step=63120, preemption_count=0, score=21460.725561, test/accuracy=0.499900, test/loss=2.285917, test/num_examples=10000, total_duration=22227.461197, train/accuracy=0.699019, train/loss=1.168933, validation/accuracy=0.616740, validation/loss=1.581971, validation/num_examples=50000
I0128 13:28:36.139400 139866171975424 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.7751150131225586, loss=1.8431333303451538
I0128 13:29:09.945755 139866163582720 logging_writer.py:48] [63300] global_step=63300, grad_norm=4.7742600440979, loss=1.7357802391052246
I0128 13:29:43.763783 139866171975424 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.5437614917755127, loss=1.7766456604003906
I0128 13:30:17.597846 139866163582720 logging_writer.py:48] [63500] global_step=63500, grad_norm=4.3170647621154785, loss=1.8404080867767334
I0128 13:30:51.453663 139866171975424 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.7657787799835205, loss=1.892076849937439
I0128 13:31:25.300625 139866163582720 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.964555025100708, loss=1.8634482622146606
I0128 13:31:59.122530 139866171975424 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.935847759246826, loss=1.7761107683181763
I0128 13:32:32.939320 139866163582720 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.6468331813812256, loss=1.7979657649993896
I0128 13:33:06.763753 139866171975424 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.633089303970337, loss=1.7248657941818237
I0128 13:33:40.735290 139866163582720 logging_writer.py:48] [64100] global_step=64100, grad_norm=4.522792816162109, loss=1.7763464450836182
I0128 13:34:14.560617 139866171975424 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.903381109237671, loss=1.6451797485351562
I0128 13:34:48.391777 139866163582720 logging_writer.py:48] [64300] global_step=64300, grad_norm=4.031569480895996, loss=1.754425287246704
I0128 13:35:22.237319 139866171975424 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.5300867557525635, loss=1.7391512393951416
I0128 13:35:56.056171 139866163582720 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.582860231399536, loss=1.8204278945922852
I0128 13:36:29.906545 139866171975424 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.4577443599700928, loss=1.7717335224151611
I0128 13:36:38.837130 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:36:45.102824 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:36:54.087380 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:36:56.702466 140027215431488 submission_runner.py:408] Time since start: 22755.41s, 	Step: 64628, 	{'train/accuracy': 0.6875, 'train/loss': 1.2308303117752075, 'validation/accuracy': 0.6247199773788452, 'validation/loss': 1.5516197681427002, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.266094207763672, 'test/num_examples': 10000, 'score': 21970.70988535881, 'total_duration': 22755.408446788788, 'accumulated_submission_time': 21970.70988535881, 'accumulated_eval_time': 780.9508357048035, 'accumulated_logging_time': 1.477266550064087}
I0128 13:36:56.734894 139865240893184 logging_writer.py:48] [64628] accumulated_eval_time=780.950836, accumulated_logging_time=1.477267, accumulated_submission_time=21970.709885, global_step=64628, preemption_count=0, score=21970.709885, test/accuracy=0.499700, test/loss=2.266094, test/num_examples=10000, total_duration=22755.408447, train/accuracy=0.687500, train/loss=1.230830, validation/accuracy=0.624720, validation/loss=1.551620, validation/num_examples=50000
I0128 13:37:21.417226 139865760950016 logging_writer.py:48] [64700] global_step=64700, grad_norm=4.026586055755615, loss=1.7837775945663452
I0128 13:37:55.243271 139865240893184 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.994366407394409, loss=1.7759897708892822
I0128 13:38:29.061790 139865760950016 logging_writer.py:48] [64900] global_step=64900, grad_norm=4.32246732711792, loss=1.7941157817840576
I0128 13:39:02.911354 139865240893184 logging_writer.py:48] [65000] global_step=65000, grad_norm=4.914578437805176, loss=1.803632140159607
I0128 13:39:36.866008 139865760950016 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.560068368911743, loss=1.8735967874526978
I0128 13:40:10.718361 139865240893184 logging_writer.py:48] [65200] global_step=65200, grad_norm=4.4335036277771, loss=1.8138922452926636
I0128 13:40:44.574385 139865760950016 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.9659488201141357, loss=1.9094703197479248
I0128 13:41:18.381145 139865240893184 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.550459623336792, loss=1.8435032367706299
I0128 13:41:52.224432 139865760950016 logging_writer.py:48] [65500] global_step=65500, grad_norm=4.2003302574157715, loss=1.839705228805542
I0128 13:42:26.013417 139865240893184 logging_writer.py:48] [65600] global_step=65600, grad_norm=4.041382312774658, loss=1.7715290784835815
I0128 13:42:59.860036 139865760950016 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.6492538452148438, loss=1.7246615886688232
I0128 13:43:33.696227 139865240893184 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.4848005771636963, loss=1.7696282863616943
I0128 13:44:07.526061 139865760950016 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.6390485763549805, loss=1.7781587839126587
I0128 13:44:41.345516 139865240893184 logging_writer.py:48] [66000] global_step=66000, grad_norm=4.0583319664001465, loss=1.8813393115997314
I0128 13:45:15.172245 139865760950016 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.9919238090515137, loss=1.7435771226882935
I0128 13:45:26.828786 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:45:33.042798 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:45:41.745724 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:45:44.332379 140027215431488 submission_runner.py:408] Time since start: 23283.04s, 	Step: 66136, 	{'train/accuracy': 0.6802455186843872, 'train/loss': 1.267142415046692, 'validation/accuracy': 0.6260600090026855, 'validation/loss': 1.5435158014297485, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2609455585479736, 'test/num_examples': 10000, 'score': 22480.74132156372, 'total_duration': 23283.03837132454, 'accumulated_submission_time': 22480.74132156372, 'accumulated_eval_time': 798.4543952941895, 'accumulated_logging_time': 1.5191307067871094}
I0128 13:45:44.362216 139866171975424 logging_writer.py:48] [66136] accumulated_eval_time=798.454395, accumulated_logging_time=1.519131, accumulated_submission_time=22480.741322, global_step=66136, preemption_count=0, score=22480.741322, test/accuracy=0.504100, test/loss=2.260946, test/num_examples=10000, total_duration=23283.038371, train/accuracy=0.680246, train/loss=1.267142, validation/accuracy=0.626060, validation/loss=1.543516, validation/num_examples=50000
I0128 13:46:06.427203 139866180368128 logging_writer.py:48] [66200] global_step=66200, grad_norm=4.397244453430176, loss=1.898463487625122
I0128 13:46:40.257816 139866171975424 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.7475197315216064, loss=1.859330654144287
I0128 13:47:14.063632 139866180368128 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.841290235519409, loss=1.806218147277832
I0128 13:47:47.906593 139866171975424 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.9826126098632812, loss=1.7486826181411743
I0128 13:48:21.736716 139866180368128 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.5983071327209473, loss=1.8072474002838135
I0128 13:48:55.551949 139866171975424 logging_writer.py:48] [66700] global_step=66700, grad_norm=4.5131049156188965, loss=1.8800123929977417
I0128 13:49:29.374487 139866180368128 logging_writer.py:48] [66800] global_step=66800, grad_norm=4.581377029418945, loss=1.80863356590271
I0128 13:50:03.199086 139866171975424 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.977773427963257, loss=1.8555247783660889
I0128 13:50:37.028379 139866180368128 logging_writer.py:48] [67000] global_step=67000, grad_norm=4.167977809906006, loss=1.8224645853042603
I0128 13:51:10.860731 139866171975424 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.629013776779175, loss=1.8076647520065308
I0128 13:51:44.666729 139866180368128 logging_writer.py:48] [67200] global_step=67200, grad_norm=4.472995758056641, loss=1.7783827781677246
I0128 13:52:18.585900 139866171975424 logging_writer.py:48] [67300] global_step=67300, grad_norm=4.319244861602783, loss=1.7418955564498901
I0128 13:52:52.426237 139866180368128 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.669416666030884, loss=1.7289092540740967
I0128 13:53:26.288169 139866171975424 logging_writer.py:48] [67500] global_step=67500, grad_norm=4.768314361572266, loss=1.7749464511871338
I0128 13:54:00.109525 139866180368128 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.592804193496704, loss=1.8273937702178955
I0128 13:54:14.482879 140027215431488 spec.py:321] Evaluating on the training split.
I0128 13:54:20.749228 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 13:54:29.429195 140027215431488 spec.py:349] Evaluating on the test split.
I0128 13:54:31.967919 140027215431488 submission_runner.py:408] Time since start: 23810.67s, 	Step: 67644, 	{'train/accuracy': 0.6720344424247742, 'train/loss': 1.2921645641326904, 'validation/accuracy': 0.624779999256134, 'validation/loss': 1.5412269830703735, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.2861030101776123, 'test/num_examples': 10000, 'score': 22990.79816222191, 'total_duration': 23810.67390537262, 'accumulated_submission_time': 22990.79816222191, 'accumulated_eval_time': 815.9394180774689, 'accumulated_logging_time': 1.5590641498565674}
I0128 13:54:31.997687 139865240893184 logging_writer.py:48] [67644] accumulated_eval_time=815.939418, accumulated_logging_time=1.559064, accumulated_submission_time=22990.798162, global_step=67644, preemption_count=0, score=22990.798162, test/accuracy=0.495300, test/loss=2.286103, test/num_examples=10000, total_duration=23810.673905, train/accuracy=0.672034, train/loss=1.292165, validation/accuracy=0.624780, validation/loss=1.541227, validation/num_examples=50000
I0128 13:54:51.250438 139865760950016 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.7506706714630127, loss=1.7134442329406738
I0128 13:55:25.061974 139865240893184 logging_writer.py:48] [67800] global_step=67800, grad_norm=4.434528827667236, loss=1.8260705471038818
I0128 13:55:58.882419 139865760950016 logging_writer.py:48] [67900] global_step=67900, grad_norm=4.229590892791748, loss=1.7691420316696167
I0128 13:56:32.698848 139865240893184 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.7799806594848633, loss=1.8301582336425781
I0128 13:57:06.552500 139865760950016 logging_writer.py:48] [68100] global_step=68100, grad_norm=4.228023052215576, loss=1.7698434591293335
I0128 13:57:40.408107 139865240893184 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.88098406791687, loss=1.7818632125854492
I0128 13:58:14.246002 139865760950016 logging_writer.py:48] [68300] global_step=68300, grad_norm=4.171462059020996, loss=1.7450910806655884
I0128 13:58:48.193670 139865240893184 logging_writer.py:48] [68400] global_step=68400, grad_norm=4.493406772613525, loss=1.7896630764007568
I0128 13:59:22.060251 139865760950016 logging_writer.py:48] [68500] global_step=68500, grad_norm=4.588710784912109, loss=1.874070405960083
I0128 13:59:55.897633 139865240893184 logging_writer.py:48] [68600] global_step=68600, grad_norm=4.716363906860352, loss=1.7255220413208008
I0128 14:00:29.725489 139865760950016 logging_writer.py:48] [68700] global_step=68700, grad_norm=4.153574466705322, loss=1.7948634624481201
I0128 14:01:03.537560 139865240893184 logging_writer.py:48] [68800] global_step=68800, grad_norm=4.106506824493408, loss=1.7846553325653076
I0128 14:01:37.418284 139865760950016 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.629375457763672, loss=1.8366682529449463
I0128 14:02:11.280957 139865240893184 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.664809226989746, loss=1.7190155982971191
I0128 14:02:45.110502 139865760950016 logging_writer.py:48] [69100] global_step=69100, grad_norm=4.458841800689697, loss=1.9136685132980347
I0128 14:03:02.180125 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:03:08.390797 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:03:17.037795 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:03:19.549050 140027215431488 submission_runner.py:408] Time since start: 24338.26s, 	Step: 69152, 	{'train/accuracy': 0.6754822731018066, 'train/loss': 1.2919354438781738, 'validation/accuracy': 0.626800000667572, 'validation/loss': 1.539273738861084, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.2561073303222656, 'test/num_examples': 10000, 'score': 23500.92076444626, 'total_duration': 24338.255034685135, 'accumulated_submission_time': 23500.92076444626, 'accumulated_eval_time': 833.3083035945892, 'accumulated_logging_time': 1.5970518589019775}
I0128 14:03:19.582048 139866163582720 logging_writer.py:48] [69152] accumulated_eval_time=833.308304, accumulated_logging_time=1.597052, accumulated_submission_time=23500.920764, global_step=69152, preemption_count=0, score=23500.920764, test/accuracy=0.499600, test/loss=2.256107, test/num_examples=10000, total_duration=24338.255035, train/accuracy=0.675482, train/loss=1.291935, validation/accuracy=0.626800, validation/loss=1.539274, validation/num_examples=50000
I0128 14:03:36.161941 139866171975424 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.6135473251342773, loss=1.7077748775482178
I0128 14:04:09.972487 139866163582720 logging_writer.py:48] [69300] global_step=69300, grad_norm=4.625568866729736, loss=1.7251794338226318
I0128 14:04:43.750692 139866171975424 logging_writer.py:48] [69400] global_step=69400, grad_norm=5.359853744506836, loss=1.6783097982406616
I0128 14:05:17.677910 139866163582720 logging_writer.py:48] [69500] global_step=69500, grad_norm=4.036044597625732, loss=1.8295066356658936
I0128 14:05:51.548702 139866171975424 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.505375862121582, loss=1.647813320159912
I0128 14:06:25.388023 139866163582720 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.835574150085449, loss=1.7400636672973633
I0128 14:06:59.235006 139866171975424 logging_writer.py:48] [69800] global_step=69800, grad_norm=4.7192277908325195, loss=1.820740818977356
I0128 14:07:33.068777 139866163582720 logging_writer.py:48] [69900] global_step=69900, grad_norm=4.77565860748291, loss=1.8084278106689453
I0128 14:08:06.918218 139866171975424 logging_writer.py:48] [70000] global_step=70000, grad_norm=4.292618274688721, loss=1.7976995706558228
I0128 14:08:40.798556 139866163582720 logging_writer.py:48] [70100] global_step=70100, grad_norm=4.164241790771484, loss=1.7257983684539795
I0128 14:09:14.597548 139866171975424 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.5309762954711914, loss=1.7065374851226807
I0128 14:09:48.447689 139866163582720 logging_writer.py:48] [70300] global_step=70300, grad_norm=4.421632289886475, loss=1.7150405645370483
I0128 14:10:22.279060 139866171975424 logging_writer.py:48] [70400] global_step=70400, grad_norm=4.108333110809326, loss=1.7925479412078857
I0128 14:10:56.113340 139866163582720 logging_writer.py:48] [70500] global_step=70500, grad_norm=4.004658222198486, loss=1.74332594871521
I0128 14:11:30.089816 139866171975424 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.87442946434021, loss=1.8027424812316895
I0128 14:11:49.846950 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:11:56.095378 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:12:05.046939 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:12:07.797103 140027215431488 submission_runner.py:408] Time since start: 24866.50s, 	Step: 70660, 	{'train/accuracy': 0.6835139989852905, 'train/loss': 1.258325219154358, 'validation/accuracy': 0.6360599994659424, 'validation/loss': 1.496769666671753, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.2187583446502686, 'test/num_examples': 10000, 'score': 24011.123149871826, 'total_duration': 24866.50309228897, 'accumulated_submission_time': 24011.123149871826, 'accumulated_eval_time': 851.2584345340729, 'accumulated_logging_time': 1.6391994953155518}
I0128 14:12:07.826133 139865224107776 logging_writer.py:48] [70660] accumulated_eval_time=851.258435, accumulated_logging_time=1.639199, accumulated_submission_time=24011.123150, global_step=70660, preemption_count=0, score=24011.123150, test/accuracy=0.505600, test/loss=2.218758, test/num_examples=10000, total_duration=24866.503092, train/accuracy=0.683514, train/loss=1.258325, validation/accuracy=0.636060, validation/loss=1.496770, validation/num_examples=50000
I0128 14:12:21.731216 139865232500480 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.620081901550293, loss=1.7048606872558594
I0128 14:12:55.552541 139865224107776 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.731461524963379, loss=1.745794415473938
I0128 14:13:29.398472 139865232500480 logging_writer.py:48] [70900] global_step=70900, grad_norm=4.4512529373168945, loss=1.8354620933532715
I0128 14:14:03.224858 139865224107776 logging_writer.py:48] [71000] global_step=71000, grad_norm=4.044696807861328, loss=1.8499449491500854
I0128 14:14:37.071637 139865232500480 logging_writer.py:48] [71100] global_step=71100, grad_norm=4.1405253410339355, loss=1.914945125579834
I0128 14:15:10.889333 139865224107776 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.699507474899292, loss=1.865958333015442
I0128 14:15:44.725913 139865232500480 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.83923602104187, loss=1.6889275312423706
I0128 14:16:18.544363 139865224107776 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.737609386444092, loss=1.7601181268692017
I0128 14:16:52.368955 139865232500480 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.849283218383789, loss=1.8329976797103882
I0128 14:17:26.175427 139865224107776 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.6808576583862305, loss=1.8894009590148926
I0128 14:18:00.077873 139865232500480 logging_writer.py:48] [71700] global_step=71700, grad_norm=4.056038856506348, loss=1.731872320175171
I0128 14:18:33.944007 139865224107776 logging_writer.py:48] [71800] global_step=71800, grad_norm=4.273508071899414, loss=1.794643759727478
I0128 14:19:07.780521 139865232500480 logging_writer.py:48] [71900] global_step=71900, grad_norm=4.651276111602783, loss=1.7866560220718384
I0128 14:19:41.607901 139865224107776 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.303450107574463, loss=1.874873399734497
I0128 14:20:15.445133 139865232500480 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.462419271469116, loss=1.7891569137573242
I0128 14:20:37.920414 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:20:44.105894 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:20:52.928673 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:20:55.545468 140027215431488 submission_runner.py:408] Time since start: 25394.25s, 	Step: 72168, 	{'train/accuracy': 0.7145846486091614, 'train/loss': 1.0963265895843506, 'validation/accuracy': 0.6308599710464478, 'validation/loss': 1.5195053815841675, 'validation/num_examples': 50000, 'test/accuracy': 0.5113000273704529, 'test/loss': 2.2167670726776123, 'test/num_examples': 10000, 'score': 24521.15534877777, 'total_duration': 25394.251450777054, 'accumulated_submission_time': 24521.15534877777, 'accumulated_eval_time': 868.8834428787231, 'accumulated_logging_time': 1.677199125289917}
I0128 14:20:55.575835 139866171975424 logging_writer.py:48] [72168] accumulated_eval_time=868.883443, accumulated_logging_time=1.677199, accumulated_submission_time=24521.155349, global_step=72168, preemption_count=0, score=24521.155349, test/accuracy=0.511300, test/loss=2.216767, test/num_examples=10000, total_duration=25394.251451, train/accuracy=0.714585, train/loss=1.096327, validation/accuracy=0.630860, validation/loss=1.519505, validation/num_examples=50000
I0128 14:21:06.714556 139866180368128 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.51899790763855, loss=1.703296184539795
I0128 14:21:40.493965 139866171975424 logging_writer.py:48] [72300] global_step=72300, grad_norm=4.4538445472717285, loss=1.8177392482757568
I0128 14:22:14.321310 139866180368128 logging_writer.py:48] [72400] global_step=72400, grad_norm=4.2271037101745605, loss=1.826416254043579
I0128 14:22:48.114608 139866171975424 logging_writer.py:48] [72500] global_step=72500, grad_norm=4.075352191925049, loss=1.9037655591964722
I0128 14:23:21.931901 139866180368128 logging_writer.py:48] [72600] global_step=72600, grad_norm=4.071858882904053, loss=1.9247053861618042
I0128 14:23:55.777737 139866171975424 logging_writer.py:48] [72700] global_step=72700, grad_norm=4.150609016418457, loss=1.7675949335098267
I0128 14:24:29.708398 139866180368128 logging_writer.py:48] [72800] global_step=72800, grad_norm=4.093103885650635, loss=1.7539643049240112
I0128 14:25:03.562213 139866171975424 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.992929220199585, loss=1.7344660758972168
I0128 14:25:37.416687 139866180368128 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.69927716255188, loss=1.7481920719146729
I0128 14:26:11.262886 139866171975424 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.9850127696990967, loss=1.7822753190994263
I0128 14:26:45.090416 139866180368128 logging_writer.py:48] [73200] global_step=73200, grad_norm=4.0388031005859375, loss=1.7414313554763794
I0128 14:27:18.928961 139866171975424 logging_writer.py:48] [73300] global_step=73300, grad_norm=4.223296642303467, loss=1.7050318717956543
I0128 14:27:52.746285 139866180368128 logging_writer.py:48] [73400] global_step=73400, grad_norm=4.151447772979736, loss=1.8086662292480469
I0128 14:28:26.560966 139866171975424 logging_writer.py:48] [73500] global_step=73500, grad_norm=4.489145278930664, loss=1.8008315563201904
I0128 14:29:00.381229 139866180368128 logging_writer.py:48] [73600] global_step=73600, grad_norm=5.092217445373535, loss=1.7683180570602417
I0128 14:29:25.548049 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:29:31.728044 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:29:40.635723 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:29:43.272225 140027215431488 submission_runner.py:408] Time since start: 25921.98s, 	Step: 73676, 	{'train/accuracy': 0.698660671710968, 'train/loss': 1.1710692644119263, 'validation/accuracy': 0.6325399875640869, 'validation/loss': 1.5120478868484497, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.2187116146087646, 'test/num_examples': 10000, 'score': 25031.063472747803, 'total_duration': 25921.97821545601, 'accumulated_submission_time': 25031.063472747803, 'accumulated_eval_time': 886.6075978279114, 'accumulated_logging_time': 1.7175266742706299}
I0128 14:29:43.303713 139865240893184 logging_writer.py:48] [73676] accumulated_eval_time=886.607598, accumulated_logging_time=1.717527, accumulated_submission_time=25031.063473, global_step=73676, preemption_count=0, score=25031.063473, test/accuracy=0.509900, test/loss=2.218712, test/num_examples=10000, total_duration=25921.978215, train/accuracy=0.698661, train/loss=1.171069, validation/accuracy=0.632540, validation/loss=1.512048, validation/num_examples=50000
I0128 14:29:51.780755 139865760950016 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.817512273788452, loss=1.799651861190796
I0128 14:30:25.682292 139865240893184 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.7102205753326416, loss=1.8668296337127686
I0128 14:30:59.483508 139865760950016 logging_writer.py:48] [73900] global_step=73900, grad_norm=4.278139114379883, loss=1.7688134908676147
I0128 14:31:33.268405 139865240893184 logging_writer.py:48] [74000] global_step=74000, grad_norm=4.233357906341553, loss=1.7157843112945557
I0128 14:32:07.107993 139865760950016 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.8374743461608887, loss=1.7381969690322876
I0128 14:32:40.905435 139865240893184 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.6067392826080322, loss=1.7441028356552124
I0128 14:33:14.739806 139865760950016 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.846070289611816, loss=1.8107187747955322
I0128 14:33:48.554685 139865240893184 logging_writer.py:48] [74400] global_step=74400, grad_norm=4.06571102142334, loss=1.8827153444290161
I0128 14:34:22.406450 139865760950016 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.9591238498687744, loss=1.782582402229309
I0128 14:34:56.214641 139865240893184 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.8996126651763916, loss=1.8351936340332031
I0128 14:35:30.037485 139865760950016 logging_writer.py:48] [74700] global_step=74700, grad_norm=4.212183475494385, loss=1.8211816549301147
I0128 14:36:03.853578 139865240893184 logging_writer.py:48] [74800] global_step=74800, grad_norm=4.366143226623535, loss=1.6565015316009521
I0128 14:36:37.685432 139865760950016 logging_writer.py:48] [74900] global_step=74900, grad_norm=4.030428409576416, loss=1.8323330879211426
I0128 14:37:11.591866 139865240893184 logging_writer.py:48] [75000] global_step=75000, grad_norm=4.575010299682617, loss=1.8620506525039673
I0128 14:37:45.418339 139865760950016 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.7370002269744873, loss=1.7304736375808716
I0128 14:38:13.353283 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:38:19.542711 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:38:28.510695 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:38:31.153076 140027215431488 submission_runner.py:408] Time since start: 26449.86s, 	Step: 75184, 	{'train/accuracy': 0.6887555718421936, 'train/loss': 1.2242296934127808, 'validation/accuracy': 0.6289199590682983, 'validation/loss': 1.5229800939559937, 'validation/num_examples': 50000, 'test/accuracy': 0.5022000074386597, 'test/loss': 2.2825558185577393, 'test/num_examples': 10000, 'score': 25541.049762248993, 'total_duration': 26449.859035730362, 'accumulated_submission_time': 25541.049762248993, 'accumulated_eval_time': 904.4073250293732, 'accumulated_logging_time': 1.7588274478912354}
I0128 14:38:31.193488 139865240893184 logging_writer.py:48] [75184] accumulated_eval_time=904.407325, accumulated_logging_time=1.758827, accumulated_submission_time=25541.049762, global_step=75184, preemption_count=0, score=25541.049762, test/accuracy=0.502200, test/loss=2.282556, test/num_examples=10000, total_duration=26449.859036, train/accuracy=0.688756, train/loss=1.224230, validation/accuracy=0.628920, validation/loss=1.522980, validation/num_examples=50000
I0128 14:38:36.962430 139866171975424 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.696958303451538, loss=1.741143822669983
I0128 14:39:10.791852 139865240893184 logging_writer.py:48] [75300] global_step=75300, grad_norm=4.063018798828125, loss=1.7027573585510254
I0128 14:39:44.635150 139866171975424 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.495734691619873, loss=1.6258867979049683
I0128 14:40:18.489950 139865240893184 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.289809226989746, loss=1.751822590827942
I0128 14:40:52.341602 139866171975424 logging_writer.py:48] [75600] global_step=75600, grad_norm=4.540640830993652, loss=1.7905821800231934
I0128 14:41:26.188172 139865240893184 logging_writer.py:48] [75700] global_step=75700, grad_norm=4.32692813873291, loss=1.8008766174316406
I0128 14:42:00.012619 139866171975424 logging_writer.py:48] [75800] global_step=75800, grad_norm=4.639347076416016, loss=1.7918545007705688
I0128 14:42:33.833258 139865240893184 logging_writer.py:48] [75900] global_step=75900, grad_norm=4.526854515075684, loss=1.806391954421997
I0128 14:43:07.741461 139866171975424 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.9176862239837646, loss=1.694319725036621
I0128 14:43:41.614645 139865240893184 logging_writer.py:48] [76100] global_step=76100, grad_norm=4.041715621948242, loss=1.7191674709320068
I0128 14:44:15.448768 139866171975424 logging_writer.py:48] [76200] global_step=76200, grad_norm=4.476136207580566, loss=1.590003252029419
I0128 14:44:49.295303 139865240893184 logging_writer.py:48] [76300] global_step=76300, grad_norm=4.194316864013672, loss=1.6626012325286865
I0128 14:45:23.168231 139866171975424 logging_writer.py:48] [76400] global_step=76400, grad_norm=4.357485771179199, loss=1.7815101146697998
I0128 14:45:57.021257 139865240893184 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.9042463302612305, loss=1.6955909729003906
I0128 14:46:30.853004 139866171975424 logging_writer.py:48] [76600] global_step=76600, grad_norm=4.277634620666504, loss=1.8146858215332031
I0128 14:47:01.452559 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:47:07.748414 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:47:16.651157 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:47:19.280874 140027215431488 submission_runner.py:408] Time since start: 26977.99s, 	Step: 76692, 	{'train/accuracy': 0.6845503449440002, 'train/loss': 1.2397161722183228, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.4972995519638062, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.2224087715148926, 'test/num_examples': 10000, 'score': 26051.24068403244, 'total_duration': 26977.98685336113, 'accumulated_submission_time': 26051.24068403244, 'accumulated_eval_time': 922.2355952262878, 'accumulated_logging_time': 1.8136036396026611}
I0128 14:47:19.312439 139865240893184 logging_writer.py:48] [76692] accumulated_eval_time=922.235595, accumulated_logging_time=1.813604, accumulated_submission_time=26051.240684, global_step=76692, preemption_count=0, score=26051.240684, test/accuracy=0.505100, test/loss=2.222409, test/num_examples=10000, total_duration=26977.986853, train/accuracy=0.684550, train/loss=1.239716, validation/accuracy=0.632680, validation/loss=1.497300, validation/num_examples=50000
I0128 14:47:22.354639 139865760950016 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.9204559326171875, loss=1.8675185441970825
I0128 14:47:56.179105 139865240893184 logging_writer.py:48] [76800] global_step=76800, grad_norm=4.0320258140563965, loss=1.739933967590332
I0128 14:48:30.057703 139865760950016 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.9921579360961914, loss=1.6965441703796387
I0128 14:49:03.913796 139865240893184 logging_writer.py:48] [77000] global_step=77000, grad_norm=4.351263523101807, loss=1.6549608707427979
I0128 14:49:37.791399 139865760950016 logging_writer.py:48] [77100] global_step=77100, grad_norm=4.069894313812256, loss=1.7722804546356201
I0128 14:50:11.607047 139865240893184 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.7642693519592285, loss=1.6557435989379883
I0128 14:50:45.487772 139865760950016 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.9170989990234375, loss=1.7591795921325684
I0128 14:51:19.345597 139865240893184 logging_writer.py:48] [77400] global_step=77400, grad_norm=5.532930850982666, loss=1.7870399951934814
I0128 14:51:53.164469 139865760950016 logging_writer.py:48] [77500] global_step=77500, grad_norm=5.105004787445068, loss=1.7764275074005127
I0128 14:52:27.017447 139865240893184 logging_writer.py:48] [77600] global_step=77600, grad_norm=4.130629539489746, loss=1.7226656675338745
I0128 14:53:00.843415 139865760950016 logging_writer.py:48] [77700] global_step=77700, grad_norm=4.296389579772949, loss=1.7170236110687256
I0128 14:53:34.666504 139865240893184 logging_writer.py:48] [77800] global_step=77800, grad_norm=4.051342010498047, loss=1.7934885025024414
I0128 14:54:08.458640 139865760950016 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.9599661827087402, loss=1.670369267463684
I0128 14:54:42.294429 139865240893184 logging_writer.py:48] [78000] global_step=78000, grad_norm=4.452681541442871, loss=1.6908291578292847
I0128 14:55:16.121084 139865760950016 logging_writer.py:48] [78100] global_step=78100, grad_norm=4.347780227661133, loss=1.618958592414856
I0128 14:55:49.502068 140027215431488 spec.py:321] Evaluating on the training split.
I0128 14:55:55.775902 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 14:56:04.633840 140027215431488 spec.py:349] Evaluating on the test split.
I0128 14:56:07.191233 140027215431488 submission_runner.py:408] Time since start: 27505.90s, 	Step: 78200, 	{'train/accuracy': 0.6881178021430969, 'train/loss': 1.235331654548645, 'validation/accuracy': 0.6344999670982361, 'validation/loss': 1.5009933710098267, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.199228525161743, 'test/num_examples': 10000, 'score': 26561.368300914764, 'total_duration': 27505.897212982178, 'accumulated_submission_time': 26561.368300914764, 'accumulated_eval_time': 939.9247233867645, 'accumulated_logging_time': 1.8545491695404053}
I0128 14:56:07.225130 139865232500480 logging_writer.py:48] [78200] accumulated_eval_time=939.924723, accumulated_logging_time=1.854549, accumulated_submission_time=26561.368301, global_step=78200, preemption_count=0, score=26561.368301, test/accuracy=0.510400, test/loss=2.199229, test/num_examples=10000, total_duration=27505.897213, train/accuracy=0.688118, train/loss=1.235332, validation/accuracy=0.634500, validation/loss=1.500993, validation/num_examples=50000
I0128 14:56:07.571761 139865240893184 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.827972888946533, loss=1.7198725938796997
I0128 14:56:41.393628 139865232500480 logging_writer.py:48] [78300] global_step=78300, grad_norm=4.685840606689453, loss=1.7531731128692627
I0128 14:57:15.192943 139865240893184 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.9912235736846924, loss=1.6718788146972656
I0128 14:57:49.042515 139865232500480 logging_writer.py:48] [78500] global_step=78500, grad_norm=4.323589324951172, loss=1.6194610595703125
I0128 14:58:22.909888 139865240893184 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.351187705993652, loss=1.7838672399520874
I0128 14:58:56.774909 139865232500480 logging_writer.py:48] [78700] global_step=78700, grad_norm=4.205593109130859, loss=1.7454135417938232
I0128 14:59:30.626892 139865240893184 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.92622709274292, loss=1.6025793552398682
I0128 15:00:04.471782 139865232500480 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.757575750350952, loss=1.6720325946807861
I0128 15:00:38.327070 139865240893184 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.888298511505127, loss=1.7285665273666382
I0128 15:01:12.149123 139865232500480 logging_writer.py:48] [79100] global_step=79100, grad_norm=4.203561782836914, loss=1.7273776531219482
I0128 15:01:45.987783 139865240893184 logging_writer.py:48] [79200] global_step=79200, grad_norm=4.298799991607666, loss=1.9022998809814453
I0128 15:02:19.933056 139865232500480 logging_writer.py:48] [79300] global_step=79300, grad_norm=4.3108110427856445, loss=1.7003459930419922
I0128 15:02:53.776585 139865240893184 logging_writer.py:48] [79400] global_step=79400, grad_norm=4.417102336883545, loss=1.7742538452148438
I0128 15:03:27.607171 139865232500480 logging_writer.py:48] [79500] global_step=79500, grad_norm=4.290044784545898, loss=1.9037071466445923
I0128 15:04:01.469025 139865240893184 logging_writer.py:48] [79600] global_step=79600, grad_norm=4.78637170791626, loss=1.7258388996124268
I0128 15:04:35.334480 139865232500480 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.307072639465332, loss=1.6762969493865967
I0128 15:04:37.519804 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:04:43.764884 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:04:52.682614 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:04:55.243466 140027215431488 submission_runner.py:408] Time since start: 28033.95s, 	Step: 79708, 	{'train/accuracy': 0.6924824714660645, 'train/loss': 1.204252004623413, 'validation/accuracy': 0.6411600112915039, 'validation/loss': 1.4582104682922363, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.2048068046569824, 'test/num_examples': 10000, 'score': 27071.600769996643, 'total_duration': 28033.949457883835, 'accumulated_submission_time': 27071.600769996643, 'accumulated_eval_time': 957.6483449935913, 'accumulated_logging_time': 1.8976809978485107}
I0128 15:04:55.278302 139865232500480 logging_writer.py:48] [79708] accumulated_eval_time=957.648345, accumulated_logging_time=1.897681, accumulated_submission_time=27071.600770, global_step=79708, preemption_count=0, score=27071.600770, test/accuracy=0.517900, test/loss=2.204807, test/num_examples=10000, total_duration=28033.949458, train/accuracy=0.692482, train/loss=1.204252, validation/accuracy=0.641160, validation/loss=1.458210, validation/num_examples=50000
I0128 15:05:26.731290 139866163582720 logging_writer.py:48] [79800] global_step=79800, grad_norm=4.118162631988525, loss=1.6951473951339722
I0128 15:06:00.553873 139865232500480 logging_writer.py:48] [79900] global_step=79900, grad_norm=5.116606712341309, loss=1.8747128248214722
I0128 15:06:34.367761 139866163582720 logging_writer.py:48] [80000] global_step=80000, grad_norm=4.358523845672607, loss=1.6795308589935303
I0128 15:07:08.212439 139865232500480 logging_writer.py:48] [80100] global_step=80100, grad_norm=4.536154270172119, loss=1.743396282196045
I0128 15:07:42.074601 139866163582720 logging_writer.py:48] [80200] global_step=80200, grad_norm=4.138458728790283, loss=1.7252461910247803
I0128 15:08:15.915367 139865232500480 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.415004253387451, loss=1.785003423690796
I0128 15:08:49.817131 139866163582720 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.6470353603363037, loss=1.7671656608581543
I0128 15:09:23.646534 139865232500480 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.329269886016846, loss=1.737044095993042
I0128 15:09:57.490489 139866163582720 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.850557327270508, loss=1.6925921440124512
I0128 15:10:31.334325 139865232500480 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.7242796421051025, loss=1.7839854955673218
I0128 15:11:05.196788 139866163582720 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.8986761569976807, loss=1.7597395181655884
I0128 15:11:39.032609 139865232500480 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.0836005210876465, loss=1.5981271266937256
I0128 15:12:12.898480 139866163582720 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.9211690425872803, loss=1.7054970264434814
I0128 15:12:46.723872 139865232500480 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.9904253482818604, loss=1.7994335889816284
I0128 15:13:20.547440 139866163582720 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.394547939300537, loss=1.6666361093521118
I0128 15:13:25.431158 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:13:31.658456 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:13:40.663650 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:13:43.276748 140027215431488 submission_runner.py:408] Time since start: 28561.98s, 	Step: 81216, 	{'train/accuracy': 0.7299705147743225, 'train/loss': 1.0363904237747192, 'validation/accuracy': 0.6387400031089783, 'validation/loss': 1.4843218326568604, 'validation/num_examples': 50000, 'test/accuracy': 0.522100031375885, 'test/loss': 2.1594133377075195, 'test/num_examples': 10000, 'score': 27581.69038462639, 'total_duration': 28561.98272919655, 'accumulated_submission_time': 27581.69038462639, 'accumulated_eval_time': 975.4938888549805, 'accumulated_logging_time': 1.9428644180297852}
I0128 15:13:43.307675 139865232500480 logging_writer.py:48] [81216] accumulated_eval_time=975.493889, accumulated_logging_time=1.942864, accumulated_submission_time=27581.690385, global_step=81216, preemption_count=0, score=27581.690385, test/accuracy=0.522100, test/loss=2.159413, test/num_examples=10000, total_duration=28561.982729, train/accuracy=0.729971, train/loss=1.036390, validation/accuracy=0.638740, validation/loss=1.484322, validation/num_examples=50000
I0128 15:14:12.056261 139865240893184 logging_writer.py:48] [81300] global_step=81300, grad_norm=4.18912935256958, loss=1.6936016082763672
I0128 15:14:46.045235 139865232500480 logging_writer.py:48] [81400] global_step=81400, grad_norm=4.0652360916137695, loss=1.706451177597046
I0128 15:15:19.888225 139865240893184 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.0755085945129395, loss=1.8439491987228394
I0128 15:15:53.747077 139865232500480 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.9660089015960693, loss=1.7051191329956055
I0128 15:16:27.537416 139865240893184 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.1712775230407715, loss=1.7659229040145874
I0128 15:17:01.402606 139865232500480 logging_writer.py:48] [81800] global_step=81800, grad_norm=4.245609283447266, loss=1.72477126121521
I0128 15:17:35.190760 139865240893184 logging_writer.py:48] [81900] global_step=81900, grad_norm=4.429338455200195, loss=1.649469017982483
I0128 15:18:09.066073 139865232500480 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.912668228149414, loss=1.7192606925964355
I0128 15:18:42.893638 139865240893184 logging_writer.py:48] [82100] global_step=82100, grad_norm=4.30086612701416, loss=1.6441127061843872
I0128 15:19:16.723622 139865232500480 logging_writer.py:48] [82200] global_step=82200, grad_norm=4.733517646789551, loss=1.6398062705993652
I0128 15:19:50.576968 139865240893184 logging_writer.py:48] [82300] global_step=82300, grad_norm=4.984017848968506, loss=1.6649203300476074
I0128 15:20:24.445511 139865232500480 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.940469741821289, loss=1.6756231784820557
I0128 15:20:58.273635 139865240893184 logging_writer.py:48] [82500] global_step=82500, grad_norm=4.220505714416504, loss=1.6648319959640503
I0128 15:21:32.271036 139865232500480 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.791949987411499, loss=1.77261483669281
I0128 15:22:06.090519 139865240893184 logging_writer.py:48] [82700] global_step=82700, grad_norm=4.428921222686768, loss=1.7790846824645996
I0128 15:22:13.345618 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:22:19.516168 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:22:28.274026 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:22:30.884772 140027215431488 submission_runner.py:408] Time since start: 29089.59s, 	Step: 82723, 	{'train/accuracy': 0.7124919891357422, 'train/loss': 1.1045640707015991, 'validation/accuracy': 0.6467999815940857, 'validation/loss': 1.4450737237930298, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.1748580932617188, 'test/num_examples': 10000, 'score': 28091.66532254219, 'total_duration': 29089.590743780136, 'accumulated_submission_time': 28091.66532254219, 'accumulated_eval_time': 993.0329856872559, 'accumulated_logging_time': 1.9836251735687256}
I0128 15:22:30.919672 139866163582720 logging_writer.py:48] [82723] accumulated_eval_time=993.032986, accumulated_logging_time=1.983625, accumulated_submission_time=28091.665323, global_step=82723, preemption_count=0, score=28091.665323, test/accuracy=0.513700, test/loss=2.174858, test/num_examples=10000, total_duration=29089.590744, train/accuracy=0.712492, train/loss=1.104564, validation/accuracy=0.646800, validation/loss=1.445074, validation/num_examples=50000
I0128 15:22:57.312283 139866180368128 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.087693214416504, loss=1.6492643356323242
I0128 15:23:31.126689 139866163582720 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.09938907623291, loss=1.7476320266723633
I0128 15:24:04.973954 139866180368128 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.675516605377197, loss=1.6891238689422607
I0128 15:24:38.791592 139866163582720 logging_writer.py:48] [83100] global_step=83100, grad_norm=4.317776679992676, loss=1.7760246992111206
I0128 15:25:12.628518 139866180368128 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.492744207382202, loss=1.5276715755462646
I0128 15:25:46.445574 139866163582720 logging_writer.py:48] [83300] global_step=83300, grad_norm=4.495047092437744, loss=1.7494158744812012
I0128 15:26:20.292582 139866180368128 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.7054970264434814, loss=1.7204244136810303
I0128 15:26:54.121553 139866163582720 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.394596576690674, loss=1.7028958797454834
I0128 15:27:27.938411 139866180368128 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.091450214385986, loss=1.699030876159668
I0128 15:28:01.852065 139866163582720 logging_writer.py:48] [83700] global_step=83700, grad_norm=4.676965713500977, loss=1.7085843086242676
I0128 15:28:35.717418 139866180368128 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.895308971405029, loss=1.8057092428207397
I0128 15:29:09.539728 139866163582720 logging_writer.py:48] [83900] global_step=83900, grad_norm=5.155115604400635, loss=1.7902748584747314
I0128 15:29:43.383780 139866180368128 logging_writer.py:48] [84000] global_step=84000, grad_norm=4.739511489868164, loss=1.7290456295013428
I0128 15:30:17.231350 139866163582720 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.194325923919678, loss=1.7348339557647705
I0128 15:30:51.020525 139866180368128 logging_writer.py:48] [84200] global_step=84200, grad_norm=4.342391490936279, loss=1.7825884819030762
I0128 15:31:00.999081 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:31:07.273513 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:31:15.922625 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:31:18.521026 140027215431488 submission_runner.py:408] Time since start: 29617.23s, 	Step: 84231, 	{'train/accuracy': 0.708426296710968, 'train/loss': 1.1359773874282837, 'validation/accuracy': 0.6473000049591064, 'validation/loss': 1.4457052946090698, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.1410605907440186, 'test/num_examples': 10000, 'score': 28601.680331230164, 'total_duration': 29617.22701382637, 'accumulated_submission_time': 28601.680331230164, 'accumulated_eval_time': 1010.5548868179321, 'accumulated_logging_time': 2.0314993858337402}
I0128 15:31:18.553038 139865232500480 logging_writer.py:48] [84231] accumulated_eval_time=1010.554887, accumulated_logging_time=2.031499, accumulated_submission_time=28601.680331, global_step=84231, preemption_count=0, score=28601.680331, test/accuracy=0.524400, test/loss=2.141061, test/num_examples=10000, total_duration=29617.227014, train/accuracy=0.708426, train/loss=1.135977, validation/accuracy=0.647300, validation/loss=1.445705, validation/num_examples=50000
I0128 15:31:42.233331 139865240893184 logging_writer.py:48] [84300] global_step=84300, grad_norm=4.202449321746826, loss=1.6767933368682861
I0128 15:32:16.030891 139865232500480 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.7719621658325195, loss=1.678566813468933
I0128 15:32:49.858077 139865240893184 logging_writer.py:48] [84500] global_step=84500, grad_norm=4.011539459228516, loss=1.64883291721344
I0128 15:33:23.677949 139865232500480 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.684633255004883, loss=1.6944228410720825
I0128 15:33:57.595038 139865240893184 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.401801109313965, loss=1.596752405166626
I0128 15:34:31.480362 139865232500480 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.0109124183654785, loss=1.599084734916687
I0128 15:35:05.322868 139865240893184 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.791199684143066, loss=1.863881230354309
I0128 15:35:39.140533 139865232500480 logging_writer.py:48] [85000] global_step=85000, grad_norm=4.299272060394287, loss=1.7110497951507568
I0128 15:36:12.962784 139865240893184 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.845306873321533, loss=1.7265419960021973
I0128 15:36:46.817594 139865232500480 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.877196788787842, loss=1.7348450422286987
I0128 15:37:20.688767 139865240893184 logging_writer.py:48] [85300] global_step=85300, grad_norm=4.033442497253418, loss=1.6278828382492065
I0128 15:37:54.520873 139865232500480 logging_writer.py:48] [85400] global_step=85400, grad_norm=4.548038482666016, loss=1.7092663049697876
I0128 15:38:28.338982 139865240893184 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.847843647003174, loss=1.7026698589324951
I0128 15:39:02.160361 139865232500480 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.9277913570404053, loss=1.6647212505340576
I0128 15:39:35.980612 139865240893184 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.844175338745117, loss=1.8430371284484863
I0128 15:39:48.629543 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:39:54.883919 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:40:04.107090 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:40:06.714719 140027215431488 submission_runner.py:408] Time since start: 30145.42s, 	Step: 85739, 	{'train/accuracy': 0.697684109210968, 'train/loss': 1.191365122795105, 'validation/accuracy': 0.64028000831604, 'validation/loss': 1.475667953491211, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.1744232177734375, 'test/num_examples': 10000, 'score': 29111.69335460663, 'total_duration': 30145.420708179474, 'accumulated_submission_time': 29111.69335460663, 'accumulated_eval_time': 1028.6400225162506, 'accumulated_logging_time': 2.074836492538452}
I0128 15:40:06.751089 139865232500480 logging_writer.py:48] [85739] accumulated_eval_time=1028.640023, accumulated_logging_time=2.074836, accumulated_submission_time=29111.693355, global_step=85739, preemption_count=0, score=29111.693355, test/accuracy=0.510900, test/loss=2.174423, test/num_examples=10000, total_duration=30145.420708, train/accuracy=0.697684, train/loss=1.191365, validation/accuracy=0.640280, validation/loss=1.475668, validation/num_examples=50000
I0128 15:40:27.779053 139866163582720 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.714719772338867, loss=1.6847527027130127
I0128 15:41:01.594692 139865232500480 logging_writer.py:48] [85900] global_step=85900, grad_norm=4.426223278045654, loss=1.7252670526504517
I0128 15:41:35.420183 139866163582720 logging_writer.py:48] [86000] global_step=86000, grad_norm=4.228134632110596, loss=1.7895634174346924
I0128 15:42:09.267793 139865232500480 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.822831630706787, loss=1.6638588905334473
I0128 15:42:43.104421 139866163582720 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.243559837341309, loss=1.599365472793579
I0128 15:43:16.934966 139865232500480 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.132861614227295, loss=1.7539632320404053
I0128 15:43:50.773395 139866163582720 logging_writer.py:48] [86400] global_step=86400, grad_norm=4.099310874938965, loss=1.6109881401062012
I0128 15:44:24.605437 139865232500480 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.7231550216674805, loss=1.6650960445404053
I0128 15:44:58.469961 139866163582720 logging_writer.py:48] [86600] global_step=86600, grad_norm=4.130448818206787, loss=1.6734977960586548
I0128 15:45:32.296162 139865232500480 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.99440598487854, loss=1.7591439485549927
I0128 15:46:06.108677 139866163582720 logging_writer.py:48] [86800] global_step=86800, grad_norm=5.006904125213623, loss=1.7957158088684082
I0128 15:46:40.140619 139865232500480 logging_writer.py:48] [86900] global_step=86900, grad_norm=4.358703136444092, loss=1.7250051498413086
I0128 15:47:14.407927 139866163582720 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.120973587036133, loss=1.6256532669067383
I0128 15:47:48.284775 139865232500480 logging_writer.py:48] [87100] global_step=87100, grad_norm=4.541003227233887, loss=1.7073346376419067
I0128 15:48:22.126783 139866163582720 logging_writer.py:48] [87200] global_step=87200, grad_norm=4.543406009674072, loss=1.7333829402923584
I0128 15:48:36.812659 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:48:43.149440 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:48:51.829701 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:48:54.437446 140027215431488 submission_runner.py:408] Time since start: 30673.14s, 	Step: 87245, 	{'train/accuracy': 0.7034239172935486, 'train/loss': 1.1476686000823975, 'validation/accuracy': 0.6473599672317505, 'validation/loss': 1.432129144668579, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.1454508304595947, 'test/num_examples': 10000, 'score': 29621.69142627716, 'total_duration': 30673.143434762955, 'accumulated_submission_time': 29621.69142627716, 'accumulated_eval_time': 1046.26478099823, 'accumulated_logging_time': 2.1219582557678223}
I0128 15:48:54.470078 139865240893184 logging_writer.py:48] [87245] accumulated_eval_time=1046.264781, accumulated_logging_time=2.121958, accumulated_submission_time=29621.691426, global_step=87245, preemption_count=0, score=29621.691426, test/accuracy=0.524900, test/loss=2.145451, test/num_examples=10000, total_duration=30673.143435, train/accuracy=0.703424, train/loss=1.147669, validation/accuracy=0.647360, validation/loss=1.432129, validation/num_examples=50000
I0128 15:49:13.390808 139865760950016 logging_writer.py:48] [87300] global_step=87300, grad_norm=4.046217441558838, loss=1.6236172914505005
I0128 15:49:47.208917 139865240893184 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.444753170013428, loss=1.6817541122436523
I0128 15:50:21.023447 139865760950016 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.441547870635986, loss=1.7381081581115723
I0128 15:50:54.861651 139865240893184 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.583740711212158, loss=1.7016412019729614
I0128 15:51:28.672343 139865760950016 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.517735958099365, loss=1.646829605102539
I0128 15:52:02.519275 139865240893184 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.339170455932617, loss=1.6828899383544922
I0128 15:52:36.332539 139865760950016 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.974825859069824, loss=1.6253918409347534
I0128 15:53:10.305375 139865240893184 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.857051849365234, loss=1.6512033939361572
I0128 15:53:44.120882 139865760950016 logging_writer.py:48] [88100] global_step=88100, grad_norm=4.748677730560303, loss=1.803493857383728
I0128 15:54:17.983438 139865240893184 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.193155288696289, loss=1.620801568031311
I0128 15:54:51.806672 139865760950016 logging_writer.py:48] [88300] global_step=88300, grad_norm=4.223418712615967, loss=1.7291131019592285
I0128 15:55:25.652850 139865240893184 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.322244644165039, loss=1.6707630157470703
I0128 15:55:59.486643 139865760950016 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.003148078918457, loss=1.6226978302001953
I0128 15:56:33.299327 139865240893184 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.885563850402832, loss=1.7170648574829102
I0128 15:57:07.114529 139865760950016 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.394937992095947, loss=1.6263973712921143
I0128 15:57:24.535538 140027215431488 spec.py:321] Evaluating on the training split.
I0128 15:57:30.728716 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 15:57:39.707329 140027215431488 spec.py:349] Evaluating on the test split.
I0128 15:57:42.221661 140027215431488 submission_runner.py:408] Time since start: 31200.93s, 	Step: 88753, 	{'train/accuracy': 0.69921875, 'train/loss': 1.1784825325012207, 'validation/accuracy': 0.6462599635124207, 'validation/loss': 1.4546048641204834, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.169255256652832, 'test/num_examples': 10000, 'score': 30131.69265937805, 'total_duration': 31200.92763876915, 'accumulated_submission_time': 30131.69265937805, 'accumulated_eval_time': 1063.9508562088013, 'accumulated_logging_time': 2.1666407585144043}
I0128 15:57:42.257488 139865240893184 logging_writer.py:48] [88753] accumulated_eval_time=1063.950856, accumulated_logging_time=2.166641, accumulated_submission_time=30131.692659, global_step=88753, preemption_count=0, score=30131.692659, test/accuracy=0.525000, test/loss=2.169255, test/num_examples=10000, total_duration=31200.927639, train/accuracy=0.699219, train/loss=1.178483, validation/accuracy=0.646260, validation/loss=1.454605, validation/num_examples=50000
I0128 15:57:58.494712 139866163582720 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.0678229331970215, loss=1.6567307710647583
I0128 15:58:32.330782 139865240893184 logging_writer.py:48] [88900] global_step=88900, grad_norm=4.402039527893066, loss=1.6689317226409912
I0128 15:59:06.146149 139866163582720 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.010507583618164, loss=1.692819356918335
I0128 15:59:40.087795 139865240893184 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.021628379821777, loss=1.6716036796569824
I0128 16:00:13.914463 139866163582720 logging_writer.py:48] [89200] global_step=89200, grad_norm=4.209723472595215, loss=1.492584228515625
I0128 16:00:47.747202 139865240893184 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.459072589874268, loss=1.684142827987671
I0128 16:01:21.575982 139866163582720 logging_writer.py:48] [89400] global_step=89400, grad_norm=4.36848783493042, loss=1.6286791563034058
I0128 16:01:55.402083 139865240893184 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.9196934700012207, loss=1.6307117938995361
I0128 16:02:29.215604 139866163582720 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.62974214553833, loss=1.7082915306091309
I0128 16:03:03.040330 139865240893184 logging_writer.py:48] [89700] global_step=89700, grad_norm=4.485578536987305, loss=1.554425835609436
I0128 16:03:36.856740 139866163582720 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.310628414154053, loss=1.8031227588653564
I0128 16:04:10.695466 139865240893184 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.66264009475708, loss=1.806128740310669
I0128 16:04:44.539034 139866163582720 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.897732734680176, loss=1.6444061994552612
I0128 16:05:18.396926 139865240893184 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.66726016998291, loss=1.62641441822052
I0128 16:05:52.265863 139866163582720 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.13700532913208, loss=1.6445475816726685
I0128 16:06:12.383542 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:06:18.592769 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:06:27.394756 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:06:30.160602 140027215431488 submission_runner.py:408] Time since start: 31728.87s, 	Step: 90261, 	{'train/accuracy': 0.7489835619926453, 'train/loss': 0.9775881171226501, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.4023665189743042, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.106358766555786, 'test/num_examples': 10000, 'score': 30641.757912397385, 'total_duration': 31728.86659193039, 'accumulated_submission_time': 30641.757912397385, 'accumulated_eval_time': 1081.7278769016266, 'accumulated_logging_time': 2.2114133834838867}
I0128 16:06:30.194158 139865240893184 logging_writer.py:48] [90261] accumulated_eval_time=1081.727877, accumulated_logging_time=2.211413, accumulated_submission_time=30641.757912, global_step=90261, preemption_count=0, score=30641.757912, test/accuracy=0.534400, test/loss=2.106359, test/num_examples=10000, total_duration=31728.866592, train/accuracy=0.748984, train/loss=0.977588, validation/accuracy=0.658100, validation/loss=1.402367, validation/num_examples=50000
I0128 16:06:43.727360 139865760950016 logging_writer.py:48] [90300] global_step=90300, grad_norm=4.707559585571289, loss=1.616769790649414
I0128 16:07:17.541162 139865240893184 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.836249828338623, loss=1.7679235935211182
I0128 16:07:51.344618 139865760950016 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.505106449127197, loss=1.6688756942749023
I0128 16:08:25.174404 139865240893184 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.325167655944824, loss=1.7512956857681274
I0128 16:08:58.997605 139865760950016 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.201393127441406, loss=1.5718809366226196
I0128 16:09:32.857751 139865240893184 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.597687721252441, loss=1.613499402999878
I0128 16:10:06.702068 139865760950016 logging_writer.py:48] [90900] global_step=90900, grad_norm=4.0504021644592285, loss=1.6063300371170044
I0128 16:10:40.524746 139865240893184 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.669969081878662, loss=1.6310513019561768
I0128 16:11:14.394290 139865760950016 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.7295424938201904, loss=1.6032631397247314
I0128 16:11:48.265533 139865240893184 logging_writer.py:48] [91200] global_step=91200, grad_norm=4.163849830627441, loss=1.6441066265106201
I0128 16:12:22.296216 139865760950016 logging_writer.py:48] [91300] global_step=91300, grad_norm=4.103337287902832, loss=1.6020046472549438
I0128 16:12:56.116737 139865240893184 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.914015293121338, loss=1.5215044021606445
I0128 16:13:29.968386 139865760950016 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.2373199462890625, loss=1.6705105304718018
I0128 16:14:03.819125 139865240893184 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.344958305358887, loss=1.6933636665344238
I0128 16:14:37.616984 139865760950016 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.7942585945129395, loss=1.7739275693893433
I0128 16:15:00.429500 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:15:06.726666 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:15:15.489598 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:15:18.127819 140027215431488 submission_runner.py:408] Time since start: 32256.83s, 	Step: 91769, 	{'train/accuracy': 0.720703125, 'train/loss': 1.0760672092437744, 'validation/accuracy': 0.6516799926757812, 'validation/loss': 1.4240094423294067, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.1127941608428955, 'test/num_examples': 10000, 'score': 31151.930895090103, 'total_duration': 32256.83377289772, 'accumulated_submission_time': 31151.930895090103, 'accumulated_eval_time': 1099.4261264801025, 'accumulated_logging_time': 2.254272222518921}
I0128 16:15:18.165565 139866171975424 logging_writer.py:48] [91769] accumulated_eval_time=1099.426126, accumulated_logging_time=2.254272, accumulated_submission_time=31151.930895, global_step=91769, preemption_count=0, score=31151.930895, test/accuracy=0.531300, test/loss=2.112794, test/num_examples=10000, total_duration=32256.833773, train/accuracy=0.720703, train/loss=1.076067, validation/accuracy=0.651680, validation/loss=1.424009, validation/num_examples=50000
I0128 16:15:28.993524 139866180368128 logging_writer.py:48] [91800] global_step=91800, grad_norm=5.219588279724121, loss=1.7004692554473877
I0128 16:16:02.803801 139866171975424 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.40955924987793, loss=1.581355333328247
I0128 16:16:36.620136 139866180368128 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.485383033752441, loss=1.7354490756988525
I0128 16:17:10.449168 139866171975424 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.2462005615234375, loss=1.6712284088134766
I0128 16:17:44.285238 139866180368128 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.509585380554199, loss=1.5755784511566162
I0128 16:18:18.118324 139866171975424 logging_writer.py:48] [92300] global_step=92300, grad_norm=4.262593746185303, loss=1.572633981704712
I0128 16:18:52.113677 139866180368128 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.976627349853516, loss=1.65903902053833
I0128 16:19:25.951334 139866171975424 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.14453125, loss=1.711782693862915
I0128 16:19:59.790016 139866180368128 logging_writer.py:48] [92600] global_step=92600, grad_norm=4.206911563873291, loss=1.677707314491272
I0128 16:20:33.669363 139866171975424 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.301546573638916, loss=1.6310632228851318
I0128 16:21:07.503507 139866180368128 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.50478458404541, loss=1.816389560699463
I0128 16:21:41.316059 139866171975424 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.3314714431762695, loss=1.6551467180252075
I0128 16:22:15.150750 139866180368128 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.750448703765869, loss=1.6731594800949097
I0128 16:22:48.981573 139866171975424 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.81952428817749, loss=1.6520899534225464
I0128 16:23:22.798285 139866180368128 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.502680778503418, loss=1.6943893432617188
I0128 16:23:48.309573 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:23:54.587759 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:24:03.525679 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:24:06.118770 140027215431488 submission_runner.py:408] Time since start: 32784.82s, 	Step: 93277, 	{'train/accuracy': 0.7053571343421936, 'train/loss': 1.1407818794250488, 'validation/accuracy': 0.6459000110626221, 'validation/loss': 1.4545795917510986, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.1714203357696533, 'test/num_examples': 10000, 'score': 31662.01350545883, 'total_duration': 32784.82475876808, 'accumulated_submission_time': 31662.01350545883, 'accumulated_eval_time': 1117.2352879047394, 'accumulated_logging_time': 2.3011181354522705}
I0128 16:24:06.153312 139865760950016 logging_writer.py:48] [93277] accumulated_eval_time=1117.235288, accumulated_logging_time=2.301118, accumulated_submission_time=31662.013505, global_step=93277, preemption_count=0, score=31662.013505, test/accuracy=0.518400, test/loss=2.171420, test/num_examples=10000, total_duration=32784.824759, train/accuracy=0.705357, train/loss=1.140782, validation/accuracy=0.645900, validation/loss=1.454580, validation/num_examples=50000
I0128 16:24:14.286288 139865769342720 logging_writer.py:48] [93300] global_step=93300, grad_norm=5.717562198638916, loss=1.709029197692871
I0128 16:24:48.223040 139865760950016 logging_writer.py:48] [93400] global_step=93400, grad_norm=4.247354984283447, loss=1.5708566904067993
I0128 16:25:22.034037 139865769342720 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.29998779296875, loss=1.6276590824127197
I0128 16:25:55.813187 139865760950016 logging_writer.py:48] [93600] global_step=93600, grad_norm=5.426577091217041, loss=1.5870534181594849
I0128 16:26:29.638484 139865769342720 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.9112725257873535, loss=1.8131964206695557
I0128 16:27:03.460135 139865760950016 logging_writer.py:48] [93800] global_step=93800, grad_norm=5.01694917678833, loss=1.6255033016204834
I0128 16:27:37.316086 139865769342720 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.580008029937744, loss=1.6749202013015747
I0128 16:28:11.112670 139865760950016 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.348435878753662, loss=1.6407371759414673
I0128 16:28:44.948655 139865769342720 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.389021873474121, loss=1.5609004497528076
I0128 16:29:18.764341 139865760950016 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.098680019378662, loss=1.67570161819458
I0128 16:29:52.619870 139865769342720 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.444734573364258, loss=1.8135054111480713
I0128 16:30:26.404454 139865760950016 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.243246555328369, loss=1.5098291635513306
I0128 16:31:00.249963 139865769342720 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.62756872177124, loss=1.824576735496521
I0128 16:31:34.154151 139865760950016 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.744018077850342, loss=1.571077585220337
I0128 16:32:07.997940 139865769342720 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.728120803833008, loss=1.717647910118103
I0128 16:32:36.247131 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:32:42.460624 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:32:51.392278 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:32:53.979478 140027215431488 submission_runner.py:408] Time since start: 33312.69s, 	Step: 94785, 	{'train/accuracy': 0.7093630433082581, 'train/loss': 1.1220866441726685, 'validation/accuracy': 0.6515799760818481, 'validation/loss': 1.4239590167999268, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.137367010116577, 'test/num_examples': 10000, 'score': 32172.043387413025, 'total_duration': 33312.68545603752, 'accumulated_submission_time': 32172.043387413025, 'accumulated_eval_time': 1134.9675867557526, 'accumulated_logging_time': 2.345402479171753}
I0128 16:32:54.013458 139865240893184 logging_writer.py:48] [94785] accumulated_eval_time=1134.967587, accumulated_logging_time=2.345402, accumulated_submission_time=32172.043387, global_step=94785, preemption_count=0, score=32172.043387, test/accuracy=0.523800, test/loss=2.137367, test/num_examples=10000, total_duration=33312.685456, train/accuracy=0.709363, train/loss=1.122087, validation/accuracy=0.651580, validation/loss=1.423959, validation/num_examples=50000
I0128 16:32:59.423586 139865760950016 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.988329887390137, loss=1.7408978939056396
I0128 16:33:33.249362 139865240893184 logging_writer.py:48] [94900] global_step=94900, grad_norm=6.2146430015563965, loss=1.715617060661316
I0128 16:34:07.047353 139865760950016 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.8760986328125, loss=1.6369330883026123
I0128 16:34:40.885137 139865240893184 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.24795389175415, loss=1.5324651002883911
I0128 16:35:14.684506 139865760950016 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.380878448486328, loss=1.5676469802856445
I0128 16:35:48.511136 139865240893184 logging_writer.py:48] [95300] global_step=95300, grad_norm=4.039812088012695, loss=1.598290205001831
I0128 16:36:22.338673 139865760950016 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.569859504699707, loss=1.659364938735962
I0128 16:36:56.163119 139865240893184 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.901078224182129, loss=1.650307059288025
I0128 16:37:30.062171 139865760950016 logging_writer.py:48] [95600] global_step=95600, grad_norm=5.47645902633667, loss=1.6118907928466797
I0128 16:38:03.864311 139865240893184 logging_writer.py:48] [95700] global_step=95700, grad_norm=5.05409049987793, loss=1.633467197418213
I0128 16:38:37.700842 139865760950016 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.476241588592529, loss=1.7074263095855713
I0128 16:39:11.541208 139865240893184 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.473782539367676, loss=1.5454671382904053
I0128 16:39:45.338257 139865760950016 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.244853973388672, loss=1.671910047531128
I0128 16:40:19.192848 139865240893184 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.188175201416016, loss=1.6091943979263306
I0128 16:40:53.011322 139865760950016 logging_writer.py:48] [96200] global_step=96200, grad_norm=5.322726726531982, loss=1.6161798238754272
I0128 16:41:24.286799 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:41:30.618013 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:41:39.346189 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:41:41.961092 140027215431488 submission_runner.py:408] Time since start: 33840.67s, 	Step: 96294, 	{'train/accuracy': 0.7216597199440002, 'train/loss': 1.0726561546325684, 'validation/accuracy': 0.666979968547821, 'validation/loss': 1.358321189880371, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.063443183898926, 'test/num_examples': 10000, 'score': 32682.25458574295, 'total_duration': 33840.66708111763, 'accumulated_submission_time': 32682.25458574295, 'accumulated_eval_time': 1152.641860485077, 'accumulated_logging_time': 2.3891897201538086}
I0128 16:41:41.996237 139865232500480 logging_writer.py:48] [96294] accumulated_eval_time=1152.641860, accumulated_logging_time=2.389190, accumulated_submission_time=32682.254586, global_step=96294, preemption_count=0, score=32682.254586, test/accuracy=0.538800, test/loss=2.063443, test/num_examples=10000, total_duration=33840.667081, train/accuracy=0.721660, train/loss=1.072656, validation/accuracy=0.666980, validation/loss=1.358321, validation/num_examples=50000
I0128 16:41:44.363112 139865240893184 logging_writer.py:48] [96300] global_step=96300, grad_norm=5.220495223999023, loss=1.5740398168563843
I0128 16:42:18.163216 139865232500480 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.927459239959717, loss=1.686137080192566
I0128 16:42:51.993569 139865240893184 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.288634777069092, loss=1.6496599912643433
I0128 16:43:25.846175 139865232500480 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.629822254180908, loss=1.6974000930786133
I0128 16:43:59.739642 139865240893184 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.898000240325928, loss=1.578321933746338
I0128 16:44:33.548040 139865232500480 logging_writer.py:48] [96800] global_step=96800, grad_norm=5.348310470581055, loss=1.6628631353378296
I0128 16:45:07.381393 139865240893184 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.548811435699463, loss=1.622426152229309
I0128 16:45:41.244026 139865232500480 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.251448631286621, loss=1.5060687065124512
I0128 16:46:15.085351 139865240893184 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.566000938415527, loss=1.6200408935546875
I0128 16:46:48.891319 139865232500480 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.710562229156494, loss=1.6474772691726685
I0128 16:47:22.734210 139865240893184 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.610977649688721, loss=1.6655956506729126
I0128 16:47:56.561898 139865232500480 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.35747766494751, loss=1.5550236701965332
I0128 16:48:30.384062 139865240893184 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.9730873107910156, loss=1.552642583847046
I0128 16:49:04.197394 139865232500480 logging_writer.py:48] [97600] global_step=97600, grad_norm=5.053328037261963, loss=1.669052243232727
I0128 16:49:38.003857 139865240893184 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.316213130950928, loss=1.5756982564926147
I0128 16:50:11.891387 139865232500480 logging_writer.py:48] [97800] global_step=97800, grad_norm=5.090575218200684, loss=1.738535761833191
I0128 16:50:12.039041 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:50:18.272179 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:50:27.248758 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:50:29.850760 140027215431488 submission_runner.py:408] Time since start: 34368.56s, 	Step: 97802, 	{'train/accuracy': 0.7195671200752258, 'train/loss': 1.0867538452148438, 'validation/accuracy': 0.6625799536705017, 'validation/loss': 1.3720060586929321, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.0853664875030518, 'test/num_examples': 10000, 'score': 33192.23504304886, 'total_duration': 34368.55675005913, 'accumulated_submission_time': 33192.23504304886, 'accumulated_eval_time': 1170.453558921814, 'accumulated_logging_time': 2.4338431358337402}
I0128 16:50:29.890172 139866163582720 logging_writer.py:48] [97802] accumulated_eval_time=1170.453559, accumulated_logging_time=2.433843, accumulated_submission_time=33192.235043, global_step=97802, preemption_count=0, score=33192.235043, test/accuracy=0.533500, test/loss=2.085366, test/num_examples=10000, total_duration=34368.556750, train/accuracy=0.719567, train/loss=1.086754, validation/accuracy=0.662580, validation/loss=1.372006, validation/num_examples=50000
I0128 16:51:03.343833 139866171975424 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.667674541473389, loss=1.5113060474395752
I0128 16:51:37.155417 139866163582720 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.741245269775391, loss=1.619360327720642
I0128 16:52:10.977288 139866171975424 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.649353981018066, loss=1.6860049962997437
I0128 16:52:44.791118 139866163582720 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.797965049743652, loss=1.558703899383545
I0128 16:53:18.643666 139866171975424 logging_writer.py:48] [98300] global_step=98300, grad_norm=5.087027072906494, loss=1.6197230815887451
I0128 16:53:52.429847 139866163582720 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.205947399139404, loss=1.5743281841278076
I0128 16:54:26.278313 139866171975424 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.344347953796387, loss=1.595847725868225
I0128 16:55:00.086392 139866163582720 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.129317760467529, loss=1.6333091259002686
I0128 16:55:33.917470 139866171975424 logging_writer.py:48] [98700] global_step=98700, grad_norm=5.0022501945495605, loss=1.5966838598251343
I0128 16:56:07.725064 139866163582720 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.944377899169922, loss=1.708125352859497
I0128 16:56:41.645069 139866171975424 logging_writer.py:48] [98900] global_step=98900, grad_norm=5.981154918670654, loss=1.5886735916137695
I0128 16:57:15.504120 139866163582720 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.075072288513184, loss=1.6974632740020752
I0128 16:57:49.357963 139866171975424 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.285950183868408, loss=1.5747339725494385
I0128 16:58:23.142680 139866163582720 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.8516526222229, loss=1.6076102256774902
I0128 16:58:56.969758 139866171975424 logging_writer.py:48] [99300] global_step=99300, grad_norm=5.10426139831543, loss=1.608616828918457
I0128 16:59:00.170659 140027215431488 spec.py:321] Evaluating on the training split.
I0128 16:59:06.543653 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 16:59:15.205889 140027215431488 spec.py:349] Evaluating on the test split.
I0128 16:59:17.771768 140027215431488 submission_runner.py:408] Time since start: 34896.48s, 	Step: 99311, 	{'train/accuracy': 0.7441206574440002, 'train/loss': 0.9868006110191345, 'validation/accuracy': 0.6504799723625183, 'validation/loss': 1.4178804159164429, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.124274492263794, 'test/num_examples': 10000, 'score': 33702.44986701012, 'total_duration': 34896.47751951218, 'accumulated_submission_time': 33702.44986701012, 'accumulated_eval_time': 1188.0544037818909, 'accumulated_logging_time': 2.4853873252868652}
I0128 16:59:17.807347 139865232500480 logging_writer.py:48] [99311] accumulated_eval_time=1188.054404, accumulated_logging_time=2.485387, accumulated_submission_time=33702.449867, global_step=99311, preemption_count=0, score=33702.449867, test/accuracy=0.525600, test/loss=2.124274, test/num_examples=10000, total_duration=34896.477520, train/accuracy=0.744121, train/loss=0.986801, validation/accuracy=0.650480, validation/loss=1.417880, validation/num_examples=50000
I0128 16:59:48.227219 139865240893184 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.713755130767822, loss=1.6280086040496826
I0128 17:00:22.010329 139865232500480 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.233271598815918, loss=1.6125514507293701
I0128 17:00:55.823204 139865240893184 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.497134208679199, loss=1.6377958059310913
I0128 17:01:29.650767 139865232500480 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.118011474609375, loss=1.59394109249115
I0128 17:02:03.466505 139865240893184 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.8266241550445557, loss=1.4693928956985474
I0128 17:02:37.309252 139865232500480 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.662248134613037, loss=1.6749961376190186
I0128 17:03:11.231223 139865240893184 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.597842693328857, loss=1.4820024967193604
I0128 17:03:45.082470 139865232500480 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.17698860168457, loss=1.5814799070358276
I0128 17:04:18.896400 139865240893184 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.9320387840271, loss=1.6002984046936035
I0128 17:04:52.736497 139865232500480 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.840169906616211, loss=1.5256283283233643
I0128 17:05:26.566095 139865240893184 logging_writer.py:48] [100400] global_step=100400, grad_norm=5.211447715759277, loss=1.6245973110198975
I0128 17:06:00.400255 139865232500480 logging_writer.py:48] [100500] global_step=100500, grad_norm=5.447390556335449, loss=1.545997977256775
I0128 17:06:34.216734 139865240893184 logging_writer.py:48] [100600] global_step=100600, grad_norm=5.13605260848999, loss=1.4872746467590332
I0128 17:07:08.046001 139865232500480 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.248920440673828, loss=1.5516769886016846
I0128 17:07:41.837692 139865240893184 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.192753314971924, loss=1.5095075368881226
I0128 17:07:48.071448 140027215431488 spec.py:321] Evaluating on the training split.
I0128 17:07:54.242124 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 17:08:03.248067 140027215431488 spec.py:349] Evaluating on the test split.
I0128 17:08:05.836202 140027215431488 submission_runner.py:408] Time since start: 35424.54s, 	Step: 100820, 	{'train/accuracy': 0.7390784025192261, 'train/loss': 0.9912664294242859, 'validation/accuracy': 0.6676999926567078, 'validation/loss': 1.349393606185913, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.032019853591919, 'test/num_examples': 10000, 'score': 34212.6481218338, 'total_duration': 35424.54218220711, 'accumulated_submission_time': 34212.6481218338, 'accumulated_eval_time': 1205.8191084861755, 'accumulated_logging_time': 2.532451629638672}
I0128 17:08:05.875079 139866163582720 logging_writer.py:48] [100820] accumulated_eval_time=1205.819108, accumulated_logging_time=2.532452, accumulated_submission_time=34212.648122, global_step=100820, preemption_count=0, score=34212.648122, test/accuracy=0.552200, test/loss=2.032020, test/num_examples=10000, total_duration=35424.542182, train/accuracy=0.739078, train/loss=0.991266, validation/accuracy=0.667700, validation/loss=1.349394, validation/num_examples=50000
I0128 17:08:33.272547 139866171975424 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.537499904632568, loss=1.6585947275161743
I0128 17:09:07.081682 139866163582720 logging_writer.py:48] [101000] global_step=101000, grad_norm=5.169952869415283, loss=1.622422695159912
I0128 17:09:40.951199 139866171975424 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.5810394287109375, loss=1.6779814958572388
I0128 17:10:14.803352 139866163582720 logging_writer.py:48] [101200] global_step=101200, grad_norm=5.337602138519287, loss=1.611030101776123
I0128 17:10:48.641857 139866171975424 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.575399398803711, loss=1.5711627006530762
I0128 17:11:22.478043 139866163582720 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.565938472747803, loss=1.5303083658218384
I0128 17:11:56.267102 139866171975424 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.83254337310791, loss=1.6104636192321777
I0128 17:12:30.109719 139866163582720 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.627031326293945, loss=1.5628535747528076
I0128 17:13:03.935700 139866171975424 logging_writer.py:48] [101700] global_step=101700, grad_norm=5.113036632537842, loss=1.6740162372589111
I0128 17:13:37.754490 139866163582720 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.363670349121094, loss=1.5396478176116943
I0128 17:14:11.579988 139866171975424 logging_writer.py:48] [101900] global_step=101900, grad_norm=5.12000846862793, loss=1.5687522888183594
I0128 17:14:45.404675 139866163582720 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.454466819763184, loss=1.5761542320251465
I0128 17:15:19.203445 139866171975424 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.6374101638793945, loss=1.5903171300888062
I0128 17:15:53.085890 139866163582720 logging_writer.py:48] [102200] global_step=102200, grad_norm=5.213529586791992, loss=1.637736201286316
I0128 17:16:26.905195 139866171975424 logging_writer.py:48] [102300] global_step=102300, grad_norm=6.246445655822754, loss=1.6139863729476929
I0128 17:16:36.165996 140027215431488 spec.py:321] Evaluating on the training split.
I0128 17:16:42.367136 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 17:16:51.240203 140027215431488 spec.py:349] Evaluating on the test split.
I0128 17:16:53.862414 140027215431488 submission_runner.py:408] Time since start: 35952.57s, 	Step: 102329, 	{'train/accuracy': 0.7271803021430969, 'train/loss': 1.0407520532608032, 'validation/accuracy': 0.6602399945259094, 'validation/loss': 1.3748583793640137, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.074155807495117, 'test/num_examples': 10000, 'score': 34722.8763525486, 'total_duration': 35952.56840515137, 'accumulated_submission_time': 34722.8763525486, 'accumulated_eval_time': 1223.5154864788055, 'accumulated_logging_time': 2.5815184116363525}
I0128 17:16:53.900743 139865232500480 logging_writer.py:48] [102329] accumulated_eval_time=1223.515486, accumulated_logging_time=2.581518, accumulated_submission_time=34722.876353, global_step=102329, preemption_count=0, score=34722.876353, test/accuracy=0.537300, test/loss=2.074156, test/num_examples=10000, total_duration=35952.568405, train/accuracy=0.727180, train/loss=1.040752, validation/accuracy=0.660240, validation/loss=1.374858, validation/num_examples=50000
I0128 17:17:18.226229 139865240893184 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.3135199546813965, loss=1.5401419401168823
I0128 17:17:52.050570 139865232500480 logging_writer.py:48] [102500] global_step=102500, grad_norm=5.001903533935547, loss=1.5035487413406372
I0128 17:18:25.860663 139865240893184 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.939873695373535, loss=1.5335842370986938
I0128 17:18:59.675517 139865232500480 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.737154483795166, loss=1.5118473768234253
I0128 17:19:33.504128 139865240893184 logging_writer.py:48] [102800] global_step=102800, grad_norm=5.0578932762146, loss=1.6182538270950317
I0128 17:20:07.314172 139865232500480 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.682993412017822, loss=1.5257525444030762
I0128 17:20:41.179404 139865240893184 logging_writer.py:48] [103000] global_step=103000, grad_norm=5.21793794631958, loss=1.546002984046936
I0128 17:21:14.997407 139865232500480 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.5591864585876465, loss=1.5029690265655518
I0128 17:21:48.849046 139865240893184 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.461493968963623, loss=1.5314915180206299
I0128 17:22:22.779967 139865232500480 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.5414719581604, loss=1.616011381149292
I0128 17:22:56.618645 139865240893184 logging_writer.py:48] [103400] global_step=103400, grad_norm=5.178160667419434, loss=1.5875096321105957
I0128 17:23:30.441128 139865232500480 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.667430400848389, loss=1.5574599504470825
I0128 17:24:04.278469 139865240893184 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.858612060546875, loss=1.5905852317810059
I0128 17:24:38.130750 139865232500480 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.756846904754639, loss=1.526396632194519
I0128 17:25:11.915093 139865240893184 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.601531982421875, loss=1.4822728633880615
I0128 17:25:23.920104 140027215431488 spec.py:321] Evaluating on the training split.
I0128 17:25:30.118025 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 17:25:39.047800 140027215431488 spec.py:349] Evaluating on the test split.
I0128 17:25:41.640719 140027215431488 submission_runner.py:408] Time since start: 36480.35s, 	Step: 103837, 	{'train/accuracy': 0.7323620915412903, 'train/loss': 1.0326040983200073, 'validation/accuracy': 0.6691199541091919, 'validation/loss': 1.3446305990219116, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.035710573196411, 'test/num_examples': 10000, 'score': 35232.8334004879, 'total_duration': 36480.34671187401, 'accumulated_submission_time': 35232.8334004879, 'accumulated_eval_time': 1241.2360637187958, 'accumulated_logging_time': 2.6300241947174072}
I0128 17:25:41.679226 139866163582720 logging_writer.py:48] [103837] accumulated_eval_time=1241.236064, accumulated_logging_time=2.630024, accumulated_submission_time=35232.833400, global_step=103837, preemption_count=0, score=35232.833400, test/accuracy=0.545400, test/loss=2.035711, test/num_examples=10000, total_duration=36480.346712, train/accuracy=0.732362, train/loss=1.032604, validation/accuracy=0.669120, validation/loss=1.344631, validation/num_examples=50000
I0128 17:26:03.316480 139866171975424 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.578151226043701, loss=1.5091192722320557
I0128 17:26:37.124094 139866163582720 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.574577331542969, loss=1.4791018962860107
I0128 17:27:10.948987 139866171975424 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.569926738739014, loss=1.5855321884155273
I0128 17:27:44.788515 139866163582720 logging_writer.py:48] [104200] global_step=104200, grad_norm=5.749112129211426, loss=1.5758838653564453
I0128 17:28:18.614056 139866171975424 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.59360933303833, loss=1.5250636339187622
I0128 17:28:52.627130 139866163582720 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.461791038513184, loss=1.6008546352386475
I0128 17:29:26.463328 139866171975424 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.958200454711914, loss=1.6245115995407104
I0128 17:30:00.297202 139866163582720 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.880735397338867, loss=1.6242235898971558
I0128 17:30:34.101095 139866171975424 logging_writer.py:48] [104700] global_step=104700, grad_norm=5.629020690917969, loss=1.6111860275268555
I0128 17:31:07.936182 139866163582720 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.676169395446777, loss=1.591591715812683
I0128 17:31:41.784683 139866171975424 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.964977741241455, loss=1.6237075328826904
I0128 17:32:15.628271 139866163582720 logging_writer.py:48] [105000] global_step=105000, grad_norm=4.623450756072998, loss=1.4926905632019043
I0128 17:32:49.462299 139866171975424 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.939338207244873, loss=1.5203075408935547
I0128 17:33:23.280946 139866163582720 logging_writer.py:48] [105200] global_step=105200, grad_norm=5.289797782897949, loss=1.6989178657531738
I0128 17:33:57.116841 139866171975424 logging_writer.py:48] [105300] global_step=105300, grad_norm=5.454666614532471, loss=1.499657154083252
I0128 17:34:11.784156 140027215431488 spec.py:321] Evaluating on the training split.
I0128 17:34:18.011937 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 17:34:26.726524 140027215431488 spec.py:349] Evaluating on the test split.
I0128 17:34:29.240540 140027215431488 submission_runner.py:408] Time since start: 37007.95s, 	Step: 105345, 	{'train/accuracy': 0.7277383208274841, 'train/loss': 1.057060718536377, 'validation/accuracy': 0.6671000123023987, 'validation/loss': 1.3545143604278564, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.032726526260376, 'test/num_examples': 10000, 'score': 35742.87636375427, 'total_duration': 37007.94652104378, 'accumulated_submission_time': 35742.87636375427, 'accumulated_eval_time': 1258.6924047470093, 'accumulated_logging_time': 2.6776812076568604}
I0128 17:34:29.276502 139865769342720 logging_writer.py:48] [105345] accumulated_eval_time=1258.692405, accumulated_logging_time=2.677681, accumulated_submission_time=35742.876364, global_step=105345, preemption_count=0, score=35742.876364, test/accuracy=0.545400, test/loss=2.032727, test/num_examples=10000, total_duration=37007.946521, train/accuracy=0.727738, train/loss=1.057061, validation/accuracy=0.667100, validation/loss=1.354514, validation/num_examples=50000
I0128 17:34:48.337433 139866188760832 logging_writer.py:48] [105400] global_step=105400, grad_norm=5.433166980743408, loss=1.6739726066589355
I0128 17:35:22.143801 139865769342720 logging_writer.py:48] [105500] global_step=105500, grad_norm=5.46173095703125, loss=1.6192920207977295
I0128 17:35:55.962517 139866188760832 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.495158672332764, loss=1.583139181137085
I0128 17:36:29.786268 139865769342720 logging_writer.py:48] [105700] global_step=105700, grad_norm=5.341714859008789, loss=1.5952024459838867
I0128 17:37:03.637937 139866188760832 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.783806324005127, loss=1.552289366722107
I0128 17:37:37.427796 139865769342720 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.698089122772217, loss=1.500677227973938
I0128 17:38:11.277507 139866188760832 logging_writer.py:48] [106000] global_step=106000, grad_norm=5.191278457641602, loss=1.597618579864502
I0128 17:38:45.081288 139865769342720 logging_writer.py:48] [106100] global_step=106100, grad_norm=5.049793243408203, loss=1.5129629373550415
I0128 17:39:18.927814 139866188760832 logging_writer.py:48] [106200] global_step=106200, grad_norm=5.328943729400635, loss=1.4792758226394653
I0128 17:39:52.756633 139865769342720 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.650685787200928, loss=1.4536101818084717
I0128 17:40:26.585066 139866188760832 logging_writer.py:48] [106400] global_step=106400, grad_norm=5.116538047790527, loss=1.628900170326233
I0128 17:41:00.386064 139865769342720 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.460164546966553, loss=1.6402561664581299
I0128 17:41:34.289138 139866188760832 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.983405113220215, loss=1.5627233982086182
I0128 17:42:08.074200 139865769342720 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.759819030761719, loss=1.6096467971801758
I0128 17:42:41.910764 139866188760832 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.6226630210876465, loss=1.4994502067565918
I0128 17:42:59.274019 140027215431488 spec.py:321] Evaluating on the training split.
I0128 17:43:05.521346 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 17:43:14.056296 140027215431488 spec.py:349] Evaluating on the test split.
I0128 17:43:16.678266 140027215431488 submission_runner.py:408] Time since start: 37535.38s, 	Step: 106853, 	{'train/accuracy': 0.7320830821990967, 'train/loss': 1.035598635673523, 'validation/accuracy': 0.6726599931716919, 'validation/loss': 1.330732822418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.024749994277954, 'test/num_examples': 10000, 'score': 36252.81159281731, 'total_duration': 37535.38425660133, 'accumulated_submission_time': 36252.81159281731, 'accumulated_eval_time': 1276.0966138839722, 'accumulated_logging_time': 2.7229397296905518}
I0128 17:43:16.715087 139865760950016 logging_writer.py:48] [106853] accumulated_eval_time=1276.096614, accumulated_logging_time=2.722940, accumulated_submission_time=36252.811593, global_step=106853, preemption_count=0, score=36252.811593, test/accuracy=0.548900, test/loss=2.024750, test/num_examples=10000, total_duration=37535.384257, train/accuracy=0.732083, train/loss=1.035599, validation/accuracy=0.672660, validation/loss=1.330733, validation/num_examples=50000
I0128 17:43:32.930948 139866163582720 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.538477897644043, loss=1.5129048824310303
I0128 17:44:06.738096 139865760950016 logging_writer.py:48] [107000] global_step=107000, grad_norm=4.226216793060303, loss=1.4751951694488525
I0128 17:44:40.556241 139866163582720 logging_writer.py:48] [107100] global_step=107100, grad_norm=5.222317218780518, loss=1.5623403787612915
I0128 17:45:14.385094 139865760950016 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.98236608505249, loss=1.5303637981414795
I0128 17:45:48.202015 139866163582720 logging_writer.py:48] [107300] global_step=107300, grad_norm=5.017171382904053, loss=1.4714326858520508
I0128 17:46:22.021389 139865760950016 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.71305513381958, loss=1.5065630674362183
I0128 17:46:55.852641 139866163582720 logging_writer.py:48] [107500] global_step=107500, grad_norm=5.386957168579102, loss=1.458147406578064
I0128 17:47:29.737913 139865760950016 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.885505676269531, loss=1.5256977081298828
I0128 17:48:03.564061 139866163582720 logging_writer.py:48] [107700] global_step=107700, grad_norm=5.0727996826171875, loss=1.6378029584884644
I0128 17:48:37.419021 139865760950016 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.773295879364014, loss=1.5397181510925293
I0128 17:49:11.224988 139866163582720 logging_writer.py:48] [107900] global_step=107900, grad_norm=5.232626914978027, loss=1.6099902391433716
I0128 17:49:45.093504 139865760950016 logging_writer.py:48] [108000] global_step=108000, grad_norm=5.250172138214111, loss=1.5812222957611084
I0128 17:50:18.925252 139866163582720 logging_writer.py:48] [108100] global_step=108100, grad_norm=5.093142986297607, loss=1.5229154825210571
I0128 17:50:52.761031 139865760950016 logging_writer.py:48] [108200] global_step=108200, grad_norm=5.41316032409668, loss=1.5060977935791016
I0128 17:51:26.576927 139866163582720 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.7578535079956055, loss=1.5215954780578613
I0128 17:51:46.693388 140027215431488 spec.py:321] Evaluating on the training split.
I0128 17:51:52.866517 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 17:52:01.694916 140027215431488 spec.py:349] Evaluating on the test split.
I0128 17:52:04.339849 140027215431488 submission_runner.py:408] Time since start: 38063.05s, 	Step: 108361, 	{'train/accuracy': 0.7787786722183228, 'train/loss': 0.8223951458930969, 'validation/accuracy': 0.6761599779129028, 'validation/loss': 1.301857590675354, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 1.9950766563415527, 'test/num_examples': 10000, 'score': 36762.72781395912, 'total_duration': 38063.045838832855, 'accumulated_submission_time': 36762.72781395912, 'accumulated_eval_time': 1293.7430353164673, 'accumulated_logging_time': 2.769303321838379}
I0128 17:52:04.381841 139865240893184 logging_writer.py:48] [108361] accumulated_eval_time=1293.743035, accumulated_logging_time=2.769303, accumulated_submission_time=36762.727814, global_step=108361, preemption_count=0, score=36762.727814, test/accuracy=0.554700, test/loss=1.995077, test/num_examples=10000, total_duration=38063.045839, train/accuracy=0.778779, train/loss=0.822395, validation/accuracy=0.676160, validation/loss=1.301858, validation/num_examples=50000
I0128 17:52:17.907298 139865760950016 logging_writer.py:48] [108400] global_step=108400, grad_norm=5.655508518218994, loss=1.5744225978851318
I0128 17:52:51.719138 139865240893184 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.635561466217041, loss=1.40809965133667
I0128 17:53:25.545398 139865760950016 logging_writer.py:48] [108600] global_step=108600, grad_norm=5.4905900955200195, loss=1.5981895923614502
I0128 17:53:59.436300 139865240893184 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.761775016784668, loss=1.5767881870269775
I0128 17:54:33.262534 139865760950016 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.796820163726807, loss=1.529482364654541
I0128 17:55:07.119587 139865240893184 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.679643630981445, loss=1.46248197555542
I0128 17:55:40.968592 139865760950016 logging_writer.py:48] [109000] global_step=109000, grad_norm=5.364057540893555, loss=1.5891647338867188
I0128 17:56:14.791661 139865240893184 logging_writer.py:48] [109100] global_step=109100, grad_norm=5.191985607147217, loss=1.6161185503005981
I0128 17:56:48.626683 139865760950016 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.596734523773193, loss=1.5110676288604736
I0128 17:57:22.439545 139865240893184 logging_writer.py:48] [109300] global_step=109300, grad_norm=5.033236026763916, loss=1.561753749847412
I0128 17:57:56.252868 139865760950016 logging_writer.py:48] [109400] global_step=109400, grad_norm=5.368706226348877, loss=1.4814679622650146
I0128 17:58:30.081070 139865240893184 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.552588939666748, loss=1.5102595090866089
I0128 17:59:03.898178 139865760950016 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.848721027374268, loss=1.6432596445083618
I0128 17:59:37.710018 139865240893184 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.4871087074279785, loss=1.570900559425354
I0128 18:00:11.634408 139865760950016 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.970787525177002, loss=1.5235445499420166
I0128 18:00:34.426438 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:00:40.645310 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:00:49.670373 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:00:52.289638 140027215431488 submission_runner.py:408] Time since start: 38591.00s, 	Step: 109869, 	{'train/accuracy': 0.7466916441917419, 'train/loss': 0.9609189033508301, 'validation/accuracy': 0.6674599647521973, 'validation/loss': 1.3489824533462524, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.03249454498291, 'test/num_examples': 10000, 'score': 37272.709463596344, 'total_duration': 38590.995624780655, 'accumulated_submission_time': 37272.709463596344, 'accumulated_eval_time': 1311.6061923503876, 'accumulated_logging_time': 2.820514678955078}
I0128 18:00:52.330572 139865240893184 logging_writer.py:48] [109869] accumulated_eval_time=1311.606192, accumulated_logging_time=2.820515, accumulated_submission_time=37272.709464, global_step=109869, preemption_count=0, score=37272.709464, test/accuracy=0.539900, test/loss=2.032495, test/num_examples=10000, total_duration=38590.995625, train/accuracy=0.746692, train/loss=0.960919, validation/accuracy=0.667460, validation/loss=1.348982, validation/num_examples=50000
I0128 18:01:03.162560 139865760950016 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.69952917098999, loss=1.5172216892242432
I0128 18:01:36.947416 139865240893184 logging_writer.py:48] [110000] global_step=110000, grad_norm=6.423996925354004, loss=1.5657978057861328
I0128 18:02:10.771337 139865760950016 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.788744926452637, loss=1.5252255201339722
I0128 18:02:44.590554 139865240893184 logging_writer.py:48] [110200] global_step=110200, grad_norm=5.10845422744751, loss=1.5280646085739136
I0128 18:03:18.428522 139865760950016 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.99003791809082, loss=1.3658089637756348
I0128 18:03:52.256886 139865240893184 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.643867492675781, loss=1.5565085411071777
I0128 18:04:26.097295 139865760950016 logging_writer.py:48] [110500] global_step=110500, grad_norm=5.171328067779541, loss=1.531227469444275
I0128 18:04:59.930102 139865240893184 logging_writer.py:48] [110600] global_step=110600, grad_norm=5.564796447753906, loss=1.4890625476837158
I0128 18:05:33.764817 139865760950016 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.89484977722168, loss=1.5736135244369507
I0128 18:06:07.619242 139865240893184 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.964599132537842, loss=1.486618995666504
I0128 18:06:41.510662 139865760950016 logging_writer.py:48] [110900] global_step=110900, grad_norm=5.066575050354004, loss=1.5987788438796997
I0128 18:07:15.316692 139865240893184 logging_writer.py:48] [111000] global_step=111000, grad_norm=5.248890399932861, loss=1.438901662826538
I0128 18:07:49.188255 139865760950016 logging_writer.py:48] [111100] global_step=111100, grad_norm=5.4072160720825195, loss=1.3941622972488403
I0128 18:08:22.997119 139865240893184 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.7159881591796875, loss=1.5535888671875
I0128 18:08:56.837751 139865760950016 logging_writer.py:48] [111300] global_step=111300, grad_norm=6.022502899169922, loss=1.4898823499679565
I0128 18:09:22.371699 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:09:28.531708 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:09:37.202716 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:09:39.873290 140027215431488 submission_runner.py:408] Time since start: 39118.58s, 	Step: 111377, 	{'train/accuracy': 0.7472297549247742, 'train/loss': 0.9534629583358765, 'validation/accuracy': 0.6775999665260315, 'validation/loss': 1.3055094480514526, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 1.9932727813720703, 'test/num_examples': 10000, 'score': 37782.685536146164, 'total_duration': 39118.579266786575, 'accumulated_submission_time': 37782.685536146164, 'accumulated_eval_time': 1329.1077308654785, 'accumulated_logging_time': 2.870955228805542}
I0128 18:09:39.909936 139865769342720 logging_writer.py:48] [111377] accumulated_eval_time=1329.107731, accumulated_logging_time=2.870955, accumulated_submission_time=37782.685536, global_step=111377, preemption_count=0, score=37782.685536, test/accuracy=0.550900, test/loss=1.993273, test/num_examples=10000, total_duration=39118.579267, train/accuracy=0.747230, train/loss=0.953463, validation/accuracy=0.677600, validation/loss=1.305509, validation/num_examples=50000
I0128 18:09:48.057351 139866163582720 logging_writer.py:48] [111400] global_step=111400, grad_norm=5.219486236572266, loss=1.5234707593917847
I0128 18:10:21.864118 139865769342720 logging_writer.py:48] [111500] global_step=111500, grad_norm=5.804258823394775, loss=1.4874958992004395
I0128 18:10:55.689826 139866163582720 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.924359321594238, loss=1.5306479930877686
I0128 18:11:29.505883 139865769342720 logging_writer.py:48] [111700] global_step=111700, grad_norm=5.312121868133545, loss=1.459224820137024
I0128 18:12:03.351308 139866163582720 logging_writer.py:48] [111800] global_step=111800, grad_norm=5.540892601013184, loss=1.5902687311172485
I0128 18:12:37.169106 139865769342720 logging_writer.py:48] [111900] global_step=111900, grad_norm=5.276551723480225, loss=1.4939298629760742
I0128 18:13:11.148875 139866163582720 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.710489749908447, loss=1.545751929283142
I0128 18:13:44.980657 139865769342720 logging_writer.py:48] [112100] global_step=112100, grad_norm=5.769615650177002, loss=1.462862491607666
I0128 18:14:18.821369 139866163582720 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.621835231781006, loss=1.5307490825653076
I0128 18:14:52.656176 139865769342720 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.950534820556641, loss=1.5351158380508423
I0128 18:15:26.530834 139866163582720 logging_writer.py:48] [112400] global_step=112400, grad_norm=5.551715850830078, loss=1.4569450616836548
I0128 18:16:00.364739 139865769342720 logging_writer.py:48] [112500] global_step=112500, grad_norm=5.6879377365112305, loss=1.4995924234390259
I0128 18:16:34.205673 139866163582720 logging_writer.py:48] [112600] global_step=112600, grad_norm=5.192164897918701, loss=1.5684995651245117
I0128 18:17:08.020836 139865769342720 logging_writer.py:48] [112700] global_step=112700, grad_norm=6.0941572189331055, loss=1.547766923904419
I0128 18:17:41.842190 139866163582720 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.727733612060547, loss=1.4745943546295166
I0128 18:18:10.056149 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:18:16.235764 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:18:24.896209 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:18:27.485562 140027215431488 submission_runner.py:408] Time since start: 39646.19s, 	Step: 112885, 	{'train/accuracy': 0.7496611475944519, 'train/loss': 0.9466625452041626, 'validation/accuracy': 0.6813600063323975, 'validation/loss': 1.2942183017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 1.9724422693252563, 'test/num_examples': 10000, 'score': 38292.7685611248, 'total_duration': 39646.19147348404, 'accumulated_submission_time': 38292.7685611248, 'accumulated_eval_time': 1346.5370290279388, 'accumulated_logging_time': 2.9173662662506104}
I0128 18:18:27.521469 139865232500480 logging_writer.py:48] [112885] accumulated_eval_time=1346.537029, accumulated_logging_time=2.917366, accumulated_submission_time=38292.768561, global_step=112885, preemption_count=0, score=38292.768561, test/accuracy=0.559500, test/loss=1.972442, test/num_examples=10000, total_duration=39646.191473, train/accuracy=0.749661, train/loss=0.946663, validation/accuracy=0.681360, validation/loss=1.294218, validation/num_examples=50000
I0128 18:18:32.920220 139865240893184 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.396485328674316, loss=1.526441216468811
I0128 18:19:06.693155 139865232500480 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.877381801605225, loss=1.5805871486663818
I0128 18:19:40.671745 139865240893184 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.936036586761475, loss=1.3702207803726196
I0128 18:20:14.528327 139865232500480 logging_writer.py:48] [113200] global_step=113200, grad_norm=5.999122142791748, loss=1.4695682525634766
I0128 18:20:48.374854 139865240893184 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.771483898162842, loss=1.5379877090454102
I0128 18:21:22.200977 139865232500480 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.772058963775635, loss=1.3335769176483154
I0128 18:21:56.015266 139865240893184 logging_writer.py:48] [113500] global_step=113500, grad_norm=5.4427876472473145, loss=1.3991588354110718
I0128 18:22:29.810128 139865232500480 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.7971906661987305, loss=1.4836163520812988
I0128 18:23:03.657917 139865240893184 logging_writer.py:48] [113700] global_step=113700, grad_norm=5.484535217285156, loss=1.4597232341766357
I0128 18:23:37.497641 139865232500480 logging_writer.py:48] [113800] global_step=113800, grad_norm=5.689034938812256, loss=1.5249779224395752
I0128 18:24:11.321898 139865240893184 logging_writer.py:48] [113900] global_step=113900, grad_norm=5.2400593757629395, loss=1.4553364515304565
I0128 18:24:45.135998 139865232500480 logging_writer.py:48] [114000] global_step=114000, grad_norm=5.19128942489624, loss=1.4356919527053833
I0128 18:25:18.948908 139865240893184 logging_writer.py:48] [114100] global_step=114100, grad_norm=5.38208532333374, loss=1.5096278190612793
I0128 18:25:52.932298 139865232500480 logging_writer.py:48] [114200] global_step=114200, grad_norm=5.069667339324951, loss=1.5683131217956543
I0128 18:26:26.767129 139865240893184 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.915010929107666, loss=1.5170273780822754
I0128 18:26:57.713145 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:27:03.962536 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:27:12.629540 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:27:15.193468 140027215431488 submission_runner.py:408] Time since start: 40173.90s, 	Step: 114393, 	{'train/accuracy': 0.7482461333274841, 'train/loss': 0.9572451114654541, 'validation/accuracy': 0.682699978351593, 'validation/loss': 1.284542202949524, 'validation/num_examples': 50000, 'test/accuracy': 0.5559000372886658, 'test/loss': 1.9868924617767334, 'test/num_examples': 10000, 'score': 38802.898661613464, 'total_duration': 40173.89938187599, 'accumulated_submission_time': 38802.898661613464, 'accumulated_eval_time': 1364.0172460079193, 'accumulated_logging_time': 2.962613582611084}
I0128 18:27:15.231310 139866180368128 logging_writer.py:48] [114393] accumulated_eval_time=1364.017246, accumulated_logging_time=2.962614, accumulated_submission_time=38802.898662, global_step=114393, preemption_count=0, score=38802.898662, test/accuracy=0.555900, test/loss=1.986892, test/num_examples=10000, total_duration=40173.899382, train/accuracy=0.748246, train/loss=0.957245, validation/accuracy=0.682700, validation/loss=1.284542, validation/num_examples=50000
I0128 18:27:17.950457 139866188760832 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.840806007385254, loss=1.468688726425171
I0128 18:27:51.736730 139866180368128 logging_writer.py:48] [114500] global_step=114500, grad_norm=5.176026821136475, loss=1.3791444301605225
I0128 18:28:25.561633 139866188760832 logging_writer.py:48] [114600] global_step=114600, grad_norm=6.245967864990234, loss=1.6237479448318481
I0128 18:28:59.323645 139866180368128 logging_writer.py:48] [114700] global_step=114700, grad_norm=5.292389869689941, loss=1.455902099609375
I0128 18:29:33.185307 139866188760832 logging_writer.py:48] [114800] global_step=114800, grad_norm=5.1340179443359375, loss=1.4165775775909424
I0128 18:30:06.973268 139866180368128 logging_writer.py:48] [114900] global_step=114900, grad_norm=5.484484672546387, loss=1.4334838390350342
I0128 18:30:40.822351 139866188760832 logging_writer.py:48] [115000] global_step=115000, grad_norm=6.62111234664917, loss=1.3868824243545532
I0128 18:31:14.697540 139866180368128 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.815929412841797, loss=1.4379503726959229
I0128 18:31:48.611785 139866188760832 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.517477035522461, loss=1.4753401279449463
I0128 18:32:22.460425 139866180368128 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.922256946563721, loss=1.3193080425262451
I0128 18:32:56.277620 139866188760832 logging_writer.py:48] [115400] global_step=115400, grad_norm=5.432362079620361, loss=1.550615906715393
I0128 18:33:30.145773 139866180368128 logging_writer.py:48] [115500] global_step=115500, grad_norm=5.655003070831299, loss=1.458174467086792
I0128 18:34:03.968077 139866188760832 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.559681415557861, loss=1.4558207988739014
I0128 18:34:37.808320 139866180368128 logging_writer.py:48] [115700] global_step=115700, grad_norm=5.675417423248291, loss=1.6098350286483765
I0128 18:35:11.660742 139866188760832 logging_writer.py:48] [115800] global_step=115800, grad_norm=5.490873336791992, loss=1.4431321620941162
I0128 18:35:45.461019 139866180368128 logging_writer.py:48] [115900] global_step=115900, grad_norm=5.456552505493164, loss=1.58160400390625
I0128 18:35:45.469180 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:35:51.787860 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:36:00.437442 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:36:03.173977 140027215431488 submission_runner.py:408] Time since start: 40701.88s, 	Step: 115901, 	{'train/accuracy': 0.7508171200752258, 'train/loss': 0.9416345953941345, 'validation/accuracy': 0.6827799677848816, 'validation/loss': 1.2685418128967285, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 1.9634768962860107, 'test/num_examples': 10000, 'score': 39313.07150053978, 'total_duration': 40701.87993502617, 'accumulated_submission_time': 39313.07150053978, 'accumulated_eval_time': 1381.7219486236572, 'accumulated_logging_time': 3.011996269226074}
I0128 18:36:03.214905 139865232500480 logging_writer.py:48] [115901] accumulated_eval_time=1381.721949, accumulated_logging_time=3.011996, accumulated_submission_time=39313.071501, global_step=115901, preemption_count=0, score=39313.071501, test/accuracy=0.555200, test/loss=1.963477, test/num_examples=10000, total_duration=40701.879935, train/accuracy=0.750817, train/loss=0.941635, validation/accuracy=0.682780, validation/loss=1.268542, validation/num_examples=50000
I0128 18:36:37.012215 139865240893184 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.719518184661865, loss=1.3531548976898193
I0128 18:37:10.813538 139865232500480 logging_writer.py:48] [116100] global_step=116100, grad_norm=5.213812828063965, loss=1.5959267616271973
I0128 18:37:44.623052 139865240893184 logging_writer.py:48] [116200] global_step=116200, grad_norm=5.073115348815918, loss=1.4334862232208252
I0128 18:38:18.464660 139865232500480 logging_writer.py:48] [116300] global_step=116300, grad_norm=5.263270378112793, loss=1.4359813928604126
I0128 18:38:52.284228 139865240893184 logging_writer.py:48] [116400] global_step=116400, grad_norm=5.4823527336120605, loss=1.4778554439544678
I0128 18:39:26.102490 139865232500480 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.843839645385742, loss=1.3847402334213257
I0128 18:39:59.912333 139865240893184 logging_writer.py:48] [116600] global_step=116600, grad_norm=5.516899108886719, loss=1.514206886291504
I0128 18:40:33.724707 139865232500480 logging_writer.py:48] [116700] global_step=116700, grad_norm=5.395869731903076, loss=1.5192807912826538
I0128 18:41:07.552560 139865240893184 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.862368583679199, loss=1.468685507774353
I0128 18:41:41.381370 139865232500480 logging_writer.py:48] [116900] global_step=116900, grad_norm=5.3271918296813965, loss=1.500596284866333
I0128 18:42:15.185453 139865240893184 logging_writer.py:48] [117000] global_step=117000, grad_norm=5.010392665863037, loss=1.337887167930603
I0128 18:42:49.015741 139865232500480 logging_writer.py:48] [117100] global_step=117100, grad_norm=5.1904401779174805, loss=1.3662644624710083
I0128 18:43:22.844365 139865240893184 logging_writer.py:48] [117200] global_step=117200, grad_norm=5.037278175354004, loss=1.400137186050415
I0128 18:43:56.688060 139865232500480 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.428598880767822, loss=1.5042917728424072
I0128 18:44:30.709762 139865240893184 logging_writer.py:48] [117400] global_step=117400, grad_norm=5.577929496765137, loss=1.4785709381103516
I0128 18:44:33.236569 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:44:39.466505 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:44:48.206605 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:44:50.806935 140027215431488 submission_runner.py:408] Time since start: 41229.51s, 	Step: 117409, 	{'train/accuracy': 0.7893415093421936, 'train/loss': 0.7836189866065979, 'validation/accuracy': 0.6830199956893921, 'validation/loss': 1.2751917839050293, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 1.9720399379730225, 'test/num_examples': 10000, 'score': 39823.0315990448, 'total_duration': 41229.51292562485, 'accumulated_submission_time': 39823.0315990448, 'accumulated_eval_time': 1399.2922735214233, 'accumulated_logging_time': 3.0626745223999023}
I0128 18:44:50.844396 139865232500480 logging_writer.py:48] [117409] accumulated_eval_time=1399.292274, accumulated_logging_time=3.062675, accumulated_submission_time=39823.031599, global_step=117409, preemption_count=0, score=39823.031599, test/accuracy=0.562400, test/loss=1.972040, test/num_examples=10000, total_duration=41229.512926, train/accuracy=0.789342, train/loss=0.783619, validation/accuracy=0.683020, validation/loss=1.275192, validation/num_examples=50000
I0128 18:45:21.928275 139866171975424 logging_writer.py:48] [117500] global_step=117500, grad_norm=5.363271713256836, loss=1.4336754083633423
I0128 18:45:55.754754 139865232500480 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.50551700592041, loss=1.4887198209762573
I0128 18:46:29.571155 139866171975424 logging_writer.py:48] [117700] global_step=117700, grad_norm=6.141664981842041, loss=1.4686198234558105
I0128 18:47:03.438303 139865232500480 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.325035095214844, loss=1.5599861145019531
I0128 18:47:37.310069 139866171975424 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.759353160858154, loss=1.4357329607009888
I0128 18:48:11.115535 139865232500480 logging_writer.py:48] [118000] global_step=118000, grad_norm=5.3424506187438965, loss=1.532328724861145
I0128 18:48:44.951347 139866171975424 logging_writer.py:48] [118100] global_step=118100, grad_norm=5.583199501037598, loss=1.55777907371521
I0128 18:49:18.826133 139865232500480 logging_writer.py:48] [118200] global_step=118200, grad_norm=5.334231376647949, loss=1.5086865425109863
I0128 18:49:52.683481 139866171975424 logging_writer.py:48] [118300] global_step=118300, grad_norm=5.625811576843262, loss=1.5014344453811646
I0128 18:50:26.521975 139865232500480 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.208555221557617, loss=1.3545068502426147
I0128 18:51:00.451261 139866171975424 logging_writer.py:48] [118500] global_step=118500, grad_norm=5.207555770874023, loss=1.444990634918213
I0128 18:51:34.303070 139865232500480 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.671586990356445, loss=1.4207720756530762
I0128 18:52:08.145857 139866171975424 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.911022663116455, loss=1.3953180313110352
I0128 18:52:41.964879 139865232500480 logging_writer.py:48] [118800] global_step=118800, grad_norm=5.770277500152588, loss=1.3967559337615967
I0128 18:53:15.806937 139866171975424 logging_writer.py:48] [118900] global_step=118900, grad_norm=5.8143630027771, loss=1.4459470510482788
I0128 18:53:21.022436 140027215431488 spec.py:321] Evaluating on the training split.
I0128 18:53:27.391774 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 18:53:36.303640 140027215431488 spec.py:349] Evaluating on the test split.
I0128 18:53:38.800854 140027215431488 submission_runner.py:408] Time since start: 41757.51s, 	Step: 118917, 	{'train/accuracy': 0.7732780575752258, 'train/loss': 0.8545231819152832, 'validation/accuracy': 0.6861199736595154, 'validation/loss': 1.2665618658065796, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 1.9834524393081665, 'test/num_examples': 10000, 'score': 40333.14848613739, 'total_duration': 41757.50674414635, 'accumulated_submission_time': 40333.14848613739, 'accumulated_eval_time': 1417.0705609321594, 'accumulated_logging_time': 3.109504461288452}
I0128 18:53:38.839478 139866171975424 logging_writer.py:48] [118917] accumulated_eval_time=1417.070561, accumulated_logging_time=3.109504, accumulated_submission_time=40333.148486, global_step=118917, preemption_count=0, score=40333.148486, test/accuracy=0.560100, test/loss=1.983452, test/num_examples=10000, total_duration=41757.506744, train/accuracy=0.773278, train/loss=0.854523, validation/accuracy=0.686120, validation/loss=1.266562, validation/num_examples=50000
I0128 18:54:07.247469 139866180368128 logging_writer.py:48] [119000] global_step=119000, grad_norm=6.2111921310424805, loss=1.5239852666854858
I0128 18:54:41.024964 139866171975424 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.854959964752197, loss=1.4850633144378662
I0128 18:55:14.851742 139866180368128 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.6653218269348145, loss=1.3715400695800781
I0128 18:55:48.652662 139866171975424 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.0379252433776855, loss=1.3905311822891235
I0128 18:56:22.468680 139866180368128 logging_writer.py:48] [119400] global_step=119400, grad_norm=5.343726634979248, loss=1.4389081001281738
I0128 18:56:56.283360 139866171975424 logging_writer.py:48] [119500] global_step=119500, grad_norm=5.2278242111206055, loss=1.444419503211975
I0128 18:57:30.209914 139866180368128 logging_writer.py:48] [119600] global_step=119600, grad_norm=5.320430755615234, loss=1.4284636974334717
I0128 18:58:04.030079 139866171975424 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.4442644119262695, loss=1.4310033321380615
I0128 18:58:37.853645 139866180368128 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.546684741973877, loss=1.5523145198822021
I0128 18:59:11.705131 139866171975424 logging_writer.py:48] [119900] global_step=119900, grad_norm=5.329413890838623, loss=1.4229485988616943
I0128 18:59:45.537665 139866180368128 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.418872833251953, loss=1.412048101425171
I0128 19:00:19.382063 139866171975424 logging_writer.py:48] [120100] global_step=120100, grad_norm=5.635773181915283, loss=1.4394948482513428
I0128 19:00:53.203472 139866180368128 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.3810296058654785, loss=1.4084490537643433
I0128 19:01:27.031477 139866171975424 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.192853927612305, loss=1.4700480699539185
I0128 19:02:00.854910 139866180368128 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.976063251495361, loss=1.3578052520751953
I0128 19:02:09.134077 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:02:15.332456 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:02:24.124600 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:02:26.733838 140027215431488 submission_runner.py:408] Time since start: 42285.44s, 	Step: 120426, 	{'train/accuracy': 0.7666015625, 'train/loss': 0.8653361201286316, 'validation/accuracy': 0.6888200044631958, 'validation/loss': 1.2589260339736938, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 1.9661813974380493, 'test/num_examples': 10000, 'score': 40843.37981677055, 'total_duration': 42285.4398317337, 'accumulated_submission_time': 40843.37981677055, 'accumulated_eval_time': 1434.6702933311462, 'accumulated_logging_time': 3.158001184463501}
I0128 19:02:26.772772 139865240893184 logging_writer.py:48] [120426] accumulated_eval_time=1434.670293, accumulated_logging_time=3.158001, accumulated_submission_time=40843.379817, global_step=120426, preemption_count=0, score=40843.379817, test/accuracy=0.558700, test/loss=1.966181, test/num_examples=10000, total_duration=42285.439832, train/accuracy=0.766602, train/loss=0.865336, validation/accuracy=0.688820, validation/loss=1.258926, validation/num_examples=50000
I0128 19:02:52.140928 139865760950016 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.560408592224121, loss=1.3863990306854248
I0128 19:03:25.939063 139865240893184 logging_writer.py:48] [120600] global_step=120600, grad_norm=5.484844207763672, loss=1.4383118152618408
I0128 19:03:59.868907 139865760950016 logging_writer.py:48] [120700] global_step=120700, grad_norm=5.362722396850586, loss=1.4147192239761353
I0128 19:04:33.689579 139865240893184 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.928829193115234, loss=1.3789933919906616
I0128 19:05:07.496292 139865760950016 logging_writer.py:48] [120900] global_step=120900, grad_norm=5.996537208557129, loss=1.304023027420044
I0128 19:05:41.312720 139865240893184 logging_writer.py:48] [121000] global_step=121000, grad_norm=5.072939395904541, loss=1.509332537651062
I0128 19:06:15.139095 139865760950016 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.025907516479492, loss=1.374114751815796
I0128 19:06:48.959664 139865240893184 logging_writer.py:48] [121200] global_step=121200, grad_norm=5.568915843963623, loss=1.2474623918533325
I0128 19:07:22.767339 139865760950016 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.389822483062744, loss=1.4047213792800903
I0128 19:07:56.603925 139865240893184 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.249091148376465, loss=1.3871219158172607
I0128 19:08:30.439080 139865760950016 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.484818935394287, loss=1.405219554901123
I0128 19:09:04.298804 139865240893184 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.876509666442871, loss=1.4692566394805908
I0128 19:09:38.167133 139865760950016 logging_writer.py:48] [121700] global_step=121700, grad_norm=5.557895183563232, loss=1.375524640083313
I0128 19:10:12.053278 139865240893184 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.269051551818848, loss=1.449655532836914
I0128 19:10:45.890248 139865760950016 logging_writer.py:48] [121900] global_step=121900, grad_norm=5.478846073150635, loss=1.3920172452926636
I0128 19:10:56.879942 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:11:03.147447 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:11:11.917370 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:11:14.522087 140027215431488 submission_runner.py:408] Time since start: 42813.23s, 	Step: 121934, 	{'train/accuracy': 0.76664137840271, 'train/loss': 0.8641605377197266, 'validation/accuracy': 0.6898999810218811, 'validation/loss': 1.2518686056137085, 'validation/num_examples': 50000, 'test/accuracy': 0.5608000159263611, 'test/loss': 1.9467543363571167, 'test/num_examples': 10000, 'score': 41353.42318153381, 'total_duration': 42813.22805428505, 'accumulated_submission_time': 41353.42318153381, 'accumulated_eval_time': 1452.312379360199, 'accumulated_logging_time': 3.207282543182373}
I0128 19:11:14.569670 139866171975424 logging_writer.py:48] [121934] accumulated_eval_time=1452.312379, accumulated_logging_time=3.207283, accumulated_submission_time=41353.423182, global_step=121934, preemption_count=0, score=41353.423182, test/accuracy=0.560800, test/loss=1.946754, test/num_examples=10000, total_duration=42813.228054, train/accuracy=0.766641, train/loss=0.864161, validation/accuracy=0.689900, validation/loss=1.251869, validation/num_examples=50000
I0128 19:11:37.188946 139866180368128 logging_writer.py:48] [122000] global_step=122000, grad_norm=5.046165466308594, loss=1.3754820823669434
I0128 19:12:11.018528 139866171975424 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.934054851531982, loss=1.4450054168701172
I0128 19:12:44.832974 139866180368128 logging_writer.py:48] [122200] global_step=122200, grad_norm=5.142872333526611, loss=1.3934742212295532
I0128 19:13:18.650126 139866171975424 logging_writer.py:48] [122300] global_step=122300, grad_norm=6.05105447769165, loss=1.3666253089904785
I0128 19:13:52.483829 139866180368128 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.394599914550781, loss=1.3967032432556152
I0128 19:14:26.297640 139866171975424 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.904728889465332, loss=1.3620190620422363
I0128 19:15:00.188982 139866180368128 logging_writer.py:48] [122600] global_step=122600, grad_norm=6.5254387855529785, loss=1.285317063331604
I0128 19:15:34.031277 139866171975424 logging_writer.py:48] [122700] global_step=122700, grad_norm=6.232264518737793, loss=1.4270741939544678
I0128 19:16:07.825346 139866180368128 logging_writer.py:48] [122800] global_step=122800, grad_norm=5.67903470993042, loss=1.3718631267547607
I0128 19:16:41.743192 139866171975424 logging_writer.py:48] [122900] global_step=122900, grad_norm=5.817943572998047, loss=1.4203535318374634
I0128 19:17:15.619915 139866180368128 logging_writer.py:48] [123000] global_step=123000, grad_norm=6.013043403625488, loss=1.406812310218811
I0128 19:17:49.469392 139866171975424 logging_writer.py:48] [123100] global_step=123100, grad_norm=5.0044965744018555, loss=1.238808274269104
I0128 19:18:23.297943 139866180368128 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.3108954429626465, loss=1.395458698272705
I0128 19:18:57.160378 139866171975424 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.590099334716797, loss=1.4417113065719604
I0128 19:19:31.003871 139866180368128 logging_writer.py:48] [123400] global_step=123400, grad_norm=6.29990291595459, loss=1.4650424718856812
I0128 19:19:44.652101 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:19:51.050093 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:19:59.668575 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:20:02.312428 140027215431488 submission_runner.py:408] Time since start: 43341.02s, 	Step: 123442, 	{'train/accuracy': 0.7724210619926453, 'train/loss': 0.8580043315887451, 'validation/accuracy': 0.6927599906921387, 'validation/loss': 1.2386715412139893, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9441684484481812, 'test/num_examples': 10000, 'score': 41863.442735910416, 'total_duration': 43341.018417835236, 'accumulated_submission_time': 41863.442735910416, 'accumulated_eval_time': 1469.9726836681366, 'accumulated_logging_time': 3.264508008956909}
I0128 19:20:02.354585 139865240893184 logging_writer.py:48] [123442] accumulated_eval_time=1469.972684, accumulated_logging_time=3.264508, accumulated_submission_time=41863.442736, global_step=123442, preemption_count=0, score=41863.442736, test/accuracy=0.567800, test/loss=1.944168, test/num_examples=10000, total_duration=43341.018418, train/accuracy=0.772421, train/loss=0.858004, validation/accuracy=0.692760, validation/loss=1.238672, validation/num_examples=50000
I0128 19:20:22.349280 139865760950016 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.239877700805664, loss=1.3783612251281738
I0128 19:20:56.148213 139865240893184 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.495849132537842, loss=1.2939536571502686
I0128 19:21:29.998198 139865760950016 logging_writer.py:48] [123700] global_step=123700, grad_norm=5.5727081298828125, loss=1.4077739715576172
I0128 19:22:03.804417 139865240893184 logging_writer.py:48] [123800] global_step=123800, grad_norm=6.153191566467285, loss=1.3172402381896973
I0128 19:22:37.729501 139865760950016 logging_writer.py:48] [123900] global_step=123900, grad_norm=6.935540676116943, loss=1.396574854850769
I0128 19:23:11.594340 139865240893184 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.189268112182617, loss=1.3737924098968506
I0128 19:23:45.462253 139865760950016 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.483318328857422, loss=1.4357609748840332
I0128 19:24:19.292294 139865240893184 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.651119709014893, loss=1.4034546613693237
I0128 19:24:53.153135 139865760950016 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.478804111480713, loss=1.2587049007415771
I0128 19:25:26.976599 139865240893184 logging_writer.py:48] [124400] global_step=124400, grad_norm=6.124422550201416, loss=1.4830824136734009
I0128 19:26:00.808840 139865760950016 logging_writer.py:48] [124500] global_step=124500, grad_norm=6.751874923706055, loss=1.4697766304016113
I0128 19:26:34.633934 139865240893184 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.506319999694824, loss=1.4796022176742554
I0128 19:27:08.456972 139865760950016 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.136279582977295, loss=1.3482930660247803
I0128 19:27:42.261663 139865240893184 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.52368688583374, loss=1.3147556781768799
I0128 19:28:16.095740 139865760950016 logging_writer.py:48] [124900] global_step=124900, grad_norm=6.474143981933594, loss=1.4808809757232666
I0128 19:28:32.456323 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:28:39.422808 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:28:48.255779 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:28:50.893187 140027215431488 submission_runner.py:408] Time since start: 43869.60s, 	Step: 124950, 	{'train/accuracy': 0.7725605964660645, 'train/loss': 0.8497375845909119, 'validation/accuracy': 0.6963599920272827, 'validation/loss': 1.2183761596679688, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9078247547149658, 'test/num_examples': 10000, 'score': 42373.48074388504, 'total_duration': 43869.59917449951, 'accumulated_submission_time': 42373.48074388504, 'accumulated_eval_time': 1488.4095180034637, 'accumulated_logging_time': 3.31595516204834}
I0128 19:28:50.934480 139865240893184 logging_writer.py:48] [124950] accumulated_eval_time=1488.409518, accumulated_logging_time=3.315955, accumulated_submission_time=42373.480744, global_step=124950, preemption_count=0, score=42373.480744, test/accuracy=0.569700, test/loss=1.907825, test/num_examples=10000, total_duration=43869.599174, train/accuracy=0.772561, train/loss=0.849738, validation/accuracy=0.696360, validation/loss=1.218376, validation/num_examples=50000
I0128 19:29:08.273618 139866171975424 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.872887134552002, loss=1.442589282989502
I0128 19:29:42.121390 139865240893184 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.527176380157471, loss=1.386496901512146
I0128 19:30:15.972152 139866171975424 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.8155412673950195, loss=1.312699556350708
I0128 19:30:49.828623 139865240893184 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.892719745635986, loss=1.4471715688705444
I0128 19:31:23.651113 139866171975424 logging_writer.py:48] [125400] global_step=125400, grad_norm=5.416306972503662, loss=1.3121753931045532
I0128 19:31:57.479681 139865240893184 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.493122100830078, loss=1.4047577381134033
I0128 19:32:31.287929 139866171975424 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.632505893707275, loss=1.3597145080566406
I0128 19:33:05.146202 139865240893184 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.828019142150879, loss=1.3312253952026367
I0128 19:33:38.989393 139866171975424 logging_writer.py:48] [125800] global_step=125800, grad_norm=6.210410118103027, loss=1.427917718887329
I0128 19:34:12.810312 139865240893184 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.954946517944336, loss=1.2422640323638916
I0128 19:34:46.641501 139866171975424 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.865302562713623, loss=1.4095100164413452
I0128 19:35:20.557434 139865240893184 logging_writer.py:48] [126100] global_step=126100, grad_norm=7.309144020080566, loss=1.2922227382659912
I0128 19:35:54.437054 139866171975424 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.166843414306641, loss=1.3149127960205078
I0128 19:36:28.278502 139865240893184 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.79891300201416, loss=1.4148377180099487
I0128 19:37:02.105680 139866171975424 logging_writer.py:48] [126400] global_step=126400, grad_norm=6.185355186462402, loss=1.323430061340332
I0128 19:37:21.178547 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:37:27.379625 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:37:36.324252 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:37:38.956088 140027215431488 submission_runner.py:408] Time since start: 44397.66s, 	Step: 126458, 	{'train/accuracy': 0.8109255433082581, 'train/loss': 0.6936679482460022, 'validation/accuracy': 0.7005800008773804, 'validation/loss': 1.2135677337646484, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.912022590637207, 'test/num_examples': 10000, 'score': 42883.66209101677, 'total_duration': 44397.662073135376, 'accumulated_submission_time': 42883.66209101677, 'accumulated_eval_time': 1506.1870160102844, 'accumulated_logging_time': 3.367685079574585}
I0128 19:37:38.996548 139865760950016 logging_writer.py:48] [126458] accumulated_eval_time=1506.187016, accumulated_logging_time=3.367685, accumulated_submission_time=42883.662091, global_step=126458, preemption_count=0, score=42883.662091, test/accuracy=0.572200, test/loss=1.912023, test/num_examples=10000, total_duration=44397.662073, train/accuracy=0.810926, train/loss=0.693668, validation/accuracy=0.700580, validation/loss=1.213568, validation/num_examples=50000
I0128 19:37:53.579599 139865769342720 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.68628454208374, loss=1.3857554197311401
I0128 19:38:27.370486 139865760950016 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.481722354888916, loss=1.3482539653778076
I0128 19:39:01.183294 139865769342720 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.816861629486084, loss=1.3394536972045898
I0128 19:39:34.981981 139865760950016 logging_writer.py:48] [126800] global_step=126800, grad_norm=6.058051109313965, loss=1.3899999856948853
I0128 19:40:08.805690 139865769342720 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.447991847991943, loss=1.2760815620422363
I0128 19:40:42.673229 139865760950016 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.416101455688477, loss=1.3248980045318604
I0128 19:41:16.507265 139865769342720 logging_writer.py:48] [127100] global_step=127100, grad_norm=6.480231761932373, loss=1.3928788900375366
I0128 19:41:50.567925 139865760950016 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.794765472412109, loss=1.3385777473449707
I0128 19:42:24.408677 139865769342720 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.58890962600708, loss=1.343402624130249
I0128 19:42:58.238755 139865760950016 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.904726028442383, loss=1.4307053089141846
I0128 19:43:32.094193 139865769342720 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.727078914642334, loss=1.4387997388839722
I0128 19:44:05.916345 139865760950016 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.953841209411621, loss=1.3289004564285278
I0128 19:44:39.744045 139865769342720 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.262668132781982, loss=1.3884624242782593
I0128 19:45:13.581151 139865760950016 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.339305400848389, loss=1.278239130973816
I0128 19:45:47.409127 139865769342720 logging_writer.py:48] [127900] global_step=127900, grad_norm=6.698043346405029, loss=1.3892717361450195
I0128 19:46:09.206929 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:46:15.379713 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:46:24.156876 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:46:26.774587 140027215431488 submission_runner.py:408] Time since start: 44925.48s, 	Step: 127966, 	{'train/accuracy': 0.7948222160339355, 'train/loss': 0.7559593319892883, 'validation/accuracy': 0.7019599676132202, 'validation/loss': 1.1997932195663452, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.9222203493118286, 'test/num_examples': 10000, 'score': 43393.808292627335, 'total_duration': 44925.48057103157, 'accumulated_submission_time': 43393.808292627335, 'accumulated_eval_time': 1523.7546339035034, 'accumulated_logging_time': 3.41774845123291}
I0128 19:46:26.815568 139866171975424 logging_writer.py:48] [127966] accumulated_eval_time=1523.754634, accumulated_logging_time=3.417748, accumulated_submission_time=43393.808293, global_step=127966, preemption_count=0, score=43393.808293, test/accuracy=0.576800, test/loss=1.922220, test/num_examples=10000, total_duration=44925.480571, train/accuracy=0.794822, train/loss=0.755959, validation/accuracy=0.701960, validation/loss=1.199793, validation/num_examples=50000
I0128 19:46:38.648228 139866180368128 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.961411952972412, loss=1.4623152017593384
I0128 19:47:12.462064 139866171975424 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.458096504211426, loss=1.373515009880066
I0128 19:47:46.274214 139866180368128 logging_writer.py:48] [128200] global_step=128200, grad_norm=6.75447416305542, loss=1.338606834411621
I0128 19:48:20.244609 139866171975424 logging_writer.py:48] [128300] global_step=128300, grad_norm=6.276997089385986, loss=1.3063695430755615
I0128 19:48:54.094618 139866180368128 logging_writer.py:48] [128400] global_step=128400, grad_norm=6.209495544433594, loss=1.3187921047210693
I0128 19:49:27.933972 139866171975424 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.9468302726745605, loss=1.3250378370285034
I0128 19:50:01.753740 139866180368128 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.6712517738342285, loss=1.309552550315857
I0128 19:50:35.580093 139866171975424 logging_writer.py:48] [128700] global_step=128700, grad_norm=6.01945161819458, loss=1.242676854133606
I0128 19:51:09.401628 139866180368128 logging_writer.py:48] [128800] global_step=128800, grad_norm=6.062089920043945, loss=1.341484546661377
I0128 19:51:43.201309 139866171975424 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.201402187347412, loss=1.282530665397644
I0128 19:52:17.025690 139866180368128 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.7454071044921875, loss=1.3240634202957153
I0128 19:52:50.834394 139866171975424 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.653390884399414, loss=1.2629117965698242
I0128 19:53:24.649476 139866180368128 logging_writer.py:48] [129200] global_step=129200, grad_norm=6.066542625427246, loss=1.3490667343139648
I0128 19:53:58.462583 139866171975424 logging_writer.py:48] [129300] global_step=129300, grad_norm=6.455324649810791, loss=1.383776068687439
I0128 19:54:32.378726 139866180368128 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.597468376159668, loss=1.1928719282150269
I0128 19:54:56.879484 140027215431488 spec.py:321] Evaluating on the training split.
I0128 19:55:03.158028 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 19:55:12.049553 140027215431488 spec.py:349] Evaluating on the test split.
I0128 19:55:14.664592 140027215431488 submission_runner.py:408] Time since start: 45453.37s, 	Step: 129474, 	{'train/accuracy': 0.7926099896430969, 'train/loss': 0.7674739360809326, 'validation/accuracy': 0.70305997133255, 'validation/loss': 1.1998165845870972, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 1.9009696245193481, 'test/num_examples': 10000, 'score': 43903.809019088745, 'total_duration': 45453.370572805405, 'accumulated_submission_time': 43903.809019088745, 'accumulated_eval_time': 1541.539691209793, 'accumulated_logging_time': 3.468662738800049}
I0128 19:55:14.704519 139865769342720 logging_writer.py:48] [129474] accumulated_eval_time=1541.539691, accumulated_logging_time=3.468663, accumulated_submission_time=43903.809019, global_step=129474, preemption_count=0, score=43903.809019, test/accuracy=0.568600, test/loss=1.900970, test/num_examples=10000, total_duration=45453.370573, train/accuracy=0.792610, train/loss=0.767474, validation/accuracy=0.703060, validation/loss=1.199817, validation/num_examples=50000
I0128 19:55:23.851255 139866163582720 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.861608505249023, loss=1.369400978088379
I0128 19:55:57.662194 139865769342720 logging_writer.py:48] [129600] global_step=129600, grad_norm=6.2004313468933105, loss=1.4598535299301147
I0128 19:56:31.464849 139866163582720 logging_writer.py:48] [129700] global_step=129700, grad_norm=6.1130781173706055, loss=1.3253173828125
I0128 19:57:05.294326 139865769342720 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.447851181030273, loss=1.2324579954147339
I0128 19:57:39.115800 139866163582720 logging_writer.py:48] [129900] global_step=129900, grad_norm=6.069821357727051, loss=1.2805392742156982
I0128 19:58:12.965180 139865769342720 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.86732292175293, loss=1.3254127502441406
I0128 19:58:46.747839 139866163582720 logging_writer.py:48] [130100] global_step=130100, grad_norm=5.769944667816162, loss=1.271193027496338
I0128 19:59:20.580372 139865769342720 logging_writer.py:48] [130200] global_step=130200, grad_norm=6.189676761627197, loss=1.2180242538452148
I0128 19:59:54.396754 139866163582720 logging_writer.py:48] [130300] global_step=130300, grad_norm=8.337021827697754, loss=1.3091193437576294
I0128 20:00:28.211400 139865769342720 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.5659050941467285, loss=1.2572182416915894
I0128 20:01:02.070967 139866163582720 logging_writer.py:48] [130500] global_step=130500, grad_norm=6.45841646194458, loss=1.2437108755111694
I0128 20:01:35.880047 139865769342720 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.861972332000732, loss=1.3177958726882935
I0128 20:02:09.716672 139866163582720 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.910877227783203, loss=1.2925267219543457
I0128 20:02:43.535340 139865769342720 logging_writer.py:48] [130800] global_step=130800, grad_norm=6.436888217926025, loss=1.4372395277023315
I0128 20:03:17.368191 139866163582720 logging_writer.py:48] [130900] global_step=130900, grad_norm=6.085353851318359, loss=1.428229570388794
I0128 20:03:44.915743 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:03:51.090426 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:03:59.958200 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:04:02.537723 140027215431488 submission_runner.py:408] Time since start: 45981.24s, 	Step: 130983, 	{'train/accuracy': 0.7925103306770325, 'train/loss': 0.7734469175338745, 'validation/accuracy': 0.7093999981880188, 'validation/loss': 1.1662369966506958, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.8419197797775269, 'test/num_examples': 10000, 'score': 44413.95647978783, 'total_duration': 45981.24368643761, 'accumulated_submission_time': 44413.95647978783, 'accumulated_eval_time': 1559.1616296768188, 'accumulated_logging_time': 3.518587350845337}
I0128 20:04:02.582122 139865760950016 logging_writer.py:48] [130983] accumulated_eval_time=1559.161630, accumulated_logging_time=3.518587, accumulated_submission_time=44413.956480, global_step=130983, preemption_count=0, score=44413.956480, test/accuracy=0.580900, test/loss=1.841920, test/num_examples=10000, total_duration=45981.243686, train/accuracy=0.792510, train/loss=0.773447, validation/accuracy=0.709400, validation/loss=1.166237, validation/num_examples=50000
I0128 20:04:08.672194 139866171975424 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.642070770263672, loss=1.2523646354675293
I0128 20:04:42.505951 139865760950016 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.816009521484375, loss=1.3061463832855225
I0128 20:05:16.319259 139866171975424 logging_writer.py:48] [131200] global_step=131200, grad_norm=6.52694034576416, loss=1.2631949186325073
I0128 20:05:50.157058 139865760950016 logging_writer.py:48] [131300] global_step=131300, grad_norm=6.236052989959717, loss=1.2788188457489014
I0128 20:06:24.021323 139866171975424 logging_writer.py:48] [131400] global_step=131400, grad_norm=6.799520969390869, loss=1.4515807628631592
I0128 20:06:57.830292 139865760950016 logging_writer.py:48] [131500] global_step=131500, grad_norm=7.1582560539245605, loss=1.3151872158050537
I0128 20:07:31.731482 139866171975424 logging_writer.py:48] [131600] global_step=131600, grad_norm=6.464532852172852, loss=1.3621037006378174
I0128 20:08:05.562909 139865760950016 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.802720546722412, loss=1.3135507106781006
I0128 20:08:39.421046 139866171975424 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.671998977661133, loss=1.1826364994049072
I0128 20:09:13.232631 139865760950016 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.855808734893799, loss=1.1614044904708862
I0128 20:09:47.088173 139866171975424 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.4765520095825195, loss=1.2598021030426025
I0128 20:10:20.913826 139865760950016 logging_writer.py:48] [132100] global_step=132100, grad_norm=6.254724979400635, loss=1.233522891998291
I0128 20:10:54.778058 139866171975424 logging_writer.py:48] [132200] global_step=132200, grad_norm=6.019629955291748, loss=1.3112373352050781
I0128 20:11:28.595798 139865760950016 logging_writer.py:48] [132300] global_step=132300, grad_norm=6.503139972686768, loss=1.3723796606063843
I0128 20:12:02.446285 139866171975424 logging_writer.py:48] [132400] global_step=132400, grad_norm=6.351258277893066, loss=1.2699286937713623
I0128 20:12:32.705826 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:12:38.935634 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:12:47.888355 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:12:50.452220 140027215431488 submission_runner.py:408] Time since start: 46509.16s, 	Step: 132491, 	{'train/accuracy': 0.7926897406578064, 'train/loss': 0.7695133090019226, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.17943274974823, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8631477355957031, 'test/num_examples': 10000, 'score': 44924.015117406845, 'total_duration': 46509.158210754395, 'accumulated_submission_time': 44924.015117406845, 'accumulated_eval_time': 1576.9079988002777, 'accumulated_logging_time': 3.5731399059295654}
I0128 20:12:50.496115 139865240893184 logging_writer.py:48] [132491] accumulated_eval_time=1576.907999, accumulated_logging_time=3.573140, accumulated_submission_time=44924.015117, global_step=132491, preemption_count=0, score=44924.015117, test/accuracy=0.578100, test/loss=1.863148, test/num_examples=10000, total_duration=46509.158211, train/accuracy=0.792690, train/loss=0.769513, validation/accuracy=0.707920, validation/loss=1.179433, validation/num_examples=50000
I0128 20:12:53.894086 139865769342720 logging_writer.py:48] [132500] global_step=132500, grad_norm=6.095834255218506, loss=1.2866294384002686
I0128 20:13:27.691058 139865240893184 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.388156414031982, loss=1.2870514392852783
I0128 20:14:01.668244 139865769342720 logging_writer.py:48] [132700] global_step=132700, grad_norm=6.8433966636657715, loss=1.2832725048065186
I0128 20:14:35.479317 139865240893184 logging_writer.py:48] [132800] global_step=132800, grad_norm=6.384541988372803, loss=1.3225892782211304
I0128 20:15:09.314881 139865769342720 logging_writer.py:48] [132900] global_step=132900, grad_norm=6.458711624145508, loss=1.3141207695007324
I0128 20:15:43.149027 139865240893184 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.779582500457764, loss=1.2610663175582886
I0128 20:16:16.966631 139865769342720 logging_writer.py:48] [133100] global_step=133100, grad_norm=6.1811347007751465, loss=1.3110177516937256
I0128 20:16:50.797827 139865240893184 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.524655342102051, loss=1.170755386352539
I0128 20:17:24.612740 139865769342720 logging_writer.py:48] [133300] global_step=133300, grad_norm=7.09701681137085, loss=1.3445541858673096
I0128 20:17:58.427411 139865240893184 logging_writer.py:48] [133400] global_step=133400, grad_norm=6.200253486633301, loss=1.2845276594161987
I0128 20:18:32.247505 139865769342720 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.715569019317627, loss=1.3168665170669556
I0128 20:19:06.058144 139865240893184 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.942860126495361, loss=1.1548689603805542
I0128 20:19:39.884177 139865769342720 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.748987197875977, loss=1.2685987949371338
I0128 20:20:13.900082 139865240893184 logging_writer.py:48] [133800] global_step=133800, grad_norm=5.639805793762207, loss=1.169233798980713
I0128 20:20:47.717833 139865769342720 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.989879608154297, loss=1.2305840253829956
I0128 20:21:20.661144 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:21:26.812883 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:21:35.800094 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:21:38.435343 140027215431488 submission_runner.py:408] Time since start: 47037.14s, 	Step: 133999, 	{'train/accuracy': 0.7933274507522583, 'train/loss': 0.7550987601280212, 'validation/accuracy': 0.711899995803833, 'validation/loss': 1.1559367179870605, 'validation/num_examples': 50000, 'test/accuracy': 0.5849000215530396, 'test/loss': 1.8598921298980713, 'test/num_examples': 10000, 'score': 45434.115975379944, 'total_duration': 47037.14133429527, 'accumulated_submission_time': 45434.115975379944, 'accumulated_eval_time': 1594.682165145874, 'accumulated_logging_time': 3.6277785301208496}
I0128 20:21:38.475974 139866171975424 logging_writer.py:48] [133999] accumulated_eval_time=1594.682165, accumulated_logging_time=3.627779, accumulated_submission_time=45434.115975, global_step=133999, preemption_count=0, score=45434.115975, test/accuracy=0.584900, test/loss=1.859892, test/num_examples=10000, total_duration=47037.141334, train/accuracy=0.793327, train/loss=0.755099, validation/accuracy=0.711900, validation/loss=1.155937, validation/num_examples=50000
I0128 20:21:39.161031 139866180368128 logging_writer.py:48] [134000] global_step=134000, grad_norm=6.336475849151611, loss=1.2749899625778198
I0128 20:22:13.016562 139866171975424 logging_writer.py:48] [134100] global_step=134100, grad_norm=6.235588550567627, loss=1.319488286972046
I0128 20:22:46.837735 139866180368128 logging_writer.py:48] [134200] global_step=134200, grad_norm=6.34189510345459, loss=1.2483874559402466
I0128 20:23:20.673507 139866171975424 logging_writer.py:48] [134300] global_step=134300, grad_norm=6.28579568862915, loss=1.253547191619873
I0128 20:23:54.530527 139866180368128 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.715662956237793, loss=1.2653427124023438
I0128 20:24:28.326970 139866171975424 logging_writer.py:48] [134500] global_step=134500, grad_norm=6.782635688781738, loss=1.2724430561065674
I0128 20:25:02.184927 139866180368128 logging_writer.py:48] [134600] global_step=134600, grad_norm=6.567634582519531, loss=1.2711350917816162
I0128 20:25:35.977855 139866171975424 logging_writer.py:48] [134700] global_step=134700, grad_norm=6.448416233062744, loss=1.257603645324707
I0128 20:26:09.817067 139866180368128 logging_writer.py:48] [134800] global_step=134800, grad_norm=6.486982822418213, loss=1.2847462892532349
I0128 20:26:43.766513 139866171975424 logging_writer.py:48] [134900] global_step=134900, grad_norm=6.193145275115967, loss=1.2792617082595825
I0128 20:27:17.605996 139866180368128 logging_writer.py:48] [135000] global_step=135000, grad_norm=5.797550678253174, loss=1.3139265775680542
I0128 20:27:51.424118 139866171975424 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.717225074768066, loss=1.2687346935272217
I0128 20:28:25.239877 139866180368128 logging_writer.py:48] [135200] global_step=135200, grad_norm=6.6209845542907715, loss=1.2764787673950195
I0128 20:28:59.073760 139866171975424 logging_writer.py:48] [135300] global_step=135300, grad_norm=6.220749855041504, loss=1.1817854642868042
I0128 20:29:32.919218 139866180368128 logging_writer.py:48] [135400] global_step=135400, grad_norm=6.103880405426025, loss=1.197113275527954
I0128 20:30:06.759344 139866171975424 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.9428229331970215, loss=1.232591986656189
I0128 20:30:08.597467 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:30:14.818363 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:30:23.522653 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:30:26.102400 140027215431488 submission_runner.py:408] Time since start: 47564.81s, 	Step: 135507, 	{'train/accuracy': 0.8343032598495483, 'train/loss': 0.6008027791976929, 'validation/accuracy': 0.7145000100135803, 'validation/loss': 1.1467500925064087, 'validation/num_examples': 50000, 'test/accuracy': 0.591200053691864, 'test/loss': 1.8425389528274536, 'test/num_examples': 10000, 'score': 45944.17465925217, 'total_duration': 47564.80839204788, 'accumulated_submission_time': 45944.17465925217, 'accumulated_eval_time': 1612.1870589256287, 'accumulated_logging_time': 3.67779278755188}
I0128 20:30:26.144855 139865232500480 logging_writer.py:48] [135507] accumulated_eval_time=1612.187059, accumulated_logging_time=3.677793, accumulated_submission_time=45944.174659, global_step=135507, preemption_count=0, score=45944.174659, test/accuracy=0.591200, test/loss=1.842539, test/num_examples=10000, total_duration=47564.808392, train/accuracy=0.834303, train/loss=0.600803, validation/accuracy=0.714500, validation/loss=1.146750, validation/num_examples=50000
I0128 20:30:57.947628 139865240893184 logging_writer.py:48] [135600] global_step=135600, grad_norm=6.437462329864502, loss=1.2339880466461182
I0128 20:31:31.752882 139865232500480 logging_writer.py:48] [135700] global_step=135700, grad_norm=6.529047966003418, loss=1.2289490699768066
I0128 20:32:05.590983 139865240893184 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.652502536773682, loss=1.273313283920288
I0128 20:32:39.518419 139865232500480 logging_writer.py:48] [135900] global_step=135900, grad_norm=6.829075813293457, loss=1.2892946004867554
I0128 20:33:13.384854 139865240893184 logging_writer.py:48] [136000] global_step=136000, grad_norm=6.643710613250732, loss=1.3140946626663208
I0128 20:33:47.214703 139865232500480 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.919368267059326, loss=1.2429139614105225
I0128 20:34:21.059696 139865240893184 logging_writer.py:48] [136200] global_step=136200, grad_norm=5.96890926361084, loss=1.20168137550354
I0128 20:34:54.887424 139865232500480 logging_writer.py:48] [136300] global_step=136300, grad_norm=6.925604343414307, loss=1.3215901851654053
I0128 20:35:28.732124 139865240893184 logging_writer.py:48] [136400] global_step=136400, grad_norm=6.445261001586914, loss=1.2693389654159546
I0128 20:36:02.551625 139865232500480 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.558602809906006, loss=1.3428373336791992
I0128 20:36:36.405256 139865240893184 logging_writer.py:48] [136600] global_step=136600, grad_norm=6.536679267883301, loss=1.3449629545211792
I0128 20:37:10.238525 139865232500480 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.6004719734191895, loss=1.2842061519622803
I0128 20:37:44.078409 139865240893184 logging_writer.py:48] [136800] global_step=136800, grad_norm=7.012642860412598, loss=1.3235994577407837
I0128 20:38:17.906046 139865232500480 logging_writer.py:48] [136900] global_step=136900, grad_norm=6.455179214477539, loss=1.2064149379730225
I0128 20:38:51.723281 139865240893184 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.831845283508301, loss=1.152557373046875
I0128 20:38:56.250638 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:39:02.547190 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:39:11.194748 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:39:13.785983 140027215431488 submission_runner.py:408] Time since start: 48092.49s, 	Step: 137013, 	{'train/accuracy': 0.8226243257522583, 'train/loss': 0.6424916386604309, 'validation/accuracy': 0.7174199819564819, 'validation/loss': 1.132103443145752, 'validation/num_examples': 50000, 'test/accuracy': 0.5916000008583069, 'test/loss': 1.8219414949417114, 'test/num_examples': 10000, 'score': 46454.21942257881, 'total_duration': 48092.49197125435, 'accumulated_submission_time': 46454.21942257881, 'accumulated_eval_time': 1629.722366809845, 'accumulated_logging_time': 3.7293410301208496}
I0128 20:39:13.828375 139866163582720 logging_writer.py:48] [137013] accumulated_eval_time=1629.722367, accumulated_logging_time=3.729341, accumulated_submission_time=46454.219423, global_step=137013, preemption_count=0, score=46454.219423, test/accuracy=0.591600, test/loss=1.821941, test/num_examples=10000, total_duration=48092.491971, train/accuracy=0.822624, train/loss=0.642492, validation/accuracy=0.717420, validation/loss=1.132103, validation/num_examples=50000
I0128 20:39:43.584385 139866171975424 logging_writer.py:48] [137100] global_step=137100, grad_norm=6.7947282791137695, loss=1.257695198059082
I0128 20:40:17.415712 139866163582720 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.880581855773926, loss=1.1532951593399048
I0128 20:40:51.243044 139866171975424 logging_writer.py:48] [137300] global_step=137300, grad_norm=6.284740447998047, loss=1.2415231466293335
I0128 20:41:25.059840 139866163582720 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.31978178024292, loss=1.23714017868042
I0128 20:41:58.906116 139866171975424 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.012685298919678, loss=1.20572829246521
I0128 20:42:32.719343 139866163582720 logging_writer.py:48] [137600] global_step=137600, grad_norm=7.405253887176514, loss=1.3069403171539307
I0128 20:43:06.560449 139866171975424 logging_writer.py:48] [137700] global_step=137700, grad_norm=6.784097671508789, loss=1.2647898197174072
I0128 20:43:40.390732 139866163582720 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.165740966796875, loss=1.1365422010421753
I0128 20:44:14.236194 139866171975424 logging_writer.py:48] [137900] global_step=137900, grad_norm=6.76914119720459, loss=1.3064271211624146
I0128 20:44:48.062649 139866163582720 logging_writer.py:48] [138000] global_step=138000, grad_norm=7.039729118347168, loss=1.2825579643249512
I0128 20:45:22.063015 139866171975424 logging_writer.py:48] [138100] global_step=138100, grad_norm=6.796441078186035, loss=1.2362498044967651
I0128 20:45:55.905988 139866163582720 logging_writer.py:48] [138200] global_step=138200, grad_norm=7.336369514465332, loss=1.360639214515686
I0128 20:46:29.777860 139866171975424 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.301424980163574, loss=1.1881977319717407
I0128 20:47:03.622201 139866163582720 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.487231731414795, loss=1.3350927829742432
I0128 20:47:37.438233 139866171975424 logging_writer.py:48] [138500] global_step=138500, grad_norm=7.153749465942383, loss=1.1769598722457886
I0128 20:47:43.996631 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:47:50.182425 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:47:59.088377 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:48:01.710885 140027215431488 submission_runner.py:408] Time since start: 48620.42s, 	Step: 138521, 	{'train/accuracy': 0.8116230964660645, 'train/loss': 0.6831390261650085, 'validation/accuracy': 0.7134799957275391, 'validation/loss': 1.1457061767578125, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.8496946096420288, 'test/num_examples': 10000, 'score': 46964.32555747032, 'total_duration': 48620.41685676575, 'accumulated_submission_time': 46964.32555747032, 'accumulated_eval_time': 1647.4365646839142, 'accumulated_logging_time': 3.780620813369751}
I0128 20:48:01.754234 139865760950016 logging_writer.py:48] [138521] accumulated_eval_time=1647.436565, accumulated_logging_time=3.780621, accumulated_submission_time=46964.325557, global_step=138521, preemption_count=0, score=46964.325557, test/accuracy=0.588800, test/loss=1.849695, test/num_examples=10000, total_duration=48620.416857, train/accuracy=0.811623, train/loss=0.683139, validation/accuracy=0.713480, validation/loss=1.145706, validation/num_examples=50000
I0128 20:48:28.794736 139865769342720 logging_writer.py:48] [138600] global_step=138600, grad_norm=6.343550682067871, loss=1.1370720863342285
I0128 20:49:02.555989 139865760950016 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.084204196929932, loss=1.258007287979126
I0128 20:49:36.408385 139865769342720 logging_writer.py:48] [138800] global_step=138800, grad_norm=7.5528082847595215, loss=1.2461718320846558
I0128 20:50:10.204671 139865760950016 logging_writer.py:48] [138900] global_step=138900, grad_norm=7.259511947631836, loss=1.2131472826004028
I0128 20:50:44.024751 139865769342720 logging_writer.py:48] [139000] global_step=139000, grad_norm=7.051342964172363, loss=1.312746524810791
I0128 20:51:17.867194 139865760950016 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.56456184387207, loss=1.198957920074463
I0128 20:51:51.834356 139865769342720 logging_writer.py:48] [139200] global_step=139200, grad_norm=6.0761213302612305, loss=1.1596097946166992
I0128 20:52:25.692064 139865760950016 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.396548748016357, loss=1.1390243768692017
I0128 20:52:59.538275 139865769342720 logging_writer.py:48] [139400] global_step=139400, grad_norm=7.055026531219482, loss=1.2773076295852661
I0128 20:53:33.354586 139865760950016 logging_writer.py:48] [139500] global_step=139500, grad_norm=5.960484027862549, loss=1.1323169469833374
I0128 20:54:07.185550 139865769342720 logging_writer.py:48] [139600] global_step=139600, grad_norm=6.468718528747559, loss=1.1949560642242432
I0128 20:54:41.048243 139865760950016 logging_writer.py:48] [139700] global_step=139700, grad_norm=6.490781784057617, loss=1.2096747159957886
I0128 20:55:14.858476 139865769342720 logging_writer.py:48] [139800] global_step=139800, grad_norm=7.489205360412598, loss=1.2669914960861206
I0128 20:55:48.703964 139865760950016 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.152289867401123, loss=1.1581742763519287
I0128 20:56:22.564357 139865769342720 logging_writer.py:48] [140000] global_step=140000, grad_norm=6.84382963180542, loss=1.105973720550537
I0128 20:56:31.828748 140027215431488 spec.py:321] Evaluating on the training split.
I0128 20:56:38.003051 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 20:56:46.543685 140027215431488 spec.py:349] Evaluating on the test split.
I0128 20:56:49.120984 140027215431488 submission_runner.py:408] Time since start: 49147.83s, 	Step: 140029, 	{'train/accuracy': 0.8157086968421936, 'train/loss': 0.6734943389892578, 'validation/accuracy': 0.7203199863433838, 'validation/loss': 1.1174829006195068, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.816175103187561, 'test/num_examples': 10000, 'score': 47474.33515691757, 'total_duration': 49147.82697439194, 'accumulated_submission_time': 47474.33515691757, 'accumulated_eval_time': 1664.7287590503693, 'accumulated_logging_time': 3.8349015712738037}
I0128 20:56:49.166180 139866163582720 logging_writer.py:48] [140029] accumulated_eval_time=1664.728759, accumulated_logging_time=3.834902, accumulated_submission_time=47474.335157, global_step=140029, preemption_count=0, score=47474.335157, test/accuracy=0.594400, test/loss=1.816175, test/num_examples=10000, total_duration=49147.826974, train/accuracy=0.815709, train/loss=0.673494, validation/accuracy=0.720320, validation/loss=1.117483, validation/num_examples=50000
I0128 20:57:13.494302 139866171975424 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.765304088592529, loss=1.1788510084152222
I0128 20:57:47.343359 139866163582720 logging_writer.py:48] [140200] global_step=140200, grad_norm=6.9281134605407715, loss=1.1872118711471558
I0128 20:58:21.264862 139866171975424 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.370326995849609, loss=1.1921577453613281
I0128 20:58:55.103422 139866163582720 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.720545291900635, loss=1.2481073141098022
I0128 20:59:28.987647 139866171975424 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.441998481750488, loss=1.1974167823791504
I0128 21:00:02.842425 139866163582720 logging_writer.py:48] [140600] global_step=140600, grad_norm=7.260250091552734, loss=1.232296109199524
I0128 21:00:36.657088 139866171975424 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.366748332977295, loss=1.1399987936019897
I0128 21:01:10.478394 139866163582720 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.438482761383057, loss=1.2000863552093506
I0128 21:01:44.284961 139866171975424 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.836400032043457, loss=1.1294697523117065
I0128 21:02:18.126669 139866163582720 logging_writer.py:48] [141000] global_step=141000, grad_norm=7.029638767242432, loss=1.1787935495376587
I0128 21:02:51.955383 139866171975424 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.786075592041016, loss=1.2346245050430298
I0128 21:03:25.793138 139866163582720 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.679507732391357, loss=1.143825888633728
I0128 21:03:59.613263 139866171975424 logging_writer.py:48] [141300] global_step=141300, grad_norm=6.508141994476318, loss=1.180647611618042
I0128 21:04:33.454965 139866163582720 logging_writer.py:48] [141400] global_step=141400, grad_norm=7.957087516784668, loss=1.1535980701446533
I0128 21:05:07.300701 139866171975424 logging_writer.py:48] [141500] global_step=141500, grad_norm=6.660256385803223, loss=1.2340022325515747
I0128 21:05:19.291953 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:05:25.466348 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:05:34.084822 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:05:36.650059 140027215431488 submission_runner.py:408] Time since start: 49675.36s, 	Step: 141537, 	{'train/accuracy': 0.8167450428009033, 'train/loss': 0.6674581170082092, 'validation/accuracy': 0.7184199690818787, 'validation/loss': 1.126118779182434, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.8183460235595703, 'test/num_examples': 10000, 'score': 47984.39821815491, 'total_duration': 49675.356055021286, 'accumulated_submission_time': 47984.39821815491, 'accumulated_eval_time': 1682.08682847023, 'accumulated_logging_time': 3.8889708518981934}
I0128 21:05:36.696043 139865760950016 logging_writer.py:48] [141537] accumulated_eval_time=1682.086828, accumulated_logging_time=3.888971, accumulated_submission_time=47984.398218, global_step=141537, preemption_count=0, score=47984.398218, test/accuracy=0.597100, test/loss=1.818346, test/num_examples=10000, total_duration=49675.356055, train/accuracy=0.816745, train/loss=0.667458, validation/accuracy=0.718420, validation/loss=1.126119, validation/num_examples=50000
I0128 21:05:58.344792 139865769342720 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.904385566711426, loss=1.2185943126678467
I0128 21:06:32.152566 139865760950016 logging_writer.py:48] [141700] global_step=141700, grad_norm=6.626587390899658, loss=1.1502070426940918
I0128 21:07:05.988570 139865769342720 logging_writer.py:48] [141800] global_step=141800, grad_norm=7.097398281097412, loss=1.1961697340011597
I0128 21:07:39.804593 139865760950016 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.439252853393555, loss=1.1911641359329224
I0128 21:08:13.671496 139865769342720 logging_writer.py:48] [142000] global_step=142000, grad_norm=6.362183094024658, loss=1.1830763816833496
I0128 21:08:47.527793 139865760950016 logging_writer.py:48] [142100] global_step=142100, grad_norm=7.633021831512451, loss=1.1307471990585327
I0128 21:09:21.370389 139865769342720 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.355067729949951, loss=1.1514678001403809
I0128 21:09:55.187998 139865760950016 logging_writer.py:48] [142300] global_step=142300, grad_norm=6.521448612213135, loss=1.1071919202804565
I0128 21:10:29.036601 139865769342720 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.8874030113220215, loss=1.192239761352539
I0128 21:11:02.928606 139865760950016 logging_writer.py:48] [142500] global_step=142500, grad_norm=6.3676886558532715, loss=1.1252508163452148
I0128 21:11:36.733334 139865769342720 logging_writer.py:48] [142600] global_step=142600, grad_norm=7.062135219573975, loss=1.2160552740097046
I0128 21:12:10.580426 139865760950016 logging_writer.py:48] [142700] global_step=142700, grad_norm=7.344798564910889, loss=1.2256909608840942
I0128 21:12:44.402366 139865769342720 logging_writer.py:48] [142800] global_step=142800, grad_norm=7.207043647766113, loss=1.2274293899536133
I0128 21:13:18.244474 139865760950016 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.843334197998047, loss=1.25035560131073
I0128 21:13:52.053597 139865769342720 logging_writer.py:48] [143000] global_step=143000, grad_norm=7.179265022277832, loss=1.1569041013717651
I0128 21:14:06.719788 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:14:12.935301 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:14:21.944415 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:14:24.528821 140027215431488 submission_runner.py:408] Time since start: 50203.23s, 	Step: 143045, 	{'train/accuracy': 0.8122807741165161, 'train/loss': 0.6797454953193665, 'validation/accuracy': 0.7173999547958374, 'validation/loss': 1.137130618095398, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.8193696737289429, 'test/num_examples': 10000, 'score': 48494.36148881912, 'total_duration': 50203.234813690186, 'accumulated_submission_time': 48494.36148881912, 'accumulated_eval_time': 1699.8958258628845, 'accumulated_logging_time': 3.9439799785614014}
I0128 21:14:24.571089 139865240893184 logging_writer.py:48] [143045] accumulated_eval_time=1699.895826, accumulated_logging_time=3.943980, accumulated_submission_time=48494.361489, global_step=143045, preemption_count=0, score=48494.361489, test/accuracy=0.592500, test/loss=1.819370, test/num_examples=10000, total_duration=50203.234814, train/accuracy=0.812281, train/loss=0.679745, validation/accuracy=0.717400, validation/loss=1.137131, validation/num_examples=50000
I0128 21:14:43.541649 139866163582720 logging_writer.py:48] [143100] global_step=143100, grad_norm=7.294017314910889, loss=1.1518118381500244
I0128 21:15:17.340108 139865240893184 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.943268775939941, loss=1.106647253036499
I0128 21:15:51.190243 139866163582720 logging_writer.py:48] [143300] global_step=143300, grad_norm=7.1201276779174805, loss=1.1786961555480957
I0128 21:16:25.025036 139865240893184 logging_writer.py:48] [143400] global_step=143400, grad_norm=7.983119964599609, loss=1.2446205615997314
I0128 21:16:58.829883 139866163582720 logging_writer.py:48] [143500] global_step=143500, grad_norm=8.991490364074707, loss=1.1804571151733398
I0128 21:17:32.732312 139865240893184 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.963778972625732, loss=1.2198474407196045
I0128 21:18:06.581728 139866163582720 logging_writer.py:48] [143700] global_step=143700, grad_norm=7.154732704162598, loss=1.1962424516677856
I0128 21:18:40.425540 139865240893184 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.91462516784668, loss=1.1455010175704956
I0128 21:19:14.244057 139866163582720 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.842865467071533, loss=1.0665972232818604
I0128 21:19:48.087237 139865240893184 logging_writer.py:48] [144000] global_step=144000, grad_norm=7.336439609527588, loss=1.1632256507873535
I0128 21:20:21.868850 139866163582720 logging_writer.py:48] [144100] global_step=144100, grad_norm=7.861058235168457, loss=1.1765660047531128
I0128 21:20:55.721250 139865240893184 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.486393451690674, loss=1.0564029216766357
I0128 21:21:29.552039 139866163582720 logging_writer.py:48] [144300] global_step=144300, grad_norm=7.239807605743408, loss=1.0977951288223267
I0128 21:22:03.396472 139865240893184 logging_writer.py:48] [144400] global_step=144400, grad_norm=7.506396770477295, loss=1.2557222843170166
I0128 21:22:37.263324 139866163582720 logging_writer.py:48] [144500] global_step=144500, grad_norm=7.04633903503418, loss=1.1297882795333862
I0128 21:22:54.692796 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:23:00.866647 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:23:09.683634 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:23:12.285990 140027215431488 submission_runner.py:408] Time since start: 50730.99s, 	Step: 144553, 	{'train/accuracy': 0.8585180044174194, 'train/loss': 0.5102043747901917, 'validation/accuracy': 0.7265799641609192, 'validation/loss': 1.0943148136138916, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.7970024347305298, 'test/num_examples': 10000, 'score': 49004.41987133026, 'total_duration': 50730.99198412895, 'accumulated_submission_time': 49004.41987133026, 'accumulated_eval_time': 1717.4889857769012, 'accumulated_logging_time': 3.996210813522339}
I0128 21:23:12.329254 139865240893184 logging_writer.py:48] [144553] accumulated_eval_time=1717.488986, accumulated_logging_time=3.996211, accumulated_submission_time=49004.419871, global_step=144553, preemption_count=0, score=49004.419871, test/accuracy=0.601600, test/loss=1.797002, test/num_examples=10000, total_duration=50730.991984, train/accuracy=0.858518, train/loss=0.510204, validation/accuracy=0.726580, validation/loss=1.094315, validation/num_examples=50000
I0128 21:23:28.561807 139865760950016 logging_writer.py:48] [144600] global_step=144600, grad_norm=7.633310794830322, loss=1.1401478052139282
I0128 21:24:02.554830 139865240893184 logging_writer.py:48] [144700] global_step=144700, grad_norm=7.519674301147461, loss=1.2197260856628418
I0128 21:24:36.377646 139865760950016 logging_writer.py:48] [144800] global_step=144800, grad_norm=7.302438735961914, loss=1.1068077087402344
I0128 21:25:10.226821 139865240893184 logging_writer.py:48] [144900] global_step=144900, grad_norm=7.288255214691162, loss=1.2379968166351318
I0128 21:25:44.042191 139865760950016 logging_writer.py:48] [145000] global_step=145000, grad_norm=7.291068077087402, loss=1.1575008630752563
I0128 21:26:17.874112 139865240893184 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.769758701324463, loss=1.1260178089141846
I0128 21:26:51.686082 139865760950016 logging_writer.py:48] [145200] global_step=145200, grad_norm=7.125131607055664, loss=1.1362476348876953
I0128 21:27:25.512510 139865240893184 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.833196640014648, loss=1.2422335147857666
I0128 21:27:59.352614 139865760950016 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.4637861251831055, loss=1.150444746017456
I0128 21:28:33.205348 139865240893184 logging_writer.py:48] [145500] global_step=145500, grad_norm=7.038014888763428, loss=1.021420955657959
I0128 21:29:07.084062 139865760950016 logging_writer.py:48] [145600] global_step=145600, grad_norm=7.596026420593262, loss=1.1598021984100342
I0128 21:29:40.948018 139865240893184 logging_writer.py:48] [145700] global_step=145700, grad_norm=7.096909523010254, loss=1.1134450435638428
I0128 21:30:14.940538 139865760950016 logging_writer.py:48] [145800] global_step=145800, grad_norm=7.327549934387207, loss=1.0969548225402832
I0128 21:30:48.786788 139865240893184 logging_writer.py:48] [145900] global_step=145900, grad_norm=7.565307140350342, loss=1.140509843826294
I0128 21:31:22.618036 139865760950016 logging_writer.py:48] [146000] global_step=146000, grad_norm=7.272583484649658, loss=1.177706241607666
I0128 21:31:42.364043 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:31:48.630151 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:31:57.466000 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:32:00.063114 140027215431488 submission_runner.py:408] Time since start: 51258.77s, 	Step: 146060, 	{'train/accuracy': 0.8483139276504517, 'train/loss': 0.5484762787818909, 'validation/accuracy': 0.729420006275177, 'validation/loss': 1.085327386856079, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.7772631645202637, 'test/num_examples': 10000, 'score': 49514.39406490326, 'total_duration': 51258.76909947395, 'accumulated_submission_time': 49514.39406490326, 'accumulated_eval_time': 1735.1880152225494, 'accumulated_logging_time': 4.048548221588135}
I0128 21:32:00.106882 139865240893184 logging_writer.py:48] [146060] accumulated_eval_time=1735.188015, accumulated_logging_time=4.048548, accumulated_submission_time=49514.394065, global_step=146060, preemption_count=0, score=49514.394065, test/accuracy=0.603500, test/loss=1.777263, test/num_examples=10000, total_duration=51258.769099, train/accuracy=0.848314, train/loss=0.548476, validation/accuracy=0.729420, validation/loss=1.085327, validation/num_examples=50000
I0128 21:32:13.941746 139865760950016 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.953700542449951, loss=1.080926537513733
I0128 21:32:47.772583 139865240893184 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.758581638336182, loss=1.1196258068084717
I0128 21:33:21.612155 139865760950016 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.758962631225586, loss=1.126695990562439
I0128 21:33:55.464152 139865240893184 logging_writer.py:48] [146400] global_step=146400, grad_norm=7.4357805252075195, loss=1.1207005977630615
I0128 21:34:29.321334 139865760950016 logging_writer.py:48] [146500] global_step=146500, grad_norm=6.947884559631348, loss=1.0994457006454468
I0128 21:35:03.111266 139865240893184 logging_writer.py:48] [146600] global_step=146600, grad_norm=7.6009416580200195, loss=1.1766239404678345
I0128 21:35:36.987936 139865760950016 logging_writer.py:48] [146700] global_step=146700, grad_norm=7.296792984008789, loss=1.085575819015503
I0128 21:36:10.844286 139865240893184 logging_writer.py:48] [146800] global_step=146800, grad_norm=8.053945541381836, loss=1.0452330112457275
I0128 21:36:44.677984 139865760950016 logging_writer.py:48] [146900] global_step=146900, grad_norm=7.120468616485596, loss=1.1527527570724487
I0128 21:37:18.548842 139865240893184 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.871227741241455, loss=1.1096761226654053
I0128 21:37:52.388376 139865760950016 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.753215312957764, loss=1.1097816228866577
I0128 21:38:26.218114 139865240893184 logging_writer.py:48] [147200] global_step=147200, grad_norm=8.361964225769043, loss=1.1049885749816895
I0128 21:39:00.048900 139865760950016 logging_writer.py:48] [147300] global_step=147300, grad_norm=7.053365230560303, loss=1.2074263095855713
I0128 21:39:33.923924 139865240893184 logging_writer.py:48] [147400] global_step=147400, grad_norm=8.481319427490234, loss=1.095123291015625
I0128 21:40:07.778972 139865760950016 logging_writer.py:48] [147500] global_step=147500, grad_norm=7.1523895263671875, loss=1.1139233112335205
I0128 21:40:30.219144 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:40:36.434468 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:40:45.424461 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:40:47.956551 140027215431488 submission_runner.py:408] Time since start: 51786.66s, 	Step: 147568, 	{'train/accuracy': 0.8409398794174194, 'train/loss': 0.5652621984481812, 'validation/accuracy': 0.7280199527740479, 'validation/loss': 1.0854853391647339, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.7862012386322021, 'test/num_examples': 10000, 'score': 50024.444157123566, 'total_duration': 51786.662527799606, 'accumulated_submission_time': 50024.444157123566, 'accumulated_eval_time': 1752.9253692626953, 'accumulated_logging_time': 4.101878881454468}
I0128 21:40:48.002970 139866180368128 logging_writer.py:48] [147568] accumulated_eval_time=1752.925369, accumulated_logging_time=4.101879, accumulated_submission_time=50024.444157, global_step=147568, preemption_count=0, score=50024.444157, test/accuracy=0.603600, test/loss=1.786201, test/num_examples=10000, total_duration=51786.662528, train/accuracy=0.840940, train/loss=0.565262, validation/accuracy=0.728020, validation/loss=1.085485, validation/num_examples=50000
I0128 21:40:59.137555 139866188760832 logging_writer.py:48] [147600] global_step=147600, grad_norm=7.152168273925781, loss=1.0010135173797607
I0128 21:41:32.985429 139866180368128 logging_writer.py:48] [147700] global_step=147700, grad_norm=7.742621898651123, loss=1.0778828859329224
I0128 21:42:06.798254 139866188760832 logging_writer.py:48] [147800] global_step=147800, grad_norm=7.184283256530762, loss=1.099028468132019
I0128 21:42:40.705603 139866180368128 logging_writer.py:48] [147900] global_step=147900, grad_norm=7.025010108947754, loss=1.0424247980117798
I0128 21:43:14.573088 139866188760832 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.684537887573242, loss=1.0936379432678223
I0128 21:43:48.377489 139866180368128 logging_writer.py:48] [148100] global_step=148100, grad_norm=7.612107276916504, loss=1.0409188270568848
I0128 21:44:22.236154 139866188760832 logging_writer.py:48] [148200] global_step=148200, grad_norm=7.6669816970825195, loss=1.2155317068099976
I0128 21:44:56.107426 139866180368128 logging_writer.py:48] [148300] global_step=148300, grad_norm=7.5315752029418945, loss=1.1013835668563843
I0128 21:45:29.930836 139866188760832 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.580112934112549, loss=1.1271703243255615
I0128 21:46:03.765044 139866180368128 logging_writer.py:48] [148500] global_step=148500, grad_norm=7.917426109313965, loss=1.105272650718689
I0128 21:46:37.620307 139866188760832 logging_writer.py:48] [148600] global_step=148600, grad_norm=7.402672290802002, loss=1.2295117378234863
I0128 21:47:11.462899 139866180368128 logging_writer.py:48] [148700] global_step=148700, grad_norm=8.061415672302246, loss=1.0605496168136597
I0128 21:47:45.312827 139866188760832 logging_writer.py:48] [148800] global_step=148800, grad_norm=8.897591590881348, loss=1.1380176544189453
I0128 21:48:19.170680 139866180368128 logging_writer.py:48] [148900] global_step=148900, grad_norm=7.310903549194336, loss=1.0944005250930786
I0128 21:48:53.015451 139866188760832 logging_writer.py:48] [149000] global_step=149000, grad_norm=7.188079357147217, loss=1.0579603910446167
I0128 21:49:17.968812 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:49:24.146218 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:49:32.833500 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:49:35.399698 140027215431488 submission_runner.py:408] Time since start: 52314.11s, 	Step: 149075, 	{'train/accuracy': 0.8464604616165161, 'train/loss': 0.5500763654708862, 'validation/accuracy': 0.7320799827575684, 'validation/loss': 1.0699559450149536, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.7584848403930664, 'test/num_examples': 10000, 'score': 50534.34899163246, 'total_duration': 52314.105689525604, 'accumulated_submission_time': 50534.34899163246, 'accumulated_eval_time': 1770.3562195301056, 'accumulated_logging_time': 4.158680438995361}
I0128 21:49:35.446934 139865232500480 logging_writer.py:48] [149075] accumulated_eval_time=1770.356220, accumulated_logging_time=4.158680, accumulated_submission_time=50534.348992, global_step=149075, preemption_count=0, score=50534.348992, test/accuracy=0.610200, test/loss=1.758485, test/num_examples=10000, total_duration=52314.105690, train/accuracy=0.846460, train/loss=0.550076, validation/accuracy=0.732080, validation/loss=1.069956, validation/num_examples=50000
I0128 21:49:44.244150 139865760950016 logging_writer.py:48] [149100] global_step=149100, grad_norm=7.726055145263672, loss=1.1062589883804321
I0128 21:50:18.028578 139865232500480 logging_writer.py:48] [149200] global_step=149200, grad_norm=7.4651360511779785, loss=1.2025066614151
I0128 21:50:51.821027 139865760950016 logging_writer.py:48] [149300] global_step=149300, grad_norm=7.765456676483154, loss=1.1267451047897339
I0128 21:51:25.630121 139865232500480 logging_writer.py:48] [149400] global_step=149400, grad_norm=7.473111629486084, loss=1.1320216655731201
I0128 21:51:59.457497 139865760950016 logging_writer.py:48] [149500] global_step=149500, grad_norm=8.372238159179688, loss=1.1124342679977417
I0128 21:52:33.277777 139865232500480 logging_writer.py:48] [149600] global_step=149600, grad_norm=7.46003532409668, loss=1.1369140148162842
I0128 21:53:07.102673 139865760950016 logging_writer.py:48] [149700] global_step=149700, grad_norm=7.7490410804748535, loss=1.0802452564239502
I0128 21:53:40.911969 139865232500480 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.3328752517700195, loss=1.1853195428848267
I0128 21:54:14.732167 139865760950016 logging_writer.py:48] [149900] global_step=149900, grad_norm=7.2798614501953125, loss=1.1113934516906738
I0128 21:54:48.568382 139865232500480 logging_writer.py:48] [150000] global_step=150000, grad_norm=7.609920501708984, loss=1.0182106494903564
I0128 21:55:22.496945 139865760950016 logging_writer.py:48] [150100] global_step=150100, grad_norm=7.355993747711182, loss=1.0814769268035889
I0128 21:55:56.326466 139865232500480 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.942188262939453, loss=0.9855389595031738
I0128 21:56:30.168807 139865760950016 logging_writer.py:48] [150300] global_step=150300, grad_norm=7.901308536529541, loss=1.1021919250488281
I0128 21:57:04.017171 139865232500480 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.2728753089904785, loss=1.0946037769317627
I0128 21:57:37.862467 139865760950016 logging_writer.py:48] [150500] global_step=150500, grad_norm=8.04404067993164, loss=1.0564813613891602
I0128 21:58:05.734584 140027215431488 spec.py:321] Evaluating on the training split.
I0128 21:58:11.955538 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 21:58:20.701160 140027215431488 spec.py:349] Evaluating on the test split.
I0128 21:58:23.314487 140027215431488 submission_runner.py:408] Time since start: 52842.02s, 	Step: 150584, 	{'train/accuracy': 0.8461614847183228, 'train/loss': 0.5413140058517456, 'validation/accuracy': 0.7317999601364136, 'validation/loss': 1.075062870979309, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.7552403211593628, 'test/num_examples': 10000, 'score': 51044.57275414467, 'total_duration': 52842.02047371864, 'accumulated_submission_time': 51044.57275414467, 'accumulated_eval_time': 1787.9360961914062, 'accumulated_logging_time': 4.216421842575073}
I0128 21:58:23.361382 139866171975424 logging_writer.py:48] [150584] accumulated_eval_time=1787.936096, accumulated_logging_time=4.216422, accumulated_submission_time=51044.572754, global_step=150584, preemption_count=0, score=51044.572754, test/accuracy=0.612500, test/loss=1.755240, test/num_examples=10000, total_duration=52842.020474, train/accuracy=0.846161, train/loss=0.541314, validation/accuracy=0.731800, validation/loss=1.075063, validation/num_examples=50000
I0128 21:58:29.138005 139866180368128 logging_writer.py:48] [150600] global_step=150600, grad_norm=7.275847434997559, loss=0.9999207258224487
I0128 21:59:02.952339 139866171975424 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.903049945831299, loss=1.0193442106246948
I0128 21:59:36.710409 139866180368128 logging_writer.py:48] [150800] global_step=150800, grad_norm=7.447610855102539, loss=1.1685525178909302
I0128 22:00:10.559968 139866171975424 logging_writer.py:48] [150900] global_step=150900, grad_norm=7.349667549133301, loss=1.0464603900909424
I0128 22:00:44.348057 139866180368128 logging_writer.py:48] [151000] global_step=151000, grad_norm=8.01407241821289, loss=1.058711051940918
I0128 22:01:18.181804 139866171975424 logging_writer.py:48] [151100] global_step=151100, grad_norm=7.248626708984375, loss=1.0144754648208618
I0128 22:01:52.033503 139866180368128 logging_writer.py:48] [151200] global_step=151200, grad_norm=7.6618876457214355, loss=1.048714280128479
I0128 22:02:25.842124 139866171975424 logging_writer.py:48] [151300] global_step=151300, grad_norm=7.257572174072266, loss=1.0419901609420776
I0128 22:02:59.696639 139866180368128 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.975829124450684, loss=0.9349499344825745
I0128 22:03:33.542847 139866171975424 logging_writer.py:48] [151500] global_step=151500, grad_norm=7.925884246826172, loss=0.9951910376548767
I0128 22:04:07.375298 139866180368128 logging_writer.py:48] [151600] global_step=151600, grad_norm=7.37454891204834, loss=1.0162850618362427
I0128 22:04:41.210221 139866171975424 logging_writer.py:48] [151700] global_step=151700, grad_norm=7.439957141876221, loss=1.1109910011291504
I0128 22:05:15.038617 139866180368128 logging_writer.py:48] [151800] global_step=151800, grad_norm=7.792884349822998, loss=0.9965989589691162
I0128 22:05:48.869785 139866171975424 logging_writer.py:48] [151900] global_step=151900, grad_norm=6.937437534332275, loss=1.050007700920105
I0128 22:06:22.692490 139866180368128 logging_writer.py:48] [152000] global_step=152000, grad_norm=8.082174301147461, loss=0.9949614405632019
I0128 22:06:53.640880 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:06:59.816313 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:07:08.459937 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:07:11.014700 140027215431488 submission_runner.py:408] Time since start: 53369.72s, 	Step: 152093, 	{'train/accuracy': 0.8462013602256775, 'train/loss': 0.5413247346878052, 'validation/accuracy': 0.737339973449707, 'validation/loss': 1.056695580482483, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.747403621673584, 'test/num_examples': 10000, 'score': 51554.790496110916, 'total_duration': 53369.720680475235, 'accumulated_submission_time': 51554.790496110916, 'accumulated_eval_time': 1805.3098711967468, 'accumulated_logging_time': 4.273014545440674}
I0128 22:07:11.058531 139865232500480 logging_writer.py:48] [152093] accumulated_eval_time=1805.309871, accumulated_logging_time=4.273015, accumulated_submission_time=51554.790496, global_step=152093, preemption_count=0, score=51554.790496, test/accuracy=0.608900, test/loss=1.747404, test/num_examples=10000, total_duration=53369.720680, train/accuracy=0.846201, train/loss=0.541325, validation/accuracy=0.737340, validation/loss=1.056696, validation/num_examples=50000
I0128 22:07:13.775065 139865240893184 logging_writer.py:48] [152100] global_step=152100, grad_norm=7.730215072631836, loss=0.9993722438812256
I0128 22:07:47.595363 139865232500480 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.5775275230407715, loss=0.988248348236084
I0128 22:08:21.503239 139865240893184 logging_writer.py:48] [152300] global_step=152300, grad_norm=7.758509635925293, loss=1.007071614265442
I0128 22:08:55.341042 139865232500480 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.737218379974365, loss=0.9678757786750793
I0128 22:09:29.190047 139865240893184 logging_writer.py:48] [152500] global_step=152500, grad_norm=8.18779468536377, loss=1.0731289386749268
I0128 22:10:03.003530 139865232500480 logging_writer.py:48] [152600] global_step=152600, grad_norm=8.14737606048584, loss=1.052872896194458
I0128 22:10:36.851856 139865240893184 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.962454795837402, loss=1.1722544431686401
I0128 22:11:10.664524 139865232500480 logging_writer.py:48] [152800] global_step=152800, grad_norm=8.15774154663086, loss=1.0837558507919312
I0128 22:11:44.478787 139865240893184 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.712120532989502, loss=1.0341590642929077
I0128 22:12:18.301865 139865232500480 logging_writer.py:48] [153000] global_step=153000, grad_norm=8.398785591125488, loss=1.0107616186141968
I0128 22:12:52.116763 139865240893184 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.503098487854004, loss=0.9838613271713257
I0128 22:13:25.934844 139865232500480 logging_writer.py:48] [153200] global_step=153200, grad_norm=7.515707492828369, loss=1.002690076828003
I0128 22:13:59.776793 139865240893184 logging_writer.py:48] [153300] global_step=153300, grad_norm=7.077674865722656, loss=0.9863365292549133
I0128 22:14:33.678567 139865232500480 logging_writer.py:48] [153400] global_step=153400, grad_norm=8.046302795410156, loss=1.0077548027038574
I0128 22:15:07.508958 139865240893184 logging_writer.py:48] [153500] global_step=153500, grad_norm=7.859060287475586, loss=1.0578052997589111
I0128 22:15:41.335347 139865232500480 logging_writer.py:48] [153600] global_step=153600, grad_norm=7.819485664367676, loss=1.0640389919281006
I0128 22:15:41.343229 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:15:47.503709 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:15:56.391104 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:15:58.938455 140027215431488 submission_runner.py:408] Time since start: 53897.64s, 	Step: 153601, 	{'train/accuracy': 0.8833306431770325, 'train/loss': 0.42076045274734497, 'validation/accuracy': 0.7371399998664856, 'validation/loss': 1.053296685218811, 'validation/num_examples': 50000, 'test/accuracy': 0.612000048160553, 'test/loss': 1.7551151514053345, 'test/num_examples': 10000, 'score': 52065.01314878464, 'total_duration': 53897.644444942474, 'accumulated_submission_time': 52065.01314878464, 'accumulated_eval_time': 1822.9050323963165, 'accumulated_logging_time': 4.3264100551605225}
I0128 22:15:58.986500 139865224107776 logging_writer.py:48] [153601] accumulated_eval_time=1822.905032, accumulated_logging_time=4.326410, accumulated_submission_time=52065.013149, global_step=153601, preemption_count=0, score=52065.013149, test/accuracy=0.612000, test/loss=1.755115, test/num_examples=10000, total_duration=53897.644445, train/accuracy=0.883331, train/loss=0.420760, validation/accuracy=0.737140, validation/loss=1.053297, validation/num_examples=50000
I0128 22:16:32.829330 139865232500480 logging_writer.py:48] [153700] global_step=153700, grad_norm=7.455714702606201, loss=0.955153226852417
I0128 22:17:06.659282 139865224107776 logging_writer.py:48] [153800] global_step=153800, grad_norm=8.104909896850586, loss=1.0044220685958862
I0128 22:17:40.508348 139865232500480 logging_writer.py:48] [153900] global_step=153900, grad_norm=8.357874870300293, loss=1.0131855010986328
I0128 22:18:14.338565 139865224107776 logging_writer.py:48] [154000] global_step=154000, grad_norm=8.448685646057129, loss=1.0655046701431274
I0128 22:18:48.209673 139865232500480 logging_writer.py:48] [154100] global_step=154100, grad_norm=7.738091945648193, loss=0.9873272180557251
I0128 22:19:22.067766 139865224107776 logging_writer.py:48] [154200] global_step=154200, grad_norm=8.116046905517578, loss=1.0828666687011719
I0128 22:19:55.884501 139865232500480 logging_writer.py:48] [154300] global_step=154300, grad_norm=7.311187267303467, loss=0.9482596516609192
I0128 22:20:29.724667 139865224107776 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.932472229003906, loss=0.9663988351821899
I0128 22:21:03.600711 139865232500480 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.422262191772461, loss=1.0462700128555298
I0128 22:21:37.467957 139865224107776 logging_writer.py:48] [154600] global_step=154600, grad_norm=7.451042175292969, loss=0.99561607837677
I0128 22:22:11.328633 139865232500480 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.586824417114258, loss=0.9726513624191284
I0128 22:22:45.167534 139865224107776 logging_writer.py:48] [154800] global_step=154800, grad_norm=7.706926345825195, loss=0.9898337125778198
I0128 22:23:18.976240 139865232500480 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.812802791595459, loss=1.0021655559539795
I0128 22:23:52.805811 139865224107776 logging_writer.py:48] [155000] global_step=155000, grad_norm=7.734492301940918, loss=1.067618489265442
I0128 22:24:26.613583 139865232500480 logging_writer.py:48] [155100] global_step=155100, grad_norm=7.320491313934326, loss=1.0246670246124268
I0128 22:24:29.127580 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:24:35.351356 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:24:44.347306 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:24:46.867848 140027215431488 submission_runner.py:408] Time since start: 54425.57s, 	Step: 155109, 	{'train/accuracy': 0.8740832209587097, 'train/loss': 0.44152387976646423, 'validation/accuracy': 0.7404199838638306, 'validation/loss': 1.0409177541732788, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.7276402711868286, 'test/num_examples': 10000, 'score': 52575.09206676483, 'total_duration': 54425.57376766205, 'accumulated_submission_time': 52575.09206676483, 'accumulated_eval_time': 1840.645209312439, 'accumulated_logging_time': 4.384929418563843}
I0128 22:24:46.912781 139866171975424 logging_writer.py:48] [155109] accumulated_eval_time=1840.645209, accumulated_logging_time=4.384929, accumulated_submission_time=52575.092067, global_step=155109, preemption_count=0, score=52575.092067, test/accuracy=0.614500, test/loss=1.727640, test/num_examples=10000, total_duration=54425.573768, train/accuracy=0.874083, train/loss=0.441524, validation/accuracy=0.740420, validation/loss=1.040918, validation/num_examples=50000
I0128 22:25:17.969503 139866180368128 logging_writer.py:48] [155200] global_step=155200, grad_norm=8.193318367004395, loss=1.0705608129501343
I0128 22:25:51.789327 139866171975424 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.633335590362549, loss=1.0390015840530396
I0128 22:26:25.601821 139866180368128 logging_writer.py:48] [155400] global_step=155400, grad_norm=8.647013664245605, loss=1.0524892807006836
I0128 22:26:59.436178 139866171975424 logging_writer.py:48] [155500] global_step=155500, grad_norm=8.123226165771484, loss=0.9758361577987671
I0128 22:27:33.404420 139866180368128 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.599721908569336, loss=1.040631890296936
I0128 22:28:07.262003 139866171975424 logging_writer.py:48] [155700] global_step=155700, grad_norm=8.817473411560059, loss=1.0571178197860718
I0128 22:28:41.052344 139866180368128 logging_writer.py:48] [155800] global_step=155800, grad_norm=7.2297444343566895, loss=0.9839935898780823
I0128 22:29:14.884399 139866171975424 logging_writer.py:48] [155900] global_step=155900, grad_norm=8.492618560791016, loss=1.055091381072998
I0128 22:29:48.710435 139866180368128 logging_writer.py:48] [156000] global_step=156000, grad_norm=8.191563606262207, loss=0.9461438059806824
I0128 22:30:22.533398 139866171975424 logging_writer.py:48] [156100] global_step=156100, grad_norm=8.058928489685059, loss=0.9873297810554504
I0128 22:30:56.356398 139866180368128 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.8125, loss=1.0941927433013916
I0128 22:31:30.185283 139866171975424 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.986752986907959, loss=0.9232498407363892
I0128 22:32:04.018132 139866180368128 logging_writer.py:48] [156400] global_step=156400, grad_norm=7.502874851226807, loss=0.9620941281318665
I0128 22:32:37.853248 139866171975424 logging_writer.py:48] [156500] global_step=156500, grad_norm=7.320225238800049, loss=1.0076197385787964
I0128 22:33:11.647646 139866180368128 logging_writer.py:48] [156600] global_step=156600, grad_norm=7.474123001098633, loss=0.8678957223892212
I0128 22:33:16.874728 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:33:23.055925 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:33:31.911847 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:33:34.856233 140027215431488 submission_runner.py:408] Time since start: 54953.56s, 	Step: 156617, 	{'train/accuracy': 0.8761957883834839, 'train/loss': 0.4427817463874817, 'validation/accuracy': 0.7436800003051758, 'validation/loss': 1.0364357233047485, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.732064127922058, 'test/num_examples': 10000, 'score': 53084.98903775215, 'total_duration': 54953.56215620041, 'accumulated_submission_time': 53084.98903775215, 'accumulated_eval_time': 1858.6266074180603, 'accumulated_logging_time': 4.439204216003418}
I0128 22:33:34.901369 139865240893184 logging_writer.py:48] [156617] accumulated_eval_time=1858.626607, accumulated_logging_time=4.439204, accumulated_submission_time=53084.989038, global_step=156617, preemption_count=0, score=53084.989038, test/accuracy=0.617800, test/loss=1.732064, test/num_examples=10000, total_duration=54953.562156, train/accuracy=0.876196, train/loss=0.442782, validation/accuracy=0.743680, validation/loss=1.036436, validation/num_examples=50000
I0128 22:34:03.290656 139865760950016 logging_writer.py:48] [156700] global_step=156700, grad_norm=8.140548706054688, loss=0.980819821357727
I0128 22:34:37.098784 139865240893184 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.954267501831055, loss=0.9956903457641602
I0128 22:35:10.962172 139865760950016 logging_writer.py:48] [156900] global_step=156900, grad_norm=7.539258003234863, loss=0.9021522402763367
I0128 22:35:44.791106 139865240893184 logging_writer.py:48] [157000] global_step=157000, grad_norm=8.78193187713623, loss=0.8766463994979858
I0128 22:36:18.614163 139865760950016 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.48146915435791, loss=0.9362430572509766
I0128 22:36:52.454526 139865240893184 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.7671427726745605, loss=1.0110254287719727
I0128 22:37:26.282505 139865760950016 logging_writer.py:48] [157300] global_step=157300, grad_norm=7.575869083404541, loss=0.9489935636520386
I0128 22:38:00.120767 139865240893184 logging_writer.py:48] [157400] global_step=157400, grad_norm=8.544242858886719, loss=1.0366392135620117
I0128 22:38:33.955511 139865760950016 logging_writer.py:48] [157500] global_step=157500, grad_norm=8.257792472839355, loss=0.9244084358215332
I0128 22:39:07.787209 139865240893184 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.914868354797363, loss=0.9810611605644226
I0128 22:39:41.615196 139865760950016 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.736119747161865, loss=0.9592850208282471
I0128 22:40:15.524557 139865240893184 logging_writer.py:48] [157800] global_step=157800, grad_norm=8.316285133361816, loss=1.0521762371063232
I0128 22:40:49.334898 139865760950016 logging_writer.py:48] [157900] global_step=157900, grad_norm=8.37828540802002, loss=0.9608176946640015
I0128 22:41:23.145766 139865240893184 logging_writer.py:48] [158000] global_step=158000, grad_norm=9.718364715576172, loss=0.9186446070671082
I0128 22:41:56.963497 139865760950016 logging_writer.py:48] [158100] global_step=158100, grad_norm=7.649162292480469, loss=0.9529288411140442
I0128 22:42:04.880676 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:42:11.019340 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:42:19.920558 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:42:22.532423 140027215431488 submission_runner.py:408] Time since start: 55481.24s, 	Step: 158125, 	{'train/accuracy': 0.8801418542861938, 'train/loss': 0.43101683259010315, 'validation/accuracy': 0.7454400062561035, 'validation/loss': 1.0232517719268799, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.7091346979141235, 'test/num_examples': 10000, 'score': 53594.90204691887, 'total_duration': 55481.23839688301, 'accumulated_submission_time': 53594.90204691887, 'accumulated_eval_time': 1876.2782986164093, 'accumulated_logging_time': 4.496425151824951}
I0128 22:42:22.581009 139866171975424 logging_writer.py:48] [158125] accumulated_eval_time=1876.278299, accumulated_logging_time=4.496425, accumulated_submission_time=53594.902047, global_step=158125, preemption_count=0, score=53594.902047, test/accuracy=0.620400, test/loss=1.709135, test/num_examples=10000, total_duration=55481.238397, train/accuracy=0.880142, train/loss=0.431017, validation/accuracy=0.745440, validation/loss=1.023252, validation/num_examples=50000
I0128 22:42:48.276111 139866180368128 logging_writer.py:48] [158200] global_step=158200, grad_norm=8.302590370178223, loss=0.8982784152030945
I0128 22:43:22.111157 139866171975424 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.58359432220459, loss=0.9118666648864746
I0128 22:43:55.915025 139866180368128 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.785971641540527, loss=0.864499032497406
I0128 22:44:29.751222 139866171975424 logging_writer.py:48] [158500] global_step=158500, grad_norm=8.029497146606445, loss=1.0279606580734253
I0128 22:45:03.576465 139866180368128 logging_writer.py:48] [158600] global_step=158600, grad_norm=9.1594820022583, loss=1.02357816696167
I0128 22:45:37.410932 139866171975424 logging_writer.py:48] [158700] global_step=158700, grad_norm=7.848904609680176, loss=0.9787716865539551
I0128 22:46:11.262732 139866180368128 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.2684454917907715, loss=0.8795154094696045
I0128 22:46:45.193916 139866171975424 logging_writer.py:48] [158900] global_step=158900, grad_norm=7.491494178771973, loss=0.9468928575515747
I0128 22:47:19.034164 139866180368128 logging_writer.py:48] [159000] global_step=159000, grad_norm=8.002367973327637, loss=0.9130183458328247
I0128 22:47:52.847924 139866171975424 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.801846027374268, loss=0.9129372239112854
I0128 22:48:26.691866 139866180368128 logging_writer.py:48] [159200] global_step=159200, grad_norm=7.881913185119629, loss=0.9514958262443542
I0128 22:49:00.499335 139866171975424 logging_writer.py:48] [159300] global_step=159300, grad_norm=8.672527313232422, loss=0.9556307196617126
I0128 22:49:34.347904 139866180368128 logging_writer.py:48] [159400] global_step=159400, grad_norm=8.865687370300293, loss=0.9753545522689819
I0128 22:50:08.187215 139866171975424 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.620377540588379, loss=0.8535424470901489
I0128 22:50:42.003471 139866180368128 logging_writer.py:48] [159600] global_step=159600, grad_norm=8.979525566101074, loss=1.0186378955841064
I0128 22:50:52.652775 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:50:58.788570 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:51:07.844674 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:51:10.347927 140027215431488 submission_runner.py:408] Time since start: 56009.05s, 	Step: 159633, 	{'train/accuracy': 0.8810586333274841, 'train/loss': 0.4157596826553345, 'validation/accuracy': 0.7454599738121033, 'validation/loss': 1.0245261192321777, 'validation/num_examples': 50000, 'test/accuracy': 0.6222000122070312, 'test/loss': 1.717750072479248, 'test/num_examples': 10000, 'score': 54104.91116786003, 'total_duration': 56009.05380535126, 'accumulated_submission_time': 54104.91116786003, 'accumulated_eval_time': 1893.9733023643494, 'accumulated_logging_time': 4.5553436279296875}
I0128 22:51:10.391922 139865760950016 logging_writer.py:48] [159633] accumulated_eval_time=1893.973302, accumulated_logging_time=4.555344, accumulated_submission_time=54104.911168, global_step=159633, preemption_count=0, score=54104.911168, test/accuracy=0.622200, test/loss=1.717750, test/num_examples=10000, total_duration=56009.053805, train/accuracy=0.881059, train/loss=0.415760, validation/accuracy=0.745460, validation/loss=1.024526, validation/num_examples=50000
I0128 22:51:33.402201 139865769342720 logging_writer.py:48] [159700] global_step=159700, grad_norm=9.270965576171875, loss=0.9290116429328918
I0128 22:52:07.237925 139865760950016 logging_writer.py:48] [159800] global_step=159800, grad_norm=8.26220989227295, loss=0.8982442617416382
I0128 22:52:41.170574 139865769342720 logging_writer.py:48] [159900] global_step=159900, grad_norm=8.174073219299316, loss=0.9467077255249023
I0128 22:53:14.976822 139865760950016 logging_writer.py:48] [160000] global_step=160000, grad_norm=8.495528221130371, loss=0.8437643051147461
I0128 22:53:48.834424 139865769342720 logging_writer.py:48] [160100] global_step=160100, grad_norm=8.589654922485352, loss=0.8959529399871826
I0128 22:54:22.688137 139865760950016 logging_writer.py:48] [160200] global_step=160200, grad_norm=8.804290771484375, loss=0.9017423391342163
I0128 22:54:56.512304 139865769342720 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.994381427764893, loss=0.9139918088912964
I0128 22:55:30.376123 139865760950016 logging_writer.py:48] [160400] global_step=160400, grad_norm=9.422520637512207, loss=0.9308269023895264
I0128 22:56:04.200277 139865769342720 logging_writer.py:48] [160500] global_step=160500, grad_norm=8.934355735778809, loss=0.9466989040374756
I0128 22:56:38.027754 139865760950016 logging_writer.py:48] [160600] global_step=160600, grad_norm=8.662055015563965, loss=0.8543281555175781
I0128 22:57:11.863260 139865769342720 logging_writer.py:48] [160700] global_step=160700, grad_norm=8.103726387023926, loss=0.9702601432800293
I0128 22:57:45.661675 139865760950016 logging_writer.py:48] [160800] global_step=160800, grad_norm=8.406006813049316, loss=0.9676475524902344
I0128 22:58:19.536313 139865769342720 logging_writer.py:48] [160900] global_step=160900, grad_norm=8.99068546295166, loss=0.8900076150894165
I0128 22:58:53.379257 139865760950016 logging_writer.py:48] [161000] global_step=161000, grad_norm=8.308975219726562, loss=0.9355758428573608
I0128 22:59:27.313197 139865769342720 logging_writer.py:48] [161100] global_step=161100, grad_norm=9.337422370910645, loss=1.0440726280212402
I0128 22:59:40.643111 140027215431488 spec.py:321] Evaluating on the training split.
I0128 22:59:46.962628 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 22:59:56.004752 140027215431488 spec.py:349] Evaluating on the test split.
I0128 22:59:58.536427 140027215431488 submission_runner.py:408] Time since start: 56537.24s, 	Step: 161141, 	{'train/accuracy': 0.8859813213348389, 'train/loss': 0.40513113141059875, 'validation/accuracy': 0.7473999857902527, 'validation/loss': 1.0201231241226196, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.703717827796936, 'test/num_examples': 10000, 'score': 54615.10005736351, 'total_duration': 56537.24241113663, 'accumulated_submission_time': 54615.10005736351, 'accumulated_eval_time': 1911.8665721416473, 'accumulated_logging_time': 4.608543395996094}
I0128 22:59:58.581529 139866171975424 logging_writer.py:48] [161141] accumulated_eval_time=1911.866572, accumulated_logging_time=4.608543, accumulated_submission_time=54615.100057, global_step=161141, preemption_count=0, score=54615.100057, test/accuracy=0.623900, test/loss=1.703718, test/num_examples=10000, total_duration=56537.242411, train/accuracy=0.885981, train/loss=0.405131, validation/accuracy=0.747400, validation/loss=1.020123, validation/num_examples=50000
I0128 23:00:18.879654 139866180368128 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.7206501960754395, loss=0.8925401568412781
I0128 23:00:52.690716 139866171975424 logging_writer.py:48] [161300] global_step=161300, grad_norm=8.312663078308105, loss=0.9020470380783081
I0128 23:01:26.513632 139866180368128 logging_writer.py:48] [161400] global_step=161400, grad_norm=8.079310417175293, loss=0.9089019894599915
I0128 23:02:00.340415 139866171975424 logging_writer.py:48] [161500] global_step=161500, grad_norm=8.20617389678955, loss=0.8853128552436829
I0128 23:02:34.183728 139866180368128 logging_writer.py:48] [161600] global_step=161600, grad_norm=8.534459114074707, loss=0.9705941081047058
I0128 23:03:08.003472 139866171975424 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.71712064743042, loss=0.8643635511398315
I0128 23:03:41.868182 139866180368128 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.934224128723145, loss=0.8659433722496033
I0128 23:04:15.717486 139866171975424 logging_writer.py:48] [161900] global_step=161900, grad_norm=9.432147026062012, loss=0.9134918451309204
I0128 23:04:49.554817 139866180368128 logging_writer.py:48] [162000] global_step=162000, grad_norm=7.714831352233887, loss=0.909062922000885
I0128 23:05:23.469186 139866171975424 logging_writer.py:48] [162100] global_step=162100, grad_norm=8.491842269897461, loss=0.9392344951629639
I0128 23:05:57.344753 139866180368128 logging_writer.py:48] [162200] global_step=162200, grad_norm=8.665948867797852, loss=0.889522910118103
I0128 23:06:31.189684 139866171975424 logging_writer.py:48] [162300] global_step=162300, grad_norm=9.18675422668457, loss=1.019578456878662
I0128 23:07:05.017301 139866180368128 logging_writer.py:48] [162400] global_step=162400, grad_norm=9.832728385925293, loss=0.9849764704704285
I0128 23:07:38.834961 139866171975424 logging_writer.py:48] [162500] global_step=162500, grad_norm=8.653518676757812, loss=0.8995619416236877
I0128 23:08:12.636290 139866180368128 logging_writer.py:48] [162600] global_step=162600, grad_norm=8.573843955993652, loss=0.9036420583724976
I0128 23:08:28.695847 140027215431488 spec.py:321] Evaluating on the training split.
I0128 23:08:34.940551 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 23:08:44.090098 140027215431488 spec.py:349] Evaluating on the test split.
I0128 23:08:46.541694 140027215431488 submission_runner.py:408] Time since start: 57065.25s, 	Step: 162649, 	{'train/accuracy': 0.9079041481018066, 'train/loss': 0.3245623707771301, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.0081355571746826, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.6878162622451782, 'test/num_examples': 10000, 'score': 55125.14937853813, 'total_duration': 57065.24767613411, 'accumulated_submission_time': 55125.14937853813, 'accumulated_eval_time': 1929.71240234375, 'accumulated_logging_time': 4.6652233600616455}
I0128 23:08:46.591957 139865760950016 logging_writer.py:48] [162649] accumulated_eval_time=1929.712402, accumulated_logging_time=4.665223, accumulated_submission_time=55125.149379, global_step=162649, preemption_count=0, score=55125.149379, test/accuracy=0.625700, test/loss=1.687816, test/num_examples=10000, total_duration=57065.247676, train/accuracy=0.907904, train/loss=0.324562, validation/accuracy=0.750320, validation/loss=1.008136, validation/num_examples=50000
I0128 23:09:04.186186 139865769342720 logging_writer.py:48] [162700] global_step=162700, grad_norm=8.878698348999023, loss=1.0118823051452637
I0128 23:09:38.011208 139865760950016 logging_writer.py:48] [162800] global_step=162800, grad_norm=8.381387710571289, loss=0.8270719647407532
I0128 23:10:11.831589 139865769342720 logging_writer.py:48] [162900] global_step=162900, grad_norm=8.496919631958008, loss=0.9379966855049133
I0128 23:10:45.655498 139865760950016 logging_writer.py:48] [163000] global_step=163000, grad_norm=8.715214729309082, loss=0.8978936672210693
I0128 23:11:19.511909 139865769342720 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.981662750244141, loss=0.8207559585571289
I0128 23:11:53.440362 139865760950016 logging_writer.py:48] [163200] global_step=163200, grad_norm=8.946406364440918, loss=0.917384922504425
I0128 23:12:27.269854 139865769342720 logging_writer.py:48] [163300] global_step=163300, grad_norm=8.670243263244629, loss=0.8824347257614136
I0128 23:13:01.125665 139865760950016 logging_writer.py:48] [163400] global_step=163400, grad_norm=9.08825969696045, loss=0.8704233169555664
I0128 23:13:34.952041 139865769342720 logging_writer.py:48] [163500] global_step=163500, grad_norm=9.279474258422852, loss=0.9417997598648071
I0128 23:14:08.826528 139865760950016 logging_writer.py:48] [163600] global_step=163600, grad_norm=9.010950088500977, loss=0.936734676361084
I0128 23:14:42.609534 139865769342720 logging_writer.py:48] [163700] global_step=163700, grad_norm=8.546756744384766, loss=0.8630161881446838
I0128 23:15:16.464489 139865760950016 logging_writer.py:48] [163800] global_step=163800, grad_norm=8.122143745422363, loss=0.8267287015914917
I0128 23:15:50.281748 139865769342720 logging_writer.py:48] [163900] global_step=163900, grad_norm=7.989537239074707, loss=0.942358136177063
I0128 23:16:24.131373 139865760950016 logging_writer.py:48] [164000] global_step=164000, grad_norm=9.509092330932617, loss=0.821932315826416
I0128 23:16:57.967034 139865769342720 logging_writer.py:48] [164100] global_step=164100, grad_norm=8.82136058807373, loss=0.8675985336303711
I0128 23:17:16.734098 140027215431488 spec.py:321] Evaluating on the training split.
I0128 23:17:22.951152 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 23:17:31.548599 140027215431488 spec.py:349] Evaluating on the test split.
I0128 23:17:34.122753 140027215431488 submission_runner.py:408] Time since start: 57592.83s, 	Step: 164157, 	{'train/accuracy': 0.9044961333274841, 'train/loss': 0.33790427446365356, 'validation/accuracy': 0.7519399523735046, 'validation/loss': 1.0083187818527222, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.704638957977295, 'test/num_examples': 10000, 'score': 55635.22860836983, 'total_duration': 57592.82874393463, 'accumulated_submission_time': 55635.22860836983, 'accumulated_eval_time': 1947.1010339260101, 'accumulated_logging_time': 4.725534439086914}
I0128 23:17:34.169896 139865240893184 logging_writer.py:48] [164157] accumulated_eval_time=1947.101034, accumulated_logging_time=4.725534, accumulated_submission_time=55635.228608, global_step=164157, preemption_count=0, score=55635.228608, test/accuracy=0.624600, test/loss=1.704639, test/num_examples=10000, total_duration=57592.828744, train/accuracy=0.904496, train/loss=0.337904, validation/accuracy=0.751940, validation/loss=1.008319, validation/num_examples=50000
I0128 23:17:49.020772 139866171975424 logging_writer.py:48] [164200] global_step=164200, grad_norm=9.649420738220215, loss=0.8499207496643066
I0128 23:18:22.876786 139865240893184 logging_writer.py:48] [164300] global_step=164300, grad_norm=9.352213859558105, loss=0.8964391350746155
I0128 23:18:56.690425 139866171975424 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.821630477905273, loss=0.915090799331665
I0128 23:19:30.522782 139865240893184 logging_writer.py:48] [164500] global_step=164500, grad_norm=8.347014427185059, loss=0.8801214694976807
I0128 23:20:04.376188 139866171975424 logging_writer.py:48] [164600] global_step=164600, grad_norm=8.214927673339844, loss=0.9059078097343445
I0128 23:20:38.225354 139865240893184 logging_writer.py:48] [164700] global_step=164700, grad_norm=7.761392116546631, loss=0.8440552949905396
I0128 23:21:12.062555 139866171975424 logging_writer.py:48] [164800] global_step=164800, grad_norm=8.429160118103027, loss=0.882178783416748
I0128 23:21:45.877854 139865240893184 logging_writer.py:48] [164900] global_step=164900, grad_norm=8.523624420166016, loss=0.8637754917144775
I0128 23:22:19.701097 139866171975424 logging_writer.py:48] [165000] global_step=165000, grad_norm=8.33536148071289, loss=0.847494900226593
I0128 23:22:53.530469 139865240893184 logging_writer.py:48] [165100] global_step=165100, grad_norm=8.601655006408691, loss=0.9084421396255493
I0128 23:23:27.356313 139866171975424 logging_writer.py:48] [165200] global_step=165200, grad_norm=8.27027702331543, loss=0.8472856283187866
I0128 23:24:01.193737 139865240893184 logging_writer.py:48] [165300] global_step=165300, grad_norm=8.159276008605957, loss=0.898438036441803
I0128 23:24:35.099574 139866171975424 logging_writer.py:48] [165400] global_step=165400, grad_norm=8.99166488647461, loss=0.8486089110374451
I0128 23:25:08.931633 139865240893184 logging_writer.py:48] [165500] global_step=165500, grad_norm=8.868814468383789, loss=0.8406431674957275
I0128 23:25:42.771929 139866171975424 logging_writer.py:48] [165600] global_step=165600, grad_norm=8.555185317993164, loss=0.8238571286201477
I0128 23:26:04.236587 140027215431488 spec.py:321] Evaluating on the training split.
I0128 23:26:10.394186 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 23:26:19.271499 140027215431488 spec.py:349] Evaluating on the test split.
I0128 23:26:21.880995 140027215431488 submission_runner.py:408] Time since start: 58120.59s, 	Step: 165665, 	{'train/accuracy': 0.9044164419174194, 'train/loss': 0.3358021378517151, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 0.9994009137153625, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.6860158443450928, 'test/num_examples': 10000, 'score': 56145.23153233528, 'total_duration': 58120.586990594864, 'accumulated_submission_time': 56145.23153233528, 'accumulated_eval_time': 1964.745409488678, 'accumulated_logging_time': 4.782070636749268}
I0128 23:26:21.928818 139865240893184 logging_writer.py:48] [165665] accumulated_eval_time=1964.745409, accumulated_logging_time=4.782071, accumulated_submission_time=56145.231532, global_step=165665, preemption_count=0, score=56145.231532, test/accuracy=0.628600, test/loss=1.686016, test/num_examples=10000, total_duration=58120.586991, train/accuracy=0.904416, train/loss=0.335802, validation/accuracy=0.752320, validation/loss=0.999401, validation/num_examples=50000
I0128 23:26:34.135345 139865760950016 logging_writer.py:48] [165700] global_step=165700, grad_norm=8.85574722290039, loss=0.8293225765228271
I0128 23:27:07.904691 139865240893184 logging_writer.py:48] [165800] global_step=165800, grad_norm=8.480935096740723, loss=0.8590056896209717
I0128 23:27:41.747888 139865760950016 logging_writer.py:48] [165900] global_step=165900, grad_norm=8.45776081085205, loss=0.8005587458610535
I0128 23:28:15.547384 139865240893184 logging_writer.py:48] [166000] global_step=166000, grad_norm=10.226591110229492, loss=0.7915534377098083
I0128 23:28:49.389529 139865760950016 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.734864234924316, loss=0.7664296627044678
I0128 23:29:23.212848 139865240893184 logging_writer.py:48] [166200] global_step=166200, grad_norm=8.253902435302734, loss=0.8828112483024597
I0128 23:29:57.064169 139865760950016 logging_writer.py:48] [166300] global_step=166300, grad_norm=10.187463760375977, loss=0.8598737120628357
I0128 23:30:30.888675 139865240893184 logging_writer.py:48] [166400] global_step=166400, grad_norm=9.514904975891113, loss=0.8533768653869629
I0128 23:31:04.827610 139865760950016 logging_writer.py:48] [166500] global_step=166500, grad_norm=8.240789413452148, loss=0.7843698859214783
I0128 23:31:38.650413 139865240893184 logging_writer.py:48] [166600] global_step=166600, grad_norm=8.455405235290527, loss=0.8366563320159912
I0128 23:32:12.482707 139865760950016 logging_writer.py:48] [166700] global_step=166700, grad_norm=9.057286262512207, loss=0.8849438428878784
I0128 23:32:46.314508 139865240893184 logging_writer.py:48] [166800] global_step=166800, grad_norm=8.515296936035156, loss=0.8310803174972534
I0128 23:33:20.147710 139865760950016 logging_writer.py:48] [166900] global_step=166900, grad_norm=9.878423690795898, loss=0.7646420001983643
I0128 23:33:53.988947 139865240893184 logging_writer.py:48] [167000] global_step=167000, grad_norm=9.359017372131348, loss=0.835663378238678
I0128 23:34:27.871757 139865760950016 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.289095878601074, loss=0.8537487387657166
I0128 23:34:52.016562 140027215431488 spec.py:321] Evaluating on the training split.
I0128 23:34:58.223377 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 23:35:07.122837 140027215431488 spec.py:349] Evaluating on the test split.
I0128 23:35:09.692912 140027215431488 submission_runner.py:408] Time since start: 58648.40s, 	Step: 167173, 	{'train/accuracy': 0.9084821343421936, 'train/loss': 0.3253271281719208, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 0.9877996444702148, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.6967089176177979, 'test/num_examples': 10000, 'score': 56655.2565972805, 'total_duration': 58648.39890432358, 'accumulated_submission_time': 56655.2565972805, 'accumulated_eval_time': 1982.4217264652252, 'accumulated_logging_time': 4.839404821395874}
I0128 23:35:09.739315 139865240893184 logging_writer.py:48] [167173] accumulated_eval_time=1982.421726, accumulated_logging_time=4.839405, accumulated_submission_time=56655.256597, global_step=167173, preemption_count=0, score=56655.256597, test/accuracy=0.630300, test/loss=1.696709, test/num_examples=10000, total_duration=58648.398904, train/accuracy=0.908482, train/loss=0.325327, validation/accuracy=0.755720, validation/loss=0.987800, validation/num_examples=50000
I0128 23:35:19.209713 139866171975424 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.84377384185791, loss=0.8210593461990356
I0128 23:35:53.025787 139865240893184 logging_writer.py:48] [167300] global_step=167300, grad_norm=8.811020851135254, loss=0.828284740447998
I0128 23:36:26.851496 139866171975424 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.659585952758789, loss=0.8236523866653442
I0128 23:37:00.731340 139865240893184 logging_writer.py:48] [167500] global_step=167500, grad_norm=8.144775390625, loss=0.8139153718948364
I0128 23:37:34.749686 139866171975424 logging_writer.py:48] [167600] global_step=167600, grad_norm=8.864810943603516, loss=0.7778948545455933
I0128 23:38:08.580085 139865240893184 logging_writer.py:48] [167700] global_step=167700, grad_norm=10.238482475280762, loss=0.8699613809585571
I0128 23:38:42.425711 139866171975424 logging_writer.py:48] [167800] global_step=167800, grad_norm=9.138619422912598, loss=0.8355365991592407
I0128 23:39:16.244804 139865240893184 logging_writer.py:48] [167900] global_step=167900, grad_norm=9.40131950378418, loss=0.8200490474700928
I0128 23:39:50.086459 139866171975424 logging_writer.py:48] [168000] global_step=168000, grad_norm=9.215380668640137, loss=0.8160635232925415
I0128 23:40:23.925477 139865240893184 logging_writer.py:48] [168100] global_step=168100, grad_norm=10.007152557373047, loss=0.8217164278030396
I0128 23:40:57.757691 139866171975424 logging_writer.py:48] [168200] global_step=168200, grad_norm=8.477956771850586, loss=0.7775791883468628
I0128 23:41:31.594726 139865240893184 logging_writer.py:48] [168300] global_step=168300, grad_norm=8.685454368591309, loss=0.784358024597168
I0128 23:42:05.439985 139866171975424 logging_writer.py:48] [168400] global_step=168400, grad_norm=8.79334545135498, loss=0.8603493571281433
I0128 23:42:39.256875 139865240893184 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.988948345184326, loss=0.8185907006263733
I0128 23:43:13.085365 139866171975424 logging_writer.py:48] [168600] global_step=168600, grad_norm=9.47128677368164, loss=0.8723981380462646
I0128 23:43:39.788880 140027215431488 spec.py:321] Evaluating on the training split.
I0128 23:43:45.966946 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 23:43:54.741171 140027215431488 spec.py:349] Evaluating on the test split.
I0128 23:43:57.436079 140027215431488 submission_runner.py:408] Time since start: 59176.14s, 	Step: 168680, 	{'train/accuracy': 0.9084821343421936, 'train/loss': 0.3189391791820526, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 0.9814531207084656, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.6696397066116333, 'test/num_examples': 10000, 'score': 57165.24439907074, 'total_duration': 59176.142056941986, 'accumulated_submission_time': 57165.24439907074, 'accumulated_eval_time': 2000.0688755512238, 'accumulated_logging_time': 4.895292043685913}
I0128 23:43:57.486336 139865769342720 logging_writer.py:48] [168680] accumulated_eval_time=2000.068876, accumulated_logging_time=4.895292, accumulated_submission_time=57165.244399, global_step=168680, preemption_count=0, score=57165.244399, test/accuracy=0.630200, test/loss=1.669640, test/num_examples=10000, total_duration=59176.142057, train/accuracy=0.908482, train/loss=0.318939, validation/accuracy=0.757200, validation/loss=0.981453, validation/num_examples=50000
I0128 23:44:04.594669 139866163582720 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.508657455444336, loss=0.8586486577987671
I0128 23:44:38.443916 139865769342720 logging_writer.py:48] [168800] global_step=168800, grad_norm=10.331772804260254, loss=0.8683077096939087
I0128 23:45:12.282523 139866163582720 logging_writer.py:48] [168900] global_step=168900, grad_norm=8.442362785339355, loss=0.7950059175491333
I0128 23:45:46.112227 139865769342720 logging_writer.py:48] [169000] global_step=169000, grad_norm=8.598603248596191, loss=0.8256201148033142
I0128 23:46:19.945605 139866163582720 logging_writer.py:48] [169100] global_step=169100, grad_norm=7.913420677185059, loss=0.7180321216583252
I0128 23:46:53.786396 139865769342720 logging_writer.py:48] [169200] global_step=169200, grad_norm=8.654848098754883, loss=0.7898211479187012
I0128 23:47:27.637276 139866163582720 logging_writer.py:48] [169300] global_step=169300, grad_norm=8.475311279296875, loss=0.8046666979789734
I0128 23:48:01.515509 139865769342720 logging_writer.py:48] [169400] global_step=169400, grad_norm=10.066752433776855, loss=0.9073834419250488
I0128 23:48:35.384720 139866163582720 logging_writer.py:48] [169500] global_step=169500, grad_norm=8.195788383483887, loss=0.7965511083602905
I0128 23:49:09.246987 139865769342720 logging_writer.py:48] [169600] global_step=169600, grad_norm=8.952693939208984, loss=0.8467379808425903
I0128 23:49:43.075448 139866163582720 logging_writer.py:48] [169700] global_step=169700, grad_norm=10.093282699584961, loss=0.8373688459396362
I0128 23:50:17.145445 139865769342720 logging_writer.py:48] [169800] global_step=169800, grad_norm=7.7012858390808105, loss=0.7353009581565857
I0128 23:50:50.991150 139866163582720 logging_writer.py:48] [169900] global_step=169900, grad_norm=9.479924201965332, loss=0.8940581679344177
I0128 23:51:24.840105 139865769342720 logging_writer.py:48] [170000] global_step=170000, grad_norm=8.493184089660645, loss=0.8415154218673706
I0128 23:51:58.676150 139866163582720 logging_writer.py:48] [170100] global_step=170100, grad_norm=9.761829376220703, loss=0.772244930267334
I0128 23:52:27.598153 140027215431488 spec.py:321] Evaluating on the training split.
I0128 23:52:33.834856 140027215431488 spec.py:333] Evaluating on the validation split.
I0128 23:52:42.462891 140027215431488 spec.py:349] Evaluating on the test split.
I0128 23:52:45.049067 140027215431488 submission_runner.py:408] Time since start: 59703.76s, 	Step: 170187, 	{'train/accuracy': 0.9106544852256775, 'train/loss': 0.3087378144264221, 'validation/accuracy': 0.7574599981307983, 'validation/loss': 0.9799865484237671, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.6687065362930298, 'test/num_examples': 10000, 'score': 57675.29413366318, 'total_duration': 59703.755058288574, 'accumulated_submission_time': 57675.29413366318, 'accumulated_eval_time': 2017.519765138626, 'accumulated_logging_time': 4.954912900924683}
I0128 23:52:45.099884 139865240893184 logging_writer.py:48] [170187] accumulated_eval_time=2017.519765, accumulated_logging_time=4.954913, accumulated_submission_time=57675.294134, global_step=170187, preemption_count=0, score=57675.294134, test/accuracy=0.633800, test/loss=1.668707, test/num_examples=10000, total_duration=59703.755058, train/accuracy=0.910654, train/loss=0.308738, validation/accuracy=0.757460, validation/loss=0.979987, validation/num_examples=50000
I0128 23:52:49.833498 139865760950016 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.371581077575684, loss=0.796271800994873
I0128 23:53:23.648275 139865240893184 logging_writer.py:48] [170300] global_step=170300, grad_norm=9.856999397277832, loss=0.8392453193664551
I0128 23:53:57.470527 139865760950016 logging_writer.py:48] [170400] global_step=170400, grad_norm=9.112533569335938, loss=0.7719615697860718
I0128 23:54:31.289911 139865240893184 logging_writer.py:48] [170500] global_step=170500, grad_norm=9.184377670288086, loss=0.777157187461853
I0128 23:55:05.142481 139865760950016 logging_writer.py:48] [170600] global_step=170600, grad_norm=9.725001335144043, loss=0.8902588486671448
I0128 23:55:38.993333 139865240893184 logging_writer.py:48] [170700] global_step=170700, grad_norm=8.953536987304688, loss=0.7410813570022583
I0128 23:56:12.838715 139865760950016 logging_writer.py:48] [170800] global_step=170800, grad_norm=9.315862655639648, loss=0.767452597618103
I0128 23:56:46.827416 139865240893184 logging_writer.py:48] [170900] global_step=170900, grad_norm=9.185582160949707, loss=0.7591304779052734
I0128 23:57:20.631701 139865760950016 logging_writer.py:48] [171000] global_step=171000, grad_norm=9.197354316711426, loss=0.7172853350639343
I0128 23:57:54.502875 139865240893184 logging_writer.py:48] [171100] global_step=171100, grad_norm=9.521381378173828, loss=0.8279507756233215
I0128 23:58:28.296409 139865760950016 logging_writer.py:48] [171200] global_step=171200, grad_norm=9.272112846374512, loss=0.7308589816093445
I0128 23:59:02.156593 139865240893184 logging_writer.py:48] [171300] global_step=171300, grad_norm=8.657753944396973, loss=0.8223077058792114
I0128 23:59:36.000988 139865760950016 logging_writer.py:48] [171400] global_step=171400, grad_norm=8.78705883026123, loss=0.7009286880493164
I0129 00:00:09.837374 139865240893184 logging_writer.py:48] [171500] global_step=171500, grad_norm=8.47010612487793, loss=0.7648642659187317
I0129 00:00:43.694577 139865760950016 logging_writer.py:48] [171600] global_step=171600, grad_norm=8.377723693847656, loss=0.7464312314987183
I0129 00:01:15.314467 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:01:21.540487 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:01:30.149998 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:01:32.886970 140027215431488 submission_runner.py:408] Time since start: 60231.59s, 	Step: 171695, 	{'train/accuracy': 0.924226701259613, 'train/loss': 0.2707524597644806, 'validation/accuracy': 0.757099986076355, 'validation/loss': 0.9797187447547913, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.6713703870773315, 'test/num_examples': 10000, 'score': 58185.4462184906, 'total_duration': 60231.59297156334, 'accumulated_submission_time': 58185.4462184906, 'accumulated_eval_time': 2035.09224319458, 'accumulated_logging_time': 5.01508355140686}
I0129 00:01:32.926464 139865224107776 logging_writer.py:48] [171695] accumulated_eval_time=2035.092243, accumulated_logging_time=5.015084, accumulated_submission_time=58185.446218, global_step=171695, preemption_count=0, score=58185.446218, test/accuracy=0.638300, test/loss=1.671370, test/num_examples=10000, total_duration=60231.592972, train/accuracy=0.924227, train/loss=0.270752, validation/accuracy=0.757100, validation/loss=0.979719, validation/num_examples=50000
I0129 00:01:34.951129 139865232500480 logging_writer.py:48] [171700] global_step=171700, grad_norm=9.54483413696289, loss=0.8691950440406799
I0129 00:02:08.760793 139865224107776 logging_writer.py:48] [171800] global_step=171800, grad_norm=8.847728729248047, loss=0.7446587681770325
I0129 00:02:42.764083 139865232500480 logging_writer.py:48] [171900] global_step=171900, grad_norm=9.054957389831543, loss=0.776504635810852
I0129 00:03:16.584935 139865224107776 logging_writer.py:48] [172000] global_step=172000, grad_norm=9.095484733581543, loss=0.7647770047187805
I0129 00:03:50.396248 139865232500480 logging_writer.py:48] [172100] global_step=172100, grad_norm=9.215188980102539, loss=0.826827347278595
I0129 00:04:24.255013 139865224107776 logging_writer.py:48] [172200] global_step=172200, grad_norm=9.731484413146973, loss=0.8254429697990417
I0129 00:04:58.077473 139865232500480 logging_writer.py:48] [172300] global_step=172300, grad_norm=8.558599472045898, loss=0.7445368766784668
I0129 00:05:31.928031 139865224107776 logging_writer.py:48] [172400] global_step=172400, grad_norm=8.39907169342041, loss=0.746286928653717
I0129 00:06:05.759079 139865232500480 logging_writer.py:48] [172500] global_step=172500, grad_norm=9.89528751373291, loss=0.8028975129127502
I0129 00:06:39.584908 139865224107776 logging_writer.py:48] [172600] global_step=172600, grad_norm=9.327244758605957, loss=0.7470872402191162
I0129 00:07:13.441373 139865232500480 logging_writer.py:48] [172700] global_step=172700, grad_norm=9.282231330871582, loss=0.7740225791931152
I0129 00:07:47.294437 139865224107776 logging_writer.py:48] [172800] global_step=172800, grad_norm=10.086448669433594, loss=0.7353618741035461
I0129 00:08:21.098374 139865232500480 logging_writer.py:48] [172900] global_step=172900, grad_norm=10.435881614685059, loss=0.8186031579971313
I0129 00:08:54.970672 139865224107776 logging_writer.py:48] [173000] global_step=173000, grad_norm=8.729207992553711, loss=0.7310966849327087
I0129 00:09:28.915789 139865232500480 logging_writer.py:48] [173100] global_step=173100, grad_norm=9.08029842376709, loss=0.8296714425086975
I0129 00:10:02.755640 139865224107776 logging_writer.py:48] [173200] global_step=173200, grad_norm=9.989049911499023, loss=0.7846443057060242
I0129 00:10:02.906127 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:10:09.217416 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:10:17.883706 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:10:20.461201 140027215431488 submission_runner.py:408] Time since start: 60759.17s, 	Step: 173202, 	{'train/accuracy': 0.9278340339660645, 'train/loss': 0.26252445578575134, 'validation/accuracy': 0.758899986743927, 'validation/loss': 0.9757165312767029, 'validation/num_examples': 50000, 'test/accuracy': 0.6396000385284424, 'test/loss': 1.6715961694717407, 'test/num_examples': 10000, 'score': 58695.36663389206, 'total_duration': 60759.16719126701, 'accumulated_submission_time': 58695.36663389206, 'accumulated_eval_time': 2052.647294998169, 'accumulated_logging_time': 5.063016414642334}
I0129 00:10:20.507487 139866171975424 logging_writer.py:48] [173202] accumulated_eval_time=2052.647295, accumulated_logging_time=5.063016, accumulated_submission_time=58695.366634, global_step=173202, preemption_count=0, score=58695.366634, test/accuracy=0.639600, test/loss=1.671596, test/num_examples=10000, total_duration=60759.167191, train/accuracy=0.927834, train/loss=0.262524, validation/accuracy=0.758900, validation/loss=0.975717, validation/num_examples=50000
I0129 00:10:53.981607 139866180368128 logging_writer.py:48] [173300] global_step=173300, grad_norm=8.854166030883789, loss=0.8203431963920593
I0129 00:11:27.788371 139866171975424 logging_writer.py:48] [173400] global_step=173400, grad_norm=9.611990928649902, loss=0.6872867345809937
I0129 00:12:01.609919 139866180368128 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.691308975219727, loss=0.7672543525695801
I0129 00:12:35.438093 139866171975424 logging_writer.py:48] [173600] global_step=173600, grad_norm=9.394769668579102, loss=0.8047080636024475
I0129 00:13:09.308369 139866180368128 logging_writer.py:48] [173700] global_step=173700, grad_norm=10.401324272155762, loss=0.8028625249862671
I0129 00:13:43.162729 139866171975424 logging_writer.py:48] [173800] global_step=173800, grad_norm=9.925180435180664, loss=0.8395438194274902
I0129 00:14:16.996634 139866180368128 logging_writer.py:48] [173900] global_step=173900, grad_norm=9.3811674118042, loss=0.8204537034034729
I0129 00:14:50.813484 139866171975424 logging_writer.py:48] [174000] global_step=174000, grad_norm=10.064152717590332, loss=0.7895525693893433
I0129 00:15:24.746273 139866180368128 logging_writer.py:48] [174100] global_step=174100, grad_norm=9.07109260559082, loss=0.7926024198532104
I0129 00:15:58.587127 139866171975424 logging_writer.py:48] [174200] global_step=174200, grad_norm=9.859471321105957, loss=0.7862271070480347
I0129 00:16:32.431061 139866180368128 logging_writer.py:48] [174300] global_step=174300, grad_norm=8.91720962524414, loss=0.7823803424835205
I0129 00:17:06.264521 139866171975424 logging_writer.py:48] [174400] global_step=174400, grad_norm=10.126909255981445, loss=0.7695626616477966
I0129 00:17:40.080190 139866180368128 logging_writer.py:48] [174500] global_step=174500, grad_norm=8.61559772491455, loss=0.8078199625015259
I0129 00:18:13.960288 139866171975424 logging_writer.py:48] [174600] global_step=174600, grad_norm=9.049288749694824, loss=0.8506655097007751
I0129 00:18:47.817922 139866180368128 logging_writer.py:48] [174700] global_step=174700, grad_norm=9.075789451599121, loss=0.8227005004882812
I0129 00:18:50.661403 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:18:56.925648 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:19:05.565933 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:19:08.157122 140027215431488 submission_runner.py:408] Time since start: 61286.86s, 	Step: 174710, 	{'train/accuracy': 0.9278539419174194, 'train/loss': 0.2555326819419861, 'validation/accuracy': 0.7615399956703186, 'validation/loss': 0.9703835844993591, 'validation/num_examples': 50000, 'test/accuracy': 0.6392000317573547, 'test/loss': 1.6742520332336426, 'test/num_examples': 10000, 'score': 59205.45822405815, 'total_duration': 61286.8631067276, 'accumulated_submission_time': 59205.45822405815, 'accumulated_eval_time': 2070.1429677009583, 'accumulated_logging_time': 5.118837833404541}
I0129 00:19:08.204382 139865240893184 logging_writer.py:48] [174710] accumulated_eval_time=2070.142968, accumulated_logging_time=5.118838, accumulated_submission_time=59205.458224, global_step=174710, preemption_count=0, score=59205.458224, test/accuracy=0.639200, test/loss=1.674252, test/num_examples=10000, total_duration=61286.863107, train/accuracy=0.927854, train/loss=0.255533, validation/accuracy=0.761540, validation/loss=0.970384, validation/num_examples=50000
I0129 00:19:39.013227 139865760950016 logging_writer.py:48] [174800] global_step=174800, grad_norm=9.366888046264648, loss=0.745271623134613
I0129 00:20:12.823184 139865240893184 logging_writer.py:48] [174900] global_step=174900, grad_norm=9.064383506774902, loss=0.7776153087615967
I0129 00:20:46.673727 139865760950016 logging_writer.py:48] [175000] global_step=175000, grad_norm=8.548725128173828, loss=0.7903555631637573
I0129 00:21:20.532222 139865240893184 logging_writer.py:48] [175100] global_step=175100, grad_norm=9.889936447143555, loss=0.7835204601287842
I0129 00:21:54.476059 139865760950016 logging_writer.py:48] [175200] global_step=175200, grad_norm=9.544049263000488, loss=0.785355806350708
I0129 00:22:28.287972 139865240893184 logging_writer.py:48] [175300] global_step=175300, grad_norm=9.813368797302246, loss=0.8378628492355347
I0129 00:23:02.145526 139865760950016 logging_writer.py:48] [175400] global_step=175400, grad_norm=9.621997833251953, loss=0.7031869888305664
I0129 00:23:35.984554 139865240893184 logging_writer.py:48] [175500] global_step=175500, grad_norm=9.349079132080078, loss=0.8423136472702026
I0129 00:24:09.805519 139865760950016 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.680082321166992, loss=0.7396261692047119
I0129 00:24:43.667269 139865240893184 logging_writer.py:48] [175700] global_step=175700, grad_norm=8.955927848815918, loss=0.7982541918754578
I0129 00:25:17.470283 139865760950016 logging_writer.py:48] [175800] global_step=175800, grad_norm=9.193892478942871, loss=0.7644134759902954
I0129 00:25:51.331732 139865240893184 logging_writer.py:48] [175900] global_step=175900, grad_norm=9.61790657043457, loss=0.8624522089958191
I0129 00:26:25.117385 139865760950016 logging_writer.py:48] [176000] global_step=176000, grad_norm=9.666374206542969, loss=0.7938886880874634
I0129 00:26:58.958281 139865240893184 logging_writer.py:48] [176100] global_step=176100, grad_norm=8.290899276733398, loss=0.7240000367164612
I0129 00:27:32.814580 139865760950016 logging_writer.py:48] [176200] global_step=176200, grad_norm=9.638470649719238, loss=0.8055585622787476
I0129 00:27:38.379041 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:27:44.554495 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:27:53.015756 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:27:55.597065 140027215431488 submission_runner.py:408] Time since start: 61814.30s, 	Step: 176218, 	{'train/accuracy': 0.9264189600944519, 'train/loss': 0.2620689570903778, 'validation/accuracy': 0.7624199986457825, 'validation/loss': 0.9670127034187317, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.6667834520339966, 'test/num_examples': 10000, 'score': 59715.56953692436, 'total_duration': 61814.30304598808, 'accumulated_submission_time': 59715.56953692436, 'accumulated_eval_time': 2087.3609421253204, 'accumulated_logging_time': 5.175340414047241}
I0129 00:27:55.641262 139865224107776 logging_writer.py:48] [176218] accumulated_eval_time=2087.360942, accumulated_logging_time=5.175340, accumulated_submission_time=59715.569537, global_step=176218, preemption_count=0, score=59715.569537, test/accuracy=0.640200, test/loss=1.666783, test/num_examples=10000, total_duration=61814.303046, train/accuracy=0.926419, train/loss=0.262069, validation/accuracy=0.762420, validation/loss=0.967013, validation/num_examples=50000
I0129 00:28:23.781304 139866163582720 logging_writer.py:48] [176300] global_step=176300, grad_norm=9.706517219543457, loss=0.7887592315673828
I0129 00:28:57.588123 139865224107776 logging_writer.py:48] [176400] global_step=176400, grad_norm=9.371302604675293, loss=0.8009281158447266
I0129 00:29:31.418354 139866163582720 logging_writer.py:48] [176500] global_step=176500, grad_norm=9.37679386138916, loss=0.7876346111297607
I0129 00:30:05.270807 139865224107776 logging_writer.py:48] [176600] global_step=176600, grad_norm=10.1292085647583, loss=0.7197939157485962
I0129 00:30:39.126452 139866163582720 logging_writer.py:48] [176700] global_step=176700, grad_norm=9.140055656433105, loss=0.8205260634422302
I0129 00:31:12.970138 139865224107776 logging_writer.py:48] [176800] global_step=176800, grad_norm=10.281441688537598, loss=0.7812769412994385
I0129 00:31:46.819174 139866163582720 logging_writer.py:48] [176900] global_step=176900, grad_norm=10.253676414489746, loss=0.7865869402885437
I0129 00:32:20.659272 139865224107776 logging_writer.py:48] [177000] global_step=177000, grad_norm=10.034852981567383, loss=0.7964683175086975
I0129 00:32:54.480718 139866163582720 logging_writer.py:48] [177100] global_step=177100, grad_norm=9.255141258239746, loss=0.8384553790092468
I0129 00:33:28.301804 139865224107776 logging_writer.py:48] [177200] global_step=177200, grad_norm=8.993339538574219, loss=0.7158235311508179
I0129 00:34:02.120054 139866163582720 logging_writer.py:48] [177300] global_step=177300, grad_norm=10.01392936706543, loss=0.7475413084030151
I0129 00:34:36.077156 139865224107776 logging_writer.py:48] [177400] global_step=177400, grad_norm=8.337831497192383, loss=0.7063390612602234
I0129 00:35:09.929392 139866163582720 logging_writer.py:48] [177500] global_step=177500, grad_norm=10.47398567199707, loss=0.8109001517295837
I0129 00:35:43.772118 139865224107776 logging_writer.py:48] [177600] global_step=177600, grad_norm=9.311040878295898, loss=0.7104383707046509
I0129 00:36:17.601612 139866163582720 logging_writer.py:48] [177700] global_step=177700, grad_norm=10.06475830078125, loss=0.7667352557182312
I0129 00:36:25.885664 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:36:32.177157 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:36:41.052085 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:36:43.671760 140027215431488 submission_runner.py:408] Time since start: 62342.38s, 	Step: 177726, 	{'train/accuracy': 0.927156388759613, 'train/loss': 0.25953570008277893, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 0.9643649458885193, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.6655884981155396, 'test/num_examples': 10000, 'score': 60225.75075531006, 'total_duration': 62342.37773346901, 'accumulated_submission_time': 60225.75075531006, 'accumulated_eval_time': 2105.146994113922, 'accumulated_logging_time': 5.229976654052734}
I0129 00:36:43.723670 139865232500480 logging_writer.py:48] [177726] accumulated_eval_time=2105.146994, accumulated_logging_time=5.229977, accumulated_submission_time=60225.750755, global_step=177726, preemption_count=0, score=60225.750755, test/accuracy=0.639900, test/loss=1.665588, test/num_examples=10000, total_duration=62342.377733, train/accuracy=0.927156, train/loss=0.259536, validation/accuracy=0.762580, validation/loss=0.964365, validation/num_examples=50000
I0129 00:37:09.087853 139865240893184 logging_writer.py:48] [177800] global_step=177800, grad_norm=8.89057731628418, loss=0.7921909093856812
I0129 00:37:42.887240 139865232500480 logging_writer.py:48] [177900] global_step=177900, grad_norm=9.470848083496094, loss=0.8306916356086731
I0129 00:38:16.755689 139865240893184 logging_writer.py:48] [178000] global_step=178000, grad_norm=9.75574016571045, loss=0.7569425106048584
I0129 00:38:50.595954 139865232500480 logging_writer.py:48] [178100] global_step=178100, grad_norm=9.075533866882324, loss=0.7561671137809753
I0129 00:39:24.442212 139865240893184 logging_writer.py:48] [178200] global_step=178200, grad_norm=9.263660430908203, loss=0.7423679828643799
I0129 00:39:58.282005 139865232500480 logging_writer.py:48] [178300] global_step=178300, grad_norm=9.373937606811523, loss=0.7601569890975952
I0129 00:40:32.148868 139865240893184 logging_writer.py:48] [178400] global_step=178400, grad_norm=9.003463745117188, loss=0.7115294933319092
I0129 00:41:06.068234 139865232500480 logging_writer.py:48] [178500] global_step=178500, grad_norm=9.16910171508789, loss=0.7353979349136353
I0129 00:41:39.959507 139865240893184 logging_writer.py:48] [178600] global_step=178600, grad_norm=9.310588836669922, loss=0.8582133054733276
I0129 00:42:13.794542 139865232500480 logging_writer.py:48] [178700] global_step=178700, grad_norm=9.106974601745605, loss=0.7552422285079956
I0129 00:42:47.621872 139865240893184 logging_writer.py:48] [178800] global_step=178800, grad_norm=10.328778266906738, loss=0.7558171153068542
I0129 00:43:21.441781 139865232500480 logging_writer.py:48] [178900] global_step=178900, grad_norm=9.72590160369873, loss=0.7696838974952698
I0129 00:43:55.265586 139865240893184 logging_writer.py:48] [179000] global_step=179000, grad_norm=8.261819839477539, loss=0.7432406544685364
I0129 00:44:29.110510 139865232500480 logging_writer.py:48] [179100] global_step=179100, grad_norm=9.593957901000977, loss=0.8052176237106323
I0129 00:45:02.946637 139865240893184 logging_writer.py:48] [179200] global_step=179200, grad_norm=9.086088180541992, loss=0.7114354372024536
I0129 00:45:13.958442 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:45:20.136500 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:45:28.938923 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:45:31.523580 140027215431488 submission_runner.py:408] Time since start: 62870.23s, 	Step: 179234, 	{'train/accuracy': 0.9292888641357422, 'train/loss': 0.2532636523246765, 'validation/accuracy': 0.7625399827957153, 'validation/loss': 0.9626427888870239, 'validation/num_examples': 50000, 'test/accuracy': 0.6411000490188599, 'test/loss': 1.6631746292114258, 'test/num_examples': 10000, 'score': 60735.92390227318, 'total_duration': 62870.22953939438, 'accumulated_submission_time': 60735.92390227318, 'accumulated_eval_time': 2122.7120769023895, 'accumulated_logging_time': 5.291700601577759}
I0129 00:45:31.579788 139866171975424 logging_writer.py:48] [179234] accumulated_eval_time=2122.712077, accumulated_logging_time=5.291701, accumulated_submission_time=60735.923902, global_step=179234, preemption_count=0, score=60735.923902, test/accuracy=0.641100, test/loss=1.663175, test/num_examples=10000, total_duration=62870.229539, train/accuracy=0.929289, train/loss=0.253264, validation/accuracy=0.762540, validation/loss=0.962643, validation/num_examples=50000
I0129 00:45:54.241272 139866180368128 logging_writer.py:48] [179300] global_step=179300, grad_norm=9.29757022857666, loss=0.7393746972084045
I0129 00:46:28.071882 139866171975424 logging_writer.py:48] [179400] global_step=179400, grad_norm=8.921832084655762, loss=0.711304783821106
I0129 00:47:01.887462 139866180368128 logging_writer.py:48] [179500] global_step=179500, grad_norm=9.126083374023438, loss=0.7566083669662476
I0129 00:47:35.856696 139866171975424 logging_writer.py:48] [179600] global_step=179600, grad_norm=8.977775573730469, loss=0.7159683108329773
I0129 00:48:09.698040 139866180368128 logging_writer.py:48] [179700] global_step=179700, grad_norm=8.123849868774414, loss=0.7000072002410889
I0129 00:48:43.508175 139866171975424 logging_writer.py:48] [179800] global_step=179800, grad_norm=9.17586612701416, loss=0.7199035882949829
I0129 00:49:17.384285 139866180368128 logging_writer.py:48] [179900] global_step=179900, grad_norm=8.641133308410645, loss=0.690488874912262
I0129 00:49:51.258728 139866171975424 logging_writer.py:48] [180000] global_step=180000, grad_norm=8.731986999511719, loss=0.6965073943138123
I0129 00:50:25.094702 139866180368128 logging_writer.py:48] [180100] global_step=180100, grad_norm=10.52234935760498, loss=0.7849076986312866
I0129 00:50:58.921050 139866171975424 logging_writer.py:48] [180200] global_step=180200, grad_norm=9.596565246582031, loss=0.6941108703613281
I0129 00:51:32.773372 139866180368128 logging_writer.py:48] [180300] global_step=180300, grad_norm=9.186325073242188, loss=0.7276897430419922
I0129 00:52:06.560367 139866171975424 logging_writer.py:48] [180400] global_step=180400, grad_norm=9.392251968383789, loss=0.7778475284576416
I0129 00:52:40.426412 139866180368128 logging_writer.py:48] [180500] global_step=180500, grad_norm=9.686813354492188, loss=0.7368492484092712
I0129 00:53:14.265083 139866171975424 logging_writer.py:48] [180600] global_step=180600, grad_norm=10.348261833190918, loss=0.7079735994338989
I0129 00:53:48.163693 139866180368128 logging_writer.py:48] [180700] global_step=180700, grad_norm=10.419376373291016, loss=0.7901297211647034
I0129 00:54:01.826300 140027215431488 spec.py:321] Evaluating on the training split.
I0129 00:54:07.967424 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 00:54:16.889739 140027215431488 spec.py:349] Evaluating on the test split.
I0129 00:54:19.465599 140027215431488 submission_runner.py:408] Time since start: 63398.17s, 	Step: 180742, 	{'train/accuracy': 0.9323381781578064, 'train/loss': 0.2413637936115265, 'validation/accuracy': 0.763480007648468, 'validation/loss': 0.9599735736846924, 'validation/num_examples': 50000, 'test/accuracy': 0.6431000232696533, 'test/loss': 1.6595665216445923, 'test/num_examples': 10000, 'score': 61246.10619521141, 'total_duration': 63398.17158651352, 'accumulated_submission_time': 61246.10619521141, 'accumulated_eval_time': 2140.3513338565826, 'accumulated_logging_time': 5.3609983921051025}
I0129 00:54:19.514612 139865760950016 logging_writer.py:48] [180742] accumulated_eval_time=2140.351334, accumulated_logging_time=5.360998, accumulated_submission_time=61246.106195, global_step=180742, preemption_count=0, score=61246.106195, test/accuracy=0.643100, test/loss=1.659567, test/num_examples=10000, total_duration=63398.171587, train/accuracy=0.932338, train/loss=0.241364, validation/accuracy=0.763480, validation/loss=0.959974, validation/num_examples=50000
I0129 00:54:39.471794 139865769342720 logging_writer.py:48] [180800] global_step=180800, grad_norm=9.593069076538086, loss=0.7570319771766663
I0129 00:55:13.328289 139865760950016 logging_writer.py:48] [180900] global_step=180900, grad_norm=9.66425609588623, loss=0.7563938498497009
I0129 00:55:47.172358 139865769342720 logging_writer.py:48] [181000] global_step=181000, grad_norm=9.246563911437988, loss=0.7217956781387329
I0129 00:56:21.029328 139865760950016 logging_writer.py:48] [181100] global_step=181100, grad_norm=9.737207412719727, loss=0.8004010915756226
I0129 00:56:54.907623 139865769342720 logging_writer.py:48] [181200] global_step=181200, grad_norm=8.600393295288086, loss=0.7344937920570374
I0129 00:57:28.767899 139865760950016 logging_writer.py:48] [181300] global_step=181300, grad_norm=8.744046211242676, loss=0.6636404991149902
I0129 00:58:02.636956 139865769342720 logging_writer.py:48] [181400] global_step=181400, grad_norm=8.919175148010254, loss=0.7773916125297546
I0129 00:58:36.499427 139865760950016 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.330707550048828, loss=0.6632562875747681
I0129 00:59:10.348621 139865769342720 logging_writer.py:48] [181600] global_step=181600, grad_norm=10.105835914611816, loss=0.7571882009506226
I0129 00:59:44.190250 139865760950016 logging_writer.py:48] [181700] global_step=181700, grad_norm=8.908929824829102, loss=0.7457718849182129
I0129 01:00:18.117442 139865769342720 logging_writer.py:48] [181800] global_step=181800, grad_norm=9.56629467010498, loss=0.8058377504348755
I0129 01:00:51.958927 139865760950016 logging_writer.py:48] [181900] global_step=181900, grad_norm=8.955477714538574, loss=0.7698382139205933
I0129 01:01:25.785162 139865769342720 logging_writer.py:48] [182000] global_step=182000, grad_norm=9.090497970581055, loss=0.742519736289978
I0129 01:01:59.627161 139865760950016 logging_writer.py:48] [182100] global_step=182100, grad_norm=9.354804039001465, loss=0.7456885576248169
I0129 01:02:33.471407 139865769342720 logging_writer.py:48] [182200] global_step=182200, grad_norm=9.361648559570312, loss=0.7770351767539978
I0129 01:02:49.530514 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:02:55.686666 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:03:04.555155 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:03:07.130874 140027215431488 submission_runner.py:408] Time since start: 63925.84s, 	Step: 182249, 	{'train/accuracy': 0.9336535334587097, 'train/loss': 0.2402791529893875, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 0.9584994316101074, 'validation/num_examples': 50000, 'test/accuracy': 0.6432000398635864, 'test/loss': 1.664105772972107, 'test/num_examples': 10000, 'score': 61756.06179380417, 'total_duration': 63925.83683180809, 'accumulated_submission_time': 61756.06179380417, 'accumulated_eval_time': 2157.9516232013702, 'accumulated_logging_time': 5.419076681137085}
I0129 01:03:07.183005 139866171975424 logging_writer.py:48] [182249] accumulated_eval_time=2157.951623, accumulated_logging_time=5.419077, accumulated_submission_time=61756.061794, global_step=182249, preemption_count=0, score=61756.061794, test/accuracy=0.643200, test/loss=1.664106, test/num_examples=10000, total_duration=63925.836832, train/accuracy=0.933654, train/loss=0.240279, validation/accuracy=0.763560, validation/loss=0.958499, validation/num_examples=50000
I0129 01:03:24.762654 139866180368128 logging_writer.py:48] [182300] global_step=182300, grad_norm=8.876930236816406, loss=0.738982617855072
I0129 01:03:58.610745 139866171975424 logging_writer.py:48] [182400] global_step=182400, grad_norm=10.202564239501953, loss=0.7616934180259705
I0129 01:04:32.436746 139866180368128 logging_writer.py:48] [182500] global_step=182500, grad_norm=8.57305908203125, loss=0.71323561668396
I0129 01:05:06.290580 139866171975424 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.452940940856934, loss=0.7401094436645508
I0129 01:05:40.122453 139866180368128 logging_writer.py:48] [182700] global_step=182700, grad_norm=10.234291076660156, loss=0.7666406631469727
I0129 01:06:14.047166 139866171975424 logging_writer.py:48] [182800] global_step=182800, grad_norm=10.032936096191406, loss=0.747433066368103
I0129 01:06:47.898369 139866180368128 logging_writer.py:48] [182900] global_step=182900, grad_norm=10.340071678161621, loss=0.7729756236076355
I0129 01:07:21.746433 139866171975424 logging_writer.py:48] [183000] global_step=183000, grad_norm=9.866048812866211, loss=0.7595627307891846
I0129 01:07:55.570911 139866180368128 logging_writer.py:48] [183100] global_step=183100, grad_norm=9.806910514831543, loss=0.7634347677230835
I0129 01:08:29.395269 139866171975424 logging_writer.py:48] [183200] global_step=183200, grad_norm=8.674625396728516, loss=0.7099077701568604
I0129 01:09:03.237566 139866180368128 logging_writer.py:48] [183300] global_step=183300, grad_norm=8.932649612426758, loss=0.8252754807472229
I0129 01:09:37.062269 139866171975424 logging_writer.py:48] [183400] global_step=183400, grad_norm=9.24728775024414, loss=0.7588610053062439
I0129 01:10:10.936196 139866180368128 logging_writer.py:48] [183500] global_step=183500, grad_norm=9.279394149780273, loss=0.7591660022735596
I0129 01:10:44.798243 139866171975424 logging_writer.py:48] [183600] global_step=183600, grad_norm=9.930377006530762, loss=0.8403344750404358
I0129 01:11:18.628398 139866180368128 logging_writer.py:48] [183700] global_step=183700, grad_norm=8.618325233459473, loss=0.6544539332389832
I0129 01:11:37.392217 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:11:43.584462 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:11:52.480976 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:11:55.064440 140027215431488 submission_runner.py:408] Time since start: 64453.77s, 	Step: 183757, 	{'train/accuracy': 0.935566782951355, 'train/loss': 0.24063031375408173, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 0.9566633105278015, 'validation/num_examples': 50000, 'test/accuracy': 0.6440000534057617, 'test/loss': 1.6585973501205444, 'test/num_examples': 10000, 'score': 62266.209573984146, 'total_duration': 64453.770429611206, 'accumulated_submission_time': 62266.209573984146, 'accumulated_eval_time': 2175.6238107681274, 'accumulated_logging_time': 5.481184005737305}
I0129 01:11:55.117519 139866163582720 logging_writer.py:48] [183757] accumulated_eval_time=2175.623811, accumulated_logging_time=5.481184, accumulated_submission_time=62266.209574, global_step=183757, preemption_count=0, score=62266.209574, test/accuracy=0.644000, test/loss=1.658597, test/num_examples=10000, total_duration=64453.770430, train/accuracy=0.935567, train/loss=0.240630, validation/accuracy=0.764700, validation/loss=0.956663, validation/num_examples=50000
I0129 01:12:10.002048 139866188760832 logging_writer.py:48] [183800] global_step=183800, grad_norm=9.416919708251953, loss=0.7346706390380859
I0129 01:12:43.860822 139866163582720 logging_writer.py:48] [183900] global_step=183900, grad_norm=9.040457725524902, loss=0.7858412265777588
I0129 01:13:17.708383 139866188760832 logging_writer.py:48] [184000] global_step=184000, grad_norm=9.886034965515137, loss=0.7905357480049133
I0129 01:13:51.544106 139866163582720 logging_writer.py:48] [184100] global_step=184100, grad_norm=8.774415016174316, loss=0.7772209048271179
I0129 01:14:25.378126 139866188760832 logging_writer.py:48] [184200] global_step=184200, grad_norm=9.388259887695312, loss=0.7800962924957275
I0129 01:14:59.210597 139866163582720 logging_writer.py:48] [184300] global_step=184300, grad_norm=8.993066787719727, loss=0.7101660370826721
I0129 01:15:33.026550 139866188760832 logging_writer.py:48] [184400] global_step=184400, grad_norm=8.690764427185059, loss=0.7560520768165588
I0129 01:16:06.849975 139866163582720 logging_writer.py:48] [184500] global_step=184500, grad_norm=9.309555053710938, loss=0.8104429244995117
I0129 01:16:40.656846 139866188760832 logging_writer.py:48] [184600] global_step=184600, grad_norm=8.490862846374512, loss=0.7082759737968445
I0129 01:17:14.526295 139866163582720 logging_writer.py:48] [184700] global_step=184700, grad_norm=9.124921798706055, loss=0.7410099506378174
I0129 01:17:48.409522 139866188760832 logging_writer.py:48] [184800] global_step=184800, grad_norm=10.091350555419922, loss=0.6976105570793152
I0129 01:18:22.245528 139866163582720 logging_writer.py:48] [184900] global_step=184900, grad_norm=9.10731315612793, loss=0.744875431060791
I0129 01:18:56.176759 139866188760832 logging_writer.py:48] [185000] global_step=185000, grad_norm=9.335831642150879, loss=0.7238143086433411
I0129 01:19:30.037683 139866163582720 logging_writer.py:48] [185100] global_step=185100, grad_norm=9.469693183898926, loss=0.7665613293647766
I0129 01:20:03.882440 139866188760832 logging_writer.py:48] [185200] global_step=185200, grad_norm=10.078446388244629, loss=0.7091473937034607
I0129 01:20:25.336884 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:20:31.473173 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:20:40.154657 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:20:42.682314 140027215431488 submission_runner.py:408] Time since start: 64981.39s, 	Step: 185265, 	{'train/accuracy': 0.9326968789100647, 'train/loss': 0.24263811111450195, 'validation/accuracy': 0.7636399865150452, 'validation/loss': 0.9573678374290466, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.6598241329193115, 'test/num_examples': 10000, 'score': 62776.36577916145, 'total_duration': 64981.38829827309, 'accumulated_submission_time': 62776.36577916145, 'accumulated_eval_time': 2192.9691956043243, 'accumulated_logging_time': 5.543259382247925}
I0129 01:20:42.733380 139865232500480 logging_writer.py:48] [185265] accumulated_eval_time=2192.969196, accumulated_logging_time=5.543259, accumulated_submission_time=62776.365779, global_step=185265, preemption_count=0, score=62776.365779, test/accuracy=0.643400, test/loss=1.659824, test/num_examples=10000, total_duration=64981.388298, train/accuracy=0.932697, train/loss=0.242638, validation/accuracy=0.763640, validation/loss=0.957368, validation/num_examples=50000
I0129 01:20:54.907416 139865240893184 logging_writer.py:48] [185300] global_step=185300, grad_norm=9.85788345336914, loss=0.6966859102249146
I0129 01:21:28.766130 139865232500480 logging_writer.py:48] [185400] global_step=185400, grad_norm=9.26400089263916, loss=0.7182425260543823
I0129 01:22:02.598141 139865240893184 logging_writer.py:48] [185500] global_step=185500, grad_norm=9.430654525756836, loss=0.7109719514846802
I0129 01:22:36.458350 139865232500480 logging_writer.py:48] [185600] global_step=185600, grad_norm=9.443063735961914, loss=0.7991711497306824
I0129 01:23:10.309480 139865240893184 logging_writer.py:48] [185700] global_step=185700, grad_norm=8.747756958007812, loss=0.7139369249343872
I0129 01:23:44.145311 139865232500480 logging_writer.py:48] [185800] global_step=185800, grad_norm=9.126130104064941, loss=0.7208414077758789
I0129 01:24:17.983926 139865240893184 logging_writer.py:48] [185900] global_step=185900, grad_norm=9.119729995727539, loss=0.7582873106002808
I0129 01:24:34.718802 139865232500480 logging_writer.py:48] [185951] global_step=185951, preemption_count=0, score=63008.286588
I0129 01:24:35.165849 140027215431488 checkpoints.py:490] Saving checkpoint at step: 185951
I0129 01:24:36.220294 140027215431488 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3/checkpoint_185951
I0129 01:24:36.244659 140027215431488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_3/checkpoint_185951.
I0129 01:24:36.977660 140027215431488 submission_runner.py:583] Tuning trial 3/5
I0129 01:24:36.977862 140027215431488 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0129 01:24:36.994686 140027215431488 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006776147638447583, 'train/loss': 6.912829399108887, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 36.148282289505005, 'total_duration': 53.93392467498779, 'accumulated_submission_time': 36.148282289505005, 'accumulated_eval_time': 17.785524606704712, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1500, {'train/accuracy': 0.07029256969690323, 'train/loss': 5.318418025970459, 'validation/accuracy': 0.06721999496221542, 'validation/loss': 5.3727641105651855, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.6236677169799805, 'test/num_examples': 10000, 'score': 546.2268629074097, 'total_duration': 581.7664752006531, 'accumulated_submission_time': 546.2268629074097, 'accumulated_eval_time': 35.466519832611084, 'accumulated_logging_time': 0.0201263427734375, 'global_step': 1500, 'preemption_count': 0}), (2999, {'train/accuracy': 0.17598053812980652, 'train/loss': 4.236908912658691, 'validation/accuracy': 0.1589599996805191, 'validation/loss': 4.364312648773193, 'validation/num_examples': 50000, 'test/accuracy': 0.11150000244379044, 'test/loss': 4.811929702758789, 'test/num_examples': 10000, 'score': 1056.4533824920654, 'total_duration': 1109.996464729309, 'accumulated_submission_time': 1056.4533824920654, 'accumulated_eval_time': 53.3863730430603, 'accumulated_logging_time': 0.050855398178100586, 'global_step': 2999, 'preemption_count': 0}), (4427, {'train/accuracy': 0.27032843232154846, 'train/loss': 3.506873846054077, 'validation/accuracy': 0.25033998489379883, 'validation/loss': 3.6432690620422363, 'validation/num_examples': 50000, 'test/accuracy': 0.17840000987052917, 'test/loss': 4.235896110534668, 'test/num_examples': 10000, 'score': 1566.5691194534302, 'total_duration': 1638.1481268405914, 'accumulated_submission_time': 1566.5691194534302, 'accumulated_eval_time': 71.34168648719788, 'accumulated_logging_time': 0.08032560348510742, 'global_step': 4427, 'preemption_count': 0}), (5925, {'train/accuracy': 0.3623844087123871, 'train/loss': 2.9141533374786377, 'validation/accuracy': 0.3374599814414978, 'validation/loss': 3.0710597038269043, 'validation/num_examples': 50000, 'test/accuracy': 0.2468000054359436, 'test/loss': 3.741469383239746, 'test/num_examples': 10000, 'score': 2076.4831607341766, 'total_duration': 2166.0223004817963, 'accumulated_submission_time': 2076.4831607341766, 'accumulated_eval_time': 89.21532106399536, 'accumulated_logging_time': 0.11384129524230957, 'global_step': 5925, 'preemption_count': 0}), (7424, {'train/accuracy': 0.43327486515045166, 'train/loss': 2.47650408744812, 'validation/accuracy': 0.38857999444007874, 'validation/loss': 2.7649340629577637, 'validation/num_examples': 50000, 'test/accuracy': 0.289000004529953, 'test/loss': 3.5003814697265625, 'test/num_examples': 10000, 'score': 2586.6451222896576, 'total_duration': 2694.1260364055634, 'accumulated_submission_time': 2586.6451222896576, 'accumulated_eval_time': 107.07217979431152, 'accumulated_logging_time': 0.14467430114746094, 'global_step': 7424, 'preemption_count': 0}), (8923, {'train/accuracy': 0.5002790093421936, 'train/loss': 2.140385389328003, 'validation/accuracy': 0.4495999813079834, 'validation/loss': 2.4287495613098145, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.144484043121338, 'test/num_examples': 10000, 'score': 3096.7352623939514, 'total_duration': 3223.0867116451263, 'accumulated_submission_time': 3096.7352623939514, 'accumulated_eval_time': 125.86094260215759, 'accumulated_logging_time': 0.1740093231201172, 'global_step': 8923, 'preemption_count': 0}), (10423, {'train/accuracy': 0.5160634517669678, 'train/loss': 2.058394432067871, 'validation/accuracy': 0.4732999801635742, 'validation/loss': 2.3019864559173584, 'validation/num_examples': 50000, 'test/accuracy': 0.3750000298023224, 'test/loss': 2.9806602001190186, 'test/num_examples': 10000, 'score': 3606.813559293747, 'total_duration': 3751.127242565155, 'accumulated_submission_time': 3606.813559293747, 'accumulated_eval_time': 143.74170780181885, 'accumulated_logging_time': 0.20235371589660645, 'global_step': 10423, 'preemption_count': 0}), (11923, {'train/accuracy': 0.5541493892669678, 'train/loss': 1.873650074005127, 'validation/accuracy': 0.510159969329834, 'validation/loss': 2.0996718406677246, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.822295665740967, 'test/num_examples': 10000, 'score': 4116.920515537262, 'total_duration': 4279.158988952637, 'accumulated_submission_time': 4116.920515537262, 'accumulated_eval_time': 161.58436393737793, 'accumulated_logging_time': 0.23344969749450684, 'global_step': 11923, 'preemption_count': 0}), (13425, {'train/accuracy': 0.5691764950752258, 'train/loss': 1.8064440488815308, 'validation/accuracy': 0.527899980545044, 'validation/loss': 2.0273935794830322, 'validation/num_examples': 50000, 'test/accuracy': 0.40710002183914185, 'test/loss': 2.8018851280212402, 'test/num_examples': 10000, 'score': 4627.143044948578, 'total_duration': 4807.1185483932495, 'accumulated_submission_time': 4627.143044948578, 'accumulated_eval_time': 179.24028539657593, 'accumulated_logging_time': 0.2617185115814209, 'global_step': 13425, 'preemption_count': 0}), (14927, {'train/accuracy': 0.5782844424247742, 'train/loss': 1.7499090433120728, 'validation/accuracy': 0.5433799624443054, 'validation/loss': 1.9397135972976685, 'validation/num_examples': 50000, 'test/accuracy': 0.41850000619888306, 'test/loss': 2.6923470497131348, 'test/num_examples': 10000, 'score': 5137.219739675522, 'total_duration': 5334.8082802295685, 'accumulated_submission_time': 5137.219739675522, 'accumulated_eval_time': 196.7713873386383, 'accumulated_logging_time': 0.29082202911376953, 'global_step': 14927, 'preemption_count': 0}), (16430, {'train/accuracy': 0.5916174650192261, 'train/loss': 1.6916300058364868, 'validation/accuracy': 0.5521799921989441, 'validation/loss': 1.8945378065109253, 'validation/num_examples': 50000, 'test/accuracy': 0.4326000213623047, 'test/loss': 2.6046390533447266, 'test/num_examples': 10000, 'score': 5647.431823968887, 'total_duration': 5862.699553728104, 'accumulated_submission_time': 5647.431823968887, 'accumulated_eval_time': 214.36780762672424, 'accumulated_logging_time': 0.3212883472442627, 'global_step': 16430, 'preemption_count': 0}), (17934, {'train/accuracy': 0.6174266338348389, 'train/loss': 1.5416178703308105, 'validation/accuracy': 0.5548799633979797, 'validation/loss': 1.8838409185409546, 'validation/num_examples': 50000, 'test/accuracy': 0.4327000081539154, 'test/loss': 2.6345584392547607, 'test/num_examples': 10000, 'score': 6157.574437856674, 'total_duration': 6390.7123556137085, 'accumulated_submission_time': 6157.574437856674, 'accumulated_eval_time': 232.15265464782715, 'accumulated_logging_time': 0.3548550605773926, 'global_step': 17934, 'preemption_count': 0}), (19438, {'train/accuracy': 0.6119060516357422, 'train/loss': 1.5986028909683228, 'validation/accuracy': 0.5607999563217163, 'validation/loss': 1.8535418510437012, 'validation/num_examples': 50000, 'test/accuracy': 0.43640002608299255, 'test/loss': 2.5843722820281982, 'test/num_examples': 10000, 'score': 6667.758923768997, 'total_duration': 6918.904389619827, 'accumulated_submission_time': 6667.758923768997, 'accumulated_eval_time': 250.0759198665619, 'accumulated_logging_time': 0.38729166984558105, 'global_step': 19438, 'preemption_count': 0}), (20942, {'train/accuracy': 0.6103515625, 'train/loss': 1.5754483938217163, 'validation/accuracy': 0.5648599863052368, 'validation/loss': 1.8282480239868164, 'validation/num_examples': 50000, 'test/accuracy': 0.45250001549720764, 'test/loss': 2.538072109222412, 'test/num_examples': 10000, 'score': 7177.744247436523, 'total_duration': 7446.508990764618, 'accumulated_submission_time': 7177.744247436523, 'accumulated_eval_time': 267.61257815361023, 'accumulated_logging_time': 0.4182553291320801, 'global_step': 20942, 'preemption_count': 0}), (22446, {'train/accuracy': 0.6111288070678711, 'train/loss': 1.5875287055969238, 'validation/accuracy': 0.572219967842102, 'validation/loss': 1.8043186664581299, 'validation/num_examples': 50000, 'test/accuracy': 0.4520000219345093, 'test/loss': 2.550936698913574, 'test/num_examples': 10000, 'score': 7687.6893701553345, 'total_duration': 7974.141838550568, 'accumulated_submission_time': 7687.6893701553345, 'accumulated_eval_time': 285.21574664115906, 'accumulated_logging_time': 0.4493522644042969, 'global_step': 22446, 'preemption_count': 0}), (23950, {'train/accuracy': 0.6212531924247742, 'train/loss': 1.5448447465896606, 'validation/accuracy': 0.5806999802589417, 'validation/loss': 1.7550562620162964, 'validation/num_examples': 50000, 'test/accuracy': 0.4563000202178955, 'test/loss': 2.495838165283203, 'test/num_examples': 10000, 'score': 8197.739970445633, 'total_duration': 8501.958456516266, 'accumulated_submission_time': 8197.739970445633, 'accumulated_eval_time': 302.8937132358551, 'accumulated_logging_time': 0.4838879108428955, 'global_step': 23950, 'preemption_count': 0}), (25455, {'train/accuracy': 0.6300023794174194, 'train/loss': 1.4953486919403076, 'validation/accuracy': 0.5855000019073486, 'validation/loss': 1.7113131284713745, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.4377589225769043, 'test/num_examples': 10000, 'score': 8707.943544864655, 'total_duration': 9029.98591208458, 'accumulated_submission_time': 8707.943544864655, 'accumulated_eval_time': 320.63299918174744, 'accumulated_logging_time': 0.5171177387237549, 'global_step': 25455, 'preemption_count': 0}), (26960, {'train/accuracy': 0.6515864133834839, 'train/loss': 1.3849291801452637, 'validation/accuracy': 0.5827599763870239, 'validation/loss': 1.7557555437088013, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.4801058769226074, 'test/num_examples': 10000, 'score': 9218.090360164642, 'total_duration': 9558.22905421257, 'accumulated_submission_time': 9218.090360164642, 'accumulated_eval_time': 338.6408226490021, 'accumulated_logging_time': 0.5527477264404297, 'global_step': 26960, 'preemption_count': 0}), (28465, {'train/accuracy': 0.6382134556770325, 'train/loss': 1.4549899101257324, 'validation/accuracy': 0.5848399996757507, 'validation/loss': 1.7341421842575073, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.44994854927063, 'test/num_examples': 10000, 'score': 9728.102895259857, 'total_duration': 10085.848005533218, 'accumulated_submission_time': 9728.102895259857, 'accumulated_eval_time': 356.16178250312805, 'accumulated_logging_time': 0.5854485034942627, 'global_step': 28465, 'preemption_count': 0}), (29971, {'train/accuracy': 0.6387914419174194, 'train/loss': 1.4706834554672241, 'validation/accuracy': 0.5878399610519409, 'validation/loss': 1.7269160747528076, 'validation/num_examples': 50000, 'test/accuracy': 0.4678000211715698, 'test/loss': 2.4534895420074463, 'test/num_examples': 10000, 'score': 10238.321905136108, 'total_duration': 10613.808283567429, 'accumulated_submission_time': 10238.321905136108, 'accumulated_eval_time': 373.8170976638794, 'accumulated_logging_time': 0.6196649074554443, 'global_step': 29971, 'preemption_count': 0}), (31476, {'train/accuracy': 0.6308194994926453, 'train/loss': 1.480445384979248, 'validation/accuracy': 0.5906999707221985, 'validation/loss': 1.7181880474090576, 'validation/num_examples': 50000, 'test/accuracy': 0.46960002183914185, 'test/loss': 2.4204015731811523, 'test/num_examples': 10000, 'score': 10748.38038611412, 'total_duration': 11141.573992013931, 'accumulated_submission_time': 10748.38038611412, 'accumulated_eval_time': 391.4266884326935, 'accumulated_logging_time': 0.6621429920196533, 'global_step': 31476, 'preemption_count': 0}), (32982, {'train/accuracy': 0.6412228941917419, 'train/loss': 1.4475443363189697, 'validation/accuracy': 0.598639965057373, 'validation/loss': 1.678407907485962, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.3977839946746826, 'test/num_examples': 10000, 'score': 11258.525603055954, 'total_duration': 11669.392268419266, 'accumulated_submission_time': 11258.525603055954, 'accumulated_eval_time': 409.01349687576294, 'accumulated_logging_time': 0.6950950622558594, 'global_step': 32982, 'preemption_count': 0}), (34487, {'train/accuracy': 0.6408242583274841, 'train/loss': 1.449402093887329, 'validation/accuracy': 0.5999000072479248, 'validation/loss': 1.667544960975647, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.399554967880249, 'test/num_examples': 10000, 'score': 11768.464093208313, 'total_duration': 12196.891924381256, 'accumulated_submission_time': 11768.464093208313, 'accumulated_eval_time': 426.486044883728, 'accumulated_logging_time': 0.7312412261962891, 'global_step': 34487, 'preemption_count': 0}), (35993, {'train/accuracy': 0.668965220451355, 'train/loss': 1.3015565872192383, 'validation/accuracy': 0.5925999879837036, 'validation/loss': 1.6919569969177246, 'validation/num_examples': 50000, 'test/accuracy': 0.4733000099658966, 'test/loss': 2.408229112625122, 'test/num_examples': 10000, 'score': 12278.674576044083, 'total_duration': 12725.251027584076, 'accumulated_submission_time': 12278.674576044083, 'accumulated_eval_time': 444.545640707016, 'accumulated_logging_time': 0.7679879665374756, 'global_step': 35993, 'preemption_count': 0}), (37498, {'train/accuracy': 0.6574656963348389, 'train/loss': 1.356908917427063, 'validation/accuracy': 0.6019399762153625, 'validation/loss': 1.649037480354309, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.377382278442383, 'test/num_examples': 10000, 'score': 12788.770513534546, 'total_duration': 13253.0048995018, 'accumulated_submission_time': 12788.770513534546, 'accumulated_eval_time': 462.1173481941223, 'accumulated_logging_time': 0.802004337310791, 'global_step': 37498, 'preemption_count': 0}), (39004, {'train/accuracy': 0.6500119566917419, 'train/loss': 1.40574049949646, 'validation/accuracy': 0.5988199710845947, 'validation/loss': 1.6665188074111938, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.388904571533203, 'test/num_examples': 10000, 'score': 13298.82503771782, 'total_duration': 13780.851271390915, 'accumulated_submission_time': 13298.82503771782, 'accumulated_eval_time': 479.8211030960083, 'accumulated_logging_time': 0.8365299701690674, 'global_step': 39004, 'preemption_count': 0}), (40510, {'train/accuracy': 0.6475805044174194, 'train/loss': 1.4063873291015625, 'validation/accuracy': 0.6014999747276306, 'validation/loss': 1.6467198133468628, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.3571691513061523, 'test/num_examples': 10000, 'score': 13808.78684091568, 'total_duration': 14308.921383857727, 'accumulated_submission_time': 13808.78684091568, 'accumulated_eval_time': 497.84239530563354, 'accumulated_logging_time': 0.8696949481964111, 'global_step': 40510, 'preemption_count': 0}), (42017, {'train/accuracy': 0.6520049571990967, 'train/loss': 1.4068289995193481, 'validation/accuracy': 0.6034199595451355, 'validation/loss': 1.6507673263549805, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.4058632850646973, 'test/num_examples': 10000, 'score': 14318.928505182266, 'total_duration': 14836.609621286392, 'accumulated_submission_time': 14318.928505182266, 'accumulated_eval_time': 515.2958283424377, 'accumulated_logging_time': 0.9089441299438477, 'global_step': 42017, 'preemption_count': 0}), (43524, {'train/accuracy': 0.6497727632522583, 'train/loss': 1.3987377882003784, 'validation/accuracy': 0.6049000024795532, 'validation/loss': 1.6419485807418823, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.3409223556518555, 'test/num_examples': 10000, 'score': 14829.094542264938, 'total_duration': 15364.478039741516, 'accumulated_submission_time': 14829.094542264938, 'accumulated_eval_time': 532.904883146286, 'accumulated_logging_time': 0.9475512504577637, 'global_step': 43524, 'preemption_count': 0}), (45031, {'train/accuracy': 0.6812220811843872, 'train/loss': 1.246896505355835, 'validation/accuracy': 0.6029199957847595, 'validation/loss': 1.6404823064804077, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3501029014587402, 'test/num_examples': 10000, 'score': 15339.251924276352, 'total_duration': 15892.332380533218, 'accumulated_submission_time': 15339.251924276352, 'accumulated_eval_time': 550.5144395828247, 'accumulated_logging_time': 0.9823529720306396, 'global_step': 45031, 'preemption_count': 0}), (46537, {'train/accuracy': 0.6583425998687744, 'train/loss': 1.357541561126709, 'validation/accuracy': 0.600600004196167, 'validation/loss': 1.6586617231369019, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3764281272888184, 'test/num_examples': 10000, 'score': 15849.248585939407, 'total_duration': 16420.061936855316, 'accumulated_submission_time': 15849.248585939407, 'accumulated_eval_time': 568.1608099937439, 'accumulated_logging_time': 1.0166702270507812, 'global_step': 46537, 'preemption_count': 0}), (48044, {'train/accuracy': 0.6581233739852905, 'train/loss': 1.347723126411438, 'validation/accuracy': 0.6038599610328674, 'validation/loss': 1.6352239847183228, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.325078010559082, 'test/num_examples': 10000, 'score': 16359.366846084595, 'total_duration': 16948.598692417145, 'accumulated_submission_time': 16359.366846084595, 'accumulated_eval_time': 586.4911158084869, 'accumulated_logging_time': 1.051117181777954, 'global_step': 48044, 'preemption_count': 0}), (49551, {'train/accuracy': 0.6349250674247742, 'train/loss': 1.4702372550964355, 'validation/accuracy': 0.5914999842643738, 'validation/loss': 1.712287425994873, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.4408493041992188, 'test/num_examples': 10000, 'score': 16869.45350933075, 'total_duration': 17476.268760681152, 'accumulated_submission_time': 16869.45350933075, 'accumulated_eval_time': 603.9879791736603, 'accumulated_logging_time': 1.0860350131988525, 'global_step': 49551, 'preemption_count': 0}), (51058, {'train/accuracy': 0.6541573405265808, 'train/loss': 1.3928905725479126, 'validation/accuracy': 0.6054399609565735, 'validation/loss': 1.634318232536316, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.329887628555298, 'test/num_examples': 10000, 'score': 17379.67597913742, 'total_duration': 18003.970739126205, 'accumulated_submission_time': 17379.67597913742, 'accumulated_eval_time': 621.3797891139984, 'accumulated_logging_time': 1.1208219528198242, 'global_step': 51058, 'preemption_count': 0}), (52566, {'train/accuracy': 0.6519849896430969, 'train/loss': 1.3961294889450073, 'validation/accuracy': 0.6064199805259705, 'validation/loss': 1.6361829042434692, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.343546152114868, 'test/num_examples': 10000, 'score': 17889.827553749084, 'total_duration': 18531.911148548126, 'accumulated_submission_time': 17889.827553749084, 'accumulated_eval_time': 639.0722250938416, 'accumulated_logging_time': 1.1622974872589111, 'global_step': 52566, 'preemption_count': 0}), (54074, {'train/accuracy': 0.6962292790412903, 'train/loss': 1.2013616561889648, 'validation/accuracy': 0.6180799603462219, 'validation/loss': 1.5881798267364502, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.290510654449463, 'test/num_examples': 10000, 'score': 18400.011061429977, 'total_duration': 19059.650450468063, 'accumulated_submission_time': 18400.011061429977, 'accumulated_eval_time': 656.536458492279, 'accumulated_logging_time': 1.201387643814087, 'global_step': 54074, 'preemption_count': 0}), (55581, {'train/accuracy': 0.680086076259613, 'train/loss': 1.259409785270691, 'validation/accuracy': 0.6174600124359131, 'validation/loss': 1.571677565574646, 'validation/num_examples': 50000, 'test/accuracy': 0.5017000436782837, 'test/loss': 2.253861904144287, 'test/num_examples': 10000, 'score': 18910.092315673828, 'total_duration': 19587.41103410721, 'accumulated_submission_time': 18910.092315673828, 'accumulated_eval_time': 674.1192197799683, 'accumulated_logging_time': 1.2447161674499512, 'global_step': 55581, 'preemption_count': 0}), (57089, {'train/accuracy': 0.6758211255073547, 'train/loss': 1.280925989151001, 'validation/accuracy': 0.6241399645805359, 'validation/loss': 1.5517821311950684, 'validation/num_examples': 50000, 'test/accuracy': 0.4967000186443329, 'test/loss': 2.2936458587646484, 'test/num_examples': 10000, 'score': 19420.2594435215, 'total_duration': 20115.578449726105, 'accumulated_submission_time': 19420.2594435215, 'accumulated_eval_time': 692.030259847641, 'accumulated_logging_time': 1.281590461730957, 'global_step': 57089, 'preemption_count': 0}), (58596, {'train/accuracy': 0.6629464030265808, 'train/loss': 1.3316421508789062, 'validation/accuracy': 0.6131399869918823, 'validation/loss': 1.5955449342727661, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.289567470550537, 'test/num_examples': 10000, 'score': 19930.20670747757, 'total_duration': 20643.429992198944, 'accumulated_submission_time': 19930.20670747757, 'accumulated_eval_time': 709.8440093994141, 'accumulated_logging_time': 1.3187220096588135, 'global_step': 58596, 'preemption_count': 0}), (60104, {'train/accuracy': 0.6681082248687744, 'train/loss': 1.3214025497436523, 'validation/accuracy': 0.6195399761199951, 'validation/loss': 1.5713294744491577, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.2615723609924316, 'test/num_examples': 10000, 'score': 20440.350209712982, 'total_duration': 21171.720563173294, 'accumulated_submission_time': 20440.350209712982, 'accumulated_eval_time': 727.8984732627869, 'accumulated_logging_time': 1.3597385883331299, 'global_step': 60104, 'preemption_count': 0}), (61612, {'train/accuracy': 0.6642019748687744, 'train/loss': 1.337678074836731, 'validation/accuracy': 0.6198999881744385, 'validation/loss': 1.577870488166809, 'validation/num_examples': 50000, 'test/accuracy': 0.49220001697540283, 'test/loss': 2.2768988609313965, 'test/num_examples': 10000, 'score': 20950.476552248, 'total_duration': 21699.60069823265, 'accumulated_submission_time': 20950.476552248, 'accumulated_eval_time': 745.5649440288544, 'accumulated_logging_time': 1.3957176208496094, 'global_step': 61612, 'preemption_count': 0}), (63120, {'train/accuracy': 0.6990194320678711, 'train/loss': 1.1689326763153076, 'validation/accuracy': 0.6167399883270264, 'validation/loss': 1.5819714069366455, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.285916805267334, 'test/num_examples': 10000, 'score': 21460.725561141968, 'total_duration': 22227.461196899414, 'accumulated_submission_time': 21460.725561141968, 'accumulated_eval_time': 763.08553647995, 'accumulated_logging_time': 1.4341421127319336, 'global_step': 63120, 'preemption_count': 0}), (64628, {'train/accuracy': 0.6875, 'train/loss': 1.2308303117752075, 'validation/accuracy': 0.6247199773788452, 'validation/loss': 1.5516197681427002, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.266094207763672, 'test/num_examples': 10000, 'score': 21970.70988535881, 'total_duration': 22755.408446788788, 'accumulated_submission_time': 21970.70988535881, 'accumulated_eval_time': 780.9508357048035, 'accumulated_logging_time': 1.477266550064087, 'global_step': 64628, 'preemption_count': 0}), (66136, {'train/accuracy': 0.6802455186843872, 'train/loss': 1.267142415046692, 'validation/accuracy': 0.6260600090026855, 'validation/loss': 1.5435158014297485, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2609455585479736, 'test/num_examples': 10000, 'score': 22480.74132156372, 'total_duration': 23283.03837132454, 'accumulated_submission_time': 22480.74132156372, 'accumulated_eval_time': 798.4543952941895, 'accumulated_logging_time': 1.5191307067871094, 'global_step': 66136, 'preemption_count': 0}), (67644, {'train/accuracy': 0.6720344424247742, 'train/loss': 1.2921645641326904, 'validation/accuracy': 0.624779999256134, 'validation/loss': 1.5412269830703735, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.2861030101776123, 'test/num_examples': 10000, 'score': 22990.79816222191, 'total_duration': 23810.67390537262, 'accumulated_submission_time': 22990.79816222191, 'accumulated_eval_time': 815.9394180774689, 'accumulated_logging_time': 1.5590641498565674, 'global_step': 67644, 'preemption_count': 0}), (69152, {'train/accuracy': 0.6754822731018066, 'train/loss': 1.2919354438781738, 'validation/accuracy': 0.626800000667572, 'validation/loss': 1.539273738861084, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.2561073303222656, 'test/num_examples': 10000, 'score': 23500.92076444626, 'total_duration': 24338.255034685135, 'accumulated_submission_time': 23500.92076444626, 'accumulated_eval_time': 833.3083035945892, 'accumulated_logging_time': 1.5970518589019775, 'global_step': 69152, 'preemption_count': 0}), (70660, {'train/accuracy': 0.6835139989852905, 'train/loss': 1.258325219154358, 'validation/accuracy': 0.6360599994659424, 'validation/loss': 1.496769666671753, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.2187583446502686, 'test/num_examples': 10000, 'score': 24011.123149871826, 'total_duration': 24866.50309228897, 'accumulated_submission_time': 24011.123149871826, 'accumulated_eval_time': 851.2584345340729, 'accumulated_logging_time': 1.6391994953155518, 'global_step': 70660, 'preemption_count': 0}), (72168, {'train/accuracy': 0.7145846486091614, 'train/loss': 1.0963265895843506, 'validation/accuracy': 0.6308599710464478, 'validation/loss': 1.5195053815841675, 'validation/num_examples': 50000, 'test/accuracy': 0.5113000273704529, 'test/loss': 2.2167670726776123, 'test/num_examples': 10000, 'score': 24521.15534877777, 'total_duration': 25394.251450777054, 'accumulated_submission_time': 24521.15534877777, 'accumulated_eval_time': 868.8834428787231, 'accumulated_logging_time': 1.677199125289917, 'global_step': 72168, 'preemption_count': 0}), (73676, {'train/accuracy': 0.698660671710968, 'train/loss': 1.1710692644119263, 'validation/accuracy': 0.6325399875640869, 'validation/loss': 1.5120478868484497, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.2187116146087646, 'test/num_examples': 10000, 'score': 25031.063472747803, 'total_duration': 25921.97821545601, 'accumulated_submission_time': 25031.063472747803, 'accumulated_eval_time': 886.6075978279114, 'accumulated_logging_time': 1.7175266742706299, 'global_step': 73676, 'preemption_count': 0}), (75184, {'train/accuracy': 0.6887555718421936, 'train/loss': 1.2242296934127808, 'validation/accuracy': 0.6289199590682983, 'validation/loss': 1.5229800939559937, 'validation/num_examples': 50000, 'test/accuracy': 0.5022000074386597, 'test/loss': 2.2825558185577393, 'test/num_examples': 10000, 'score': 25541.049762248993, 'total_duration': 26449.859035730362, 'accumulated_submission_time': 25541.049762248993, 'accumulated_eval_time': 904.4073250293732, 'accumulated_logging_time': 1.7588274478912354, 'global_step': 75184, 'preemption_count': 0}), (76692, {'train/accuracy': 0.6845503449440002, 'train/loss': 1.2397161722183228, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.4972995519638062, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.2224087715148926, 'test/num_examples': 10000, 'score': 26051.24068403244, 'total_duration': 26977.98685336113, 'accumulated_submission_time': 26051.24068403244, 'accumulated_eval_time': 922.2355952262878, 'accumulated_logging_time': 1.8136036396026611, 'global_step': 76692, 'preemption_count': 0}), (78200, {'train/accuracy': 0.6881178021430969, 'train/loss': 1.235331654548645, 'validation/accuracy': 0.6344999670982361, 'validation/loss': 1.5009933710098267, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.199228525161743, 'test/num_examples': 10000, 'score': 26561.368300914764, 'total_duration': 27505.897212982178, 'accumulated_submission_time': 26561.368300914764, 'accumulated_eval_time': 939.9247233867645, 'accumulated_logging_time': 1.8545491695404053, 'global_step': 78200, 'preemption_count': 0}), (79708, {'train/accuracy': 0.6924824714660645, 'train/loss': 1.204252004623413, 'validation/accuracy': 0.6411600112915039, 'validation/loss': 1.4582104682922363, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.2048068046569824, 'test/num_examples': 10000, 'score': 27071.600769996643, 'total_duration': 28033.949457883835, 'accumulated_submission_time': 27071.600769996643, 'accumulated_eval_time': 957.6483449935913, 'accumulated_logging_time': 1.8976809978485107, 'global_step': 79708, 'preemption_count': 0}), (81216, {'train/accuracy': 0.7299705147743225, 'train/loss': 1.0363904237747192, 'validation/accuracy': 0.6387400031089783, 'validation/loss': 1.4843218326568604, 'validation/num_examples': 50000, 'test/accuracy': 0.522100031375885, 'test/loss': 2.1594133377075195, 'test/num_examples': 10000, 'score': 27581.69038462639, 'total_duration': 28561.98272919655, 'accumulated_submission_time': 27581.69038462639, 'accumulated_eval_time': 975.4938888549805, 'accumulated_logging_time': 1.9428644180297852, 'global_step': 81216, 'preemption_count': 0}), (82723, {'train/accuracy': 0.7124919891357422, 'train/loss': 1.1045640707015991, 'validation/accuracy': 0.6467999815940857, 'validation/loss': 1.4450737237930298, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.1748580932617188, 'test/num_examples': 10000, 'score': 28091.66532254219, 'total_duration': 29089.590743780136, 'accumulated_submission_time': 28091.66532254219, 'accumulated_eval_time': 993.0329856872559, 'accumulated_logging_time': 1.9836251735687256, 'global_step': 82723, 'preemption_count': 0}), (84231, {'train/accuracy': 0.708426296710968, 'train/loss': 1.1359773874282837, 'validation/accuracy': 0.6473000049591064, 'validation/loss': 1.4457052946090698, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.1410605907440186, 'test/num_examples': 10000, 'score': 28601.680331230164, 'total_duration': 29617.22701382637, 'accumulated_submission_time': 28601.680331230164, 'accumulated_eval_time': 1010.5548868179321, 'accumulated_logging_time': 2.0314993858337402, 'global_step': 84231, 'preemption_count': 0}), (85739, {'train/accuracy': 0.697684109210968, 'train/loss': 1.191365122795105, 'validation/accuracy': 0.64028000831604, 'validation/loss': 1.475667953491211, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.1744232177734375, 'test/num_examples': 10000, 'score': 29111.69335460663, 'total_duration': 30145.420708179474, 'accumulated_submission_time': 29111.69335460663, 'accumulated_eval_time': 1028.6400225162506, 'accumulated_logging_time': 2.074836492538452, 'global_step': 85739, 'preemption_count': 0}), (87245, {'train/accuracy': 0.7034239172935486, 'train/loss': 1.1476686000823975, 'validation/accuracy': 0.6473599672317505, 'validation/loss': 1.432129144668579, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.1454508304595947, 'test/num_examples': 10000, 'score': 29621.69142627716, 'total_duration': 30673.143434762955, 'accumulated_submission_time': 29621.69142627716, 'accumulated_eval_time': 1046.26478099823, 'accumulated_logging_time': 2.1219582557678223, 'global_step': 87245, 'preemption_count': 0}), (88753, {'train/accuracy': 0.69921875, 'train/loss': 1.1784825325012207, 'validation/accuracy': 0.6462599635124207, 'validation/loss': 1.4546048641204834, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.169255256652832, 'test/num_examples': 10000, 'score': 30131.69265937805, 'total_duration': 31200.92763876915, 'accumulated_submission_time': 30131.69265937805, 'accumulated_eval_time': 1063.9508562088013, 'accumulated_logging_time': 2.1666407585144043, 'global_step': 88753, 'preemption_count': 0}), (90261, {'train/accuracy': 0.7489835619926453, 'train/loss': 0.9775881171226501, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.4023665189743042, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.106358766555786, 'test/num_examples': 10000, 'score': 30641.757912397385, 'total_duration': 31728.86659193039, 'accumulated_submission_time': 30641.757912397385, 'accumulated_eval_time': 1081.7278769016266, 'accumulated_logging_time': 2.2114133834838867, 'global_step': 90261, 'preemption_count': 0}), (91769, {'train/accuracy': 0.720703125, 'train/loss': 1.0760672092437744, 'validation/accuracy': 0.6516799926757812, 'validation/loss': 1.4240094423294067, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.1127941608428955, 'test/num_examples': 10000, 'score': 31151.930895090103, 'total_duration': 32256.83377289772, 'accumulated_submission_time': 31151.930895090103, 'accumulated_eval_time': 1099.4261264801025, 'accumulated_logging_time': 2.254272222518921, 'global_step': 91769, 'preemption_count': 0}), (93277, {'train/accuracy': 0.7053571343421936, 'train/loss': 1.1407818794250488, 'validation/accuracy': 0.6459000110626221, 'validation/loss': 1.4545795917510986, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.1714203357696533, 'test/num_examples': 10000, 'score': 31662.01350545883, 'total_duration': 32784.82475876808, 'accumulated_submission_time': 31662.01350545883, 'accumulated_eval_time': 1117.2352879047394, 'accumulated_logging_time': 2.3011181354522705, 'global_step': 93277, 'preemption_count': 0}), (94785, {'train/accuracy': 0.7093630433082581, 'train/loss': 1.1220866441726685, 'validation/accuracy': 0.6515799760818481, 'validation/loss': 1.4239590167999268, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.137367010116577, 'test/num_examples': 10000, 'score': 32172.043387413025, 'total_duration': 33312.68545603752, 'accumulated_submission_time': 32172.043387413025, 'accumulated_eval_time': 1134.9675867557526, 'accumulated_logging_time': 2.345402479171753, 'global_step': 94785, 'preemption_count': 0}), (96294, {'train/accuracy': 0.7216597199440002, 'train/loss': 1.0726561546325684, 'validation/accuracy': 0.666979968547821, 'validation/loss': 1.358321189880371, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.063443183898926, 'test/num_examples': 10000, 'score': 32682.25458574295, 'total_duration': 33840.66708111763, 'accumulated_submission_time': 32682.25458574295, 'accumulated_eval_time': 1152.641860485077, 'accumulated_logging_time': 2.3891897201538086, 'global_step': 96294, 'preemption_count': 0}), (97802, {'train/accuracy': 0.7195671200752258, 'train/loss': 1.0867538452148438, 'validation/accuracy': 0.6625799536705017, 'validation/loss': 1.3720060586929321, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.0853664875030518, 'test/num_examples': 10000, 'score': 33192.23504304886, 'total_duration': 34368.55675005913, 'accumulated_submission_time': 33192.23504304886, 'accumulated_eval_time': 1170.453558921814, 'accumulated_logging_time': 2.4338431358337402, 'global_step': 97802, 'preemption_count': 0}), (99311, {'train/accuracy': 0.7441206574440002, 'train/loss': 0.9868006110191345, 'validation/accuracy': 0.6504799723625183, 'validation/loss': 1.4178804159164429, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.124274492263794, 'test/num_examples': 10000, 'score': 33702.44986701012, 'total_duration': 34896.47751951218, 'accumulated_submission_time': 33702.44986701012, 'accumulated_eval_time': 1188.0544037818909, 'accumulated_logging_time': 2.4853873252868652, 'global_step': 99311, 'preemption_count': 0}), (100820, {'train/accuracy': 0.7390784025192261, 'train/loss': 0.9912664294242859, 'validation/accuracy': 0.6676999926567078, 'validation/loss': 1.349393606185913, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.032019853591919, 'test/num_examples': 10000, 'score': 34212.6481218338, 'total_duration': 35424.54218220711, 'accumulated_submission_time': 34212.6481218338, 'accumulated_eval_time': 1205.8191084861755, 'accumulated_logging_time': 2.532451629638672, 'global_step': 100820, 'preemption_count': 0}), (102329, {'train/accuracy': 0.7271803021430969, 'train/loss': 1.0407520532608032, 'validation/accuracy': 0.6602399945259094, 'validation/loss': 1.3748583793640137, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.074155807495117, 'test/num_examples': 10000, 'score': 34722.8763525486, 'total_duration': 35952.56840515137, 'accumulated_submission_time': 34722.8763525486, 'accumulated_eval_time': 1223.5154864788055, 'accumulated_logging_time': 2.5815184116363525, 'global_step': 102329, 'preemption_count': 0}), (103837, {'train/accuracy': 0.7323620915412903, 'train/loss': 1.0326040983200073, 'validation/accuracy': 0.6691199541091919, 'validation/loss': 1.3446305990219116, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.035710573196411, 'test/num_examples': 10000, 'score': 35232.8334004879, 'total_duration': 36480.34671187401, 'accumulated_submission_time': 35232.8334004879, 'accumulated_eval_time': 1241.2360637187958, 'accumulated_logging_time': 2.6300241947174072, 'global_step': 103837, 'preemption_count': 0}), (105345, {'train/accuracy': 0.7277383208274841, 'train/loss': 1.057060718536377, 'validation/accuracy': 0.6671000123023987, 'validation/loss': 1.3545143604278564, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.032726526260376, 'test/num_examples': 10000, 'score': 35742.87636375427, 'total_duration': 37007.94652104378, 'accumulated_submission_time': 35742.87636375427, 'accumulated_eval_time': 1258.6924047470093, 'accumulated_logging_time': 2.6776812076568604, 'global_step': 105345, 'preemption_count': 0}), (106853, {'train/accuracy': 0.7320830821990967, 'train/loss': 1.035598635673523, 'validation/accuracy': 0.6726599931716919, 'validation/loss': 1.330732822418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.024749994277954, 'test/num_examples': 10000, 'score': 36252.81159281731, 'total_duration': 37535.38425660133, 'accumulated_submission_time': 36252.81159281731, 'accumulated_eval_time': 1276.0966138839722, 'accumulated_logging_time': 2.7229397296905518, 'global_step': 106853, 'preemption_count': 0}), (108361, {'train/accuracy': 0.7787786722183228, 'train/loss': 0.8223951458930969, 'validation/accuracy': 0.6761599779129028, 'validation/loss': 1.301857590675354, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 1.9950766563415527, 'test/num_examples': 10000, 'score': 36762.72781395912, 'total_duration': 38063.045838832855, 'accumulated_submission_time': 36762.72781395912, 'accumulated_eval_time': 1293.7430353164673, 'accumulated_logging_time': 2.769303321838379, 'global_step': 108361, 'preemption_count': 0}), (109869, {'train/accuracy': 0.7466916441917419, 'train/loss': 0.9609189033508301, 'validation/accuracy': 0.6674599647521973, 'validation/loss': 1.3489824533462524, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.03249454498291, 'test/num_examples': 10000, 'score': 37272.709463596344, 'total_duration': 38590.995624780655, 'accumulated_submission_time': 37272.709463596344, 'accumulated_eval_time': 1311.6061923503876, 'accumulated_logging_time': 2.820514678955078, 'global_step': 109869, 'preemption_count': 0}), (111377, {'train/accuracy': 0.7472297549247742, 'train/loss': 0.9534629583358765, 'validation/accuracy': 0.6775999665260315, 'validation/loss': 1.3055094480514526, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 1.9932727813720703, 'test/num_examples': 10000, 'score': 37782.685536146164, 'total_duration': 39118.579266786575, 'accumulated_submission_time': 37782.685536146164, 'accumulated_eval_time': 1329.1077308654785, 'accumulated_logging_time': 2.870955228805542, 'global_step': 111377, 'preemption_count': 0}), (112885, {'train/accuracy': 0.7496611475944519, 'train/loss': 0.9466625452041626, 'validation/accuracy': 0.6813600063323975, 'validation/loss': 1.2942183017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 1.9724422693252563, 'test/num_examples': 10000, 'score': 38292.7685611248, 'total_duration': 39646.19147348404, 'accumulated_submission_time': 38292.7685611248, 'accumulated_eval_time': 1346.5370290279388, 'accumulated_logging_time': 2.9173662662506104, 'global_step': 112885, 'preemption_count': 0}), (114393, {'train/accuracy': 0.7482461333274841, 'train/loss': 0.9572451114654541, 'validation/accuracy': 0.682699978351593, 'validation/loss': 1.284542202949524, 'validation/num_examples': 50000, 'test/accuracy': 0.5559000372886658, 'test/loss': 1.9868924617767334, 'test/num_examples': 10000, 'score': 38802.898661613464, 'total_duration': 40173.89938187599, 'accumulated_submission_time': 38802.898661613464, 'accumulated_eval_time': 1364.0172460079193, 'accumulated_logging_time': 2.962613582611084, 'global_step': 114393, 'preemption_count': 0}), (115901, {'train/accuracy': 0.7508171200752258, 'train/loss': 0.9416345953941345, 'validation/accuracy': 0.6827799677848816, 'validation/loss': 1.2685418128967285, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 1.9634768962860107, 'test/num_examples': 10000, 'score': 39313.07150053978, 'total_duration': 40701.87993502617, 'accumulated_submission_time': 39313.07150053978, 'accumulated_eval_time': 1381.7219486236572, 'accumulated_logging_time': 3.011996269226074, 'global_step': 115901, 'preemption_count': 0}), (117409, {'train/accuracy': 0.7893415093421936, 'train/loss': 0.7836189866065979, 'validation/accuracy': 0.6830199956893921, 'validation/loss': 1.2751917839050293, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 1.9720399379730225, 'test/num_examples': 10000, 'score': 39823.0315990448, 'total_duration': 41229.51292562485, 'accumulated_submission_time': 39823.0315990448, 'accumulated_eval_time': 1399.2922735214233, 'accumulated_logging_time': 3.0626745223999023, 'global_step': 117409, 'preemption_count': 0}), (118917, {'train/accuracy': 0.7732780575752258, 'train/loss': 0.8545231819152832, 'validation/accuracy': 0.6861199736595154, 'validation/loss': 1.2665618658065796, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 1.9834524393081665, 'test/num_examples': 10000, 'score': 40333.14848613739, 'total_duration': 41757.50674414635, 'accumulated_submission_time': 40333.14848613739, 'accumulated_eval_time': 1417.0705609321594, 'accumulated_logging_time': 3.109504461288452, 'global_step': 118917, 'preemption_count': 0}), (120426, {'train/accuracy': 0.7666015625, 'train/loss': 0.8653361201286316, 'validation/accuracy': 0.6888200044631958, 'validation/loss': 1.2589260339736938, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 1.9661813974380493, 'test/num_examples': 10000, 'score': 40843.37981677055, 'total_duration': 42285.4398317337, 'accumulated_submission_time': 40843.37981677055, 'accumulated_eval_time': 1434.6702933311462, 'accumulated_logging_time': 3.158001184463501, 'global_step': 120426, 'preemption_count': 0}), (121934, {'train/accuracy': 0.76664137840271, 'train/loss': 0.8641605377197266, 'validation/accuracy': 0.6898999810218811, 'validation/loss': 1.2518686056137085, 'validation/num_examples': 50000, 'test/accuracy': 0.5608000159263611, 'test/loss': 1.9467543363571167, 'test/num_examples': 10000, 'score': 41353.42318153381, 'total_duration': 42813.22805428505, 'accumulated_submission_time': 41353.42318153381, 'accumulated_eval_time': 1452.312379360199, 'accumulated_logging_time': 3.207282543182373, 'global_step': 121934, 'preemption_count': 0}), (123442, {'train/accuracy': 0.7724210619926453, 'train/loss': 0.8580043315887451, 'validation/accuracy': 0.6927599906921387, 'validation/loss': 1.2386715412139893, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9441684484481812, 'test/num_examples': 10000, 'score': 41863.442735910416, 'total_duration': 43341.018417835236, 'accumulated_submission_time': 41863.442735910416, 'accumulated_eval_time': 1469.9726836681366, 'accumulated_logging_time': 3.264508008956909, 'global_step': 123442, 'preemption_count': 0}), (124950, {'train/accuracy': 0.7725605964660645, 'train/loss': 0.8497375845909119, 'validation/accuracy': 0.6963599920272827, 'validation/loss': 1.2183761596679688, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9078247547149658, 'test/num_examples': 10000, 'score': 42373.48074388504, 'total_duration': 43869.59917449951, 'accumulated_submission_time': 42373.48074388504, 'accumulated_eval_time': 1488.4095180034637, 'accumulated_logging_time': 3.31595516204834, 'global_step': 124950, 'preemption_count': 0}), (126458, {'train/accuracy': 0.8109255433082581, 'train/loss': 0.6936679482460022, 'validation/accuracy': 0.7005800008773804, 'validation/loss': 1.2135677337646484, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.912022590637207, 'test/num_examples': 10000, 'score': 42883.66209101677, 'total_duration': 44397.662073135376, 'accumulated_submission_time': 42883.66209101677, 'accumulated_eval_time': 1506.1870160102844, 'accumulated_logging_time': 3.367685079574585, 'global_step': 126458, 'preemption_count': 0}), (127966, {'train/accuracy': 0.7948222160339355, 'train/loss': 0.7559593319892883, 'validation/accuracy': 0.7019599676132202, 'validation/loss': 1.1997932195663452, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.9222203493118286, 'test/num_examples': 10000, 'score': 43393.808292627335, 'total_duration': 44925.48057103157, 'accumulated_submission_time': 43393.808292627335, 'accumulated_eval_time': 1523.7546339035034, 'accumulated_logging_time': 3.41774845123291, 'global_step': 127966, 'preemption_count': 0}), (129474, {'train/accuracy': 0.7926099896430969, 'train/loss': 0.7674739360809326, 'validation/accuracy': 0.70305997133255, 'validation/loss': 1.1998165845870972, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 1.9009696245193481, 'test/num_examples': 10000, 'score': 43903.809019088745, 'total_duration': 45453.370572805405, 'accumulated_submission_time': 43903.809019088745, 'accumulated_eval_time': 1541.539691209793, 'accumulated_logging_time': 3.468662738800049, 'global_step': 129474, 'preemption_count': 0}), (130983, {'train/accuracy': 0.7925103306770325, 'train/loss': 0.7734469175338745, 'validation/accuracy': 0.7093999981880188, 'validation/loss': 1.1662369966506958, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 1.8419197797775269, 'test/num_examples': 10000, 'score': 44413.95647978783, 'total_duration': 45981.24368643761, 'accumulated_submission_time': 44413.95647978783, 'accumulated_eval_time': 1559.1616296768188, 'accumulated_logging_time': 3.518587350845337, 'global_step': 130983, 'preemption_count': 0}), (132491, {'train/accuracy': 0.7926897406578064, 'train/loss': 0.7695133090019226, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.17943274974823, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8631477355957031, 'test/num_examples': 10000, 'score': 44924.015117406845, 'total_duration': 46509.158210754395, 'accumulated_submission_time': 44924.015117406845, 'accumulated_eval_time': 1576.9079988002777, 'accumulated_logging_time': 3.5731399059295654, 'global_step': 132491, 'preemption_count': 0}), (133999, {'train/accuracy': 0.7933274507522583, 'train/loss': 0.7550987601280212, 'validation/accuracy': 0.711899995803833, 'validation/loss': 1.1559367179870605, 'validation/num_examples': 50000, 'test/accuracy': 0.5849000215530396, 'test/loss': 1.8598921298980713, 'test/num_examples': 10000, 'score': 45434.115975379944, 'total_duration': 47037.14133429527, 'accumulated_submission_time': 45434.115975379944, 'accumulated_eval_time': 1594.682165145874, 'accumulated_logging_time': 3.6277785301208496, 'global_step': 133999, 'preemption_count': 0}), (135507, {'train/accuracy': 0.8343032598495483, 'train/loss': 0.6008027791976929, 'validation/accuracy': 0.7145000100135803, 'validation/loss': 1.1467500925064087, 'validation/num_examples': 50000, 'test/accuracy': 0.591200053691864, 'test/loss': 1.8425389528274536, 'test/num_examples': 10000, 'score': 45944.17465925217, 'total_duration': 47564.80839204788, 'accumulated_submission_time': 45944.17465925217, 'accumulated_eval_time': 1612.1870589256287, 'accumulated_logging_time': 3.67779278755188, 'global_step': 135507, 'preemption_count': 0}), (137013, {'train/accuracy': 0.8226243257522583, 'train/loss': 0.6424916386604309, 'validation/accuracy': 0.7174199819564819, 'validation/loss': 1.132103443145752, 'validation/num_examples': 50000, 'test/accuracy': 0.5916000008583069, 'test/loss': 1.8219414949417114, 'test/num_examples': 10000, 'score': 46454.21942257881, 'total_duration': 48092.49197125435, 'accumulated_submission_time': 46454.21942257881, 'accumulated_eval_time': 1629.722366809845, 'accumulated_logging_time': 3.7293410301208496, 'global_step': 137013, 'preemption_count': 0}), (138521, {'train/accuracy': 0.8116230964660645, 'train/loss': 0.6831390261650085, 'validation/accuracy': 0.7134799957275391, 'validation/loss': 1.1457061767578125, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.8496946096420288, 'test/num_examples': 10000, 'score': 46964.32555747032, 'total_duration': 48620.41685676575, 'accumulated_submission_time': 46964.32555747032, 'accumulated_eval_time': 1647.4365646839142, 'accumulated_logging_time': 3.780620813369751, 'global_step': 138521, 'preemption_count': 0}), (140029, {'train/accuracy': 0.8157086968421936, 'train/loss': 0.6734943389892578, 'validation/accuracy': 0.7203199863433838, 'validation/loss': 1.1174829006195068, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.816175103187561, 'test/num_examples': 10000, 'score': 47474.33515691757, 'total_duration': 49147.82697439194, 'accumulated_submission_time': 47474.33515691757, 'accumulated_eval_time': 1664.7287590503693, 'accumulated_logging_time': 3.8349015712738037, 'global_step': 140029, 'preemption_count': 0}), (141537, {'train/accuracy': 0.8167450428009033, 'train/loss': 0.6674581170082092, 'validation/accuracy': 0.7184199690818787, 'validation/loss': 1.126118779182434, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.8183460235595703, 'test/num_examples': 10000, 'score': 47984.39821815491, 'total_duration': 49675.356055021286, 'accumulated_submission_time': 47984.39821815491, 'accumulated_eval_time': 1682.08682847023, 'accumulated_logging_time': 3.8889708518981934, 'global_step': 141537, 'preemption_count': 0}), (143045, {'train/accuracy': 0.8122807741165161, 'train/loss': 0.6797454953193665, 'validation/accuracy': 0.7173999547958374, 'validation/loss': 1.137130618095398, 'validation/num_examples': 50000, 'test/accuracy': 0.5925000309944153, 'test/loss': 1.8193696737289429, 'test/num_examples': 10000, 'score': 48494.36148881912, 'total_duration': 50203.234813690186, 'accumulated_submission_time': 48494.36148881912, 'accumulated_eval_time': 1699.8958258628845, 'accumulated_logging_time': 3.9439799785614014, 'global_step': 143045, 'preemption_count': 0}), (144553, {'train/accuracy': 0.8585180044174194, 'train/loss': 0.5102043747901917, 'validation/accuracy': 0.7265799641609192, 'validation/loss': 1.0943148136138916, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.7970024347305298, 'test/num_examples': 10000, 'score': 49004.41987133026, 'total_duration': 50730.99198412895, 'accumulated_submission_time': 49004.41987133026, 'accumulated_eval_time': 1717.4889857769012, 'accumulated_logging_time': 3.996210813522339, 'global_step': 144553, 'preemption_count': 0}), (146060, {'train/accuracy': 0.8483139276504517, 'train/loss': 0.5484762787818909, 'validation/accuracy': 0.729420006275177, 'validation/loss': 1.085327386856079, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.7772631645202637, 'test/num_examples': 10000, 'score': 49514.39406490326, 'total_duration': 51258.76909947395, 'accumulated_submission_time': 49514.39406490326, 'accumulated_eval_time': 1735.1880152225494, 'accumulated_logging_time': 4.048548221588135, 'global_step': 146060, 'preemption_count': 0}), (147568, {'train/accuracy': 0.8409398794174194, 'train/loss': 0.5652621984481812, 'validation/accuracy': 0.7280199527740479, 'validation/loss': 1.0854853391647339, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.7862012386322021, 'test/num_examples': 10000, 'score': 50024.444157123566, 'total_duration': 51786.662527799606, 'accumulated_submission_time': 50024.444157123566, 'accumulated_eval_time': 1752.9253692626953, 'accumulated_logging_time': 4.101878881454468, 'global_step': 147568, 'preemption_count': 0}), (149075, {'train/accuracy': 0.8464604616165161, 'train/loss': 0.5500763654708862, 'validation/accuracy': 0.7320799827575684, 'validation/loss': 1.0699559450149536, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.7584848403930664, 'test/num_examples': 10000, 'score': 50534.34899163246, 'total_duration': 52314.105689525604, 'accumulated_submission_time': 50534.34899163246, 'accumulated_eval_time': 1770.3562195301056, 'accumulated_logging_time': 4.158680438995361, 'global_step': 149075, 'preemption_count': 0}), (150584, {'train/accuracy': 0.8461614847183228, 'train/loss': 0.5413140058517456, 'validation/accuracy': 0.7317999601364136, 'validation/loss': 1.075062870979309, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.7552403211593628, 'test/num_examples': 10000, 'score': 51044.57275414467, 'total_duration': 52842.02047371864, 'accumulated_submission_time': 51044.57275414467, 'accumulated_eval_time': 1787.9360961914062, 'accumulated_logging_time': 4.216421842575073, 'global_step': 150584, 'preemption_count': 0}), (152093, {'train/accuracy': 0.8462013602256775, 'train/loss': 0.5413247346878052, 'validation/accuracy': 0.737339973449707, 'validation/loss': 1.056695580482483, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.747403621673584, 'test/num_examples': 10000, 'score': 51554.790496110916, 'total_duration': 53369.720680475235, 'accumulated_submission_time': 51554.790496110916, 'accumulated_eval_time': 1805.3098711967468, 'accumulated_logging_time': 4.273014545440674, 'global_step': 152093, 'preemption_count': 0}), (153601, {'train/accuracy': 0.8833306431770325, 'train/loss': 0.42076045274734497, 'validation/accuracy': 0.7371399998664856, 'validation/loss': 1.053296685218811, 'validation/num_examples': 50000, 'test/accuracy': 0.612000048160553, 'test/loss': 1.7551151514053345, 'test/num_examples': 10000, 'score': 52065.01314878464, 'total_duration': 53897.644444942474, 'accumulated_submission_time': 52065.01314878464, 'accumulated_eval_time': 1822.9050323963165, 'accumulated_logging_time': 4.3264100551605225, 'global_step': 153601, 'preemption_count': 0}), (155109, {'train/accuracy': 0.8740832209587097, 'train/loss': 0.44152387976646423, 'validation/accuracy': 0.7404199838638306, 'validation/loss': 1.0409177541732788, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.7276402711868286, 'test/num_examples': 10000, 'score': 52575.09206676483, 'total_duration': 54425.57376766205, 'accumulated_submission_time': 52575.09206676483, 'accumulated_eval_time': 1840.645209312439, 'accumulated_logging_time': 4.384929418563843, 'global_step': 155109, 'preemption_count': 0}), (156617, {'train/accuracy': 0.8761957883834839, 'train/loss': 0.4427817463874817, 'validation/accuracy': 0.7436800003051758, 'validation/loss': 1.0364357233047485, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.732064127922058, 'test/num_examples': 10000, 'score': 53084.98903775215, 'total_duration': 54953.56215620041, 'accumulated_submission_time': 53084.98903775215, 'accumulated_eval_time': 1858.6266074180603, 'accumulated_logging_time': 4.439204216003418, 'global_step': 156617, 'preemption_count': 0}), (158125, {'train/accuracy': 0.8801418542861938, 'train/loss': 0.43101683259010315, 'validation/accuracy': 0.7454400062561035, 'validation/loss': 1.0232517719268799, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.7091346979141235, 'test/num_examples': 10000, 'score': 53594.90204691887, 'total_duration': 55481.23839688301, 'accumulated_submission_time': 53594.90204691887, 'accumulated_eval_time': 1876.2782986164093, 'accumulated_logging_time': 4.496425151824951, 'global_step': 158125, 'preemption_count': 0}), (159633, {'train/accuracy': 0.8810586333274841, 'train/loss': 0.4157596826553345, 'validation/accuracy': 0.7454599738121033, 'validation/loss': 1.0245261192321777, 'validation/num_examples': 50000, 'test/accuracy': 0.6222000122070312, 'test/loss': 1.717750072479248, 'test/num_examples': 10000, 'score': 54104.91116786003, 'total_duration': 56009.05380535126, 'accumulated_submission_time': 54104.91116786003, 'accumulated_eval_time': 1893.9733023643494, 'accumulated_logging_time': 4.5553436279296875, 'global_step': 159633, 'preemption_count': 0}), (161141, {'train/accuracy': 0.8859813213348389, 'train/loss': 0.40513113141059875, 'validation/accuracy': 0.7473999857902527, 'validation/loss': 1.0201231241226196, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.703717827796936, 'test/num_examples': 10000, 'score': 54615.10005736351, 'total_duration': 56537.24241113663, 'accumulated_submission_time': 54615.10005736351, 'accumulated_eval_time': 1911.8665721416473, 'accumulated_logging_time': 4.608543395996094, 'global_step': 161141, 'preemption_count': 0}), (162649, {'train/accuracy': 0.9079041481018066, 'train/loss': 0.3245623707771301, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.0081355571746826, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.6878162622451782, 'test/num_examples': 10000, 'score': 55125.14937853813, 'total_duration': 57065.24767613411, 'accumulated_submission_time': 55125.14937853813, 'accumulated_eval_time': 1929.71240234375, 'accumulated_logging_time': 4.6652233600616455, 'global_step': 162649, 'preemption_count': 0}), (164157, {'train/accuracy': 0.9044961333274841, 'train/loss': 0.33790427446365356, 'validation/accuracy': 0.7519399523735046, 'validation/loss': 1.0083187818527222, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.704638957977295, 'test/num_examples': 10000, 'score': 55635.22860836983, 'total_duration': 57592.82874393463, 'accumulated_submission_time': 55635.22860836983, 'accumulated_eval_time': 1947.1010339260101, 'accumulated_logging_time': 4.725534439086914, 'global_step': 164157, 'preemption_count': 0}), (165665, {'train/accuracy': 0.9044164419174194, 'train/loss': 0.3358021378517151, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 0.9994009137153625, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.6860158443450928, 'test/num_examples': 10000, 'score': 56145.23153233528, 'total_duration': 58120.586990594864, 'accumulated_submission_time': 56145.23153233528, 'accumulated_eval_time': 1964.745409488678, 'accumulated_logging_time': 4.782070636749268, 'global_step': 165665, 'preemption_count': 0}), (167173, {'train/accuracy': 0.9084821343421936, 'train/loss': 0.3253271281719208, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 0.9877996444702148, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.6967089176177979, 'test/num_examples': 10000, 'score': 56655.2565972805, 'total_duration': 58648.39890432358, 'accumulated_submission_time': 56655.2565972805, 'accumulated_eval_time': 1982.4217264652252, 'accumulated_logging_time': 4.839404821395874, 'global_step': 167173, 'preemption_count': 0}), (168680, {'train/accuracy': 0.9084821343421936, 'train/loss': 0.3189391791820526, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 0.9814531207084656, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.6696397066116333, 'test/num_examples': 10000, 'score': 57165.24439907074, 'total_duration': 59176.142056941986, 'accumulated_submission_time': 57165.24439907074, 'accumulated_eval_time': 2000.0688755512238, 'accumulated_logging_time': 4.895292043685913, 'global_step': 168680, 'preemption_count': 0}), (170187, {'train/accuracy': 0.9106544852256775, 'train/loss': 0.3087378144264221, 'validation/accuracy': 0.7574599981307983, 'validation/loss': 0.9799865484237671, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.6687065362930298, 'test/num_examples': 10000, 'score': 57675.29413366318, 'total_duration': 59703.755058288574, 'accumulated_submission_time': 57675.29413366318, 'accumulated_eval_time': 2017.519765138626, 'accumulated_logging_time': 4.954912900924683, 'global_step': 170187, 'preemption_count': 0}), (171695, {'train/accuracy': 0.924226701259613, 'train/loss': 0.2707524597644806, 'validation/accuracy': 0.757099986076355, 'validation/loss': 0.9797187447547913, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.6713703870773315, 'test/num_examples': 10000, 'score': 58185.4462184906, 'total_duration': 60231.59297156334, 'accumulated_submission_time': 58185.4462184906, 'accumulated_eval_time': 2035.09224319458, 'accumulated_logging_time': 5.01508355140686, 'global_step': 171695, 'preemption_count': 0}), (173202, {'train/accuracy': 0.9278340339660645, 'train/loss': 0.26252445578575134, 'validation/accuracy': 0.758899986743927, 'validation/loss': 0.9757165312767029, 'validation/num_examples': 50000, 'test/accuracy': 0.6396000385284424, 'test/loss': 1.6715961694717407, 'test/num_examples': 10000, 'score': 58695.36663389206, 'total_duration': 60759.16719126701, 'accumulated_submission_time': 58695.36663389206, 'accumulated_eval_time': 2052.647294998169, 'accumulated_logging_time': 5.063016414642334, 'global_step': 173202, 'preemption_count': 0}), (174710, {'train/accuracy': 0.9278539419174194, 'train/loss': 0.2555326819419861, 'validation/accuracy': 0.7615399956703186, 'validation/loss': 0.9703835844993591, 'validation/num_examples': 50000, 'test/accuracy': 0.6392000317573547, 'test/loss': 1.6742520332336426, 'test/num_examples': 10000, 'score': 59205.45822405815, 'total_duration': 61286.8631067276, 'accumulated_submission_time': 59205.45822405815, 'accumulated_eval_time': 2070.1429677009583, 'accumulated_logging_time': 5.118837833404541, 'global_step': 174710, 'preemption_count': 0}), (176218, {'train/accuracy': 0.9264189600944519, 'train/loss': 0.2620689570903778, 'validation/accuracy': 0.7624199986457825, 'validation/loss': 0.9670127034187317, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.6667834520339966, 'test/num_examples': 10000, 'score': 59715.56953692436, 'total_duration': 61814.30304598808, 'accumulated_submission_time': 59715.56953692436, 'accumulated_eval_time': 2087.3609421253204, 'accumulated_logging_time': 5.175340414047241, 'global_step': 176218, 'preemption_count': 0}), (177726, {'train/accuracy': 0.927156388759613, 'train/loss': 0.25953570008277893, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 0.9643649458885193, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.6655884981155396, 'test/num_examples': 10000, 'score': 60225.75075531006, 'total_duration': 62342.37773346901, 'accumulated_submission_time': 60225.75075531006, 'accumulated_eval_time': 2105.146994113922, 'accumulated_logging_time': 5.229976654052734, 'global_step': 177726, 'preemption_count': 0}), (179234, {'train/accuracy': 0.9292888641357422, 'train/loss': 0.2532636523246765, 'validation/accuracy': 0.7625399827957153, 'validation/loss': 0.9626427888870239, 'validation/num_examples': 50000, 'test/accuracy': 0.6411000490188599, 'test/loss': 1.6631746292114258, 'test/num_examples': 10000, 'score': 60735.92390227318, 'total_duration': 62870.22953939438, 'accumulated_submission_time': 60735.92390227318, 'accumulated_eval_time': 2122.7120769023895, 'accumulated_logging_time': 5.291700601577759, 'global_step': 179234, 'preemption_count': 0}), (180742, {'train/accuracy': 0.9323381781578064, 'train/loss': 0.2413637936115265, 'validation/accuracy': 0.763480007648468, 'validation/loss': 0.9599735736846924, 'validation/num_examples': 50000, 'test/accuracy': 0.6431000232696533, 'test/loss': 1.6595665216445923, 'test/num_examples': 10000, 'score': 61246.10619521141, 'total_duration': 63398.17158651352, 'accumulated_submission_time': 61246.10619521141, 'accumulated_eval_time': 2140.3513338565826, 'accumulated_logging_time': 5.3609983921051025, 'global_step': 180742, 'preemption_count': 0}), (182249, {'train/accuracy': 0.9336535334587097, 'train/loss': 0.2402791529893875, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 0.9584994316101074, 'validation/num_examples': 50000, 'test/accuracy': 0.6432000398635864, 'test/loss': 1.664105772972107, 'test/num_examples': 10000, 'score': 61756.06179380417, 'total_duration': 63925.83683180809, 'accumulated_submission_time': 61756.06179380417, 'accumulated_eval_time': 2157.9516232013702, 'accumulated_logging_time': 5.419076681137085, 'global_step': 182249, 'preemption_count': 0}), (183757, {'train/accuracy': 0.935566782951355, 'train/loss': 0.24063031375408173, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 0.9566633105278015, 'validation/num_examples': 50000, 'test/accuracy': 0.6440000534057617, 'test/loss': 1.6585973501205444, 'test/num_examples': 10000, 'score': 62266.209573984146, 'total_duration': 64453.770429611206, 'accumulated_submission_time': 62266.209573984146, 'accumulated_eval_time': 2175.6238107681274, 'accumulated_logging_time': 5.481184005737305, 'global_step': 183757, 'preemption_count': 0}), (185265, {'train/accuracy': 0.9326968789100647, 'train/loss': 0.24263811111450195, 'validation/accuracy': 0.7636399865150452, 'validation/loss': 0.9573678374290466, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.6598241329193115, 'test/num_examples': 10000, 'score': 62776.36577916145, 'total_duration': 64981.38829827309, 'accumulated_submission_time': 62776.36577916145, 'accumulated_eval_time': 2192.9691956043243, 'accumulated_logging_time': 5.543259382247925, 'global_step': 185265, 'preemption_count': 0})], 'global_step': 185951}
I0129 01:24:36.995028 140027215431488 submission_runner.py:586] Timing: 63008.286588430405
I0129 01:24:36.995095 140027215431488 submission_runner.py:588] Total number of evals: 124
I0129 01:24:36.995136 140027215431488 submission_runner.py:589] ====================
I0129 01:24:36.995179 140027215431488 submission_runner.py:542] Using RNG seed 3078694106
I0129 01:24:36.996668 140027215431488 submission_runner.py:551] --- Tuning run 4/5 ---
I0129 01:24:36.996782 140027215431488 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4.
I0129 01:24:36.998093 140027215431488 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4/hparams.json.
I0129 01:24:36.998876 140027215431488 submission_runner.py:206] Initializing dataset.
I0129 01:24:37.007450 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 01:24:37.017656 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 01:24:37.819596 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 01:24:38.059052 140027215431488 submission_runner.py:213] Initializing model.
I0129 01:24:43.985626 140027215431488 submission_runner.py:255] Initializing optimizer.
I0129 01:24:44.375454 140027215431488 submission_runner.py:262] Initializing metrics bundle.
I0129 01:24:44.375636 140027215431488 submission_runner.py:280] Initializing checkpoint and logger.
I0129 01:24:44.389873 140027215431488 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0129 01:24:44.390013 140027215431488 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 01:24:55.614503 140027215431488 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 01:25:06.654681 140027215431488 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4/flags_0.json.
I0129 01:25:06.660128 140027215431488 submission_runner.py:314] Starting training loop.
I0129 01:25:40.429540 139865760950016 logging_writer.py:48] [0] global_step=0, grad_norm=0.6595183610916138, loss=6.916248798370361
I0129 01:25:40.444804 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:25:46.655120 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:25:55.407727 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:25:58.034598 140027215431488 submission_runner.py:408] Time since start: 51.37s, 	Step: 1, 	{'train/accuracy': 0.0007772640092298388, 'train/loss': 6.911434173583984, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 33.78456735610962, 'total_duration': 51.374412059783936, 'accumulated_submission_time': 33.78456735610962, 'accumulated_eval_time': 17.589736700057983, 'accumulated_logging_time': 0}
I0129 01:25:58.043840 139865232500480 logging_writer.py:48] [1] accumulated_eval_time=17.589737, accumulated_logging_time=0, accumulated_submission_time=33.784567, global_step=1, preemption_count=0, score=33.784567, test/accuracy=0.001300, test/loss=6.911728, test/num_examples=10000, total_duration=51.374412, train/accuracy=0.000777, train/loss=6.911434, validation/accuracy=0.000680, validation/loss=6.912051, validation/num_examples=50000
I0129 01:26:32.017569 139865240893184 logging_writer.py:48] [100] global_step=100, grad_norm=0.7728391885757446, loss=6.657430648803711
I0129 01:27:06.021581 139865232500480 logging_writer.py:48] [200] global_step=200, grad_norm=0.9639778137207031, loss=6.270465850830078
I0129 01:27:40.057331 139865240893184 logging_writer.py:48] [300] global_step=300, grad_norm=4.097598075866699, loss=5.949130535125732
I0129 01:28:14.123650 139865232500480 logging_writer.py:48] [400] global_step=400, grad_norm=3.0694587230682373, loss=5.611301898956299
I0129 01:28:48.161646 139865240893184 logging_writer.py:48] [500] global_step=500, grad_norm=5.152012348175049, loss=5.506866455078125
I0129 01:29:22.283250 139865232500480 logging_writer.py:48] [600] global_step=600, grad_norm=3.902653455734253, loss=5.286532878875732
I0129 01:29:56.347323 139865240893184 logging_writer.py:48] [700] global_step=700, grad_norm=3.4540302753448486, loss=5.225470542907715
I0129 01:30:30.394547 139865232500480 logging_writer.py:48] [800] global_step=800, grad_norm=5.173513889312744, loss=5.19395637512207
I0129 01:31:04.432528 139865240893184 logging_writer.py:48] [900] global_step=900, grad_norm=4.661944389343262, loss=4.893309593200684
I0129 01:31:38.520552 139865232500480 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.643945693969727, loss=4.715913772583008
I0129 01:32:12.553319 139865240893184 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.9989495277404785, loss=4.660079479217529
I0129 01:32:46.610865 139865232500480 logging_writer.py:48] [1200] global_step=1200, grad_norm=4.226677417755127, loss=4.385209560394287
I0129 01:33:20.675536 139865240893184 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.2668089866638184, loss=4.387207984924316
I0129 01:33:54.694587 139865232500480 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.483505964279175, loss=4.137399673461914
I0129 01:34:28.198140 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:34:34.493241 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:34:43.220500 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:34:45.886827 140027215431488 submission_runner.py:408] Time since start: 579.23s, 	Step: 1500, 	{'train/accuracy': 0.22211813926696777, 'train/loss': 3.8356406688690186, 'validation/accuracy': 0.2069999873638153, 'validation/loss': 3.960334539413452, 'validation/num_examples': 50000, 'test/accuracy': 0.15680000185966492, 'test/loss': 4.447728633880615, 'test/num_examples': 10000, 'score': 543.8763875961304, 'total_duration': 579.2266387939453, 'accumulated_submission_time': 543.8763875961304, 'accumulated_eval_time': 35.278390645980835, 'accumulated_logging_time': 0.019817829132080078}
I0129 01:34:45.908863 139865232500480 logging_writer.py:48] [1500] accumulated_eval_time=35.278391, accumulated_logging_time=0.019818, accumulated_submission_time=543.876388, global_step=1500, preemption_count=0, score=543.876388, test/accuracy=0.156800, test/loss=4.447729, test/num_examples=10000, total_duration=579.226639, train/accuracy=0.222118, train/loss=3.835641, validation/accuracy=0.207000, validation/loss=3.960335, validation/num_examples=50000
I0129 01:34:46.269257 139865240893184 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4561779499053955, loss=4.156347274780273
I0129 01:35:20.256254 139865232500480 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.624830722808838, loss=3.9620182514190674
I0129 01:35:54.353847 139865240893184 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8491352796554565, loss=3.910888433456421
I0129 01:36:28.353990 139865232500480 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7104265689849854, loss=3.8140625953674316
I0129 01:37:02.377189 139865240893184 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.3654370307922363, loss=3.8650059700012207
I0129 01:37:36.374211 139865232500480 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6789199113845825, loss=3.613668918609619
I0129 01:38:10.386412 139865240893184 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.5820014476776123, loss=3.388026714324951
I0129 01:38:44.363668 139865232500480 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.9339687824249268, loss=3.4941091537475586
I0129 01:39:18.364729 139865240893184 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.6066499948501587, loss=3.4178149700164795
I0129 01:39:52.342303 139865232500480 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.7239843606948853, loss=3.364912986755371
I0129 01:40:26.327599 139865240893184 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.149001121520996, loss=3.2485291957855225
I0129 01:41:00.308360 139865232500480 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.31614351272583, loss=3.207876205444336
I0129 01:41:34.237369 139865240893184 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2875128984451294, loss=3.37770414352417
I0129 01:42:08.261727 139865232500480 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2037094831466675, loss=3.1407129764556885
I0129 01:42:42.219752 139865240893184 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1408294439315796, loss=3.089859962463379
I0129 01:43:16.178005 139865232500480 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.3762242794036865, loss=3.138617992401123
I0129 01:43:16.186030 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:43:22.520556 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:43:31.219270 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:43:34.119723 140027215431488 submission_runner.py:408] Time since start: 1107.46s, 	Step: 3001, 	{'train/accuracy': 0.33910635113716125, 'train/loss': 3.0905704498291016, 'validation/accuracy': 0.3193399906158447, 'validation/loss': 3.2351529598236084, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 3.8936009407043457, 'test/num_examples': 10000, 'score': 1054.0921666622162, 'total_duration': 1107.459528684616, 'accumulated_submission_time': 1054.0921666622162, 'accumulated_eval_time': 53.212013721466064, 'accumulated_logging_time': 0.05116891860961914}
I0129 01:43:34.140110 139866180368128 logging_writer.py:48] [3001] accumulated_eval_time=53.212014, accumulated_logging_time=0.051169, accumulated_submission_time=1054.092167, global_step=3001, preemption_count=0, score=1054.092167, test/accuracy=0.245700, test/loss=3.893601, test/num_examples=10000, total_duration=1107.459529, train/accuracy=0.339106, train/loss=3.090570, validation/accuracy=0.319340, validation/loss=3.235153, validation/num_examples=50000
I0129 01:44:08.079696 139866188760832 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1789216995239258, loss=3.0518715381622314
I0129 01:44:42.007977 139866180368128 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1696014404296875, loss=3.028724193572998
I0129 01:45:15.968278 139866188760832 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.188464879989624, loss=3.247199535369873
I0129 01:45:49.911949 139866180368128 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1070592403411865, loss=3.1234521865844727
I0129 01:46:23.865837 139866188760832 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9610374569892883, loss=3.122365951538086
I0129 01:46:57.817174 139866180368128 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.2151154279708862, loss=3.0240933895111084
I0129 01:47:31.764208 139866188760832 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9146728515625, loss=2.858419418334961
I0129 01:48:05.730708 139866180368128 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.938973069190979, loss=2.9578630924224854
I0129 01:48:39.757088 139866188760832 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.950302243232727, loss=2.88395094871521
I0129 01:49:13.707850 139866180368128 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9553036093711853, loss=2.950784921646118
I0129 01:49:47.663997 139866188760832 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9909541606903076, loss=2.8193821907043457
I0129 01:50:21.620493 139866180368128 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.077216386795044, loss=2.7409589290618896
I0129 01:50:55.545579 139866188760832 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.036068320274353, loss=2.8799068927764893
I0129 01:51:29.494111 139866180368128 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9693871736526489, loss=2.8935279846191406
I0129 01:52:03.451448 139866188760832 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0536494255065918, loss=2.950266122817993
I0129 01:52:04.271025 140027215431488 spec.py:321] Evaluating on the training split.
I0129 01:52:10.515583 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 01:52:19.393871 140027215431488 spec.py:349] Evaluating on the test split.
I0129 01:52:22.054288 140027215431488 submission_runner.py:408] Time since start: 1635.39s, 	Step: 4504, 	{'train/accuracy': 0.34586256742477417, 'train/loss': 3.1005280017852783, 'validation/accuracy': 0.3310000002384186, 'validation/loss': 3.232429265975952, 'validation/num_examples': 50000, 'test/accuracy': 0.24320000410079956, 'test/loss': 4.029863357543945, 'test/num_examples': 10000, 'score': 1564.1626377105713, 'total_duration': 1635.3940970897675, 'accumulated_submission_time': 1564.1626377105713, 'accumulated_eval_time': 70.99523687362671, 'accumulated_logging_time': 0.08113479614257812}
I0129 01:52:22.073812 139865224107776 logging_writer.py:48] [4504] accumulated_eval_time=70.995237, accumulated_logging_time=0.081135, accumulated_submission_time=1564.162638, global_step=4504, preemption_count=0, score=1564.162638, test/accuracy=0.243200, test/loss=4.029863, test/num_examples=10000, total_duration=1635.394097, train/accuracy=0.345863, train/loss=3.100528, validation/accuracy=0.331000, validation/loss=3.232429, validation/num_examples=50000
I0129 01:52:54.960284 139865232500480 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8514507412910461, loss=2.7478067874908447
I0129 01:53:28.907992 139865224107776 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.1343072652816772, loss=2.7844886779785156
I0129 01:54:02.829256 139865232500480 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.895152747631073, loss=2.9111905097961426
I0129 01:54:36.751738 139865224107776 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.2163771390914917, loss=2.7505970001220703
I0129 01:55:10.876867 139865232500480 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8372358679771423, loss=2.8880014419555664
I0129 01:55:44.785964 139865224107776 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.0370644330978394, loss=2.74845027923584
I0129 01:56:18.714133 139865232500480 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9540496468544006, loss=2.831105947494507
I0129 01:56:52.694345 139865224107776 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.821829080581665, loss=2.621466636657715
I0129 01:57:26.624390 139865232500480 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.877177357673645, loss=2.64135479927063
I0129 01:58:00.555571 139865224107776 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.993018388748169, loss=2.6104209423065186
I0129 01:58:34.481176 139865232500480 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.0381687879562378, loss=2.616030216217041
I0129 01:59:08.370248 139865224107776 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9840512275695801, loss=2.5745315551757812
I0129 01:59:42.302567 139865232500480 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9328573346138, loss=2.545203924179077
I0129 02:00:16.217053 139865224107776 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8826257586479187, loss=2.7341294288635254
I0129 02:00:50.132240 139865232500480 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.2327549457550049, loss=2.5454161167144775
I0129 02:00:52.315813 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:00:58.619535 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:01:07.414594 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:01:10.021956 140027215431488 submission_runner.py:408] Time since start: 2163.36s, 	Step: 6008, 	{'train/accuracy': 0.37824854254722595, 'train/loss': 2.8953018188476562, 'validation/accuracy': 0.33563998341560364, 'validation/loss': 3.1820812225341797, 'validation/num_examples': 50000, 'test/accuracy': 0.2597000002861023, 'test/loss': 3.796529531478882, 'test/num_examples': 10000, 'score': 2074.3404870033264, 'total_duration': 2163.361767053604, 'accumulated_submission_time': 2074.3404870033264, 'accumulated_eval_time': 88.70134115219116, 'accumulated_logging_time': 0.11190533638000488}
I0129 02:01:10.040435 139865232500480 logging_writer.py:48] [6008] accumulated_eval_time=88.701341, accumulated_logging_time=0.111905, accumulated_submission_time=2074.340487, global_step=6008, preemption_count=0, score=2074.340487, test/accuracy=0.259700, test/loss=3.796530, test/num_examples=10000, total_duration=2163.361767, train/accuracy=0.378249, train/loss=2.895302, validation/accuracy=0.335640, validation/loss=3.182081, validation/num_examples=50000
I0129 02:01:41.690464 139866163582720 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.0347154140472412, loss=2.6073362827301025
I0129 02:02:15.567766 139865232500480 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8809722661972046, loss=2.6102240085601807
I0129 02:02:49.462996 139866163582720 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0261714458465576, loss=2.653059959411621
I0129 02:03:23.377622 139865232500480 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.0912152528762817, loss=2.593353509902954
I0129 02:03:57.272705 139866163582720 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9150018095970154, loss=2.6049556732177734
I0129 02:04:31.204124 139865232500480 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.0242797136306763, loss=2.4522409439086914
I0129 02:05:05.071001 139866163582720 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.1925134658813477, loss=2.570984363555908
I0129 02:05:38.950158 139865232500480 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9013592600822449, loss=2.532853126525879
I0129 02:06:12.886745 139866163582720 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9708198308944702, loss=2.5434117317199707
I0129 02:06:46.757988 139865232500480 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.064671277999878, loss=2.6335155963897705
I0129 02:07:20.700270 139866163582720 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9274951219558716, loss=2.536630868911743
I0129 02:07:54.645273 139865232500480 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9061469435691833, loss=2.465376377105713
I0129 02:08:28.544782 139866163582720 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9203402400016785, loss=2.5138742923736572
I0129 02:09:02.410600 139865232500480 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0210297107696533, loss=2.387970447540283
I0129 02:09:36.325536 139866163582720 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9652711153030396, loss=2.6446805000305176
I0129 02:09:40.200322 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:09:46.464376 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:09:55.159503 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:09:57.799148 140027215431488 submission_runner.py:408] Time since start: 2691.14s, 	Step: 7513, 	{'train/accuracy': 0.2871890962123871, 'train/loss': 3.6383469104766846, 'validation/accuracy': 0.26170000433921814, 'validation/loss': 3.8255012035369873, 'validation/num_examples': 50000, 'test/accuracy': 0.18610000610351562, 'test/loss': 4.576222896575928, 'test/num_examples': 10000, 'score': 2584.4371135234833, 'total_duration': 2691.138946533203, 'accumulated_submission_time': 2584.4371135234833, 'accumulated_eval_time': 106.30011343955994, 'accumulated_logging_time': 0.1416637897491455}
I0129 02:09:57.817244 139865769342720 logging_writer.py:48] [7513] accumulated_eval_time=106.300113, accumulated_logging_time=0.141664, accumulated_submission_time=2584.437114, global_step=7513, preemption_count=0, score=2584.437114, test/accuracy=0.186100, test/loss=4.576223, test/num_examples=10000, total_duration=2691.138947, train/accuracy=0.287189, train/loss=3.638347, validation/accuracy=0.261700, validation/loss=3.825501, validation/num_examples=50000
I0129 02:10:27.546756 139866180368128 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9698967933654785, loss=2.409508228302002
I0129 02:11:01.443036 139865769342720 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9485242962837219, loss=2.517381191253662
I0129 02:11:35.330041 139866180368128 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.121189832687378, loss=2.443920850753784
I0129 02:12:09.205061 139865769342720 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9868667721748352, loss=2.576249599456787
I0129 02:12:43.098504 139866180368128 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9629639387130737, loss=2.466069221496582
I0129 02:13:16.949267 139865769342720 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9454323649406433, loss=2.523344039916992
I0129 02:13:50.812903 139866180368128 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.1646193265914917, loss=2.4181978702545166
I0129 02:14:24.780535 139865769342720 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9007010459899902, loss=2.4328484535217285
I0129 02:14:58.660554 139866180368128 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.996127724647522, loss=2.5517759323120117
I0129 02:15:32.566672 139865769342720 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9186828136444092, loss=2.6304359436035156
I0129 02:16:06.422846 139866180368128 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0590107440948486, loss=2.50960111618042
I0129 02:16:40.267106 139865769342720 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9544998407363892, loss=2.46171498298645
I0129 02:17:14.159332 139866180368128 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0856212377548218, loss=2.557725667953491
I0129 02:17:48.043783 139865769342720 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0805540084838867, loss=2.3320653438568115
I0129 02:18:21.898769 139866180368128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.9564350247383118, loss=2.640493631362915
I0129 02:18:28.121091 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:18:34.441674 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:18:43.248356 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:18:45.876373 140027215431488 submission_runner.py:408] Time since start: 3219.22s, 	Step: 9020, 	{'train/accuracy': 0.39819833636283875, 'train/loss': 2.802988052368164, 'validation/accuracy': 0.36583998799324036, 'validation/loss': 3.020524501800537, 'validation/num_examples': 50000, 'test/accuracy': 0.27470001578330994, 'test/loss': 3.7215497493743896, 'test/num_examples': 10000, 'score': 3094.679023504257, 'total_duration': 3219.2161860466003, 'accumulated_submission_time': 3094.679023504257, 'accumulated_eval_time': 124.0553548336029, 'accumulated_logging_time': 0.16915607452392578}
I0129 02:18:45.895776 139866163582720 logging_writer.py:48] [9020] accumulated_eval_time=124.055355, accumulated_logging_time=0.169156, accumulated_submission_time=3094.679024, global_step=9020, preemption_count=0, score=3094.679024, test/accuracy=0.274700, test/loss=3.721550, test/num_examples=10000, total_duration=3219.216186, train/accuracy=0.398198, train/loss=2.802988, validation/accuracy=0.365840, validation/loss=3.020525, validation/num_examples=50000
I0129 02:19:13.272604 139866171975424 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9209325313568115, loss=2.4154701232910156
I0129 02:19:47.137256 139866163582720 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9360784292221069, loss=2.5615482330322266
I0129 02:20:20.973034 139866171975424 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.061613917350769, loss=2.4612176418304443
I0129 02:20:54.892567 139866163582720 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9955411553382874, loss=2.3766286373138428
I0129 02:21:28.759534 139866171975424 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9846712350845337, loss=2.4627323150634766
I0129 02:22:02.628295 139866163582720 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.0158783197402954, loss=2.3818225860595703
I0129 02:22:36.471810 139866171975424 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.07988440990448, loss=2.464327335357666
I0129 02:23:10.300006 139866163582720 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9742587208747864, loss=2.417008876800537
I0129 02:23:44.151785 139866171975424 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9952279925346375, loss=2.423720359802246
I0129 02:24:17.989112 139866163582720 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9152403473854065, loss=2.447633981704712
I0129 02:24:51.833120 139866171975424 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.1112509965896606, loss=2.6036734580993652
I0129 02:25:25.657228 139866163582720 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9101141691207886, loss=2.379288673400879
I0129 02:25:59.499412 139866171975424 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0572010278701782, loss=2.4976117610931396
I0129 02:26:33.326970 139866163582720 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9937258958816528, loss=2.4734675884246826
I0129 02:27:07.172420 139866171975424 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0990841388702393, loss=2.4834699630737305
I0129 02:27:15.878673 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:27:22.149305 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:27:30.961935 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:27:34.279030 140027215431488 submission_runner.py:408] Time since start: 3747.62s, 	Step: 10527, 	{'train/accuracy': 0.3869379758834839, 'train/loss': 2.846888542175293, 'validation/accuracy': 0.36423999071121216, 'validation/loss': 3.0101521015167236, 'validation/num_examples': 50000, 'test/accuracy': 0.26900002360343933, 'test/loss': 3.7982981204986572, 'test/num_examples': 10000, 'score': 3604.599910259247, 'total_duration': 3747.6188309192657, 'accumulated_submission_time': 3604.599910259247, 'accumulated_eval_time': 142.4556610584259, 'accumulated_logging_time': 0.19795799255371094}
I0129 02:27:34.295328 139865232500480 logging_writer.py:48] [10527] accumulated_eval_time=142.455661, accumulated_logging_time=0.197958, accumulated_submission_time=3604.599910, global_step=10527, preemption_count=0, score=3604.599910, test/accuracy=0.269000, test/loss=3.798298, test/num_examples=10000, total_duration=3747.618831, train/accuracy=0.386938, train/loss=2.846889, validation/accuracy=0.364240, validation/loss=3.010152, validation/num_examples=50000
I0129 02:27:59.308196 139865240893184 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.0046823024749756, loss=2.327423095703125
I0129 02:28:33.139435 139865232500480 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9867146015167236, loss=2.3862481117248535
I0129 02:29:06.980721 139865240893184 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.0308454036712646, loss=2.393803119659424
I0129 02:29:40.855770 139865232500480 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9819153547286987, loss=2.595850944519043
I0129 02:30:14.727298 139865240893184 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.054992914199829, loss=2.541213274002075
I0129 02:30:48.487636 139865232500480 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9619333148002625, loss=2.296682119369507
I0129 02:31:22.353397 139865240893184 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9296558499336243, loss=2.3443362712860107
I0129 02:31:56.142501 139865232500480 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9881409406661987, loss=2.3672420978546143
I0129 02:32:29.991867 139865240893184 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0451972484588623, loss=2.3891806602478027
I0129 02:33:03.847405 139865232500480 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.0638900995254517, loss=2.493825912475586
I0129 02:33:37.717350 139865240893184 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9882027506828308, loss=2.529120445251465
I0129 02:34:11.611289 139865232500480 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0123263597488403, loss=2.377146005630493
I0129 02:34:45.424379 139865240893184 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.0110762119293213, loss=2.546260356903076
I0129 02:35:19.263071 139865232500480 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8832753896713257, loss=2.460993766784668
I0129 02:35:53.093839 139865240893184 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0685381889343262, loss=2.4670114517211914
I0129 02:36:04.446636 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:36:10.672438 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:36:19.349548 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:36:21.993767 140027215431488 submission_runner.py:408] Time since start: 4275.33s, 	Step: 12035, 	{'train/accuracy': 0.30215638875961304, 'train/loss': 3.548335313796997, 'validation/accuracy': 0.2874999940395355, 'validation/loss': 3.7057454586029053, 'validation/num_examples': 50000, 'test/accuracy': 0.20580001175403595, 'test/loss': 4.551807880401611, 'test/num_examples': 10000, 'score': 4114.688268184662, 'total_duration': 4275.333572149277, 'accumulated_submission_time': 4114.688268184662, 'accumulated_eval_time': 160.00274682044983, 'accumulated_logging_time': 0.2236948013305664}
I0129 02:36:22.015856 139866171975424 logging_writer.py:48] [12035] accumulated_eval_time=160.002747, accumulated_logging_time=0.223695, accumulated_submission_time=4114.688268, global_step=12035, preemption_count=0, score=4114.688268, test/accuracy=0.205800, test/loss=4.551808, test/num_examples=10000, total_duration=4275.333572, train/accuracy=0.302156, train/loss=3.548335, validation/accuracy=0.287500, validation/loss=3.705745, validation/num_examples=50000
I0129 02:36:44.339467 139866180368128 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0601903200149536, loss=2.348623752593994
I0129 02:37:18.180965 139866171975424 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9191566109657288, loss=2.4315874576568604
I0129 02:37:51.989453 139866180368128 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.125742793083191, loss=2.468684196472168
I0129 02:38:25.836079 139866171975424 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.9727146029472351, loss=2.371748924255371
I0129 02:38:59.642426 139866180368128 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.1626514196395874, loss=2.4539096355438232
I0129 02:39:33.506007 139866171975424 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0604380369186401, loss=2.607975482940674
I0129 02:40:07.318151 139866180368128 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.9175156950950623, loss=2.359858989715576
I0129 02:40:41.355001 139866171975424 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0163588523864746, loss=2.3872737884521484
I0129 02:41:15.159224 139866180368128 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8689382672309875, loss=2.3563923835754395
I0129 02:41:49.016042 139866171975424 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.0163596868515015, loss=2.415076732635498
I0129 02:42:22.820792 139866180368128 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9633172750473022, loss=2.4211835861206055
I0129 02:42:56.674513 139866171975424 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9274186491966248, loss=2.3631255626678467
I0129 02:43:30.483434 139866180368128 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0211039781570435, loss=2.486579179763794
I0129 02:44:04.345489 139866171975424 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9371631741523743, loss=2.4133408069610596
I0129 02:44:38.147939 139866180368128 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0992710590362549, loss=2.457099199295044
I0129 02:44:52.153715 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:44:58.518390 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:45:07.166107 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:45:09.790809 140027215431488 submission_runner.py:408] Time since start: 4803.13s, 	Step: 13543, 	{'train/accuracy': 0.21027980744838715, 'train/loss': 4.674249649047852, 'validation/accuracy': 0.1959799975156784, 'validation/loss': 4.8388237953186035, 'validation/num_examples': 50000, 'test/accuracy': 0.15770000219345093, 'test/loss': 5.386991500854492, 'test/num_examples': 10000, 'score': 4624.764115333557, 'total_duration': 4803.130608320236, 'accumulated_submission_time': 4624.764115333557, 'accumulated_eval_time': 177.63978910446167, 'accumulated_logging_time': 0.25537919998168945}
I0129 02:45:09.811323 139865769342720 logging_writer.py:48] [13543] accumulated_eval_time=177.639789, accumulated_logging_time=0.255379, accumulated_submission_time=4624.764115, global_step=13543, preemption_count=0, score=4624.764115, test/accuracy=0.157700, test/loss=5.386992, test/num_examples=10000, total_duration=4803.130608, train/accuracy=0.210280, train/loss=4.674250, validation/accuracy=0.195980, validation/loss=4.838824, validation/num_examples=50000
I0129 02:45:29.431596 139866163582720 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9804770350456238, loss=2.3034188747406006
I0129 02:46:03.226852 139865769342720 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0248429775238037, loss=2.274845600128174
I0129 02:46:36.967640 139866163582720 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9288447499275208, loss=2.3915138244628906
I0129 02:47:10.923936 139865769342720 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0239530801773071, loss=2.3539328575134277
I0129 02:47:44.719635 139866163582720 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9920064210891724, loss=2.316629409790039
I0129 02:48:18.541003 139865769342720 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.020117998123169, loss=2.4330101013183594
I0129 02:48:52.374662 139866163582720 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.3452520370483398, loss=2.377964973449707
I0129 02:49:26.153789 139865769342720 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.025585412979126, loss=2.47235369682312
I0129 02:49:59.946801 139866163582720 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9959924817085266, loss=2.3681695461273193
I0129 02:50:33.736955 139865769342720 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0465632677078247, loss=2.385435104370117
I0129 02:51:07.525516 139866163582720 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.1592415571212769, loss=2.3371005058288574
I0129 02:51:41.331063 139865769342720 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0450859069824219, loss=2.2923974990844727
I0129 02:52:15.140008 139866163582720 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.056313157081604, loss=2.459604263305664
I0129 02:52:48.928481 139865769342720 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9944194555282593, loss=2.3691437244415283
I0129 02:53:22.844315 139866163582720 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.0725455284118652, loss=2.3685667514801025
I0129 02:53:39.842689 140027215431488 spec.py:321] Evaluating on the training split.
I0129 02:53:46.721480 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 02:53:55.472743 140027215431488 spec.py:349] Evaluating on the test split.
I0129 02:53:58.160235 140027215431488 submission_runner.py:408] Time since start: 5331.50s, 	Step: 15052, 	{'train/accuracy': 0.23541134595870972, 'train/loss': 4.341064929962158, 'validation/accuracy': 0.21069999039173126, 'validation/loss': 4.6160736083984375, 'validation/num_examples': 50000, 'test/accuracy': 0.15610000491142273, 'test/loss': 5.416417598724365, 'test/num_examples': 10000, 'score': 5134.7330057621, 'total_duration': 5331.500044822693, 'accumulated_submission_time': 5134.7330057621, 'accumulated_eval_time': 195.9572970867157, 'accumulated_logging_time': 0.2865011692047119}
I0129 02:53:58.180652 139865760950016 logging_writer.py:48] [15052] accumulated_eval_time=195.957297, accumulated_logging_time=0.286501, accumulated_submission_time=5134.733006, global_step=15052, preemption_count=0, score=5134.733006, test/accuracy=0.156100, test/loss=5.416418, test/num_examples=10000, total_duration=5331.500045, train/accuracy=0.235411, train/loss=4.341065, validation/accuracy=0.210700, validation/loss=4.616074, validation/num_examples=50000
I0129 02:54:14.716336 139865769342720 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9424484968185425, loss=2.51931095123291
I0129 02:54:48.482768 139865760950016 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9575165510177612, loss=2.469266891479492
I0129 02:55:22.285617 139865769342720 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0443484783172607, loss=2.442730188369751
I0129 02:55:56.017851 139865760950016 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9739823937416077, loss=2.453707695007324
I0129 02:56:29.819752 139865769342720 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.0596115589141846, loss=2.456648349761963
I0129 02:57:03.618668 139865760950016 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.1186740398406982, loss=2.4351890087127686
I0129 02:57:37.421236 139865769342720 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9680991768836975, loss=2.2629878520965576
I0129 02:58:11.228817 139865760950016 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0673223733901978, loss=2.3609492778778076
I0129 02:58:44.964266 139865769342720 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.007128119468689, loss=2.400102138519287
I0129 02:59:18.750986 139865760950016 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.951886773109436, loss=2.38979434967041
I0129 02:59:52.620013 139865769342720 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.045000433921814, loss=2.3518612384796143
I0129 03:00:26.384930 139865760950016 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9687660932540894, loss=2.4279632568359375
I0129 03:01:00.158313 139865769342720 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.24083411693573, loss=2.3334295749664307
I0129 03:01:33.944447 139865760950016 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0638642311096191, loss=2.5513367652893066
I0129 03:02:07.774793 139865769342720 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9485368132591248, loss=2.3650901317596436
I0129 03:02:28.210919 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:02:34.477922 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:02:43.267314 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:02:45.807183 140027215431488 submission_runner.py:408] Time since start: 5859.15s, 	Step: 16562, 	{'train/accuracy': 0.23078761994838715, 'train/loss': 4.050477504730225, 'validation/accuracy': 0.2169799953699112, 'validation/loss': 4.200684070587158, 'validation/num_examples': 50000, 'test/accuracy': 0.15130001306533813, 'test/loss': 4.930394649505615, 'test/num_examples': 10000, 'score': 5644.699779033661, 'total_duration': 5859.146992444992, 'accumulated_submission_time': 5644.699779033661, 'accumulated_eval_time': 213.5535204410553, 'accumulated_logging_time': 0.3168478012084961}
I0129 03:02:45.827390 139865224107776 logging_writer.py:48] [16562] accumulated_eval_time=213.553520, accumulated_logging_time=0.316848, accumulated_submission_time=5644.699779, global_step=16562, preemption_count=0, score=5644.699779, test/accuracy=0.151300, test/loss=4.930395, test/num_examples=10000, total_duration=5859.146992, train/accuracy=0.230788, train/loss=4.050478, validation/accuracy=0.216980, validation/loss=4.200684, validation/num_examples=50000
I0129 03:02:59.021622 139865240893184 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1325016021728516, loss=2.5022523403167725
I0129 03:03:32.768580 139865224107776 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9556933641433716, loss=2.2552170753479004
I0129 03:04:06.547365 139865240893184 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.0292633771896362, loss=2.2312095165252686
I0129 03:04:40.313008 139865224107776 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.029808759689331, loss=2.420283079147339
I0129 03:05:14.069643 139865240893184 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.2443007230758667, loss=2.353905439376831
I0129 03:05:47.837354 139865224107776 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.04144287109375, loss=2.2813217639923096
I0129 03:06:21.750659 139865240893184 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9844167232513428, loss=2.3918771743774414
I0129 03:06:55.516791 139865224107776 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.1215004920959473, loss=2.388493061065674
I0129 03:07:29.325939 139865240893184 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.1286756992340088, loss=2.364508628845215
I0129 03:08:03.083274 139865224107776 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9816696047782898, loss=2.4027740955352783
I0129 03:08:36.832332 139865240893184 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0260639190673828, loss=2.4159207344055176
I0129 03:09:10.522174 139865224107776 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.2580548524856567, loss=2.3592545986175537
I0129 03:09:44.338912 139865240893184 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1112700700759888, loss=2.541684865951538
I0129 03:10:18.125476 139865224107776 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0067970752716064, loss=2.2864980697631836
I0129 03:10:51.878114 139865240893184 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0394521951675415, loss=2.4461140632629395
I0129 03:11:16.013414 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:11:22.341651 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:11:31.199527 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:11:34.158281 140027215431488 submission_runner.py:408] Time since start: 6387.50s, 	Step: 18073, 	{'train/accuracy': 0.3052654564380646, 'train/loss': 3.361721992492676, 'validation/accuracy': 0.28553998470306396, 'validation/loss': 3.5417702198028564, 'validation/num_examples': 50000, 'test/accuracy': 0.20990000665187836, 'test/loss': 4.276243209838867, 'test/num_examples': 10000, 'score': 6154.821240901947, 'total_duration': 6387.498101949692, 'accumulated_submission_time': 6154.821240901947, 'accumulated_eval_time': 231.69837379455566, 'accumulated_logging_time': 0.34916210174560547}
I0129 03:11:34.175617 139865232500480 logging_writer.py:48] [18073] accumulated_eval_time=231.698374, accumulated_logging_time=0.349162, accumulated_submission_time=6154.821241, global_step=18073, preemption_count=0, score=6154.821241, test/accuracy=0.209900, test/loss=4.276243, test/num_examples=10000, total_duration=6387.498102, train/accuracy=0.305265, train/loss=3.361722, validation/accuracy=0.285540, validation/loss=3.541770, validation/num_examples=50000
I0129 03:11:43.629444 139865760950016 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9813001155853271, loss=2.3139023780822754
I0129 03:12:17.441406 139865232500480 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.0975818634033203, loss=2.441114664077759
I0129 03:12:51.331180 139865760950016 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.1598215103149414, loss=2.4638710021972656
I0129 03:13:25.094927 139865232500480 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.0226197242736816, loss=2.282299280166626
I0129 03:13:58.886109 139865760950016 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1009629964828491, loss=2.3805160522460938
I0129 03:14:32.624278 139865232500480 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9893773794174194, loss=2.3191139698028564
I0129 03:15:06.408219 139865760950016 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9938384294509888, loss=2.3512024879455566
I0129 03:15:40.134557 139865232500480 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0817897319793701, loss=2.3672239780426025
I0129 03:16:13.912408 139865760950016 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.1257659196853638, loss=2.3290274143218994
I0129 03:16:47.644960 139865232500480 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.0315208435058594, loss=2.3072495460510254
I0129 03:17:21.418779 139865760950016 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.2046395540237427, loss=2.3292689323425293
I0129 03:17:55.148736 139865232500480 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.2508306503295898, loss=2.4608278274536133
I0129 03:18:28.919538 139865760950016 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.2050682306289673, loss=2.4015989303588867
I0129 03:19:02.657139 139865232500480 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.074213981628418, loss=2.3350868225097656
I0129 03:19:36.502748 139865760950016 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9962083101272583, loss=2.2141072750091553
I0129 03:20:04.373271 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:20:10.612139 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:20:19.532162 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:20:22.189374 140027215431488 submission_runner.py:408] Time since start: 6915.53s, 	Step: 19584, 	{'train/accuracy': 0.24420040845870972, 'train/loss': 3.913287878036499, 'validation/accuracy': 0.230119988322258, 'validation/loss': 4.02189826965332, 'validation/num_examples': 50000, 'test/accuracy': 0.17020000517368317, 'test/loss': 4.693854331970215, 'test/num_examples': 10000, 'score': 6664.954342365265, 'total_duration': 6915.529188632965, 'accumulated_submission_time': 6664.954342365265, 'accumulated_eval_time': 249.51444363594055, 'accumulated_logging_time': 0.3746833801269531}
I0129 03:20:22.212523 139866180368128 logging_writer.py:48] [19584] accumulated_eval_time=249.514444, accumulated_logging_time=0.374683, accumulated_submission_time=6664.954342, global_step=19584, preemption_count=0, score=6664.954342, test/accuracy=0.170200, test/loss=4.693854, test/num_examples=10000, total_duration=6915.529189, train/accuracy=0.244200, train/loss=3.913288, validation/accuracy=0.230120, validation/loss=4.021898, validation/num_examples=50000
I0129 03:20:27.960493 139866188760832 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0650317668914795, loss=2.3213741779327393
I0129 03:21:01.769266 139866180368128 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0033149719238281, loss=2.3039183616638184
I0129 03:21:35.454532 139866188760832 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1572368144989014, loss=2.3612399101257324
I0129 03:22:09.195875 139866180368128 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0939470529556274, loss=2.54630184173584
I0129 03:22:42.953681 139866188760832 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1674484014511108, loss=2.2774996757507324
I0129 03:23:16.755524 139866180368128 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9990646839141846, loss=2.2979815006256104
I0129 03:23:50.489471 139866188760832 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.9949296116828918, loss=2.384166717529297
I0129 03:24:24.244324 139866180368128 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.961876630783081, loss=2.368892192840576
I0129 03:24:58.016871 139866188760832 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0154812335968018, loss=2.4733824729919434
I0129 03:25:31.726677 139866180368128 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.1044379472732544, loss=2.4285566806793213
I0129 03:26:05.549505 139866188760832 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.109703540802002, loss=2.4683995246887207
I0129 03:26:39.243509 139866180368128 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9605069160461426, loss=2.3030102252960205
I0129 03:27:13.010847 139866188760832 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.04207444190979, loss=2.355067729949951
I0129 03:27:46.838569 139866180368128 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.047173261642456, loss=2.3593969345092773
I0129 03:28:20.575304 139866188760832 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9636780619621277, loss=2.3649280071258545
I0129 03:28:52.471277 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:28:58.720825 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:29:07.605604 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:29:10.218293 140027215431488 submission_runner.py:408] Time since start: 7443.56s, 	Step: 21096, 	{'train/accuracy': 0.24226722121238708, 'train/loss': 3.8922784328460693, 'validation/accuracy': 0.2260199934244156, 'validation/loss': 4.0060505867004395, 'validation/num_examples': 50000, 'test/accuracy': 0.16990001499652863, 'test/loss': 4.684741973876953, 'test/num_examples': 10000, 'score': 7175.1495196819305, 'total_duration': 7443.557811498642, 'accumulated_submission_time': 7175.1495196819305, 'accumulated_eval_time': 267.2611298561096, 'accumulated_logging_time': 0.4081254005432129}
I0129 03:29:10.239587 139865232500480 logging_writer.py:48] [21096] accumulated_eval_time=267.261130, accumulated_logging_time=0.408125, accumulated_submission_time=7175.149520, global_step=21096, preemption_count=0, score=7175.149520, test/accuracy=0.169900, test/loss=4.684742, test/num_examples=10000, total_duration=7443.557811, train/accuracy=0.242267, train/loss=3.892278, validation/accuracy=0.226020, validation/loss=4.006051, validation/num_examples=50000
I0129 03:29:11.940404 139865240893184 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0275877714157104, loss=2.369300365447998
I0129 03:29:45.682145 139865232500480 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.1381853818893433, loss=2.353280782699585
I0129 03:30:19.401806 139865240893184 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.1550743579864502, loss=2.299281358718872
I0129 03:30:53.072949 139865232500480 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9816099405288696, loss=2.391113758087158
I0129 03:31:26.776847 139865240893184 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.159920573234558, loss=2.400214195251465
I0129 03:32:00.452982 139865232500480 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.0936859846115112, loss=2.3795204162597656
I0129 03:32:34.313282 139865240893184 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0780344009399414, loss=2.236384630203247
I0129 03:33:07.986999 139865232500480 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.1150915622711182, loss=2.2880492210388184
I0129 03:33:41.685500 139865240893184 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.1764862537384033, loss=2.275129556655884
I0129 03:34:15.376185 139865232500480 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.18901526927948, loss=2.3116567134857178
I0129 03:34:49.160387 139865240893184 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0813745260238647, loss=2.426184892654419
I0129 03:35:22.839712 139865232500480 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2870285511016846, loss=2.337157726287842
I0129 03:35:56.608444 139865240893184 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9883558750152588, loss=2.2521910667419434
I0129 03:36:30.349367 139865232500480 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.1361349821090698, loss=2.38093900680542
I0129 03:37:04.116581 139865240893184 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.1454005241394043, loss=2.3209068775177
I0129 03:37:37.888203 139865232500480 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0572032928466797, loss=2.340837240219116
I0129 03:37:40.406088 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:37:46.615850 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:37:55.555120 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:37:58.110897 140027215431488 submission_runner.py:408] Time since start: 7971.45s, 	Step: 22609, 	{'train/accuracy': 0.21225287020206451, 'train/loss': 4.563592433929443, 'validation/accuracy': 0.20136000216007233, 'validation/loss': 4.659169673919678, 'validation/num_examples': 50000, 'test/accuracy': 0.15480001270771027, 'test/loss': 5.273361682891846, 'test/num_examples': 10000, 'score': 7685.252676725388, 'total_duration': 7971.4506142139435, 'accumulated_submission_time': 7685.252676725388, 'accumulated_eval_time': 284.9658041000366, 'accumulated_logging_time': 0.439424991607666}
I0129 03:37:58.131909 139865232500480 logging_writer.py:48] [22609] accumulated_eval_time=284.965804, accumulated_logging_time=0.439425, accumulated_submission_time=7685.252677, global_step=22609, preemption_count=0, score=7685.252677, test/accuracy=0.154800, test/loss=5.273362, test/num_examples=10000, total_duration=7971.450614, train/accuracy=0.212253, train/loss=4.563592, validation/accuracy=0.201360, validation/loss=4.659170, validation/num_examples=50000
I0129 03:38:29.189698 139866171975424 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.104413628578186, loss=2.336972951889038
I0129 03:39:03.085281 139865232500480 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0085124969482422, loss=2.3604650497436523
I0129 03:39:36.785621 139866171975424 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0676047801971436, loss=2.2959041595458984
I0129 03:40:10.594072 139865232500480 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.2011665105819702, loss=2.2809677124023438
I0129 03:40:44.281801 139866171975424 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0213382244110107, loss=2.3286242485046387
I0129 03:41:18.095803 139865232500480 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0473623275756836, loss=2.3202595710754395
I0129 03:41:51.789745 139866171975424 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.010290265083313, loss=2.4605345726013184
I0129 03:42:25.582202 139865232500480 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.146641731262207, loss=2.3705241680145264
I0129 03:42:59.369379 139866171975424 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.1979718208312988, loss=2.4662890434265137
I0129 03:43:33.071090 139865232500480 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.048660397529602, loss=2.3328418731689453
I0129 03:44:06.759771 139866171975424 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.186760425567627, loss=2.3743019104003906
I0129 03:44:40.539587 139865232500480 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1610420942306519, loss=2.4321494102478027
I0129 03:45:14.383291 139866171975424 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1056864261627197, loss=2.3509981632232666
I0129 03:45:48.111089 139865232500480 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1574485301971436, loss=2.375828981399536
I0129 03:46:21.769225 139866171975424 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0772242546081543, loss=2.307612419128418
I0129 03:46:28.324460 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:46:34.585717 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:46:43.164091 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:46:45.780233 140027215431488 submission_runner.py:408] Time since start: 8499.12s, 	Step: 24121, 	{'train/accuracy': 0.24152980744838715, 'train/loss': 4.111358165740967, 'validation/accuracy': 0.2184399962425232, 'validation/loss': 4.350162506103516, 'validation/num_examples': 50000, 'test/accuracy': 0.1574000120162964, 'test/loss': 5.048336029052734, 'test/num_examples': 10000, 'score': 8195.383947610855, 'total_duration': 8499.119955062866, 'accumulated_submission_time': 8195.383947610855, 'accumulated_eval_time': 302.4214491844177, 'accumulated_logging_time': 0.47034239768981934}
I0129 03:46:45.802391 139865232500480 logging_writer.py:48] [24121] accumulated_eval_time=302.421449, accumulated_logging_time=0.470342, accumulated_submission_time=8195.383948, global_step=24121, preemption_count=0, score=8195.383948, test/accuracy=0.157400, test/loss=5.048336, test/num_examples=10000, total_duration=8499.119955, train/accuracy=0.241530, train/loss=4.111358, validation/accuracy=0.218440, validation/loss=4.350163, validation/num_examples=50000
I0129 03:47:12.764658 139865240893184 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1648578643798828, loss=2.346087694168091
I0129 03:47:46.445957 139865232500480 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.134692907333374, loss=2.2914113998413086
I0129 03:48:20.147081 139865240893184 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0062652826309204, loss=2.3796708583831787
I0129 03:48:53.833663 139865232500480 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0727380514144897, loss=2.3258113861083984
I0129 03:49:27.595227 139865240893184 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1247334480285645, loss=2.347130060195923
I0129 03:50:01.312034 139865232500480 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0818346738815308, loss=2.233595848083496
I0129 03:50:35.070281 139865240893184 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0516247749328613, loss=2.3196985721588135
I0129 03:51:08.784951 139865232500480 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0873037576675415, loss=2.3462963104248047
I0129 03:51:42.631970 139865240893184 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0645747184753418, loss=2.2624294757843018
I0129 03:52:16.418358 139865232500480 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0707465410232544, loss=2.2560782432556152
I0129 03:52:50.162693 139865240893184 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1303960084915161, loss=2.3037450313568115
I0129 03:53:23.932908 139865232500480 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.3076025247573853, loss=2.3278284072875977
I0129 03:53:57.630062 139865240893184 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0048507452011108, loss=2.284125566482544
I0129 03:54:31.319005 139865232500480 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1630758047103882, loss=2.2659387588500977
I0129 03:55:05.010300 139865240893184 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1152242422103882, loss=2.267364025115967
I0129 03:55:15.926375 140027215431488 spec.py:321] Evaluating on the training split.
I0129 03:55:22.157661 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 03:55:30.783004 140027215431488 spec.py:349] Evaluating on the test split.
I0129 03:55:33.762644 140027215431488 submission_runner.py:408] Time since start: 9027.10s, 	Step: 25634, 	{'train/accuracy': 0.2726801633834839, 'train/loss': 3.7686870098114014, 'validation/accuracy': 0.2542800009250641, 'validation/loss': 3.9192233085632324, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.636255741119385, 'test/num_examples': 10000, 'score': 8705.445008277893, 'total_duration': 9027.102459907532, 'accumulated_submission_time': 8705.445008277893, 'accumulated_eval_time': 320.2576837539673, 'accumulated_logging_time': 0.502802848815918}
I0129 03:55:33.781353 139865240893184 logging_writer.py:48] [25634] accumulated_eval_time=320.257684, accumulated_logging_time=0.502803, accumulated_submission_time=8705.445008, global_step=25634, preemption_count=0, score=8705.445008, test/accuracy=0.179100, test/loss=4.636256, test/num_examples=10000, total_duration=9027.102460, train/accuracy=0.272680, train/loss=3.768687, validation/accuracy=0.254280, validation/loss=3.919223, validation/num_examples=50000
I0129 03:55:56.363045 139866163582720 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1057074069976807, loss=2.367565631866455
I0129 03:56:30.031317 139865240893184 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.2199331521987915, loss=2.3916192054748535
I0129 03:57:03.833409 139866163582720 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0857099294662476, loss=2.4346463680267334
I0129 03:57:37.492745 139865240893184 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0738765001296997, loss=2.316956043243408
I0129 03:58:11.402044 139866163582720 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9705473184585571, loss=2.284043550491333
I0129 03:58:45.183157 139865240893184 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0278786420822144, loss=2.3584647178649902
I0129 03:59:18.907704 139866163582720 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1217386722564697, loss=2.267204999923706
I0129 03:59:52.669264 139865240893184 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.0352938175201416, loss=2.217754364013672
I0129 04:00:26.393999 139866163582720 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1161564588546753, loss=2.2583749294281006
I0129 04:01:00.158220 139865240893184 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9905751347541809, loss=2.220452308654785
I0129 04:01:33.897615 139866163582720 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.1803568601608276, loss=2.3396365642547607
I0129 04:02:07.584296 139865240893184 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.0676460266113281, loss=2.428924798965454
I0129 04:02:41.382678 139866163582720 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1539186239242554, loss=2.353163003921509
I0129 04:03:15.047975 139865240893184 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.113237738609314, loss=2.2940731048583984
I0129 04:03:48.786420 139866163582720 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1007647514343262, loss=2.234781503677368
I0129 04:04:03.764768 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:04:10.019530 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:04:18.634619 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:04:21.365136 140027215431488 submission_runner.py:408] Time since start: 9554.70s, 	Step: 27146, 	{'train/accuracy': 0.21406647562980652, 'train/loss': 4.525050640106201, 'validation/accuracy': 0.20149999856948853, 'validation/loss': 4.717775344848633, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 5.444266319274902, 'test/num_examples': 10000, 'score': 9215.368250131607, 'total_duration': 9554.7049472332, 'accumulated_submission_time': 9215.368250131607, 'accumulated_eval_time': 337.85802817344666, 'accumulated_logging_time': 0.5298073291778564}
I0129 04:04:21.387791 139865769342720 logging_writer.py:48] [27146] accumulated_eval_time=337.858028, accumulated_logging_time=0.529807, accumulated_submission_time=9215.368250, global_step=27146, preemption_count=0, score=9215.368250, test/accuracy=0.152200, test/loss=5.444266, test/num_examples=10000, total_duration=9554.704947, train/accuracy=0.214066, train/loss=4.525051, validation/accuracy=0.201500, validation/loss=4.717775, validation/num_examples=50000
I0129 04:04:40.053006 139866180368128 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9778232574462891, loss=2.1920104026794434
I0129 04:05:13.742754 139865769342720 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9377580285072327, loss=2.1197147369384766
I0129 04:05:47.461261 139866180368128 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1264703273773193, loss=2.4024338722229004
I0129 04:06:21.136882 139865769342720 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.9992930889129639, loss=2.2197446823120117
I0129 04:06:54.835447 139866180368128 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.0022531747817993, loss=2.2675888538360596
I0129 04:07:28.518944 139865769342720 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.109969973564148, loss=2.275874614715576
I0129 04:08:02.238020 139866180368128 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.2511500120162964, loss=2.380779266357422
I0129 04:08:35.920395 139865769342720 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0868725776672363, loss=2.2481179237365723
I0129 04:09:09.620831 139866180368128 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.12355375289917, loss=2.32228946685791
I0129 04:09:43.301488 139865769342720 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.1279934644699097, loss=2.3404910564422607
I0129 04:10:17.003356 139866180368128 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0815951824188232, loss=2.274184465408325
I0129 04:10:50.880318 139865769342720 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.0378316640853882, loss=2.3529412746429443
I0129 04:11:24.677583 139866180368128 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.068742036819458, loss=2.3173365592956543
I0129 04:11:58.382664 139865769342720 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.2002747058868408, loss=2.2514760494232178
I0129 04:12:32.165967 139866180368128 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2022027969360352, loss=2.3023669719696045
I0129 04:12:51.498346 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:12:57.826465 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:13:06.669614 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:13:09.324914 140027215431488 submission_runner.py:408] Time since start: 10082.66s, 	Step: 28659, 	{'train/accuracy': 0.1899513602256775, 'train/loss': 4.6598968505859375, 'validation/accuracy': 0.1812800019979477, 'validation/loss': 4.777307510375977, 'validation/num_examples': 50000, 'test/accuracy': 0.13740000128746033, 'test/loss': 5.371987819671631, 'test/num_examples': 10000, 'score': 9725.415110111237, 'total_duration': 10082.664724826813, 'accumulated_submission_time': 9725.415110111237, 'accumulated_eval_time': 355.68455719947815, 'accumulated_logging_time': 0.5628311634063721}
I0129 04:13:09.347676 139865240893184 logging_writer.py:48] [28659] accumulated_eval_time=355.684557, accumulated_logging_time=0.562831, accumulated_submission_time=9725.415110, global_step=28659, preemption_count=0, score=9725.415110, test/accuracy=0.137400, test/loss=5.371988, test/num_examples=10000, total_duration=10082.664725, train/accuracy=0.189951, train/loss=4.659897, validation/accuracy=0.181280, validation/loss=4.777308, validation/num_examples=50000
I0129 04:13:23.516162 139865760950016 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1038450002670288, loss=2.320986747741699
I0129 04:13:57.292220 139865240893184 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0811980962753296, loss=2.3626480102539062
I0129 04:14:30.980146 139865760950016 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.006414532661438, loss=2.2161383628845215
I0129 04:15:04.778349 139865240893184 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.1251130104064941, loss=2.2882080078125
I0129 04:15:38.439609 139865760950016 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.12327241897583, loss=2.347522258758545
I0129 04:16:12.175860 139865240893184 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.2064927816390991, loss=2.3796815872192383
I0129 04:16:45.844169 139865760950016 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.156544804573059, loss=2.5029804706573486
I0129 04:17:19.692677 139865240893184 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.1416475772857666, loss=2.3675999641418457
I0129 04:17:53.382470 139865760950016 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.0588696002960205, loss=2.272026538848877
I0129 04:18:27.099984 139865240893184 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.124596357345581, loss=2.429251194000244
I0129 04:19:00.833071 139865760950016 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0778719186782837, loss=2.244232177734375
I0129 04:19:34.602792 139865240893184 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.3042147159576416, loss=2.365067481994629
I0129 04:20:08.365401 139865760950016 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.098560094833374, loss=2.29580020904541
I0129 04:20:42.077373 139865240893184 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.2009592056274414, loss=2.3765738010406494
I0129 04:21:15.841087 139865760950016 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9835156202316284, loss=2.274851083755493
I0129 04:21:39.607266 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:21:45.858976 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:21:54.358652 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:21:56.981476 140027215431488 submission_runner.py:408] Time since start: 10610.32s, 	Step: 30172, 	{'train/accuracy': 0.19816246628761292, 'train/loss': 4.559650897979736, 'validation/accuracy': 0.19246000051498413, 'validation/loss': 4.613406181335449, 'validation/num_examples': 50000, 'test/accuracy': 0.13440001010894775, 'test/loss': 5.338951110839844, 'test/num_examples': 10000, 'score': 10235.613961458206, 'total_duration': 10610.321287631989, 'accumulated_submission_time': 10235.613961458206, 'accumulated_eval_time': 373.0587408542633, 'accumulated_logging_time': 0.594775915145874}
I0129 04:21:57.004927 139865240893184 logging_writer.py:48] [30172] accumulated_eval_time=373.058741, accumulated_logging_time=0.594776, accumulated_submission_time=10235.613961, global_step=30172, preemption_count=0, score=10235.613961, test/accuracy=0.134400, test/loss=5.338951, test/num_examples=10000, total_duration=10610.321288, train/accuracy=0.198162, train/loss=4.559651, validation/accuracy=0.192460, validation/loss=4.613406, validation/num_examples=50000
I0129 04:22:06.810634 139865769342720 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.146978735923767, loss=2.3421730995178223
I0129 04:22:40.541104 139865240893184 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0435752868652344, loss=2.308229684829712
I0129 04:23:14.228985 139865769342720 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1230442523956299, loss=2.3496737480163574
I0129 04:23:48.125463 139865240893184 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.096205472946167, loss=2.3891613483428955
I0129 04:24:21.815098 139865769342720 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.2001429796218872, loss=2.3011205196380615
I0129 04:24:55.535495 139865240893184 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0701160430908203, loss=2.23384428024292
I0129 04:25:29.209514 139865769342720 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.0583934783935547, loss=2.369370222091675
I0129 04:26:02.890933 139865240893184 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0646984577178955, loss=2.425743579864502
I0129 04:26:36.570737 139865769342720 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1190670728683472, loss=2.227503776550293
I0129 04:27:10.264463 139865240893184 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.0646604299545288, loss=2.2944324016571045
I0129 04:27:43.934823 139865769342720 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.0816988945007324, loss=2.2566215991973877
I0129 04:28:17.641047 139865240893184 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0837171077728271, loss=2.3457584381103516
I0129 04:28:51.315983 139865769342720 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1014111042022705, loss=2.2920875549316406
I0129 04:29:25.017538 139865240893184 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1353256702423096, loss=2.258504629135132
I0129 04:29:58.672372 139865769342720 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0707762241363525, loss=2.2511301040649414
I0129 04:30:27.266781 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:30:33.483795 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:30:42.444789 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:30:45.005953 140027215431488 submission_runner.py:408] Time since start: 11138.35s, 	Step: 31686, 	{'train/accuracy': 0.3178212642669678, 'train/loss': 3.394036054611206, 'validation/accuracy': 0.3035599887371063, 'validation/loss': 3.541114330291748, 'validation/num_examples': 50000, 'test/accuracy': 0.22830000519752502, 'test/loss': 4.332971572875977, 'test/num_examples': 10000, 'score': 10745.81271648407, 'total_duration': 11138.345761299133, 'accumulated_submission_time': 10745.81271648407, 'accumulated_eval_time': 390.7978858947754, 'accumulated_logging_time': 0.6274514198303223}
I0129 04:30:45.032904 139865240893184 logging_writer.py:48] [31686] accumulated_eval_time=390.797886, accumulated_logging_time=0.627451, accumulated_submission_time=10745.812716, global_step=31686, preemption_count=0, score=10745.812716, test/accuracy=0.228300, test/loss=4.332972, test/num_examples=10000, total_duration=11138.345761, train/accuracy=0.317821, train/loss=3.394036, validation/accuracy=0.303560, validation/loss=3.541114, validation/num_examples=50000
I0129 04:30:50.084188 139865760950016 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9856457710266113, loss=2.2076549530029297
I0129 04:31:23.816416 139865240893184 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.1490744352340698, loss=2.3211493492126465
I0129 04:31:57.531457 139865760950016 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.0941693782806396, loss=2.2272074222564697
I0129 04:32:31.208058 139865240893184 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.0730531215667725, loss=2.3591384887695312
I0129 04:33:04.923388 139865760950016 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0678789615631104, loss=2.213630437850952
I0129 04:33:38.604362 139865240893184 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.1540809869766235, loss=2.2282626628875732
I0129 04:34:12.341156 139865760950016 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.1535564661026, loss=2.1524224281311035
I0129 04:34:46.057065 139865240893184 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1655209064483643, loss=2.3026909828186035
I0129 04:35:19.776996 139865760950016 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1667264699935913, loss=2.364192485809326
I0129 04:35:53.441348 139865240893184 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0147037506103516, loss=2.340278387069702
I0129 04:36:27.149629 139865760950016 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.0747877359390259, loss=2.231879949569702
I0129 04:37:00.973916 139865240893184 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.2167555093765259, loss=2.403261661529541
I0129 04:37:34.685837 139865760950016 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.0516012907028198, loss=2.302370548248291
I0129 04:38:08.355440 139865240893184 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0781804323196411, loss=2.2372891902923584
I0129 04:38:42.052188 139865760950016 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.1735223531723022, loss=2.165281057357788
I0129 04:39:15.184370 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:39:21.391516 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:39:29.973043 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:39:32.613656 140027215431488 submission_runner.py:408] Time since start: 11665.95s, 	Step: 33200, 	{'train/accuracy': 0.17191486060619354, 'train/loss': 5.012552261352539, 'validation/accuracy': 0.15573999285697937, 'validation/loss': 5.170931339263916, 'validation/num_examples': 50000, 'test/accuracy': 0.11110000312328339, 'test/loss': 5.935586452484131, 'test/num_examples': 10000, 'score': 11255.900620937347, 'total_duration': 11665.95344877243, 'accumulated_submission_time': 11255.900620937347, 'accumulated_eval_time': 408.22711849212646, 'accumulated_logging_time': 0.6634583473205566}
I0129 04:39:32.640688 139865232500480 logging_writer.py:48] [33200] accumulated_eval_time=408.227118, accumulated_logging_time=0.663458, accumulated_submission_time=11255.900621, global_step=33200, preemption_count=0, score=11255.900621, test/accuracy=0.111100, test/loss=5.935586, test/num_examples=10000, total_duration=11665.953449, train/accuracy=0.171915, train/loss=5.012552, validation/accuracy=0.155740, validation/loss=5.170931, validation/num_examples=50000
I0129 04:39:32.996570 139866171975424 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.187669277191162, loss=2.168255090713501
I0129 04:40:06.708131 139865232500480 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1295205354690552, loss=2.272838830947876
I0129 04:40:40.417920 139866171975424 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.0950360298156738, loss=2.339855432510376
I0129 04:41:14.081016 139865232500480 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.152440071105957, loss=2.3025927543640137
I0129 04:41:47.788014 139866171975424 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.1130201816558838, loss=2.2785134315490723
I0129 04:42:21.454509 139865232500480 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.0399888753890991, loss=2.251152515411377
I0129 04:42:55.171755 139866171975424 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1488022804260254, loss=2.476735830307007
I0129 04:43:28.999238 139865232500480 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.21491277217865, loss=2.2282912731170654
I0129 04:44:02.720778 139866171975424 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.2285622358322144, loss=2.3598268032073975
I0129 04:44:36.381057 139865232500480 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.3289813995361328, loss=2.317239999771118
I0129 04:45:10.188392 139866171975424 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.0703322887420654, loss=2.2515974044799805
I0129 04:45:43.849874 139865232500480 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0922608375549316, loss=2.3346290588378906
I0129 04:46:17.561149 139866171975424 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.1233659982681274, loss=2.304579734802246
I0129 04:46:51.223456 139865232500480 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.1299134492874146, loss=2.273763656616211
I0129 04:47:25.020447 139866171975424 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.0361683368682861, loss=2.2032036781311035
I0129 04:47:58.693898 139865232500480 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.1715168952941895, loss=2.294170379638672
I0129 04:48:02.880397 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:48:09.097436 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:48:17.913242 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:48:20.567920 140027215431488 submission_runner.py:408] Time since start: 12193.91s, 	Step: 34714, 	{'train/accuracy': 0.32238519191741943, 'train/loss': 3.2682571411132812, 'validation/accuracy': 0.3014200031757355, 'validation/loss': 3.499361515045166, 'validation/num_examples': 50000, 'test/accuracy': 0.22610001266002655, 'test/loss': 4.192788124084473, 'test/num_examples': 10000, 'score': 11766.076352834702, 'total_duration': 12193.907732248306, 'accumulated_submission_time': 11766.076352834702, 'accumulated_eval_time': 425.9146020412445, 'accumulated_logging_time': 0.7024168968200684}
I0129 04:48:20.593315 139866163582720 logging_writer.py:48] [34714] accumulated_eval_time=425.914602, accumulated_logging_time=0.702417, accumulated_submission_time=11766.076353, global_step=34714, preemption_count=0, score=11766.076353, test/accuracy=0.226100, test/loss=4.192788, test/num_examples=10000, total_duration=12193.907732, train/accuracy=0.322385, train/loss=3.268257, validation/accuracy=0.301420, validation/loss=3.499362, validation/num_examples=50000
I0129 04:48:49.847992 139866188760832 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.2389377355575562, loss=2.2657546997070312
I0129 04:49:23.512142 139866163582720 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.2075444459915161, loss=2.2925565242767334
I0129 04:49:57.387435 139866188760832 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.2078096866607666, loss=2.344252824783325
I0129 04:50:31.058444 139866163582720 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1339420080184937, loss=2.012295961380005
I0129 04:51:04.781153 139866188760832 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.2791273593902588, loss=2.2806246280670166
I0129 04:51:38.443421 139866163582720 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.055875301361084, loss=2.271674156188965
I0129 04:52:12.196118 139866188760832 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.0901389122009277, loss=2.163591146469116
I0129 04:52:45.883448 139866163582720 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0537762641906738, loss=2.307572364807129
I0129 04:53:19.616330 139866188760832 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0726150274276733, loss=2.248257637023926
I0129 04:53:53.288594 139866163582720 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0993387699127197, loss=2.2367470264434814
I0129 04:54:26.981666 139866188760832 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1721569299697876, loss=2.2823143005371094
I0129 04:55:00.667026 139866163582720 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.251107096672058, loss=2.299290180206299
I0129 04:55:34.396255 139866188760832 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.0691230297088623, loss=2.2194650173187256
I0129 04:56:08.206844 139866163582720 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.1254148483276367, loss=2.3224937915802
I0129 04:56:41.929486 139866188760832 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.100378155708313, loss=2.2112042903900146
I0129 04:56:50.820344 140027215431488 spec.py:321] Evaluating on the training split.
I0129 04:56:57.040371 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 04:57:05.691079 140027215431488 spec.py:349] Evaluating on the test split.
I0129 04:57:08.334194 140027215431488 submission_runner.py:408] Time since start: 12721.67s, 	Step: 36228, 	{'train/accuracy': 0.2633928656578064, 'train/loss': 3.819126605987549, 'validation/accuracy': 0.24873998761177063, 'validation/loss': 3.9294168949127197, 'validation/num_examples': 50000, 'test/accuracy': 0.18330000340938568, 'test/loss': 4.704871654510498, 'test/num_examples': 10000, 'score': 12276.239780902863, 'total_duration': 12721.674006223679, 'accumulated_submission_time': 12276.239780902863, 'accumulated_eval_time': 443.42841243743896, 'accumulated_logging_time': 0.7372820377349854}
I0129 04:57:08.358849 139865232500480 logging_writer.py:48] [36228] accumulated_eval_time=443.428412, accumulated_logging_time=0.737282, accumulated_submission_time=12276.239781, global_step=36228, preemption_count=0, score=12276.239781, test/accuracy=0.183300, test/loss=4.704872, test/num_examples=10000, total_duration=12721.674006, train/accuracy=0.263393, train/loss=3.819127, validation/accuracy=0.248740, validation/loss=3.929417, validation/num_examples=50000
I0129 04:57:32.937611 139865240893184 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1179578304290771, loss=2.3035333156585693
I0129 04:58:06.668456 139865232500480 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.0889158248901367, loss=2.237514019012451
I0129 04:58:40.400032 139865240893184 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1472333669662476, loss=2.1906309127807617
I0129 04:59:14.049377 139865232500480 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.3216649293899536, loss=2.258925437927246
I0129 04:59:47.760794 139865240893184 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0192769765853882, loss=2.199347734451294
I0129 05:00:21.436841 139865232500480 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1151870489120483, loss=2.3500547409057617
I0129 05:00:55.230526 139865240893184 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1088647842407227, loss=2.285032033920288
I0129 05:01:28.892174 139865232500480 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.1323611736297607, loss=2.4171571731567383
I0129 05:02:02.606115 139865240893184 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.0964053869247437, loss=2.396204710006714
I0129 05:02:36.399276 139865232500480 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1225587129592896, loss=2.297220468521118
I0129 05:03:10.132181 139865240893184 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.1145657300949097, loss=2.3453705310821533
I0129 05:03:43.801343 139865232500480 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.3450546264648438, loss=2.367574453353882
I0129 05:04:17.532030 139865240893184 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1114449501037598, loss=2.340952157974243
I0129 05:04:51.220137 139865232500480 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0182238817214966, loss=2.223020553588867
I0129 05:05:24.938600 139865240893184 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.257957100868225, loss=2.2387304306030273
I0129 05:05:38.539536 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:05:44.749091 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:05:53.575398 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:05:56.213824 140027215431488 submission_runner.py:408] Time since start: 13249.55s, 	Step: 37742, 	{'train/accuracy': 0.3377511203289032, 'train/loss': 3.210239887237549, 'validation/accuracy': 0.31939998269081116, 'validation/loss': 3.303650140762329, 'validation/num_examples': 50000, 'test/accuracy': 0.23080001771450043, 'test/loss': 4.0718674659729, 'test/num_examples': 10000, 'score': 12786.358525514603, 'total_duration': 13249.55362534523, 'accumulated_submission_time': 12786.358525514603, 'accumulated_eval_time': 461.10265159606934, 'accumulated_logging_time': 0.7711968421936035}
I0129 05:05:56.239003 139866171975424 logging_writer.py:48] [37742] accumulated_eval_time=461.102652, accumulated_logging_time=0.771197, accumulated_submission_time=12786.358526, global_step=37742, preemption_count=0, score=12786.358526, test/accuracy=0.230800, test/loss=4.071867, test/num_examples=10000, total_duration=13249.553625, train/accuracy=0.337751, train/loss=3.210240, validation/accuracy=0.319400, validation/loss=3.303650, validation/num_examples=50000
I0129 05:06:16.199057 139866180368128 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.2145718336105347, loss=2.214965343475342
I0129 05:06:49.895169 139866171975424 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.2598720788955688, loss=2.212024211883545
I0129 05:07:23.597112 139866180368128 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.084150791168213, loss=2.231308698654175
I0129 05:07:57.275023 139866171975424 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1255762577056885, loss=2.2340476512908936
I0129 05:08:30.958366 139866180368128 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1469950675964355, loss=2.1936116218566895
I0129 05:09:04.785042 139866171975424 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.15245521068573, loss=2.1124267578125
I0129 05:09:38.486321 139866180368128 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.164094090461731, loss=2.293508768081665
I0129 05:10:12.183310 139866171975424 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.1161259412765503, loss=2.2049901485443115
I0129 05:10:45.868002 139866180368128 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.1309493780136108, loss=2.2634899616241455
I0129 05:11:19.555334 139866171975424 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.1209183931350708, loss=2.2122573852539062
I0129 05:11:53.232566 139866180368128 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.141454815864563, loss=2.3589582443237305
I0129 05:12:26.934174 139866171975424 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.0389599800109863, loss=2.195878505706787
I0129 05:13:00.641871 139866180368128 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1865501403808594, loss=2.344233274459839
I0129 05:13:34.335864 139866171975424 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.2474708557128906, loss=2.3157944679260254
I0129 05:14:08.125902 139866180368128 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1165443658828735, loss=2.2412121295928955
I0129 05:14:26.493144 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:14:32.789775 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:14:41.444144 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:14:43.962537 140027215431488 submission_runner.py:408] Time since start: 13777.30s, 	Step: 39256, 	{'train/accuracy': 0.18891501426696777, 'train/loss': 4.7008209228515625, 'validation/accuracy': 0.1738400012254715, 'validation/loss': 4.816714763641357, 'validation/num_examples': 50000, 'test/accuracy': 0.11620000749826431, 'test/loss': 5.706114768981934, 'test/num_examples': 10000, 'score': 13296.550779342651, 'total_duration': 13777.302340745926, 'accumulated_submission_time': 13296.550779342651, 'accumulated_eval_time': 478.57200956344604, 'accumulated_logging_time': 0.8064799308776855}
I0129 05:14:43.991266 139865232500480 logging_writer.py:48] [39256] accumulated_eval_time=478.572010, accumulated_logging_time=0.806480, accumulated_submission_time=13296.550779, global_step=39256, preemption_count=0, score=13296.550779, test/accuracy=0.116200, test/loss=5.706115, test/num_examples=10000, total_duration=13777.302341, train/accuracy=0.188915, train/loss=4.700821, validation/accuracy=0.173840, validation/loss=4.816715, validation/num_examples=50000
I0129 05:14:59.132666 139865240893184 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.124659538269043, loss=2.3137712478637695
I0129 05:15:32.974194 139865232500480 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.0563281774520874, loss=2.1303532123565674
I0129 05:16:06.665984 139865240893184 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.4132927656173706, loss=2.343299627304077
I0129 05:16:40.385913 139865232500480 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.2360060214996338, loss=2.1776721477508545
I0129 05:17:14.051748 139865240893184 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.1966472864151, loss=2.2468929290771484
I0129 05:17:47.762383 139865232500480 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.0647590160369873, loss=2.2178945541381836
I0129 05:18:21.434254 139865240893184 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.1261389255523682, loss=2.1743083000183105
I0129 05:18:55.160935 139865232500480 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.0600311756134033, loss=2.1437506675720215
I0129 05:19:28.922970 139865240893184 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.2679580450057983, loss=2.22053861618042
I0129 05:20:02.634336 139865232500480 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.134748935699463, loss=2.24525785446167
I0129 05:20:36.277279 139865240893184 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1091434955596924, loss=2.2553515434265137
I0129 05:21:09.994743 139865232500480 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.3288182020187378, loss=2.3660101890563965
I0129 05:21:43.739970 139865240893184 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1567248106002808, loss=2.207188844680786
I0129 05:22:17.519538 139865232500480 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1679290533065796, loss=2.366426944732666
I0129 05:22:51.179676 139865240893184 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0862759351730347, loss=2.3121094703674316
I0129 05:23:14.254423 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:23:20.472047 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:23:28.984409 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:23:31.604598 140027215431488 submission_runner.py:408] Time since start: 14304.94s, 	Step: 40770, 	{'train/accuracy': 0.09825414419174194, 'train/loss': 6.791675090789795, 'validation/accuracy': 0.09229999780654907, 'validation/loss': 6.967357158660889, 'validation/num_examples': 50000, 'test/accuracy': 0.06430000066757202, 'test/loss': 7.548539638519287, 'test/num_examples': 10000, 'score': 13806.753144979477, 'total_duration': 14304.94440293312, 'accumulated_submission_time': 13806.753144979477, 'accumulated_eval_time': 495.9221394062042, 'accumulated_logging_time': 0.8445472717285156}
I0129 05:23:31.629434 139865240893184 logging_writer.py:48] [40770] accumulated_eval_time=495.922139, accumulated_logging_time=0.844547, accumulated_submission_time=13806.753145, global_step=40770, preemption_count=0, score=13806.753145, test/accuracy=0.064300, test/loss=7.548540, test/num_examples=10000, total_duration=14304.944403, train/accuracy=0.098254, train/loss=6.791675, validation/accuracy=0.092300, validation/loss=6.967357, validation/num_examples=50000
I0129 05:23:42.073505 139866171975424 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.175048828125, loss=2.3764548301696777
I0129 05:24:15.837549 139865240893184 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.2447117567062378, loss=2.3526690006256104
I0129 05:24:49.533854 139866171975424 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.1413193941116333, loss=2.1635701656341553
I0129 05:25:23.245554 139865240893184 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1284140348434448, loss=2.3548178672790527
I0129 05:25:56.912519 139866171975424 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.2543864250183105, loss=2.36269211769104
I0129 05:26:30.610836 139865240893184 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.0767993927001953, loss=2.27573561668396
I0129 05:27:04.277794 139866171975424 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.3166340589523315, loss=2.4402315616607666
I0129 05:27:37.973432 139865240893184 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2094310522079468, loss=2.1877684593200684
I0129 05:28:12.338864 139866171975424 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1015856266021729, loss=2.3149564266204834
I0129 05:28:46.039549 139865240893184 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1102977991104126, loss=2.2882251739501953
I0129 05:29:19.770328 139866171975424 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.0744822025299072, loss=2.2506115436553955
I0129 05:29:53.480743 139865240893184 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1966687440872192, loss=2.4153409004211426
I0129 05:30:27.140472 139866171975424 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0879008769989014, loss=2.3095104694366455
I0129 05:31:00.853514 139865240893184 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.2798161506652832, loss=2.324510097503662
I0129 05:31:34.593691 139866171975424 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.2295299768447876, loss=2.4258906841278076
I0129 05:32:01.696791 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:32:07.941434 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:32:16.841710 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:32:19.462710 140027215431488 submission_runner.py:408] Time since start: 14832.80s, 	Step: 42282, 	{'train/accuracy': 0.3134167790412903, 'train/loss': 3.368189573287964, 'validation/accuracy': 0.2895599901676178, 'validation/loss': 3.5763092041015625, 'validation/num_examples': 50000, 'test/accuracy': 0.21320000290870667, 'test/loss': 4.345427513122559, 'test/num_examples': 10000, 'score': 14316.756876945496, 'total_duration': 14832.802520275116, 'accumulated_submission_time': 14316.756876945496, 'accumulated_eval_time': 513.688027381897, 'accumulated_logging_time': 0.8791120052337646}
I0129 05:32:19.488167 139865232500480 logging_writer.py:48] [42282] accumulated_eval_time=513.688027, accumulated_logging_time=0.879112, accumulated_submission_time=14316.756877, global_step=42282, preemption_count=0, score=14316.756877, test/accuracy=0.213200, test/loss=4.345428, test/num_examples=10000, total_duration=14832.802520, train/accuracy=0.313417, train/loss=3.368190, validation/accuracy=0.289560, validation/loss=3.576309, validation/num_examples=50000
I0129 05:32:25.910268 139865760950016 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1667557954788208, loss=2.2457313537597656
I0129 05:32:59.567369 139865232500480 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.1800142526626587, loss=2.359816074371338
I0129 05:33:33.292824 139865760950016 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.1508855819702148, loss=2.2113261222839355
I0129 05:34:06.940654 139865232500480 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.142284631729126, loss=2.2461981773376465
I0129 05:34:40.798135 139865760950016 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2128242254257202, loss=2.276074171066284
I0129 05:35:14.491112 139865232500480 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.1439296007156372, loss=2.2586631774902344
I0129 05:35:48.210523 139865760950016 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.183528184890747, loss=2.258591651916504
I0129 05:36:21.941872 139865232500480 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0976604223251343, loss=2.181462526321411
I0129 05:36:55.676889 139865760950016 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.2257136106491089, loss=2.2438323497772217
I0129 05:37:29.366303 139865232500480 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.143277883529663, loss=2.1170382499694824
I0129 05:38:03.104001 139865760950016 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.0743879079818726, loss=2.165881395339966
I0129 05:38:36.784167 139865232500480 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1766886711120605, loss=2.37446928024292
I0129 05:39:10.499678 139865760950016 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1236318349838257, loss=2.29799747467041
I0129 05:39:44.165795 139865232500480 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.162423014640808, loss=2.1512491703033447
I0129 05:40:17.865429 139865760950016 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1464022397994995, loss=2.264894723892212
I0129 05:40:49.665798 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:40:55.911881 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:41:04.859102 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:41:07.817224 140027215431488 submission_runner.py:408] Time since start: 15361.16s, 	Step: 43796, 	{'train/accuracy': 0.2523716390132904, 'train/loss': 3.879667043685913, 'validation/accuracy': 0.23787999153137207, 'validation/loss': 4.018867015838623, 'validation/num_examples': 50000, 'test/accuracy': 0.1721000075340271, 'test/loss': 4.735334873199463, 'test/num_examples': 10000, 'score': 14826.869350671768, 'total_duration': 15361.157025814056, 'accumulated_submission_time': 14826.869350671768, 'accumulated_eval_time': 531.8394250869751, 'accumulated_logging_time': 0.916710615158081}
I0129 05:41:07.842376 139866171975424 logging_writer.py:48] [43796] accumulated_eval_time=531.839425, accumulated_logging_time=0.916711, accumulated_submission_time=14826.869351, global_step=43796, preemption_count=0, score=14826.869351, test/accuracy=0.172100, test/loss=4.735335, test/num_examples=10000, total_duration=15361.157026, train/accuracy=0.252372, train/loss=3.879667, validation/accuracy=0.237880, validation/loss=4.018867, validation/num_examples=50000
I0129 05:41:09.536840 139866180368128 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1569286584854126, loss=2.3215982913970947
I0129 05:41:43.214359 139866171975424 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2646210193634033, loss=2.396078586578369
I0129 05:42:16.912779 139866180368128 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1775596141815186, loss=2.364374876022339
I0129 05:42:50.612849 139866171975424 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.1541388034820557, loss=2.3387115001678467
I0129 05:43:24.327016 139866180368128 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.118143916130066, loss=2.2902028560638428
I0129 05:43:58.002441 139866171975424 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1535042524337769, loss=2.284010648727417
I0129 05:44:31.703211 139866180368128 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.362077236175537, loss=2.3565196990966797
I0129 05:45:05.376636 139866171975424 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.0955848693847656, loss=2.2130279541015625
I0129 05:45:39.070386 139866180368128 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.3635705709457397, loss=2.268780469894409
I0129 05:46:12.740356 139866171975424 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.299033761024475, loss=2.1863787174224854
I0129 05:46:46.444788 139866180368128 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.591180443763733, loss=2.315155029296875
I0129 05:47:20.112398 139866171975424 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.1512912511825562, loss=2.2528791427612305
I0129 05:47:53.961681 139866180368128 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1927491426467896, loss=2.22416353225708
I0129 05:48:27.615581 139866171975424 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.2106610536575317, loss=2.336841583251953
I0129 05:49:01.324959 139866180368128 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.3630207777023315, loss=2.306807041168213
I0129 05:49:34.989169 139866171975424 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.3552076816558838, loss=2.2620480060577393
I0129 05:49:37.823379 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:49:44.029403 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:49:52.995139 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:49:55.587345 140027215431488 submission_runner.py:408] Time since start: 15888.93s, 	Step: 45310, 	{'train/accuracy': 0.1259765625, 'train/loss': 5.498228073120117, 'validation/accuracy': 0.11673999577760696, 'validation/loss': 5.669344425201416, 'validation/num_examples': 50000, 'test/accuracy': 0.08700000494718552, 'test/loss': 6.223611831665039, 'test/num_examples': 10000, 'score': 15336.784964323044, 'total_duration': 15888.927158594131, 'accumulated_submission_time': 15336.784964323044, 'accumulated_eval_time': 549.6033554077148, 'accumulated_logging_time': 0.9554266929626465}
I0129 05:49:55.617424 139865240893184 logging_writer.py:48] [45310] accumulated_eval_time=549.603355, accumulated_logging_time=0.955427, accumulated_submission_time=15336.784964, global_step=45310, preemption_count=0, score=15336.784964, test/accuracy=0.087000, test/loss=6.223612, test/num_examples=10000, total_duration=15888.927159, train/accuracy=0.125977, train/loss=5.498228, validation/accuracy=0.116740, validation/loss=5.669344, validation/num_examples=50000
I0129 05:50:26.322219 139865760950016 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1614665985107422, loss=2.3676857948303223
I0129 05:51:00.007116 139865240893184 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.2004350423812866, loss=2.2727012634277344
I0129 05:51:33.723295 139865760950016 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.141565203666687, loss=2.2589192390441895
I0129 05:52:07.365956 139865240893184 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1008213758468628, loss=2.2122864723205566
I0129 05:52:41.085500 139865760950016 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1245590448379517, loss=2.248755931854248
I0129 05:53:14.771472 139865240893184 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1507067680358887, loss=2.1731088161468506
I0129 05:53:48.486193 139865760950016 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.2574032545089722, loss=2.2757012844085693
I0129 05:54:22.315531 139865240893184 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.1091896295547485, loss=2.239863157272339
I0129 05:54:56.017616 139865760950016 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.1439874172210693, loss=2.5118331909179688
I0129 05:55:29.675799 139865240893184 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.2979917526245117, loss=2.3062679767608643
I0129 05:56:03.388790 139865760950016 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.229276418685913, loss=2.2964582443237305
I0129 05:56:37.058982 139865240893184 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.3665025234222412, loss=2.3303277492523193
I0129 05:57:10.757810 139865760950016 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.216346025466919, loss=2.3121392726898193
I0129 05:57:44.429325 139865240893184 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.250078797340393, loss=2.32427716255188
I0129 05:58:18.156440 139865760950016 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.2665655612945557, loss=2.2045772075653076
I0129 05:58:25.696868 140027215431488 spec.py:321] Evaluating on the training split.
I0129 05:58:31.986455 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 05:58:40.933171 140027215431488 spec.py:349] Evaluating on the test split.
I0129 05:58:43.459529 140027215431488 submission_runner.py:408] Time since start: 16416.80s, 	Step: 46824, 	{'train/accuracy': 0.3132772445678711, 'train/loss': 3.4213531017303467, 'validation/accuracy': 0.2969000041484833, 'validation/loss': 3.581291675567627, 'validation/num_examples': 50000, 'test/accuracy': 0.21540001034736633, 'test/loss': 4.327939033508301, 'test/num_examples': 10000, 'score': 15846.803076267242, 'total_duration': 16416.799332618713, 'accumulated_submission_time': 15846.803076267242, 'accumulated_eval_time': 567.3659672737122, 'accumulated_logging_time': 0.9953644275665283}
I0129 05:58:43.488149 139866171975424 logging_writer.py:48] [46824] accumulated_eval_time=567.365967, accumulated_logging_time=0.995364, accumulated_submission_time=15846.803076, global_step=46824, preemption_count=0, score=15846.803076, test/accuracy=0.215400, test/loss=4.327939, test/num_examples=10000, total_duration=16416.799333, train/accuracy=0.313277, train/loss=3.421353, validation/accuracy=0.296900, validation/loss=3.581292, validation/num_examples=50000
I0129 05:59:09.448362 139866180368128 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.1405179500579834, loss=2.2064878940582275
I0129 05:59:43.101617 139866171975424 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1721532344818115, loss=2.285773992538452
I0129 06:00:16.820116 139866180368128 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.3717615604400635, loss=2.3150930404663086
I0129 06:00:50.632442 139866171975424 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.2511265277862549, loss=2.2370858192443848
I0129 06:01:24.362347 139866180368128 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.2443974018096924, loss=2.2576606273651123
I0129 06:01:58.035739 139866171975424 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1905137300491333, loss=2.2996132373809814
I0129 06:02:31.737138 139866180368128 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.158013105392456, loss=2.1735618114471436
I0129 06:03:05.423376 139866171975424 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.3122913837432861, loss=2.2996609210968018
I0129 06:03:39.136146 139866180368128 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2280287742614746, loss=2.308737277984619
I0129 06:04:12.798837 139866171975424 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.244030237197876, loss=2.3534786701202393
I0129 06:04:46.518098 139866180368128 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.2509369850158691, loss=2.2904911041259766
I0129 06:05:20.204941 139866171975424 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0969502925872803, loss=2.233786106109619
I0129 06:05:53.936678 139866180368128 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.274546504020691, loss=2.3813533782958984
I0129 06:06:27.601268 139866171975424 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.2325645685195923, loss=2.1080474853515625
I0129 06:07:01.401634 139866180368128 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.257328987121582, loss=2.2712619304656982
I0129 06:07:13.701864 140027215431488 spec.py:321] Evaluating on the training split.
I0129 06:07:19.871956 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 06:07:28.599645 140027215431488 spec.py:349] Evaluating on the test split.
I0129 06:07:31.253072 140027215431488 submission_runner.py:408] Time since start: 16944.59s, 	Step: 48338, 	{'train/accuracy': 0.3341836631298065, 'train/loss': 3.1928305625915527, 'validation/accuracy': 0.31797999143600464, 'validation/loss': 3.339296817779541, 'validation/num_examples': 50000, 'test/accuracy': 0.23990000784397125, 'test/loss': 4.064082145690918, 'test/num_examples': 10000, 'score': 16356.954234361649, 'total_duration': 16944.592866659164, 'accumulated_submission_time': 16356.954234361649, 'accumulated_eval_time': 584.9171187877655, 'accumulated_logging_time': 1.0334186553955078}
I0129 06:07:31.281647 139865769342720 logging_writer.py:48] [48338] accumulated_eval_time=584.917119, accumulated_logging_time=1.033419, accumulated_submission_time=16356.954234, global_step=48338, preemption_count=0, score=16356.954234, test/accuracy=0.239900, test/loss=4.064082, test/num_examples=10000, total_duration=16944.592867, train/accuracy=0.334184, train/loss=3.192831, validation/accuracy=0.317980, validation/loss=3.339297, validation/num_examples=50000
I0129 06:07:52.563431 139866163582720 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1799651384353638, loss=2.2607967853546143
I0129 06:08:26.267530 139865769342720 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.159729242324829, loss=2.0130553245544434
I0129 06:08:59.965259 139866163582720 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.2249327898025513, loss=2.284257173538208
I0129 06:09:33.669779 139865769342720 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1603364944458008, loss=2.1112172603607178
I0129 06:10:07.356175 139866163582720 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.1977514028549194, loss=2.3013553619384766
I0129 06:10:41.056657 139865769342720 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.3166329860687256, loss=2.2335567474365234
I0129 06:11:14.736945 139866163582720 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.221824288368225, loss=2.3335187435150146
I0129 06:11:48.463301 139865769342720 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.206084966659546, loss=2.4105987548828125
I0129 06:12:22.117548 139866163582720 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.3179072141647339, loss=2.1110424995422363
I0129 06:12:55.815136 139865769342720 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.2050920724868774, loss=2.2604706287384033
I0129 06:13:29.684746 139866163582720 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1982555389404297, loss=2.16153621673584
I0129 06:14:03.405381 139865769342720 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.10688054561615, loss=2.397414207458496
I0129 06:14:37.100695 139866163582720 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.190546989440918, loss=2.2244577407836914
I0129 06:15:10.806980 139865769342720 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.1738436222076416, loss=2.2709429264068604
I0129 06:15:44.479712 139866163582720 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1866692304611206, loss=2.1438801288604736
I0129 06:16:01.500283 140027215431488 spec.py:321] Evaluating on the training split.
I0129 06:16:07.768899 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 06:16:16.566019 140027215431488 spec.py:349] Evaluating on the test split.
I0129 06:16:19.198871 140027215431488 submission_runner.py:408] Time since start: 17472.54s, 	Step: 49852, 	{'train/accuracy': 0.2771245241165161, 'train/loss': 3.7622814178466797, 'validation/accuracy': 0.2566399872303009, 'validation/loss': 3.951295852661133, 'validation/num_examples': 50000, 'test/accuracy': 0.1909000128507614, 'test/loss': 4.771968841552734, 'test/num_examples': 10000, 'score': 16867.11060857773, 'total_duration': 17472.53868341446, 'accumulated_submission_time': 16867.11060857773, 'accumulated_eval_time': 602.6156764030457, 'accumulated_logging_time': 1.071347713470459}
I0129 06:16:19.226215 139865240893184 logging_writer.py:48] [49852] accumulated_eval_time=602.615676, accumulated_logging_time=1.071348, accumulated_submission_time=16867.110609, global_step=49852, preemption_count=0, score=16867.110609, test/accuracy=0.190900, test/loss=4.771969, test/num_examples=10000, total_duration=17472.538683, train/accuracy=0.277125, train/loss=3.762281, validation/accuracy=0.256640, validation/loss=3.951296, validation/num_examples=50000
I0129 06:16:35.766974 139865760950016 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.3797152042388916, loss=2.2996468544006348
I0129 06:17:09.491389 139865240893184 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.2377634048461914, loss=2.2042300701141357
I0129 06:17:43.164554 139865760950016 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.1856275796890259, loss=2.238589286804199
I0129 06:18:16.880750 139865240893184 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.2125096321105957, loss=2.3258376121520996
I0129 06:18:50.513652 139865760950016 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.0801700353622437, loss=2.2226006984710693
I0129 06:19:24.224033 139865240893184 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2961570024490356, loss=2.209707498550415
I0129 06:19:58.029724 139865760950016 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.2280899286270142, loss=2.268519163131714
I0129 06:20:31.756839 139865240893184 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1728248596191406, loss=2.1025984287261963
I0129 06:21:05.423083 139865760950016 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.3237396478652954, loss=2.2396368980407715
I0129 06:21:39.145642 139865240893184 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.17390775680542, loss=2.2165749073028564
I0129 06:22:12.815653 139865760950016 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.192111611366272, loss=2.0887610912323
I0129 06:22:46.530181 139865240893184 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.2109781503677368, loss=2.2444469928741455
I0129 06:23:20.184730 139865760950016 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.2201576232910156, loss=2.1826171875
I0129 06:23:53.903648 139865240893184 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.2632955312728882, loss=2.2358222007751465
I0129 06:24:27.561097 139865760950016 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.133063793182373, loss=2.3550517559051514
I0129 06:24:49.263969 140027215431488 spec.py:321] Evaluating on the training split.
I0129 06:24:55.702206 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 06:25:04.654087 140027215431488 spec.py:349] Evaluating on the test split.
I0129 06:25:07.184668 140027215431488 submission_runner.py:408] Time since start: 18000.52s, 	Step: 51366, 	{'train/accuracy': 0.10586734116077423, 'train/loss': 6.146651268005371, 'validation/accuracy': 0.09307999908924103, 'validation/loss': 6.373527526855469, 'validation/num_examples': 50000, 'test/accuracy': 0.06830000132322311, 'test/loss': 6.9438652992248535, 'test/num_examples': 10000, 'score': 17377.086690425873, 'total_duration': 18000.52445960045, 'accumulated_submission_time': 17377.086690425873, 'accumulated_eval_time': 620.5363309383392, 'accumulated_logging_time': 1.1086018085479736}
I0129 06:25:07.210010 139865232500480 logging_writer.py:48] [51366] accumulated_eval_time=620.536331, accumulated_logging_time=1.108602, accumulated_submission_time=17377.086690, global_step=51366, preemption_count=0, score=17377.086690, test/accuracy=0.068300, test/loss=6.943865, test/num_examples=10000, total_duration=18000.524460, train/accuracy=0.105867, train/loss=6.146651, validation/accuracy=0.093080, validation/loss=6.373528, validation/num_examples=50000
I0129 06:25:19.016431 139865240893184 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.139901876449585, loss=2.255927324295044
I0129 06:25:52.698505 139865232500480 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.229779601097107, loss=2.38800311088562
I0129 06:26:26.529247 139865240893184 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2829787731170654, loss=2.2304184436798096
I0129 06:27:00.251260 139865232500480 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1781777143478394, loss=2.205522060394287
I0129 06:27:33.948569 139865240893184 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.3737542629241943, loss=2.2059485912323
I0129 06:28:07.645985 139865232500480 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2030586004257202, loss=2.2166178226470947
I0129 06:28:41.347507 139865240893184 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.3766677379608154, loss=2.205413341522217
I0129 06:29:15.016755 139865232500480 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.2480779886245728, loss=2.255341053009033
I0129 06:29:48.725628 139865240893184 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.1888123750686646, loss=2.118781089782715
I0129 06:30:22.397314 139865232500480 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.168389916419983, loss=2.1176235675811768
I0129 06:30:56.092941 139865240893184 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.3316634893417358, loss=2.217129945755005
I0129 06:31:29.767909 139865232500480 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.3386775255203247, loss=2.254189968109131
I0129 06:32:03.471368 139865240893184 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.5019848346710205, loss=2.222806453704834
I0129 06:32:37.165587 139865232500480 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.3216569423675537, loss=2.3231780529022217
I0129 06:33:10.988775 139865240893184 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.060807704925537, loss=2.162473201751709
I0129 06:33:37.406383 140027215431488 spec.py:321] Evaluating on the training split.
I0129 06:33:43.613025 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 06:33:52.554540 140027215431488 spec.py:349] Evaluating on the test split.
I0129 06:33:55.115458 140027215431488 submission_runner.py:408] Time since start: 18528.46s, 	Step: 52880, 	{'train/accuracy': 0.4368024468421936, 'train/loss': 2.5360634326934814, 'validation/accuracy': 0.40303999185562134, 'validation/loss': 2.7369699478149414, 'validation/num_examples': 50000, 'test/accuracy': 0.30410000681877136, 'test/loss': 3.4496142864227295, 'test/num_examples': 10000, 'score': 17887.21930384636, 'total_duration': 18528.455255270004, 'accumulated_submission_time': 17887.21930384636, 'accumulated_eval_time': 638.2453672885895, 'accumulated_logging_time': 1.144514799118042}
I0129 06:33:55.141901 139865232500480 logging_writer.py:48] [52880] accumulated_eval_time=638.245367, accumulated_logging_time=1.144515, accumulated_submission_time=17887.219304, global_step=52880, preemption_count=0, score=17887.219304, test/accuracy=0.304100, test/loss=3.449614, test/num_examples=10000, total_duration=18528.455255, train/accuracy=0.436802, train/loss=2.536063, validation/accuracy=0.403040, validation/loss=2.736970, validation/num_examples=50000
I0129 06:34:02.203177 139865240893184 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1767313480377197, loss=2.1006195545196533
I0129 06:34:35.851515 139865232500480 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.2135053873062134, loss=2.1939594745635986
I0129 06:35:09.559871 139865240893184 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2071841955184937, loss=2.196000814437866
I0129 06:35:43.213735 139865232500480 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.3547228574752808, loss=2.291762351989746
I0129 06:36:16.928422 139865240893184 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.263160228729248, loss=2.3364851474761963
I0129 06:36:50.587295 139865232500480 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2879050970077515, loss=2.19852876663208
I0129 06:37:24.308370 139865240893184 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.176720380783081, loss=2.2536494731903076
I0129 06:37:57.971254 139865232500480 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3166855573654175, loss=2.306770086288452
I0129 06:38:31.680619 139865240893184 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.2340900897979736, loss=2.334367275238037
I0129 06:39:05.337768 139865232500480 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.1520158052444458, loss=2.3226795196533203
I0129 06:39:39.254139 139865240893184 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.156813144683838, loss=2.2087552547454834
I0129 06:40:12.921294 139865232500480 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.2617700099945068, loss=2.212085247039795
I0129 06:40:46.644479 139865240893184 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.1588693857192993, loss=2.2131540775299072
I0129 06:41:20.303504 139865232500480 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1035950183868408, loss=2.188462018966675
I0129 06:41:54.012762 139865240893184 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.1783332824707031, loss=2.2213869094848633
I0129 06:42:25.116120 140027215431488 spec.py:321] Evaluating on the training split.
I0129 06:42:31.367293 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 06:42:40.117651 140027215431488 spec.py:349] Evaluating on the test split.
I0129 06:42:42.755064 140027215431488 submission_runner.py:408] Time since start: 19056.09s, 	Step: 54394, 	{'train/accuracy': 0.2947225570678711, 'train/loss': 3.6311120986938477, 'validation/accuracy': 0.28077998757362366, 'validation/loss': 3.7400660514831543, 'validation/num_examples': 50000, 'test/accuracy': 0.19930000603199005, 'test/loss': 4.530226230621338, 'test/num_examples': 10000, 'score': 18397.130579948425, 'total_duration': 19056.094877958298, 'accumulated_submission_time': 18397.130579948425, 'accumulated_eval_time': 655.88427901268, 'accumulated_logging_time': 1.1814467906951904}
I0129 06:42:42.782715 139866163582720 logging_writer.py:48] [54394] accumulated_eval_time=655.884279, accumulated_logging_time=1.181447, accumulated_submission_time=18397.130580, global_step=54394, preemption_count=0, score=18397.130580, test/accuracy=0.199300, test/loss=4.530226, test/num_examples=10000, total_duration=19056.094878, train/accuracy=0.294723, train/loss=3.631112, validation/accuracy=0.280780, validation/loss=3.740066, validation/num_examples=50000
I0129 06:42:45.158521 139866180368128 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.3057132959365845, loss=2.192021369934082
I0129 06:43:18.813673 139866163582720 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.3817689418792725, loss=2.2148821353912354
I0129 06:43:52.508798 139866180368128 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.2427475452423096, loss=2.1353073120117188
I0129 06:44:26.185751 139866163582720 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.3482722043991089, loss=2.2143263816833496
I0129 06:44:59.887012 139866180368128 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.3419630527496338, loss=2.0232160091400146
I0129 06:45:33.583346 139866163582720 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.280634880065918, loss=2.219343900680542
I0129 06:46:07.448124 139866180368128 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2959654331207275, loss=2.1004621982574463
I0129 06:46:41.143728 139866163582720 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.1622198820114136, loss=2.1965250968933105
I0129 06:47:14.830032 139866180368128 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.419910192489624, loss=2.1768319606781006
I0129 06:47:48.522663 139866163582720 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.213712215423584, loss=2.284780979156494
I0129 06:48:22.227904 139866180368128 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2340247631072998, loss=2.1660211086273193
I0129 06:48:55.911355 139866163582720 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.268640160560608, loss=2.045898675918579
I0129 06:49:29.601444 139866180368128 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2386289834976196, loss=2.2106916904449463
I0129 06:50:03.287452 139866163582720 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.193089485168457, loss=2.106067180633545
I0129 06:50:36.976809 139866180368128 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.2502045631408691, loss=2.1936349868774414
I0129 06:51:10.667271 139866163582720 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.4165230989456177, loss=2.22959041595459
I0129 06:51:12.836538 140027215431488 spec.py:321] Evaluating on the training split.
I0129 06:51:19.001458 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 06:51:27.754500 140027215431488 spec.py:349] Evaluating on the test split.
I0129 06:51:30.374571 140027215431488 submission_runner.py:408] Time since start: 19583.71s, 	Step: 55908, 	{'train/accuracy': 0.18937340378761292, 'train/loss': 4.625549793243408, 'validation/accuracy': 0.1741199940443039, 'validation/loss': 4.754756927490234, 'validation/num_examples': 50000, 'test/accuracy': 0.1299000084400177, 'test/loss': 5.389272212982178, 'test/num_examples': 10000, 'score': 18907.1184194088, 'total_duration': 19583.714373588562, 'accumulated_submission_time': 18907.1184194088, 'accumulated_eval_time': 673.4222629070282, 'accumulated_logging_time': 1.218961477279663}
I0129 06:51:30.403522 139865760950016 logging_writer.py:48] [55908] accumulated_eval_time=673.422263, accumulated_logging_time=1.218961, accumulated_submission_time=18907.118419, global_step=55908, preemption_count=0, score=18907.118419, test/accuracy=0.129900, test/loss=5.389272, test/num_examples=10000, total_duration=19583.714374, train/accuracy=0.189373, train/loss=4.625550, validation/accuracy=0.174120, validation/loss=4.754757, validation/num_examples=50000
I0129 06:52:01.744545 139865769342720 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0892091989517212, loss=2.183471918106079
I0129 06:52:35.556163 139865760950016 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.4644250869750977, loss=2.3215107917785645
I0129 06:53:09.284636 139865769342720 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.2051243782043457, loss=2.230607748031616
I0129 06:53:42.939738 139865760950016 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.232005000114441, loss=2.2186386585235596
I0129 06:54:16.643604 139865769342720 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2194175720214844, loss=2.113420248031616
I0129 06:54:50.294899 139865760950016 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2030079364776611, loss=2.2718403339385986
I0129 06:55:24.002728 139865769342720 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.2854511737823486, loss=2.1619277000427246
I0129 06:55:57.666428 139865760950016 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.3931314945220947, loss=2.3314263820648193
I0129 06:56:31.377099 139865769342720 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.3683522939682007, loss=2.126333475112915
I0129 06:57:05.059335 139865760950016 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.269298791885376, loss=2.175368547439575
I0129 06:57:38.754405 139865769342720 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.2803443670272827, loss=2.253934860229492
I0129 06:58:12.410698 139865760950016 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.2977083921432495, loss=2.2252354621887207
I0129 06:58:46.212600 139865769342720 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.5251548290252686, loss=2.23994517326355
I0129 06:59:19.908875 139865760950016 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.331045150756836, loss=2.115105628967285
I0129 06:59:53.616312 139865769342720 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2670612335205078, loss=2.245760917663574
I0129 07:00:00.498225 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:00:06.903843 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:00:15.393161 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:00:18.037892 140027215431488 submission_runner.py:408] Time since start: 20111.38s, 	Step: 57422, 	{'train/accuracy': 0.2808115482330322, 'train/loss': 3.7090742588043213, 'validation/accuracy': 0.26372000575065613, 'validation/loss': 3.829373598098755, 'validation/num_examples': 50000, 'test/accuracy': 0.19340001046657562, 'test/loss': 4.597098350524902, 'test/num_examples': 10000, 'score': 19417.15093255043, 'total_duration': 20111.377707242966, 'accumulated_submission_time': 19417.15093255043, 'accumulated_eval_time': 690.9619073867798, 'accumulated_logging_time': 1.2572824954986572}
I0129 07:00:18.064891 139865240893184 logging_writer.py:48] [57422] accumulated_eval_time=690.961907, accumulated_logging_time=1.257282, accumulated_submission_time=19417.150933, global_step=57422, preemption_count=0, score=19417.150933, test/accuracy=0.193400, test/loss=4.597098, test/num_examples=10000, total_duration=20111.377707, train/accuracy=0.280812, train/loss=3.709074, validation/accuracy=0.263720, validation/loss=3.829374, validation/num_examples=50000
I0129 07:00:44.740614 139865760950016 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.3065569400787354, loss=2.1896796226501465
I0129 07:01:18.428147 139865240893184 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.4565629959106445, loss=2.230377674102783
I0129 07:01:52.139790 139865760950016 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.4430956840515137, loss=2.1604745388031006
I0129 07:02:25.812733 139865240893184 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1782796382904053, loss=2.2245240211486816
I0129 07:02:59.490249 139865760950016 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.1924481391906738, loss=2.058940887451172
I0129 07:03:33.159418 139865240893184 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1454432010650635, loss=2.2591750621795654
I0129 07:04:06.858578 139865760950016 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.285833716392517, loss=2.2889199256896973
I0129 07:04:40.537036 139865240893184 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.252951741218567, loss=2.302891731262207
I0129 07:05:14.289167 139865760950016 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.3129193782806396, loss=2.1256632804870605
I0129 07:05:48.003347 139865240893184 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.1179828643798828, loss=2.2151694297790527
I0129 07:06:21.714217 139865760950016 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.213912010192871, loss=2.2325870990753174
I0129 07:06:55.382097 139865240893184 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.2841821908950806, loss=2.1269278526306152
I0129 07:07:29.068317 139865760950016 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1173663139343262, loss=2.195511817932129
I0129 07:08:02.718356 139865240893184 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.2576760053634644, loss=2.154149055480957
I0129 07:08:36.418198 139865760950016 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.2066856622695923, loss=2.2733154296875
I0129 07:08:48.362761 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:08:54.700268 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:09:03.543043 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:09:06.049427 140027215431488 submission_runner.py:408] Time since start: 20639.39s, 	Step: 58937, 	{'train/accuracy': 0.22622369229793549, 'train/loss': 4.323338985443115, 'validation/accuracy': 0.20683999359607697, 'validation/loss': 4.579726219177246, 'validation/num_examples': 50000, 'test/accuracy': 0.1551000028848648, 'test/loss': 5.24745512008667, 'test/num_examples': 10000, 'score': 19927.386483430862, 'total_duration': 20639.389159202576, 'accumulated_submission_time': 19927.386483430862, 'accumulated_eval_time': 708.6484682559967, 'accumulated_logging_time': 1.293562412261963}
I0129 07:09:06.079025 139865232500480 logging_writer.py:48] [58937] accumulated_eval_time=708.648468, accumulated_logging_time=1.293562, accumulated_submission_time=19927.386483, global_step=58937, preemption_count=0, score=19927.386483, test/accuracy=0.155100, test/loss=5.247455, test/num_examples=10000, total_duration=20639.389159, train/accuracy=0.226224, train/loss=4.323339, validation/accuracy=0.206840, validation/loss=4.579726, validation/num_examples=50000
I0129 07:09:27.613837 139865240893184 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2292678356170654, loss=2.1812195777893066
I0129 07:10:01.359596 139865232500480 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.4686710834503174, loss=2.2357306480407715
I0129 07:10:35.032946 139865240893184 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.2986818552017212, loss=2.170968770980835
I0129 07:11:08.732041 139865232500480 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2197171449661255, loss=2.109929084777832
I0129 07:11:42.519953 139865240893184 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2807152271270752, loss=2.1976938247680664
I0129 07:12:16.267121 139865232500480 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.3305798768997192, loss=2.1726136207580566
I0129 07:12:49.931730 139865240893184 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.323840856552124, loss=2.312096118927002
I0129 07:13:23.643609 139865232500480 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2209407091140747, loss=2.1216559410095215
I0129 07:13:57.293473 139865240893184 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.324188232421875, loss=2.14262056350708
I0129 07:14:31.006913 139865232500480 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1670987606048584, loss=2.1402599811553955
I0129 07:15:04.670116 139865240893184 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.410579800605774, loss=2.1720433235168457
I0129 07:15:38.380044 139865232500480 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3086931705474854, loss=2.159728527069092
I0129 07:16:12.060782 139865240893184 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.2140247821807861, loss=2.225430965423584
I0129 07:16:45.784299 139865232500480 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.1846303939819336, loss=2.2485923767089844
I0129 07:17:19.452224 139865240893184 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.229934811592102, loss=2.2491235733032227
I0129 07:17:36.125106 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:17:42.377070 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:17:51.156354 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:17:53.792575 140027215431488 submission_runner.py:408] Time since start: 21167.13s, 	Step: 60451, 	{'train/accuracy': 0.38809388875961304, 'train/loss': 2.8707337379455566, 'validation/accuracy': 0.3535799980163574, 'validation/loss': 3.1261491775512695, 'validation/num_examples': 50000, 'test/accuracy': 0.2665000259876251, 'test/loss': 3.855259656906128, 'test/num_examples': 10000, 'score': 20437.370439767838, 'total_duration': 21167.13238477707, 'accumulated_submission_time': 20437.370439767838, 'accumulated_eval_time': 726.3159120082855, 'accumulated_logging_time': 1.3322150707244873}
I0129 07:17:53.821511 139866171975424 logging_writer.py:48] [60451] accumulated_eval_time=726.315912, accumulated_logging_time=1.332215, accumulated_submission_time=20437.370440, global_step=60451, preemption_count=0, score=20437.370440, test/accuracy=0.266500, test/loss=3.855260, test/num_examples=10000, total_duration=21167.132385, train/accuracy=0.388094, train/loss=2.870734, validation/accuracy=0.353580, validation/loss=3.126149, validation/num_examples=50000
I0129 07:18:10.756931 139866180368128 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.3066352605819702, loss=2.0782687664031982
I0129 07:18:44.475368 139866171975424 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.2887052297592163, loss=2.3158605098724365
I0129 07:19:18.158066 139866180368128 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.338422179222107, loss=2.1659975051879883
I0129 07:19:51.859005 139866171975424 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.324432134628296, loss=2.172333002090454
I0129 07:20:25.526805 139866180368128 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.350508689880371, loss=2.3919591903686523
I0129 07:20:59.217580 139866171975424 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2528712749481201, loss=2.3570282459259033
I0129 07:21:32.888854 139866180368128 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2787162065505981, loss=2.295034646987915
I0129 07:22:06.566195 139866171975424 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.2932931184768677, loss=2.211787700653076
I0129 07:22:40.242423 139866180368128 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.4074280261993408, loss=2.3045835494995117
I0129 07:23:13.927514 139866171975424 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.2040550708770752, loss=2.109173059463501
I0129 07:23:47.606364 139866180368128 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.282010793685913, loss=2.1687428951263428
I0129 07:24:21.386369 139866171975424 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.339713454246521, loss=2.149369239807129
I0129 07:24:55.096495 139866180368128 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.3805358409881592, loss=2.1063222885131836
I0129 07:25:28.807784 139866171975424 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.4688022136688232, loss=2.1816656589508057
I0129 07:26:02.465815 139866180368128 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.3740543127059937, loss=2.141294479370117
I0129 07:26:23.822679 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:26:30.019150 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:26:38.611982 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:26:41.323602 140027215431488 submission_runner.py:408] Time since start: 21694.66s, 	Step: 61965, 	{'train/accuracy': 0.2292729616165161, 'train/loss': 4.253842830657959, 'validation/accuracy': 0.22010000050067902, 'validation/loss': 4.375511646270752, 'validation/num_examples': 50000, 'test/accuracy': 0.16340000927448273, 'test/loss': 4.972453594207764, 'test/num_examples': 10000, 'score': 20947.30847287178, 'total_duration': 21694.66338658333, 'accumulated_submission_time': 20947.30847287178, 'accumulated_eval_time': 743.8167865276337, 'accumulated_logging_time': 1.3707172870635986}
I0129 07:26:41.353129 139865240893184 logging_writer.py:48] [61965] accumulated_eval_time=743.816787, accumulated_logging_time=1.370717, accumulated_submission_time=20947.308473, global_step=61965, preemption_count=0, score=20947.308473, test/accuracy=0.163400, test/loss=4.972454, test/num_examples=10000, total_duration=21694.663387, train/accuracy=0.229273, train/loss=4.253843, validation/accuracy=0.220100, validation/loss=4.375512, validation/num_examples=50000
I0129 07:26:53.477849 139865760950016 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2715779542922974, loss=2.1927433013916016
I0129 07:27:27.149162 139865240893184 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.4393640756607056, loss=2.175577163696289
I0129 07:28:00.866729 139865760950016 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.4473135471343994, loss=2.167097568511963
I0129 07:28:34.542203 139865240893184 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.2806919813156128, loss=2.2080094814300537
I0129 07:29:08.256817 139865760950016 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.2905882596969604, loss=2.1979002952575684
I0129 07:29:41.927353 139865240893184 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.3033446073532104, loss=2.245581865310669
I0129 07:30:15.630576 139865760950016 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4037166833877563, loss=2.1348116397857666
I0129 07:30:49.302517 139865240893184 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.336627721786499, loss=2.2169339656829834
I0129 07:31:23.196566 139865760950016 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.32815682888031, loss=2.2226569652557373
I0129 07:31:56.864842 139865240893184 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.398558259010315, loss=2.2615392208099365
I0129 07:32:30.579947 139865760950016 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3224692344665527, loss=2.088552713394165
I0129 07:33:04.258754 139865240893184 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.3520489931106567, loss=2.312063217163086
I0129 07:33:37.974678 139865760950016 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.2594845294952393, loss=2.2103137969970703
I0129 07:34:11.632597 139865240893184 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.269634485244751, loss=2.100698947906494
I0129 07:34:45.337743 139865760950016 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3700320720672607, loss=2.102555274963379
I0129 07:35:11.391567 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:35:17.571289 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:35:26.135880 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:35:28.767164 140027215431488 submission_runner.py:408] Time since start: 22222.11s, 	Step: 63479, 	{'train/accuracy': 0.28366151452064514, 'train/loss': 3.6376185417175293, 'validation/accuracy': 0.27041998505592346, 'validation/loss': 3.7318127155303955, 'validation/num_examples': 50000, 'test/accuracy': 0.18970000743865967, 'test/loss': 4.5615234375, 'test/num_examples': 10000, 'score': 21457.283179998398, 'total_duration': 22222.106975317, 'accumulated_submission_time': 21457.283179998398, 'accumulated_eval_time': 761.1923484802246, 'accumulated_logging_time': 1.4095752239227295}
I0129 07:35:28.795636 139865232500480 logging_writer.py:48] [63479] accumulated_eval_time=761.192348, accumulated_logging_time=1.409575, accumulated_submission_time=21457.283180, global_step=63479, preemption_count=0, score=21457.283180, test/accuracy=0.189700, test/loss=4.561523, test/num_examples=10000, total_duration=22222.106975, train/accuracy=0.283662, train/loss=3.637619, validation/accuracy=0.270420, validation/loss=3.731813, validation/num_examples=50000
I0129 07:35:36.197543 139866163582720 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.3273385763168335, loss=2.2740423679351807
I0129 07:36:09.837771 139865232500480 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3338204622268677, loss=2.2782070636749268
I0129 07:36:43.549393 139866163582720 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.2575958967208862, loss=2.2589266300201416
I0129 07:37:17.334173 139865232500480 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.386413812637329, loss=2.1508309841156006
I0129 07:37:51.068516 139866163582720 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3409345149993896, loss=2.231775999069214
I0129 07:38:24.732183 139865232500480 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.528187870979309, loss=2.0924787521362305
I0129 07:38:58.444832 139866163582720 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.3145461082458496, loss=2.1702749729156494
I0129 07:39:32.118014 139865232500480 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.254350185394287, loss=1.9529467821121216
I0129 07:40:05.826346 139866163582720 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.3907371759414673, loss=2.1854381561279297
I0129 07:40:39.464101 139865232500480 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1662989854812622, loss=2.073293447494507
I0129 07:41:13.155268 139866163582720 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.4150505065917969, loss=2.1955935955047607
I0129 07:41:46.843804 139865232500480 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.290480375289917, loss=2.145562171936035
I0129 07:42:20.546223 139866163582720 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.372200608253479, loss=2.147557497024536
I0129 07:42:54.198842 139865232500480 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.348308801651001, loss=2.135485887527466
I0129 07:43:27.918811 139866163582720 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2734365463256836, loss=2.1424450874328613
I0129 07:43:58.837353 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:44:05.153340 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:44:13.810627 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:44:16.338364 140027215431488 submission_runner.py:408] Time since start: 22749.68s, 	Step: 64993, 	{'train/accuracy': 0.2693319320678711, 'train/loss': 3.9684836864471436, 'validation/accuracy': 0.26183998584747314, 'validation/loss': 4.07616662979126, 'validation/num_examples': 50000, 'test/accuracy': 0.19790001213550568, 'test/loss': 4.845719337463379, 'test/num_examples': 10000, 'score': 21967.26203918457, 'total_duration': 22749.678109169006, 'accumulated_submission_time': 21967.26203918457, 'accumulated_eval_time': 778.6932566165924, 'accumulated_logging_time': 1.4470484256744385}
I0129 07:44:16.370650 139865769342720 logging_writer.py:48] [64993] accumulated_eval_time=778.693257, accumulated_logging_time=1.447048, accumulated_submission_time=21967.262039, global_step=64993, preemption_count=0, score=21967.262039, test/accuracy=0.197900, test/loss=4.845719, test/num_examples=10000, total_duration=22749.678109, train/accuracy=0.269332, train/loss=3.968484, validation/accuracy=0.261840, validation/loss=4.076167, validation/num_examples=50000
I0129 07:44:19.077110 139866171975424 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3153630495071411, loss=2.1270103454589844
I0129 07:44:52.746306 139865769342720 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.353595495223999, loss=2.2268130779266357
I0129 07:45:26.420382 139866171975424 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.2679131031036377, loss=2.1046934127807617
I0129 07:46:00.134427 139865769342720 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.367283582687378, loss=2.370783567428589
I0129 07:46:33.814690 139866171975424 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.3390151262283325, loss=2.185391902923584
I0129 07:47:07.502902 139865769342720 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3826287984848022, loss=2.254920244216919
I0129 07:47:41.177123 139866171975424 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2827616930007935, loss=2.1081793308258057
I0129 07:48:14.879935 139865769342720 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.4395405054092407, loss=2.1157078742980957
I0129 07:48:48.567127 139866171975424 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.5145491361618042, loss=2.0730366706848145
I0129 07:49:22.260761 139865769342720 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.3838043212890625, loss=2.2013771533966064
I0129 07:49:55.960935 139866171975424 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.213093638420105, loss=2.182008981704712
I0129 07:50:29.799845 139865769342720 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.3304449319839478, loss=2.1184916496276855
I0129 07:51:03.502759 139866171975424 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.307650089263916, loss=2.2713463306427
I0129 07:51:37.205634 139865769342720 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3602951765060425, loss=2.2213478088378906
I0129 07:52:10.876586 139866171975424 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.3376755714416504, loss=2.1553711891174316
I0129 07:52:44.564742 139865769342720 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.3292062282562256, loss=2.16795015335083
I0129 07:52:46.394448 140027215431488 spec.py:321] Evaluating on the training split.
I0129 07:52:52.742817 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 07:53:01.503025 140027215431488 spec.py:349] Evaluating on the test split.
I0129 07:53:04.167545 140027215431488 submission_runner.py:408] Time since start: 23277.51s, 	Step: 66507, 	{'train/accuracy': 0.3372528553009033, 'train/loss': 3.1919519901275635, 'validation/accuracy': 0.31836000084877014, 'validation/loss': 3.3297736644744873, 'validation/num_examples': 50000, 'test/accuracy': 0.24860000610351562, 'test/loss': 4.025635242462158, 'test/num_examples': 10000, 'score': 22477.223001003265, 'total_duration': 23277.50735592842, 'accumulated_submission_time': 22477.223001003265, 'accumulated_eval_time': 796.4663238525391, 'accumulated_logging_time': 1.4883835315704346}
I0129 07:53:04.200919 139865224107776 logging_writer.py:48] [66507] accumulated_eval_time=796.466324, accumulated_logging_time=1.488384, accumulated_submission_time=22477.223001, global_step=66507, preemption_count=0, score=22477.223001, test/accuracy=0.248600, test/loss=4.025635, test/num_examples=10000, total_duration=23277.507356, train/accuracy=0.337253, train/loss=3.191952, validation/accuracy=0.318360, validation/loss=3.329774, validation/num_examples=50000
I0129 07:53:35.888802 139865232500480 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.5561127662658691, loss=2.1860105991363525
I0129 07:54:09.565342 139865224107776 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.4204083681106567, loss=2.2341508865356445
I0129 07:54:43.271879 139865232500480 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.2149112224578857, loss=2.1381454467773438
I0129 07:55:16.935171 139865224107776 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3428466320037842, loss=2.217125415802002
I0129 07:55:50.637590 139865232500480 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.450298547744751, loss=2.1986823081970215
I0129 07:56:24.320968 139865224107776 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3313730955123901, loss=2.2324483394622803
I0129 07:56:58.090072 139865232500480 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3026208877563477, loss=2.1060636043548584
I0129 07:57:31.780995 139865224107776 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.432938575744629, loss=2.065737247467041
I0129 07:58:05.518209 139865232500480 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2925351858139038, loss=2.1326870918273926
I0129 07:58:39.167709 139865224107776 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.564653754234314, loss=2.1570754051208496
I0129 07:59:12.886664 139865232500480 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3783661127090454, loss=2.1871302127838135
I0129 07:59:46.549761 139865224107776 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.2128393650054932, loss=2.085118293762207
I0129 08:00:20.274784 139865232500480 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.3845247030258179, loss=2.1867330074310303
I0129 08:00:53.951839 139865224107776 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2958393096923828, loss=2.1349501609802246
I0129 08:01:27.654436 139865232500480 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3961138725280762, loss=2.247202157974243
I0129 08:01:34.193799 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:01:40.457696 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:01:49.255590 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:01:51.893309 140027215431488 submission_runner.py:408] Time since start: 23805.23s, 	Step: 68021, 	{'train/accuracy': 0.3380899131298065, 'train/loss': 3.2569570541381836, 'validation/accuracy': 0.293720006942749, 'validation/loss': 3.590768575668335, 'validation/num_examples': 50000, 'test/accuracy': 0.2199000120162964, 'test/loss': 4.2673563957214355, 'test/num_examples': 10000, 'score': 22987.152262687683, 'total_duration': 23805.233120441437, 'accumulated_submission_time': 22987.152262687683, 'accumulated_eval_time': 814.1657972335815, 'accumulated_logging_time': 1.5314984321594238}
I0129 08:01:51.926541 139865232500480 logging_writer.py:48] [68021] accumulated_eval_time=814.165797, accumulated_logging_time=1.531498, accumulated_submission_time=22987.152263, global_step=68021, preemption_count=0, score=22987.152263, test/accuracy=0.219900, test/loss=4.267356, test/num_examples=10000, total_duration=23805.233120, train/accuracy=0.338090, train/loss=3.256957, validation/accuracy=0.293720, validation/loss=3.590769, validation/num_examples=50000
I0129 08:02:18.952271 139866171975424 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.3835759162902832, loss=2.195972442626953
I0129 08:02:52.663903 139865232500480 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3570092916488647, loss=2.15572452545166
I0129 08:03:26.495112 139866171975424 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.26469886302948, loss=2.0922420024871826
I0129 08:04:00.209227 139865232500480 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.4473761320114136, loss=2.1242809295654297
I0129 08:04:33.904763 139866171975424 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3907870054244995, loss=2.1866915225982666
I0129 08:05:07.557679 139865232500480 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.4783968925476074, loss=2.053109884262085
I0129 08:05:41.261051 139866171975424 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.434328317642212, loss=2.160464286804199
I0129 08:06:14.928409 139865232500480 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.5111128091812134, loss=2.1839444637298584
I0129 08:06:48.616738 139866171975424 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2751091718673706, loss=2.2638661861419678
I0129 08:07:22.308315 139865232500480 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.2884032726287842, loss=2.0591049194335938
I0129 08:07:55.990122 139866171975424 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.387242317199707, loss=2.232071876525879
I0129 08:08:29.690109 139865232500480 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3536618947982788, loss=2.0592517852783203
I0129 08:09:03.386330 139866171975424 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.3380850553512573, loss=2.025643825531006
I0129 08:09:37.184297 139865232500480 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.405454397201538, loss=2.084137201309204
I0129 08:10:10.902046 139866171975424 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.2677901983261108, loss=2.1967363357543945
I0129 08:10:22.164146 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:10:28.386088 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:10:37.304778 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:10:39.882246 140027215431488 submission_runner.py:408] Time since start: 24333.22s, 	Step: 69535, 	{'train/accuracy': 0.3651546537876129, 'train/loss': 3.035815715789795, 'validation/accuracy': 0.3411799967288971, 'validation/loss': 3.2265498638153076, 'validation/num_examples': 50000, 'test/accuracy': 0.2547000050544739, 'test/loss': 4.050992488861084, 'test/num_examples': 10000, 'score': 23497.329399108887, 'total_duration': 24333.222053050995, 'accumulated_submission_time': 23497.329399108887, 'accumulated_eval_time': 831.8838548660278, 'accumulated_logging_time': 1.574218988418579}
I0129 08:10:39.914034 139865232500480 logging_writer.py:48] [69535] accumulated_eval_time=831.883855, accumulated_logging_time=1.574219, accumulated_submission_time=23497.329399, global_step=69535, preemption_count=0, score=23497.329399, test/accuracy=0.254700, test/loss=4.050992, test/num_examples=10000, total_duration=24333.222053, train/accuracy=0.365155, train/loss=3.035816, validation/accuracy=0.341180, validation/loss=3.226550, validation/num_examples=50000
I0129 08:11:02.184468 139865240893184 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.512102723121643, loss=2.0350210666656494
I0129 08:11:35.891033 139865232500480 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.3493750095367432, loss=2.0721964836120605
I0129 08:12:09.555050 139865240893184 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.5020525455474854, loss=2.124176025390625
I0129 08:12:43.261701 139865232500480 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.4854079484939575, loss=2.1287784576416016
I0129 08:13:16.931968 139865240893184 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3251692056655884, loss=2.1602847576141357
I0129 08:13:50.630648 139865232500480 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.3106735944747925, loss=2.0915536880493164
I0129 08:14:24.310010 139865240893184 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.2087968587875366, loss=2.0493292808532715
I0129 08:14:58.039944 139865232500480 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.3500858545303345, loss=2.072784900665283
I0129 08:15:31.712574 139865240893184 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.441771388053894, loss=2.1421701908111572
I0129 08:16:05.522191 139865232500480 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.3808003664016724, loss=2.18367862701416
I0129 08:16:39.218254 139865240893184 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3789305686950684, loss=2.2570180892944336
I0129 08:17:12.930009 139865232500480 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2736351490020752, loss=1.9678959846496582
I0129 08:17:46.595984 139865240893184 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3646330833435059, loss=2.129950523376465
I0129 08:18:20.317712 139865232500480 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.500224232673645, loss=2.2059216499328613
I0129 08:18:53.991671 139865240893184 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.309874415397644, loss=2.2269604206085205
I0129 08:19:09.980101 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:19:16.162364 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:19:24.910481 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:19:27.566789 140027215431488 submission_runner.py:408] Time since start: 24860.91s, 	Step: 71049, 	{'train/accuracy': 0.38948899507522583, 'train/loss': 2.9351134300231934, 'validation/accuracy': 0.36055999994277954, 'validation/loss': 3.162463903427124, 'validation/num_examples': 50000, 'test/accuracy': 0.265500009059906, 'test/loss': 4.025924205780029, 'test/num_examples': 10000, 'score': 24007.331367492676, 'total_duration': 24860.906602859497, 'accumulated_submission_time': 24007.331367492676, 'accumulated_eval_time': 849.4705073833466, 'accumulated_logging_time': 1.6176283359527588}
I0129 08:19:27.596097 139866171975424 logging_writer.py:48] [71049] accumulated_eval_time=849.470507, accumulated_logging_time=1.617628, accumulated_submission_time=24007.331367, global_step=71049, preemption_count=0, score=24007.331367, test/accuracy=0.265500, test/loss=4.025924, test/num_examples=10000, total_duration=24860.906603, train/accuracy=0.389489, train/loss=2.935113, validation/accuracy=0.360560, validation/loss=3.162464, validation/num_examples=50000
I0129 08:19:45.135297 139866180368128 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.4892245531082153, loss=2.3405470848083496
I0129 08:20:18.854145 139866171975424 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3280963897705078, loss=2.2295498847961426
I0129 08:20:52.530217 139866180368128 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3742777109146118, loss=2.0763940811157227
I0129 08:21:26.243956 139866171975424 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.4350813627243042, loss=2.041234016418457
I0129 08:21:59.929422 139866180368128 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2474782466888428, loss=2.1134233474731445
I0129 08:22:33.683504 139866171975424 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2529641389846802, loss=2.139195442199707
I0129 08:23:07.427752 139866180368128 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.376058578491211, loss=2.0745046138763428
I0129 08:23:41.134677 139866171975424 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.8515534400939941, loss=2.2535669803619385
I0129 08:24:14.818463 139866180368128 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.2455018758773804, loss=2.0895516872406006
I0129 08:24:48.529087 139866171975424 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.3461970090866089, loss=2.182476758956909
I0129 08:25:22.223534 139866180368128 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.5863370895385742, loss=2.17547869682312
I0129 08:25:55.924932 139866171975424 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.4058928489685059, loss=2.110668897628784
I0129 08:26:29.606705 139866180368128 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3625800609588623, loss=2.2076258659362793
I0129 08:27:03.312731 139866171975424 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.4461162090301514, loss=2.1306352615356445
I0129 08:27:36.987058 139866180368128 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3320000171661377, loss=2.248744249343872
I0129 08:27:57.653324 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:28:03.903681 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:28:12.592394 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:28:15.221330 140027215431488 submission_runner.py:408] Time since start: 25388.56s, 	Step: 72563, 	{'train/accuracy': 0.2634526491165161, 'train/loss': 3.8071141242980957, 'validation/accuracy': 0.25356000661849976, 'validation/loss': 3.922708034515381, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.710382461547852, 'test/num_examples': 10000, 'score': 24517.32631087303, 'total_duration': 25388.561143636703, 'accumulated_submission_time': 24517.32631087303, 'accumulated_eval_time': 867.0384802818298, 'accumulated_logging_time': 1.6555404663085938}
I0129 08:28:15.251974 139865224107776 logging_writer.py:48] [72563] accumulated_eval_time=867.038480, accumulated_logging_time=1.655540, accumulated_submission_time=24517.326311, global_step=72563, preemption_count=0, score=24517.326311, test/accuracy=0.179100, test/loss=4.710382, test/num_examples=10000, total_duration=25388.561144, train/accuracy=0.263453, train/loss=3.807114, validation/accuracy=0.253560, validation/loss=3.922708, validation/num_examples=50000
I0129 08:28:28.101335 139865232500480 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.3396928310394287, loss=2.2503936290740967
I0129 08:29:01.888281 139865224107776 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.493497371673584, loss=2.1207046508789062
I0129 08:29:35.647467 139865232500480 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3952783346176147, loss=2.153139114379883
I0129 08:30:09.306478 139865224107776 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.3806904554367065, loss=2.0323402881622314
I0129 08:30:43.016571 139865232500480 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3041406869888306, loss=2.12796950340271
I0129 08:31:16.683105 139865224107776 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3847873210906982, loss=2.108006477355957
I0129 08:31:50.367112 139865232500480 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.4217413663864136, loss=2.1074371337890625
I0129 08:32:24.028249 139865224107776 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.4805183410644531, loss=2.0634994506835938
I0129 08:32:57.746974 139865232500480 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4552382230758667, loss=2.150961399078369
I0129 08:33:31.408933 139865224107776 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.4815242290496826, loss=2.1402978897094727
I0129 08:34:05.110765 139865232500480 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.555023431777954, loss=2.128988742828369
I0129 08:34:38.764332 139865224107776 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.4387396574020386, loss=2.2087655067443848
I0129 08:35:12.495170 139865232500480 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2898627519607544, loss=2.1853458881378174
I0129 08:35:46.303142 139865224107776 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.4855942726135254, loss=2.1659812927246094
I0129 08:36:20.015229 139865232500480 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.4794576168060303, loss=2.101285934448242
I0129 08:36:45.403406 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:36:51.599644 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:37:00.182406 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:37:02.931251 140027215431488 submission_runner.py:408] Time since start: 25916.27s, 	Step: 74077, 	{'train/accuracy': 0.38167649507522583, 'train/loss': 2.9451732635498047, 'validation/accuracy': 0.3625199794769287, 'validation/loss': 3.1048009395599365, 'validation/num_examples': 50000, 'test/accuracy': 0.2621999979019165, 'test/loss': 4.026857376098633, 'test/num_examples': 10000, 'score': 25027.415630340576, 'total_duration': 25916.27106308937, 'accumulated_submission_time': 25027.415630340576, 'accumulated_eval_time': 884.5662899017334, 'accumulated_logging_time': 1.6954331398010254}
I0129 08:37:02.965033 139865224107776 logging_writer.py:48] [74077] accumulated_eval_time=884.566290, accumulated_logging_time=1.695433, accumulated_submission_time=25027.415630, global_step=74077, preemption_count=0, score=25027.415630, test/accuracy=0.262200, test/loss=4.026857, test/num_examples=10000, total_duration=25916.271063, train/accuracy=0.381676, train/loss=2.945173, validation/accuracy=0.362520, validation/loss=3.104801, validation/num_examples=50000
I0129 08:37:11.075885 139865232500480 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3817052841186523, loss=2.0958430767059326
I0129 08:37:44.787593 139865224107776 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.716679573059082, loss=2.0576305389404297
I0129 08:38:18.494224 139865232500480 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4947195053100586, loss=2.217871904373169
I0129 08:38:52.176905 139865224107776 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.4456734657287598, loss=2.1852478981018066
I0129 08:39:25.901704 139865232500480 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2945436239242554, loss=2.11867618560791
I0129 08:39:59.579297 139865224107776 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.461843729019165, loss=2.220198154449463
I0129 08:40:33.283918 139865232500480 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3151676654815674, loss=2.13668155670166
I0129 08:41:06.960072 139865224107776 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.418383240699768, loss=2.058873176574707
I0129 08:41:40.657619 139865232500480 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.346469521522522, loss=2.1601014137268066
I0129 08:42:14.476486 139865224107776 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.4151853322982788, loss=2.168978691101074
I0129 08:42:48.189540 139865232500480 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.488990306854248, loss=2.0732054710388184
I0129 08:43:21.866748 139865224107776 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.376229166984558, loss=2.117689847946167
I0129 08:43:55.564822 139865232500480 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.3971877098083496, loss=2.045100688934326
I0129 08:44:29.237237 139865224107776 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.517189860343933, loss=2.0381836891174316
I0129 08:45:02.948203 139865232500480 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.541215419769287, loss=2.1244001388549805
I0129 08:45:33.050724 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:45:39.257584 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:45:47.820749 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:45:50.460646 140027215431488 submission_runner.py:408] Time since start: 26443.80s, 	Step: 75591, 	{'train/accuracy': 0.1342075914144516, 'train/loss': 5.668425559997559, 'validation/accuracy': 0.1243399977684021, 'validation/loss': 5.764382839202881, 'validation/num_examples': 50000, 'test/accuracy': 0.08700000494718552, 'test/loss': 6.647572040557861, 'test/num_examples': 10000, 'score': 25537.438853025436, 'total_duration': 26443.800446033478, 'accumulated_submission_time': 25537.438853025436, 'accumulated_eval_time': 901.976181268692, 'accumulated_logging_time': 1.7384414672851562}
I0129 08:45:50.494215 139865232500480 logging_writer.py:48] [75591] accumulated_eval_time=901.976181, accumulated_logging_time=1.738441, accumulated_submission_time=25537.438853, global_step=75591, preemption_count=0, score=25537.438853, test/accuracy=0.087000, test/loss=6.647572, test/num_examples=10000, total_duration=26443.800446, train/accuracy=0.134208, train/loss=5.668426, validation/accuracy=0.124340, validation/loss=5.764383, validation/num_examples=50000
I0129 08:45:53.891308 139865240893184 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.6530755758285522, loss=2.184105396270752
I0129 08:46:27.604785 139865232500480 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.4736409187316895, loss=2.187452554702759
I0129 08:47:01.262695 139865240893184 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3695099353790283, loss=2.149103879928589
I0129 08:47:34.968958 139865232500480 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.5362548828125, loss=2.1535656452178955
I0129 08:48:08.640568 139865240893184 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.4764858484268188, loss=1.9920365810394287
I0129 08:48:42.429334 139865232500480 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.5597482919692993, loss=2.1057097911834717
I0129 08:49:16.109560 139865240893184 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.45936119556427, loss=2.0076446533203125
I0129 08:49:49.828433 139865232500480 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.4175496101379395, loss=1.997391700744629
I0129 08:50:23.494093 139865240893184 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.3339307308197021, loss=2.172074556350708
I0129 08:50:57.207103 139865232500480 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.324576497077942, loss=2.090363025665283
I0129 08:51:30.864393 139865240893184 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.367263674736023, loss=2.0566654205322266
I0129 08:52:04.567521 139865232500480 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4739704132080078, loss=2.1818599700927734
I0129 08:52:38.218483 139865240893184 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.30672025680542, loss=2.1094839572906494
I0129 08:53:11.914626 139865232500480 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.5691505670547485, loss=2.0549259185791016
I0129 08:53:45.576107 139865240893184 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4121772050857544, loss=2.0358736515045166
I0129 08:54:19.269683 139865232500480 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.4276666641235352, loss=2.121985912322998
I0129 08:54:20.758169 140027215431488 spec.py:321] Evaluating on the training split.
I0129 08:54:27.000979 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 08:54:35.539102 140027215431488 spec.py:349] Evaluating on the test split.
I0129 08:54:38.200852 140027215431488 submission_runner.py:408] Time since start: 26971.54s, 	Step: 77106, 	{'train/accuracy': 0.3316725194454193, 'train/loss': 3.21846866607666, 'validation/accuracy': 0.2920999825000763, 'validation/loss': 3.503586769104004, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 4.244337558746338, 'test/num_examples': 10000, 'score': 26047.63839364052, 'total_duration': 26971.54065322876, 'accumulated_submission_time': 26047.63839364052, 'accumulated_eval_time': 919.41881108284, 'accumulated_logging_time': 1.7832458019256592}
I0129 08:54:38.232270 139865224107776 logging_writer.py:48] [77106] accumulated_eval_time=919.418811, accumulated_logging_time=1.783246, accumulated_submission_time=26047.638394, global_step=77106, preemption_count=0, score=26047.638394, test/accuracy=0.220500, test/loss=4.244338, test/num_examples=10000, total_duration=26971.540653, train/accuracy=0.331673, train/loss=3.218469, validation/accuracy=0.292100, validation/loss=3.503587, validation/num_examples=50000
I0129 08:55:10.419906 139865232500480 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.5426793098449707, loss=2.020900011062622
I0129 08:55:44.100244 139865224107776 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.4834457635879517, loss=2.1278605461120605
I0129 08:56:17.822602 139865232500480 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4957972764968872, loss=2.1123945713043213
I0129 08:56:51.487642 139865224107776 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.5303127765655518, loss=2.1549038887023926
I0129 08:57:25.182406 139865232500480 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.411247730255127, loss=2.052518129348755
I0129 08:57:58.852578 139865224107776 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.5155092477798462, loss=2.039017915725708
I0129 08:58:32.578974 139865232500480 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.3639334440231323, loss=2.1400883197784424
I0129 08:59:06.259395 139865224107776 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.366809368133545, loss=2.0630059242248535
I0129 08:59:39.963267 139865232500480 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.5424705743789673, loss=2.068202257156372
I0129 09:00:13.632937 139865224107776 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.5013599395751953, loss=2.0154616832733154
I0129 09:00:47.313386 139865232500480 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.4146316051483154, loss=2.0186233520507812
I0129 09:01:21.120138 139865224107776 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.4844542741775513, loss=2.084958553314209
I0129 09:01:54.848933 139865232500480 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.4440687894821167, loss=2.0484116077423096
I0129 09:02:28.522248 139865224107776 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.4150210618972778, loss=1.9762749671936035
I0129 09:03:02.225222 139865232500480 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.5400217771530151, loss=2.1814966201782227
I0129 09:03:08.435708 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:03:14.645392 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:03:23.213575 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:03:25.860342 140027215431488 submission_runner.py:408] Time since start: 27499.20s, 	Step: 78620, 	{'train/accuracy': 0.3699776828289032, 'train/loss': 3.103986978530884, 'validation/accuracy': 0.35266000032424927, 'validation/loss': 3.277158737182617, 'validation/num_examples': 50000, 'test/accuracy': 0.2669000029563904, 'test/loss': 4.146973609924316, 'test/num_examples': 10000, 'score': 26557.776461839676, 'total_duration': 27499.20015025139, 'accumulated_submission_time': 26557.776461839676, 'accumulated_eval_time': 936.843403339386, 'accumulated_logging_time': 1.8256030082702637}
I0129 09:03:25.891455 139866163582720 logging_writer.py:48] [78620] accumulated_eval_time=936.843403, accumulated_logging_time=1.825603, accumulated_submission_time=26557.776462, global_step=78620, preemption_count=0, score=26557.776462, test/accuracy=0.266900, test/loss=4.146974, test/num_examples=10000, total_duration=27499.200150, train/accuracy=0.369978, train/loss=3.103987, validation/accuracy=0.352660, validation/loss=3.277159, validation/num_examples=50000
I0129 09:03:53.194582 139866171975424 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.4547110795974731, loss=2.14279842376709
I0129 09:04:26.901352 139866163582720 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.425837755203247, loss=1.9397333860397339
I0129 09:05:00.574093 139866171975424 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.5289216041564941, loss=2.0786991119384766
I0129 09:05:34.285066 139866163582720 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.3785978555679321, loss=2.1047184467315674
I0129 09:06:07.950036 139866171975424 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.3157191276550293, loss=2.119385004043579
I0129 09:06:41.677100 139866163582720 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.371153473854065, loss=2.204861640930176
I0129 09:07:15.344523 139866171975424 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.4729379415512085, loss=2.0156118869781494
I0129 09:07:49.114672 139866163582720 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.4491970539093018, loss=2.1279242038726807
I0129 09:08:22.846077 139866171975424 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.4279215335845947, loss=2.25207257270813
I0129 09:08:56.553997 139866163582720 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.6103882789611816, loss=2.1027822494506836
I0129 09:09:30.232332 139866171975424 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4095423221588135, loss=2.0366692543029785
I0129 09:10:03.936400 139866163582720 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.6316181421279907, loss=2.0184388160705566
I0129 09:10:37.625124 139866171975424 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.5430315732955933, loss=2.2174103260040283
I0129 09:11:11.330461 139866163582720 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.5236809253692627, loss=1.9665926694869995
I0129 09:11:45.008801 139866171975424 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.483217716217041, loss=2.1116716861724854
I0129 09:11:55.946906 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:12:02.207321 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:12:10.761905 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:12:13.388720 140027215431488 submission_runner.py:408] Time since start: 28026.73s, 	Step: 80134, 	{'train/accuracy': 0.3420161008834839, 'train/loss': 3.218864917755127, 'validation/accuracy': 0.3225799798965454, 'validation/loss': 3.411599636077881, 'validation/num_examples': 50000, 'test/accuracy': 0.24210001528263092, 'test/loss': 4.21008825302124, 'test/num_examples': 10000, 'score': 27067.766325950623, 'total_duration': 28026.72852373123, 'accumulated_submission_time': 27067.766325950623, 'accumulated_eval_time': 954.2851715087891, 'accumulated_logging_time': 1.868180513381958}
I0129 09:12:13.419579 139865240893184 logging_writer.py:48] [80134] accumulated_eval_time=954.285172, accumulated_logging_time=1.868181, accumulated_submission_time=27067.766326, global_step=80134, preemption_count=0, score=27067.766326, test/accuracy=0.242100, test/loss=4.210088, test/num_examples=10000, total_duration=28026.728524, train/accuracy=0.342016, train/loss=3.218865, validation/accuracy=0.322580, validation/loss=3.411600, validation/num_examples=50000
I0129 09:12:35.995709 139865760950016 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.6500060558319092, loss=2.131199598312378
I0129 09:13:09.698616 139865240893184 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4356790781021118, loss=2.1483139991760254
I0129 09:13:43.384288 139865760950016 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4716030359268188, loss=2.082624912261963
I0129 09:14:17.261105 139865240893184 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.5251275300979614, loss=2.143406867980957
I0129 09:14:50.937544 139865760950016 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.4050612449645996, loss=2.0192739963531494
I0129 09:15:24.635269 139865240893184 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.3621431589126587, loss=2.0884196758270264
I0129 09:15:58.292423 139865760950016 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.566300392150879, loss=2.1384899616241455
I0129 09:16:32.001112 139865240893184 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.395596981048584, loss=1.931416392326355
I0129 09:17:05.648489 139865760950016 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.469558835029602, loss=2.037612199783325
I0129 09:17:39.356439 139865240893184 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.4129530191421509, loss=2.180366277694702
I0129 09:18:13.039597 139865760950016 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.5203574895858765, loss=2.0562078952789307
I0129 09:18:46.748245 139865240893184 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.5808947086334229, loss=2.0519590377807617
I0129 09:19:20.406822 139865760950016 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.8334685564041138, loss=2.0879361629486084
I0129 09:19:54.106870 139865240893184 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.3641070127487183, loss=2.1849238872528076
I0129 09:20:27.777766 139865760950016 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.4404653310775757, loss=2.0384280681610107
I0129 09:20:43.595559 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:20:49.781615 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:20:58.515223 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:21:01.152076 140027215431488 submission_runner.py:408] Time since start: 28554.49s, 	Step: 81648, 	{'train/accuracy': 0.39485010504722595, 'train/loss': 2.805603265762329, 'validation/accuracy': 0.3705599904060364, 'validation/loss': 2.987741470336914, 'validation/num_examples': 50000, 'test/accuracy': 0.281900018453598, 'test/loss': 3.800844669342041, 'test/num_examples': 10000, 'score': 27577.878240585327, 'total_duration': 28554.491887807846, 'accumulated_submission_time': 27577.878240585327, 'accumulated_eval_time': 971.8416512012482, 'accumulated_logging_time': 1.9092822074890137}
I0129 09:21:01.184407 139865232500480 logging_writer.py:48] [81648] accumulated_eval_time=971.841651, accumulated_logging_time=1.909282, accumulated_submission_time=27577.878241, global_step=81648, preemption_count=0, score=27577.878241, test/accuracy=0.281900, test/loss=3.800845, test/num_examples=10000, total_duration=28554.491888, train/accuracy=0.394850, train/loss=2.805603, validation/accuracy=0.370560, validation/loss=2.987741, validation/num_examples=50000
I0129 09:21:19.066449 139865240893184 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.47255277633667, loss=2.1285016536712646
I0129 09:21:52.747489 139865232500480 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.5356950759887695, loss=2.015693187713623
I0129 09:22:26.452726 139865240893184 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.5042600631713867, loss=2.014192819595337
I0129 09:23:00.124346 139865232500480 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.3610312938690186, loss=2.1171329021453857
I0129 09:23:33.824664 139865240893184 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.5008697509765625, loss=1.995779037475586
I0129 09:24:07.483919 139865232500480 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.535907506942749, loss=2.0161774158477783
I0129 09:24:41.181924 139865240893184 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3295528888702393, loss=2.025325059890747
I0129 09:25:14.854805 139865232500480 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.4320565462112427, loss=2.0467331409454346
I0129 09:25:48.562005 139865240893184 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.5975549221038818, loss=2.06762433052063
I0129 09:26:22.246407 139865232500480 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.5348154306411743, loss=2.1586148738861084
I0129 09:26:55.944784 139865240893184 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.6525123119354248, loss=2.103257179260254
I0129 09:27:29.781479 139865232500480 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.5904185771942139, loss=2.08524227142334
I0129 09:28:03.487602 139865240893184 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.4178798198699951, loss=2.052690267562866
I0129 09:28:37.162566 139865232500480 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.4594427347183228, loss=2.0075416564941406
I0129 09:29:10.849557 139865240893184 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.5544426441192627, loss=2.099029064178467
I0129 09:29:31.192040 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:29:37.442264 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:29:46.341934 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:29:48.995229 140027215431488 submission_runner.py:408] Time since start: 29082.34s, 	Step: 83162, 	{'train/accuracy': 0.2846579849720001, 'train/loss': 3.9252586364746094, 'validation/accuracy': 0.27432000637054443, 'validation/loss': 4.041182041168213, 'validation/num_examples': 50000, 'test/accuracy': 0.19210000336170197, 'test/loss': 5.06205940246582, 'test/num_examples': 10000, 'score': 28087.819554805756, 'total_duration': 29082.335039138794, 'accumulated_submission_time': 28087.819554805756, 'accumulated_eval_time': 989.6448218822479, 'accumulated_logging_time': 1.954078197479248}
I0129 09:29:49.028156 139865232500480 logging_writer.py:48] [83162] accumulated_eval_time=989.644822, accumulated_logging_time=1.954078, accumulated_submission_time=28087.819555, global_step=83162, preemption_count=0, score=28087.819555, test/accuracy=0.192100, test/loss=5.062059, test/num_examples=10000, total_duration=29082.335039, train/accuracy=0.284658, train/loss=3.925259, validation/accuracy=0.274320, validation/loss=4.041182, validation/num_examples=50000
I0129 09:30:02.183381 139866171975424 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.457330346107483, loss=1.8910725116729736
I0129 09:30:35.856864 139865232500480 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.5405033826828003, loss=2.1136622428894043
I0129 09:31:09.570284 139866171975424 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.5407012701034546, loss=2.040675401687622
I0129 09:31:43.234496 139865232500480 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.5680034160614014, loss=2.0225729942321777
I0129 09:32:16.932863 139866171975424 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.5531363487243652, loss=2.1237735748291016
I0129 09:32:50.607053 139865232500480 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.7165926694869995, loss=2.1310312747955322
I0129 09:33:24.400186 139866171975424 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.402747631072998, loss=2.0780229568481445
I0129 09:33:58.124505 139865232500480 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.5907708406448364, loss=2.1306023597717285
I0129 09:34:31.833791 139866171975424 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.5550963878631592, loss=2.1463637351989746
I0129 09:35:05.489423 139865232500480 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.4867644309997559, loss=2.072646379470825
I0129 09:35:39.200158 139866171975424 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.8196535110473633, loss=2.1650052070617676
I0129 09:36:12.853750 139865232500480 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.5470420122146606, loss=2.0547919273376465
I0129 09:36:46.566915 139866171975424 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.435357928276062, loss=1.9912086725234985
I0129 09:37:20.224380 139865232500480 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.5221415758132935, loss=2.0154740810394287
I0129 09:37:53.936180 139866171975424 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.4318870306015015, loss=2.0500500202178955
I0129 09:38:19.318715 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:38:25.516395 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:38:34.033769 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:38:36.696031 140027215431488 submission_runner.py:408] Time since start: 29610.04s, 	Step: 84677, 	{'train/accuracy': 0.46466436982154846, 'train/loss': 2.3826076984405518, 'validation/accuracy': 0.4369199872016907, 'validation/loss': 2.5394155979156494, 'validation/num_examples': 50000, 'test/accuracy': 0.331900030374527, 'test/loss': 3.2385549545288086, 'test/num_examples': 10000, 'score': 28598.049216508865, 'total_duration': 29610.035831689835, 'accumulated_submission_time': 28598.049216508865, 'accumulated_eval_time': 1007.0220937728882, 'accumulated_logging_time': 1.9960856437683105}
I0129 09:38:36.729356 139865760950016 logging_writer.py:48] [84677] accumulated_eval_time=1007.022094, accumulated_logging_time=1.996086, accumulated_submission_time=28598.049217, global_step=84677, preemption_count=0, score=28598.049217, test/accuracy=0.331900, test/loss=3.238555, test/num_examples=10000, total_duration=29610.035832, train/accuracy=0.464664, train/loss=2.382608, validation/accuracy=0.436920, validation/loss=2.539416, validation/num_examples=50000
I0129 09:38:44.816627 139865769342720 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.5071361064910889, loss=1.9517245292663574
I0129 09:39:18.490332 139865760950016 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.6252288818359375, loss=2.005988359451294
I0129 09:39:52.259819 139865769342720 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.503597617149353, loss=2.267657995223999
I0129 09:40:25.951967 139865760950016 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.5227553844451904, loss=2.056899070739746
I0129 09:40:59.654694 139865769342720 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.5759015083312988, loss=2.1598141193389893
I0129 09:41:33.343823 139865760950016 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.5232805013656616, loss=2.156670331954956
I0129 09:42:07.024784 139865769342720 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.6580389738082886, loss=2.062612533569336
I0129 09:42:40.724164 139865760950016 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.462925672531128, loss=2.0655300617218018
I0129 09:43:14.409426 139865769342720 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.40485417842865, loss=2.038562536239624
I0129 09:43:48.087950 139865760950016 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.4683146476745605, loss=2.0120391845703125
I0129 09:44:21.780034 139865769342720 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.5516819953918457, loss=2.1697797775268555
I0129 09:44:55.458376 139865760950016 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.458382248878479, loss=2.0351932048797607
I0129 09:45:29.141217 139865769342720 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.6105623245239258, loss=2.069901466369629
I0129 09:46:02.828634 139865760950016 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.4958231449127197, loss=2.1413943767547607
I0129 09:46:36.649307 139865769342720 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.5359058380126953, loss=1.9941761493682861
I0129 09:47:06.781880 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:47:12.979996 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:47:21.810326 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:47:24.347534 140027215431488 submission_runner.py:408] Time since start: 30137.69s, 	Step: 86191, 	{'train/accuracy': 0.48212292790412903, 'train/loss': 2.2993392944335938, 'validation/accuracy': 0.4251599907875061, 'validation/loss': 2.6702864170074463, 'validation/num_examples': 50000, 'test/accuracy': 0.3320000171661377, 'test/loss': 3.385972738265991, 'test/num_examples': 10000, 'score': 29108.04040503502, 'total_duration': 30137.687334775925, 'accumulated_submission_time': 29108.04040503502, 'accumulated_eval_time': 1024.5876967906952, 'accumulated_logging_time': 2.03897762298584}
I0129 09:47:24.379823 139865240893184 logging_writer.py:48] [86191] accumulated_eval_time=1024.587697, accumulated_logging_time=2.038978, accumulated_submission_time=29108.040405, global_step=86191, preemption_count=0, score=29108.040405, test/accuracy=0.332000, test/loss=3.385973, test/num_examples=10000, total_duration=30137.687335, train/accuracy=0.482123, train/loss=2.299339, validation/accuracy=0.425160, validation/loss=2.670286, validation/num_examples=50000
I0129 09:47:27.756405 139865760950016 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.5454732179641724, loss=1.9897501468658447
I0129 09:48:01.478058 139865240893184 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.5685985088348389, loss=2.1382884979248047
I0129 09:48:35.139592 139865760950016 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.5269428491592407, loss=1.9719030857086182
I0129 09:49:08.846644 139865240893184 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.754202127456665, loss=2.053664445877075
I0129 09:49:42.523537 139865760950016 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.4755308628082275, loss=2.014971971511841
I0129 09:50:16.225203 139865240893184 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.4203240871429443, loss=2.0721564292907715
I0129 09:50:49.892288 139865760950016 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.5218065977096558, loss=2.155289888381958
I0129 09:51:23.594495 139865240893184 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.707660436630249, loss=2.1072511672973633
I0129 09:51:57.250609 139865760950016 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.4457238912582397, loss=1.931917667388916
I0129 09:52:30.962100 139865240893184 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.5537943840026855, loss=2.059741735458374
I0129 09:53:04.727894 139865760950016 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.552030086517334, loss=2.0783371925354004
I0129 09:53:38.454646 139865240893184 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.5120370388031006, loss=1.995126724243164
I0129 09:54:12.124198 139865760950016 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.7274200916290283, loss=2.07881760597229
I0129 09:54:45.833036 139865240893184 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.4092826843261719, loss=2.046945810317993
I0129 09:55:19.519505 139865760950016 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.5042917728424072, loss=2.0087051391601562
I0129 09:55:53.245233 139865240893184 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.4533843994140625, loss=1.9648759365081787
I0129 09:55:54.393878 140027215431488 spec.py:321] Evaluating on the training split.
I0129 09:56:00.598463 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 09:56:09.209772 140027215431488 spec.py:349] Evaluating on the test split.
I0129 09:56:11.871249 140027215431488 submission_runner.py:408] Time since start: 30665.21s, 	Step: 87705, 	{'train/accuracy': 0.33380499482154846, 'train/loss': 3.2080514430999756, 'validation/accuracy': 0.30441999435424805, 'validation/loss': 3.4460508823394775, 'validation/num_examples': 50000, 'test/accuracy': 0.23690001666545868, 'test/loss': 4.14555025100708, 'test/num_examples': 10000, 'score': 29617.99115371704, 'total_duration': 30665.211062908173, 'accumulated_submission_time': 29617.99115371704, 'accumulated_eval_time': 1042.0650265216827, 'accumulated_logging_time': 2.081550359725952}
I0129 09:56:11.907899 139865769342720 logging_writer.py:48] [87705] accumulated_eval_time=1042.065027, accumulated_logging_time=2.081550, accumulated_submission_time=29617.991154, global_step=87705, preemption_count=0, score=29617.991154, test/accuracy=0.236900, test/loss=4.145550, test/num_examples=10000, total_duration=30665.211063, train/accuracy=0.333805, train/loss=3.208051, validation/accuracy=0.304420, validation/loss=3.446051, validation/num_examples=50000
I0129 09:56:44.199498 139866163582720 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.5813947916030884, loss=2.0511820316314697
I0129 09:57:17.897615 139865769342720 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.663495659828186, loss=2.033968925476074
I0129 09:57:51.586476 139866163582720 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.5143100023269653, loss=1.9658498764038086
I0129 09:58:25.271869 139865769342720 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.5896427631378174, loss=2.1012730598449707
I0129 09:58:58.978570 139866163582720 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.5124499797821045, loss=1.982345461845398
I0129 09:59:32.798795 139865769342720 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.5376708507537842, loss=2.0919129848480225
I0129 10:00:06.522150 139866163582720 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.4791862964630127, loss=2.0408859252929688
I0129 10:00:40.209882 139865769342720 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.5237311124801636, loss=1.9582175016403198
I0129 10:01:13.910454 139866163582720 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.5816717147827148, loss=2.0227980613708496
I0129 10:01:47.591385 139865769342720 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.640217661857605, loss=2.0084800720214844
I0129 10:02:21.286759 139866163582720 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.575060486793518, loss=2.0676541328430176
I0129 10:02:54.967917 139865769342720 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.573221206665039, loss=2.105438709259033
I0129 10:03:28.664245 139866163582720 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.5931851863861084, loss=2.0858867168426514
I0129 10:04:02.345093 139865769342720 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.4675601720809937, loss=2.0676074028015137
I0129 10:04:36.037503 139866163582720 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.578113079071045, loss=1.904119610786438
I0129 10:04:41.908361 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:04:48.131047 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:04:56.649054 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:04:59.251523 140027215431488 submission_runner.py:408] Time since start: 31192.59s, 	Step: 89219, 	{'train/accuracy': 0.3150709569454193, 'train/loss': 3.5599539279937744, 'validation/accuracy': 0.2946600019931793, 'validation/loss': 3.770634651184082, 'validation/num_examples': 50000, 'test/accuracy': 0.22530001401901245, 'test/loss': 4.554732322692871, 'test/num_examples': 10000, 'score': 30127.92679142952, 'total_duration': 31192.591334342957, 'accumulated_submission_time': 30127.92679142952, 'accumulated_eval_time': 1059.4081492424011, 'accumulated_logging_time': 2.1293301582336426}
I0129 10:04:59.286868 139865760950016 logging_writer.py:48] [89219] accumulated_eval_time=1059.408149, accumulated_logging_time=2.129330, accumulated_submission_time=30127.926791, global_step=89219, preemption_count=0, score=30127.926791, test/accuracy=0.225300, test/loss=4.554732, test/num_examples=10000, total_duration=31192.591334, train/accuracy=0.315071, train/loss=3.559954, validation/accuracy=0.294660, validation/loss=3.770635, validation/num_examples=50000
I0129 10:05:26.940260 139866171975424 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.682076334953308, loss=2.07456636428833
I0129 10:06:00.750810 139865760950016 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.4342293739318848, loss=1.9762099981307983
I0129 10:06:34.446245 139866171975424 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.3840317726135254, loss=1.957844614982605
I0129 10:07:08.163161 139865760950016 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.7615412473678589, loss=2.1467342376708984
I0129 10:07:41.837089 139866171975424 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.556296706199646, loss=1.9323976039886475
I0129 10:08:15.553801 139865760950016 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.746519923210144, loss=2.1848831176757812
I0129 10:08:49.226293 139866171975424 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.5379856824874878, loss=2.122521162033081
I0129 10:09:22.931438 139865760950016 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.4868148565292358, loss=2.056969165802002
I0129 10:09:56.603280 139866171975424 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6140596866607666, loss=2.003933906555176
I0129 10:10:30.308354 139865760950016 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.445265531539917, loss=1.945478081703186
I0129 10:11:03.960042 139866171975424 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.5586633682250977, loss=2.0501437187194824
I0129 10:11:37.657856 139865760950016 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.7479870319366455, loss=2.1358389854431152
I0129 10:12:11.391492 139866171975424 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.7278705835342407, loss=2.031611680984497
I0129 10:12:45.101822 139865760950016 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.4338515996932983, loss=2.0820724964141846
I0129 10:13:18.777825 139866171975424 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.656524896621704, loss=1.951949954032898
I0129 10:13:29.374881 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:13:35.738419 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:13:44.232333 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:13:46.894560 140027215431488 submission_runner.py:408] Time since start: 31720.23s, 	Step: 90733, 	{'train/accuracy': 0.39708226919174194, 'train/loss': 2.781815528869629, 'validation/accuracy': 0.36855998635292053, 'validation/loss': 2.9695539474487305, 'validation/num_examples': 50000, 'test/accuracy': 0.27320000529289246, 'test/loss': 3.7446415424346924, 'test/num_examples': 10000, 'score': 30637.952219963074, 'total_duration': 31720.2343685627, 'accumulated_submission_time': 30637.952219963074, 'accumulated_eval_time': 1076.9277966022491, 'accumulated_logging_time': 2.1744236946105957}
I0129 10:13:46.931089 139865760950016 logging_writer.py:48] [90733] accumulated_eval_time=1076.927797, accumulated_logging_time=2.174424, accumulated_submission_time=30637.952220, global_step=90733, preemption_count=0, score=30637.952220, test/accuracy=0.273200, test/loss=3.744642, test/num_examples=10000, total_duration=31720.234369, train/accuracy=0.397082, train/loss=2.781816, validation/accuracy=0.368560, validation/loss=2.969554, validation/num_examples=50000
I0129 10:14:09.855422 139865769342720 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.5946564674377441, loss=1.952261209487915
I0129 10:14:43.574892 139865760950016 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.6237386465072632, loss=1.9928514957427979
I0129 10:15:17.257364 139865769342720 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.5352658033370972, loss=1.9630517959594727
I0129 10:15:50.964766 139865760950016 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.6225281953811646, loss=1.9897754192352295
I0129 10:16:24.625690 139865769342720 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.4233628511428833, loss=1.9889662265777588
I0129 10:16:58.329413 139865760950016 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.4787590503692627, loss=1.9739447832107544
I0129 10:17:31.991124 139865769342720 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.686111330986023, loss=1.9503021240234375
I0129 10:18:05.693003 139865760950016 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.5156922340393066, loss=2.0350680351257324
I0129 10:18:39.554040 139865769342720 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.5665628910064697, loss=2.0746309757232666
I0129 10:19:13.712398 139865760950016 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.5986040830612183, loss=2.1069321632385254
I0129 10:19:47.422270 139865769342720 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.728447437286377, loss=2.1247081756591797
I0129 10:20:21.115493 139865760950016 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.9159877300262451, loss=2.0019326210021973
I0129 10:20:54.791341 139865769342720 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.5740375518798828, loss=2.097930431365967
I0129 10:21:28.486759 139865760950016 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.5979503393173218, loss=2.013277292251587
I0129 10:22:02.161187 139865769342720 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.4850282669067383, loss=1.9172013998031616
I0129 10:22:17.136540 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:22:24.083827 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:22:32.562028 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:22:35.201736 140027215431488 submission_runner.py:408] Time since start: 32248.54s, 	Step: 92246, 	{'train/accuracy': 0.45434072613716125, 'train/loss': 2.392561912536621, 'validation/accuracy': 0.4245999753475189, 'validation/loss': 2.587425708770752, 'validation/num_examples': 50000, 'test/accuracy': 0.320000022649765, 'test/loss': 3.2951889038085938, 'test/num_examples': 10000, 'score': 31148.09440755844, 'total_duration': 32248.54153752327, 'accumulated_submission_time': 31148.09440755844, 'accumulated_eval_time': 1094.9929592609406, 'accumulated_logging_time': 2.2205803394317627}
I0129 10:22:35.235845 139865232500480 logging_writer.py:48] [92246] accumulated_eval_time=1094.992959, accumulated_logging_time=2.220580, accumulated_submission_time=31148.094408, global_step=92246, preemption_count=0, score=31148.094408, test/accuracy=0.320000, test/loss=3.295189, test/num_examples=10000, total_duration=32248.541538, train/accuracy=0.454341, train/loss=2.392562, validation/accuracy=0.424600, validation/loss=2.587426, validation/num_examples=50000
I0129 10:22:53.742422 139865240893184 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.6607389450073242, loss=1.9316521883010864
I0129 10:23:27.469283 139865232500480 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.8380634784698486, loss=2.075253486633301
I0129 10:24:01.182886 139865240893184 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.615299105644226, loss=2.1215689182281494
I0129 10:24:34.844518 139865232500480 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.5830186605453491, loss=1.9966928958892822
I0129 10:25:08.690018 139865240893184 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.7167494297027588, loss=1.980761170387268
I0129 10:25:42.367861 139865232500480 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.8406264781951904, loss=2.1359498500823975
I0129 10:26:16.079398 139865240893184 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.5734485387802124, loss=2.00500226020813
I0129 10:26:49.732922 139865232500480 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.5362443923950195, loss=2.0395243167877197
I0129 10:27:23.439651 139865240893184 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.575324535369873, loss=2.0285799503326416
I0129 10:27:57.096144 139865232500480 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.5942567586898804, loss=2.0123002529144287
I0129 10:28:30.798459 139865240893184 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7911275625228882, loss=2.054074287414551
I0129 10:29:04.476600 139865232500480 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.661238670349121, loss=1.9611550569534302
I0129 10:29:38.182917 139865240893184 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.5818510055541992, loss=2.007861852645874
I0129 10:30:11.849186 139865232500480 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.9702184200286865, loss=1.9552057981491089
I0129 10:30:45.559786 139865240893184 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.691194772720337, loss=2.199403762817383
I0129 10:31:05.225376 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:31:11.425001 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:31:19.953964 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:31:22.649641 140027215431488 submission_runner.py:408] Time since start: 32775.99s, 	Step: 93760, 	{'train/accuracy': 0.45868542790412903, 'train/loss': 2.434083938598633, 'validation/accuracy': 0.4264200031757355, 'validation/loss': 2.6672022342681885, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.445615291595459, 'test/num_examples': 10000, 'score': 31658.021085500717, 'total_duration': 32775.989436626434, 'accumulated_submission_time': 31658.021085500717, 'accumulated_eval_time': 1112.41717171669, 'accumulated_logging_time': 2.264895439147949}
I0129 10:31:22.687584 139866180368128 logging_writer.py:48] [93760] accumulated_eval_time=1112.417172, accumulated_logging_time=2.264895, accumulated_submission_time=31658.021086, global_step=93760, preemption_count=0, score=31658.021086, test/accuracy=0.336800, test/loss=3.445615, test/num_examples=10000, total_duration=32775.989437, train/accuracy=0.458685, train/loss=2.434084, validation/accuracy=0.426420, validation/loss=2.667202, validation/num_examples=50000
I0129 10:31:36.567032 139866188760832 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.6263967752456665, loss=2.001394748687744
I0129 10:32:10.218761 139866180368128 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.674930453300476, loss=2.010303497314453
I0129 10:32:43.909361 139866188760832 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.4873682260513306, loss=2.060459852218628
I0129 10:33:17.586364 139866180368128 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.5294760465621948, loss=1.902247667312622
I0129 10:33:51.288415 139866188760832 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.475021243095398, loss=1.966088891029358
I0129 10:34:24.971364 139866180368128 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.6255110502243042, loss=2.1273820400238037
I0129 10:34:58.654478 139866188760832 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.5966156721115112, loss=1.8690617084503174
I0129 10:35:32.310625 139866180368128 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.579871416091919, loss=2.118318796157837
I0129 10:36:05.998870 139866188760832 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.4490129947662354, loss=1.9413622617721558
I0129 10:36:39.658326 139866180368128 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.5826523303985596, loss=2.0807931423187256
I0129 10:37:13.347494 139866188760832 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.7215908765792847, loss=2.0904550552368164
I0129 10:37:47.018181 139866180368128 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.5804263353347778, loss=2.1301074028015137
I0129 10:38:20.889712 139866188760832 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.6973962783813477, loss=2.029313802719116
I0129 10:38:54.551874 139866180368128 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.6866469383239746, loss=1.9378149509429932
I0129 10:39:28.257443 139866188760832 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.7747442722320557, loss=1.9530277252197266
I0129 10:39:52.975624 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:39:59.154026 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:40:07.660370 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:40:10.334417 140027215431488 submission_runner.py:408] Time since start: 33303.67s, 	Step: 95275, 	{'train/accuracy': 0.37956392765045166, 'train/loss': 2.966967821121216, 'validation/accuracy': 0.340179979801178, 'validation/loss': 3.3062493801116943, 'validation/num_examples': 50000, 'test/accuracy': 0.25950002670288086, 'test/loss': 4.024302959442139, 'test/num_examples': 10000, 'score': 32168.247616052628, 'total_duration': 33303.67395091057, 'accumulated_submission_time': 32168.247616052628, 'accumulated_eval_time': 1129.7756674289703, 'accumulated_logging_time': 2.3125619888305664}
I0129 10:40:10.372168 139865232500480 logging_writer.py:48] [95275] accumulated_eval_time=1129.775667, accumulated_logging_time=2.312562, accumulated_submission_time=32168.247616, global_step=95275, preemption_count=0, score=32168.247616, test/accuracy=0.259500, test/loss=4.024303, test/num_examples=10000, total_duration=33303.673951, train/accuracy=0.379564, train/loss=2.966968, validation/accuracy=0.340180, validation/loss=3.306249, validation/num_examples=50000
I0129 10:40:19.162024 139865240893184 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.620052695274353, loss=1.8991587162017822
I0129 10:40:52.874094 139865232500480 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.5104252099990845, loss=1.9875108003616333
I0129 10:41:26.529559 139865240893184 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.6292288303375244, loss=2.045725107192993
I0129 10:42:00.238149 139865232500480 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.568184733390808, loss=1.860191822052002
I0129 10:42:33.906090 139865240893184 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.6288468837738037, loss=1.9356746673583984
I0129 10:43:07.609689 139865232500480 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.5635699033737183, loss=2.072589635848999
I0129 10:43:41.230427 139865240893184 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.708533763885498, loss=1.9192347526550293
I0129 10:44:15.016625 139865232500480 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.9701448678970337, loss=2.0255560874938965
I0129 10:44:48.756460 139865240893184 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.6754658222198486, loss=2.027009963989258
I0129 10:45:22.440664 139865232500480 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.9097332954406738, loss=1.9602973461151123
I0129 10:45:56.113286 139865240893184 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.6615785360336304, loss=1.9253344535827637
I0129 10:46:29.799649 139865232500480 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.655820369720459, loss=1.996838092803955
I0129 10:47:03.457875 139865240893184 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.6682945489883423, loss=2.0715386867523193
I0129 10:47:37.149385 139865232500480 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.7127819061279297, loss=2.0840437412261963
I0129 10:48:10.806301 139865240893184 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.6588854789733887, loss=2.003185749053955
I0129 10:48:40.622395 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:48:46.842540 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:48:55.342677 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:48:58.036979 140027215431488 submission_runner.py:408] Time since start: 33831.38s, 	Step: 96790, 	{'train/accuracy': 0.4096579849720001, 'train/loss': 2.8069357872009277, 'validation/accuracy': 0.37689998745918274, 'validation/loss': 3.023160934448242, 'validation/num_examples': 50000, 'test/accuracy': 0.2849000096321106, 'test/loss': 3.783292055130005, 'test/num_examples': 10000, 'score': 32678.433045387268, 'total_duration': 33831.3767824173, 'accumulated_submission_time': 32678.433045387268, 'accumulated_eval_time': 1147.1902074813843, 'accumulated_logging_time': 2.3623785972595215}
I0129 10:48:58.072283 139866171975424 logging_writer.py:48] [96790] accumulated_eval_time=1147.190207, accumulated_logging_time=2.362379, accumulated_submission_time=32678.433045, global_step=96790, preemption_count=0, score=32678.433045, test/accuracy=0.284900, test/loss=3.783292, test/num_examples=10000, total_duration=33831.376782, train/accuracy=0.409658, train/loss=2.806936, validation/accuracy=0.376900, validation/loss=3.023161, validation/num_examples=50000
I0129 10:49:01.787039 139866180368128 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.7054252624511719, loss=2.066030979156494
I0129 10:49:35.502998 139866171975424 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.7242145538330078, loss=2.0196304321289062
I0129 10:50:09.152628 139866180368128 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.6195989847183228, loss=1.8162922859191895
I0129 10:50:42.950652 139866171975424 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.5540339946746826, loss=1.9848127365112305
I0129 10:51:16.668987 139866180368128 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.8185381889343262, loss=2.053210973739624
I0129 10:51:50.381099 139866171975424 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.6050513982772827, loss=2.0041840076446533
I0129 10:52:24.041348 139866180368128 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.6857926845550537, loss=1.93776273727417
I0129 10:52:57.749860 139866171975424 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.6911863088607788, loss=1.8731619119644165
I0129 10:53:31.424144 139866180368128 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.674269437789917, loss=2.105165958404541
I0129 10:54:05.115181 139866171975424 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.563334345817566, loss=1.9057401418685913
I0129 10:54:38.791459 139866180368128 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.0821783542633057, loss=2.152820110321045
I0129 10:55:12.511684 139866171975424 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.6402314901351929, loss=1.9320025444030762
I0129 10:55:46.166613 139866180368128 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.7858649492263794, loss=1.9973723888397217
I0129 10:56:19.872789 139866171975424 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.848641276359558, loss=2.0711727142333984
I0129 10:56:53.548399 139866180368128 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.8206709623336792, loss=1.8976260423660278
I0129 10:57:27.437803 139866171975424 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.69661545753479, loss=1.9957563877105713
I0129 10:57:28.263738 140027215431488 spec.py:321] Evaluating on the training split.
I0129 10:57:34.430344 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 10:57:43.236716 140027215431488 spec.py:349] Evaluating on the test split.
I0129 10:57:45.900245 140027215431488 submission_runner.py:408] Time since start: 34359.24s, 	Step: 98304, 	{'train/accuracy': 0.351283460855484, 'train/loss': 3.222519874572754, 'validation/accuracy': 0.33215999603271484, 'validation/loss': 3.3702187538146973, 'validation/num_examples': 50000, 'test/accuracy': 0.2519000172615051, 'test/loss': 4.132585525512695, 'test/num_examples': 10000, 'score': 33188.55919909477, 'total_duration': 34359.24005818367, 'accumulated_submission_time': 33188.55919909477, 'accumulated_eval_time': 1164.826674938202, 'accumulated_logging_time': 2.409458637237549}
I0129 10:57:45.937386 139865240893184 logging_writer.py:48] [98304] accumulated_eval_time=1164.826675, accumulated_logging_time=2.409459, accumulated_submission_time=33188.559199, global_step=98304, preemption_count=0, score=33188.559199, test/accuracy=0.251900, test/loss=4.132586, test/num_examples=10000, total_duration=34359.240058, train/accuracy=0.351283, train/loss=3.222520, validation/accuracy=0.332160, validation/loss=3.370219, validation/num_examples=50000
I0129 10:58:18.588263 139865760950016 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.7523138523101807, loss=1.9386835098266602
I0129 10:58:52.274243 139865240893184 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.8463643789291382, loss=1.9795539379119873
I0129 10:59:25.945086 139865760950016 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.7820965051651, loss=2.0372214317321777
I0129 10:59:59.640935 139865240893184 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.7318848371505737, loss=1.9853849411010742
I0129 11:00:33.331935 139865760950016 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.900256872177124, loss=2.0795822143554688
I0129 11:01:07.019793 139865240893184 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.662784218788147, loss=1.867941975593567
I0129 11:01:40.698298 139865760950016 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.683362603187561, loss=2.067589044570923
I0129 11:02:14.368194 139865240893184 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.6218453645706177, loss=1.9233907461166382
I0129 11:02:48.050542 139865760950016 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.1027824878692627, loss=2.0081887245178223
I0129 11:03:21.751878 139865240893184 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.7073920965194702, loss=1.9092522859573364
I0129 11:03:55.584829 139865760950016 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.9799724817276, loss=1.9904017448425293
I0129 11:04:29.292071 139865240893184 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.6247189044952393, loss=1.9935907125473022
I0129 11:05:02.981477 139865760950016 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.0574193000793457, loss=2.03587007522583
I0129 11:05:36.680924 139865240893184 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.7869224548339844, loss=2.009134531021118
I0129 11:06:10.361967 139865760950016 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.6549110412597656, loss=1.8429800271987915
I0129 11:06:15.904740 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:06:22.135173 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:06:31.025448 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:06:33.940542 140027215431488 submission_runner.py:408] Time since start: 34887.28s, 	Step: 99818, 	{'train/accuracy': 0.328125, 'train/loss': 3.5659189224243164, 'validation/accuracy': 0.3105199933052063, 'validation/loss': 3.713387966156006, 'validation/num_examples': 50000, 'test/accuracy': 0.2345000058412552, 'test/loss': 4.5583319664001465, 'test/num_examples': 10000, 'score': 33698.464007377625, 'total_duration': 34887.28036427498, 'accumulated_submission_time': 33698.464007377625, 'accumulated_eval_time': 1182.862447977066, 'accumulated_logging_time': 2.4572508335113525}
I0129 11:06:33.970415 139866171975424 logging_writer.py:48] [99818] accumulated_eval_time=1182.862448, accumulated_logging_time=2.457251, accumulated_submission_time=33698.464007, global_step=99818, preemption_count=0, score=33698.464007, test/accuracy=0.234500, test/loss=4.558332, test/num_examples=10000, total_duration=34887.280364, train/accuracy=0.328125, train/loss=3.565919, validation/accuracy=0.310520, validation/loss=3.713388, validation/num_examples=50000
I0129 11:07:01.938264 139866180368128 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.8273351192474365, loss=2.031379461288452
I0129 11:07:35.652053 139866171975424 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.6472012996673584, loss=1.8589441776275635
I0129 11:08:09.321928 139866180368128 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.6986525058746338, loss=1.940154790878296
I0129 11:08:43.021842 139866171975424 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.7807780504226685, loss=1.9615371227264404
I0129 11:09:16.684294 139866180368128 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.5766700506210327, loss=1.9032483100891113
I0129 11:09:50.392072 139866171975424 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.8006584644317627, loss=1.978210210800171
I0129 11:10:24.325582 139866180368128 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.905745029449463, loss=1.8966349363327026
I0129 11:10:58.044232 139866171975424 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.727231502532959, loss=1.8651022911071777
I0129 11:11:31.721794 139866180368128 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.6780359745025635, loss=1.9285633563995361
I0129 11:12:05.445381 139866171975424 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.643296718597412, loss=1.8670871257781982
I0129 11:12:39.102814 139866180368128 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.6972931623458862, loss=1.941388726234436
I0129 11:13:12.814072 139866171975424 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.6412899494171143, loss=1.9437732696533203
I0129 11:13:46.481230 139866180368128 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.08736515045166, loss=2.0610389709472656
I0129 11:14:20.188608 139866171975424 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.5973924398422241, loss=1.8881185054779053
I0129 11:14:53.840884 139866180368128 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.6536309719085693, loss=1.9161698818206787
I0129 11:15:04.079805 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:15:10.312566 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:15:19.004665 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:15:21.610381 140027215431488 submission_runner.py:408] Time since start: 35414.95s, 	Step: 101332, 	{'train/accuracy': 0.4641461968421936, 'train/loss': 2.4268994331359863, 'validation/accuracy': 0.43977999687194824, 'validation/loss': 2.5999388694763184, 'validation/num_examples': 50000, 'test/accuracy': 0.33330002427101135, 'test/loss': 3.3893141746520996, 'test/num_examples': 10000, 'score': 34208.510954380035, 'total_duration': 35414.95019340515, 'accumulated_submission_time': 34208.510954380035, 'accumulated_eval_time': 1200.3929872512817, 'accumulated_logging_time': 2.495596408843994}
I0129 11:15:21.646235 139865232500480 logging_writer.py:48] [101332] accumulated_eval_time=1200.392987, accumulated_logging_time=2.495596, accumulated_submission_time=34208.510954, global_step=101332, preemption_count=0, score=34208.510954, test/accuracy=0.333300, test/loss=3.389314, test/num_examples=10000, total_duration=35414.950193, train/accuracy=0.464146, train/loss=2.426899, validation/accuracy=0.439780, validation/loss=2.599939, validation/num_examples=50000
I0129 11:15:44.858010 139865240893184 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.8830770254135132, loss=1.8952031135559082
I0129 11:16:18.535274 139865232500480 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.8257217407226562, loss=2.004443645477295
I0129 11:16:52.366312 139865240893184 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.694318175315857, loss=1.9180892705917358
I0129 11:17:26.063328 139865232500480 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.01410174369812, loss=2.138458251953125
I0129 11:17:59.742371 139865240893184 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.8668609857559204, loss=1.907464861869812
I0129 11:18:33.443983 139865232500480 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.8809449672698975, loss=2.0029430389404297
I0129 11:19:07.126303 139865240893184 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.8120478391647339, loss=1.97568941116333
I0129 11:19:40.812552 139865232500480 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.7223055362701416, loss=1.9705924987792969
I0129 11:20:14.501041 139865240893184 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.8560459613800049, loss=1.9943172931671143
I0129 11:20:48.203073 139865232500480 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.0129220485687256, loss=1.9619672298431396
I0129 11:21:21.889644 139865240893184 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.7310222387313843, loss=1.8907506465911865
I0129 11:21:55.571881 139865232500480 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.66081702709198, loss=1.8579063415527344
I0129 11:22:29.272718 139865240893184 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.9675867557525635, loss=1.948197364807129
I0129 11:23:03.017796 139865232500480 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.8118512630462646, loss=1.9198027849197388
I0129 11:23:36.724265 139865240893184 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.7582690715789795, loss=1.997398853302002
I0129 11:23:51.688137 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:23:57.857633 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:24:06.718163 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:24:09.413380 140027215431488 submission_runner.py:408] Time since start: 35942.75s, 	Step: 102846, 	{'train/accuracy': 0.4216557741165161, 'train/loss': 2.711665391921997, 'validation/accuracy': 0.3911399841308594, 'validation/loss': 2.9025368690490723, 'validation/num_examples': 50000, 'test/accuracy': 0.30580002069473267, 'test/loss': 3.579463481903076, 'test/num_examples': 10000, 'score': 34718.491564273834, 'total_duration': 35942.75317645073, 'accumulated_submission_time': 34718.491564273834, 'accumulated_eval_time': 1218.1181762218475, 'accumulated_logging_time': 2.540222406387329}
I0129 11:24:09.446866 139866171975424 logging_writer.py:48] [102846] accumulated_eval_time=1218.118176, accumulated_logging_time=2.540222, accumulated_submission_time=34718.491564, global_step=102846, preemption_count=0, score=34718.491564, test/accuracy=0.305800, test/loss=3.579463, test/num_examples=10000, total_duration=35942.753176, train/accuracy=0.421656, train/loss=2.711665, validation/accuracy=0.391140, validation/loss=2.902537, validation/num_examples=50000
I0129 11:24:27.990283 139866180368128 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.842973232269287, loss=1.9393362998962402
I0129 11:25:01.669643 139866171975424 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.0578291416168213, loss=1.9210617542266846
I0129 11:25:35.380346 139866180368128 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.7493878602981567, loss=1.871501088142395
I0129 11:26:09.052328 139866171975424 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.7810183763504028, loss=1.9022009372711182
I0129 11:26:42.764117 139866180368128 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.7070366144180298, loss=1.9606622457504272
I0129 11:27:16.413164 139866171975424 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.8122509717941284, loss=1.9742774963378906
I0129 11:27:50.128762 139866180368128 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.8169890642166138, loss=1.9113306999206543
I0129 11:28:23.797635 139866171975424 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.8327672481536865, loss=2.0108258724212646
I0129 11:28:57.505999 139866180368128 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.0002973079681396, loss=1.8796474933624268
I0129 11:29:31.325623 139866171975424 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.7636805772781372, loss=1.828956127166748
I0129 11:30:05.036561 139866180368128 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.960391879081726, loss=1.9236503839492798
I0129 11:30:38.723461 139866171975424 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.5823049545288086, loss=1.7935667037963867
I0129 11:31:12.439285 139866180368128 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.8157726526260376, loss=1.9690630435943604
I0129 11:31:46.083045 139866171975424 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.8117800951004028, loss=1.9395925998687744
I0129 11:32:19.796589 139866180368128 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.7493170499801636, loss=1.943575382232666
I0129 11:32:39.460307 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:32:45.656588 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:32:54.550322 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:32:57.174198 140027215431488 submission_runner.py:408] Time since start: 36470.51s, 	Step: 104360, 	{'train/accuracy': 0.5540298223495483, 'train/loss': 1.8740363121032715, 'validation/accuracy': 0.4943999946117401, 'validation/loss': 2.240631580352783, 'validation/num_examples': 50000, 'test/accuracy': 0.37880000472068787, 'test/loss': 3.0557496547698975, 'test/num_examples': 10000, 'score': 35228.44272494316, 'total_duration': 36470.514008522034, 'accumulated_submission_time': 35228.44272494316, 'accumulated_eval_time': 1235.8320398330688, 'accumulated_logging_time': 2.584373712539673}
I0129 11:32:57.209478 139865240893184 logging_writer.py:48] [104360] accumulated_eval_time=1235.832040, accumulated_logging_time=2.584374, accumulated_submission_time=35228.442725, global_step=104360, preemption_count=0, score=35228.442725, test/accuracy=0.378800, test/loss=3.055750, test/num_examples=10000, total_duration=36470.514009, train/accuracy=0.554030, train/loss=1.874036, validation/accuracy=0.494400, validation/loss=2.240632, validation/num_examples=50000
I0129 11:33:11.013771 139865760950016 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.7753608226776123, loss=1.9278538227081299
I0129 11:33:44.733679 139865240893184 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.052065372467041, loss=2.0256845951080322
I0129 11:34:18.440843 139865760950016 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.8360134363174438, loss=1.9999191761016846
I0129 11:34:52.115497 139865240893184 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.723318338394165, loss=1.931310772895813
I0129 11:35:25.829078 139865760950016 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.877951979637146, loss=1.9623682498931885
I0129 11:35:59.649620 139865240893184 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.885613203048706, loss=1.987102746963501
I0129 11:36:33.351715 139865760950016 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.8395248651504517, loss=1.8287107944488525
I0129 11:37:07.039133 139865240893184 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.7342716455459595, loss=1.884971261024475
I0129 11:37:40.743444 139865760950016 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.953989028930664, loss=2.0624170303344727
I0129 11:38:14.410320 139865240893184 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.6385782957077026, loss=1.831355333328247
I0129 11:38:48.104918 139865760950016 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.8765888214111328, loss=1.9523894786834717
I0129 11:39:21.787048 139865240893184 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.8521664142608643, loss=1.9786999225616455
I0129 11:39:55.489913 139865760950016 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.803818702697754, loss=1.9103765487670898
I0129 11:40:29.150615 139865240893184 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.7927929162979126, loss=1.91837739944458
I0129 11:41:02.837818 139865760950016 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.9041268825531006, loss=1.8997076749801636
I0129 11:41:27.230293 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:41:33.403534 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:41:42.094434 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:41:44.639513 140027215431488 submission_runner.py:408] Time since start: 36997.98s, 	Step: 105874, 	{'train/accuracy': 0.5051219463348389, 'train/loss': 2.1285979747772217, 'validation/accuracy': 0.45809999108314514, 'validation/loss': 2.4042491912841797, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.1027300357818604, 'test/num_examples': 10000, 'score': 35738.398156404495, 'total_duration': 36997.97931480408, 'accumulated_submission_time': 35738.398156404495, 'accumulated_eval_time': 1253.2412164211273, 'accumulated_logging_time': 2.6315665245056152}
I0129 11:41:44.681967 139866171975424 logging_writer.py:48] [105874] accumulated_eval_time=1253.241216, accumulated_logging_time=2.631567, accumulated_submission_time=35738.398156, global_step=105874, preemption_count=0, score=35738.398156, test/accuracy=0.361300, test/loss=3.102730, test/num_examples=10000, total_duration=36997.979315, train/accuracy=0.505122, train/loss=2.128598, validation/accuracy=0.458100, validation/loss=2.404249, validation/num_examples=50000
I0129 11:41:53.773884 139866180368128 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.825915813446045, loss=1.8257598876953125
I0129 11:42:27.551718 139866171975424 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.896245002746582, loss=1.9809240102767944
I0129 11:43:01.247891 139866180368128 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.9151875972747803, loss=1.8906235694885254
I0129 11:43:34.958895 139866171975424 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.851871132850647, loss=1.8383647203445435
I0129 11:44:08.606260 139866180368128 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.7763906717300415, loss=1.8882921934127808
I0129 11:44:42.340650 139866171975424 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.8948249816894531, loss=2.0275862216949463
I0129 11:45:16.018279 139866180368128 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.7054845094680786, loss=1.9575839042663574
I0129 11:45:49.731316 139866171975424 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.8916391134262085, loss=1.9362260103225708
I0129 11:46:23.382030 139866180368128 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.8850102424621582, loss=1.9218109846115112
I0129 11:46:57.088083 139866171975424 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.8151397705078125, loss=1.8033366203308105
I0129 11:47:30.760988 139866180368128 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.9014500379562378, loss=1.9021192789077759
I0129 11:48:04.465193 139866171975424 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.007941722869873, loss=1.864391803741455
I0129 11:48:38.134320 139866180368128 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.0503153800964355, loss=1.878497838973999
I0129 11:49:11.942677 139866171975424 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.8505340814590454, loss=1.8901690244674683
I0129 11:49:45.605660 139866180368128 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.7506636381149292, loss=1.8251163959503174
I0129 11:50:14.756613 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:50:21.087409 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:50:29.965377 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:50:32.630542 140027215431488 submission_runner.py:408] Time since start: 37525.97s, 	Step: 107388, 	{'train/accuracy': 0.5316087007522583, 'train/loss': 2.0331740379333496, 'validation/accuracy': 0.4949599802494049, 'validation/loss': 2.2442522048950195, 'validation/num_examples': 50000, 'test/accuracy': 0.3841000199317932, 'test/loss': 3.025560140609741, 'test/num_examples': 10000, 'score': 36248.40759110451, 'total_duration': 37525.970309495926, 'accumulated_submission_time': 36248.40759110451, 'accumulated_eval_time': 1271.1150813102722, 'accumulated_logging_time': 2.683964490890503}
I0129 11:50:32.690286 139865240893184 logging_writer.py:48] [107388] accumulated_eval_time=1271.115081, accumulated_logging_time=2.683964, accumulated_submission_time=36248.407591, global_step=107388, preemption_count=0, score=36248.407591, test/accuracy=0.384100, test/loss=3.025560, test/num_examples=10000, total_duration=37525.970309, train/accuracy=0.531609, train/loss=2.033174, validation/accuracy=0.494960, validation/loss=2.244252, validation/num_examples=50000
I0129 11:50:37.099065 139865760950016 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.9336141347885132, loss=1.8859562873840332
I0129 11:51:10.812782 139865240893184 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.006033420562744, loss=1.788991928100586
I0129 11:51:44.493617 139865760950016 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.7901183366775513, loss=1.900984525680542
I0129 11:52:18.197800 139865240893184 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.7166563272476196, loss=1.9704774618148804
I0129 11:52:51.881617 139865760950016 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.8819243907928467, loss=1.9293410778045654
I0129 11:53:25.578929 139865240893184 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.8690900802612305, loss=1.9415669441223145
I0129 11:53:59.250342 139865760950016 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.1666202545166016, loss=1.9836360216140747
I0129 11:54:32.945061 139865240893184 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.8291417360305786, loss=1.8892407417297363
I0129 11:55:06.607488 139865760950016 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.9194612503051758, loss=1.805437445640564
I0129 11:55:40.476742 139865240893184 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.831318736076355, loss=1.8742859363555908
I0129 11:56:14.160941 139865760950016 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.778900146484375, loss=1.8914700746536255
I0129 11:56:47.868870 139865240893184 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.892580509185791, loss=1.791445016860962
I0129 11:57:21.540025 139865760950016 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.9781349897384644, loss=1.9436753988265991
I0129 11:57:55.226103 139865240893184 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.993303656578064, loss=1.9141100645065308
I0129 11:58:28.898030 139865760950016 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.8068934679031372, loss=1.8681226968765259
I0129 11:59:02.591962 139865240893184 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.8525267839431763, loss=1.819788932800293
I0129 11:59:02.746521 140027215431488 spec.py:321] Evaluating on the training split.
I0129 11:59:09.002381 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 11:59:17.543794 140027215431488 spec.py:349] Evaluating on the test split.
I0129 11:59:20.164689 140027215431488 submission_runner.py:408] Time since start: 38053.50s, 	Step: 108902, 	{'train/accuracy': 0.5332629084587097, 'train/loss': 2.0092248916625977, 'validation/accuracy': 0.49629998207092285, 'validation/loss': 2.2380595207214355, 'validation/num_examples': 50000, 'test/accuracy': 0.3831000328063965, 'test/loss': 3.0195724964141846, 'test/num_examples': 10000, 'score': 36758.39783191681, 'total_duration': 38053.5044836998, 'accumulated_submission_time': 36758.39783191681, 'accumulated_eval_time': 1288.5332021713257, 'accumulated_logging_time': 2.7576067447662354}
I0129 11:59:20.202187 139865240893184 logging_writer.py:48] [108902] accumulated_eval_time=1288.533202, accumulated_logging_time=2.757607, accumulated_submission_time=36758.397832, global_step=108902, preemption_count=0, score=36758.397832, test/accuracy=0.383100, test/loss=3.019572, test/num_examples=10000, total_duration=38053.504484, train/accuracy=0.533263, train/loss=2.009225, validation/accuracy=0.496300, validation/loss=2.238060, validation/num_examples=50000
I0129 11:59:53.553764 139865760950016 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.9479355812072754, loss=1.9476537704467773
I0129 12:00:27.278603 139865240893184 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.8936843872070312, loss=1.965086817741394
I0129 12:01:00.960647 139865760950016 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.765745997428894, loss=1.8362579345703125
I0129 12:01:34.665656 139865240893184 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.8654870986938477, loss=1.9492151737213135
I0129 12:02:08.479271 139865760950016 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.8831242322921753, loss=1.792869210243225
I0129 12:02:42.179410 139865240893184 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.9379040002822876, loss=1.8536463975906372
I0129 12:03:15.848614 139865760950016 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.8680987358093262, loss=1.9956586360931396
I0129 12:03:49.541563 139865240893184 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.2613534927368164, loss=1.9442408084869385
I0129 12:04:23.177836 139865760950016 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.0047528743743896, loss=1.9247406721115112
I0129 12:04:56.889430 139865240893184 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.9168322086334229, loss=1.9246857166290283
I0129 12:05:30.541347 139865760950016 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.0535006523132324, loss=1.939621090888977
I0129 12:06:04.260751 139865240893184 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.9338150024414062, loss=1.856446623802185
I0129 12:06:37.923981 139865760950016 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.8357517719268799, loss=1.856809377670288
I0129 12:07:11.646055 139865240893184 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.9420535564422607, loss=1.8070478439331055
I0129 12:07:45.315372 139865760950016 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.879645586013794, loss=1.9282283782958984
I0129 12:07:50.191856 140027215431488 spec.py:321] Evaluating on the training split.
I0129 12:07:56.650430 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 12:08:05.251459 140027215431488 spec.py:349] Evaluating on the test split.
I0129 12:08:07.871730 140027215431488 submission_runner.py:408] Time since start: 38581.21s, 	Step: 110416, 	{'train/accuracy': 0.5140505433082581, 'train/loss': 2.1145224571228027, 'validation/accuracy': 0.4885999858379364, 'validation/loss': 2.261136531829834, 'validation/num_examples': 50000, 'test/accuracy': 0.3677000105381012, 'test/loss': 3.0803956985473633, 'test/num_examples': 10000, 'score': 37268.326226234436, 'total_duration': 38581.21153998375, 'accumulated_submission_time': 37268.326226234436, 'accumulated_eval_time': 1306.213036775589, 'accumulated_logging_time': 2.8046953678131104}
I0129 12:08:07.909151 139865769342720 logging_writer.py:48] [110416] accumulated_eval_time=1306.213037, accumulated_logging_time=2.804695, accumulated_submission_time=37268.326226, global_step=110416, preemption_count=0, score=37268.326226, test/accuracy=0.367700, test/loss=3.080396, test/num_examples=10000, total_duration=38581.211540, train/accuracy=0.514051, train/loss=2.114522, validation/accuracy=0.488600, validation/loss=2.261137, validation/num_examples=50000
I0129 12:08:36.571253 139866163582720 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.8254263401031494, loss=1.8671590089797974
I0129 12:09:10.303118 139865769342720 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.923695683479309, loss=1.8272819519042969
I0129 12:09:43.973962 139866163582720 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.8143573999404907, loss=1.940289855003357
I0129 12:10:17.687159 139865769342720 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.839129090309143, loss=1.7850159406661987
I0129 12:10:51.345137 139866163582720 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.9724457263946533, loss=1.9595296382904053
I0129 12:11:25.043059 139865769342720 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.969543218612671, loss=1.7577465772628784
I0129 12:11:58.730684 139866163582720 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.922972559928894, loss=1.7303810119628906
I0129 12:12:32.448076 139865769342720 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.0210111141204834, loss=1.9402525424957275
I0129 12:13:06.100248 139866163582720 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.0102005004882812, loss=1.8123067617416382
I0129 12:13:39.795533 139865769342720 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.10243821144104, loss=1.8437882661819458
I0129 12:14:13.465638 139866163582720 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.2046635150909424, loss=1.857935905456543
I0129 12:14:47.324117 139865769342720 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.8857327699661255, loss=1.8363010883331299
I0129 12:15:21.001343 139866163582720 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.053375720977783, loss=1.824923038482666
I0129 12:15:54.707511 139865769342720 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.1446447372436523, loss=1.9682856798171997
I0129 12:16:28.347702 139866163582720 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.076951265335083, loss=1.8646780252456665
I0129 12:16:37.917674 140027215431488 spec.py:321] Evaluating on the training split.
I0129 12:16:44.131398 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 12:16:52.928379 140027215431488 spec.py:349] Evaluating on the test split.
I0129 12:16:55.573396 140027215431488 submission_runner.py:408] Time since start: 39108.91s, 	Step: 111930, 	{'train/accuracy': 0.5302335619926453, 'train/loss': 2.0475571155548096, 'validation/accuracy': 0.48955997824668884, 'validation/loss': 2.2830638885498047, 'validation/num_examples': 50000, 'test/accuracy': 0.36970001459121704, 'test/loss': 3.0804600715637207, 'test/num_examples': 10000, 'score': 37778.27315187454, 'total_duration': 39108.91320705414, 'accumulated_submission_time': 37778.27315187454, 'accumulated_eval_time': 1323.868717432022, 'accumulated_logging_time': 2.851999282836914}
I0129 12:16:55.614699 139865760950016 logging_writer.py:48] [111930] accumulated_eval_time=1323.868717, accumulated_logging_time=2.851999, accumulated_submission_time=37778.273152, global_step=111930, preemption_count=0, score=37778.273152, test/accuracy=0.369700, test/loss=3.080460, test/num_examples=10000, total_duration=39108.913207, train/accuracy=0.530234, train/loss=2.047557, validation/accuracy=0.489560, validation/loss=2.283064, validation/num_examples=50000
I0129 12:17:19.527337 139865769342720 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.0728533267974854, loss=1.907271385192871
I0129 12:17:53.188933 139865760950016 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.146141529083252, loss=1.8460345268249512
I0129 12:18:26.891752 139865769342720 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.06542706489563, loss=1.8930068016052246
I0129 12:19:00.564988 139865760950016 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.9264748096466064, loss=1.8565404415130615
I0129 12:19:34.260765 139865769342720 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.9096757173538208, loss=1.8303239345550537
I0129 12:20:07.958933 139865760950016 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.07782244682312, loss=1.8671280145645142
I0129 12:20:41.664520 139865769342720 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.079028367996216, loss=1.8909682035446167
I0129 12:21:15.475220 139865760950016 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.12733793258667, loss=1.907676339149475
I0129 12:21:49.179831 139865769342720 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.927894949913025, loss=1.7399884462356567
I0129 12:22:22.835999 139865760950016 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.066287040710449, loss=1.8787736892700195
I0129 12:22:56.537693 139865769342720 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.9527958631515503, loss=1.9211480617523193
I0129 12:23:30.213840 139865760950016 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.166306972503662, loss=1.7943226099014282
I0129 12:24:03.912749 139865769342720 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.2635624408721924, loss=1.8704627752304077
I0129 12:24:37.585238 139865760950016 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.0634403228759766, loss=1.9269241094589233
I0129 12:25:11.282409 139865769342720 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.9363514184951782, loss=1.725522518157959
I0129 12:25:25.905463 140027215431488 spec.py:321] Evaluating on the training split.
I0129 12:25:32.163664 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 12:25:41.126184 140027215431488 spec.py:349] Evaluating on the test split.
I0129 12:25:43.674642 140027215431488 submission_runner.py:408] Time since start: 39637.01s, 	Step: 113445, 	{'train/accuracy': 0.5375478267669678, 'train/loss': 1.958598017692566, 'validation/accuracy': 0.4891199767589569, 'validation/loss': 2.234560489654541, 'validation/num_examples': 50000, 'test/accuracy': 0.3817000091075897, 'test/loss': 2.9632489681243896, 'test/num_examples': 10000, 'score': 38288.500826358795, 'total_duration': 39637.01444840431, 'accumulated_submission_time': 38288.500826358795, 'accumulated_eval_time': 1341.6378679275513, 'accumulated_logging_time': 2.9030396938323975}
I0129 12:25:43.714913 139865232500480 logging_writer.py:48] [113445] accumulated_eval_time=1341.637868, accumulated_logging_time=2.903040, accumulated_submission_time=38288.500826, global_step=113445, preemption_count=0, score=38288.500826, test/accuracy=0.381700, test/loss=2.963249, test/num_examples=10000, total_duration=39637.014448, train/accuracy=0.537548, train/loss=1.958598, validation/accuracy=0.489120, validation/loss=2.234560, validation/num_examples=50000
I0129 12:26:02.597677 139865240893184 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.8321763277053833, loss=1.7756056785583496
I0129 12:26:36.251276 139865232500480 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.8243590593338013, loss=1.8329496383666992
I0129 12:27:10.048835 139865240893184 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.131666421890259, loss=1.8457759618759155
I0129 12:27:43.780086 139865232500480 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.270646333694458, loss=1.8733477592468262
I0129 12:28:17.473397 139865240893184 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.0208826065063477, loss=1.802729606628418
I0129 12:28:51.124723 139865232500480 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.0395116806030273, loss=1.776808261871338
I0129 12:29:24.810982 139865240893184 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.224257707595825, loss=1.933158040046692
I0129 12:29:58.486585 139865232500480 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.0808513164520264, loss=1.8777260780334473
I0129 12:30:32.200278 139865240893184 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.9468573331832886, loss=1.844774842262268
I0129 12:31:05.857932 139865232500480 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.1073060035705566, loss=1.8694778680801392
I0129 12:31:39.568726 139865240893184 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.206329345703125, loss=1.7631218433380127
I0129 12:32:13.215263 139865232500480 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.396585464477539, loss=1.9549853801727295
I0129 12:32:46.926585 139865240893184 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.129286289215088, loss=1.7962284088134766
I0129 12:33:20.587538 139865232500480 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.8992609977722168, loss=1.7986327409744263
I0129 12:33:54.311192 139865240893184 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.1759350299835205, loss=1.8607181310653687
I0129 12:34:13.957461 140027215431488 spec.py:321] Evaluating on the training split.
I0129 12:34:20.139703 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 12:34:28.616336 140027215431488 spec.py:349] Evaluating on the test split.
I0129 12:34:31.291472 140027215431488 submission_runner.py:408] Time since start: 40164.63s, 	Step: 114960, 	{'train/accuracy': 0.5321866869926453, 'train/loss': 1.9915037155151367, 'validation/accuracy': 0.49831998348236084, 'validation/loss': 2.20292329788208, 'validation/num_examples': 50000, 'test/accuracy': 0.379800021648407, 'test/loss': 3.0490152835845947, 'test/num_examples': 10000, 'score': 38798.68224453926, 'total_duration': 40164.63128519058, 'accumulated_submission_time': 38798.68224453926, 'accumulated_eval_time': 1358.971853017807, 'accumulated_logging_time': 2.952772617340088}
I0129 12:34:31.328675 139865232500480 logging_writer.py:48] [114960] accumulated_eval_time=1358.971853, accumulated_logging_time=2.952773, accumulated_submission_time=38798.682245, global_step=114960, preemption_count=0, score=38798.682245, test/accuracy=0.379800, test/loss=3.049015, test/num_examples=10000, total_duration=40164.631285, train/accuracy=0.532187, train/loss=1.991504, validation/accuracy=0.498320, validation/loss=2.202923, validation/num_examples=50000
I0129 12:34:45.121297 139865769342720 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.236798048019409, loss=1.8196027278900146
I0129 12:35:18.808316 139865232500480 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.1331849098205566, loss=1.8455523252487183
I0129 12:35:52.512830 139865769342720 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.183668851852417, loss=1.8704123497009277
I0129 12:36:26.187707 139865232500480 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.0790812969207764, loss=1.706244707107544
I0129 12:36:59.872548 139865769342720 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.0311124324798584, loss=1.9603979587554932
I0129 12:37:33.567190 139865232500480 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.051298141479492, loss=1.7905477285385132
I0129 12:38:07.239784 139865769342720 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.9199059009552002, loss=1.7927768230438232
I0129 12:38:40.916331 139865232500480 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.2000436782836914, loss=1.9858293533325195
I0129 12:39:14.592750 139865769342720 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.0823774337768555, loss=1.7701025009155273
I0129 12:39:48.278797 139865232500480 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.9841209650039673, loss=1.9733786582946777
I0129 12:40:22.081018 139865769342720 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.000375747680664, loss=1.7557342052459717
I0129 12:40:55.769096 139865232500480 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.2257776260375977, loss=1.9666707515716553
I0129 12:41:29.466997 139865769342720 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.9768054485321045, loss=1.8114285469055176
I0129 12:42:03.151020 139865232500480 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.0422492027282715, loss=1.822283387184143
I0129 12:42:36.846305 139865769342720 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.047119379043579, loss=1.847093105316162
I0129 12:43:01.564345 140027215431488 spec.py:321] Evaluating on the training split.
I0129 12:43:07.865483 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 12:43:16.439266 140027215431488 spec.py:349] Evaluating on the test split.
I0129 12:43:19.090257 140027215431488 submission_runner.py:408] Time since start: 40692.43s, 	Step: 116475, 	{'train/accuracy': 0.5823102593421936, 'train/loss': 1.767098069190979, 'validation/accuracy': 0.5386199951171875, 'validation/loss': 2.0045840740203857, 'validation/num_examples': 50000, 'test/accuracy': 0.41780000925064087, 'test/loss': 2.817715644836426, 'test/num_examples': 10000, 'score': 39308.85549354553, 'total_duration': 40692.430067777634, 'accumulated_submission_time': 39308.85549354553, 'accumulated_eval_time': 1376.4977297782898, 'accumulated_logging_time': 2.9995055198669434}
I0129 12:43:19.128659 139865760950016 logging_writer.py:48] [116475] accumulated_eval_time=1376.497730, accumulated_logging_time=2.999506, accumulated_submission_time=39308.855494, global_step=116475, preemption_count=0, score=39308.855494, test/accuracy=0.417800, test/loss=2.817716, test/num_examples=10000, total_duration=40692.430068, train/accuracy=0.582310, train/loss=1.767098, validation/accuracy=0.538620, validation/loss=2.004584, validation/num_examples=50000
I0129 12:43:27.903795 139866163582720 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.197484016418457, loss=1.7914996147155762
I0129 12:44:01.643865 139865760950016 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.9409703016281128, loss=1.9221482276916504
I0129 12:44:35.313848 139866163582720 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.0758464336395264, loss=1.8414709568023682
I0129 12:45:09.034458 139865760950016 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.1512036323547363, loss=1.7902640104293823
I0129 12:45:42.670693 139866163582720 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.097982168197632, loss=1.7812423706054688
I0129 12:46:16.388695 139865760950016 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.0222110748291016, loss=1.6141632795333862
I0129 12:46:50.191741 139866163582720 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.112947463989258, loss=1.7457404136657715
I0129 12:47:23.913875 139865760950016 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.098986864089966, loss=1.8543442487716675
I0129 12:47:57.572392 139866163582720 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.228271722793579, loss=1.9122108221054077
I0129 12:48:31.287376 139865760950016 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.2506837844848633, loss=1.8575153350830078
I0129 12:49:04.950790 139866163582720 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.1162872314453125, loss=1.8115404844284058
I0129 12:49:38.677175 139865760950016 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.1865596771240234, loss=1.8801617622375488
I0129 12:50:12.345088 139866163582720 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.2977077960968018, loss=1.7884501218795776
I0129 12:50:46.053051 139865760950016 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.0836501121520996, loss=1.9728946685791016
I0129 12:51:19.704384 139866163582720 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.2331125736236572, loss=1.875514030456543
I0129 12:51:49.183479 140027215431488 spec.py:321] Evaluating on the training split.
I0129 12:51:55.442975 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 12:52:04.207163 140027215431488 spec.py:349] Evaluating on the test split.
I0129 12:52:06.841825 140027215431488 submission_runner.py:408] Time since start: 41220.18s, 	Step: 117989, 	{'train/accuracy': 0.4283721148967743, 'train/loss': 2.6851370334625244, 'validation/accuracy': 0.3919200003147125, 'validation/loss': 2.899744987487793, 'validation/num_examples': 50000, 'test/accuracy': 0.2914000153541565, 'test/loss': 3.691535711288452, 'test/num_examples': 10000, 'score': 39818.84736561775, 'total_duration': 41220.18163561821, 'accumulated_submission_time': 39818.84736561775, 'accumulated_eval_time': 1394.1560413837433, 'accumulated_logging_time': 3.0487263202667236}
I0129 12:52:06.883820 139865769342720 logging_writer.py:48] [117989] accumulated_eval_time=1394.156041, accumulated_logging_time=3.048726, accumulated_submission_time=39818.847366, global_step=117989, preemption_count=0, score=39818.847366, test/accuracy=0.291400, test/loss=3.691536, test/num_examples=10000, total_duration=41220.181636, train/accuracy=0.428372, train/loss=2.685137, validation/accuracy=0.391920, validation/loss=2.899745, validation/num_examples=50000
I0129 12:52:10.922095 139866171975424 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.122758626937866, loss=1.8914225101470947
I0129 12:52:44.603215 139865769342720 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.14072322845459, loss=1.980032205581665
I0129 12:53:18.399991 139866171975424 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.0603561401367188, loss=1.8777265548706055
I0129 12:53:52.103315 139865769342720 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.1484479904174805, loss=1.8644053936004639
I0129 12:54:25.773873 139866171975424 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.1569299697875977, loss=1.7464697360992432
I0129 12:54:59.453768 139865769342720 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.1545774936676025, loss=1.7996187210083008
I0129 12:55:33.146363 139866171975424 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.1238858699798584, loss=1.7386606931686401
I0129 12:56:06.833845 139865769342720 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.200434923171997, loss=1.7989444732666016
I0129 12:56:40.522974 139866171975424 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.070801019668579, loss=1.7941818237304688
I0129 12:57:14.214970 139865769342720 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.1761910915374756, loss=1.7344447374343872
I0129 12:57:47.901485 139866171975424 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.075131416320801, loss=1.87700617313385
I0129 12:58:21.606181 139865769342720 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.3002665042877197, loss=1.8439964056015015
I0129 12:58:55.287875 139866171975424 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.1871204376220703, loss=1.7225360870361328
I0129 12:59:29.051282 139865769342720 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.173062562942505, loss=1.80631422996521
I0129 13:00:02.773422 139866171975424 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.253068685531616, loss=1.8506698608398438
I0129 13:00:36.458949 139865769342720 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.1696090698242188, loss=1.783403992652893
I0129 13:00:36.945183 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:00:43.183948 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:00:51.524525 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:00:54.145171 140027215431488 submission_runner.py:408] Time since start: 41747.48s, 	Step: 119503, 	{'train/accuracy': 0.44764429330825806, 'train/loss': 2.602922201156616, 'validation/accuracy': 0.42197999358177185, 'validation/loss': 2.8036701679229736, 'validation/num_examples': 50000, 'test/accuracy': 0.32200002670288086, 'test/loss': 3.63352370262146, 'test/num_examples': 10000, 'score': 40328.843329668045, 'total_duration': 41747.484981536865, 'accumulated_submission_time': 40328.843329668045, 'accumulated_eval_time': 1411.3559973239899, 'accumulated_logging_time': 3.1032016277313232}
I0129 13:00:54.183788 139865232500480 logging_writer.py:48] [119503] accumulated_eval_time=1411.355997, accumulated_logging_time=3.103202, accumulated_submission_time=40328.843330, global_step=119503, preemption_count=0, score=40328.843330, test/accuracy=0.322000, test/loss=3.633524, test/num_examples=10000, total_duration=41747.484982, train/accuracy=0.447644, train/loss=2.602922, validation/accuracy=0.421980, validation/loss=2.803670, validation/num_examples=50000
I0129 13:01:27.230073 139865240893184 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.254850387573242, loss=1.8645325899124146
I0129 13:02:00.914999 139865232500480 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.189101219177246, loss=1.7814966440200806
I0129 13:02:34.586462 139865240893184 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.314028739929199, loss=1.9197994470596313
I0129 13:03:08.295351 139865232500480 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.0995430946350098, loss=1.7683604955673218
I0129 13:03:41.955922 139865240893184 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.1572470664978027, loss=1.7764030694961548
I0129 13:04:15.669159 139865232500480 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.1330175399780273, loss=1.7676295042037964
I0129 13:04:49.329026 139865240893184 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.294698715209961, loss=1.7917107343673706
I0129 13:05:23.055504 139865232500480 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.3700339794158936, loss=1.8240046501159668
I0129 13:05:56.796809 139865240893184 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.115962505340576, loss=1.6732959747314453
I0129 13:06:30.542604 139865232500480 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.1029434204101562, loss=1.7372817993164062
I0129 13:07:04.201021 139865240893184 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.1842339038848877, loss=1.8201541900634766
I0129 13:07:37.903827 139865232500480 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.1886727809906006, loss=1.7953201532363892
I0129 13:08:11.548430 139865240893184 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.0748050212860107, loss=1.7494674921035767
I0129 13:08:45.265594 139865232500480 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.1901931762695312, loss=1.6887223720550537
I0129 13:09:18.944673 139865240893184 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.2763376235961914, loss=1.8741693496704102
I0129 13:09:24.148401 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:09:30.334071 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:09:39.191370 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:09:41.712170 140027215431488 submission_runner.py:408] Time since start: 42275.05s, 	Step: 121017, 	{'train/accuracy': 0.5178372263908386, 'train/loss': 2.0767385959625244, 'validation/accuracy': 0.46949997544288635, 'validation/loss': 2.3726882934570312, 'validation/num_examples': 50000, 'test/accuracy': 0.35590001940727234, 'test/loss': 3.1724939346313477, 'test/num_examples': 10000, 'score': 40838.74562501907, 'total_duration': 42275.05197453499, 'accumulated_submission_time': 40838.74562501907, 'accumulated_eval_time': 1428.919724702835, 'accumulated_logging_time': 3.1509153842926025}
I0129 13:09:41.754551 139866163582720 logging_writer.py:48] [121017] accumulated_eval_time=1428.919725, accumulated_logging_time=3.150915, accumulated_submission_time=40838.745625, global_step=121017, preemption_count=0, score=40838.745625, test/accuracy=0.355900, test/loss=3.172494, test/num_examples=10000, total_duration=42275.051975, train/accuracy=0.517837, train/loss=2.076739, validation/accuracy=0.469500, validation/loss=2.372688, validation/num_examples=50000
I0129 13:10:10.094552 139866171975424 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.14025616645813, loss=1.7472155094146729
I0129 13:10:43.782233 139866163582720 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.144054412841797, loss=1.635131597518921
I0129 13:11:17.457713 139866171975424 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.2539291381835938, loss=1.7443459033966064
I0129 13:11:51.151030 139866163582720 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.252580165863037, loss=1.7822630405426025
I0129 13:12:24.927824 139866171975424 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.18855357170105, loss=1.8047256469726562
I0129 13:12:58.658224 139866163582720 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.24221134185791, loss=1.7459824085235596
I0129 13:13:32.324206 139866171975424 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.401510238647461, loss=1.7291985750198364
I0129 13:14:06.026817 139866163582720 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.303288459777832, loss=1.7914097309112549
I0129 13:14:39.714657 139866171975424 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.3761801719665527, loss=1.7368053197860718
I0129 13:15:13.409360 139866163582720 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.159064769744873, loss=1.6902797222137451
I0129 13:15:47.082313 139866171975424 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.1525957584381104, loss=1.8297297954559326
I0129 13:16:20.784977 139866163582720 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.1726861000061035, loss=1.7197966575622559
I0129 13:16:54.463706 139866171975424 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.2682716846466064, loss=1.802554965019226
I0129 13:17:28.161127 139866163582720 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.215428590774536, loss=1.7316807508468628
I0129 13:18:01.825484 139866171975424 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.440512180328369, loss=1.7559208869934082
I0129 13:18:11.720324 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:18:17.949251 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:18:26.489004 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:18:29.148823 140027215431488 submission_runner.py:408] Time since start: 42802.49s, 	Step: 122531, 	{'train/accuracy': 0.6288264989852905, 'train/loss': 1.5047695636749268, 'validation/accuracy': 0.5694999694824219, 'validation/loss': 1.8335968255996704, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.6093904972076416, 'test/num_examples': 10000, 'score': 41348.64927601814, 'total_duration': 42802.48861408234, 'accumulated_submission_time': 41348.64927601814, 'accumulated_eval_time': 1446.3481650352478, 'accumulated_logging_time': 3.2026822566986084}
I0129 13:18:29.192059 139865769342720 logging_writer.py:48] [122531] accumulated_eval_time=1446.348165, accumulated_logging_time=3.202682, accumulated_submission_time=41348.649276, global_step=122531, preemption_count=0, score=41348.649276, test/accuracy=0.445600, test/loss=2.609390, test/num_examples=10000, total_duration=42802.488614, train/accuracy=0.628826, train/loss=1.504770, validation/accuracy=0.569500, validation/loss=1.833597, validation/num_examples=50000
I0129 13:18:52.866753 139866180368128 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.074516534805298, loss=1.6942040920257568
I0129 13:19:26.569895 139865769342720 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.3094005584716797, loss=1.820566177368164
I0129 13:20:00.275034 139866180368128 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.28725528717041, loss=1.7798511981964111
I0129 13:20:33.945648 139865769342720 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.1931843757629395, loss=1.7948365211486816
I0129 13:21:07.664344 139866180368128 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.235243082046509, loss=1.7636626958847046
I0129 13:21:41.310203 139865769342720 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.1470470428466797, loss=1.6009101867675781
I0129 13:22:15.004332 139866180368128 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.303567409515381, loss=1.7323691844940186
I0129 13:22:48.660630 139865769342720 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.4531450271606445, loss=1.7667526006698608
I0129 13:23:22.365954 139866180368128 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.3543355464935303, loss=1.8157824277877808
I0129 13:23:56.033004 139865769342720 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.356053352355957, loss=1.7693421840667725
I0129 13:24:29.736127 139866180368128 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.6913836002349854, loss=1.6608588695526123
I0129 13:25:03.400945 139865769342720 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.3963770866394043, loss=1.778174638748169
I0129 13:25:37.188241 139866180368128 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.176400899887085, loss=1.6685954332351685
I0129 13:26:10.879447 139865769342720 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.276097536087036, loss=1.7848076820373535
I0129 13:26:44.596187 139866180368128 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.2479751110076904, loss=1.7462676763534546
I0129 13:26:59.218668 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:27:05.739261 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:27:14.196077 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:27:17.001700 140027215431488 submission_runner.py:408] Time since start: 43330.34s, 	Step: 124045, 	{'train/accuracy': 0.6246811151504517, 'train/loss': 1.5214416980743408, 'validation/accuracy': 0.5735200047492981, 'validation/loss': 1.811548113822937, 'validation/num_examples': 50000, 'test/accuracy': 0.44520002603530884, 'test/loss': 2.5815134048461914, 'test/num_examples': 10000, 'score': 41858.61288642883, 'total_duration': 43330.341500520706, 'accumulated_submission_time': 41858.61288642883, 'accumulated_eval_time': 1464.131145477295, 'accumulated_logging_time': 3.255311965942383}
I0129 13:27:17.046071 139866163582720 logging_writer.py:48] [124045] accumulated_eval_time=1464.131145, accumulated_logging_time=3.255312, accumulated_submission_time=41858.612886, global_step=124045, preemption_count=0, score=41858.612886, test/accuracy=0.445200, test/loss=2.581513, test/num_examples=10000, total_duration=43330.341501, train/accuracy=0.624681, train/loss=1.521442, validation/accuracy=0.573520, validation/loss=1.811548, validation/num_examples=50000
I0129 13:27:35.916666 139866171975424 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.1871020793914795, loss=1.8010348081588745
I0129 13:28:09.579734 139866163582720 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.8661551475524902, loss=1.8033440113067627
I0129 13:28:43.294513 139866171975424 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.204235553741455, loss=1.5895628929138184
I0129 13:29:16.976254 139866163582720 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.336293935775757, loss=1.8927271366119385
I0129 13:29:50.668266 139866171975424 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.230112314224243, loss=1.8513320684432983
I0129 13:30:24.327306 139866163582720 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.351285696029663, loss=1.8551734685897827
I0129 13:30:58.013244 139866171975424 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.3338096141815186, loss=1.721466302871704
I0129 13:31:31.678221 139866163582720 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.214262008666992, loss=1.7757304906845093
I0129 13:32:05.546705 139866171975424 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.4109740257263184, loss=1.8502689599990845
I0129 13:32:39.195184 139866163582720 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.182333469390869, loss=1.8219351768493652
I0129 13:33:12.899408 139866171975424 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.1539885997772217, loss=1.7837142944335938
I0129 13:33:46.568659 139866163582720 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.420809030532837, loss=1.7131166458129883
I0129 13:34:20.275023 139866171975424 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.35335636138916, loss=1.841593623161316
I0129 13:34:53.955075 139866163582720 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.180957555770874, loss=1.670208215713501
I0129 13:35:27.663830 139866171975424 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.55542254447937, loss=1.741272211074829
I0129 13:35:47.342666 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:35:53.546564 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:36:02.441549 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:36:04.965629 140027215431488 submission_runner.py:408] Time since start: 43858.31s, 	Step: 125560, 	{'train/accuracy': 0.6104312539100647, 'train/loss': 1.618659257888794, 'validation/accuracy': 0.5592799782752991, 'validation/loss': 1.9150943756103516, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.698650360107422, 'test/num_examples': 10000, 'score': 42368.84647965431, 'total_duration': 43858.305431604385, 'accumulated_submission_time': 42368.84647965431, 'accumulated_eval_time': 1481.754063129425, 'accumulated_logging_time': 3.3087551593780518}
I0129 13:36:05.006084 139865240893184 logging_writer.py:48] [125560] accumulated_eval_time=1481.754063, accumulated_logging_time=3.308755, accumulated_submission_time=42368.846480, global_step=125560, preemption_count=0, score=42368.846480, test/accuracy=0.444800, test/loss=2.698650, test/num_examples=10000, total_duration=43858.305432, train/accuracy=0.610431, train/loss=1.618659, validation/accuracy=0.559280, validation/loss=1.915094, validation/num_examples=50000
I0129 13:36:18.802944 139865760950016 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.538419246673584, loss=1.7482410669326782
I0129 13:36:52.528298 139865240893184 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.659475564956665, loss=1.7848515510559082
I0129 13:37:26.202324 139865760950016 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.457169532775879, loss=1.8419952392578125
I0129 13:37:59.898893 139865240893184 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.4112062454223633, loss=1.5690340995788574
I0129 13:38:33.731709 139865760950016 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.6453144550323486, loss=1.7613929510116577
I0129 13:39:07.429675 139865240893184 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.0867974758148193, loss=1.5754077434539795
I0129 13:39:41.085620 139865760950016 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.2520949840545654, loss=1.61747145652771
I0129 13:40:14.787032 139865240893184 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.646181583404541, loss=1.773233413696289
I0129 13:40:48.455272 139865760950016 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.389338970184326, loss=1.7257956266403198
I0129 13:41:22.177505 139865240893184 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.515646457672119, loss=1.8098514080047607
I0129 13:41:55.862263 139865760950016 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.488166332244873, loss=1.7391417026519775
I0129 13:42:29.556542 139865240893184 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.528963565826416, loss=1.8026279211044312
I0129 13:43:03.209986 139865760950016 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.3457190990448, loss=1.7856847047805786
I0129 13:43:36.897594 139865240893184 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.4466335773468018, loss=1.729296326637268
I0129 13:44:10.555735 139865760950016 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.7035343647003174, loss=1.7338613271713257
I0129 13:44:35.148294 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:44:41.333442 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:44:50.023663 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:44:52.646052 140027215431488 submission_runner.py:408] Time since start: 44385.99s, 	Step: 127074, 	{'train/accuracy': 0.5913584232330322, 'train/loss': 1.713505506515503, 'validation/accuracy': 0.5462999939918518, 'validation/loss': 2.0027928352355957, 'validation/num_examples': 50000, 'test/accuracy': 0.42900002002716064, 'test/loss': 2.7601938247680664, 'test/num_examples': 10000, 'score': 42878.92653274536, 'total_duration': 44385.98586678505, 'accumulated_submission_time': 42878.92653274536, 'accumulated_eval_time': 1499.2517862319946, 'accumulated_logging_time': 3.358278512954712}
I0129 13:44:52.688232 139865240893184 logging_writer.py:48] [127074] accumulated_eval_time=1499.251786, accumulated_logging_time=3.358279, accumulated_submission_time=42878.926533, global_step=127074, preemption_count=0, score=42878.926533, test/accuracy=0.429000, test/loss=2.760194, test/num_examples=10000, total_duration=44385.985867, train/accuracy=0.591358, train/loss=1.713506, validation/accuracy=0.546300, validation/loss=2.002793, validation/num_examples=50000
I0129 13:45:01.796863 139866163582720 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.6392035484313965, loss=1.734935998916626
I0129 13:45:35.524930 139865240893184 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.3678765296936035, loss=1.6604936122894287
I0129 13:46:09.189155 139866163582720 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.2426156997680664, loss=1.7265198230743408
I0129 13:46:42.894645 139865240893184 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.5891377925872803, loss=1.8160581588745117
I0129 13:47:16.546862 139866163582720 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.746015787124634, loss=1.818225622177124
I0129 13:47:50.248571 139865240893184 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.466618061065674, loss=1.6784896850585938
I0129 13:48:23.916923 139866163582720 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.315574884414673, loss=1.7317334413528442
I0129 13:48:57.611694 139865240893184 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.5379488468170166, loss=1.6245006322860718
I0129 13:49:31.278910 139866163582720 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.6378746032714844, loss=1.76987624168396
I0129 13:50:04.975515 139865240893184 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.5213005542755127, loss=1.8032146692276
I0129 13:50:38.642891 139866163582720 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.491098403930664, loss=1.7466130256652832
I0129 13:51:12.482154 139865240893184 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.2770931720733643, loss=1.6689246892929077
I0129 13:51:46.183252 139866163582720 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.322211980819702, loss=1.6535453796386719
I0129 13:52:19.905858 139865240893184 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.595440149307251, loss=1.6888841390609741
I0129 13:52:53.590067 139866163582720 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.6856091022491455, loss=1.7562795877456665
I0129 13:53:22.708305 140027215431488 spec.py:321] Evaluating on the training split.
I0129 13:53:28.868402 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 13:53:37.506541 140027215431488 spec.py:349] Evaluating on the test split.
I0129 13:53:40.066052 140027215431488 submission_runner.py:408] Time since start: 44913.41s, 	Step: 128588, 	{'train/accuracy': 0.6278898119926453, 'train/loss': 1.5220273733139038, 'validation/accuracy': 0.5804799795150757, 'validation/loss': 1.792826771736145, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.5546529293060303, 'test/num_examples': 10000, 'score': 43388.882608652115, 'total_duration': 44913.40585780144, 'accumulated_submission_time': 43388.882608652115, 'accumulated_eval_time': 1516.6094889640808, 'accumulated_logging_time': 3.411818504333496}
I0129 13:53:40.108367 139865769342720 logging_writer.py:48] [128588] accumulated_eval_time=1516.609489, accumulated_logging_time=3.411819, accumulated_submission_time=43388.882609, global_step=128588, preemption_count=0, score=43388.882609, test/accuracy=0.460300, test/loss=2.554653, test/num_examples=10000, total_duration=44913.405858, train/accuracy=0.627890, train/loss=1.522027, validation/accuracy=0.580480, validation/loss=1.792827, validation/num_examples=50000
I0129 13:53:44.501875 139866180368128 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.425513982772827, loss=1.6780815124511719
I0129 13:54:18.221568 139865769342720 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.309145212173462, loss=1.5998128652572632
I0129 13:54:51.924343 139866180368128 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.4764058589935303, loss=1.7389730215072632
I0129 13:55:25.602191 139865769342720 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.488309860229492, loss=1.606607437133789
I0129 13:55:59.302895 139866180368128 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.419990062713623, loss=1.6136436462402344
I0129 13:56:32.965991 139865769342720 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.3466598987579346, loss=1.5418758392333984
I0129 13:57:06.668244 139866180368128 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.684840679168701, loss=1.727083444595337
I0129 13:57:40.511068 139865769342720 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.5130534172058105, loss=1.7546995878219604
I0129 13:58:14.184870 139866180368128 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.657456159591675, loss=1.6095612049102783
I0129 13:58:47.867633 139865769342720 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.429994583129883, loss=1.7040021419525146
I0129 13:59:21.552766 139866180368128 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.489884614944458, loss=1.8310805559158325
I0129 13:59:55.226471 139865769342720 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.5918784141540527, loss=1.6539815664291382
I0129 14:00:28.917809 139866180368128 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.4170005321502686, loss=1.559885859489441
I0129 14:01:02.566430 139865769342720 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.5494489669799805, loss=1.685346007347107
I0129 14:01:36.254299 139866180368128 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.3926308155059814, loss=1.6179862022399902
I0129 14:02:09.935149 139865769342720 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.427509069442749, loss=1.6489754915237427
I0129 14:02:10.091881 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:02:16.287363 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:02:25.201978 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:02:27.932748 140027215431488 submission_runner.py:408] Time since start: 45441.27s, 	Step: 130102, 	{'train/accuracy': 0.6101123690605164, 'train/loss': 1.616065502166748, 'validation/accuracy': 0.5356799960136414, 'validation/loss': 2.0169262886047363, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.8192737102508545, 'test/num_examples': 10000, 'score': 43898.80344581604, 'total_duration': 45441.272557497025, 'accumulated_submission_time': 43898.80344581604, 'accumulated_eval_time': 1534.450347661972, 'accumulated_logging_time': 3.4636495113372803}
I0129 14:02:27.988510 139865760950016 logging_writer.py:48] [130102] accumulated_eval_time=1534.450348, accumulated_logging_time=3.463650, accumulated_submission_time=43898.803446, global_step=130102, preemption_count=0, score=43898.803446, test/accuracy=0.416400, test/loss=2.819274, test/num_examples=10000, total_duration=45441.272557, train/accuracy=0.610112, train/loss=1.616066, validation/accuracy=0.535680, validation/loss=2.016926, validation/num_examples=50000
I0129 14:03:01.332712 139865769342720 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.4921305179595947, loss=1.5721067190170288
I0129 14:03:35.046641 139865760950016 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.525686264038086, loss=1.7326325178146362
I0129 14:04:08.839245 139865769342720 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.528752088546753, loss=1.5685200691223145
I0129 14:04:42.576972 139865760950016 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.559603452682495, loss=1.6103734970092773
I0129 14:05:16.253148 139865769342720 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.544863224029541, loss=1.761000156402588
I0129 14:05:49.955550 139865760950016 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.5379135608673096, loss=1.6114541292190552
I0129 14:06:23.619445 139865769342720 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.623267889022827, loss=1.7585811614990234
I0129 14:06:57.329265 139865760950016 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.7112770080566406, loss=1.7577934265136719
I0129 14:07:30.982699 139865769342720 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.355731964111328, loss=1.5902172327041626
I0129 14:08:04.676152 139865760950016 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.690063714981079, loss=1.6423228979110718
I0129 14:08:38.313406 139865769342720 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.646235942840576, loss=1.597100853919983
I0129 14:09:12.009064 139865760950016 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.6243550777435303, loss=1.713207721710205
I0129 14:09:45.673480 139865769342720 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.694420576095581, loss=1.7635174989700317
I0129 14:10:19.367553 139865760950016 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.637744903564453, loss=1.6425217390060425
I0129 14:10:53.203093 139865769342720 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.969231367111206, loss=1.7265039682388306
I0129 14:10:58.067011 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:11:04.310969 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:11:13.012084 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:11:15.648319 140027215431488 submission_runner.py:408] Time since start: 45968.99s, 	Step: 131616, 	{'train/accuracy': 0.6253786683082581, 'train/loss': 1.5129410028457642, 'validation/accuracy': 0.5648399591445923, 'validation/loss': 1.8565038442611694, 'validation/num_examples': 50000, 'test/accuracy': 0.44190001487731934, 'test/loss': 2.6380434036254883, 'test/num_examples': 10000, 'score': 44408.81854104996, 'total_duration': 45968.98812127113, 'accumulated_submission_time': 44408.81854104996, 'accumulated_eval_time': 1552.0316081047058, 'accumulated_logging_time': 3.529684543609619}
I0129 14:11:15.690160 139865760950016 logging_writer.py:48] [131616] accumulated_eval_time=1552.031608, accumulated_logging_time=3.529685, accumulated_submission_time=44408.818541, global_step=131616, preemption_count=0, score=44408.818541, test/accuracy=0.441900, test/loss=2.638043, test/num_examples=10000, total_duration=45968.988121, train/accuracy=0.625379, train/loss=1.512941, validation/accuracy=0.564840, validation/loss=1.856504, validation/num_examples=50000
I0129 14:11:44.313539 139866180368128 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.5318334102630615, loss=1.660630226135254
I0129 14:12:18.012458 139865760950016 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.3910231590270996, loss=1.5690356492996216
I0129 14:12:51.681301 139866180368128 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.539316415786743, loss=1.5280855894088745
I0129 14:13:25.351436 139865760950016 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.644929885864258, loss=1.6087981462478638
I0129 14:13:59.032812 139866180368128 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.76963472366333, loss=1.5934650897979736
I0129 14:14:32.721255 139865760950016 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.661264181137085, loss=1.6359021663665771
I0129 14:15:06.408644 139866180368128 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.6399343013763428, loss=1.678905963897705
I0129 14:15:40.101290 139865760950016 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.8580708503723145, loss=1.647335171699524
I0129 14:16:13.790352 139866180368128 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.604705572128296, loss=1.6938366889953613
I0129 14:16:47.468521 139865760950016 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.3705475330352783, loss=1.6211020946502686
I0129 14:17:21.304350 139866180368128 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.6420392990112305, loss=1.651617169380188
I0129 14:17:54.947781 139865760950016 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.7594895362854004, loss=1.7006690502166748
I0129 14:18:28.648514 139866180368128 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.5571391582489014, loss=1.6706721782684326
I0129 14:19:02.338498 139865760950016 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.9000566005706787, loss=1.6805903911590576
I0129 14:19:36.003194 139866180368128 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.5251076221466064, loss=1.6380431652069092
I0129 14:19:45.904756 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:19:52.239300 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:20:00.684786 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:20:03.414522 140027215431488 submission_runner.py:408] Time since start: 46496.75s, 	Step: 133131, 	{'train/accuracy': 0.6495934128761292, 'train/loss': 1.4116238355636597, 'validation/accuracy': 0.5912799835205078, 'validation/loss': 1.7342907190322876, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.4350662231445312, 'test/num_examples': 10000, 'score': 44918.97057008743, 'total_duration': 46496.75433373451, 'accumulated_submission_time': 44918.97057008743, 'accumulated_eval_time': 1569.5413410663605, 'accumulated_logging_time': 3.5810797214508057}
I0129 14:20:03.455044 139865224107776 logging_writer.py:48] [133131] accumulated_eval_time=1569.541341, accumulated_logging_time=3.581080, accumulated_submission_time=44918.970570, global_step=133131, preemption_count=0, score=44918.970570, test/accuracy=0.480000, test/loss=2.435066, test/num_examples=10000, total_duration=46496.754334, train/accuracy=0.649593, train/loss=1.411624, validation/accuracy=0.591280, validation/loss=1.734291, validation/num_examples=50000
I0129 14:20:27.009460 139865232500480 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.7455568313598633, loss=1.5707573890686035
I0129 14:21:00.733831 139865224107776 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.8649404048919678, loss=1.705165982246399
I0129 14:21:34.438012 139865232500480 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.0746500492095947, loss=1.6591074466705322
I0129 14:22:08.104287 139865224107776 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.716642141342163, loss=1.6868420839309692
I0129 14:22:41.842471 139865232500480 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.6824982166290283, loss=1.5415472984313965
I0129 14:23:15.484282 139865224107776 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.774735450744629, loss=1.682950496673584
I0129 14:23:49.352891 139865232500480 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.6890041828155518, loss=1.5945130586624146
I0129 14:24:23.005836 139865224107776 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.580392599105835, loss=1.583630084991455
I0129 14:24:56.708731 139865232500480 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.7137327194213867, loss=1.626963496208191
I0129 14:25:30.385718 139865224107776 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.581505060195923, loss=1.5943690538406372
I0129 14:26:04.103595 139865232500480 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.7557942867279053, loss=1.6522209644317627
I0129 14:26:37.784403 139865224107776 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.4062113761901855, loss=1.5785685777664185
I0129 14:27:11.501337 139865232500480 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.567222833633423, loss=1.6539404392242432
I0129 14:27:45.157386 139865224107776 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.464146375656128, loss=1.5594602823257446
I0129 14:28:18.863567 139865232500480 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2157702445983887, loss=1.6936668157577515
I0129 14:28:33.474745 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:28:39.657614 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:28:48.461521 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:28:51.093163 140027215431488 submission_runner.py:408] Time since start: 47024.43s, 	Step: 134645, 	{'train/accuracy': 0.6475008130073547, 'train/loss': 1.4163345098495483, 'validation/accuracy': 0.5905199646949768, 'validation/loss': 1.7114334106445312, 'validation/num_examples': 50000, 'test/accuracy': 0.4717000126838684, 'test/loss': 2.4368271827697754, 'test/num_examples': 10000, 'score': 45428.928008794785, 'total_duration': 47024.43297600746, 'accumulated_submission_time': 45428.928008794785, 'accumulated_eval_time': 1587.159719467163, 'accumulated_logging_time': 3.6312103271484375}
I0129 14:28:51.134493 139865232500480 logging_writer.py:48] [134645] accumulated_eval_time=1587.159719, accumulated_logging_time=3.631210, accumulated_submission_time=45428.928009, global_step=134645, preemption_count=0, score=45428.928009, test/accuracy=0.471700, test/loss=2.436827, test/num_examples=10000, total_duration=47024.432976, train/accuracy=0.647501, train/loss=1.416335, validation/accuracy=0.590520, validation/loss=1.711433, validation/num_examples=50000
I0129 14:29:10.000223 139866171975424 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.698763608932495, loss=1.664006233215332
I0129 14:29:43.665638 139865232500480 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.73537015914917, loss=1.6418848037719727
I0129 14:30:17.512560 139866171975424 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.823150634765625, loss=1.6312003135681152
I0129 14:30:51.180983 139865232500480 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.7275941371917725, loss=1.722561001777649
I0129 14:31:24.885216 139866171975424 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.6728005409240723, loss=1.621072769165039
I0129 14:31:58.554362 139865232500480 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.063993215560913, loss=1.6718920469284058
I0129 14:32:32.253095 139866171975424 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.615098714828491, loss=1.516813039779663
I0129 14:33:05.924094 139865232500480 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.6614646911621094, loss=1.5662449598312378
I0129 14:33:39.615294 139866171975424 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.677567481994629, loss=1.5904954671859741
I0129 14:34:13.292889 139865232500480 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.6266751289367676, loss=1.6139638423919678
I0129 14:34:46.980506 139866171975424 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.7419354915618896, loss=1.6567021608352661
I0129 14:35:20.642987 139865232500480 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.7119877338409424, loss=1.6158347129821777
I0129 14:35:54.328773 139866171975424 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.0915794372558594, loss=1.7015163898468018
I0129 14:36:28.088515 139865232500480 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.6875486373901367, loss=1.6805200576782227
I0129 14:37:01.861148 139866171975424 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.9322478771209717, loss=1.5814101696014404
I0129 14:37:21.214569 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:37:27.424890 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:37:36.140152 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:37:38.807405 140027215431488 submission_runner.py:408] Time since start: 47552.15s, 	Step: 136159, 	{'train/accuracy': 0.6627072691917419, 'train/loss': 1.3503409624099731, 'validation/accuracy': 0.6053799986839294, 'validation/loss': 1.6583715677261353, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.411069869995117, 'test/num_examples': 10000, 'score': 45938.94582152367, 'total_duration': 47552.14718770981, 'accumulated_submission_time': 45938.94582152367, 'accumulated_eval_time': 1604.7524976730347, 'accumulated_logging_time': 3.681864023208618}
I0129 14:37:38.848976 139865760950016 logging_writer.py:48] [136159] accumulated_eval_time=1604.752498, accumulated_logging_time=3.681864, accumulated_submission_time=45938.945822, global_step=136159, preemption_count=0, score=45938.945822, test/accuracy=0.485900, test/loss=2.411070, test/num_examples=10000, total_duration=47552.147188, train/accuracy=0.662707, train/loss=1.350341, validation/accuracy=0.605380, validation/loss=1.658372, validation/num_examples=50000
I0129 14:37:52.999658 139865769342720 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.6824607849121094, loss=1.5431755781173706
I0129 14:38:26.701483 139865760950016 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.6748456954956055, loss=1.669724464416504
I0129 14:39:00.358614 139865769342720 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.8577942848205566, loss=1.654071569442749
I0129 14:39:34.073393 139865760950016 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.77457332611084, loss=1.6965314149856567
I0129 14:40:07.740899 139865769342720 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.8940980434417725, loss=1.6876111030578613
I0129 14:40:41.426484 139865760950016 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.7675185203552246, loss=1.6524254083633423
I0129 14:41:15.095006 139865769342720 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.0931127071380615, loss=1.7254602909088135
I0129 14:41:48.800440 139865760950016 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.986114978790283, loss=1.6091829538345337
I0129 14:42:22.436687 139865769342720 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.869661331176758, loss=1.5080153942108154
I0129 14:42:56.210589 139865760950016 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.8000388145446777, loss=1.5974451303482056
I0129 14:43:29.929512 139865769342720 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.7380800247192383, loss=1.472678542137146
I0129 14:44:03.654222 139865760950016 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.0591185092926025, loss=1.6019889116287231
I0129 14:44:37.328876 139865769342720 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.0831563472747803, loss=1.6024168729782104
I0129 14:45:11.045378 139865760950016 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.8343451023101807, loss=1.5642647743225098
I0129 14:45:44.687411 139865769342720 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.9017629623413086, loss=1.674961805343628
I0129 14:46:09.111063 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:46:15.451765 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:46:24.346936 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:46:26.969210 140027215431488 submission_runner.py:408] Time since start: 48080.31s, 	Step: 137674, 	{'train/accuracy': 0.6785116195678711, 'train/loss': 1.2806422710418701, 'validation/accuracy': 0.6271799802780151, 'validation/loss': 1.5513432025909424, 'validation/num_examples': 50000, 'test/accuracy': 0.5019000172615051, 'test/loss': 2.331106662750244, 'test/num_examples': 10000, 'score': 46449.14306783676, 'total_duration': 48080.309012174606, 'accumulated_submission_time': 46449.14306783676, 'accumulated_eval_time': 1622.6106128692627, 'accumulated_logging_time': 3.7330148220062256}
I0129 14:46:27.013469 139865240893184 logging_writer.py:48] [137674] accumulated_eval_time=1622.610613, accumulated_logging_time=3.733015, accumulated_submission_time=46449.143068, global_step=137674, preemption_count=0, score=46449.143068, test/accuracy=0.501900, test/loss=2.331107, test/num_examples=10000, total_duration=48080.309012, train/accuracy=0.678512, train/loss=1.280642, validation/accuracy=0.627180, validation/loss=1.551343, validation/num_examples=50000
I0129 14:46:36.101685 139865760950016 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.105928421020508, loss=1.65045964717865
I0129 14:47:09.837125 139865240893184 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.945329189300537, loss=1.489872694015503
I0129 14:47:43.520492 139865760950016 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.9866645336151123, loss=1.6247308254241943
I0129 14:48:17.226240 139865240893184 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.0076138973236084, loss=1.6673941612243652
I0129 14:48:50.896313 139865760950016 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.7215261459350586, loss=1.555691123008728
I0129 14:49:24.665069 139865240893184 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.8902390003204346, loss=1.7270890474319458
I0129 14:49:58.383533 139865760950016 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.828214645385742, loss=1.5212167501449585
I0129 14:50:32.085502 139865240893184 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.923455238342285, loss=1.633284330368042
I0129 14:51:05.758967 139865760950016 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.0280704498291016, loss=1.580514669418335
I0129 14:51:39.471697 139865240893184 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.6956276893615723, loss=1.48402738571167
I0129 14:52:13.149979 139865760950016 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.041646957397461, loss=1.6020839214324951
I0129 14:52:46.859813 139865240893184 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.1246464252471924, loss=1.6415244340896606
I0129 14:53:20.544265 139865760950016 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.731433153152466, loss=1.5662997961044312
I0129 14:53:54.244600 139865240893184 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.1170291900634766, loss=1.719426155090332
I0129 14:54:27.917696 139865760950016 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.1636900901794434, loss=1.5814622640609741
I0129 14:54:57.034987 140027215431488 spec.py:321] Evaluating on the training split.
I0129 14:55:03.280028 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 14:55:11.773717 140027215431488 spec.py:349] Evaluating on the test split.
I0129 14:55:14.427283 140027215431488 submission_runner.py:408] Time since start: 48607.77s, 	Step: 139188, 	{'train/accuracy': 0.667410671710968, 'train/loss': 1.3150213956832886, 'validation/accuracy': 0.5842999815940857, 'validation/loss': 1.7889710664749146, 'validation/num_examples': 50000, 'test/accuracy': 0.467600017786026, 'test/loss': 2.5262668132781982, 'test/num_examples': 10000, 'score': 46959.104165792465, 'total_duration': 48607.7670943737, 'accumulated_submission_time': 46959.104165792465, 'accumulated_eval_time': 1640.0028715133667, 'accumulated_logging_time': 3.786928653717041}
I0129 14:55:14.472188 139865240893184 logging_writer.py:48] [139188] accumulated_eval_time=1640.002872, accumulated_logging_time=3.786929, accumulated_submission_time=46959.104166, global_step=139188, preemption_count=0, score=46959.104166, test/accuracy=0.467600, test/loss=2.526267, test/num_examples=10000, total_duration=48607.767094, train/accuracy=0.667411, train/loss=1.315021, validation/accuracy=0.584300, validation/loss=1.788971, validation/num_examples=50000
I0129 14:55:18.867186 139865769342720 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.9259376525878906, loss=1.5500308275222778
I0129 14:55:52.724302 139865240893184 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.939246892929077, loss=1.5294395685195923
I0129 14:56:26.410117 139865769342720 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.057502031326294, loss=1.6519033908843994
I0129 14:57:00.082972 139865240893184 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.895838737487793, loss=1.479403018951416
I0129 14:57:33.777790 139865769342720 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.8530118465423584, loss=1.5459935665130615
I0129 14:58:07.445678 139865240893184 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.9623754024505615, loss=1.5344289541244507
I0129 14:58:41.161614 139865769342720 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.22212290763855, loss=1.620031714439392
I0129 14:59:14.822384 139865240893184 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.632107734680176, loss=1.4520323276519775
I0129 14:59:48.522255 139865769342720 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.816666841506958, loss=1.4707069396972656
I0129 15:00:22.185768 139865240893184 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.1168372631073, loss=1.5458037853240967
I0129 15:00:55.895138 139865769342720 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.0954928398132324, loss=1.5632058382034302
I0129 15:01:29.554649 139865240893184 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.1647260189056396, loss=1.5866529941558838
I0129 15:02:03.263226 139865769342720 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.883666515350342, loss=1.6191563606262207
I0129 15:02:37.082042 139865240893184 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.8684804439544678, loss=1.562511920928955
I0129 15:03:10.779414 139865769342720 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.248926877975464, loss=1.637653112411499
I0129 15:03:44.453621 139865240893184 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.8646724224090576, loss=1.445290446281433
I0129 15:03:44.461658 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:03:50.687510 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:03:59.439439 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:04:02.099732 140027215431488 submission_runner.py:408] Time since start: 49135.44s, 	Step: 140701, 	{'train/accuracy': 0.7164978981018066, 'train/loss': 1.0897971391677856, 'validation/accuracy': 0.6477800011634827, 'validation/loss': 1.4555057287216187, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.2077748775482178, 'test/num_examples': 10000, 'score': 47469.02995443344, 'total_duration': 49135.43949460983, 'accumulated_submission_time': 47469.02995443344, 'accumulated_eval_time': 1657.6408331394196, 'accumulated_logging_time': 3.8422067165374756}
I0129 15:04:02.156449 139865232500480 logging_writer.py:48] [140701] accumulated_eval_time=1657.640833, accumulated_logging_time=3.842207, accumulated_submission_time=47469.029954, global_step=140701, preemption_count=0, score=47469.029954, test/accuracy=0.517200, test/loss=2.207775, test/num_examples=10000, total_duration=49135.439495, train/accuracy=0.716498, train/loss=1.089797, validation/accuracy=0.647780, validation/loss=1.455506, validation/num_examples=50000
I0129 15:04:35.791882 139865240893184 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.308140516281128, loss=1.607073426246643
I0129 15:05:09.485048 139865232500480 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.8631527423858643, loss=1.4850608110427856
I0129 15:05:43.159048 139865240893184 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.0592405796051025, loss=1.55923330783844
I0129 15:06:16.871981 139865232500480 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.0418291091918945, loss=1.6560614109039307
I0129 15:06:50.553003 139865240893184 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.977128028869629, loss=1.4649713039398193
I0129 15:07:24.263661 139865232500480 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.17071270942688, loss=1.5035157203674316
I0129 15:07:57.927680 139865240893184 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.0929975509643555, loss=1.460971474647522
I0129 15:08:31.640749 139865232500480 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.8639612197875977, loss=1.5836694240570068
I0129 15:09:05.434836 139865240893184 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.9262242317199707, loss=1.606015682220459
I0129 15:09:39.147541 139865232500480 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.1280362606048584, loss=1.5100557804107666
I0129 15:10:12.814360 139865240893184 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.0257937908172607, loss=1.5872507095336914
I0129 15:10:46.994915 139865232500480 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.9533166885375977, loss=1.6028152704238892
I0129 15:11:20.648083 139865240893184 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.460447311401367, loss=1.5316274166107178
I0129 15:11:54.353919 139865232500480 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.1235814094543457, loss=1.4962977170944214
I0129 15:12:28.006900 139865240893184 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.997299909591675, loss=1.5332098007202148
I0129 15:12:32.197088 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:12:38.433432 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:12:47.016092 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:12:49.570582 140027215431488 submission_runner.py:408] Time since start: 49662.91s, 	Step: 142214, 	{'train/accuracy': 0.7066326141357422, 'train/loss': 1.145608901977539, 'validation/accuracy': 0.6356599926948547, 'validation/loss': 1.4984140396118164, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.242482900619507, 'test/num_examples': 10000, 'score': 47979.00236058235, 'total_duration': 49662.91038489342, 'accumulated_submission_time': 47979.00236058235, 'accumulated_eval_time': 1675.0142815113068, 'accumulated_logging_time': 3.9150285720825195}
I0129 15:12:49.612985 139865240893184 logging_writer.py:48] [142214] accumulated_eval_time=1675.014282, accumulated_logging_time=3.915029, accumulated_submission_time=47979.002361, global_step=142214, preemption_count=0, score=47979.002361, test/accuracy=0.502100, test/loss=2.242483, test/num_examples=10000, total_duration=49662.910385, train/accuracy=0.706633, train/loss=1.145609, validation/accuracy=0.635660, validation/loss=1.498414, validation/num_examples=50000
I0129 15:13:18.921287 139865760950016 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.1604011058807373, loss=1.485485553741455
I0129 15:13:52.620293 139865240893184 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.139387845993042, loss=1.511000156402588
I0129 15:14:26.338267 139865760950016 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.9702444076538086, loss=1.4469256401062012
I0129 15:15:00.018835 139865240893184 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.1457455158233643, loss=1.5926684141159058
I0129 15:15:33.801938 139865760950016 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.173236608505249, loss=1.5729211568832397
I0129 15:16:07.495846 139865240893184 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.2566466331481934, loss=1.5820348262786865
I0129 15:16:41.206382 139865760950016 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.080456256866455, loss=1.6108496189117432
I0129 15:17:14.859185 139865240893184 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.315915822982788, loss=1.5065003633499146
I0129 15:17:48.560101 139865760950016 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.944326400756836, loss=1.5074135065078735
I0129 15:18:22.234658 139865240893184 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.092562437057495, loss=1.5053141117095947
I0129 15:18:55.933343 139865760950016 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.1076650619506836, loss=1.5379197597503662
I0129 15:19:29.599366 139865240893184 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.231149911880493, loss=1.5993452072143555
I0129 15:20:03.281080 139865760950016 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.0067873001098633, loss=1.4605135917663574
I0129 15:20:36.953307 139865240893184 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.4104106426239014, loss=1.5681536197662354
I0129 15:21:10.642508 139865760950016 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.767118453979492, loss=1.5747098922729492
I0129 15:21:19.876689 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:21:26.357122 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:21:35.228793 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:21:37.855735 140027215431488 submission_runner.py:408] Time since start: 50191.20s, 	Step: 143729, 	{'train/accuracy': 0.6968669891357422, 'train/loss': 1.1911671161651611, 'validation/accuracy': 0.6352399587631226, 'validation/loss': 1.5196411609649658, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.2351131439208984, 'test/num_examples': 10000, 'score': 48489.20419001579, 'total_duration': 50191.19554066658, 'accumulated_submission_time': 48489.20419001579, 'accumulated_eval_time': 1692.9932827949524, 'accumulated_logging_time': 3.9672343730926514}
I0129 15:21:37.898772 139865240893184 logging_writer.py:48] [143729] accumulated_eval_time=1692.993283, accumulated_logging_time=3.967234, accumulated_submission_time=48489.204190, global_step=143729, preemption_count=0, score=48489.204190, test/accuracy=0.508400, test/loss=2.235113, test/num_examples=10000, total_duration=50191.195541, train/accuracy=0.696867, train/loss=1.191167, validation/accuracy=0.635240, validation/loss=1.519641, validation/num_examples=50000
I0129 15:22:02.132210 139866180368128 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.064962387084961, loss=1.5024749040603638
I0129 15:22:35.797818 139865240893184 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.242648124694824, loss=1.4439644813537598
I0129 15:23:09.521160 139866180368128 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.4541237354278564, loss=1.565438985824585
I0129 15:23:43.186396 139865240893184 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.270066738128662, loss=1.5653092861175537
I0129 15:24:16.896955 139866180368128 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.5169143676757812, loss=1.4191908836364746
I0129 15:24:50.565005 139865240893184 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.0903961658477783, loss=1.4816913604736328
I0129 15:25:24.264619 139866180368128 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.350963592529297, loss=1.614189863204956
I0129 15:25:57.923543 139865240893184 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.2593934535980225, loss=1.4889650344848633
I0129 15:26:31.627329 139866180368128 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.3501181602478027, loss=1.5486656427383423
I0129 15:27:05.291509 139865240893184 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.314840793609619, loss=1.549816370010376
I0129 15:27:39.000942 139866180368128 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.3271632194519043, loss=1.5112069845199585
I0129 15:28:12.780819 139865240893184 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.205704689025879, loss=1.5068844556808472
I0129 15:28:46.496888 139866180368128 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.4611196517944336, loss=1.5231025218963623
I0129 15:29:20.160616 139865240893184 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.4123082160949707, loss=1.5295860767364502
I0129 15:29:53.886840 139866180368128 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.321988344192505, loss=1.487616777420044
I0129 15:30:08.177616 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:30:14.445622 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:30:22.997276 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:30:25.608398 140027215431488 submission_runner.py:408] Time since start: 50718.95s, 	Step: 145244, 	{'train/accuracy': 0.6825175285339355, 'train/loss': 1.2460888624191284, 'validation/accuracy': 0.616379976272583, 'validation/loss': 1.6117098331451416, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.3389248847961426, 'test/num_examples': 10000, 'score': 48999.420974969864, 'total_duration': 50718.9482088089, 'accumulated_submission_time': 48999.420974969864, 'accumulated_eval_time': 1710.424084186554, 'accumulated_logging_time': 4.0192248821258545}
I0129 15:30:25.651378 139865760950016 logging_writer.py:48] [145244] accumulated_eval_time=1710.424084, accumulated_logging_time=4.019225, accumulated_submission_time=48999.420975, global_step=145244, preemption_count=0, score=48999.420975, test/accuracy=0.508500, test/loss=2.338925, test/num_examples=10000, total_duration=50718.948209, train/accuracy=0.682518, train/loss=1.246089, validation/accuracy=0.616380, validation/loss=1.611710, validation/num_examples=50000
I0129 15:30:44.859633 139865769342720 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.4932239055633545, loss=1.6579577922821045
I0129 15:31:18.546759 139865760950016 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.0972681045532227, loss=1.460871934890747
I0129 15:31:52.233093 139865769342720 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.9071099758148193, loss=1.3712763786315918
I0129 15:32:25.908177 139865760950016 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.2163054943084717, loss=1.5562002658843994
I0129 15:32:59.585442 139865769342720 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.244006395339966, loss=1.453554391860962
I0129 15:33:33.283298 139865760950016 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.624671459197998, loss=1.4855413436889648
I0129 15:34:06.955678 139865769342720 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.4043374061584473, loss=1.4954229593276978
I0129 15:34:40.810962 139865760950016 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.5513479709625244, loss=1.5788869857788086
I0129 15:35:14.496669 139865769342720 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.0790934562683105, loss=1.4117733240127563
I0129 15:35:48.191947 139865760950016 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.544584274291992, loss=1.5202888250350952
I0129 15:36:21.888362 139865769342720 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.352524995803833, loss=1.5523306131362915
I0129 15:36:55.566865 139865760950016 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.4023144245147705, loss=1.5191177129745483
I0129 15:37:29.243312 139865769342720 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.3709375858306885, loss=1.4445570707321167
I0129 15:38:02.929700 139865760950016 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.4691171646118164, loss=1.541174292564392
I0129 15:38:36.605231 139865769342720 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.197993278503418, loss=1.4398086071014404
I0129 15:38:55.619160 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:39:01.858829 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:39:10.667463 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:39:13.284740 140027215431488 submission_runner.py:408] Time since start: 51246.62s, 	Step: 146758, 	{'train/accuracy': 0.7297114133834839, 'train/loss': 1.0454022884368896, 'validation/accuracy': 0.6575199961662292, 'validation/loss': 1.3876160383224487, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.1201376914978027, 'test/num_examples': 10000, 'score': 49509.32262468338, 'total_duration': 51246.62455034256, 'accumulated_submission_time': 49509.32262468338, 'accumulated_eval_time': 1728.0896308422089, 'accumulated_logging_time': 4.0746119022369385}
I0129 15:39:13.333988 139865232500480 logging_writer.py:48] [146758] accumulated_eval_time=1728.089631, accumulated_logging_time=4.074612, accumulated_submission_time=49509.322625, global_step=146758, preemption_count=0, score=49509.322625, test/accuracy=0.536300, test/loss=2.120138, test/num_examples=10000, total_duration=51246.624550, train/accuracy=0.729711, train/loss=1.045402, validation/accuracy=0.657520, validation/loss=1.387616, validation/num_examples=50000
I0129 15:39:27.795333 139865240893184 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.3916115760803223, loss=1.404544472694397
I0129 15:40:01.413254 139865232500480 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.382327079772949, loss=1.5140169858932495
I0129 15:40:35.129572 139865240893184 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.4222023487091064, loss=1.4878157377243042
I0129 15:41:08.920607 139865232500480 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.273562431335449, loss=1.4388970136642456
I0129 15:41:42.590314 139865240893184 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.342679023742676, loss=1.4309427738189697
I0129 15:42:16.312642 139865232500480 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.4270987510681152, loss=1.621233344078064
I0129 15:42:49.978386 139865240893184 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.540945529937744, loss=1.4463645219802856
I0129 15:43:23.694435 139865232500480 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.1981866359710693, loss=1.4584373235702515
I0129 15:43:57.360748 139865240893184 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.3212602138519287, loss=1.3557146787643433
I0129 15:44:31.050015 139865232500480 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.7209479808807373, loss=1.3969600200653076
I0129 15:45:04.713472 139865240893184 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.202019214630127, loss=1.3827428817749023
I0129 15:45:38.414889 139865232500480 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.2440502643585205, loss=1.3442052602767944
I0129 15:46:12.095260 139865240893184 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.4273219108581543, loss=1.4387242794036865
I0129 15:46:45.803219 139865232500480 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.7140157222747803, loss=1.4157227277755737
I0129 15:47:19.554628 139865240893184 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.634993553161621, loss=1.5889081954956055
I0129 15:47:43.328803 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:47:49.597522 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:47:58.089336 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:48:00.627974 140027215431488 submission_runner.py:408] Time since start: 51773.97s, 	Step: 148272, 	{'train/accuracy': 0.7495615482330322, 'train/loss': 0.9471167325973511, 'validation/accuracy': 0.6625799536705017, 'validation/loss': 1.3929927349090576, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.1541640758514404, 'test/num_examples': 10000, 'score': 50019.25556206703, 'total_duration': 51773.96777963638, 'accumulated_submission_time': 50019.25556206703, 'accumulated_eval_time': 1745.3887765407562, 'accumulated_logging_time': 4.133601903915405}
I0129 15:48:00.673039 139866171975424 logging_writer.py:48] [148272] accumulated_eval_time=1745.388777, accumulated_logging_time=4.133602, accumulated_submission_time=50019.255562, global_step=148272, preemption_count=0, score=50019.255562, test/accuracy=0.531400, test/loss=2.154164, test/num_examples=10000, total_duration=51773.967780, train/accuracy=0.749562, train/loss=0.947117, validation/accuracy=0.662580, validation/loss=1.392993, validation/num_examples=50000
I0129 15:48:10.438172 139866180368128 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.5071184635162354, loss=1.453665852546692
I0129 15:48:44.118393 139866171975424 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.2273242473602295, loss=1.396493673324585
I0129 15:49:17.795802 139866180368128 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.370417356491089, loss=1.4729491472244263
I0129 15:49:51.479601 139866171975424 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.6099209785461426, loss=1.575829029083252
I0129 15:50:25.164691 139866180368128 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.4936776161193848, loss=1.4766916036605835
I0129 15:50:58.855679 139866171975424 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.50966477394104, loss=1.4875001907348633
I0129 15:51:32.539243 139866180368128 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.8152084350585938, loss=1.4322564601898193
I0129 15:52:06.225188 139866171975424 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.465564489364624, loss=1.405900001525879
I0129 15:52:39.913532 139866180368128 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.62180757522583, loss=1.4426172971725464
I0129 15:53:13.610336 139866171975424 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.554203987121582, loss=1.5439627170562744
I0129 15:53:47.353652 139866180368128 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.4618804454803467, loss=1.4914710521697998
I0129 15:54:21.087586 139866171975424 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.7057430744171143, loss=1.5187971591949463
I0129 15:54:54.766622 139866180368128 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.5816380977630615, loss=1.431722640991211
I0129 15:55:28.445554 139866171975424 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7548317909240723, loss=1.4692213535308838
I0129 15:56:02.128718 139866180368128 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.425361156463623, loss=1.4327284097671509
I0129 15:56:30.886525 140027215431488 spec.py:321] Evaluating on the training split.
I0129 15:56:37.080085 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 15:56:45.527824 140027215431488 spec.py:349] Evaluating on the test split.
I0129 15:56:48.176331 140027215431488 submission_runner.py:408] Time since start: 52301.52s, 	Step: 149787, 	{'train/accuracy': 0.7493821382522583, 'train/loss': 0.9529452323913574, 'validation/accuracy': 0.6685400009155273, 'validation/loss': 1.3474432229995728, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.082439422607422, 'test/num_examples': 10000, 'score': 50529.4074280262, 'total_duration': 52301.51612615585, 'accumulated_submission_time': 50529.4074280262, 'accumulated_eval_time': 1762.67853140831, 'accumulated_logging_time': 4.1883015632629395}
I0129 15:56:48.219674 139865232500480 logging_writer.py:48] [149787] accumulated_eval_time=1762.678531, accumulated_logging_time=4.188302, accumulated_submission_time=50529.407428, global_step=149787, preemption_count=0, score=50529.407428, test/accuracy=0.534500, test/loss=2.082439, test/num_examples=10000, total_duration=52301.516126, train/accuracy=0.749382, train/loss=0.952945, validation/accuracy=0.668540, validation/loss=1.347443, validation/num_examples=50000
I0129 15:56:52.946493 139865240893184 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.5828137397766113, loss=1.547410249710083
I0129 15:57:26.609387 139865232500480 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.3628993034362793, loss=1.423021912574768
I0129 15:58:00.326670 139865240893184 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.7151269912719727, loss=1.3961445093154907
I0129 15:58:33.979784 139865232500480 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.509556293487549, loss=1.4391270875930786
I0129 15:59:07.683886 139865240893184 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.546229839324951, loss=1.3491407632827759
I0129 15:59:41.351204 139865232500480 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.7366576194763184, loss=1.3861271142959595
I0129 16:00:15.125467 139865240893184 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.74047589302063, loss=1.466474175453186
I0129 16:00:48.842040 139865232500480 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.432868719100952, loss=1.4167916774749756
I0129 16:01:22.565272 139865240893184 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.398176908493042, loss=1.3569267988204956
I0129 16:01:56.219740 139865232500480 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.4500534534454346, loss=1.3780251741409302
I0129 16:02:29.929399 139865240893184 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.488783597946167, loss=1.5312813520431519
I0129 16:03:03.601518 139865232500480 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.4474496841430664, loss=1.4406344890594482
I0129 16:03:37.288819 139865240893184 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.765739917755127, loss=1.3887381553649902
I0129 16:04:10.951626 139865232500480 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.428659200668335, loss=1.3502939939498901
I0129 16:04:44.660774 139865240893184 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.86773943901062, loss=1.3840395212173462
I0129 16:05:18.317688 139865232500480 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.7650063037872314, loss=1.4099000692367554
I0129 16:05:18.326901 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:05:24.533911 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:05:33.340675 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:05:36.163778 140027215431488 submission_runner.py:408] Time since start: 52829.50s, 	Step: 151301, 	{'train/accuracy': 0.7281169891357422, 'train/loss': 1.0500671863555908, 'validation/accuracy': 0.6520599722862244, 'validation/loss': 1.4359551668167114, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.173976182937622, 'test/num_examples': 10000, 'score': 51039.449070453644, 'total_duration': 52829.5035905838, 'accumulated_submission_time': 51039.449070453644, 'accumulated_eval_time': 1780.5153470039368, 'accumulated_logging_time': 4.245532751083374}
I0129 16:05:36.210584 139866171975424 logging_writer.py:48] [151301] accumulated_eval_time=1780.515347, accumulated_logging_time=4.245533, accumulated_submission_time=51039.449070, global_step=151301, preemption_count=0, score=51039.449070, test/accuracy=0.525000, test/loss=2.173976, test/num_examples=10000, total_duration=52829.503591, train/accuracy=0.728117, train/loss=1.050067, validation/accuracy=0.652060, validation/loss=1.435955, validation/num_examples=50000
I0129 16:06:09.865639 139866180368128 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.4058115482330322, loss=1.236968755722046
I0129 16:06:43.618940 139866171975424 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.4332287311553955, loss=1.3245500326156616
I0129 16:07:17.267878 139866180368128 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.452441930770874, loss=1.3165247440338135
I0129 16:07:50.976651 139866171975424 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.6270999908447266, loss=1.4788639545440674
I0129 16:08:24.655097 139866180368128 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.632081985473633, loss=1.348447322845459
I0129 16:08:58.354218 139866171975424 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.504136562347412, loss=1.3615047931671143
I0129 16:09:32.032222 139866180368128 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.5197465419769287, loss=1.303938865661621
I0129 16:10:05.730212 139866171975424 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.781320810317993, loss=1.3138959407806396
I0129 16:10:39.406040 139866180368128 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.524087429046631, loss=1.3748557567596436
I0129 16:11:13.109682 139866171975424 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.7774112224578857, loss=1.341943621635437
I0129 16:11:46.793332 139866180368128 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.737966299057007, loss=1.3474876880645752
I0129 16:12:20.490701 139866171975424 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.785104513168335, loss=1.4187440872192383
I0129 16:12:54.165642 139866180368128 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.7951622009277344, loss=1.4351136684417725
I0129 16:13:28.028645 139866171975424 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.9272994995117188, loss=1.5284743309020996
I0129 16:14:01.722477 139866180368128 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.6464426517486572, loss=1.4495394229888916
I0129 16:14:06.244607 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:14:12.576342 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:14:20.978109 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:14:23.672605 140027215431488 submission_runner.py:408] Time since start: 53357.01s, 	Step: 152815, 	{'train/accuracy': 0.7424864172935486, 'train/loss': 0.9894612431526184, 'validation/accuracy': 0.6647999882698059, 'validation/loss': 1.3785579204559326, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.086134195327759, 'test/num_examples': 10000, 'score': 51549.42072844505, 'total_duration': 53357.012419462204, 'accumulated_submission_time': 51549.42072844505, 'accumulated_eval_time': 1797.9433093070984, 'accumulated_logging_time': 4.3016557693481445}
I0129 16:14:23.719862 139865240893184 logging_writer.py:48] [152815] accumulated_eval_time=1797.943309, accumulated_logging_time=4.301656, accumulated_submission_time=51549.420728, global_step=152815, preemption_count=0, score=51549.420728, test/accuracy=0.545500, test/loss=2.086134, test/num_examples=10000, total_duration=53357.012419, train/accuracy=0.742486, train/loss=0.989461, validation/accuracy=0.664800, validation/loss=1.378558, validation/num_examples=50000
I0129 16:14:52.720984 139865760950016 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.014942646026611, loss=1.4387508630752563
I0129 16:15:26.376558 139865240893184 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.9553616046905518, loss=1.3321290016174316
I0129 16:16:00.082926 139865760950016 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.5211904048919678, loss=1.3239119052886963
I0129 16:16:33.765148 139865240893184 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.744600296020508, loss=1.3656830787658691
I0129 16:17:07.475628 139865760950016 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.972835063934326, loss=1.2993625402450562
I0129 16:17:41.146426 139865240893184 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.69731068611145, loss=1.3742707967758179
I0129 16:18:14.841338 139865760950016 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.6461613178253174, loss=1.452056884765625
I0129 16:18:48.499083 139865240893184 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.7834112644195557, loss=1.326923131942749
I0129 16:19:22.193339 139865760950016 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.7172529697418213, loss=1.2766741514205933
I0129 16:19:56.018886 139865240893184 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.9064409732818604, loss=1.2791857719421387
I0129 16:20:29.723010 139865760950016 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.6571338176727295, loss=1.316381573677063
I0129 16:21:03.381669 139865240893184 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.7852933406829834, loss=1.3684412240982056
I0129 16:21:37.107892 139865760950016 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.5801022052764893, loss=1.269755244255066
I0129 16:22:10.775239 139865240893184 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.9302377700805664, loss=1.4300665855407715
I0129 16:22:44.469998 139865760950016 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.7927792072296143, loss=1.3239388465881348
I0129 16:22:53.709227 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:22:59.853664 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:23:08.473094 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:23:11.001035 140027215431488 submission_runner.py:408] Time since start: 53884.34s, 	Step: 154329, 	{'train/accuracy': 0.7708266973495483, 'train/loss': 0.8738231658935547, 'validation/accuracy': 0.6909799575805664, 'validation/loss': 1.2597182989120483, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.0148048400878906, 'test/num_examples': 10000, 'score': 52059.3456428051, 'total_duration': 53884.340841293335, 'accumulated_submission_time': 52059.3456428051, 'accumulated_eval_time': 1815.2350759506226, 'accumulated_logging_time': 4.359313249588013}
I0129 16:23:11.045583 139866171975424 logging_writer.py:48] [154329] accumulated_eval_time=1815.235076, accumulated_logging_time=4.359313, accumulated_submission_time=52059.345643, global_step=154329, preemption_count=0, score=52059.345643, test/accuracy=0.557400, test/loss=2.014805, test/num_examples=10000, total_duration=53884.340841, train/accuracy=0.770827, train/loss=0.873823, validation/accuracy=0.690980, validation/loss=1.259718, validation/num_examples=50000
I0129 16:23:35.293529 139866180368128 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.090256214141846, loss=1.335646152496338
I0129 16:24:08.957453 139866171975424 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.8983068466186523, loss=1.3934239149093628
I0129 16:24:42.655538 139866180368128 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.8581995964050293, loss=1.3605997562408447
I0129 16:25:16.350514 139866171975424 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.6642818450927734, loss=1.2762867212295532
I0129 16:25:50.049870 139866180368128 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.033273220062256, loss=1.385615587234497
I0129 16:26:23.859290 139866171975424 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.690605640411377, loss=1.3353655338287354
I0129 16:26:57.558078 139866180368128 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.081860065460205, loss=1.4354954957962036
I0129 16:27:31.237346 139866171975424 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.085757732391357, loss=1.408693790435791
I0129 16:28:04.936186 139866180368128 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.9467356204986572, loss=1.4201927185058594
I0129 16:28:38.605448 139866171975424 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.039769172668457, loss=1.3164024353027344
I0129 16:29:12.308769 139866180368128 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.7779712677001953, loss=1.3699501752853394
I0129 16:29:45.985662 139866171975424 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.8514206409454346, loss=1.3120105266571045
I0129 16:30:19.677495 139866180368128 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.791088342666626, loss=1.3556671142578125
I0129 16:30:53.348019 139866171975424 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.129809856414795, loss=1.4224952459335327
I0129 16:31:27.049845 139866180368128 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.82228946685791, loss=1.3411242961883545
I0129 16:31:41.331926 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:31:47.522351 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:31:55.994271 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:31:58.623394 140027215431488 submission_runner.py:408] Time since start: 54411.96s, 	Step: 155844, 	{'train/accuracy': 0.7562380433082581, 'train/loss': 0.9150132536888123, 'validation/accuracy': 0.6816799640655518, 'validation/loss': 1.297837734222412, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.047738790512085, 'test/num_examples': 10000, 'score': 52569.566727399826, 'total_duration': 54411.96320319176, 'accumulated_submission_time': 52569.566727399826, 'accumulated_eval_time': 1832.526507616043, 'accumulated_logging_time': 4.413227081298828}
I0129 16:31:58.667840 139865760950016 logging_writer.py:48] [155844] accumulated_eval_time=1832.526508, accumulated_logging_time=4.413227, accumulated_submission_time=52569.566727, global_step=155844, preemption_count=0, score=52569.566727, test/accuracy=0.547800, test/loss=2.047739, test/num_examples=10000, total_duration=54411.963203, train/accuracy=0.756238, train/loss=0.915013, validation/accuracy=0.681680, validation/loss=1.297838, validation/num_examples=50000
I0129 16:32:17.839864 139865769342720 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.9784951210021973, loss=1.3988711833953857
I0129 16:32:51.623882 139865760950016 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.842783212661743, loss=1.2979352474212646
I0129 16:33:25.277296 139865769342720 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.879265546798706, loss=1.271312952041626
I0129 16:33:58.984530 139865760950016 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.062735080718994, loss=1.4691330194473267
I0129 16:34:32.644779 139865769342720 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.997359275817871, loss=1.262081265449524
I0129 16:35:06.332708 139865760950016 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.944269895553589, loss=1.3080132007598877
I0129 16:35:40.006970 139865769342720 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.8627865314483643, loss=1.331121563911438
I0129 16:36:13.709493 139865760950016 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.761317014694214, loss=1.1735022068023682
I0129 16:36:47.382556 139865769342720 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.9786646366119385, loss=1.2447168827056885
I0129 16:37:21.090213 139865760950016 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.857271909713745, loss=1.317896842956543
I0129 16:37:54.759132 139865769342720 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.6834828853607178, loss=1.22050142288208
I0129 16:38:28.452329 139865760950016 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.8850255012512207, loss=1.2171509265899658
I0129 16:39:02.232760 139865769342720 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.909487247467041, loss=1.2317134141921997
I0129 16:39:35.978337 139865760950016 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.260561943054199, loss=1.3551115989685059
I0129 16:40:09.635205 139865769342720 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.071269989013672, loss=1.2643672227859497
I0129 16:40:28.653509 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:40:34.916113 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:40:43.764467 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:40:46.437973 140027215431488 submission_runner.py:408] Time since start: 54939.78s, 	Step: 157358, 	{'train/accuracy': 0.7893216013908386, 'train/loss': 0.7879802584648132, 'validation/accuracy': 0.6877599954605103, 'validation/loss': 1.278788447380066, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.0179154872894287, 'test/num_examples': 10000, 'score': 53079.48914647102, 'total_duration': 54939.777779340744, 'accumulated_submission_time': 53079.48914647102, 'accumulated_eval_time': 1850.3109276294708, 'accumulated_logging_time': 4.46701979637146}
I0129 16:40:46.484073 139865240893184 logging_writer.py:48] [157358] accumulated_eval_time=1850.310928, accumulated_logging_time=4.467020, accumulated_submission_time=53079.489146, global_step=157358, preemption_count=0, score=53079.489146, test/accuracy=0.559000, test/loss=2.017915, test/num_examples=10000, total_duration=54939.777779, train/accuracy=0.789322, train/loss=0.787980, validation/accuracy=0.687760, validation/loss=1.278788, validation/num_examples=50000
I0129 16:41:00.956036 139866171975424 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.150851726531982, loss=1.3489755392074585
I0129 16:41:34.641570 139865240893184 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.023927211761475, loss=1.2351107597351074
I0129 16:42:08.317617 139866171975424 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.291898250579834, loss=1.3461931943893433
I0129 16:42:42.006852 139865240893184 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.337307929992676, loss=1.3063561916351318
I0129 16:43:15.669683 139866171975424 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.280866622924805, loss=1.400266170501709
I0129 16:43:49.379219 139865240893184 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.250463485717773, loss=1.253674030303955
I0129 16:44:23.050167 139866171975424 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.181476593017578, loss=1.271131157875061
I0129 16:44:56.744190 139865240893184 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.3859543800354, loss=1.322211503982544
I0129 16:45:30.552945 139866171975424 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.793926239013672, loss=1.2255396842956543
I0129 16:46:04.274804 139865240893184 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.8828132152557373, loss=1.2292503118515015
I0129 16:46:37.941101 139866171975424 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.354806423187256, loss=1.2085130214691162
I0129 16:47:11.627740 139865240893184 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.417016506195068, loss=1.3928942680358887
I0129 16:47:45.295515 139866171975424 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.375092029571533, loss=1.2684109210968018
I0129 16:48:19.004207 139865240893184 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.136724948883057, loss=1.3081361055374146
I0129 16:48:52.676357 139866171975424 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.9189112186431885, loss=1.2344515323638916
I0129 16:49:16.726531 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:49:22.950855 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:49:31.569396 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:49:34.149024 140027215431488 submission_runner.py:408] Time since start: 55467.49s, 	Step: 158873, 	{'train/accuracy': 0.7989277839660645, 'train/loss': 0.7345190644264221, 'validation/accuracy': 0.7048599720001221, 'validation/loss': 1.2018253803253174, 'validation/num_examples': 50000, 'test/accuracy': 0.5772000551223755, 'test/loss': 1.9140307903289795, 'test/num_examples': 10000, 'score': 53589.66965150833, 'total_duration': 55467.48884010315, 'accumulated_submission_time': 53589.66965150833, 'accumulated_eval_time': 1867.7333896160126, 'accumulated_logging_time': 4.52330470085144}
I0129 16:49:34.187851 139865232500480 logging_writer.py:48] [158873] accumulated_eval_time=1867.733390, accumulated_logging_time=4.523305, accumulated_submission_time=53589.669652, global_step=158873, preemption_count=0, score=53589.669652, test/accuracy=0.577200, test/loss=1.914031, test/num_examples=10000, total_duration=55467.488840, train/accuracy=0.798928, train/loss=0.734519, validation/accuracy=0.704860, validation/loss=1.201825, validation/num_examples=50000
I0129 16:49:43.639375 139865760950016 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.327171802520752, loss=1.2765706777572632
I0129 16:50:17.331558 139865232500480 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.056612491607666, loss=1.2066534757614136
I0129 16:50:51.030236 139865760950016 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.431078910827637, loss=1.280266523361206
I0129 16:51:24.698521 139865232500480 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.285449981689453, loss=1.26934015750885
I0129 16:51:58.511516 139865760950016 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.501534938812256, loss=1.2501734495162964
I0129 16:52:32.188591 139865232500480 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.311120986938477, loss=1.2565184831619263
I0129 16:53:05.875911 139865760950016 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.1237311363220215, loss=1.219677448272705
I0129 16:53:39.541896 139865232500480 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.618518829345703, loss=1.3685616254806519
I0129 16:54:13.243124 139865760950016 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.211624622344971, loss=1.2595577239990234
I0129 16:54:46.928339 139865232500480 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.167566776275635, loss=1.2140611410140991
I0129 16:55:20.643249 139865760950016 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.2066497802734375, loss=1.2727265357971191
I0129 16:55:54.350250 139865232500480 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.9531924724578857, loss=1.1290122270584106
I0129 16:56:28.042698 139865760950016 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.9585041999816895, loss=1.16071355342865
I0129 16:57:01.705428 139865232500480 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.8939948081970215, loss=1.2025158405303955
I0129 16:57:35.404939 139865760950016 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.2783203125, loss=1.2272671461105347
I0129 16:58:04.210217 140027215431488 spec.py:321] Evaluating on the training split.
I0129 16:58:10.442145 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 16:58:18.969463 140027215431488 spec.py:349] Evaluating on the test split.
I0129 16:58:21.621608 140027215431488 submission_runner.py:408] Time since start: 55994.96s, 	Step: 160387, 	{'train/accuracy': 0.7996053695678711, 'train/loss': 0.7400349974632263, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.1957767009735107, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 1.8834655284881592, 'test/num_examples': 10000, 'score': 54099.629590034485, 'total_duration': 55994.96142053604, 'accumulated_submission_time': 54099.629590034485, 'accumulated_eval_time': 1885.1447412967682, 'accumulated_logging_time': 4.571201324462891}
I0129 16:58:21.666284 139866163582720 logging_writer.py:48] [160387] accumulated_eval_time=1885.144741, accumulated_logging_time=4.571201, accumulated_submission_time=54099.629590, global_step=160387, preemption_count=0, score=54099.629590, test/accuracy=0.584200, test/loss=1.883466, test/num_examples=10000, total_duration=55994.961421, train/accuracy=0.799605, train/loss=0.740035, validation/accuracy=0.706420, validation/loss=1.195777, validation/num_examples=50000
I0129 16:58:26.382956 139866171975424 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.002323627471924, loss=1.2474772930145264
I0129 16:59:00.044801 139866163582720 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.608016490936279, loss=1.2372227907180786
I0129 16:59:33.753472 139866171975424 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.397980213165283, loss=1.1724164485931396
I0129 17:00:07.418425 139866163582720 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.173322677612305, loss=1.2866617441177368
I0129 17:00:41.134312 139866171975424 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.234574317932129, loss=1.3141226768493652
I0129 17:01:14.793402 139866163582720 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.360179901123047, loss=1.246315598487854
I0129 17:01:48.507565 139866171975424 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.324820518493652, loss=1.2615501880645752
I0129 17:02:22.152174 139866163582720 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.6644134521484375, loss=1.3300755023956299
I0129 17:02:55.863297 139866171975424 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.361529350280762, loss=1.1936968564987183
I0129 17:03:29.521119 139866163582720 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.177563667297363, loss=1.2150285243988037
I0129 17:04:03.225534 139866171975424 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.515951156616211, loss=1.216041922569275
I0129 17:04:36.985968 139866163582720 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.673601150512695, loss=1.2420353889465332
I0129 17:05:10.747654 139866171975424 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.48676872253418, loss=1.269910454750061
I0129 17:05:44.412194 139866163582720 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.13798189163208, loss=1.1300559043884277
I0129 17:06:18.126763 139866171975424 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.670782089233398, loss=1.1934316158294678
I0129 17:06:51.783656 139866163582720 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.492921352386475, loss=1.1968867778778076
I0129 17:06:51.791292 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:06:58.177240 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:07:06.890512 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:07:09.479264 140027215431488 submission_runner.py:408] Time since start: 56522.82s, 	Step: 161901, 	{'train/accuracy': 0.8086734414100647, 'train/loss': 0.7023515105247498, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.1650846004486084, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.8886688947677612, 'test/num_examples': 10000, 'score': 54609.69239163399, 'total_duration': 56522.81906700134, 'accumulated_submission_time': 54609.69239163399, 'accumulated_eval_time': 1902.8326406478882, 'accumulated_logging_time': 4.625036954879761}
I0129 17:07:09.527748 139866180368128 logging_writer.py:48] [161901] accumulated_eval_time=1902.832641, accumulated_logging_time=4.625037, accumulated_submission_time=54609.692392, global_step=161901, preemption_count=0, score=54609.692392, test/accuracy=0.586600, test/loss=1.888669, test/num_examples=10000, total_duration=56522.819067, train/accuracy=0.808673, train/loss=0.702352, validation/accuracy=0.714340, validation/loss=1.165085, validation/num_examples=50000
I0129 17:07:43.216001 139866188760832 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.845637798309326, loss=1.2435789108276367
I0129 17:08:16.907463 139866180368128 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.423084259033203, loss=1.2593928575515747
I0129 17:08:50.599288 139866188760832 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.8923113346099854, loss=1.120591640472412
I0129 17:09:24.284757 139866180368128 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.600281238555908, loss=1.347485899925232
I0129 17:09:57.978930 139866188760832 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.468874454498291, loss=1.2985374927520752
I0129 17:10:31.654299 139866180368128 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.434060096740723, loss=1.199783205986023
I0129 17:11:05.423728 139866188760832 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.370822429656982, loss=1.1823512315750122
I0129 17:11:39.177150 139866180368128 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.482280254364014, loss=1.2646185159683228
I0129 17:12:12.858064 139866188760832 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.3909077644348145, loss=1.1125787496566772
I0129 17:12:46.550061 139866180368128 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.929678440093994, loss=1.2906874418258667
I0129 17:13:20.243520 139866188760832 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.774991989135742, loss=1.2133866548538208
I0129 17:13:53.913491 139866180368128 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.5586748123168945, loss=1.1489237546920776
I0129 17:14:27.596073 139866188760832 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.507168292999268, loss=1.202721118927002
I0129 17:15:01.279811 139866180368128 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.378685474395752, loss=1.1323771476745605
I0129 17:15:34.977285 139866188760832 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.715576171875, loss=1.1723062992095947
I0129 17:15:39.494925 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:15:45.701492 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:15:54.511710 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:15:57.141202 140027215431488 submission_runner.py:408] Time since start: 57050.48s, 	Step: 163415, 	{'train/accuracy': 0.8216477632522583, 'train/loss': 0.6528841257095337, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.1238961219787598, 'validation/num_examples': 50000, 'test/accuracy': 0.5954000353813171, 'test/loss': 1.8337507247924805, 'test/num_examples': 10000, 'score': 55119.59383225441, 'total_duration': 57050.48101377487, 'accumulated_submission_time': 55119.59383225441, 'accumulated_eval_time': 1920.4788768291473, 'accumulated_logging_time': 4.685438394546509}
I0129 17:15:57.186975 139865240893184 logging_writer.py:48] [163415] accumulated_eval_time=1920.478877, accumulated_logging_time=4.685438, accumulated_submission_time=55119.593832, global_step=163415, preemption_count=0, score=55119.593832, test/accuracy=0.595400, test/loss=1.833751, test/num_examples=10000, total_duration=57050.481014, train/accuracy=0.821648, train/loss=0.652884, validation/accuracy=0.721800, validation/loss=1.123896, validation/num_examples=50000
I0129 17:16:26.106977 139865760950016 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.3816022872924805, loss=1.2458223104476929
I0129 17:16:59.746728 139865240893184 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.636625289916992, loss=1.192399501800537
I0129 17:17:33.523195 139865760950016 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.377175331115723, loss=1.1688812971115112
I0129 17:18:07.245029 139865240893184 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.532809734344482, loss=1.1442009210586548
I0129 17:18:40.973039 139865760950016 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.342331886291504, loss=1.2096633911132812
I0129 17:19:14.624561 139865240893184 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.279120922088623, loss=1.114483118057251
I0129 17:19:48.340340 139865760950016 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.302928924560547, loss=1.1340715885162354
I0129 17:20:22.004625 139865240893184 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.391297817230225, loss=1.1322216987609863
I0129 17:20:55.732395 139865760950016 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.610348224639893, loss=1.182559609413147
I0129 17:21:29.393994 139865240893184 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.526160717010498, loss=1.2081234455108643
I0129 17:22:03.128228 139865760950016 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.437829971313477, loss=1.2197866439819336
I0129 17:22:36.813265 139865240893184 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.269277572631836, loss=1.1527929306030273
I0129 17:23:10.520734 139865760950016 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.85410737991333, loss=1.1328500509262085
I0129 17:23:44.173342 139865240893184 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.8977437019348145, loss=1.2031834125518799
I0129 17:24:17.939649 139865760950016 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.61737060546875, loss=1.121333360671997
I0129 17:24:27.194442 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:24:33.410271 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:24:41.991832 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:24:44.640813 140027215431488 submission_runner.py:408] Time since start: 57577.98s, 	Step: 164929, 	{'train/accuracy': 0.8226243257522583, 'train/loss': 0.6398840546607971, 'validation/accuracy': 0.7210400104522705, 'validation/loss': 1.1339006423950195, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.8490453958511353, 'test/num_examples': 10000, 'score': 55629.537470817566, 'total_duration': 57577.97980308533, 'accumulated_submission_time': 55629.537470817566, 'accumulated_eval_time': 1937.9243867397308, 'accumulated_logging_time': 4.741180896759033}
I0129 17:24:44.685476 139865232500480 logging_writer.py:48] [164929] accumulated_eval_time=1937.924387, accumulated_logging_time=4.741181, accumulated_submission_time=55629.537471, global_step=164929, preemption_count=0, score=55629.537471, test/accuracy=0.595800, test/loss=1.849045, test/num_examples=10000, total_duration=57577.979803, train/accuracy=0.822624, train/loss=0.639884, validation/accuracy=0.721040, validation/loss=1.133901, validation/num_examples=50000
I0129 17:25:08.969057 139866171975424 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.898855209350586, loss=1.1204421520233154
I0129 17:25:42.659069 139865232500480 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.9632792472839355, loss=1.1797070503234863
I0129 17:26:16.343486 139866171975424 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.607719421386719, loss=1.1191924810409546
I0129 17:26:50.029420 139865232500480 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.74708890914917, loss=1.15596342086792
I0129 17:27:23.747088 139866171975424 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.646214962005615, loss=1.1580636501312256
I0129 17:27:57.428900 139865232500480 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.150239944458008, loss=1.1231985092163086
I0129 17:28:31.128607 139866171975424 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.77630615234375, loss=1.1583175659179688
I0129 17:29:04.815162 139865232500480 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.925024032592773, loss=1.1348364353179932
I0129 17:29:38.523086 139866171975424 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.7401604652404785, loss=1.1707220077514648
I0129 17:30:12.221007 139865232500480 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.079207420349121, loss=1.0195608139038086
I0129 17:30:46.046632 139866171975424 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.7867937088012695, loss=1.0563565492630005
I0129 17:31:19.728508 139865232500480 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.39711332321167, loss=1.027761459350586
I0129 17:31:53.428907 139866171975424 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.775777339935303, loss=1.1281403303146362
I0129 17:32:27.104623 139865232500480 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.818177700042725, loss=1.109154224395752
I0129 17:33:00.812753 139866171975424 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.59238862991333, loss=1.1434454917907715
I0129 17:33:14.773369 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:33:20.987889 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:33:29.329159 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:33:31.936975 140027215431488 submission_runner.py:408] Time since start: 58105.28s, 	Step: 166443, 	{'train/accuracy': 0.8295599222183228, 'train/loss': 0.6190734505653381, 'validation/accuracy': 0.7127199769020081, 'validation/loss': 1.169293999671936, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.897260069847107, 'test/num_examples': 10000, 'score': 56139.56280255318, 'total_duration': 58105.27678442001, 'accumulated_submission_time': 56139.56280255318, 'accumulated_eval_time': 1955.0879509449005, 'accumulated_logging_time': 4.795497179031372}
I0129 17:33:31.986226 139866163582720 logging_writer.py:48] [166443] accumulated_eval_time=1955.087951, accumulated_logging_time=4.795497, accumulated_submission_time=56139.562803, global_step=166443, preemption_count=0, score=56139.562803, test/accuracy=0.583700, test/loss=1.897260, test/num_examples=10000, total_duration=58105.276784, train/accuracy=0.829560, train/loss=0.619073, validation/accuracy=0.712720, validation/loss=1.169294, validation/num_examples=50000
I0129 17:33:51.528457 139866188760832 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.339069843292236, loss=1.0684977769851685
I0129 17:34:25.254687 139866163582720 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.587430477142334, loss=1.1131911277770996
I0129 17:34:58.933880 139866188760832 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.464480876922607, loss=1.0935953855514526
I0129 17:35:32.654890 139866163582720 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.8018388748168945, loss=1.0753048658370972
I0129 17:36:06.310411 139866188760832 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.438693046569824, loss=1.0256869792938232
I0129 17:36:40.048835 139866163582720 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.711977958679199, loss=1.131585717201233
I0129 17:37:13.864427 139866188760832 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.798819541931152, loss=1.1485724449157715
I0129 17:37:47.571081 139866163582720 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.768537521362305, loss=1.119272232055664
I0129 17:38:21.255276 139866188760832 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.789620876312256, loss=1.1180921792984009
I0129 17:38:54.968786 139866163582720 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.920841217041016, loss=1.0420622825622559
I0129 17:39:28.620360 139866188760832 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.134484767913818, loss=1.111568570137024
I0129 17:40:02.337394 139866163582720 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.8369245529174805, loss=1.0752184391021729
I0129 17:40:35.984319 139866188760832 logging_writer.py:48] [167700] global_step=167700, grad_norm=5.259337425231934, loss=1.142516016960144
I0129 17:41:09.712821 139866163582720 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.674764156341553, loss=1.1418938636779785
I0129 17:41:43.367165 139866188760832 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.1394829750061035, loss=1.0601139068603516
I0129 17:42:02.064435 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:42:08.358051 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:42:16.896726 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:42:19.447271 140027215431488 submission_runner.py:408] Time since start: 58632.79s, 	Step: 167957, 	{'train/accuracy': 0.8482740521430969, 'train/loss': 0.5581567883491516, 'validation/accuracy': 0.7278199791908264, 'validation/loss': 1.1075727939605713, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.799255132675171, 'test/num_examples': 10000, 'score': 56649.58001804352, 'total_duration': 58632.78707766533, 'accumulated_submission_time': 56649.58001804352, 'accumulated_eval_time': 1972.4707593917847, 'accumulated_logging_time': 4.853919506072998}
I0129 17:42:19.494893 139865240893184 logging_writer.py:48] [167957] accumulated_eval_time=1972.470759, accumulated_logging_time=4.853920, accumulated_submission_time=56649.580018, global_step=167957, preemption_count=0, score=56649.580018, test/accuracy=0.602200, test/loss=1.799255, test/num_examples=10000, total_duration=58632.787078, train/accuracy=0.848274, train/loss=0.558157, validation/accuracy=0.727820, validation/loss=1.107573, validation/num_examples=50000
I0129 17:42:34.331432 139865760950016 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.152622222900391, loss=1.0900981426239014
I0129 17:43:08.047127 139865240893184 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.832555770874023, loss=1.0726804733276367
I0129 17:43:41.856544 139865760950016 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.9363112449646, loss=1.0413029193878174
I0129 17:44:15.546351 139865240893184 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.865512371063232, loss=1.0254522562026978
I0129 17:44:49.233551 139865760950016 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.8840532302856445, loss=1.1647248268127441
I0129 17:45:22.929142 139865240893184 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.695593357086182, loss=1.070165753364563
I0129 17:45:56.605207 139865760950016 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.844963073730469, loss=1.0736808776855469
I0129 17:46:30.292931 139865240893184 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.072441101074219, loss=1.1556514501571655
I0129 17:47:03.967007 139865760950016 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.314480781555176, loss=1.1563630104064941
I0129 17:47:37.680008 139865240893184 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.918464660644531, loss=1.0820810794830322
I0129 17:48:11.351892 139865760950016 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.606121063232422, loss=1.0899145603179932
I0129 17:48:45.064079 139865240893184 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.4443440437316895, loss=0.9553940296173096
I0129 17:49:18.727623 139865760950016 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.332595348358154, loss=1.0491197109222412
I0129 17:49:52.518872 139865240893184 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.934654235839844, loss=1.1344882249832153
I0129 17:50:26.251592 139865760950016 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.874605178833008, loss=1.1975598335266113
I0129 17:50:49.622314 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:50:56.435383 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:51:04.941369 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:51:07.479365 140027215431488 submission_runner.py:408] Time since start: 59160.82s, 	Step: 169471, 	{'train/accuracy': 0.8517617583274841, 'train/loss': 0.5366014242172241, 'validation/accuracy': 0.7337799668312073, 'validation/loss': 1.0894720554351807, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.802993655204773, 'test/num_examples': 10000, 'score': 57159.64442586899, 'total_duration': 59160.81916928291, 'accumulated_submission_time': 57159.64442586899, 'accumulated_eval_time': 1990.3277637958527, 'accumulated_logging_time': 4.91126561164856}
I0129 17:51:07.527004 139865240893184 logging_writer.py:48] [169471] accumulated_eval_time=1990.327764, accumulated_logging_time=4.911266, accumulated_submission_time=57159.644426, global_step=169471, preemption_count=0, score=57159.644426, test/accuracy=0.606500, test/loss=1.802994, test/num_examples=10000, total_duration=59160.819169, train/accuracy=0.851762, train/loss=0.536601, validation/accuracy=0.733780, validation/loss=1.089472, validation/num_examples=50000
I0129 17:51:17.633964 139865769342720 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.875782489776611, loss=1.08816659450531
I0129 17:51:51.353987 139865240893184 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.221752166748047, loss=1.1329214572906494
I0129 17:52:25.042728 139865769342720 logging_writer.py:48] [169700] global_step=169700, grad_norm=5.256372928619385, loss=1.0971118211746216
I0129 17:52:58.718372 139865240893184 logging_writer.py:48] [169800] global_step=169800, grad_norm=5.017776012420654, loss=0.988312840461731
I0129 17:53:32.444333 139865769342720 logging_writer.py:48] [169900] global_step=169900, grad_norm=5.433498382568359, loss=1.165317416191101
I0129 17:54:06.102613 139865240893184 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.898004055023193, loss=1.105411410331726
I0129 17:54:39.798215 139865769342720 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.995131492614746, loss=1.0391650199890137
I0129 17:55:13.466243 139865240893184 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.950207710266113, loss=1.0432151556015015
I0129 17:55:47.174434 139865769342720 logging_writer.py:48] [170300] global_step=170300, grad_norm=5.173139572143555, loss=1.1040507555007935
I0129 17:56:20.939370 139865240893184 logging_writer.py:48] [170400] global_step=170400, grad_norm=5.181787967681885, loss=1.0897573232650757
I0129 17:56:54.655997 139865769342720 logging_writer.py:48] [170500] global_step=170500, grad_norm=5.10360050201416, loss=1.071427822113037
I0129 17:57:28.352511 139865240893184 logging_writer.py:48] [170600] global_step=170600, grad_norm=5.13020133972168, loss=1.1120933294296265
I0129 17:58:02.057255 139865769342720 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.892124652862549, loss=1.0230185985565186
I0129 17:58:35.722127 139865240893184 logging_writer.py:48] [170800] global_step=170800, grad_norm=5.07054328918457, loss=1.0562822818756104
I0129 17:59:09.451550 139865769342720 logging_writer.py:48] [170900] global_step=170900, grad_norm=5.1012115478515625, loss=1.0003225803375244
I0129 17:59:37.547413 140027215431488 spec.py:321] Evaluating on the training split.
I0129 17:59:43.783627 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 17:59:52.651161 140027215431488 spec.py:349] Evaluating on the test split.
I0129 17:59:55.315284 140027215431488 submission_runner.py:408] Time since start: 59688.66s, 	Step: 170985, 	{'train/accuracy': 0.8520607352256775, 'train/loss': 0.5264204144477844, 'validation/accuracy': 0.7373600006103516, 'validation/loss': 1.0713787078857422, 'validation/num_examples': 50000, 'test/accuracy': 0.6118000149726868, 'test/loss': 1.7798755168914795, 'test/num_examples': 10000, 'score': 57669.60288262367, 'total_duration': 59688.65509557724, 'accumulated_submission_time': 57669.60288262367, 'accumulated_eval_time': 2008.0955998897552, 'accumulated_logging_time': 4.968322038650513}
I0129 17:59:55.367784 139865240893184 logging_writer.py:48] [170985] accumulated_eval_time=2008.095600, accumulated_logging_time=4.968322, accumulated_submission_time=57669.602883, global_step=170985, preemption_count=0, score=57669.602883, test/accuracy=0.611800, test/loss=1.779876, test/num_examples=10000, total_duration=59688.655096, train/accuracy=0.852061, train/loss=0.526420, validation/accuracy=0.737360, validation/loss=1.071379, validation/num_examples=50000
I0129 18:00:00.776051 139865760950016 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.946556091308594, loss=1.0024378299713135
I0129 18:00:34.452156 139865240893184 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.933497905731201, loss=1.0749143362045288
I0129 18:01:08.146170 139865760950016 logging_writer.py:48] [171200] global_step=171200, grad_norm=5.150162220001221, loss=1.0071959495544434
I0129 18:01:41.819882 139865240893184 logging_writer.py:48] [171300] global_step=171300, grad_norm=5.053257465362549, loss=1.1232553720474243
I0129 18:02:15.500774 139865760950016 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.883079528808594, loss=0.9282969832420349
I0129 18:02:49.311031 139865240893184 logging_writer.py:48] [171500] global_step=171500, grad_norm=5.3743577003479, loss=1.0335322618484497
I0129 18:03:23.045779 139865760950016 logging_writer.py:48] [171600] global_step=171600, grad_norm=5.3146257400512695, loss=0.9686056971549988
I0129 18:03:56.705086 139865240893184 logging_writer.py:48] [171700] global_step=171700, grad_norm=5.533942222595215, loss=1.1165674924850464
I0129 18:04:30.407042 139865760950016 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.8292365074157715, loss=0.9995289444923401
I0129 18:05:04.074471 139865240893184 logging_writer.py:48] [171900] global_step=171900, grad_norm=5.09246301651001, loss=1.0427905321121216
I0129 18:05:37.775952 139865760950016 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.969516754150391, loss=0.9720728397369385
I0129 18:06:11.460517 139865240893184 logging_writer.py:48] [172100] global_step=172100, grad_norm=5.1253838539123535, loss=1.028944969177246
I0129 18:06:45.181637 139865760950016 logging_writer.py:48] [172200] global_step=172200, grad_norm=5.383829116821289, loss=1.0886873006820679
I0129 18:07:18.867458 139865240893184 logging_writer.py:48] [172300] global_step=172300, grad_norm=5.170103073120117, loss=0.9963415265083313
I0129 18:07:52.570650 139865760950016 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.878905296325684, loss=0.9519057869911194
I0129 18:08:25.367372 140027215431488 spec.py:321] Evaluating on the training split.
I0129 18:08:31.600463 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 18:08:40.446823 140027215431488 spec.py:349] Evaluating on the test split.
I0129 18:08:43.112676 140027215431488 submission_runner.py:408] Time since start: 60216.45s, 	Step: 172499, 	{'train/accuracy': 0.8572624325752258, 'train/loss': 0.5081093311309814, 'validation/accuracy': 0.7387199997901917, 'validation/loss': 1.0654412508010864, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.7793102264404297, 'test/num_examples': 10000, 'score': 58179.54089784622, 'total_duration': 60216.452474832535, 'accumulated_submission_time': 58179.54089784622, 'accumulated_eval_time': 2025.8408544063568, 'accumulated_logging_time': 5.029789209365845}
I0129 18:08:43.158659 139865232500480 logging_writer.py:48] [172499] accumulated_eval_time=2025.840854, accumulated_logging_time=5.029789, accumulated_submission_time=58179.540898, global_step=172499, preemption_count=0, score=58179.540898, test/accuracy=0.609900, test/loss=1.779310, test/num_examples=10000, total_duration=60216.452475, train/accuracy=0.857262, train/loss=0.508109, validation/accuracy=0.738720, validation/loss=1.065441, validation/num_examples=50000
I0129 18:08:43.847882 139865240893184 logging_writer.py:48] [172500] global_step=172500, grad_norm=5.29863977432251, loss=1.0444715023040771
I0129 18:09:17.628678 139865232500480 logging_writer.py:48] [172600] global_step=172600, grad_norm=5.387392044067383, loss=0.9867818355560303
I0129 18:09:51.357660 139865240893184 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.9044928550720215, loss=1.0002466440200806
I0129 18:10:25.050617 139865232500480 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.417090892791748, loss=0.990837574005127
I0129 18:10:58.733660 139865240893184 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.733457565307617, loss=1.0327162742614746
I0129 18:11:32.452165 139865232500480 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.713975429534912, loss=0.964828372001648
I0129 18:12:06.119481 139865240893184 logging_writer.py:48] [173100] global_step=173100, grad_norm=5.5683512687683105, loss=1.0777263641357422
I0129 18:12:39.826035 139865232500480 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.985872745513916, loss=1.0305321216583252
I0129 18:13:13.500339 139865240893184 logging_writer.py:48] [173300] global_step=173300, grad_norm=5.4947099685668945, loss=1.1047943830490112
I0129 18:13:47.203585 139865232500480 logging_writer.py:48] [173400] global_step=173400, grad_norm=5.060530662536621, loss=0.9225420355796814
I0129 18:14:20.866406 139865240893184 logging_writer.py:48] [173500] global_step=173500, grad_norm=5.2163405418396, loss=1.0089900493621826
I0129 18:14:54.576706 139865232500480 logging_writer.py:48] [173600] global_step=173600, grad_norm=5.2132391929626465, loss=1.0425106287002563
I0129 18:15:28.247953 139865240893184 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.4720845222473145, loss=0.9926208257675171
I0129 18:16:02.045437 139865232500480 logging_writer.py:48] [173800] global_step=173800, grad_norm=5.326460361480713, loss=1.0852054357528687
I0129 18:16:35.722912 139865240893184 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.888787269592285, loss=0.9923529624938965
I0129 18:17:09.430228 139865232500480 logging_writer.py:48] [174000] global_step=174000, grad_norm=5.460007667541504, loss=1.0146195888519287
I0129 18:17:13.273026 140027215431488 spec.py:321] Evaluating on the training split.
I0129 18:17:19.587441 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 18:17:28.050420 140027215431488 spec.py:349] Evaluating on the test split.
I0129 18:17:30.713523 140027215431488 submission_runner.py:408] Time since start: 60744.05s, 	Step: 174013, 	{'train/accuracy': 0.8667888641357422, 'train/loss': 0.4688793122768402, 'validation/accuracy': 0.7406599521636963, 'validation/loss': 1.0524687767028809, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.7508876323699951, 'test/num_examples': 10000, 'score': 58689.59416794777, 'total_duration': 60744.05333399773, 'accumulated_submission_time': 58689.59416794777, 'accumulated_eval_time': 2043.2813086509705, 'accumulated_logging_time': 5.085204124450684}
I0129 18:17:30.763166 139865769342720 logging_writer.py:48] [174013] accumulated_eval_time=2043.281309, accumulated_logging_time=5.085204, accumulated_submission_time=58689.594168, global_step=174013, preemption_count=0, score=58689.594168, test/accuracy=0.613900, test/loss=1.750888, test/num_examples=10000, total_duration=60744.053334, train/accuracy=0.866789, train/loss=0.468879, validation/accuracy=0.740660, validation/loss=1.052469, validation/num_examples=50000
I0129 18:18:00.400191 139866163582720 logging_writer.py:48] [174100] global_step=174100, grad_norm=5.560105800628662, loss=1.0282851457595825
I0129 18:18:34.070978 139865769342720 logging_writer.py:48] [174200] global_step=174200, grad_norm=5.404099464416504, loss=1.0276373624801636
I0129 18:19:07.771429 139866163582720 logging_writer.py:48] [174300] global_step=174300, grad_norm=5.359429359436035, loss=1.0234403610229492
I0129 18:19:41.456524 139865769342720 logging_writer.py:48] [174400] global_step=174400, grad_norm=5.227977752685547, loss=1.034677267074585
I0129 18:20:15.165983 139866163582720 logging_writer.py:48] [174500] global_step=174500, grad_norm=5.46671199798584, loss=1.0543200969696045
I0129 18:20:48.841415 139865769342720 logging_writer.py:48] [174600] global_step=174600, grad_norm=5.075387954711914, loss=1.0663689374923706
I0129 18:21:22.538548 139866163582720 logging_writer.py:48] [174700] global_step=174700, grad_norm=5.442971706390381, loss=1.0583879947662354
I0129 18:21:56.184052 139865769342720 logging_writer.py:48] [174800] global_step=174800, grad_norm=5.454042911529541, loss=0.9674539566040039
I0129 18:22:30.072151 139866163582720 logging_writer.py:48] [174900] global_step=174900, grad_norm=5.616805553436279, loss=1.0260899066925049
I0129 18:23:03.743680 139865769342720 logging_writer.py:48] [175000] global_step=175000, grad_norm=5.5650715827941895, loss=1.0468422174453735
I0129 18:23:37.435523 139866163582720 logging_writer.py:48] [175100] global_step=175100, grad_norm=5.18999719619751, loss=1.0241472721099854
I0129 18:24:11.095598 139865769342720 logging_writer.py:48] [175200] global_step=175200, grad_norm=5.271298885345459, loss=1.0336345434188843
I0129 18:24:44.796265 139866163582720 logging_writer.py:48] [175300] global_step=175300, grad_norm=5.049349308013916, loss=1.0937026739120483
I0129 18:25:18.468035 139865769342720 logging_writer.py:48] [175400] global_step=175400, grad_norm=5.210881233215332, loss=0.975354790687561
I0129 18:25:52.170457 139866163582720 logging_writer.py:48] [175500] global_step=175500, grad_norm=5.7034430503845215, loss=1.0644842386245728
I0129 18:26:00.725330 140027215431488 spec.py:321] Evaluating on the training split.
I0129 18:26:06.991943 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 18:26:15.819897 140027215431488 spec.py:349] Evaluating on the test split.
I0129 18:26:18.455233 140027215431488 submission_runner.py:408] Time since start: 61271.80s, 	Step: 175527, 	{'train/accuracy': 0.8767737150192261, 'train/loss': 0.4423072636127472, 'validation/accuracy': 0.7445200085639954, 'validation/loss': 1.0392271280288696, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.7471065521240234, 'test/num_examples': 10000, 'score': 59199.493911504745, 'total_duration': 61271.79503440857, 'accumulated_submission_time': 59199.493911504745, 'accumulated_eval_time': 2061.011162519455, 'accumulated_logging_time': 5.145601987838745}
I0129 18:26:18.504501 139865232500480 logging_writer.py:48] [175527] accumulated_eval_time=2061.011163, accumulated_logging_time=5.145602, accumulated_submission_time=59199.493912, global_step=175527, preemption_count=0, score=59199.493912, test/accuracy=0.618400, test/loss=1.747107, test/num_examples=10000, total_duration=61271.795034, train/accuracy=0.876774, train/loss=0.442307, validation/accuracy=0.744520, validation/loss=1.039227, validation/num_examples=50000
I0129 18:26:43.468073 139865240893184 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.825403213500977, loss=0.9889232516288757
I0129 18:27:17.173402 139865232500480 logging_writer.py:48] [175700] global_step=175700, grad_norm=5.361218452453613, loss=1.0514346361160278
I0129 18:27:50.840259 139865240893184 logging_writer.py:48] [175800] global_step=175800, grad_norm=5.168696880340576, loss=1.0010753870010376
I0129 18:28:24.537005 139865232500480 logging_writer.py:48] [175900] global_step=175900, grad_norm=5.614009857177734, loss=1.1260206699371338
I0129 18:28:58.345088 139865240893184 logging_writer.py:48] [176000] global_step=176000, grad_norm=5.384941101074219, loss=1.0254000425338745
I0129 18:29:32.049920 139865232500480 logging_writer.py:48] [176100] global_step=176100, grad_norm=5.028197288513184, loss=0.970085620880127
I0129 18:30:05.734837 139865240893184 logging_writer.py:48] [176200] global_step=176200, grad_norm=5.243180274963379, loss=1.0290416479110718
I0129 18:30:39.450437 139865232500480 logging_writer.py:48] [176300] global_step=176300, grad_norm=5.747759819030762, loss=1.0193052291870117
I0129 18:31:13.107767 139865240893184 logging_writer.py:48] [176400] global_step=176400, grad_norm=5.0691399574279785, loss=1.0402358770370483
I0129 18:31:46.809746 139865232500480 logging_writer.py:48] [176500] global_step=176500, grad_norm=5.38934850692749, loss=1.0415219068527222
I0129 18:32:20.503472 139865240893184 logging_writer.py:48] [176600] global_step=176600, grad_norm=5.226614475250244, loss=0.9616657495498657
I0129 18:32:54.200625 139865232500480 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.980714797973633, loss=1.0145269632339478
I0129 18:33:27.892242 139865240893184 logging_writer.py:48] [176800] global_step=176800, grad_norm=5.37823486328125, loss=1.025809645652771
I0129 18:34:01.612124 139865232500480 logging_writer.py:48] [176900] global_step=176900, grad_norm=5.415241241455078, loss=1.0259692668914795
I0129 18:34:35.270516 139865240893184 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.315621852874756, loss=0.9959933757781982
I0129 18:34:48.559214 140027215431488 spec.py:321] Evaluating on the training split.
I0129 18:34:55.083048 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 18:35:03.770951 140027215431488 spec.py:349] Evaluating on the test split.
I0129 18:35:06.536760 140027215431488 submission_runner.py:408] Time since start: 61799.88s, 	Step: 177041, 	{'train/accuracy': 0.8763552308082581, 'train/loss': 0.43424472212791443, 'validation/accuracy': 0.7455599904060364, 'validation/loss': 1.0426863431930542, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.7452729940414429, 'test/num_examples': 10000, 'score': 59709.4871468544, 'total_duration': 61799.87656879425, 'accumulated_submission_time': 59709.4871468544, 'accumulated_eval_time': 2078.988668680191, 'accumulated_logging_time': 5.2041075229644775}
I0129 18:35:06.587247 139866180368128 logging_writer.py:48] [177041] accumulated_eval_time=2078.988669, accumulated_logging_time=5.204108, accumulated_submission_time=59709.487147, global_step=177041, preemption_count=0, score=59709.487147, test/accuracy=0.616100, test/loss=1.745273, test/num_examples=10000, total_duration=61799.876569, train/accuracy=0.876355, train/loss=0.434245, validation/accuracy=0.745560, validation/loss=1.042686, validation/num_examples=50000
I0129 18:35:26.739683 139866188760832 logging_writer.py:48] [177100] global_step=177100, grad_norm=5.338843822479248, loss=1.0449014902114868
I0129 18:36:00.459089 139866180368128 logging_writer.py:48] [177200] global_step=177200, grad_norm=5.595066070556641, loss=0.920616626739502
I0129 18:36:34.130687 139866188760832 logging_writer.py:48] [177300] global_step=177300, grad_norm=5.2708740234375, loss=0.9848467707633972
I0129 18:37:07.843295 139866180368128 logging_writer.py:48] [177400] global_step=177400, grad_norm=5.1467413902282715, loss=0.9212152361869812
I0129 18:37:41.518385 139866188760832 logging_writer.py:48] [177500] global_step=177500, grad_norm=5.83482551574707, loss=1.081820011138916
I0129 18:38:15.232376 139866180368128 logging_writer.py:48] [177600] global_step=177600, grad_norm=5.161474704742432, loss=0.924753725528717
I0129 18:38:48.886779 139866188760832 logging_writer.py:48] [177700] global_step=177700, grad_norm=5.3100361824035645, loss=1.0305418968200684
I0129 18:39:22.581174 139866180368128 logging_writer.py:48] [177800] global_step=177800, grad_norm=5.56395959854126, loss=0.975214958190918
I0129 18:39:56.245823 139866188760832 logging_writer.py:48] [177900] global_step=177900, grad_norm=5.210643768310547, loss=1.0720837116241455
I0129 18:40:29.961673 139866180368128 logging_writer.py:48] [178000] global_step=178000, grad_norm=5.88263463973999, loss=0.9944227337837219
I0129 18:41:03.623769 139866188760832 logging_writer.py:48] [178100] global_step=178100, grad_norm=5.406683921813965, loss=1.015613317489624
I0129 18:41:37.361212 139866180368128 logging_writer.py:48] [178200] global_step=178200, grad_norm=5.42226505279541, loss=0.9950417280197144
I0129 18:42:11.016000 139866188760832 logging_writer.py:48] [178300] global_step=178300, grad_norm=5.303549289703369, loss=0.9800297021865845
I0129 18:42:44.726322 139866180368128 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.991267204284668, loss=0.8635880947113037
I0129 18:43:18.386345 139866188760832 logging_writer.py:48] [178500] global_step=178500, grad_norm=5.354985237121582, loss=0.9476960897445679
I0129 18:43:36.732107 140027215431488 spec.py:321] Evaluating on the training split.
I0129 18:43:42.994038 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 18:43:51.812718 140027215431488 spec.py:349] Evaluating on the test split.
I0129 18:43:54.418474 140027215431488 submission_runner.py:408] Time since start: 62327.76s, 	Step: 178556, 	{'train/accuracy': 0.8790258169174194, 'train/loss': 0.43027791380882263, 'validation/accuracy': 0.7473999857902527, 'validation/loss': 1.0301430225372314, 'validation/num_examples': 50000, 'test/accuracy': 0.6222000122070312, 'test/loss': 1.731208324432373, 'test/num_examples': 10000, 'score': 60219.56933808327, 'total_duration': 62327.75828695297, 'accumulated_submission_time': 60219.56933808327, 'accumulated_eval_time': 2096.6750016212463, 'accumulated_logging_time': 5.264262676239014}
I0129 18:43:54.470214 139865240893184 logging_writer.py:48] [178556] accumulated_eval_time=2096.675002, accumulated_logging_time=5.264263, accumulated_submission_time=60219.569338, global_step=178556, preemption_count=0, score=60219.569338, test/accuracy=0.622200, test/loss=1.731208, test/num_examples=10000, total_duration=62327.758287, train/accuracy=0.879026, train/loss=0.430278, validation/accuracy=0.747400, validation/loss=1.030143, validation/num_examples=50000
I0129 18:44:09.673849 139865760950016 logging_writer.py:48] [178600] global_step=178600, grad_norm=5.5884623527526855, loss=1.063660979270935
I0129 18:44:43.363474 139865240893184 logging_writer.py:48] [178700] global_step=178700, grad_norm=5.447368621826172, loss=0.9627907872200012
I0129 18:45:17.058575 139865760950016 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.608829975128174, loss=0.9053508043289185
I0129 18:45:50.745989 139865240893184 logging_writer.py:48] [178900] global_step=178900, grad_norm=5.49483585357666, loss=0.9735155701637268
I0129 18:46:24.407359 139865760950016 logging_writer.py:48] [179000] global_step=179000, grad_norm=5.197758674621582, loss=0.9228299856185913
I0129 18:46:58.093339 139865240893184 logging_writer.py:48] [179100] global_step=179100, grad_norm=5.394907474517822, loss=1.026921272277832
I0129 18:47:31.770351 139865760950016 logging_writer.py:48] [179200] global_step=179200, grad_norm=5.447402000427246, loss=0.9288368821144104
I0129 18:48:05.497092 139865240893184 logging_writer.py:48] [179300] global_step=179300, grad_norm=5.449533462524414, loss=0.9383844137191772
I0129 18:48:39.216042 139865760950016 logging_writer.py:48] [179400] global_step=179400, grad_norm=5.335552215576172, loss=0.9130721688270569
I0129 18:49:12.893097 139865240893184 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.961535930633545, loss=0.9404491186141968
I0129 18:49:46.576085 139865760950016 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.302426815032959, loss=0.9484786987304688
I0129 18:50:20.255808 139865240893184 logging_writer.py:48] [179700] global_step=179700, grad_norm=5.06969690322876, loss=0.9400731921195984
I0129 18:50:53.945641 139865760950016 logging_writer.py:48] [179800] global_step=179800, grad_norm=5.077901363372803, loss=0.9302562475204468
I0129 18:51:27.634696 139865240893184 logging_writer.py:48] [179900] global_step=179900, grad_norm=5.412064552307129, loss=0.90463787317276
I0129 18:52:01.315493 139865760950016 logging_writer.py:48] [180000] global_step=180000, grad_norm=5.387949466705322, loss=0.8877163529396057
I0129 18:52:24.689469 140027215431488 spec.py:321] Evaluating on the training split.
I0129 18:52:31.080160 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 18:52:39.678902 140027215431488 spec.py:349] Evaluating on the test split.
I0129 18:52:42.296333 140027215431488 submission_runner.py:408] Time since start: 62855.64s, 	Step: 180071, 	{'train/accuracy': 0.878348171710968, 'train/loss': 0.4257912039756775, 'validation/accuracy': 0.7497199773788452, 'validation/loss': 1.0273454189300537, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.7289284467697144, 'test/num_examples': 10000, 'score': 60729.72563242912, 'total_duration': 62855.63610816002, 'accumulated_submission_time': 60729.72563242912, 'accumulated_eval_time': 2114.281795501709, 'accumulated_logging_time': 5.3266565799713135}
I0129 18:52:42.349909 139865232500480 logging_writer.py:48] [180071] accumulated_eval_time=2114.281796, accumulated_logging_time=5.326657, accumulated_submission_time=60729.725632, global_step=180071, preemption_count=0, score=60729.725632, test/accuracy=0.624900, test/loss=1.728928, test/num_examples=10000, total_duration=62855.636108, train/accuracy=0.878348, train/loss=0.425791, validation/accuracy=0.749720, validation/loss=1.027345, validation/num_examples=50000
I0129 18:52:52.465343 139866171975424 logging_writer.py:48] [180100] global_step=180100, grad_norm=5.333600997924805, loss=0.9799452424049377
I0129 18:53:26.137975 139865232500480 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.396508693695068, loss=0.8872775435447693
I0129 18:53:59.868913 139866171975424 logging_writer.py:48] [180300] global_step=180300, grad_norm=5.645814418792725, loss=0.998335599899292
I0129 18:54:33.669732 139865232500480 logging_writer.py:48] [180400] global_step=180400, grad_norm=5.724287033081055, loss=1.0445401668548584
I0129 18:55:07.393952 139866171975424 logging_writer.py:48] [180500] global_step=180500, grad_norm=5.111424446105957, loss=0.9383082389831543
I0129 18:55:41.055390 139865232500480 logging_writer.py:48] [180600] global_step=180600, grad_norm=5.375144958496094, loss=0.9066442251205444
I0129 18:56:14.769283 139866171975424 logging_writer.py:48] [180700] global_step=180700, grad_norm=5.532199382781982, loss=1.0002086162567139
I0129 18:56:48.424198 139865232500480 logging_writer.py:48] [180800] global_step=180800, grad_norm=5.270418167114258, loss=0.9943610429763794
I0129 18:57:22.135474 139866171975424 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.775655746459961, loss=0.8730765581130981
I0129 18:57:55.787140 139865232500480 logging_writer.py:48] [181000] global_step=181000, grad_norm=5.126616477966309, loss=0.9339792132377625
I0129 18:58:29.502028 139866171975424 logging_writer.py:48] [181100] global_step=181100, grad_norm=5.675051689147949, loss=0.9796642065048218
I0129 18:59:03.167283 139865232500480 logging_writer.py:48] [181200] global_step=181200, grad_norm=5.347969055175781, loss=0.9662564992904663
I0129 18:59:36.882186 139866171975424 logging_writer.py:48] [181300] global_step=181300, grad_norm=5.4503068923950195, loss=0.8740125894546509
I0129 19:00:10.536058 139865232500480 logging_writer.py:48] [181400] global_step=181400, grad_norm=5.358700275421143, loss=0.9759199023246765
I0129 19:00:44.321823 139866171975424 logging_writer.py:48] [181500] global_step=181500, grad_norm=5.296700477600098, loss=0.8602308034896851
I0129 19:01:12.468027 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:01:18.698812 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:01:27.327871 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:01:30.002712 140027215431488 submission_runner.py:408] Time since start: 63383.34s, 	Step: 181585, 	{'train/accuracy': 0.8835299611091614, 'train/loss': 0.412623792886734, 'validation/accuracy': 0.750220000743866, 'validation/loss': 1.02522611618042, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.7295113801956177, 'test/num_examples': 10000, 'score': 61239.78214740753, 'total_duration': 63383.342509269714, 'accumulated_submission_time': 61239.78214740753, 'accumulated_eval_time': 2131.8164291381836, 'accumulated_logging_time': 5.389420032501221}
I0129 19:01:30.053307 139865760950016 logging_writer.py:48] [181585] accumulated_eval_time=2131.816429, accumulated_logging_time=5.389420, accumulated_submission_time=61239.782147, global_step=181585, preemption_count=0, score=61239.782147, test/accuracy=0.624900, test/loss=1.729511, test/num_examples=10000, total_duration=63383.342509, train/accuracy=0.883530, train/loss=0.412624, validation/accuracy=0.750220, validation/loss=1.025226, validation/num_examples=50000
I0129 19:01:35.440757 139865769342720 logging_writer.py:48] [181600] global_step=181600, grad_norm=5.8498077392578125, loss=0.9988376498222351
I0129 19:02:09.142396 139865760950016 logging_writer.py:48] [181700] global_step=181700, grad_norm=5.460897922515869, loss=0.9652774930000305
I0129 19:02:42.869155 139865769342720 logging_writer.py:48] [181800] global_step=181800, grad_norm=6.024317741394043, loss=1.0562912225723267
I0129 19:03:16.546893 139865760950016 logging_writer.py:48] [181900] global_step=181900, grad_norm=5.391739845275879, loss=1.0118762254714966
I0129 19:03:50.250094 139865769342720 logging_writer.py:48] [182000] global_step=182000, grad_norm=6.057145118713379, loss=0.9685839414596558
I0129 19:04:23.938289 139865760950016 logging_writer.py:48] [182100] global_step=182100, grad_norm=5.442118167877197, loss=0.981982409954071
I0129 19:04:57.646522 139865769342720 logging_writer.py:48] [182200] global_step=182200, grad_norm=5.756351947784424, loss=1.018435001373291
I0129 19:05:31.313586 139865760950016 logging_writer.py:48] [182300] global_step=182300, grad_norm=5.112840175628662, loss=0.9501465559005737
I0129 19:06:04.998332 139865769342720 logging_writer.py:48] [182400] global_step=182400, grad_norm=5.784926414489746, loss=1.0094820261001587
I0129 19:06:38.669558 139865760950016 logging_writer.py:48] [182500] global_step=182500, grad_norm=5.045401096343994, loss=0.9403886795043945
I0129 19:07:12.346429 139865769342720 logging_writer.py:48] [182600] global_step=182600, grad_norm=5.093271732330322, loss=1.0014548301696777
I0129 19:07:46.148863 139865760950016 logging_writer.py:48] [182700] global_step=182700, grad_norm=5.500417709350586, loss=0.9997559785842896
I0129 19:08:19.862874 139865769342720 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.776628017425537, loss=0.9417396187782288
I0129 19:08:53.541617 139865760950016 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.8683600425720215, loss=0.9529893398284912
I0129 19:09:27.228294 139865769342720 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.454142093658447, loss=1.0314942598342896
I0129 19:10:00.052291 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:10:06.305839 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:10:14.965073 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:10:17.626216 140027215431488 submission_runner.py:408] Time since start: 63910.97s, 	Step: 183099, 	{'train/accuracy': 0.8856026530265808, 'train/loss': 0.40698984265327454, 'validation/accuracy': 0.7508999705314636, 'validation/loss': 1.0227185487747192, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.7207680940628052, 'test/num_examples': 10000, 'score': 61749.71807217598, 'total_duration': 63910.96601963043, 'accumulated_submission_time': 61749.71807217598, 'accumulated_eval_time': 2149.3903136253357, 'accumulated_logging_time': 5.450626850128174}
I0129 19:10:17.674166 139866180368128 logging_writer.py:48] [183099] accumulated_eval_time=2149.390314, accumulated_logging_time=5.450627, accumulated_submission_time=61749.718072, global_step=183099, preemption_count=0, score=61749.718072, test/accuracy=0.625200, test/loss=1.720768, test/num_examples=10000, total_duration=63910.966020, train/accuracy=0.885603, train/loss=0.406990, validation/accuracy=0.750900, validation/loss=1.022719, validation/num_examples=50000
I0129 19:10:18.355923 139866188760832 logging_writer.py:48] [183100] global_step=183100, grad_norm=5.281952381134033, loss=0.9589397311210632
I0129 19:10:52.058853 139866180368128 logging_writer.py:48] [183200] global_step=183200, grad_norm=5.157142639160156, loss=0.8982621431350708
I0129 19:11:25.721372 139866188760832 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.714991569519043, loss=1.0061416625976562
I0129 19:11:59.434289 139866180368128 logging_writer.py:48] [183400] global_step=183400, grad_norm=5.636926651000977, loss=0.97940993309021
I0129 19:12:33.093564 139866188760832 logging_writer.py:48] [183500] global_step=183500, grad_norm=5.068763256072998, loss=0.9276409149169922
I0129 19:13:06.816083 139866180368128 logging_writer.py:48] [183600] global_step=183600, grad_norm=5.708119869232178, loss=1.0797492265701294
I0129 19:13:40.492347 139866188760832 logging_writer.py:48] [183700] global_step=183700, grad_norm=5.18794584274292, loss=0.860923707485199
I0129 19:14:14.233472 139866180368128 logging_writer.py:48] [183800] global_step=183800, grad_norm=5.513592720031738, loss=0.9456264972686768
I0129 19:14:47.951276 139866188760832 logging_writer.py:48] [183900] global_step=183900, grad_norm=5.3108439445495605, loss=0.966810405254364
I0129 19:15:21.671600 139866180368128 logging_writer.py:48] [184000] global_step=184000, grad_norm=5.201453685760498, loss=1.0052385330200195
I0129 19:15:55.350796 139866188760832 logging_writer.py:48] [184100] global_step=184100, grad_norm=5.678016662597656, loss=1.0068674087524414
I0129 19:16:29.072441 139866180368128 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.323996543884277, loss=0.9841039180755615
I0129 19:17:02.711828 139866188760832 logging_writer.py:48] [184300] global_step=184300, grad_norm=5.383182048797607, loss=0.9482177495956421
I0129 19:17:36.430043 139866180368128 logging_writer.py:48] [184400] global_step=184400, grad_norm=5.526147365570068, loss=0.9841184616088867
I0129 19:18:10.090891 139866188760832 logging_writer.py:48] [184500] global_step=184500, grad_norm=5.4791259765625, loss=1.0090162754058838
I0129 19:18:43.805790 139866180368128 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.9770827293396, loss=0.8799008727073669
I0129 19:18:47.661810 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:18:53.997828 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:19:02.646471 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:19:05.306236 140027215431488 submission_runner.py:408] Time since start: 64438.65s, 	Step: 184613, 	{'train/accuracy': 0.8846459984779358, 'train/loss': 0.4113345444202423, 'validation/accuracy': 0.7509999871253967, 'validation/loss': 1.0213669538497925, 'validation/num_examples': 50000, 'test/accuracy': 0.625, 'test/loss': 1.724463701248169, 'test/num_examples': 10000, 'score': 62259.6433134079, 'total_duration': 64438.64605140686, 'accumulated_submission_time': 62259.6433134079, 'accumulated_eval_time': 2167.0347170829773, 'accumulated_logging_time': 5.507994651794434}
I0129 19:19:05.355780 139865240893184 logging_writer.py:48] [184613] accumulated_eval_time=2167.034717, accumulated_logging_time=5.507995, accumulated_submission_time=62259.643313, global_step=184613, preemption_count=0, score=62259.643313, test/accuracy=0.625000, test/loss=1.724464, test/num_examples=10000, total_duration=64438.646051, train/accuracy=0.884646, train/loss=0.411335, validation/accuracy=0.751000, validation/loss=1.021367, validation/num_examples=50000
I0129 19:19:34.963658 139865760950016 logging_writer.py:48] [184700] global_step=184700, grad_norm=5.671864986419678, loss=0.9546581506729126
I0129 19:20:08.747216 139865240893184 logging_writer.py:48] [184800] global_step=184800, grad_norm=5.3895978927612305, loss=0.8899372220039368
I0129 19:20:42.522991 139865760950016 logging_writer.py:48] [184900] global_step=184900, grad_norm=5.3361711502075195, loss=0.9419025778770447
I0129 19:21:16.193182 139865240893184 logging_writer.py:48] [185000] global_step=185000, grad_norm=5.236453533172607, loss=0.9687122106552124
I0129 19:21:49.893957 139865760950016 logging_writer.py:48] [185100] global_step=185100, grad_norm=5.164865970611572, loss=0.9426741600036621
I0129 19:22:23.573145 139865240893184 logging_writer.py:48] [185200] global_step=185200, grad_norm=5.686863422393799, loss=0.9609963297843933
I0129 19:22:57.287031 139865760950016 logging_writer.py:48] [185300] global_step=185300, grad_norm=5.269199848175049, loss=0.9302642345428467
I0129 19:23:30.947575 139865240893184 logging_writer.py:48] [185400] global_step=185400, grad_norm=5.138663291931152, loss=0.9224807620048523
I0129 19:24:04.670393 139865760950016 logging_writer.py:48] [185500] global_step=185500, grad_norm=5.30010461807251, loss=0.9267922639846802
I0129 19:24:38.360658 139865240893184 logging_writer.py:48] [185600] global_step=185600, grad_norm=5.558737754821777, loss=1.0242112874984741
I0129 19:25:12.069575 139865760950016 logging_writer.py:48] [185700] global_step=185700, grad_norm=5.15366268157959, loss=0.9134671688079834
I0129 19:25:45.741832 139865240893184 logging_writer.py:48] [185800] global_step=185800, grad_norm=5.320826053619385, loss=0.9495917558670044
I0129 19:26:19.451455 139865760950016 logging_writer.py:48] [185900] global_step=185900, grad_norm=5.489009857177734, loss=1.0021388530731201
I0129 19:26:53.262529 139865240893184 logging_writer.py:48] [186000] global_step=186000, grad_norm=5.16766357421875, loss=0.9677553176879883
I0129 19:27:26.968328 139865760950016 logging_writer.py:48] [186100] global_step=186100, grad_norm=5.661991596221924, loss=0.9862358570098877
I0129 19:27:35.538113 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:27:41.922232 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:27:50.413296 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:27:53.031838 140027215431488 submission_runner.py:408] Time since start: 64966.37s, 	Step: 186127, 	{'train/accuracy': 0.8859614133834839, 'train/loss': 0.4078052341938019, 'validation/accuracy': 0.7506799697875977, 'validation/loss': 1.0217421054840088, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.7228504419326782, 'test/num_examples': 10000, 'score': 62769.762776851654, 'total_duration': 64966.37164711952, 'accumulated_submission_time': 62769.762776851654, 'accumulated_eval_time': 2184.528416633606, 'accumulated_logging_time': 5.566797494888306}
I0129 19:27:53.081617 139865224107776 logging_writer.py:48] [186127] accumulated_eval_time=2184.528417, accumulated_logging_time=5.566797, accumulated_submission_time=62769.762777, global_step=186127, preemption_count=0, score=62769.762777, test/accuracy=0.624300, test/loss=1.722850, test/num_examples=10000, total_duration=64966.371647, train/accuracy=0.885961, train/loss=0.407805, validation/accuracy=0.750680, validation/loss=1.021742, validation/num_examples=50000
I0129 19:28:18.051385 139865232500480 logging_writer.py:48] [186200] global_step=186200, grad_norm=5.097018241882324, loss=0.8434276580810547
I0129 19:28:51.765825 139865224107776 logging_writer.py:48] [186300] global_step=186300, grad_norm=5.433328628540039, loss=0.9781067371368408
I0129 19:29:25.433018 139865232500480 logging_writer.py:48] [186400] global_step=186400, grad_norm=5.5240254402160645, loss=0.9645751714706421
I0129 19:29:59.159278 139865224107776 logging_writer.py:48] [186500] global_step=186500, grad_norm=5.312243461608887, loss=0.9238647818565369
I0129 19:30:32.826940 139865232500480 logging_writer.py:48] [186600] global_step=186600, grad_norm=5.396263599395752, loss=0.9574086666107178
I0129 19:30:54.556503 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:31:00.867459 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:31:09.840325 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:31:12.408396 140027215431488 submission_runner.py:408] Time since start: 65165.75s, 	Step: 186666, 	{'train/accuracy': 0.8869180083274841, 'train/loss': 0.40388861298561096, 'validation/accuracy': 0.7511199712753296, 'validation/loss': 1.0213629007339478, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.722780704498291, 'test/num_examples': 10000, 'score': 62951.208920001984, 'total_duration': 65165.74819970131, 'accumulated_submission_time': 62951.208920001984, 'accumulated_eval_time': 2202.380264520645, 'accumulated_logging_time': 5.6263251304626465}
I0129 19:31:12.466876 139865240893184 logging_writer.py:48] [186666] accumulated_eval_time=2202.380265, accumulated_logging_time=5.626325, accumulated_submission_time=62951.208920, global_step=186666, preemption_count=0, score=62951.208920, test/accuracy=0.623700, test/loss=1.722781, test/num_examples=10000, total_duration=65165.748200, train/accuracy=0.886918, train/loss=0.403889, validation/accuracy=0.751120, validation/loss=1.021363, validation/num_examples=50000
I0129 19:31:12.512540 139865760950016 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62951.208920
I0129 19:31:12.871411 140027215431488 checkpoints.py:490] Saving checkpoint at step: 186666
I0129 19:31:14.049629 140027215431488 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4/checkpoint_186666
I0129 19:31:14.070306 140027215431488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_4/checkpoint_186666.
I0129 19:31:14.805156 140027215431488 submission_runner.py:583] Tuning trial 4/5
I0129 19:31:14.805379 140027215431488 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0129 19:31:14.814613 140027215431488 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007772640092298388, 'train/loss': 6.911434173583984, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 33.78456735610962, 'total_duration': 51.374412059783936, 'accumulated_submission_time': 33.78456735610962, 'accumulated_eval_time': 17.589736700057983, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1500, {'train/accuracy': 0.22211813926696777, 'train/loss': 3.8356406688690186, 'validation/accuracy': 0.2069999873638153, 'validation/loss': 3.960334539413452, 'validation/num_examples': 50000, 'test/accuracy': 0.15680000185966492, 'test/loss': 4.447728633880615, 'test/num_examples': 10000, 'score': 543.8763875961304, 'total_duration': 579.2266387939453, 'accumulated_submission_time': 543.8763875961304, 'accumulated_eval_time': 35.278390645980835, 'accumulated_logging_time': 0.019817829132080078, 'global_step': 1500, 'preemption_count': 0}), (3001, {'train/accuracy': 0.33910635113716125, 'train/loss': 3.0905704498291016, 'validation/accuracy': 0.3193399906158447, 'validation/loss': 3.2351529598236084, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 3.8936009407043457, 'test/num_examples': 10000, 'score': 1054.0921666622162, 'total_duration': 1107.459528684616, 'accumulated_submission_time': 1054.0921666622162, 'accumulated_eval_time': 53.212013721466064, 'accumulated_logging_time': 0.05116891860961914, 'global_step': 3001, 'preemption_count': 0}), (4504, {'train/accuracy': 0.34586256742477417, 'train/loss': 3.1005280017852783, 'validation/accuracy': 0.3310000002384186, 'validation/loss': 3.232429265975952, 'validation/num_examples': 50000, 'test/accuracy': 0.24320000410079956, 'test/loss': 4.029863357543945, 'test/num_examples': 10000, 'score': 1564.1626377105713, 'total_duration': 1635.3940970897675, 'accumulated_submission_time': 1564.1626377105713, 'accumulated_eval_time': 70.99523687362671, 'accumulated_logging_time': 0.08113479614257812, 'global_step': 4504, 'preemption_count': 0}), (6008, {'train/accuracy': 0.37824854254722595, 'train/loss': 2.8953018188476562, 'validation/accuracy': 0.33563998341560364, 'validation/loss': 3.1820812225341797, 'validation/num_examples': 50000, 'test/accuracy': 0.2597000002861023, 'test/loss': 3.796529531478882, 'test/num_examples': 10000, 'score': 2074.3404870033264, 'total_duration': 2163.361767053604, 'accumulated_submission_time': 2074.3404870033264, 'accumulated_eval_time': 88.70134115219116, 'accumulated_logging_time': 0.11190533638000488, 'global_step': 6008, 'preemption_count': 0}), (7513, {'train/accuracy': 0.2871890962123871, 'train/loss': 3.6383469104766846, 'validation/accuracy': 0.26170000433921814, 'validation/loss': 3.8255012035369873, 'validation/num_examples': 50000, 'test/accuracy': 0.18610000610351562, 'test/loss': 4.576222896575928, 'test/num_examples': 10000, 'score': 2584.4371135234833, 'total_duration': 2691.138946533203, 'accumulated_submission_time': 2584.4371135234833, 'accumulated_eval_time': 106.30011343955994, 'accumulated_logging_time': 0.1416637897491455, 'global_step': 7513, 'preemption_count': 0}), (9020, {'train/accuracy': 0.39819833636283875, 'train/loss': 2.802988052368164, 'validation/accuracy': 0.36583998799324036, 'validation/loss': 3.020524501800537, 'validation/num_examples': 50000, 'test/accuracy': 0.27470001578330994, 'test/loss': 3.7215497493743896, 'test/num_examples': 10000, 'score': 3094.679023504257, 'total_duration': 3219.2161860466003, 'accumulated_submission_time': 3094.679023504257, 'accumulated_eval_time': 124.0553548336029, 'accumulated_logging_time': 0.16915607452392578, 'global_step': 9020, 'preemption_count': 0}), (10527, {'train/accuracy': 0.3869379758834839, 'train/loss': 2.846888542175293, 'validation/accuracy': 0.36423999071121216, 'validation/loss': 3.0101521015167236, 'validation/num_examples': 50000, 'test/accuracy': 0.26900002360343933, 'test/loss': 3.7982981204986572, 'test/num_examples': 10000, 'score': 3604.599910259247, 'total_duration': 3747.6188309192657, 'accumulated_submission_time': 3604.599910259247, 'accumulated_eval_time': 142.4556610584259, 'accumulated_logging_time': 0.19795799255371094, 'global_step': 10527, 'preemption_count': 0}), (12035, {'train/accuracy': 0.30215638875961304, 'train/loss': 3.548335313796997, 'validation/accuracy': 0.2874999940395355, 'validation/loss': 3.7057454586029053, 'validation/num_examples': 50000, 'test/accuracy': 0.20580001175403595, 'test/loss': 4.551807880401611, 'test/num_examples': 10000, 'score': 4114.688268184662, 'total_duration': 4275.333572149277, 'accumulated_submission_time': 4114.688268184662, 'accumulated_eval_time': 160.00274682044983, 'accumulated_logging_time': 0.2236948013305664, 'global_step': 12035, 'preemption_count': 0}), (13543, {'train/accuracy': 0.21027980744838715, 'train/loss': 4.674249649047852, 'validation/accuracy': 0.1959799975156784, 'validation/loss': 4.8388237953186035, 'validation/num_examples': 50000, 'test/accuracy': 0.15770000219345093, 'test/loss': 5.386991500854492, 'test/num_examples': 10000, 'score': 4624.764115333557, 'total_duration': 4803.130608320236, 'accumulated_submission_time': 4624.764115333557, 'accumulated_eval_time': 177.63978910446167, 'accumulated_logging_time': 0.25537919998168945, 'global_step': 13543, 'preemption_count': 0}), (15052, {'train/accuracy': 0.23541134595870972, 'train/loss': 4.341064929962158, 'validation/accuracy': 0.21069999039173126, 'validation/loss': 4.6160736083984375, 'validation/num_examples': 50000, 'test/accuracy': 0.15610000491142273, 'test/loss': 5.416417598724365, 'test/num_examples': 10000, 'score': 5134.7330057621, 'total_duration': 5331.500044822693, 'accumulated_submission_time': 5134.7330057621, 'accumulated_eval_time': 195.9572970867157, 'accumulated_logging_time': 0.2865011692047119, 'global_step': 15052, 'preemption_count': 0}), (16562, {'train/accuracy': 0.23078761994838715, 'train/loss': 4.050477504730225, 'validation/accuracy': 0.2169799953699112, 'validation/loss': 4.200684070587158, 'validation/num_examples': 50000, 'test/accuracy': 0.15130001306533813, 'test/loss': 4.930394649505615, 'test/num_examples': 10000, 'score': 5644.699779033661, 'total_duration': 5859.146992444992, 'accumulated_submission_time': 5644.699779033661, 'accumulated_eval_time': 213.5535204410553, 'accumulated_logging_time': 0.3168478012084961, 'global_step': 16562, 'preemption_count': 0}), (18073, {'train/accuracy': 0.3052654564380646, 'train/loss': 3.361721992492676, 'validation/accuracy': 0.28553998470306396, 'validation/loss': 3.5417702198028564, 'validation/num_examples': 50000, 'test/accuracy': 0.20990000665187836, 'test/loss': 4.276243209838867, 'test/num_examples': 10000, 'score': 6154.821240901947, 'total_duration': 6387.498101949692, 'accumulated_submission_time': 6154.821240901947, 'accumulated_eval_time': 231.69837379455566, 'accumulated_logging_time': 0.34916210174560547, 'global_step': 18073, 'preemption_count': 0}), (19584, {'train/accuracy': 0.24420040845870972, 'train/loss': 3.913287878036499, 'validation/accuracy': 0.230119988322258, 'validation/loss': 4.02189826965332, 'validation/num_examples': 50000, 'test/accuracy': 0.17020000517368317, 'test/loss': 4.693854331970215, 'test/num_examples': 10000, 'score': 6664.954342365265, 'total_duration': 6915.529188632965, 'accumulated_submission_time': 6664.954342365265, 'accumulated_eval_time': 249.51444363594055, 'accumulated_logging_time': 0.3746833801269531, 'global_step': 19584, 'preemption_count': 0}), (21096, {'train/accuracy': 0.24226722121238708, 'train/loss': 3.8922784328460693, 'validation/accuracy': 0.2260199934244156, 'validation/loss': 4.0060505867004395, 'validation/num_examples': 50000, 'test/accuracy': 0.16990001499652863, 'test/loss': 4.684741973876953, 'test/num_examples': 10000, 'score': 7175.1495196819305, 'total_duration': 7443.557811498642, 'accumulated_submission_time': 7175.1495196819305, 'accumulated_eval_time': 267.2611298561096, 'accumulated_logging_time': 0.4081254005432129, 'global_step': 21096, 'preemption_count': 0}), (22609, {'train/accuracy': 0.21225287020206451, 'train/loss': 4.563592433929443, 'validation/accuracy': 0.20136000216007233, 'validation/loss': 4.659169673919678, 'validation/num_examples': 50000, 'test/accuracy': 0.15480001270771027, 'test/loss': 5.273361682891846, 'test/num_examples': 10000, 'score': 7685.252676725388, 'total_duration': 7971.4506142139435, 'accumulated_submission_time': 7685.252676725388, 'accumulated_eval_time': 284.9658041000366, 'accumulated_logging_time': 0.439424991607666, 'global_step': 22609, 'preemption_count': 0}), (24121, {'train/accuracy': 0.24152980744838715, 'train/loss': 4.111358165740967, 'validation/accuracy': 0.2184399962425232, 'validation/loss': 4.350162506103516, 'validation/num_examples': 50000, 'test/accuracy': 0.1574000120162964, 'test/loss': 5.048336029052734, 'test/num_examples': 10000, 'score': 8195.383947610855, 'total_duration': 8499.119955062866, 'accumulated_submission_time': 8195.383947610855, 'accumulated_eval_time': 302.4214491844177, 'accumulated_logging_time': 0.47034239768981934, 'global_step': 24121, 'preemption_count': 0}), (25634, {'train/accuracy': 0.2726801633834839, 'train/loss': 3.7686870098114014, 'validation/accuracy': 0.2542800009250641, 'validation/loss': 3.9192233085632324, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.636255741119385, 'test/num_examples': 10000, 'score': 8705.445008277893, 'total_duration': 9027.102459907532, 'accumulated_submission_time': 8705.445008277893, 'accumulated_eval_time': 320.2576837539673, 'accumulated_logging_time': 0.502802848815918, 'global_step': 25634, 'preemption_count': 0}), (27146, {'train/accuracy': 0.21406647562980652, 'train/loss': 4.525050640106201, 'validation/accuracy': 0.20149999856948853, 'validation/loss': 4.717775344848633, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 5.444266319274902, 'test/num_examples': 10000, 'score': 9215.368250131607, 'total_duration': 9554.7049472332, 'accumulated_submission_time': 9215.368250131607, 'accumulated_eval_time': 337.85802817344666, 'accumulated_logging_time': 0.5298073291778564, 'global_step': 27146, 'preemption_count': 0}), (28659, {'train/accuracy': 0.1899513602256775, 'train/loss': 4.6598968505859375, 'validation/accuracy': 0.1812800019979477, 'validation/loss': 4.777307510375977, 'validation/num_examples': 50000, 'test/accuracy': 0.13740000128746033, 'test/loss': 5.371987819671631, 'test/num_examples': 10000, 'score': 9725.415110111237, 'total_duration': 10082.664724826813, 'accumulated_submission_time': 9725.415110111237, 'accumulated_eval_time': 355.68455719947815, 'accumulated_logging_time': 0.5628311634063721, 'global_step': 28659, 'preemption_count': 0}), (30172, {'train/accuracy': 0.19816246628761292, 'train/loss': 4.559650897979736, 'validation/accuracy': 0.19246000051498413, 'validation/loss': 4.613406181335449, 'validation/num_examples': 50000, 'test/accuracy': 0.13440001010894775, 'test/loss': 5.338951110839844, 'test/num_examples': 10000, 'score': 10235.613961458206, 'total_duration': 10610.321287631989, 'accumulated_submission_time': 10235.613961458206, 'accumulated_eval_time': 373.0587408542633, 'accumulated_logging_time': 0.594775915145874, 'global_step': 30172, 'preemption_count': 0}), (31686, {'train/accuracy': 0.3178212642669678, 'train/loss': 3.394036054611206, 'validation/accuracy': 0.3035599887371063, 'validation/loss': 3.541114330291748, 'validation/num_examples': 50000, 'test/accuracy': 0.22830000519752502, 'test/loss': 4.332971572875977, 'test/num_examples': 10000, 'score': 10745.81271648407, 'total_duration': 11138.345761299133, 'accumulated_submission_time': 10745.81271648407, 'accumulated_eval_time': 390.7978858947754, 'accumulated_logging_time': 0.6274514198303223, 'global_step': 31686, 'preemption_count': 0}), (33200, {'train/accuracy': 0.17191486060619354, 'train/loss': 5.012552261352539, 'validation/accuracy': 0.15573999285697937, 'validation/loss': 5.170931339263916, 'validation/num_examples': 50000, 'test/accuracy': 0.11110000312328339, 'test/loss': 5.935586452484131, 'test/num_examples': 10000, 'score': 11255.900620937347, 'total_duration': 11665.95344877243, 'accumulated_submission_time': 11255.900620937347, 'accumulated_eval_time': 408.22711849212646, 'accumulated_logging_time': 0.6634583473205566, 'global_step': 33200, 'preemption_count': 0}), (34714, {'train/accuracy': 0.32238519191741943, 'train/loss': 3.2682571411132812, 'validation/accuracy': 0.3014200031757355, 'validation/loss': 3.499361515045166, 'validation/num_examples': 50000, 'test/accuracy': 0.22610001266002655, 'test/loss': 4.192788124084473, 'test/num_examples': 10000, 'score': 11766.076352834702, 'total_duration': 12193.907732248306, 'accumulated_submission_time': 11766.076352834702, 'accumulated_eval_time': 425.9146020412445, 'accumulated_logging_time': 0.7024168968200684, 'global_step': 34714, 'preemption_count': 0}), (36228, {'train/accuracy': 0.2633928656578064, 'train/loss': 3.819126605987549, 'validation/accuracy': 0.24873998761177063, 'validation/loss': 3.9294168949127197, 'validation/num_examples': 50000, 'test/accuracy': 0.18330000340938568, 'test/loss': 4.704871654510498, 'test/num_examples': 10000, 'score': 12276.239780902863, 'total_duration': 12721.674006223679, 'accumulated_submission_time': 12276.239780902863, 'accumulated_eval_time': 443.42841243743896, 'accumulated_logging_time': 0.7372820377349854, 'global_step': 36228, 'preemption_count': 0}), (37742, {'train/accuracy': 0.3377511203289032, 'train/loss': 3.210239887237549, 'validation/accuracy': 0.31939998269081116, 'validation/loss': 3.303650140762329, 'validation/num_examples': 50000, 'test/accuracy': 0.23080001771450043, 'test/loss': 4.0718674659729, 'test/num_examples': 10000, 'score': 12786.358525514603, 'total_duration': 13249.55362534523, 'accumulated_submission_time': 12786.358525514603, 'accumulated_eval_time': 461.10265159606934, 'accumulated_logging_time': 0.7711968421936035, 'global_step': 37742, 'preemption_count': 0}), (39256, {'train/accuracy': 0.18891501426696777, 'train/loss': 4.7008209228515625, 'validation/accuracy': 0.1738400012254715, 'validation/loss': 4.816714763641357, 'validation/num_examples': 50000, 'test/accuracy': 0.11620000749826431, 'test/loss': 5.706114768981934, 'test/num_examples': 10000, 'score': 13296.550779342651, 'total_duration': 13777.302340745926, 'accumulated_submission_time': 13296.550779342651, 'accumulated_eval_time': 478.57200956344604, 'accumulated_logging_time': 0.8064799308776855, 'global_step': 39256, 'preemption_count': 0}), (40770, {'train/accuracy': 0.09825414419174194, 'train/loss': 6.791675090789795, 'validation/accuracy': 0.09229999780654907, 'validation/loss': 6.967357158660889, 'validation/num_examples': 50000, 'test/accuracy': 0.06430000066757202, 'test/loss': 7.548539638519287, 'test/num_examples': 10000, 'score': 13806.753144979477, 'total_duration': 14304.94440293312, 'accumulated_submission_time': 13806.753144979477, 'accumulated_eval_time': 495.9221394062042, 'accumulated_logging_time': 0.8445472717285156, 'global_step': 40770, 'preemption_count': 0}), (42282, {'train/accuracy': 0.3134167790412903, 'train/loss': 3.368189573287964, 'validation/accuracy': 0.2895599901676178, 'validation/loss': 3.5763092041015625, 'validation/num_examples': 50000, 'test/accuracy': 0.21320000290870667, 'test/loss': 4.345427513122559, 'test/num_examples': 10000, 'score': 14316.756876945496, 'total_duration': 14832.802520275116, 'accumulated_submission_time': 14316.756876945496, 'accumulated_eval_time': 513.688027381897, 'accumulated_logging_time': 0.8791120052337646, 'global_step': 42282, 'preemption_count': 0}), (43796, {'train/accuracy': 0.2523716390132904, 'train/loss': 3.879667043685913, 'validation/accuracy': 0.23787999153137207, 'validation/loss': 4.018867015838623, 'validation/num_examples': 50000, 'test/accuracy': 0.1721000075340271, 'test/loss': 4.735334873199463, 'test/num_examples': 10000, 'score': 14826.869350671768, 'total_duration': 15361.157025814056, 'accumulated_submission_time': 14826.869350671768, 'accumulated_eval_time': 531.8394250869751, 'accumulated_logging_time': 0.916710615158081, 'global_step': 43796, 'preemption_count': 0}), (45310, {'train/accuracy': 0.1259765625, 'train/loss': 5.498228073120117, 'validation/accuracy': 0.11673999577760696, 'validation/loss': 5.669344425201416, 'validation/num_examples': 50000, 'test/accuracy': 0.08700000494718552, 'test/loss': 6.223611831665039, 'test/num_examples': 10000, 'score': 15336.784964323044, 'total_duration': 15888.927158594131, 'accumulated_submission_time': 15336.784964323044, 'accumulated_eval_time': 549.6033554077148, 'accumulated_logging_time': 0.9554266929626465, 'global_step': 45310, 'preemption_count': 0}), (46824, {'train/accuracy': 0.3132772445678711, 'train/loss': 3.4213531017303467, 'validation/accuracy': 0.2969000041484833, 'validation/loss': 3.581291675567627, 'validation/num_examples': 50000, 'test/accuracy': 0.21540001034736633, 'test/loss': 4.327939033508301, 'test/num_examples': 10000, 'score': 15846.803076267242, 'total_duration': 16416.799332618713, 'accumulated_submission_time': 15846.803076267242, 'accumulated_eval_time': 567.3659672737122, 'accumulated_logging_time': 0.9953644275665283, 'global_step': 46824, 'preemption_count': 0}), (48338, {'train/accuracy': 0.3341836631298065, 'train/loss': 3.1928305625915527, 'validation/accuracy': 0.31797999143600464, 'validation/loss': 3.339296817779541, 'validation/num_examples': 50000, 'test/accuracy': 0.23990000784397125, 'test/loss': 4.064082145690918, 'test/num_examples': 10000, 'score': 16356.954234361649, 'total_duration': 16944.592866659164, 'accumulated_submission_time': 16356.954234361649, 'accumulated_eval_time': 584.9171187877655, 'accumulated_logging_time': 1.0334186553955078, 'global_step': 48338, 'preemption_count': 0}), (49852, {'train/accuracy': 0.2771245241165161, 'train/loss': 3.7622814178466797, 'validation/accuracy': 0.2566399872303009, 'validation/loss': 3.951295852661133, 'validation/num_examples': 50000, 'test/accuracy': 0.1909000128507614, 'test/loss': 4.771968841552734, 'test/num_examples': 10000, 'score': 16867.11060857773, 'total_duration': 17472.53868341446, 'accumulated_submission_time': 16867.11060857773, 'accumulated_eval_time': 602.6156764030457, 'accumulated_logging_time': 1.071347713470459, 'global_step': 49852, 'preemption_count': 0}), (51366, {'train/accuracy': 0.10586734116077423, 'train/loss': 6.146651268005371, 'validation/accuracy': 0.09307999908924103, 'validation/loss': 6.373527526855469, 'validation/num_examples': 50000, 'test/accuracy': 0.06830000132322311, 'test/loss': 6.9438652992248535, 'test/num_examples': 10000, 'score': 17377.086690425873, 'total_duration': 18000.52445960045, 'accumulated_submission_time': 17377.086690425873, 'accumulated_eval_time': 620.5363309383392, 'accumulated_logging_time': 1.1086018085479736, 'global_step': 51366, 'preemption_count': 0}), (52880, {'train/accuracy': 0.4368024468421936, 'train/loss': 2.5360634326934814, 'validation/accuracy': 0.40303999185562134, 'validation/loss': 2.7369699478149414, 'validation/num_examples': 50000, 'test/accuracy': 0.30410000681877136, 'test/loss': 3.4496142864227295, 'test/num_examples': 10000, 'score': 17887.21930384636, 'total_duration': 18528.455255270004, 'accumulated_submission_time': 17887.21930384636, 'accumulated_eval_time': 638.2453672885895, 'accumulated_logging_time': 1.144514799118042, 'global_step': 52880, 'preemption_count': 0}), (54394, {'train/accuracy': 0.2947225570678711, 'train/loss': 3.6311120986938477, 'validation/accuracy': 0.28077998757362366, 'validation/loss': 3.7400660514831543, 'validation/num_examples': 50000, 'test/accuracy': 0.19930000603199005, 'test/loss': 4.530226230621338, 'test/num_examples': 10000, 'score': 18397.130579948425, 'total_duration': 19056.094877958298, 'accumulated_submission_time': 18397.130579948425, 'accumulated_eval_time': 655.88427901268, 'accumulated_logging_time': 1.1814467906951904, 'global_step': 54394, 'preemption_count': 0}), (55908, {'train/accuracy': 0.18937340378761292, 'train/loss': 4.625549793243408, 'validation/accuracy': 0.1741199940443039, 'validation/loss': 4.754756927490234, 'validation/num_examples': 50000, 'test/accuracy': 0.1299000084400177, 'test/loss': 5.389272212982178, 'test/num_examples': 10000, 'score': 18907.1184194088, 'total_duration': 19583.714373588562, 'accumulated_submission_time': 18907.1184194088, 'accumulated_eval_time': 673.4222629070282, 'accumulated_logging_time': 1.218961477279663, 'global_step': 55908, 'preemption_count': 0}), (57422, {'train/accuracy': 0.2808115482330322, 'train/loss': 3.7090742588043213, 'validation/accuracy': 0.26372000575065613, 'validation/loss': 3.829373598098755, 'validation/num_examples': 50000, 'test/accuracy': 0.19340001046657562, 'test/loss': 4.597098350524902, 'test/num_examples': 10000, 'score': 19417.15093255043, 'total_duration': 20111.377707242966, 'accumulated_submission_time': 19417.15093255043, 'accumulated_eval_time': 690.9619073867798, 'accumulated_logging_time': 1.2572824954986572, 'global_step': 57422, 'preemption_count': 0}), (58937, {'train/accuracy': 0.22622369229793549, 'train/loss': 4.323338985443115, 'validation/accuracy': 0.20683999359607697, 'validation/loss': 4.579726219177246, 'validation/num_examples': 50000, 'test/accuracy': 0.1551000028848648, 'test/loss': 5.24745512008667, 'test/num_examples': 10000, 'score': 19927.386483430862, 'total_duration': 20639.389159202576, 'accumulated_submission_time': 19927.386483430862, 'accumulated_eval_time': 708.6484682559967, 'accumulated_logging_time': 1.293562412261963, 'global_step': 58937, 'preemption_count': 0}), (60451, {'train/accuracy': 0.38809388875961304, 'train/loss': 2.8707337379455566, 'validation/accuracy': 0.3535799980163574, 'validation/loss': 3.1261491775512695, 'validation/num_examples': 50000, 'test/accuracy': 0.2665000259876251, 'test/loss': 3.855259656906128, 'test/num_examples': 10000, 'score': 20437.370439767838, 'total_duration': 21167.13238477707, 'accumulated_submission_time': 20437.370439767838, 'accumulated_eval_time': 726.3159120082855, 'accumulated_logging_time': 1.3322150707244873, 'global_step': 60451, 'preemption_count': 0}), (61965, {'train/accuracy': 0.2292729616165161, 'train/loss': 4.253842830657959, 'validation/accuracy': 0.22010000050067902, 'validation/loss': 4.375511646270752, 'validation/num_examples': 50000, 'test/accuracy': 0.16340000927448273, 'test/loss': 4.972453594207764, 'test/num_examples': 10000, 'score': 20947.30847287178, 'total_duration': 21694.66338658333, 'accumulated_submission_time': 20947.30847287178, 'accumulated_eval_time': 743.8167865276337, 'accumulated_logging_time': 1.3707172870635986, 'global_step': 61965, 'preemption_count': 0}), (63479, {'train/accuracy': 0.28366151452064514, 'train/loss': 3.6376185417175293, 'validation/accuracy': 0.27041998505592346, 'validation/loss': 3.7318127155303955, 'validation/num_examples': 50000, 'test/accuracy': 0.18970000743865967, 'test/loss': 4.5615234375, 'test/num_examples': 10000, 'score': 21457.283179998398, 'total_duration': 22222.106975317, 'accumulated_submission_time': 21457.283179998398, 'accumulated_eval_time': 761.1923484802246, 'accumulated_logging_time': 1.4095752239227295, 'global_step': 63479, 'preemption_count': 0}), (64993, {'train/accuracy': 0.2693319320678711, 'train/loss': 3.9684836864471436, 'validation/accuracy': 0.26183998584747314, 'validation/loss': 4.07616662979126, 'validation/num_examples': 50000, 'test/accuracy': 0.19790001213550568, 'test/loss': 4.845719337463379, 'test/num_examples': 10000, 'score': 21967.26203918457, 'total_duration': 22749.678109169006, 'accumulated_submission_time': 21967.26203918457, 'accumulated_eval_time': 778.6932566165924, 'accumulated_logging_time': 1.4470484256744385, 'global_step': 64993, 'preemption_count': 0}), (66507, {'train/accuracy': 0.3372528553009033, 'train/loss': 3.1919519901275635, 'validation/accuracy': 0.31836000084877014, 'validation/loss': 3.3297736644744873, 'validation/num_examples': 50000, 'test/accuracy': 0.24860000610351562, 'test/loss': 4.025635242462158, 'test/num_examples': 10000, 'score': 22477.223001003265, 'total_duration': 23277.50735592842, 'accumulated_submission_time': 22477.223001003265, 'accumulated_eval_time': 796.4663238525391, 'accumulated_logging_time': 1.4883835315704346, 'global_step': 66507, 'preemption_count': 0}), (68021, {'train/accuracy': 0.3380899131298065, 'train/loss': 3.2569570541381836, 'validation/accuracy': 0.293720006942749, 'validation/loss': 3.590768575668335, 'validation/num_examples': 50000, 'test/accuracy': 0.2199000120162964, 'test/loss': 4.2673563957214355, 'test/num_examples': 10000, 'score': 22987.152262687683, 'total_duration': 23805.233120441437, 'accumulated_submission_time': 22987.152262687683, 'accumulated_eval_time': 814.1657972335815, 'accumulated_logging_time': 1.5314984321594238, 'global_step': 68021, 'preemption_count': 0}), (69535, {'train/accuracy': 0.3651546537876129, 'train/loss': 3.035815715789795, 'validation/accuracy': 0.3411799967288971, 'validation/loss': 3.2265498638153076, 'validation/num_examples': 50000, 'test/accuracy': 0.2547000050544739, 'test/loss': 4.050992488861084, 'test/num_examples': 10000, 'score': 23497.329399108887, 'total_duration': 24333.222053050995, 'accumulated_submission_time': 23497.329399108887, 'accumulated_eval_time': 831.8838548660278, 'accumulated_logging_time': 1.574218988418579, 'global_step': 69535, 'preemption_count': 0}), (71049, {'train/accuracy': 0.38948899507522583, 'train/loss': 2.9351134300231934, 'validation/accuracy': 0.36055999994277954, 'validation/loss': 3.162463903427124, 'validation/num_examples': 50000, 'test/accuracy': 0.265500009059906, 'test/loss': 4.025924205780029, 'test/num_examples': 10000, 'score': 24007.331367492676, 'total_duration': 24860.906602859497, 'accumulated_submission_time': 24007.331367492676, 'accumulated_eval_time': 849.4705073833466, 'accumulated_logging_time': 1.6176283359527588, 'global_step': 71049, 'preemption_count': 0}), (72563, {'train/accuracy': 0.2634526491165161, 'train/loss': 3.8071141242980957, 'validation/accuracy': 0.25356000661849976, 'validation/loss': 3.922708034515381, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.710382461547852, 'test/num_examples': 10000, 'score': 24517.32631087303, 'total_duration': 25388.561143636703, 'accumulated_submission_time': 24517.32631087303, 'accumulated_eval_time': 867.0384802818298, 'accumulated_logging_time': 1.6555404663085938, 'global_step': 72563, 'preemption_count': 0}), (74077, {'train/accuracy': 0.38167649507522583, 'train/loss': 2.9451732635498047, 'validation/accuracy': 0.3625199794769287, 'validation/loss': 3.1048009395599365, 'validation/num_examples': 50000, 'test/accuracy': 0.2621999979019165, 'test/loss': 4.026857376098633, 'test/num_examples': 10000, 'score': 25027.415630340576, 'total_duration': 25916.27106308937, 'accumulated_submission_time': 25027.415630340576, 'accumulated_eval_time': 884.5662899017334, 'accumulated_logging_time': 1.6954331398010254, 'global_step': 74077, 'preemption_count': 0}), (75591, {'train/accuracy': 0.1342075914144516, 'train/loss': 5.668425559997559, 'validation/accuracy': 0.1243399977684021, 'validation/loss': 5.764382839202881, 'validation/num_examples': 50000, 'test/accuracy': 0.08700000494718552, 'test/loss': 6.647572040557861, 'test/num_examples': 10000, 'score': 25537.438853025436, 'total_duration': 26443.800446033478, 'accumulated_submission_time': 25537.438853025436, 'accumulated_eval_time': 901.976181268692, 'accumulated_logging_time': 1.7384414672851562, 'global_step': 75591, 'preemption_count': 0}), (77106, {'train/accuracy': 0.3316725194454193, 'train/loss': 3.21846866607666, 'validation/accuracy': 0.2920999825000763, 'validation/loss': 3.503586769104004, 'validation/num_examples': 50000, 'test/accuracy': 0.22050000727176666, 'test/loss': 4.244337558746338, 'test/num_examples': 10000, 'score': 26047.63839364052, 'total_duration': 26971.54065322876, 'accumulated_submission_time': 26047.63839364052, 'accumulated_eval_time': 919.41881108284, 'accumulated_logging_time': 1.7832458019256592, 'global_step': 77106, 'preemption_count': 0}), (78620, {'train/accuracy': 0.3699776828289032, 'train/loss': 3.103986978530884, 'validation/accuracy': 0.35266000032424927, 'validation/loss': 3.277158737182617, 'validation/num_examples': 50000, 'test/accuracy': 0.2669000029563904, 'test/loss': 4.146973609924316, 'test/num_examples': 10000, 'score': 26557.776461839676, 'total_duration': 27499.20015025139, 'accumulated_submission_time': 26557.776461839676, 'accumulated_eval_time': 936.843403339386, 'accumulated_logging_time': 1.8256030082702637, 'global_step': 78620, 'preemption_count': 0}), (80134, {'train/accuracy': 0.3420161008834839, 'train/loss': 3.218864917755127, 'validation/accuracy': 0.3225799798965454, 'validation/loss': 3.411599636077881, 'validation/num_examples': 50000, 'test/accuracy': 0.24210001528263092, 'test/loss': 4.21008825302124, 'test/num_examples': 10000, 'score': 27067.766325950623, 'total_duration': 28026.72852373123, 'accumulated_submission_time': 27067.766325950623, 'accumulated_eval_time': 954.2851715087891, 'accumulated_logging_time': 1.868180513381958, 'global_step': 80134, 'preemption_count': 0}), (81648, {'train/accuracy': 0.39485010504722595, 'train/loss': 2.805603265762329, 'validation/accuracy': 0.3705599904060364, 'validation/loss': 2.987741470336914, 'validation/num_examples': 50000, 'test/accuracy': 0.281900018453598, 'test/loss': 3.800844669342041, 'test/num_examples': 10000, 'score': 27577.878240585327, 'total_duration': 28554.491887807846, 'accumulated_submission_time': 27577.878240585327, 'accumulated_eval_time': 971.8416512012482, 'accumulated_logging_time': 1.9092822074890137, 'global_step': 81648, 'preemption_count': 0}), (83162, {'train/accuracy': 0.2846579849720001, 'train/loss': 3.9252586364746094, 'validation/accuracy': 0.27432000637054443, 'validation/loss': 4.041182041168213, 'validation/num_examples': 50000, 'test/accuracy': 0.19210000336170197, 'test/loss': 5.06205940246582, 'test/num_examples': 10000, 'score': 28087.819554805756, 'total_duration': 29082.335039138794, 'accumulated_submission_time': 28087.819554805756, 'accumulated_eval_time': 989.6448218822479, 'accumulated_logging_time': 1.954078197479248, 'global_step': 83162, 'preemption_count': 0}), (84677, {'train/accuracy': 0.46466436982154846, 'train/loss': 2.3826076984405518, 'validation/accuracy': 0.4369199872016907, 'validation/loss': 2.5394155979156494, 'validation/num_examples': 50000, 'test/accuracy': 0.331900030374527, 'test/loss': 3.2385549545288086, 'test/num_examples': 10000, 'score': 28598.049216508865, 'total_duration': 29610.035831689835, 'accumulated_submission_time': 28598.049216508865, 'accumulated_eval_time': 1007.0220937728882, 'accumulated_logging_time': 1.9960856437683105, 'global_step': 84677, 'preemption_count': 0}), (86191, {'train/accuracy': 0.48212292790412903, 'train/loss': 2.2993392944335938, 'validation/accuracy': 0.4251599907875061, 'validation/loss': 2.6702864170074463, 'validation/num_examples': 50000, 'test/accuracy': 0.3320000171661377, 'test/loss': 3.385972738265991, 'test/num_examples': 10000, 'score': 29108.04040503502, 'total_duration': 30137.687334775925, 'accumulated_submission_time': 29108.04040503502, 'accumulated_eval_time': 1024.5876967906952, 'accumulated_logging_time': 2.03897762298584, 'global_step': 86191, 'preemption_count': 0}), (87705, {'train/accuracy': 0.33380499482154846, 'train/loss': 3.2080514430999756, 'validation/accuracy': 0.30441999435424805, 'validation/loss': 3.4460508823394775, 'validation/num_examples': 50000, 'test/accuracy': 0.23690001666545868, 'test/loss': 4.14555025100708, 'test/num_examples': 10000, 'score': 29617.99115371704, 'total_duration': 30665.211062908173, 'accumulated_submission_time': 29617.99115371704, 'accumulated_eval_time': 1042.0650265216827, 'accumulated_logging_time': 2.081550359725952, 'global_step': 87705, 'preemption_count': 0}), (89219, {'train/accuracy': 0.3150709569454193, 'train/loss': 3.5599539279937744, 'validation/accuracy': 0.2946600019931793, 'validation/loss': 3.770634651184082, 'validation/num_examples': 50000, 'test/accuracy': 0.22530001401901245, 'test/loss': 4.554732322692871, 'test/num_examples': 10000, 'score': 30127.92679142952, 'total_duration': 31192.591334342957, 'accumulated_submission_time': 30127.92679142952, 'accumulated_eval_time': 1059.4081492424011, 'accumulated_logging_time': 2.1293301582336426, 'global_step': 89219, 'preemption_count': 0}), (90733, {'train/accuracy': 0.39708226919174194, 'train/loss': 2.781815528869629, 'validation/accuracy': 0.36855998635292053, 'validation/loss': 2.9695539474487305, 'validation/num_examples': 50000, 'test/accuracy': 0.27320000529289246, 'test/loss': 3.7446415424346924, 'test/num_examples': 10000, 'score': 30637.952219963074, 'total_duration': 31720.2343685627, 'accumulated_submission_time': 30637.952219963074, 'accumulated_eval_time': 1076.9277966022491, 'accumulated_logging_time': 2.1744236946105957, 'global_step': 90733, 'preemption_count': 0}), (92246, {'train/accuracy': 0.45434072613716125, 'train/loss': 2.392561912536621, 'validation/accuracy': 0.4245999753475189, 'validation/loss': 2.587425708770752, 'validation/num_examples': 50000, 'test/accuracy': 0.320000022649765, 'test/loss': 3.2951889038085938, 'test/num_examples': 10000, 'score': 31148.09440755844, 'total_duration': 32248.54153752327, 'accumulated_submission_time': 31148.09440755844, 'accumulated_eval_time': 1094.9929592609406, 'accumulated_logging_time': 2.2205803394317627, 'global_step': 92246, 'preemption_count': 0}), (93760, {'train/accuracy': 0.45868542790412903, 'train/loss': 2.434083938598633, 'validation/accuracy': 0.4264200031757355, 'validation/loss': 2.6672022342681885, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.445615291595459, 'test/num_examples': 10000, 'score': 31658.021085500717, 'total_duration': 32775.989436626434, 'accumulated_submission_time': 31658.021085500717, 'accumulated_eval_time': 1112.41717171669, 'accumulated_logging_time': 2.264895439147949, 'global_step': 93760, 'preemption_count': 0}), (95275, {'train/accuracy': 0.37956392765045166, 'train/loss': 2.966967821121216, 'validation/accuracy': 0.340179979801178, 'validation/loss': 3.3062493801116943, 'validation/num_examples': 50000, 'test/accuracy': 0.25950002670288086, 'test/loss': 4.024302959442139, 'test/num_examples': 10000, 'score': 32168.247616052628, 'total_duration': 33303.67395091057, 'accumulated_submission_time': 32168.247616052628, 'accumulated_eval_time': 1129.7756674289703, 'accumulated_logging_time': 2.3125619888305664, 'global_step': 95275, 'preemption_count': 0}), (96790, {'train/accuracy': 0.4096579849720001, 'train/loss': 2.8069357872009277, 'validation/accuracy': 0.37689998745918274, 'validation/loss': 3.023160934448242, 'validation/num_examples': 50000, 'test/accuracy': 0.2849000096321106, 'test/loss': 3.783292055130005, 'test/num_examples': 10000, 'score': 32678.433045387268, 'total_duration': 33831.3767824173, 'accumulated_submission_time': 32678.433045387268, 'accumulated_eval_time': 1147.1902074813843, 'accumulated_logging_time': 2.3623785972595215, 'global_step': 96790, 'preemption_count': 0}), (98304, {'train/accuracy': 0.351283460855484, 'train/loss': 3.222519874572754, 'validation/accuracy': 0.33215999603271484, 'validation/loss': 3.3702187538146973, 'validation/num_examples': 50000, 'test/accuracy': 0.2519000172615051, 'test/loss': 4.132585525512695, 'test/num_examples': 10000, 'score': 33188.55919909477, 'total_duration': 34359.24005818367, 'accumulated_submission_time': 33188.55919909477, 'accumulated_eval_time': 1164.826674938202, 'accumulated_logging_time': 2.409458637237549, 'global_step': 98304, 'preemption_count': 0}), (99818, {'train/accuracy': 0.328125, 'train/loss': 3.5659189224243164, 'validation/accuracy': 0.3105199933052063, 'validation/loss': 3.713387966156006, 'validation/num_examples': 50000, 'test/accuracy': 0.2345000058412552, 'test/loss': 4.5583319664001465, 'test/num_examples': 10000, 'score': 33698.464007377625, 'total_duration': 34887.28036427498, 'accumulated_submission_time': 33698.464007377625, 'accumulated_eval_time': 1182.862447977066, 'accumulated_logging_time': 2.4572508335113525, 'global_step': 99818, 'preemption_count': 0}), (101332, {'train/accuracy': 0.4641461968421936, 'train/loss': 2.4268994331359863, 'validation/accuracy': 0.43977999687194824, 'validation/loss': 2.5999388694763184, 'validation/num_examples': 50000, 'test/accuracy': 0.33330002427101135, 'test/loss': 3.3893141746520996, 'test/num_examples': 10000, 'score': 34208.510954380035, 'total_duration': 35414.95019340515, 'accumulated_submission_time': 34208.510954380035, 'accumulated_eval_time': 1200.3929872512817, 'accumulated_logging_time': 2.495596408843994, 'global_step': 101332, 'preemption_count': 0}), (102846, {'train/accuracy': 0.4216557741165161, 'train/loss': 2.711665391921997, 'validation/accuracy': 0.3911399841308594, 'validation/loss': 2.9025368690490723, 'validation/num_examples': 50000, 'test/accuracy': 0.30580002069473267, 'test/loss': 3.579463481903076, 'test/num_examples': 10000, 'score': 34718.491564273834, 'total_duration': 35942.75317645073, 'accumulated_submission_time': 34718.491564273834, 'accumulated_eval_time': 1218.1181762218475, 'accumulated_logging_time': 2.540222406387329, 'global_step': 102846, 'preemption_count': 0}), (104360, {'train/accuracy': 0.5540298223495483, 'train/loss': 1.8740363121032715, 'validation/accuracy': 0.4943999946117401, 'validation/loss': 2.240631580352783, 'validation/num_examples': 50000, 'test/accuracy': 0.37880000472068787, 'test/loss': 3.0557496547698975, 'test/num_examples': 10000, 'score': 35228.44272494316, 'total_duration': 36470.514008522034, 'accumulated_submission_time': 35228.44272494316, 'accumulated_eval_time': 1235.8320398330688, 'accumulated_logging_time': 2.584373712539673, 'global_step': 104360, 'preemption_count': 0}), (105874, {'train/accuracy': 0.5051219463348389, 'train/loss': 2.1285979747772217, 'validation/accuracy': 0.45809999108314514, 'validation/loss': 2.4042491912841797, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.1027300357818604, 'test/num_examples': 10000, 'score': 35738.398156404495, 'total_duration': 36997.97931480408, 'accumulated_submission_time': 35738.398156404495, 'accumulated_eval_time': 1253.2412164211273, 'accumulated_logging_time': 2.6315665245056152, 'global_step': 105874, 'preemption_count': 0}), (107388, {'train/accuracy': 0.5316087007522583, 'train/loss': 2.0331740379333496, 'validation/accuracy': 0.4949599802494049, 'validation/loss': 2.2442522048950195, 'validation/num_examples': 50000, 'test/accuracy': 0.3841000199317932, 'test/loss': 3.025560140609741, 'test/num_examples': 10000, 'score': 36248.40759110451, 'total_duration': 37525.970309495926, 'accumulated_submission_time': 36248.40759110451, 'accumulated_eval_time': 1271.1150813102722, 'accumulated_logging_time': 2.683964490890503, 'global_step': 107388, 'preemption_count': 0}), (108902, {'train/accuracy': 0.5332629084587097, 'train/loss': 2.0092248916625977, 'validation/accuracy': 0.49629998207092285, 'validation/loss': 2.2380595207214355, 'validation/num_examples': 50000, 'test/accuracy': 0.3831000328063965, 'test/loss': 3.0195724964141846, 'test/num_examples': 10000, 'score': 36758.39783191681, 'total_duration': 38053.5044836998, 'accumulated_submission_time': 36758.39783191681, 'accumulated_eval_time': 1288.5332021713257, 'accumulated_logging_time': 2.7576067447662354, 'global_step': 108902, 'preemption_count': 0}), (110416, {'train/accuracy': 0.5140505433082581, 'train/loss': 2.1145224571228027, 'validation/accuracy': 0.4885999858379364, 'validation/loss': 2.261136531829834, 'validation/num_examples': 50000, 'test/accuracy': 0.3677000105381012, 'test/loss': 3.0803956985473633, 'test/num_examples': 10000, 'score': 37268.326226234436, 'total_duration': 38581.21153998375, 'accumulated_submission_time': 37268.326226234436, 'accumulated_eval_time': 1306.213036775589, 'accumulated_logging_time': 2.8046953678131104, 'global_step': 110416, 'preemption_count': 0}), (111930, {'train/accuracy': 0.5302335619926453, 'train/loss': 2.0475571155548096, 'validation/accuracy': 0.48955997824668884, 'validation/loss': 2.2830638885498047, 'validation/num_examples': 50000, 'test/accuracy': 0.36970001459121704, 'test/loss': 3.0804600715637207, 'test/num_examples': 10000, 'score': 37778.27315187454, 'total_duration': 39108.91320705414, 'accumulated_submission_time': 37778.27315187454, 'accumulated_eval_time': 1323.868717432022, 'accumulated_logging_time': 2.851999282836914, 'global_step': 111930, 'preemption_count': 0}), (113445, {'train/accuracy': 0.5375478267669678, 'train/loss': 1.958598017692566, 'validation/accuracy': 0.4891199767589569, 'validation/loss': 2.234560489654541, 'validation/num_examples': 50000, 'test/accuracy': 0.3817000091075897, 'test/loss': 2.9632489681243896, 'test/num_examples': 10000, 'score': 38288.500826358795, 'total_duration': 39637.01444840431, 'accumulated_submission_time': 38288.500826358795, 'accumulated_eval_time': 1341.6378679275513, 'accumulated_logging_time': 2.9030396938323975, 'global_step': 113445, 'preemption_count': 0}), (114960, {'train/accuracy': 0.5321866869926453, 'train/loss': 1.9915037155151367, 'validation/accuracy': 0.49831998348236084, 'validation/loss': 2.20292329788208, 'validation/num_examples': 50000, 'test/accuracy': 0.379800021648407, 'test/loss': 3.0490152835845947, 'test/num_examples': 10000, 'score': 38798.68224453926, 'total_duration': 40164.63128519058, 'accumulated_submission_time': 38798.68224453926, 'accumulated_eval_time': 1358.971853017807, 'accumulated_logging_time': 2.952772617340088, 'global_step': 114960, 'preemption_count': 0}), (116475, {'train/accuracy': 0.5823102593421936, 'train/loss': 1.767098069190979, 'validation/accuracy': 0.5386199951171875, 'validation/loss': 2.0045840740203857, 'validation/num_examples': 50000, 'test/accuracy': 0.41780000925064087, 'test/loss': 2.817715644836426, 'test/num_examples': 10000, 'score': 39308.85549354553, 'total_duration': 40692.430067777634, 'accumulated_submission_time': 39308.85549354553, 'accumulated_eval_time': 1376.4977297782898, 'accumulated_logging_time': 2.9995055198669434, 'global_step': 116475, 'preemption_count': 0}), (117989, {'train/accuracy': 0.4283721148967743, 'train/loss': 2.6851370334625244, 'validation/accuracy': 0.3919200003147125, 'validation/loss': 2.899744987487793, 'validation/num_examples': 50000, 'test/accuracy': 0.2914000153541565, 'test/loss': 3.691535711288452, 'test/num_examples': 10000, 'score': 39818.84736561775, 'total_duration': 41220.18163561821, 'accumulated_submission_time': 39818.84736561775, 'accumulated_eval_time': 1394.1560413837433, 'accumulated_logging_time': 3.0487263202667236, 'global_step': 117989, 'preemption_count': 0}), (119503, {'train/accuracy': 0.44764429330825806, 'train/loss': 2.602922201156616, 'validation/accuracy': 0.42197999358177185, 'validation/loss': 2.8036701679229736, 'validation/num_examples': 50000, 'test/accuracy': 0.32200002670288086, 'test/loss': 3.63352370262146, 'test/num_examples': 10000, 'score': 40328.843329668045, 'total_duration': 41747.484981536865, 'accumulated_submission_time': 40328.843329668045, 'accumulated_eval_time': 1411.3559973239899, 'accumulated_logging_time': 3.1032016277313232, 'global_step': 119503, 'preemption_count': 0}), (121017, {'train/accuracy': 0.5178372263908386, 'train/loss': 2.0767385959625244, 'validation/accuracy': 0.46949997544288635, 'validation/loss': 2.3726882934570312, 'validation/num_examples': 50000, 'test/accuracy': 0.35590001940727234, 'test/loss': 3.1724939346313477, 'test/num_examples': 10000, 'score': 40838.74562501907, 'total_duration': 42275.05197453499, 'accumulated_submission_time': 40838.74562501907, 'accumulated_eval_time': 1428.919724702835, 'accumulated_logging_time': 3.1509153842926025, 'global_step': 121017, 'preemption_count': 0}), (122531, {'train/accuracy': 0.6288264989852905, 'train/loss': 1.5047695636749268, 'validation/accuracy': 0.5694999694824219, 'validation/loss': 1.8335968255996704, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.6093904972076416, 'test/num_examples': 10000, 'score': 41348.64927601814, 'total_duration': 42802.48861408234, 'accumulated_submission_time': 41348.64927601814, 'accumulated_eval_time': 1446.3481650352478, 'accumulated_logging_time': 3.2026822566986084, 'global_step': 122531, 'preemption_count': 0}), (124045, {'train/accuracy': 0.6246811151504517, 'train/loss': 1.5214416980743408, 'validation/accuracy': 0.5735200047492981, 'validation/loss': 1.811548113822937, 'validation/num_examples': 50000, 'test/accuracy': 0.44520002603530884, 'test/loss': 2.5815134048461914, 'test/num_examples': 10000, 'score': 41858.61288642883, 'total_duration': 43330.341500520706, 'accumulated_submission_time': 41858.61288642883, 'accumulated_eval_time': 1464.131145477295, 'accumulated_logging_time': 3.255311965942383, 'global_step': 124045, 'preemption_count': 0}), (125560, {'train/accuracy': 0.6104312539100647, 'train/loss': 1.618659257888794, 'validation/accuracy': 0.5592799782752991, 'validation/loss': 1.9150943756103516, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.698650360107422, 'test/num_examples': 10000, 'score': 42368.84647965431, 'total_duration': 43858.305431604385, 'accumulated_submission_time': 42368.84647965431, 'accumulated_eval_time': 1481.754063129425, 'accumulated_logging_time': 3.3087551593780518, 'global_step': 125560, 'preemption_count': 0}), (127074, {'train/accuracy': 0.5913584232330322, 'train/loss': 1.713505506515503, 'validation/accuracy': 0.5462999939918518, 'validation/loss': 2.0027928352355957, 'validation/num_examples': 50000, 'test/accuracy': 0.42900002002716064, 'test/loss': 2.7601938247680664, 'test/num_examples': 10000, 'score': 42878.92653274536, 'total_duration': 44385.98586678505, 'accumulated_submission_time': 42878.92653274536, 'accumulated_eval_time': 1499.2517862319946, 'accumulated_logging_time': 3.358278512954712, 'global_step': 127074, 'preemption_count': 0}), (128588, {'train/accuracy': 0.6278898119926453, 'train/loss': 1.5220273733139038, 'validation/accuracy': 0.5804799795150757, 'validation/loss': 1.792826771736145, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.5546529293060303, 'test/num_examples': 10000, 'score': 43388.882608652115, 'total_duration': 44913.40585780144, 'accumulated_submission_time': 43388.882608652115, 'accumulated_eval_time': 1516.6094889640808, 'accumulated_logging_time': 3.411818504333496, 'global_step': 128588, 'preemption_count': 0}), (130102, {'train/accuracy': 0.6101123690605164, 'train/loss': 1.616065502166748, 'validation/accuracy': 0.5356799960136414, 'validation/loss': 2.0169262886047363, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.8192737102508545, 'test/num_examples': 10000, 'score': 43898.80344581604, 'total_duration': 45441.272557497025, 'accumulated_submission_time': 43898.80344581604, 'accumulated_eval_time': 1534.450347661972, 'accumulated_logging_time': 3.4636495113372803, 'global_step': 130102, 'preemption_count': 0}), (131616, {'train/accuracy': 0.6253786683082581, 'train/loss': 1.5129410028457642, 'validation/accuracy': 0.5648399591445923, 'validation/loss': 1.8565038442611694, 'validation/num_examples': 50000, 'test/accuracy': 0.44190001487731934, 'test/loss': 2.6380434036254883, 'test/num_examples': 10000, 'score': 44408.81854104996, 'total_duration': 45968.98812127113, 'accumulated_submission_time': 44408.81854104996, 'accumulated_eval_time': 1552.0316081047058, 'accumulated_logging_time': 3.529684543609619, 'global_step': 131616, 'preemption_count': 0}), (133131, {'train/accuracy': 0.6495934128761292, 'train/loss': 1.4116238355636597, 'validation/accuracy': 0.5912799835205078, 'validation/loss': 1.7342907190322876, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.4350662231445312, 'test/num_examples': 10000, 'score': 44918.97057008743, 'total_duration': 46496.75433373451, 'accumulated_submission_time': 44918.97057008743, 'accumulated_eval_time': 1569.5413410663605, 'accumulated_logging_time': 3.5810797214508057, 'global_step': 133131, 'preemption_count': 0}), (134645, {'train/accuracy': 0.6475008130073547, 'train/loss': 1.4163345098495483, 'validation/accuracy': 0.5905199646949768, 'validation/loss': 1.7114334106445312, 'validation/num_examples': 50000, 'test/accuracy': 0.4717000126838684, 'test/loss': 2.4368271827697754, 'test/num_examples': 10000, 'score': 45428.928008794785, 'total_duration': 47024.43297600746, 'accumulated_submission_time': 45428.928008794785, 'accumulated_eval_time': 1587.159719467163, 'accumulated_logging_time': 3.6312103271484375, 'global_step': 134645, 'preemption_count': 0}), (136159, {'train/accuracy': 0.6627072691917419, 'train/loss': 1.3503409624099731, 'validation/accuracy': 0.6053799986839294, 'validation/loss': 1.6583715677261353, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.411069869995117, 'test/num_examples': 10000, 'score': 45938.94582152367, 'total_duration': 47552.14718770981, 'accumulated_submission_time': 45938.94582152367, 'accumulated_eval_time': 1604.7524976730347, 'accumulated_logging_time': 3.681864023208618, 'global_step': 136159, 'preemption_count': 0}), (137674, {'train/accuracy': 0.6785116195678711, 'train/loss': 1.2806422710418701, 'validation/accuracy': 0.6271799802780151, 'validation/loss': 1.5513432025909424, 'validation/num_examples': 50000, 'test/accuracy': 0.5019000172615051, 'test/loss': 2.331106662750244, 'test/num_examples': 10000, 'score': 46449.14306783676, 'total_duration': 48080.309012174606, 'accumulated_submission_time': 46449.14306783676, 'accumulated_eval_time': 1622.6106128692627, 'accumulated_logging_time': 3.7330148220062256, 'global_step': 137674, 'preemption_count': 0}), (139188, {'train/accuracy': 0.667410671710968, 'train/loss': 1.3150213956832886, 'validation/accuracy': 0.5842999815940857, 'validation/loss': 1.7889710664749146, 'validation/num_examples': 50000, 'test/accuracy': 0.467600017786026, 'test/loss': 2.5262668132781982, 'test/num_examples': 10000, 'score': 46959.104165792465, 'total_duration': 48607.7670943737, 'accumulated_submission_time': 46959.104165792465, 'accumulated_eval_time': 1640.0028715133667, 'accumulated_logging_time': 3.786928653717041, 'global_step': 139188, 'preemption_count': 0}), (140701, {'train/accuracy': 0.7164978981018066, 'train/loss': 1.0897971391677856, 'validation/accuracy': 0.6477800011634827, 'validation/loss': 1.4555057287216187, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.2077748775482178, 'test/num_examples': 10000, 'score': 47469.02995443344, 'total_duration': 49135.43949460983, 'accumulated_submission_time': 47469.02995443344, 'accumulated_eval_time': 1657.6408331394196, 'accumulated_logging_time': 3.8422067165374756, 'global_step': 140701, 'preemption_count': 0}), (142214, {'train/accuracy': 0.7066326141357422, 'train/loss': 1.145608901977539, 'validation/accuracy': 0.6356599926948547, 'validation/loss': 1.4984140396118164, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.242482900619507, 'test/num_examples': 10000, 'score': 47979.00236058235, 'total_duration': 49662.91038489342, 'accumulated_submission_time': 47979.00236058235, 'accumulated_eval_time': 1675.0142815113068, 'accumulated_logging_time': 3.9150285720825195, 'global_step': 142214, 'preemption_count': 0}), (143729, {'train/accuracy': 0.6968669891357422, 'train/loss': 1.1911671161651611, 'validation/accuracy': 0.6352399587631226, 'validation/loss': 1.5196411609649658, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.2351131439208984, 'test/num_examples': 10000, 'score': 48489.20419001579, 'total_duration': 50191.19554066658, 'accumulated_submission_time': 48489.20419001579, 'accumulated_eval_time': 1692.9932827949524, 'accumulated_logging_time': 3.9672343730926514, 'global_step': 143729, 'preemption_count': 0}), (145244, {'train/accuracy': 0.6825175285339355, 'train/loss': 1.2460888624191284, 'validation/accuracy': 0.616379976272583, 'validation/loss': 1.6117098331451416, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.3389248847961426, 'test/num_examples': 10000, 'score': 48999.420974969864, 'total_duration': 50718.9482088089, 'accumulated_submission_time': 48999.420974969864, 'accumulated_eval_time': 1710.424084186554, 'accumulated_logging_time': 4.0192248821258545, 'global_step': 145244, 'preemption_count': 0}), (146758, {'train/accuracy': 0.7297114133834839, 'train/loss': 1.0454022884368896, 'validation/accuracy': 0.6575199961662292, 'validation/loss': 1.3876160383224487, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.1201376914978027, 'test/num_examples': 10000, 'score': 49509.32262468338, 'total_duration': 51246.62455034256, 'accumulated_submission_time': 49509.32262468338, 'accumulated_eval_time': 1728.0896308422089, 'accumulated_logging_time': 4.0746119022369385, 'global_step': 146758, 'preemption_count': 0}), (148272, {'train/accuracy': 0.7495615482330322, 'train/loss': 0.9471167325973511, 'validation/accuracy': 0.6625799536705017, 'validation/loss': 1.3929927349090576, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.1541640758514404, 'test/num_examples': 10000, 'score': 50019.25556206703, 'total_duration': 51773.96777963638, 'accumulated_submission_time': 50019.25556206703, 'accumulated_eval_time': 1745.3887765407562, 'accumulated_logging_time': 4.133601903915405, 'global_step': 148272, 'preemption_count': 0}), (149787, {'train/accuracy': 0.7493821382522583, 'train/loss': 0.9529452323913574, 'validation/accuracy': 0.6685400009155273, 'validation/loss': 1.3474432229995728, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.082439422607422, 'test/num_examples': 10000, 'score': 50529.4074280262, 'total_duration': 52301.51612615585, 'accumulated_submission_time': 50529.4074280262, 'accumulated_eval_time': 1762.67853140831, 'accumulated_logging_time': 4.1883015632629395, 'global_step': 149787, 'preemption_count': 0}), (151301, {'train/accuracy': 0.7281169891357422, 'train/loss': 1.0500671863555908, 'validation/accuracy': 0.6520599722862244, 'validation/loss': 1.4359551668167114, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.173976182937622, 'test/num_examples': 10000, 'score': 51039.449070453644, 'total_duration': 52829.5035905838, 'accumulated_submission_time': 51039.449070453644, 'accumulated_eval_time': 1780.5153470039368, 'accumulated_logging_time': 4.245532751083374, 'global_step': 151301, 'preemption_count': 0}), (152815, {'train/accuracy': 0.7424864172935486, 'train/loss': 0.9894612431526184, 'validation/accuracy': 0.6647999882698059, 'validation/loss': 1.3785579204559326, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.086134195327759, 'test/num_examples': 10000, 'score': 51549.42072844505, 'total_duration': 53357.012419462204, 'accumulated_submission_time': 51549.42072844505, 'accumulated_eval_time': 1797.9433093070984, 'accumulated_logging_time': 4.3016557693481445, 'global_step': 152815, 'preemption_count': 0}), (154329, {'train/accuracy': 0.7708266973495483, 'train/loss': 0.8738231658935547, 'validation/accuracy': 0.6909799575805664, 'validation/loss': 1.2597182989120483, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.0148048400878906, 'test/num_examples': 10000, 'score': 52059.3456428051, 'total_duration': 53884.340841293335, 'accumulated_submission_time': 52059.3456428051, 'accumulated_eval_time': 1815.2350759506226, 'accumulated_logging_time': 4.359313249588013, 'global_step': 154329, 'preemption_count': 0}), (155844, {'train/accuracy': 0.7562380433082581, 'train/loss': 0.9150132536888123, 'validation/accuracy': 0.6816799640655518, 'validation/loss': 1.297837734222412, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.047738790512085, 'test/num_examples': 10000, 'score': 52569.566727399826, 'total_duration': 54411.96320319176, 'accumulated_submission_time': 52569.566727399826, 'accumulated_eval_time': 1832.526507616043, 'accumulated_logging_time': 4.413227081298828, 'global_step': 155844, 'preemption_count': 0}), (157358, {'train/accuracy': 0.7893216013908386, 'train/loss': 0.7879802584648132, 'validation/accuracy': 0.6877599954605103, 'validation/loss': 1.278788447380066, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.0179154872894287, 'test/num_examples': 10000, 'score': 53079.48914647102, 'total_duration': 54939.777779340744, 'accumulated_submission_time': 53079.48914647102, 'accumulated_eval_time': 1850.3109276294708, 'accumulated_logging_time': 4.46701979637146, 'global_step': 157358, 'preemption_count': 0}), (158873, {'train/accuracy': 0.7989277839660645, 'train/loss': 0.7345190644264221, 'validation/accuracy': 0.7048599720001221, 'validation/loss': 1.2018253803253174, 'validation/num_examples': 50000, 'test/accuracy': 0.5772000551223755, 'test/loss': 1.9140307903289795, 'test/num_examples': 10000, 'score': 53589.66965150833, 'total_duration': 55467.48884010315, 'accumulated_submission_time': 53589.66965150833, 'accumulated_eval_time': 1867.7333896160126, 'accumulated_logging_time': 4.52330470085144, 'global_step': 158873, 'preemption_count': 0}), (160387, {'train/accuracy': 0.7996053695678711, 'train/loss': 0.7400349974632263, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.1957767009735107, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 1.8834655284881592, 'test/num_examples': 10000, 'score': 54099.629590034485, 'total_duration': 55994.96142053604, 'accumulated_submission_time': 54099.629590034485, 'accumulated_eval_time': 1885.1447412967682, 'accumulated_logging_time': 4.571201324462891, 'global_step': 160387, 'preemption_count': 0}), (161901, {'train/accuracy': 0.8086734414100647, 'train/loss': 0.7023515105247498, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.1650846004486084, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.8886688947677612, 'test/num_examples': 10000, 'score': 54609.69239163399, 'total_duration': 56522.81906700134, 'accumulated_submission_time': 54609.69239163399, 'accumulated_eval_time': 1902.8326406478882, 'accumulated_logging_time': 4.625036954879761, 'global_step': 161901, 'preemption_count': 0}), (163415, {'train/accuracy': 0.8216477632522583, 'train/loss': 0.6528841257095337, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.1238961219787598, 'validation/num_examples': 50000, 'test/accuracy': 0.5954000353813171, 'test/loss': 1.8337507247924805, 'test/num_examples': 10000, 'score': 55119.59383225441, 'total_duration': 57050.48101377487, 'accumulated_submission_time': 55119.59383225441, 'accumulated_eval_time': 1920.4788768291473, 'accumulated_logging_time': 4.685438394546509, 'global_step': 163415, 'preemption_count': 0}), (164929, {'train/accuracy': 0.8226243257522583, 'train/loss': 0.6398840546607971, 'validation/accuracy': 0.7210400104522705, 'validation/loss': 1.1339006423950195, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.8490453958511353, 'test/num_examples': 10000, 'score': 55629.537470817566, 'total_duration': 57577.97980308533, 'accumulated_submission_time': 55629.537470817566, 'accumulated_eval_time': 1937.9243867397308, 'accumulated_logging_time': 4.741180896759033, 'global_step': 164929, 'preemption_count': 0}), (166443, {'train/accuracy': 0.8295599222183228, 'train/loss': 0.6190734505653381, 'validation/accuracy': 0.7127199769020081, 'validation/loss': 1.169293999671936, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.897260069847107, 'test/num_examples': 10000, 'score': 56139.56280255318, 'total_duration': 58105.27678442001, 'accumulated_submission_time': 56139.56280255318, 'accumulated_eval_time': 1955.0879509449005, 'accumulated_logging_time': 4.795497179031372, 'global_step': 166443, 'preemption_count': 0}), (167957, {'train/accuracy': 0.8482740521430969, 'train/loss': 0.5581567883491516, 'validation/accuracy': 0.7278199791908264, 'validation/loss': 1.1075727939605713, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.799255132675171, 'test/num_examples': 10000, 'score': 56649.58001804352, 'total_duration': 58632.78707766533, 'accumulated_submission_time': 56649.58001804352, 'accumulated_eval_time': 1972.4707593917847, 'accumulated_logging_time': 4.853919506072998, 'global_step': 167957, 'preemption_count': 0}), (169471, {'train/accuracy': 0.8517617583274841, 'train/loss': 0.5366014242172241, 'validation/accuracy': 0.7337799668312073, 'validation/loss': 1.0894720554351807, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.802993655204773, 'test/num_examples': 10000, 'score': 57159.64442586899, 'total_duration': 59160.81916928291, 'accumulated_submission_time': 57159.64442586899, 'accumulated_eval_time': 1990.3277637958527, 'accumulated_logging_time': 4.91126561164856, 'global_step': 169471, 'preemption_count': 0}), (170985, {'train/accuracy': 0.8520607352256775, 'train/loss': 0.5264204144477844, 'validation/accuracy': 0.7373600006103516, 'validation/loss': 1.0713787078857422, 'validation/num_examples': 50000, 'test/accuracy': 0.6118000149726868, 'test/loss': 1.7798755168914795, 'test/num_examples': 10000, 'score': 57669.60288262367, 'total_duration': 59688.65509557724, 'accumulated_submission_time': 57669.60288262367, 'accumulated_eval_time': 2008.0955998897552, 'accumulated_logging_time': 4.968322038650513, 'global_step': 170985, 'preemption_count': 0}), (172499, {'train/accuracy': 0.8572624325752258, 'train/loss': 0.5081093311309814, 'validation/accuracy': 0.7387199997901917, 'validation/loss': 1.0654412508010864, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.7793102264404297, 'test/num_examples': 10000, 'score': 58179.54089784622, 'total_duration': 60216.452474832535, 'accumulated_submission_time': 58179.54089784622, 'accumulated_eval_time': 2025.8408544063568, 'accumulated_logging_time': 5.029789209365845, 'global_step': 172499, 'preemption_count': 0}), (174013, {'train/accuracy': 0.8667888641357422, 'train/loss': 0.4688793122768402, 'validation/accuracy': 0.7406599521636963, 'validation/loss': 1.0524687767028809, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.7508876323699951, 'test/num_examples': 10000, 'score': 58689.59416794777, 'total_duration': 60744.05333399773, 'accumulated_submission_time': 58689.59416794777, 'accumulated_eval_time': 2043.2813086509705, 'accumulated_logging_time': 5.085204124450684, 'global_step': 174013, 'preemption_count': 0}), (175527, {'train/accuracy': 0.8767737150192261, 'train/loss': 0.4423072636127472, 'validation/accuracy': 0.7445200085639954, 'validation/loss': 1.0392271280288696, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.7471065521240234, 'test/num_examples': 10000, 'score': 59199.493911504745, 'total_duration': 61271.79503440857, 'accumulated_submission_time': 59199.493911504745, 'accumulated_eval_time': 2061.011162519455, 'accumulated_logging_time': 5.145601987838745, 'global_step': 175527, 'preemption_count': 0}), (177041, {'train/accuracy': 0.8763552308082581, 'train/loss': 0.43424472212791443, 'validation/accuracy': 0.7455599904060364, 'validation/loss': 1.0426863431930542, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.7452729940414429, 'test/num_examples': 10000, 'score': 59709.4871468544, 'total_duration': 61799.87656879425, 'accumulated_submission_time': 59709.4871468544, 'accumulated_eval_time': 2078.988668680191, 'accumulated_logging_time': 5.2041075229644775, 'global_step': 177041, 'preemption_count': 0}), (178556, {'train/accuracy': 0.8790258169174194, 'train/loss': 0.43027791380882263, 'validation/accuracy': 0.7473999857902527, 'validation/loss': 1.0301430225372314, 'validation/num_examples': 50000, 'test/accuracy': 0.6222000122070312, 'test/loss': 1.731208324432373, 'test/num_examples': 10000, 'score': 60219.56933808327, 'total_duration': 62327.75828695297, 'accumulated_submission_time': 60219.56933808327, 'accumulated_eval_time': 2096.6750016212463, 'accumulated_logging_time': 5.264262676239014, 'global_step': 178556, 'preemption_count': 0}), (180071, {'train/accuracy': 0.878348171710968, 'train/loss': 0.4257912039756775, 'validation/accuracy': 0.7497199773788452, 'validation/loss': 1.0273454189300537, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.7289284467697144, 'test/num_examples': 10000, 'score': 60729.72563242912, 'total_duration': 62855.63610816002, 'accumulated_submission_time': 60729.72563242912, 'accumulated_eval_time': 2114.281795501709, 'accumulated_logging_time': 5.3266565799713135, 'global_step': 180071, 'preemption_count': 0}), (181585, {'train/accuracy': 0.8835299611091614, 'train/loss': 0.412623792886734, 'validation/accuracy': 0.750220000743866, 'validation/loss': 1.02522611618042, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.7295113801956177, 'test/num_examples': 10000, 'score': 61239.78214740753, 'total_duration': 63383.342509269714, 'accumulated_submission_time': 61239.78214740753, 'accumulated_eval_time': 2131.8164291381836, 'accumulated_logging_time': 5.389420032501221, 'global_step': 181585, 'preemption_count': 0}), (183099, {'train/accuracy': 0.8856026530265808, 'train/loss': 0.40698984265327454, 'validation/accuracy': 0.7508999705314636, 'validation/loss': 1.0227185487747192, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.7207680940628052, 'test/num_examples': 10000, 'score': 61749.71807217598, 'total_duration': 63910.96601963043, 'accumulated_submission_time': 61749.71807217598, 'accumulated_eval_time': 2149.3903136253357, 'accumulated_logging_time': 5.450626850128174, 'global_step': 183099, 'preemption_count': 0}), (184613, {'train/accuracy': 0.8846459984779358, 'train/loss': 0.4113345444202423, 'validation/accuracy': 0.7509999871253967, 'validation/loss': 1.0213669538497925, 'validation/num_examples': 50000, 'test/accuracy': 0.625, 'test/loss': 1.724463701248169, 'test/num_examples': 10000, 'score': 62259.6433134079, 'total_duration': 64438.64605140686, 'accumulated_submission_time': 62259.6433134079, 'accumulated_eval_time': 2167.0347170829773, 'accumulated_logging_time': 5.507994651794434, 'global_step': 184613, 'preemption_count': 0}), (186127, {'train/accuracy': 0.8859614133834839, 'train/loss': 0.4078052341938019, 'validation/accuracy': 0.7506799697875977, 'validation/loss': 1.0217421054840088, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.7228504419326782, 'test/num_examples': 10000, 'score': 62769.762776851654, 'total_duration': 64966.37164711952, 'accumulated_submission_time': 62769.762776851654, 'accumulated_eval_time': 2184.528416633606, 'accumulated_logging_time': 5.566797494888306, 'global_step': 186127, 'preemption_count': 0}), (186666, {'train/accuracy': 0.8869180083274841, 'train/loss': 0.40388861298561096, 'validation/accuracy': 0.7511199712753296, 'validation/loss': 1.0213629007339478, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.722780704498291, 'test/num_examples': 10000, 'score': 62951.208920001984, 'total_duration': 65165.74819970131, 'accumulated_submission_time': 62951.208920001984, 'accumulated_eval_time': 2202.380264520645, 'accumulated_logging_time': 5.6263251304626465, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0129 19:31:14.814928 140027215431488 submission_runner.py:586] Timing: 62951.208920001984
I0129 19:31:14.814991 140027215431488 submission_runner.py:588] Total number of evals: 125
I0129 19:31:14.815049 140027215431488 submission_runner.py:589] ====================
I0129 19:31:14.815094 140027215431488 submission_runner.py:542] Using RNG seed 3078694106
I0129 19:31:14.816475 140027215431488 submission_runner.py:551] --- Tuning run 5/5 ---
I0129 19:31:14.816581 140027215431488 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5.
I0129 19:31:14.817883 140027215431488 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5/hparams.json.
I0129 19:31:14.818605 140027215431488 submission_runner.py:206] Initializing dataset.
I0129 19:31:14.827249 140027215431488 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 19:31:14.837559 140027215431488 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 19:31:15.025874 140027215431488 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 19:31:15.899590 140027215431488 submission_runner.py:213] Initializing model.
I0129 19:31:21.890489 140027215431488 submission_runner.py:255] Initializing optimizer.
I0129 19:31:22.281153 140027215431488 submission_runner.py:262] Initializing metrics bundle.
I0129 19:31:22.281322 140027215431488 submission_runner.py:280] Initializing checkpoint and logger.
I0129 19:31:22.296712 140027215431488 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0129 19:31:22.296831 140027215431488 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 19:31:33.769047 140027215431488 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 19:31:45.369282 140027215431488 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5/flags_0.json.
I0129 19:31:45.374224 140027215431488 submission_runner.py:314] Starting training loop.
I0129 19:32:18.444418 139865224107776 logging_writer.py:48] [0] global_step=0, grad_norm=0.6595183610916138, loss=6.916248798370361
I0129 19:32:18.459796 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:32:24.669626 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:32:33.415510 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:32:35.997667 140027215431488 submission_runner.py:408] Time since start: 50.62s, 	Step: 1, 	{'train/accuracy': 0.0005978954141028225, 'train/loss': 6.911577224731445, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 33.085120677948, 'total_duration': 50.62336611747742, 'accumulated_submission_time': 33.085120677948, 'accumulated_eval_time': 17.53779411315918, 'accumulated_logging_time': 0}
I0129 19:32:36.007429 139865088849664 logging_writer.py:48] [1] accumulated_eval_time=17.537794, accumulated_logging_time=0, accumulated_submission_time=33.085121, global_step=1, preemption_count=0, score=33.085121, test/accuracy=0.001300, test/loss=6.911728, test/num_examples=10000, total_duration=50.623366, train/accuracy=0.000598, train/loss=6.911577, validation/accuracy=0.000680, validation/loss=6.912051, validation/num_examples=50000
I0129 19:33:10.007115 139865224107776 logging_writer.py:48] [100] global_step=100, grad_norm=0.6801074147224426, loss=6.824913024902344
I0129 19:33:44.046883 139865088849664 logging_writer.py:48] [200] global_step=200, grad_norm=0.8009259700775146, loss=6.541847229003906
I0129 19:34:18.062029 139865224107776 logging_writer.py:48] [300] global_step=300, grad_norm=1.1292177438735962, loss=6.246751308441162
I0129 19:34:52.133609 139865088849664 logging_writer.py:48] [400] global_step=400, grad_norm=4.776688098907471, loss=5.9750261306762695
I0129 19:35:26.194862 139865224107776 logging_writer.py:48] [500] global_step=500, grad_norm=2.5209381580352783, loss=5.770973205566406
I0129 19:36:00.286823 139865088849664 logging_writer.py:48] [600] global_step=600, grad_norm=4.250823974609375, loss=5.532157897949219
I0129 19:36:34.343956 139865224107776 logging_writer.py:48] [700] global_step=700, grad_norm=3.548441171646118, loss=5.4972453117370605
I0129 19:37:08.413950 139865088849664 logging_writer.py:48] [800] global_step=800, grad_norm=4.251119613647461, loss=5.4037675857543945
I0129 19:37:42.551799 139865224107776 logging_writer.py:48] [900] global_step=900, grad_norm=3.630028247833252, loss=5.184548377990723
I0129 19:38:16.639433 139865088849664 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.081260681152344, loss=5.063146114349365
I0129 19:38:50.697703 139865224107776 logging_writer.py:48] [1100] global_step=1100, grad_norm=5.1402153968811035, loss=5.027618408203125
I0129 19:39:24.777246 139865088849664 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.587027072906494, loss=4.832955360412598
I0129 19:39:58.830612 139865224107776 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.3602285385131836, loss=4.712384223937988
I0129 19:40:32.881585 139865088849664 logging_writer.py:48] [1400] global_step=1400, grad_norm=7.227911949157715, loss=4.578289031982422
I0129 19:41:06.065665 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:41:12.445971 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:41:21.326856 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:41:24.006117 140027215431488 submission_runner.py:408] Time since start: 578.63s, 	Step: 1499, 	{'train/accuracy': 0.16603554785251617, 'train/loss': 4.276123523712158, 'validation/accuracy': 0.15143999457359314, 'validation/loss': 4.407378196716309, 'validation/num_examples': 50000, 'test/accuracy': 0.1145000085234642, 'test/loss': 4.87145471572876, 'test/num_examples': 10000, 'score': 543.0786073207855, 'total_duration': 578.6318206787109, 'accumulated_submission_time': 543.0786073207855, 'accumulated_eval_time': 35.47820210456848, 'accumulated_logging_time': 0.02154088020324707}
I0129 19:41:24.028232 139864066488064 logging_writer.py:48] [1499] accumulated_eval_time=35.478202, accumulated_logging_time=0.021541, accumulated_submission_time=543.078607, global_step=1499, preemption_count=0, score=543.078607, test/accuracy=0.114500, test/loss=4.871455, test/num_examples=10000, total_duration=578.631821, train/accuracy=0.166036, train/loss=4.276124, validation/accuracy=0.151440, validation/loss=4.407378, validation/num_examples=50000
I0129 19:41:24.716379 139865088849664 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.909473419189453, loss=4.617443561553955
I0129 19:41:58.742991 139864066488064 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.496116638183594, loss=4.416277885437012
I0129 19:42:32.808849 139865088849664 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.171753883361816, loss=4.32649564743042
I0129 19:43:06.871432 139864066488064 logging_writer.py:48] [1800] global_step=1800, grad_norm=6.58531379699707, loss=4.2499542236328125
I0129 19:43:40.929863 139865088849664 logging_writer.py:48] [1900] global_step=1900, grad_norm=5.24013614654541, loss=4.2635626792907715
I0129 19:44:15.066391 139864066488064 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.698690414428711, loss=4.0638508796691895
I0129 19:44:49.115010 139865088849664 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.59952974319458, loss=3.8721699714660645
I0129 19:45:23.166176 139864066488064 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.5816650390625, loss=3.9115681648254395
I0129 19:45:57.213390 139865088849664 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.846065521240234, loss=3.8097124099731445
I0129 19:46:31.248665 139864066488064 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.430903434753418, loss=3.736335515975952
I0129 19:47:05.295054 139865088849664 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.046387195587158, loss=3.6080870628356934
I0129 19:47:39.336643 139864066488064 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.0090436935424805, loss=3.531376600265503
I0129 19:48:13.391743 139865088849664 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.026344299316406, loss=3.7378578186035156
I0129 19:48:47.438050 139864066488064 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.371818542480469, loss=3.4534518718719482
I0129 19:49:21.470535 139865088849664 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.609111785888672, loss=3.415604591369629
I0129 19:49:54.267577 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:50:00.630090 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:50:09.457313 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:50:12.185367 140027215431488 submission_runner.py:408] Time since start: 1106.81s, 	Step: 2998, 	{'train/accuracy': 0.3397241532802582, 'train/loss': 3.0705549716949463, 'validation/accuracy': 0.31525999307632446, 'validation/loss': 3.223424196243286, 'validation/num_examples': 50000, 'test/accuracy': 0.2322000116109848, 'test/loss': 3.897366523742676, 'test/num_examples': 10000, 'score': 1053.2543814182281, 'total_duration': 1106.8110687732697, 'accumulated_submission_time': 1053.2543814182281, 'accumulated_eval_time': 53.395957469940186, 'accumulated_logging_time': 0.0544428825378418}
I0129 19:50:12.209730 139865232500480 logging_writer.py:48] [2998] accumulated_eval_time=53.395957, accumulated_logging_time=0.054443, accumulated_submission_time=1053.254381, global_step=2998, preemption_count=0, score=1053.254381, test/accuracy=0.232200, test/loss=3.897367, test/num_examples=10000, total_duration=1106.811069, train/accuracy=0.339724, train/loss=3.070555, validation/accuracy=0.315260, validation/loss=3.223424, validation/num_examples=50000
I0129 19:50:13.240470 139865240893184 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.0612223148345947, loss=3.3685569763183594
I0129 19:50:47.328819 139865232500480 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.621244430541992, loss=3.2711551189422607
I0129 19:51:21.320603 139865240893184 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.559821367263794, loss=3.2581090927124023
I0129 19:51:55.339745 139865232500480 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.6052329540252686, loss=3.4497592449188232
I0129 19:52:29.354429 139865240893184 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.933682680130005, loss=3.2978482246398926
I0129 19:53:03.374753 139865232500480 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.9655449390411377, loss=3.2773585319519043
I0129 19:53:37.427521 139865240893184 logging_writer.py:48] [3600] global_step=3600, grad_norm=5.290308475494385, loss=3.2145872116088867
I0129 19:54:11.483125 139865232500480 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.6310997009277344, loss=2.951575517654419
I0129 19:54:45.481014 139865240893184 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.194889783859253, loss=3.033174753189087
I0129 19:55:19.501612 139865232500480 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.2714171409606934, loss=3.0115301609039307
I0129 19:55:53.492723 139865240893184 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.686244010925293, loss=2.9983441829681396
I0129 19:56:27.531490 139865232500480 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.0652949810028076, loss=2.8755760192871094
I0129 19:57:01.545560 139865240893184 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.2397849559783936, loss=2.7643308639526367
I0129 19:57:35.638303 139865232500480 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.9219367504119873, loss=2.872738838195801
I0129 19:58:09.627489 139865240893184 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.424734592437744, loss=2.9280405044555664
I0129 19:58:42.396604 140027215431488 spec.py:321] Evaluating on the training split.
I0129 19:58:48.717560 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 19:58:57.544910 140027215431488 spec.py:349] Evaluating on the test split.
I0129 19:59:00.223032 140027215431488 submission_runner.py:408] Time since start: 1634.85s, 	Step: 4498, 	{'train/accuracy': 0.4709024131298065, 'train/loss': 2.321305751800537, 'validation/accuracy': 0.4215799868106842, 'validation/loss': 2.6061043739318848, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.398090362548828, 'test/num_examples': 10000, 'score': 1563.3771555423737, 'total_duration': 1634.8487372398376, 'accumulated_submission_time': 1563.3771555423737, 'accumulated_eval_time': 71.222341299057, 'accumulated_logging_time': 0.0895986557006836}
I0129 19:59:00.243800 139864066488064 logging_writer.py:48] [4498] accumulated_eval_time=71.222341, accumulated_logging_time=0.089599, accumulated_submission_time=1563.377156, global_step=4498, preemption_count=0, score=1563.377156, test/accuracy=0.311300, test/loss=3.398090, test/num_examples=10000, total_duration=1634.848737, train/accuracy=0.470902, train/loss=2.321306, validation/accuracy=0.421580, validation/loss=2.606104, validation/num_examples=50000
I0129 19:59:01.264692 139865088849664 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.1953790187835693, loss=2.92260479927063
I0129 19:59:35.239608 139864066488064 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.6101510524749756, loss=2.781951427459717
I0129 20:00:09.182900 139865088849664 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.1002683639526367, loss=2.7282137870788574
I0129 20:00:43.168899 139864066488064 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.6132724285125732, loss=2.882826089859009
I0129 20:01:17.184222 139865088849664 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.134066343307495, loss=2.674574851989746
I0129 20:01:51.181793 139864066488064 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.486464023590088, loss=2.8517470359802246
I0129 20:02:25.170005 139865088849664 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.470189332962036, loss=2.6333508491516113
I0129 20:02:59.164566 139864066488064 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.457719326019287, loss=2.7222213745117188
I0129 20:03:33.174682 139865088849664 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.264758586883545, loss=2.530684471130371
I0129 20:04:07.241404 139864066488064 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.344346523284912, loss=2.6186866760253906
I0129 20:04:41.232779 139865088849664 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.299121856689453, loss=2.4165754318237305
I0129 20:05:15.204794 139864066488064 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.951356053352356, loss=2.4230122566223145
I0129 20:05:49.196224 139865088849664 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.677534580230713, loss=2.406930446624756
I0129 20:06:23.163785 139864066488064 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.775019645690918, loss=2.398322820663452
I0129 20:06:57.101413 139865088849664 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.6285126209259033, loss=2.5553767681121826
I0129 20:07:30.528387 140027215431488 spec.py:321] Evaluating on the training split.
I0129 20:07:36.964617 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 20:07:45.653009 140027215431488 spec.py:349] Evaluating on the test split.
I0129 20:07:48.199486 140027215431488 submission_runner.py:408] Time since start: 2162.83s, 	Step: 6000, 	{'train/accuracy': 0.5570989847183228, 'train/loss': 1.8586288690567017, 'validation/accuracy': 0.4995799958705902, 'validation/loss': 2.171689748764038, 'validation/num_examples': 50000, 'test/accuracy': 0.3790000081062317, 'test/loss': 2.932342052459717, 'test/num_examples': 10000, 'score': 2073.5986137390137, 'total_duration': 2162.8251535892487, 'accumulated_submission_time': 2073.5986137390137, 'accumulated_eval_time': 88.89337491989136, 'accumulated_logging_time': 0.12088155746459961}
I0129 20:07:48.218281 139865224107776 logging_writer.py:48] [6000] accumulated_eval_time=88.893375, accumulated_logging_time=0.120882, accumulated_submission_time=2073.598614, global_step=6000, preemption_count=0, score=2073.598614, test/accuracy=0.379000, test/loss=2.932342, test/num_examples=10000, total_duration=2162.825154, train/accuracy=0.557099, train/loss=1.858629, validation/accuracy=0.499580, validation/loss=2.171690, validation/num_examples=50000
I0129 20:07:48.568637 139865232500480 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.5613982677459717, loss=2.352205753326416
I0129 20:08:22.516594 139865224107776 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.912440299987793, loss=2.4293971061706543
I0129 20:08:56.448588 139865232500480 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.2409470081329346, loss=2.4461703300476074
I0129 20:09:30.405357 139865224107776 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.8580008745193481, loss=2.3993849754333496
I0129 20:10:04.368340 139865232500480 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.427304983139038, loss=2.4183120727539062
I0129 20:10:38.439235 139865224107776 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.7489715814590454, loss=2.322774648666382
I0129 20:11:12.425737 139865232500480 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.9032337665557861, loss=2.269960641860962
I0129 20:11:46.394960 139865224107776 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.25300669670105, loss=2.3519792556762695
I0129 20:12:20.326768 139865232500480 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.7869231700897217, loss=2.302513837814331
I0129 20:12:54.264096 139865224107776 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.3496711254119873, loss=2.278364896774292
I0129 20:13:28.249161 139865232500480 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.5748342275619507, loss=2.4137516021728516
I0129 20:14:02.205265 139865224107776 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.943367600440979, loss=2.2872982025146484
I0129 20:14:36.180558 139865232500480 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.347822904586792, loss=2.1923704147338867
I0129 20:15:10.158248 139865224107776 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.818131446838379, loss=2.246464729309082
I0129 20:15:44.103650 139865232500480 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.7239100933074951, loss=2.1191678047180176
I0129 20:16:18.047760 139865224107776 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8980765342712402, loss=2.3309662342071533
I0129 20:16:18.528245 140027215431488 spec.py:321] Evaluating on the training split.
I0129 20:16:24.850419 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 20:16:33.433305 140027215431488 spec.py:349] Evaluating on the test split.
I0129 20:16:36.110837 140027215431488 submission_runner.py:408] Time since start: 2690.74s, 	Step: 7503, 	{'train/accuracy': 0.5871731638908386, 'train/loss': 1.7035075426101685, 'validation/accuracy': 0.5399199724197388, 'validation/loss': 1.9825940132141113, 'validation/num_examples': 50000, 'test/accuracy': 0.4199000298976898, 'test/loss': 2.727141857147217, 'test/num_examples': 10000, 'score': 2583.842520713806, 'total_duration': 2690.7365441322327, 'accumulated_submission_time': 2583.842520713806, 'accumulated_eval_time': 106.47591543197632, 'accumulated_logging_time': 0.15299773216247559}
I0129 20:16:36.130244 139864049702656 logging_writer.py:48] [7503] accumulated_eval_time=106.475915, accumulated_logging_time=0.152998, accumulated_submission_time=2583.842521, global_step=7503, preemption_count=0, score=2583.842521, test/accuracy=0.419900, test/loss=2.727142, test/num_examples=10000, total_duration=2690.736544, train/accuracy=0.587173, train/loss=1.703508, validation/accuracy=0.539920, validation/loss=1.982594, validation/num_examples=50000
I0129 20:17:09.494518 139864058095360 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.1654958724975586, loss=2.15274715423584
I0129 20:17:43.392946 139864049702656 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1889495849609375, loss=2.1814074516296387
I0129 20:18:17.335801 139864058095360 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.9347784519195557, loss=2.174790382385254
I0129 20:18:51.317166 139864049702656 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.947435736656189, loss=2.277714729309082
I0129 20:19:25.239537 139864058095360 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9943791627883911, loss=2.1952364444732666
I0129 20:19:59.188380 139864049702656 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.7601042985916138, loss=2.205733299255371
I0129 20:20:33.127458 139864058095360 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.820907473564148, loss=2.1261813640594482
I0129 20:21:07.020724 139864049702656 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.9400758743286133, loss=2.165527105331421
I0129 20:21:40.952112 139864058095360 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.9436607360839844, loss=2.2049672603607178
I0129 20:22:14.865106 139864049702656 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.0566067695617676, loss=2.3074686527252197
I0129 20:22:48.812119 139864058095360 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.4396470785140991, loss=2.1542856693267822
I0129 20:23:22.767549 139864049702656 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.012606143951416, loss=2.1652512550354004
I0129 20:23:56.793368 139864058095360 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.8962149620056152, loss=2.1922478675842285
I0129 20:24:30.765441 139864049702656 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.9535443782806396, loss=1.951919674873352
I0129 20:25:04.710376 139864058095360 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.839620590209961, loss=2.289686918258667
I0129 20:25:06.222801 140027215431488 spec.py:321] Evaluating on the training split.
I0129 20:25:12.705835 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 20:25:21.383817 140027215431488 spec.py:349] Evaluating on the test split.
I0129 20:25:24.014719 140027215431488 submission_runner.py:408] Time since start: 3218.64s, 	Step: 9006, 	{'train/accuracy': 0.6002869606018066, 'train/loss': 1.646314024925232, 'validation/accuracy': 0.550819993019104, 'validation/loss': 1.9125394821166992, 'validation/num_examples': 50000, 'test/accuracy': 0.42350003123283386, 'test/loss': 2.7236905097961426, 'test/num_examples': 10000, 'score': 3093.868242740631, 'total_duration': 3218.64040517807, 'accumulated_submission_time': 3093.868242740631, 'accumulated_eval_time': 124.2677731513977, 'accumulated_logging_time': 0.18589282035827637}
I0129 20:25:24.034757 139865224107776 logging_writer.py:48] [9006] accumulated_eval_time=124.267773, accumulated_logging_time=0.185893, accumulated_submission_time=3093.868243, global_step=9006, preemption_count=0, score=3093.868243, test/accuracy=0.423500, test/loss=2.723691, test/num_examples=10000, total_duration=3218.640405, train/accuracy=0.600287, train/loss=1.646314, validation/accuracy=0.550820, validation/loss=1.912539, validation/num_examples=50000
I0129 20:25:56.225705 139865232500480 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.4652467966079712, loss=2.031364917755127
I0129 20:26:30.165050 139865224107776 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.9295785427093506, loss=2.2101402282714844
I0129 20:27:04.058866 139865232500480 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.9060596227645874, loss=2.0953078269958496
I0129 20:27:37.988652 139865224107776 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.030909776687622, loss=2.008349657058716
I0129 20:28:11.895643 139865232500480 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.6190249919891357, loss=1.9918277263641357
I0129 20:28:45.834695 139865224107776 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.1136133670806885, loss=1.983794927597046
I0129 20:29:19.797091 139865232500480 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.4820362329483032, loss=2.0862936973571777
I0129 20:29:53.745374 139865224107776 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.0752649307250977, loss=2.0369837284088135
I0129 20:30:27.755378 139865232500480 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.1397557258605957, loss=2.033503293991089
I0129 20:31:01.718064 139865224107776 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.5160530805587769, loss=2.0524041652679443
I0129 20:31:35.630114 139865232500480 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.7271023988723755, loss=2.175230026245117
I0129 20:32:09.560586 139865224107776 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7425907850265503, loss=2.031046152114868
I0129 20:32:43.457386 139865232500480 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.7776552438735962, loss=2.086679220199585
I0129 20:33:17.399613 139865224107776 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4496488571166992, loss=2.043867588043213
I0129 20:33:51.316573 139865232500480 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4961950778961182, loss=2.048203945159912
I0129 20:33:54.191377 140027215431488 spec.py:321] Evaluating on the training split.
I0129 20:34:00.558854 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 20:34:09.269127 140027215431488 spec.py:349] Evaluating on the test split.
I0129 20:34:11.955882 140027215431488 submission_runner.py:408] Time since start: 3746.58s, 	Step: 10510, 	{'train/accuracy': 0.629324734210968, 'train/loss': 1.4985932111740112, 'validation/accuracy': 0.5819999575614929, 'validation/loss': 1.7651735544204712, 'validation/num_examples': 50000, 'test/accuracy': 0.4556000232696533, 'test/loss': 2.51633882522583, 'test/num_examples': 10000, 'score': 3603.961983203888, 'total_duration': 3746.58158493042, 'accumulated_submission_time': 3603.961983203888, 'accumulated_eval_time': 142.03222179412842, 'accumulated_logging_time': 0.21620941162109375}
I0129 20:34:11.975115 139864058095360 logging_writer.py:48] [10510] accumulated_eval_time=142.032222, accumulated_logging_time=0.216209, accumulated_submission_time=3603.961983, global_step=10510, preemption_count=0, score=3603.961983, test/accuracy=0.455600, test/loss=2.516339, test/num_examples=10000, total_duration=3746.581585, train/accuracy=0.629325, train/loss=1.498593, validation/accuracy=0.582000, validation/loss=1.765174, validation/num_examples=50000
I0129 20:34:42.821449 139864066488064 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.8262875080108643, loss=1.9633638858795166
I0129 20:35:16.717473 139864058095360 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.0524802207946777, loss=2.024348735809326
I0129 20:35:50.608918 139864066488064 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.701630711555481, loss=1.9748687744140625
I0129 20:36:24.551544 139864058095360 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.804152011871338, loss=2.1673693656921387
I0129 20:36:58.564461 139864066488064 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.7850706577301025, loss=2.120436191558838
I0129 20:37:32.484925 139864058095360 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.6689774990081787, loss=1.8890637159347534
I0129 20:38:06.414754 139864066488064 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.308842420578003, loss=1.9943619966506958
I0129 20:38:40.363558 139864058095360 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.5127791166305542, loss=1.9101272821426392
I0129 20:39:14.298359 139864066488064 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.0184173583984375, loss=2.0072247982025146
I0129 20:39:48.242821 139864058095360 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.946938157081604, loss=2.078639030456543
I0129 20:40:22.162477 139864066488064 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.4404993057250977, loss=2.09677791595459
I0129 20:40:56.101306 139864058095360 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.4216337203979492, loss=1.960299015045166
I0129 20:41:30.003581 139864066488064 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.8818891048431396, loss=2.093137741088867
I0129 20:42:03.946566 139864058095360 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4477136135101318, loss=2.0549979209899902
I0129 20:42:37.841999 139864066488064 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.5282193422317505, loss=1.9388954639434814
I0129 20:42:42.061730 140027215431488 spec.py:321] Evaluating on the training split.
I0129 20:42:48.431097 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 20:42:56.999920 140027215431488 spec.py:349] Evaluating on the test split.
I0129 20:42:59.695052 140027215431488 submission_runner.py:408] Time since start: 4274.32s, 	Step: 12014, 	{'train/accuracy': 0.629304826259613, 'train/loss': 1.5169682502746582, 'validation/accuracy': 0.5853399634361267, 'validation/loss': 1.7463421821594238, 'validation/num_examples': 50000, 'test/accuracy': 0.46230003237724304, 'test/loss': 2.5003979206085205, 'test/num_examples': 10000, 'score': 4113.984125375748, 'total_duration': 4274.320744037628, 'accumulated_submission_time': 4113.984125375748, 'accumulated_eval_time': 159.66547989845276, 'accumulated_logging_time': 0.24596118927001953}
I0129 20:42:59.714584 139864049702656 logging_writer.py:48] [12014] accumulated_eval_time=159.665480, accumulated_logging_time=0.245961, accumulated_submission_time=4113.984125, global_step=12014, preemption_count=0, score=4113.984125, test/accuracy=0.462300, test/loss=2.500398, test/num_examples=10000, total_duration=4274.320744, train/accuracy=0.629305, train/loss=1.516968, validation/accuracy=0.585340, validation/loss=1.746342, validation/num_examples=50000
I0129 20:43:29.274306 139865232500480 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.4382838010787964, loss=1.890246868133545
I0129 20:44:03.282680 139864049702656 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6054494380950928, loss=1.8961310386657715
I0129 20:44:37.256241 139865232500480 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.6974492073059082, loss=2.0193545818328857
I0129 20:45:11.183076 139864049702656 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.4918569326400757, loss=1.9158574342727661
I0129 20:45:45.096637 139865232500480 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.754968285560608, loss=1.9473881721496582
I0129 20:46:19.017936 139864049702656 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.0907161235809326, loss=2.087128162384033
I0129 20:46:52.925004 139865232500480 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.6860493421554565, loss=1.9473364353179932
I0129 20:47:26.861737 139864049702656 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.63328218460083, loss=1.9642524719238281
I0129 20:48:00.749256 139865232500480 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6643993854522705, loss=1.8928678035736084
I0129 20:48:34.677206 139864049702656 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.4290293455123901, loss=1.9091012477874756
I0129 20:49:08.585623 139865232500480 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.2983818054199219, loss=1.9078809022903442
I0129 20:49:42.498298 139864049702656 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.5427690744400024, loss=2.017841100692749
I0129 20:50:16.641801 139865232500480 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.560537338256836, loss=2.050722360610962
I0129 20:50:50.567130 139864049702656 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.3225204944610596, loss=1.9519832134246826
I0129 20:51:24.538201 139865232500480 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.6415170431137085, loss=1.9767829179763794
I0129 20:51:29.759411 140027215431488 spec.py:321] Evaluating on the training split.
I0129 20:51:36.060483 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 20:51:44.933210 140027215431488 spec.py:349] Evaluating on the test split.
I0129 20:51:47.610479 140027215431488 submission_runner.py:408] Time since start: 4802.24s, 	Step: 13517, 	{'train/accuracy': 0.6426379084587097, 'train/loss': 1.466149091720581, 'validation/accuracy': 0.589959979057312, 'validation/loss': 1.7425481081008911, 'validation/num_examples': 50000, 'test/accuracy': 0.46140003204345703, 'test/loss': 2.5226640701293945, 'test/num_examples': 10000, 'score': 4623.965626001358, 'total_duration': 4802.236160516739, 'accumulated_submission_time': 4623.965626001358, 'accumulated_eval_time': 177.51647090911865, 'accumulated_logging_time': 0.2762629985809326}
I0129 20:51:47.631134 139864041309952 logging_writer.py:48] [13517] accumulated_eval_time=177.516471, accumulated_logging_time=0.276263, accumulated_submission_time=4623.965626, global_step=13517, preemption_count=0, score=4623.965626, test/accuracy=0.461400, test/loss=2.522664, test/num_examples=10000, total_duration=4802.236161, train/accuracy=0.642638, train/loss=1.466149, validation/accuracy=0.589960, validation/loss=1.742548, validation/num_examples=50000
I0129 20:52:16.145697 139864049702656 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.5570456981658936, loss=1.8922134637832642
I0129 20:52:50.021971 139864041309952 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.5281484127044678, loss=1.8111830949783325
I0129 20:53:23.966703 139864049702656 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.287056803703308, loss=1.9499244689941406
I0129 20:53:57.860800 139864041309952 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4719254970550537, loss=1.9381057024002075
I0129 20:54:31.801023 139864049702656 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6903636455535889, loss=1.8688005208969116
I0129 20:55:05.684244 139864041309952 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.447838306427002, loss=1.9222643375396729
I0129 20:55:39.637665 139864049702656 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.50454580783844, loss=1.8571056127548218
I0129 20:56:13.541115 139864041309952 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.6686701774597168, loss=1.9602396488189697
I0129 20:56:47.614430 139864049702656 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.6681568622589111, loss=1.9099397659301758
I0129 20:57:21.532952 139864041309952 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4047926664352417, loss=1.9002842903137207
I0129 20:57:55.470385 139864049702656 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.6895922422409058, loss=1.8649497032165527
I0129 20:58:29.374944 139864041309952 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.5465375185012817, loss=1.8108067512512207
I0129 20:59:03.311445 139864049702656 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.5290560722351074, loss=1.9324758052825928
I0129 20:59:37.224855 139864041309952 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.3993473052978516, loss=1.8822953701019287
I0129 21:00:11.156293 139864049702656 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.5670874118804932, loss=1.842254400253296
I0129 21:00:17.739239 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:00:24.081252 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:00:32.705471 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:00:35.583255 140027215431488 submission_runner.py:408] Time since start: 5330.21s, 	Step: 15021, 	{'train/accuracy': 0.6707589030265808, 'train/loss': 1.304468035697937, 'validation/accuracy': 0.5949400067329407, 'validation/loss': 1.69130539894104, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.4317922592163086, 'test/num_examples': 10000, 'score': 5134.007486343384, 'total_duration': 5330.208964586258, 'accumulated_submission_time': 5134.007486343384, 'accumulated_eval_time': 195.36043906211853, 'accumulated_logging_time': 0.3092031478881836}
I0129 21:00:35.601183 139865224107776 logging_writer.py:48] [15021] accumulated_eval_time=195.360439, accumulated_logging_time=0.309203, accumulated_submission_time=5134.007486, global_step=15021, preemption_count=0, score=5134.007486, test/accuracy=0.464500, test/loss=2.431792, test/num_examples=10000, total_duration=5330.208965, train/accuracy=0.670759, train/loss=1.304468, validation/accuracy=0.594940, validation/loss=1.691305, validation/num_examples=50000
I0129 21:01:02.726673 139865232500480 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5866600275039673, loss=2.0007238388061523
I0129 21:01:36.578045 139865224107776 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.6561447381973267, loss=1.952725887298584
I0129 21:02:10.497777 139865232500480 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.548920750617981, loss=2.000373601913452
I0129 21:02:44.420489 139865224107776 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.8874168395996094, loss=1.944706678390503
I0129 21:03:18.413466 139865232500480 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.6542476415634155, loss=1.956477165222168
I0129 21:03:52.345808 139865224107776 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.7374036312103271, loss=1.9647624492645264
I0129 21:04:26.250923 139865232500480 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.506061315536499, loss=1.7936928272247314
I0129 21:05:00.181733 139865224107776 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.4657303094863892, loss=1.8457355499267578
I0129 21:05:34.095197 139865232500480 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.739540934562683, loss=1.9065213203430176
I0129 21:06:08.029918 139865224107776 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.4203600883483887, loss=1.940891981124878
I0129 21:06:41.956362 139865232500480 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.336642265319824, loss=1.8864363431930542
I0129 21:07:15.833191 139865224107776 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.6018000841140747, loss=1.896107792854309
I0129 21:07:49.733088 139865232500480 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.9139519929885864, loss=1.8510005474090576
I0129 21:08:23.638411 139865224107776 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.4338887929916382, loss=2.050355911254883
I0129 21:08:57.549394 139865232500480 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.7012851238250732, loss=1.8689355850219727
I0129 21:09:05.857892 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:09:12.284071 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:09:21.012831 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:09:23.723146 140027215431488 submission_runner.py:408] Time since start: 5858.35s, 	Step: 16526, 	{'train/accuracy': 0.6666334271430969, 'train/loss': 1.3141627311706543, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.657135009765625, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.4198639392852783, 'test/num_examples': 10000, 'score': 5644.199923276901, 'total_duration': 5858.348840475082, 'accumulated_submission_time': 5644.199923276901, 'accumulated_eval_time': 213.22563099861145, 'accumulated_logging_time': 0.33670783042907715}
I0129 21:09:23.743942 139864058095360 logging_writer.py:48] [16526] accumulated_eval_time=213.225631, accumulated_logging_time=0.336708, accumulated_submission_time=5644.199923, global_step=16526, preemption_count=0, score=5644.199923, test/accuracy=0.479300, test/loss=2.419864, test/num_examples=10000, total_duration=5858.348840, train/accuracy=0.666633, train/loss=1.314163, validation/accuracy=0.605020, validation/loss=1.657135, validation/num_examples=50000
I0129 21:09:49.169414 139864066488064 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.5108660459518433, loss=1.9499258995056152
I0129 21:10:23.169295 139864058095360 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.5618408918380737, loss=1.8416284322738647
I0129 21:10:57.084834 139864066488064 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.2865746021270752, loss=1.753293514251709
I0129 21:11:30.969923 139864058095360 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.6084166765213013, loss=1.9813563823699951
I0129 21:12:04.906097 139864066488064 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.6651452779769897, loss=1.793491005897522
I0129 21:12:38.794420 139864058095360 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.5569721460342407, loss=1.8088594675064087
I0129 21:13:12.733382 139864066488064 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4252516031265259, loss=1.8951079845428467
I0129 21:13:46.631792 139864058095360 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.647342562675476, loss=1.802929401397705
I0129 21:14:20.578547 139864066488064 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.571581482887268, loss=1.8582420349121094
I0129 21:14:54.474365 139864058095360 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.5956881046295166, loss=1.8708300590515137
I0129 21:15:28.400680 139864066488064 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.5780467987060547, loss=1.9321129322052002
I0129 21:16:02.299607 139864058095360 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.7271370887756348, loss=1.7936005592346191
I0129 21:16:36.295153 139864066488064 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.7455670833587646, loss=2.0464038848876953
I0129 21:17:10.239242 139864058095360 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.6306978464126587, loss=1.7718907594680786
I0129 21:17:44.186481 139864066488064 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.6618335247039795, loss=1.8516438007354736
I0129 21:17:53.815476 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:18:00.165906 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:18:09.131492 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:18:11.711039 140027215431488 submission_runner.py:408] Time since start: 6386.34s, 	Step: 18030, 	{'train/accuracy': 0.6596978306770325, 'train/loss': 1.3637175559997559, 'validation/accuracy': 0.6011999845504761, 'validation/loss': 1.6728837490081787, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.3898165225982666, 'test/num_examples': 10000, 'score': 6154.205993413925, 'total_duration': 6386.336737394333, 'accumulated_submission_time': 6154.205993413925, 'accumulated_eval_time': 231.12113857269287, 'accumulated_logging_time': 0.36826539039611816}
I0129 21:18:11.731267 139865224107776 logging_writer.py:48] [18030] accumulated_eval_time=231.121139, accumulated_logging_time=0.368265, accumulated_submission_time=6154.205993, global_step=18030, preemption_count=0, score=6154.205993, test/accuracy=0.478500, test/loss=2.389817, test/num_examples=10000, total_duration=6386.336737, train/accuracy=0.659698, train/loss=1.363718, validation/accuracy=0.601200, validation/loss=1.672884, validation/num_examples=50000
I0129 21:18:35.806216 139865232500480 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.6218284368515015, loss=1.8430206775665283
I0129 21:19:09.687783 139865224107776 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.1147727966308594, loss=1.9249281883239746
I0129 21:19:43.628643 139865232500480 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.0309865474700928, loss=1.8779802322387695
I0129 21:20:17.534999 139865224107776 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.520979404449463, loss=1.8057281970977783
I0129 21:20:51.485235 139865232500480 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7782217264175415, loss=1.8705087900161743
I0129 21:21:25.417287 139865224107776 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.5844799280166626, loss=1.8682225942611694
I0129 21:21:59.315498 139865232500480 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.90828537940979, loss=1.8644945621490479
I0129 21:22:33.253429 139865224107776 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.5038052797317505, loss=1.8458893299102783
I0129 21:23:07.257753 139865232500480 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.4208074808120728, loss=1.80222749710083
I0129 21:23:41.141159 139865224107776 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.457931399345398, loss=1.7735909223556519
I0129 21:24:15.081271 139865232500480 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.4099527597427368, loss=1.7956833839416504
I0129 21:24:49.007701 139865224107776 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.1383981704711914, loss=1.904695987701416
I0129 21:25:22.916031 139865232500480 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.5671840906143188, loss=1.8741041421890259
I0129 21:25:56.864376 139865224107776 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.8795220851898193, loss=1.8311452865600586
I0129 21:26:30.766930 139865232500480 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.8662018775939941, loss=1.7875405550003052
I0129 21:26:41.765410 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:26:48.956057 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:26:57.668148 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:27:00.227136 140027215431488 submission_runner.py:408] Time since start: 6914.85s, 	Step: 19534, 	{'train/accuracy': 0.6613320708274841, 'train/loss': 1.3531110286712646, 'validation/accuracy': 0.6044999957084656, 'validation/loss': 1.6473594903945923, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.365743398666382, 'test/num_examples': 10000, 'score': 6664.1773364543915, 'total_duration': 6914.852823257446, 'accumulated_submission_time': 6664.1773364543915, 'accumulated_eval_time': 249.5828001499176, 'accumulated_logging_time': 0.398468017578125}
I0129 21:27:00.251599 139864066488064 logging_writer.py:48] [19534] accumulated_eval_time=249.582800, accumulated_logging_time=0.398468, accumulated_submission_time=6664.177336, global_step=19534, preemption_count=0, score=6664.177336, test/accuracy=0.483400, test/loss=2.365743, test/num_examples=10000, total_duration=6914.852823, train/accuracy=0.661332, train/loss=1.353111, validation/accuracy=0.604500, validation/loss=1.647359, validation/num_examples=50000
I0129 21:27:23.002856 139865088849664 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.618687629699707, loss=1.8259179592132568
I0129 21:27:56.914690 139864066488064 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.5582658052444458, loss=1.7900601625442505
I0129 21:28:30.807096 139865088849664 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.811100959777832, loss=1.7713886499404907
I0129 21:29:04.733930 139864066488064 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.6502336263656616, loss=1.9958586692810059
I0129 21:29:38.667432 139865088849664 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.8240585327148438, loss=1.8493695259094238
I0129 21:30:12.686124 139864066488064 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6935226917266846, loss=1.786277174949646
I0129 21:30:46.571063 139865088849664 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.6406816244125366, loss=1.8266218900680542
I0129 21:31:20.497378 139864066488064 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6565113067626953, loss=1.8605308532714844
I0129 21:31:54.402927 139865088849664 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.8138326406478882, loss=1.9841609001159668
I0129 21:32:28.339715 139864066488064 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.586538314819336, loss=1.9197441339492798
I0129 21:33:02.301964 139865088849664 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6883749961853027, loss=1.9191001653671265
I0129 21:33:36.246769 139864066488064 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.8876043558120728, loss=1.7620537281036377
I0129 21:34:10.161993 139865088849664 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7308531999588013, loss=1.8742727041244507
I0129 21:34:44.088208 139864066488064 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.7042338848114014, loss=1.818154215812683
I0129 21:35:18.034581 139865088849664 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.7582746744155884, loss=1.8569579124450684
I0129 21:35:30.375792 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:35:36.822710 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:35:45.452636 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:35:48.131639 140027215431488 submission_runner.py:408] Time since start: 7442.76s, 	Step: 21038, 	{'train/accuracy': 0.6508888602256775, 'train/loss': 1.4064011573791504, 'validation/accuracy': 0.5990999937057495, 'validation/loss': 1.6903232336044312, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.4506237506866455, 'test/num_examples': 10000, 'score': 7174.233859062195, 'total_duration': 7442.757306337357, 'accumulated_submission_time': 7174.233859062195, 'accumulated_eval_time': 267.3385720252991, 'accumulated_logging_time': 0.4360337257385254}
I0129 21:35:48.153245 139865224107776 logging_writer.py:48] [21038] accumulated_eval_time=267.338572, accumulated_logging_time=0.436034, accumulated_submission_time=7174.233859, global_step=21038, preemption_count=0, score=7174.233859, test/accuracy=0.471300, test/loss=2.450624, test/num_examples=10000, total_duration=7442.757306, train/accuracy=0.650889, train/loss=1.406401, validation/accuracy=0.599100, validation/loss=1.690323, validation/num_examples=50000
I0129 21:36:09.552335 139865232500480 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.6674911975860596, loss=1.7874598503112793
I0129 21:36:43.517934 139865224107776 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.799996256828308, loss=1.7883522510528564
I0129 21:37:17.401558 139865232500480 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.8085215091705322, loss=1.782089114189148
I0129 21:37:51.328234 139865224107776 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.721303105354309, loss=1.812483310699463
I0129 21:38:25.228078 139865232500480 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.689427375793457, loss=1.8479461669921875
I0129 21:38:59.175669 139865224107776 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6978588104248047, loss=1.7729202508926392
I0129 21:39:33.068742 139865232500480 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.1904520988464355, loss=1.7064599990844727
I0129 21:40:06.994374 139865224107776 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.141542673110962, loss=1.777275562286377
I0129 21:40:40.886051 139865232500480 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.12686824798584, loss=1.7578850984573364
I0129 21:41:14.811978 139865224107776 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5689948797225952, loss=1.749719262123108
I0129 21:41:48.694341 139865232500480 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.885790467262268, loss=1.9173505306243896
I0129 21:42:22.612220 139865224107776 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.8797818422317505, loss=1.7536147832870483
I0129 21:42:56.638379 139865232500480 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7224260568618774, loss=1.7294834852218628
I0129 21:43:30.530613 139865224107776 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.5711005926132202, loss=1.7912054061889648
I0129 21:44:04.462792 139865232500480 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.8726123571395874, loss=1.7379167079925537
I0129 21:44:18.190733 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:44:24.648468 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:44:33.604730 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:44:36.163334 140027215431488 submission_runner.py:408] Time since start: 7970.79s, 	Step: 22542, 	{'train/accuracy': 0.6646006107330322, 'train/loss': 1.3387823104858398, 'validation/accuracy': 0.6141799688339233, 'validation/loss': 1.617658257484436, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.391890287399292, 'test/num_examples': 10000, 'score': 7684.207465171814, 'total_duration': 7970.789026260376, 'accumulated_submission_time': 7684.207465171814, 'accumulated_eval_time': 285.31112122535706, 'accumulated_logging_time': 0.46795201301574707}
I0129 21:44:36.185843 139864058095360 logging_writer.py:48] [22542] accumulated_eval_time=285.311121, accumulated_logging_time=0.467952, accumulated_submission_time=7684.207465, global_step=22542, preemption_count=0, score=7684.207465, test/accuracy=0.486000, test/loss=2.391890, test/num_examples=10000, total_duration=7970.789026, train/accuracy=0.664601, train/loss=1.338782, validation/accuracy=0.614180, validation/loss=1.617658, validation/num_examples=50000
I0129 21:44:56.158606 139864066488064 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.8753286600112915, loss=1.8015599250793457
I0129 21:45:30.089642 139864058095360 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.5186721086502075, loss=1.7437942028045654
I0129 21:46:03.988394 139864066488064 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.5715422630310059, loss=1.7801520824432373
I0129 21:46:37.934773 139864058095360 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.5551588535308838, loss=1.781156301498413
I0129 21:47:11.861851 139864066488064 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.9416583776474, loss=1.7304505109786987
I0129 21:47:45.770210 139864058095360 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7437400817871094, loss=1.773235559463501
I0129 21:48:19.689140 139864066488064 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.888023853302002, loss=1.7672194242477417
I0129 21:48:53.580591 139864058095360 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.7641551494598389, loss=1.8867542743682861
I0129 21:49:27.577569 139864066488064 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.593295931816101, loss=1.8279118537902832
I0129 21:50:01.514819 139864058095360 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.8077585697174072, loss=1.9251596927642822
I0129 21:50:35.391541 139864066488064 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.6442419290542603, loss=1.8085495233535767
I0129 21:51:09.321325 139864058095360 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.5718555450439453, loss=1.786124587059021
I0129 21:51:43.190737 139864066488064 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.1470839977264404, loss=1.8986014127731323
I0129 21:52:17.124538 139864058095360 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.7716031074523926, loss=1.7416774034500122
I0129 21:52:51.047679 139864066488064 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.8395918607711792, loss=1.7927038669586182
I0129 21:53:06.464968 140027215431488 spec.py:321] Evaluating on the training split.
I0129 21:53:12.827597 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 21:53:21.494589 140027215431488 spec.py:349] Evaluating on the test split.
I0129 21:53:24.164047 140027215431488 submission_runner.py:408] Time since start: 8498.79s, 	Step: 24047, 	{'train/accuracy': 0.6842713356018066, 'train/loss': 1.2391905784606934, 'validation/accuracy': 0.6031399965286255, 'validation/loss': 1.66865074634552, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.405961513519287, 'test/num_examples': 10000, 'score': 8194.420420408249, 'total_duration': 8498.789740085602, 'accumulated_submission_time': 8194.420420408249, 'accumulated_eval_time': 303.01013803482056, 'accumulated_logging_time': 0.5014698505401611}
I0129 21:53:24.185616 139864049702656 logging_writer.py:48] [24047] accumulated_eval_time=303.010138, accumulated_logging_time=0.501470, accumulated_submission_time=8194.420420, global_step=24047, preemption_count=0, score=8194.420420, test/accuracy=0.480800, test/loss=2.405962, test/num_examples=10000, total_duration=8498.789740, train/accuracy=0.684271, train/loss=1.239191, validation/accuracy=0.603140, validation/loss=1.668651, validation/num_examples=50000
I0129 21:53:42.487773 139865224107776 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.623335361480713, loss=1.7340670824050903
I0129 21:54:16.374105 139864049702656 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.8193858861923218, loss=1.7521331310272217
I0129 21:54:50.276460 139865224107776 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.5314604043960571, loss=1.6862398386001587
I0129 21:55:24.166815 139864049702656 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.7012138366699219, loss=1.7822810411453247
I0129 21:55:58.196002 139865224107776 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6873992681503296, loss=1.8029251098632812
I0129 21:56:32.088546 139864049702656 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.860051155090332, loss=1.8003759384155273
I0129 21:57:05.992256 139865224107776 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.6834070682525635, loss=1.7649791240692139
I0129 21:57:39.921646 139864049702656 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.6388734579086304, loss=1.7308778762817383
I0129 21:58:13.826717 139865224107776 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.883863091468811, loss=1.8152284622192383
I0129 21:58:47.746587 139864049702656 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.7216719388961792, loss=1.6648176908493042
I0129 21:59:21.645454 139865224107776 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.9126033782958984, loss=1.6746176481246948
I0129 21:59:55.533817 139864049702656 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.5748624801635742, loss=1.7528825998306274
I0129 22:00:29.448981 139865224107776 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.077667713165283, loss=1.7260818481445312
I0129 22:01:03.373007 139864049702656 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8468809127807617, loss=1.8034831285476685
I0129 22:01:37.260443 139865224107776 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.674011468887329, loss=1.707473874092102
I0129 22:01:54.386295 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:02:00.813786 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:02:09.711080 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:02:12.359255 140027215431488 submission_runner.py:408] Time since start: 9026.98s, 	Step: 25552, 	{'train/accuracy': 0.6896723508834839, 'train/loss': 1.2271901369094849, 'validation/accuracy': 0.6210399866104126, 'validation/loss': 1.5833314657211304, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.319498300552368, 'test/num_examples': 10000, 'score': 8704.557604551315, 'total_duration': 9026.98484826088, 'accumulated_submission_time': 8704.557604551315, 'accumulated_eval_time': 320.98294949531555, 'accumulated_logging_time': 0.5339152812957764}
I0129 22:02:12.381874 139864066488064 logging_writer.py:48] [25552] accumulated_eval_time=320.982949, accumulated_logging_time=0.533915, accumulated_submission_time=8704.557605, global_step=25552, preemption_count=0, score=8704.557605, test/accuracy=0.494500, test/loss=2.319498, test/num_examples=10000, total_duration=9026.984848, train/accuracy=0.689672, train/loss=1.227190, validation/accuracy=0.621040, validation/loss=1.583331, validation/num_examples=50000
I0129 22:02:28.992333 139865088849664 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.7297927141189575, loss=1.6715584993362427
I0129 22:03:02.980438 139864066488064 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.8589133024215698, loss=1.8165315389633179
I0129 22:03:36.877840 139865088849664 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.7524123191833496, loss=1.7906391620635986
I0129 22:04:10.827927 139864066488064 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.889413833618164, loss=1.8897228240966797
I0129 22:04:44.750427 139865088849664 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8534953594207764, loss=1.7601690292358398
I0129 22:05:18.629854 139864066488064 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.619737148284912, loss=1.7272299528121948
I0129 22:05:52.562494 139865088849664 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.7732806205749512, loss=1.8053542375564575
I0129 22:06:26.440400 139864066488064 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6698795557022095, loss=1.6944994926452637
I0129 22:07:00.338374 139865088849664 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.6876095533370972, loss=1.6476110219955444
I0129 22:07:34.273331 139864066488064 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.0223677158355713, loss=1.6956087350845337
I0129 22:08:08.210134 139865088849664 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.69527006149292, loss=1.6414806842803955
I0129 22:08:42.079436 139864066488064 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8160487413406372, loss=1.7638568878173828
I0129 22:09:16.070005 139865088849664 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.7347594499588013, loss=1.8434700965881348
I0129 22:09:50.031588 139864066488064 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.8650250434875488, loss=1.7845354080200195
I0129 22:10:23.967617 139865088849664 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.743845820426941, loss=1.7382742166519165
I0129 22:10:42.434418 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:10:48.753783 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:10:57.552861 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:11:00.208392 140027215431488 submission_runner.py:408] Time since start: 9554.83s, 	Step: 27056, 	{'train/accuracy': 0.683015763759613, 'train/loss': 1.2480367422103882, 'validation/accuracy': 0.6202600002288818, 'validation/loss': 1.574621319770813, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.30348801612854, 'test/num_examples': 10000, 'score': 9214.546523332596, 'total_duration': 9554.83399772644, 'accumulated_submission_time': 9214.546523332596, 'accumulated_eval_time': 338.75679183006287, 'accumulated_logging_time': 0.567237377166748}
I0129 22:11:00.231580 139864049702656 logging_writer.py:48] [27056] accumulated_eval_time=338.756792, accumulated_logging_time=0.567237, accumulated_submission_time=9214.546523, global_step=27056, preemption_count=0, score=9214.546523, test/accuracy=0.494800, test/loss=2.303488, test/num_examples=10000, total_duration=9554.833998, train/accuracy=0.683016, train/loss=1.248037, validation/accuracy=0.620260, validation/loss=1.574621, validation/num_examples=50000
I0129 22:11:15.506741 139864058095360 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.655296802520752, loss=1.6741132736206055
I0129 22:11:49.391849 139864049702656 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.8158800601959229, loss=1.6656571626663208
I0129 22:12:23.267400 139864058095360 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6903780698776245, loss=1.6453416347503662
I0129 22:12:57.135454 139864049702656 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.9467394351959229, loss=1.7818834781646729
I0129 22:13:31.073358 139864058095360 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.6999976634979248, loss=1.686028242111206
I0129 22:14:04.961541 139864049702656 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.76945960521698, loss=1.748106837272644
I0129 22:14:38.902201 139864058095360 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.7906498908996582, loss=1.7245323657989502
I0129 22:15:12.830687 139864049702656 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.9033854007720947, loss=1.7721078395843506
I0129 22:15:46.847522 139864058095360 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.6390811204910278, loss=1.674802541732788
I0129 22:16:20.739390 139864049702656 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.7487787008285522, loss=1.7084095478057861
I0129 22:16:54.634773 139864058095360 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.8146477937698364, loss=1.756585955619812
I0129 22:17:28.528577 139864049702656 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.0630197525024414, loss=1.7842808961868286
I0129 22:18:02.439129 139864058095360 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.7204573154449463, loss=1.7102285623550415
I0129 22:18:36.340316 139864049702656 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.9774975776672363, loss=1.701014757156372
I0129 22:19:10.280787 139864058095360 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.7513006925582886, loss=1.6356229782104492
I0129 22:19:30.431800 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:19:36.848389 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:19:45.430920 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:19:48.150475 140027215431488 submission_runner.py:408] Time since start: 10082.78s, 	Step: 28561, 	{'train/accuracy': 0.6730906963348389, 'train/loss': 1.2945865392684937, 'validation/accuracy': 0.6128399968147278, 'validation/loss': 1.6189769506454468, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.4035277366638184, 'test/num_examples': 10000, 'score': 9724.683393001556, 'total_duration': 10082.776176929474, 'accumulated_submission_time': 9724.683393001556, 'accumulated_eval_time': 356.4754137992859, 'accumulated_logging_time': 0.6004829406738281}
I0129 22:19:48.173621 139864049702656 logging_writer.py:48] [28561] accumulated_eval_time=356.475414, accumulated_logging_time=0.600483, accumulated_submission_time=9724.683393, global_step=28561, preemption_count=0, score=9724.683393, test/accuracy=0.481200, test/loss=2.403528, test/num_examples=10000, total_duration=10082.776177, train/accuracy=0.673091, train/loss=1.294587, validation/accuracy=0.612840, validation/loss=1.618977, validation/num_examples=50000
I0129 22:20:01.713959 139865224107776 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.878180980682373, loss=1.750646948814392
I0129 22:20:35.595506 139864049702656 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.7855454683303833, loss=1.7288196086883545
I0129 22:21:09.499860 139865224107776 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.3922739028930664, loss=1.7458618879318237
I0129 22:21:43.424671 139864049702656 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.069486618041992, loss=1.6251438856124878
I0129 22:22:17.442001 139865224107776 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.858642816543579, loss=1.7019420862197876
I0129 22:22:51.347125 139864049702656 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.8365238904953003, loss=1.752869963645935
I0129 22:23:25.245313 139865224107776 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.689680576324463, loss=1.7915934324264526
I0129 22:23:59.135735 139864049702656 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.8550809621810913, loss=1.8432207107543945
I0129 22:24:33.036440 139865224107776 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.010842800140381, loss=1.7853238582611084
I0129 22:25:06.919320 139864049702656 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.728928804397583, loss=1.6817582845687866
I0129 22:25:40.866891 139865224107776 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.9274613857269287, loss=1.8067383766174316
I0129 22:26:14.758588 139864049702656 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.5886982679367065, loss=1.736871361732483
I0129 22:26:48.693761 139865224107776 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7972792387008667, loss=1.7800735235214233
I0129 22:27:22.585163 139864049702656 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6822665929794312, loss=1.6828863620758057
I0129 22:27:56.499024 139865224107776 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.670594334602356, loss=1.7078837156295776
I0129 22:28:18.346131 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:28:24.700017 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:28:33.517044 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:28:36.214785 140027215431488 submission_runner.py:408] Time since start: 10610.84s, 	Step: 30066, 	{'train/accuracy': 0.6851283311843872, 'train/loss': 1.2363115549087524, 'validation/accuracy': 0.6257599592208862, 'validation/loss': 1.53518807888031, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.274780035018921, 'test/num_examples': 10000, 'score': 10234.78894495964, 'total_duration': 10610.840461969376, 'accumulated_submission_time': 10234.78894495964, 'accumulated_eval_time': 374.3439898490906, 'accumulated_logging_time': 0.6367506980895996}
I0129 22:28:36.242485 139864058095360 logging_writer.py:48] [30066] accumulated_eval_time=374.343990, accumulated_logging_time=0.636751, accumulated_submission_time=10234.788945, global_step=30066, preemption_count=0, score=10234.788945, test/accuracy=0.498400, test/loss=2.274780, test/num_examples=10000, total_duration=10610.840462, train/accuracy=0.685128, train/loss=1.236312, validation/accuracy=0.625760, validation/loss=1.535188, validation/num_examples=50000
I0129 22:28:48.138696 139864066488064 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.8679909706115723, loss=1.7062020301818848
I0129 22:29:22.125150 139864058095360 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.7401877641677856, loss=1.758325457572937
I0129 22:29:56.013092 139864066488064 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.7184743881225586, loss=1.7502119541168213
I0129 22:30:29.950753 139864058095360 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.5852128267288208, loss=1.7527018785476685
I0129 22:31:03.860897 139864066488064 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.689826488494873, loss=1.8542802333831787
I0129 22:31:37.770740 139864058095360 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.765101671218872, loss=1.6858339309692383
I0129 22:32:11.677377 139864066488064 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.6233428716659546, loss=1.6491808891296387
I0129 22:32:45.594503 139864058095360 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.9083770513534546, loss=1.7855583429336548
I0129 22:33:19.540171 139864066488064 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.773697018623352, loss=1.7772367000579834
I0129 22:33:53.456808 139864058095360 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.6483021974563599, loss=1.6976432800292969
I0129 22:34:27.377223 139864066488064 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.8011846542358398, loss=1.7730928659439087
I0129 22:35:01.315343 139864058095360 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.000584125518799, loss=1.7225180864334106
I0129 22:35:35.318483 139864066488064 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.7648626565933228, loss=1.7523545026779175
I0129 22:36:09.239637 139864058095360 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.678297519683838, loss=1.7091423273086548
I0129 22:36:43.178210 139864066488064 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.580819010734558, loss=1.6783819198608398
I0129 22:37:06.373338 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:37:12.787929 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:37:21.370537 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:37:24.025172 140027215431488 submission_runner.py:408] Time since start: 11138.65s, 	Step: 31570, 	{'train/accuracy': 0.686922013759613, 'train/loss': 1.2258983850479126, 'validation/accuracy': 0.6343799829483032, 'validation/loss': 1.5134379863739014, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.246699333190918, 'test/num_examples': 10000, 'score': 10744.854972839355, 'total_duration': 11138.650857448578, 'accumulated_submission_time': 10744.854972839355, 'accumulated_eval_time': 391.9957549571991, 'accumulated_logging_time': 0.6751341819763184}
I0129 22:37:24.050122 139864041309952 logging_writer.py:48] [31570] accumulated_eval_time=391.995755, accumulated_logging_time=0.675134, accumulated_submission_time=10744.854973, global_step=31570, preemption_count=0, score=10744.854973, test/accuracy=0.503500, test/loss=2.246699, test/num_examples=10000, total_duration=11138.650857, train/accuracy=0.686922, train/loss=1.225898, validation/accuracy=0.634380, validation/loss=1.513438, validation/num_examples=50000
I0129 22:37:34.582612 139864049702656 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.8766545057296753, loss=1.7116541862487793
I0129 22:38:08.475857 139864041309952 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.6689153909683228, loss=1.6695430278778076
I0129 22:38:42.416599 139864049702656 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.9787040948867798, loss=1.7776001691818237
I0129 22:39:16.304535 139864041309952 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.871806025505066, loss=1.676578402519226
I0129 22:39:50.231269 139864049702656 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.738531470298767, loss=1.7483590841293335
I0129 22:40:24.129354 139864041309952 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.6967546939849854, loss=1.6927043199539185
I0129 22:40:58.051373 139864049702656 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.5630240440368652, loss=1.7006945610046387
I0129 22:41:31.928794 139864041309952 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.811637043952942, loss=1.5404828786849976
I0129 22:42:05.861254 139864049702656 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6155648231506348, loss=1.688657522201538
I0129 22:42:39.967109 139864041309952 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.717642903327942, loss=1.8354051113128662
I0129 22:43:13.866351 139864049702656 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.5909899473190308, loss=1.6825251579284668
I0129 22:43:47.787479 139864041309952 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.9369940757751465, loss=1.6889702081680298
I0129 22:44:21.696263 139864049702656 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.910353422164917, loss=1.804451823234558
I0129 22:44:55.651298 139864041309952 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.7726188898086548, loss=1.65517258644104
I0129 22:45:29.599429 139864049702656 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.8615838289260864, loss=1.6833178997039795
I0129 22:45:54.157511 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:46:00.578737 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:46:09.393315 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:46:12.046534 140027215431488 submission_runner.py:408] Time since start: 11666.67s, 	Step: 33074, 	{'train/accuracy': 0.7069116830825806, 'train/loss': 1.1511811017990112, 'validation/accuracy': 0.6170399785041809, 'validation/loss': 1.5918534994125366, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.316349744796753, 'test/num_examples': 10000, 'score': 11254.89800453186, 'total_duration': 11666.672231435776, 'accumulated_submission_time': 11254.89800453186, 'accumulated_eval_time': 409.8847246170044, 'accumulated_logging_time': 0.7104458808898926}
I0129 22:46:12.069680 139865088849664 logging_writer.py:48] [33074] accumulated_eval_time=409.884725, accumulated_logging_time=0.710446, accumulated_submission_time=11254.898005, global_step=33074, preemption_count=0, score=11254.898005, test/accuracy=0.488000, test/loss=2.316350, test/num_examples=10000, total_duration=11666.672231, train/accuracy=0.706912, train/loss=1.151181, validation/accuracy=0.617040, validation/loss=1.591853, validation/num_examples=50000
I0129 22:46:21.231252 139865224107776 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.0718250274658203, loss=1.6227014064788818
I0129 22:46:55.106677 139865088849664 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.7643852233886719, loss=1.5650253295898438
I0129 22:47:29.009180 139865224107776 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.6489943265914917, loss=1.7115086317062378
I0129 22:48:02.915792 139865088849664 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.8043419122695923, loss=1.692278504371643
I0129 22:48:36.828592 139865224107776 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6466217041015625, loss=1.6517287492752075
I0129 22:49:10.869125 139865088849664 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.9081114530563354, loss=1.715142846107483
I0129 22:49:44.789199 139865224107776 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.7092535495758057, loss=1.6932258605957031
I0129 22:50:18.704229 139865088849664 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.029863119125366, loss=1.8707441091537476
I0129 22:50:52.623214 139865224107776 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.7419224977493286, loss=1.702835202217102
I0129 22:51:26.550625 139865088849664 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7552045583724976, loss=1.7387412786483765
I0129 22:52:00.463063 139865224107776 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.728471040725708, loss=1.7178359031677246
I0129 22:52:34.404604 139865088849664 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.7403098344802856, loss=1.7449573278427124
I0129 22:53:08.306019 139865224107776 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.6955872774124146, loss=1.68239164352417
I0129 22:53:42.220552 139865088849664 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.998246431350708, loss=1.7127217054367065
I0129 22:54:16.121891 139865224107776 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.722916841506958, loss=1.69436776638031
I0129 22:54:42.378683 140027215431488 spec.py:321] Evaluating on the training split.
I0129 22:54:48.703944 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 22:54:57.280215 140027215431488 spec.py:349] Evaluating on the test split.
I0129 22:54:59.972146 140027215431488 submission_runner.py:408] Time since start: 12194.60s, 	Step: 34579, 	{'train/accuracy': 0.6890744566917419, 'train/loss': 1.2211211919784546, 'validation/accuracy': 0.6181399822235107, 'validation/loss': 1.5859718322753906, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.303445816040039, 'test/num_examples': 10000, 'score': 11765.143156051636, 'total_duration': 12194.597846269608, 'accumulated_submission_time': 11765.143156051636, 'accumulated_eval_time': 427.47814416885376, 'accumulated_logging_time': 0.7436776161193848}
I0129 22:54:59.996415 139864058095360 logging_writer.py:48] [34579] accumulated_eval_time=427.478144, accumulated_logging_time=0.743678, accumulated_submission_time=11765.143156, global_step=34579, preemption_count=0, score=11765.143156, test/accuracy=0.494500, test/loss=2.303446, test/num_examples=10000, total_duration=12194.597846, train/accuracy=0.689074, train/loss=1.221121, validation/accuracy=0.618140, validation/loss=1.585972, validation/num_examples=50000
I0129 22:55:07.467655 139864066488064 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.6923595666885376, loss=1.5745939016342163
I0129 22:55:41.405842 139864058095360 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.034834861755371, loss=1.6623775959014893
I0129 22:56:15.277770 139864066488064 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.8208818435668945, loss=1.6202524900436401
I0129 22:56:49.235216 139864058095360 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.6534076929092407, loss=1.628962755203247
I0129 22:57:23.134134 139864066488064 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.8425848484039307, loss=1.6694717407226562
I0129 22:57:57.069001 139864058095360 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.5798287391662598, loss=1.453568935394287
I0129 22:58:31.016729 139864066488064 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7994420528411865, loss=1.619147539138794
I0129 22:59:04.910785 139864058095360 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.7955715656280518, loss=1.7010397911071777
I0129 22:59:38.831259 139864066488064 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.5874724388122559, loss=1.6109942197799683
I0129 23:00:12.721321 139864058095360 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.7678593397140503, loss=1.6843403577804565
I0129 23:00:46.653420 139864066488064 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.6626008749008179, loss=1.6373084783554077
I0129 23:01:20.548220 139864058095360 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.7687046527862549, loss=1.6763160228729248
I0129 23:01:54.487410 139864066488064 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.840576410293579, loss=1.700218677520752
I0129 23:02:28.531628 139864058095360 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.9012045860290527, loss=1.7301294803619385
I0129 23:03:02.467110 139864066488064 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.7906684875488281, loss=1.6323070526123047
I0129 23:03:30.072815 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:03:36.480579 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:03:45.345765 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:03:48.021219 140027215431488 submission_runner.py:408] Time since start: 12722.65s, 	Step: 36083, 	{'train/accuracy': 0.6932995915412903, 'train/loss': 1.1908233165740967, 'validation/accuracy': 0.6310399770736694, 'validation/loss': 1.5338072776794434, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.258314847946167, 'test/num_examples': 10000, 'score': 12275.155267238617, 'total_duration': 12722.646920681, 'accumulated_submission_time': 12275.155267238617, 'accumulated_eval_time': 445.4265134334564, 'accumulated_logging_time': 0.7787868976593018}
I0129 23:03:48.045167 139864058095360 logging_writer.py:48] [36083] accumulated_eval_time=445.426513, accumulated_logging_time=0.778787, accumulated_submission_time=12275.155267, global_step=36083, preemption_count=0, score=12275.155267, test/accuracy=0.516200, test/loss=2.258315, test/num_examples=10000, total_duration=12722.646921, train/accuracy=0.693300, train/loss=1.190823, validation/accuracy=0.631040, validation/loss=1.533807, validation/num_examples=50000
I0129 23:03:54.150517 139864066488064 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.0514276027679443, loss=1.7233045101165771
I0129 23:04:28.005846 139864058095360 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6121100187301636, loss=1.6264935731887817
I0129 23:05:01.925418 139864066488064 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.9582349061965942, loss=1.6492631435394287
I0129 23:05:35.795325 139864058095360 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.802514910697937, loss=1.7117944955825806
I0129 23:06:09.716463 139864066488064 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.9775398969650269, loss=1.667684555053711
I0129 23:06:43.615647 139864058095360 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.8417515754699707, loss=1.6700749397277832
I0129 23:07:17.555518 139864066488064 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.8189527988433838, loss=1.614838719367981
I0129 23:07:51.427637 139864058095360 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.9233402013778687, loss=1.737421989440918
I0129 23:08:25.360846 139864066488064 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.8952194452285767, loss=1.585587739944458
I0129 23:08:59.320068 139864058095360 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.9790735244750977, loss=1.8395030498504639
I0129 23:09:33.251776 139864066488064 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.655829668045044, loss=1.7461466789245605
I0129 23:10:07.143608 139864058095360 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.6991008520126343, loss=1.707108497619629
I0129 23:10:41.073131 139864066488064 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.7766292095184326, loss=1.7295935153961182
I0129 23:11:14.973202 139864058095360 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9605841636657715, loss=1.7420045137405396
I0129 23:11:48.900856 139864066488064 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.8259319067001343, loss=1.694456934928894
I0129 23:12:18.225731 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:12:24.574661 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:12:33.191594 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:12:36.514875 140027215431488 submission_runner.py:408] Time since start: 13251.14s, 	Step: 37588, 	{'train/accuracy': 0.6861447691917419, 'train/loss': 1.233417272567749, 'validation/accuracy': 0.6261999607086182, 'validation/loss': 1.5477724075317383, 'validation/num_examples': 50000, 'test/accuracy': 0.49640002846717834, 'test/loss': 2.281170606613159, 'test/num_examples': 10000, 'score': 12785.271156549454, 'total_duration': 13251.140579938889, 'accumulated_submission_time': 12785.271156549454, 'accumulated_eval_time': 463.7156083583832, 'accumulated_logging_time': 0.8150899410247803}
I0129 23:12:36.535206 139865232500480 logging_writer.py:48] [37588] accumulated_eval_time=463.715608, accumulated_logging_time=0.815090, accumulated_submission_time=12785.271157, global_step=37588, preemption_count=0, score=12785.271157, test/accuracy=0.496400, test/loss=2.281171, test/num_examples=10000, total_duration=13251.140580, train/accuracy=0.686145, train/loss=1.233417, validation/accuracy=0.626200, validation/loss=1.547772, validation/num_examples=50000
I0129 23:12:40.937790 139865240893184 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.740867257118225, loss=1.688117504119873
I0129 23:13:14.804414 139865232500480 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6409415006637573, loss=1.6753829717636108
I0129 23:13:48.634833 139865240893184 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5676634311676025, loss=1.674674391746521
I0129 23:14:22.550821 139865232500480 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.7807555198669434, loss=1.679368257522583
I0129 23:14:56.456934 139865240893184 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.7064599990844727, loss=1.6294108629226685
I0129 23:15:30.457376 139865232500480 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.6341972351074219, loss=1.6920440196990967
I0129 23:16:04.319109 139865240893184 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.6207327842712402, loss=1.6418840885162354
I0129 23:16:38.227170 139865232500480 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.7378071546554565, loss=1.6032406091690063
I0129 23:17:12.156790 139865240893184 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.8972457647323608, loss=1.6272042989730835
I0129 23:17:46.047150 139865232500480 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.0026018619537354, loss=1.6337229013442993
I0129 23:18:19.966784 139865240893184 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8567336797714233, loss=1.6346385478973389
I0129 23:18:53.886341 139865232500480 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.8502000570297241, loss=1.6877107620239258
I0129 23:19:27.797971 139865240893184 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.744429349899292, loss=1.7505269050598145
I0129 23:20:01.673032 139865232500480 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.7426406145095825, loss=1.5624444484710693
I0129 23:20:35.588047 139865240893184 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.7964218854904175, loss=1.7617809772491455
I0129 23:21:06.568243 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:21:13.002566 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:21:21.630307 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:21:24.290931 140027215431488 submission_runner.py:408] Time since start: 13778.92s, 	Step: 39093, 	{'train/accuracy': 0.7010124325752258, 'train/loss': 1.1792408227920532, 'validation/accuracy': 0.64028000831604, 'validation/loss': 1.4864907264709473, 'validation/num_examples': 50000, 'test/accuracy': 0.5124000310897827, 'test/loss': 2.243726968765259, 'test/num_examples': 10000, 'score': 13295.242151021957, 'total_duration': 13778.91663146019, 'accumulated_submission_time': 13295.242151021957, 'accumulated_eval_time': 481.4382588863373, 'accumulated_logging_time': 0.8444697856903076}
I0129 23:21:24.315896 139864058095360 logging_writer.py:48] [39093] accumulated_eval_time=481.438259, accumulated_logging_time=0.844470, accumulated_submission_time=13295.242151, global_step=39093, preemption_count=0, score=13295.242151, test/accuracy=0.512400, test/loss=2.243727, test/num_examples=10000, total_duration=13778.916631, train/accuracy=0.701012, train/loss=1.179241, validation/accuracy=0.640280, validation/loss=1.486491, validation/num_examples=50000
I0129 23:21:27.060057 139864066488064 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.810059905052185, loss=1.664344072341919
I0129 23:22:01.041309 139864058095360 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.572524905204773, loss=1.6015362739562988
I0129 23:22:34.933563 139864066488064 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.0574703216552734, loss=1.7369529008865356
I0129 23:23:08.833955 139864058095360 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.002192735671997, loss=1.5795354843139648
I0129 23:23:42.715364 139864066488064 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.904947280883789, loss=1.6503429412841797
I0129 23:24:16.607681 139864058095360 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7686517238616943, loss=1.548393726348877
I0129 23:24:50.493878 139864066488064 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7842564582824707, loss=1.654355764389038
I0129 23:25:24.396049 139864058095360 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.8078380823135376, loss=1.622566819190979
I0129 23:25:58.288288 139864066488064 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.755412220954895, loss=1.6313287019729614
I0129 23:26:32.194869 139864058095360 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.7854902744293213, loss=1.6599769592285156
I0129 23:27:06.064409 139864066488064 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.8183510303497314, loss=1.6107420921325684
I0129 23:27:39.981610 139864058095360 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.726760983467102, loss=1.6640472412109375
I0129 23:28:13.855599 139864066488064 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.7957897186279297, loss=1.622504711151123
I0129 23:28:47.895740 139864058095360 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.9614317417144775, loss=1.7297402620315552
I0129 23:29:21.809482 139864066488064 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.7342356443405151, loss=1.6337778568267822
I0129 23:29:54.513494 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:30:00.860285 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:30:09.750480 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:30:12.399607 140027215431488 submission_runner.py:408] Time since start: 14307.03s, 	Step: 40598, 	{'train/accuracy': 0.6853276491165161, 'train/loss': 1.2317752838134766, 'validation/accuracy': 0.6255800127983093, 'validation/loss': 1.5425355434417725, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.290236473083496, 'test/num_examples': 10000, 'score': 13805.37430691719, 'total_duration': 14307.025272130966, 'accumulated_submission_time': 13805.37430691719, 'accumulated_eval_time': 499.3242871761322, 'accumulated_logging_time': 0.8821501731872559}
I0129 23:30:12.429299 139865232500480 logging_writer.py:48] [40598] accumulated_eval_time=499.324287, accumulated_logging_time=0.882150, accumulated_submission_time=13805.374307, global_step=40598, preemption_count=0, score=13805.374307, test/accuracy=0.497600, test/loss=2.290236, test/num_examples=10000, total_duration=14307.025272, train/accuracy=0.685328, train/loss=1.231775, validation/accuracy=0.625580, validation/loss=1.542536, validation/num_examples=50000
I0129 23:30:13.453592 139865240893184 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.9652053117752075, loss=1.7156853675842285
I0129 23:30:47.361171 139865232500480 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.7319576740264893, loss=1.6712799072265625
I0129 23:31:21.221277 139865240893184 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.0646917819976807, loss=1.7433912754058838
I0129 23:31:55.105404 139865232500480 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.8553755283355713, loss=1.6901557445526123
I0129 23:32:29.010765 139865240893184 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.9452896118164062, loss=1.5624732971191406
I0129 23:33:02.924735 139865232500480 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.604770302772522, loss=1.6999191045761108
I0129 23:33:36.786606 139865240893184 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.0576255321502686, loss=1.6827462911605835
I0129 23:34:10.642685 139865232500480 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.8234469890594482, loss=1.7142382860183716
I0129 23:34:44.561276 139865240893184 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.200201988220215, loss=1.79299795627594
I0129 23:35:18.540138 139865232500480 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.5944242477416992, loss=1.6132889986038208
I0129 23:35:52.407679 139865240893184 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.7958216667175293, loss=1.7191014289855957
I0129 23:36:26.252202 139865232500480 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.924074411392212, loss=1.713850736618042
I0129 23:37:00.172735 139865240893184 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.680034875869751, loss=1.5910357236862183
I0129 23:37:34.065480 139865232500480 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8745259046554565, loss=1.7691417932510376
I0129 23:38:07.941310 139865240893184 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9405401945114136, loss=1.6833781003952026
I0129 23:38:41.859112 139865232500480 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.788496732711792, loss=1.660639762878418
I0129 23:38:42.676351 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:38:49.051503 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:38:57.845584 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:39:00.499080 140027215431488 submission_runner.py:408] Time since start: 14835.12s, 	Step: 42104, 	{'train/accuracy': 0.7300103306770325, 'train/loss': 1.0265074968338013, 'validation/accuracy': 0.6344799995422363, 'validation/loss': 1.5053465366363525, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.22822904586792, 'test/num_examples': 10000, 'score': 14315.556084632874, 'total_duration': 14835.124782562256, 'accumulated_submission_time': 14315.556084632874, 'accumulated_eval_time': 517.1469714641571, 'accumulated_logging_time': 0.9237699508666992}
I0129 23:39:00.524897 139864058095360 logging_writer.py:48] [42104] accumulated_eval_time=517.146971, accumulated_logging_time=0.923770, accumulated_submission_time=14315.556085, global_step=42104, preemption_count=0, score=14315.556085, test/accuracy=0.510900, test/loss=2.228229, test/num_examples=10000, total_duration=14835.124783, train/accuracy=0.730010, train/loss=1.026507, validation/accuracy=0.634480, validation/loss=1.505347, validation/num_examples=50000
I0129 23:39:33.406960 139864066488064 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.937713861465454, loss=1.782282829284668
I0129 23:40:07.295238 139864058095360 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6800506114959717, loss=1.6181262731552124
I0129 23:40:41.168099 139864066488064 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.2571773529052734, loss=1.7078568935394287
I0129 23:41:15.056821 139864058095360 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.7576724290847778, loss=1.6000365018844604
I0129 23:41:49.096793 139864066488064 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.9111502170562744, loss=1.5800873041152954
I0129 23:42:22.963903 139864058095360 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.178964376449585, loss=1.6630260944366455
I0129 23:42:56.870599 139864066488064 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.638665795326233, loss=1.6657490730285645
I0129 23:43:30.759785 139864058095360 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.8440619707107544, loss=1.6796001195907593
I0129 23:44:04.650855 139864066488064 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.7860878705978394, loss=1.5958311557769775
I0129 23:44:38.549113 139864058095360 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.704346776008606, loss=1.6383217573165894
I0129 23:45:12.435812 139864066488064 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7224093675613403, loss=1.5320451259613037
I0129 23:45:46.345503 139864058095360 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6582523584365845, loss=1.6080578565597534
I0129 23:46:20.196208 139864066488064 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.7459858655929565, loss=1.7616276741027832
I0129 23:46:54.052084 139864058095360 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.704919457435608, loss=1.6548314094543457
I0129 23:47:27.989500 139864066488064 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.8839040994644165, loss=1.4940605163574219
I0129 23:47:30.514807 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:47:36.823414 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:47:45.345040 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:47:48.194144 140027215431488 submission_runner.py:408] Time since start: 15362.82s, 	Step: 43609, 	{'train/accuracy': 0.7144451141357422, 'train/loss': 1.1129549741744995, 'validation/accuracy': 0.638700008392334, 'validation/loss': 1.4881618022918701, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.2399520874023438, 'test/num_examples': 10000, 'score': 14825.481731653214, 'total_duration': 15362.819847106934, 'accumulated_submission_time': 14825.481731653214, 'accumulated_eval_time': 534.8262553215027, 'accumulated_logging_time': 0.9605207443237305}
I0129 23:47:48.219747 139864049702656 logging_writer.py:48] [43609] accumulated_eval_time=534.826255, accumulated_logging_time=0.960521, accumulated_submission_time=14825.481732, global_step=43609, preemption_count=0, score=14825.481732, test/accuracy=0.507800, test/loss=2.239952, test/num_examples=10000, total_duration=15362.819847, train/accuracy=0.714445, train/loss=1.112955, validation/accuracy=0.638700, validation/loss=1.488162, validation/num_examples=50000
I0129 23:48:19.548001 139864058095360 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.6995024681091309, loss=1.682908535003662
I0129 23:48:53.413465 139864049702656 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.9443466663360596, loss=1.8136671781539917
I0129 23:49:27.295369 139864058095360 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.8069568872451782, loss=1.6996341943740845
I0129 23:50:05.227423 139864049702656 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7221550941467285, loss=1.6822627782821655
I0129 23:50:41.335152 139864058095360 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.8295583724975586, loss=1.6947282552719116
I0129 23:51:15.205742 139864049702656 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.1416311264038086, loss=1.6766972541809082
I0129 23:51:49.066851 139864058095360 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.7864538431167603, loss=1.613962173461914
I0129 23:52:22.966650 139864049702656 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.7747477293014526, loss=1.7078872919082642
I0129 23:52:56.859943 139864058095360 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7016854286193848, loss=1.594889521598816
I0129 23:53:30.766081 139864049702656 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8920483589172363, loss=1.664677381515503
I0129 23:54:04.657284 139864058095360 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.8396213054656982, loss=1.5601743459701538
I0129 23:54:38.556817 139864049702656 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.8965154886245728, loss=1.6130080223083496
I0129 23:55:12.516920 139864058095360 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7114025354385376, loss=1.6129413843154907
I0129 23:55:46.419385 139864049702656 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.8072617053985596, loss=1.6617558002471924
I0129 23:56:18.434844 140027215431488 spec.py:321] Evaluating on the training split.
I0129 23:56:24.849839 140027215431488 spec.py:333] Evaluating on the validation split.
I0129 23:56:33.403333 140027215431488 spec.py:349] Evaluating on the test split.
I0129 23:56:36.108002 140027215431488 submission_runner.py:408] Time since start: 15890.73s, 	Step: 45096, 	{'train/accuracy': 0.7016900181770325, 'train/loss': 1.154717206954956, 'validation/accuracy': 0.6298800110816956, 'validation/loss': 1.5299546718597412, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.263049840927124, 'test/num_examples': 10000, 'score': 15335.629780054092, 'total_duration': 15890.73356628418, 'accumulated_submission_time': 15335.629780054092, 'accumulated_eval_time': 552.4992291927338, 'accumulated_logging_time': 0.9993839263916016}
I0129 23:56:36.141369 139865088849664 logging_writer.py:48] [45096] accumulated_eval_time=552.499229, accumulated_logging_time=0.999384, accumulated_submission_time=15335.629780, global_step=45096, preemption_count=0, score=15335.629780, test/accuracy=0.508000, test/loss=2.263050, test/num_examples=10000, total_duration=15890.733566, train/accuracy=0.701690, train/loss=1.154717, validation/accuracy=0.629880, validation/loss=1.529955, validation/num_examples=50000
I0129 23:56:38.973465 139865224107776 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.863305926322937, loss=1.6557748317718506
I0129 23:57:12.870014 139865088849664 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.019355535507202, loss=1.695354700088501
I0129 23:57:46.737925 139865224107776 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8564958572387695, loss=1.6378098726272583
I0129 23:58:20.647067 139865088849664 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.46793794631958, loss=1.7751708030700684
I0129 23:58:54.545109 139865224107776 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7802048921585083, loss=1.6475121974945068
I0129 23:59:28.475129 139865088849664 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.9362932443618774, loss=1.6030364036560059
I0130 00:00:02.383715 139865224107776 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.746216893196106, loss=1.5608327388763428
I0130 00:00:36.323334 139865088849664 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.814798355102539, loss=1.6592793464660645
I0130 00:01:10.191796 139865224107776 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7043061256408691, loss=1.4849013090133667
I0130 00:01:44.181412 139865088849664 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.2485861778259277, loss=1.6540484428405762
I0130 00:02:18.092820 139865224107776 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8524094820022583, loss=1.5790106058120728
I0130 00:02:51.940536 139865088849664 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.9263960123062134, loss=1.8265933990478516
I0130 00:03:25.838929 139865224107776 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.874237298965454, loss=1.709341287612915
I0130 00:03:59.751745 139865088849664 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.7067794799804688, loss=1.551025390625
I0130 00:04:33.639894 139865224107776 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.8498682975769043, loss=1.6602684259414673
I0130 00:05:06.337317 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:05:12.732797 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:05:21.678402 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:05:24.257233 140027215431488 submission_runner.py:408] Time since start: 16418.88s, 	Step: 46598, 	{'train/accuracy': 0.7108976244926453, 'train/loss': 1.125377893447876, 'validation/accuracy': 0.6459800004959106, 'validation/loss': 1.4500062465667725, 'validation/num_examples': 50000, 'test/accuracy': 0.5160000324249268, 'test/loss': 2.1649107933044434, 'test/num_examples': 10000, 'score': 15844.633338212967, 'total_duration': 16418.882928848267, 'accumulated_submission_time': 15844.633338212967, 'accumulated_eval_time': 570.4190890789032, 'accumulated_logging_time': 2.1719322204589844}
I0130 00:05:24.283945 139864058095360 logging_writer.py:48] [46598] accumulated_eval_time=570.419089, accumulated_logging_time=2.171932, accumulated_submission_time=15844.633338, global_step=46598, preemption_count=0, score=15844.633338, test/accuracy=0.516000, test/loss=2.164911, test/num_examples=10000, total_duration=16418.882929, train/accuracy=0.710898, train/loss=1.125378, validation/accuracy=0.645980, validation/loss=1.450006, validation/num_examples=50000
I0130 00:05:25.308960 139864066488064 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.8542678356170654, loss=1.6918878555297852
I0130 00:05:59.176223 139864058095360 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.6710487604141235, loss=1.6397069692611694
I0130 00:06:33.038606 139864066488064 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.8166948556900024, loss=1.6100143194198608
I0130 00:07:06.925662 139864058095360 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.9415135383605957, loss=1.652998924255371
I0130 00:07:40.841262 139864066488064 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.9580596685409546, loss=1.6623519659042358
I0130 00:08:14.835472 139864058095360 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.0833358764648438, loss=1.6682679653167725
I0130 00:08:48.762084 139864066488064 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.8709405660629272, loss=1.5853558778762817
I0130 00:09:22.645009 139864058095360 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.816325306892395, loss=1.621378779411316
I0130 00:09:56.583655 139864066488064 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7515836954116821, loss=1.662335753440857
I0130 00:10:30.467194 139864058095360 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.80351984500885, loss=1.5532888174057007
I0130 00:11:04.404632 139864066488064 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.8888119459152222, loss=1.6569191217422485
I0130 00:11:38.286633 139864058095360 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.8308964967727661, loss=1.6883522272109985
I0130 00:12:12.238389 139864066488064 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.9734035730361938, loss=1.780745267868042
I0130 00:12:46.140438 139864058095360 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.9824951887130737, loss=1.6765774488449097
I0130 00:13:20.062727 139864066488064 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.7613598108291626, loss=1.6480284929275513
I0130 00:13:53.944013 139864058095360 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.9634473323822021, loss=1.7252907752990723
I0130 00:13:54.432480 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:14:00.761907 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:14:09.560471 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:14:12.123321 140027215431488 submission_runner.py:408] Time since start: 16946.75s, 	Step: 48103, 	{'train/accuracy': 0.6979033946990967, 'train/loss': 1.1757428646087646, 'validation/accuracy': 0.6360799670219421, 'validation/loss': 1.504656195640564, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.2049496173858643, 'test/num_examples': 10000, 'score': 16354.717643976212, 'total_duration': 16946.749007940292, 'accumulated_submission_time': 16354.717643976212, 'accumulated_eval_time': 588.1098670959473, 'accumulated_logging_time': 2.20971941947937}
I0130 00:14:12.151638 139865240893184 logging_writer.py:48] [48103] accumulated_eval_time=588.109867, accumulated_logging_time=2.209719, accumulated_submission_time=16354.717644, global_step=48103, preemption_count=0, score=16354.717644, test/accuracy=0.511400, test/loss=2.204950, test/num_examples=10000, total_duration=16946.749008, train/accuracy=0.697903, train/loss=1.175743, validation/accuracy=0.636080, validation/loss=1.504656, validation/num_examples=50000
I0130 00:14:45.441938 139865760950016 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7839717864990234, loss=1.47588050365448
I0130 00:15:19.252505 139865240893184 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.9457606077194214, loss=1.6206631660461426
I0130 00:15:53.134876 139865760950016 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.9050815105438232, loss=1.6661031246185303
I0130 00:16:27.015132 139865240893184 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.2502551078796387, loss=1.4908921718597412
I0130 00:17:00.925125 139865760950016 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.077650547027588, loss=1.7035043239593506
I0130 00:17:34.834627 139865240893184 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.7935715913772583, loss=1.5093576908111572
I0130 00:18:08.752172 139865760950016 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.8416483402252197, loss=1.669646978378296
I0130 00:18:42.644211 139865240893184 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7300516366958618, loss=1.509913682937622
I0130 00:19:16.564830 139865760950016 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.9032840728759766, loss=1.7070938348770142
I0130 00:19:50.464318 139865240893184 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.9908541440963745, loss=1.7380619049072266
I0130 00:20:24.371108 139865760950016 logging_writer.py:48] [49200] global_step=49200, grad_norm=2.1530749797821045, loss=1.5277173519134521
I0130 00:20:58.257613 139865240893184 logging_writer.py:48] [49300] global_step=49300, grad_norm=2.2566640377044678, loss=1.6514629125595093
I0130 00:21:32.262419 139865760950016 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.8322675228118896, loss=1.5387260913848877
I0130 00:22:06.091864 139865240893184 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.940328598022461, loss=1.760655164718628
I0130 00:22:39.986863 139865760950016 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.9957045316696167, loss=1.5708407163619995
I0130 00:22:42.156823 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:22:48.474872 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:22:57.090689 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:22:59.790478 140027215431488 submission_runner.py:408] Time since start: 17474.42s, 	Step: 49608, 	{'train/accuracy': 0.7112563848495483, 'train/loss': 1.128156304359436, 'validation/accuracy': 0.6507200002670288, 'validation/loss': 1.4424986839294434, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.1950442790985107, 'test/num_examples': 10000, 'score': 16864.65782546997, 'total_duration': 17474.416180849075, 'accumulated_submission_time': 16864.65782546997, 'accumulated_eval_time': 605.7434694766998, 'accumulated_logging_time': 2.248587131500244}
I0130 00:22:59.818278 139864058095360 logging_writer.py:48] [49608] accumulated_eval_time=605.743469, accumulated_logging_time=2.248587, accumulated_submission_time=16864.657825, global_step=49608, preemption_count=0, score=16864.657825, test/accuracy=0.520900, test/loss=2.195044, test/num_examples=10000, total_duration=17474.416181, train/accuracy=0.711256, train/loss=1.128156, validation/accuracy=0.650720, validation/loss=1.442499, validation/num_examples=50000
I0130 00:23:31.295282 139864066488064 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.865441918373108, loss=1.6659015417099
I0130 00:24:05.148190 139864058095360 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.112255096435547, loss=1.5638216733932495
I0130 00:24:39.002449 139864066488064 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.0724353790283203, loss=1.6103804111480713
I0130 00:25:12.901612 139864058095360 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.8529245853424072, loss=1.6129119396209717
I0130 00:25:46.782139 139864066488064 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.7538509368896484, loss=1.6299678087234497
I0130 00:26:20.644219 139864058095360 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.9867476224899292, loss=1.7359830141067505
I0130 00:26:54.550615 139864066488064 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7230983972549438, loss=1.6019830703735352
I0130 00:27:28.402843 139864058095360 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.810502290725708, loss=1.5862438678741455
I0130 00:28:02.440765 139864066488064 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.9620620012283325, loss=1.6322367191314697
I0130 00:28:36.347802 139864058095360 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.0159595012664795, loss=1.5126593112945557
I0130 00:29:10.215358 139864066488064 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.0221028327941895, loss=1.5628952980041504
I0130 00:29:44.062152 139864058095360 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.906235694885254, loss=1.6567606925964355
I0130 00:30:17.907808 139864066488064 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.7435424327850342, loss=1.4476854801177979
I0130 00:30:51.783159 139864058095360 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.767398715019226, loss=1.5565106868743896
I0130 00:31:25.650701 139864066488064 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.6291053295135498, loss=1.5653069019317627
I0130 00:31:29.861319 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:31:36.337106 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:31:44.989654 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:31:47.742633 140027215431488 submission_runner.py:408] Time since start: 18002.37s, 	Step: 51114, 	{'train/accuracy': 0.7567960619926453, 'train/loss': 0.9291717410087585, 'validation/accuracy': 0.64301997423172, 'validation/loss': 1.4702551364898682, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.2189431190490723, 'test/num_examples': 10000, 'score': 17374.636246919632, 'total_duration': 18002.36827158928, 'accumulated_submission_time': 17374.636246919632, 'accumulated_eval_time': 623.6246781349182, 'accumulated_logging_time': 2.2868897914886475}
I0130 00:31:47.775429 139865240893184 logging_writer.py:48] [51114] accumulated_eval_time=623.624678, accumulated_logging_time=2.286890, accumulated_submission_time=17374.636247, global_step=51114, preemption_count=0, score=17374.636247, test/accuracy=0.517200, test/loss=2.218943, test/num_examples=10000, total_duration=18002.368272, train/accuracy=0.756796, train/loss=0.929172, validation/accuracy=0.643020, validation/loss=1.470255, validation/num_examples=50000
I0130 00:32:17.240351 139865760950016 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.0067615509033203, loss=1.6066489219665527
I0130 00:32:51.123382 139865240893184 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9551137685775757, loss=1.731292486190796
I0130 00:33:24.977679 139865760950016 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.693948745727539, loss=1.6539528369903564
I0130 00:33:58.850074 139865240893184 logging_writer.py:48] [51500] global_step=51500, grad_norm=2.4513635635375977, loss=1.7541800737380981
I0130 00:34:32.847588 139865760950016 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.8477617502212524, loss=1.6414093971252441
I0130 00:35:06.750801 139865240893184 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8927510976791382, loss=1.6142494678497314
I0130 00:35:40.622373 139865760950016 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8264530897140503, loss=1.5864202976226807
I0130 00:36:14.537222 139865240893184 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.722344994544983, loss=1.6165924072265625
I0130 00:36:48.401702 139865760950016 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.8276457786560059, loss=1.6024891138076782
I0130 00:37:22.321245 139865240893184 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8688650131225586, loss=1.5818767547607422
I0130 00:37:56.186423 139865760950016 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8303875923156738, loss=1.5184333324432373
I0130 00:38:30.090518 139865240893184 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.3437840938568115, loss=1.5128926038742065
I0130 00:39:03.985623 139865760950016 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.7982914447784424, loss=1.6150091886520386
I0130 00:39:37.909651 139865240893184 logging_writer.py:48] [52500] global_step=52500, grad_norm=2.316246271133423, loss=1.6112840175628662
I0130 00:40:11.778876 139865760950016 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.732966661453247, loss=1.5874277353286743
I0130 00:40:18.018465 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:40:24.451106 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:40:33.273286 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:40:36.432744 140027215431488 submission_runner.py:408] Time since start: 18531.06s, 	Step: 52620, 	{'train/accuracy': 0.7090441584587097, 'train/loss': 1.130107045173645, 'validation/accuracy': 0.6304599642753601, 'validation/loss': 1.5274958610534668, 'validation/num_examples': 50000, 'test/accuracy': 0.49820002913475037, 'test/loss': 2.276724100112915, 'test/num_examples': 10000, 'score': 17884.815304279327, 'total_duration': 18531.05840587616, 'accumulated_submission_time': 17884.815304279327, 'accumulated_eval_time': 642.0388751029968, 'accumulated_logging_time': 2.3303985595703125}
I0130 00:40:36.461911 139864066488064 logging_writer.py:48] [52620] accumulated_eval_time=642.038875, accumulated_logging_time=2.330399, accumulated_submission_time=17884.815304, global_step=52620, preemption_count=0, score=17884.815304, test/accuracy=0.498200, test/loss=2.276724, test/num_examples=10000, total_duration=18531.058406, train/accuracy=0.709044, train/loss=1.130107, validation/accuracy=0.630460, validation/loss=1.527496, validation/num_examples=50000
I0130 00:41:03.981785 139865088849664 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.7196930646896362, loss=1.6201729774475098
I0130 00:41:37.862770 139864066488064 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8030959367752075, loss=1.5743322372436523
I0130 00:42:11.733393 139865088849664 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.1683003902435303, loss=1.5549906492233276
I0130 00:42:45.625457 139864066488064 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9137986898422241, loss=1.5717651844024658
I0130 00:43:19.517649 139865088849664 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.8415640592575073, loss=1.5419167280197144
I0130 00:43:53.384750 139864066488064 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.0403409004211426, loss=1.6770254373550415
I0130 00:44:27.280077 139865088849664 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.9114452600479126, loss=1.6489572525024414
I0130 00:45:01.152553 139864066488064 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.9235423803329468, loss=1.5909929275512695
I0130 00:45:34.999648 139865088849664 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.227088689804077, loss=1.650043249130249
I0130 00:46:08.905833 139864066488064 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.9493930339813232, loss=1.6409649848937988
I0130 00:46:42.821847 139865088849664 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.738422155380249, loss=1.6394519805908203
I0130 00:47:16.719258 139864066488064 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.7319759130477905, loss=1.6603302955627441
I0130 00:47:50.632883 139865088849664 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.8912686109542847, loss=1.565500259399414
I0130 00:48:24.522484 139864066488064 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.816941261291504, loss=1.57673180103302
I0130 00:48:58.403093 139865088849664 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.1846840381622314, loss=1.6018283367156982
I0130 00:49:06.707543 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:49:13.039159 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:49:21.652745 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:49:24.329548 140027215431488 submission_runner.py:408] Time since start: 19058.96s, 	Step: 54126, 	{'train/accuracy': 0.7151626348495483, 'train/loss': 1.1085048913955688, 'validation/accuracy': 0.640720009803772, 'validation/loss': 1.4724503755569458, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.202472686767578, 'test/num_examples': 10000, 'score': 18394.996651649475, 'total_duration': 19058.955248355865, 'accumulated_submission_time': 18394.996651649475, 'accumulated_eval_time': 659.6608362197876, 'accumulated_logging_time': 2.370500087738037}
I0130 00:49:24.356805 139864049702656 logging_writer.py:48] [54126] accumulated_eval_time=659.660836, accumulated_logging_time=2.370500, accumulated_submission_time=18394.996652, global_step=54126, preemption_count=0, score=18394.996652, test/accuracy=0.513700, test/loss=2.202473, test/num_examples=10000, total_duration=19058.955248, train/accuracy=0.715163, train/loss=1.108505, validation/accuracy=0.640720, validation/loss=1.472450, validation/num_examples=50000
I0130 00:49:49.739152 139864058095360 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8036587238311768, loss=1.6135210990905762
I0130 00:50:23.614949 139864049702656 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7740297317504883, loss=1.5993006229400635
I0130 00:50:57.453959 139864058095360 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8014216423034668, loss=1.5279028415679932
I0130 00:51:31.357840 139864049702656 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9268710613250732, loss=1.5179126262664795
I0130 00:52:05.230403 139864058095360 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7738780975341797, loss=1.5246965885162354
I0130 00:52:39.080281 139864049702656 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.9847224950790405, loss=1.534444808959961
I0130 00:53:13.000009 139864058095360 logging_writer.py:48] [54800] global_step=54800, grad_norm=2.0345218181610107, loss=1.4497897624969482
I0130 00:53:46.872038 139864049702656 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9527113437652588, loss=1.5294179916381836
I0130 00:54:20.839489 139864058095360 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7459696531295776, loss=1.5368274450302124
I0130 00:54:54.706573 139864049702656 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8319168090820312, loss=1.536799669265747
I0130 00:55:28.622662 139864058095360 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.12992262840271, loss=1.5620040893554688
I0130 00:56:02.495681 139864049702656 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.963493824005127, loss=1.6747803688049316
I0130 00:56:36.417816 139864058095360 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8540375232696533, loss=1.5725566148757935
I0130 00:57:10.306927 139864049702656 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.8937695026397705, loss=1.4857317209243774
I0130 00:57:44.198154 139864058095360 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.898207664489746, loss=1.571230173110962
I0130 00:57:54.519995 140027215431488 spec.py:321] Evaluating on the training split.
I0130 00:58:01.007451 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 00:58:09.657310 140027215431488 spec.py:349] Evaluating on the test split.
I0130 00:58:12.326884 140027215431488 submission_runner.py:408] Time since start: 19586.95s, 	Step: 55632, 	{'train/accuracy': 0.7169164419174194, 'train/loss': 1.0998347997665405, 'validation/accuracy': 0.6484999656677246, 'validation/loss': 1.4559392929077148, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.200547933578491, 'test/num_examples': 10000, 'score': 18905.095246076584, 'total_duration': 19586.952585458755, 'accumulated_submission_time': 18905.095246076584, 'accumulated_eval_time': 677.4676706790924, 'accumulated_logging_time': 2.4084765911102295}
I0130 00:58:12.354245 139865088849664 logging_writer.py:48] [55632] accumulated_eval_time=677.467671, accumulated_logging_time=2.408477, accumulated_submission_time=18905.095246, global_step=55632, preemption_count=0, score=18905.095246, test/accuracy=0.515800, test/loss=2.200548, test/num_examples=10000, total_duration=19586.952585, train/accuracy=0.716916, train/loss=1.099835, validation/accuracy=0.648500, validation/loss=1.455939, validation/num_examples=50000
I0130 00:58:35.709948 139865224107776 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.879847526550293, loss=1.5336360931396484
I0130 00:59:09.540165 139865088849664 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.8287349939346313, loss=1.6115407943725586
I0130 00:59:43.382320 139865224107776 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.8023483753204346, loss=1.641319990158081
I0130 01:00:17.274761 139865088849664 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7656776905059814, loss=1.5980420112609863
I0130 01:00:51.195271 139865224107776 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.844918966293335, loss=1.6424871683120728
I0130 01:01:25.102512 139865088849664 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8582508563995361, loss=1.559043526649475
I0130 01:01:58.962148 139865224107776 logging_writer.py:48] [56300] global_step=56300, grad_norm=2.1081223487854004, loss=1.602369785308838
I0130 01:02:32.878568 139865088849664 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.822705864906311, loss=1.5437366962432861
I0130 01:03:06.752108 139865224107776 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.071993589401245, loss=1.581928014755249
I0130 01:03:40.582439 139865088849664 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.730565071105957, loss=1.5492477416992188
I0130 01:04:14.444898 139865224107776 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.0382564067840576, loss=1.6459641456604004
I0130 01:04:48.369865 139865088849664 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8792024850845337, loss=1.5335747003555298
I0130 01:05:22.232910 139865224107776 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.748906135559082, loss=1.563279628753662
I0130 01:05:56.107051 139865088849664 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9275654554367065, loss=1.6037936210632324
I0130 01:06:30.005301 139865224107776 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8790814876556396, loss=1.5842270851135254
I0130 01:06:42.327397 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:06:49.278164 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:06:57.930886 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:07:00.579507 140027215431488 submission_runner.py:408] Time since start: 20115.20s, 	Step: 57138, 	{'train/accuracy': 0.71000075340271, 'train/loss': 1.1082898378372192, 'validation/accuracy': 0.6432799696922302, 'validation/loss': 1.463404655456543, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.2013065814971924, 'test/num_examples': 10000, 'score': 19415.004539966583, 'total_duration': 20115.204418182373, 'accumulated_submission_time': 19415.004539966583, 'accumulated_eval_time': 695.7189378738403, 'accumulated_logging_time': 2.4459259510040283}
I0130 01:07:00.606507 139864058095360 logging_writer.py:48] [57138] accumulated_eval_time=695.718938, accumulated_logging_time=2.445926, accumulated_submission_time=19415.004540, global_step=57138, preemption_count=0, score=19415.004540, test/accuracy=0.515800, test/loss=2.201307, test/num_examples=10000, total_duration=20115.204418, train/accuracy=0.710001, train/loss=1.108290, validation/accuracy=0.643280, validation/loss=1.463405, validation/num_examples=50000
I0130 01:07:22.109570 139864066488064 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.067905902862549, loss=1.6006404161453247
I0130 01:07:55.955042 139864058095360 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.8091063499450684, loss=1.491207242012024
I0130 01:08:29.813496 139864066488064 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9357150793075562, loss=1.580714225769043
I0130 01:09:03.691145 139864058095360 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.9087060689926147, loss=1.5527057647705078
I0130 01:09:37.579678 139864066488064 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.7608344554901123, loss=1.5606062412261963
I0130 01:10:11.486989 139864058095360 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9508213996887207, loss=1.564225196838379
I0130 01:10:45.360919 139864066488064 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8589277267456055, loss=1.55618417263031
I0130 01:11:19.257017 139864058095360 logging_writer.py:48] [57900] global_step=57900, grad_norm=2.0812063217163086, loss=1.4623029232025146
I0130 01:11:53.113752 139864066488064 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.9344426393508911, loss=1.649789810180664
I0130 01:12:26.980180 139864058095360 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.8026835918426514, loss=1.6426522731781006
I0130 01:13:00.890726 139864066488064 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.803096890449524, loss=1.6283234357833862
I0130 01:13:34.779341 139864058095360 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.144819498062134, loss=1.5478706359863281
I0130 01:14:08.782493 139864066488064 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9411113262176514, loss=1.6297204494476318
I0130 01:14:42.661476 139864058095360 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9623677730560303, loss=1.5703330039978027
I0130 01:15:16.523605 139864066488064 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.0559890270233154, loss=1.4747220277786255
I0130 01:15:30.916043 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:15:37.342153 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:15:46.072210 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:15:48.722341 140027215431488 submission_runner.py:408] Time since start: 20643.35s, 	Step: 58644, 	{'train/accuracy': 0.7024473547935486, 'train/loss': 1.1483705043792725, 'validation/accuracy': 0.6444000005722046, 'validation/loss': 1.4588139057159424, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.179934501647949, 'test/num_examples': 10000, 'score': 19925.25050020218, 'total_duration': 20643.348040819168, 'accumulated_submission_time': 19925.25050020218, 'accumulated_eval_time': 713.5251975059509, 'accumulated_logging_time': 2.4837570190429688}
I0130 01:15:48.750747 139864049702656 logging_writer.py:48] [58644] accumulated_eval_time=713.525198, accumulated_logging_time=2.483757, accumulated_submission_time=19925.250500, global_step=58644, preemption_count=0, score=19925.250500, test/accuracy=0.515900, test/loss=2.179935, test/num_examples=10000, total_duration=20643.348041, train/accuracy=0.702447, train/loss=1.148371, validation/accuracy=0.644400, validation/loss=1.458814, validation/num_examples=50000
I0130 01:16:08.020318 139864058095360 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.8134264945983887, loss=1.5492572784423828
I0130 01:16:41.900158 139864049702656 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.735432744026184, loss=1.4896618127822876
I0130 01:17:15.737459 139864058095360 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.874670386314392, loss=1.579241394996643
I0130 01:17:49.600317 139864049702656 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9632710218429565, loss=1.5338318347930908
I0130 01:18:23.486634 139864058095360 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.8172413110733032, loss=1.5460882186889648
I0130 01:18:57.360726 139864049702656 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.758305311203003, loss=1.5700849294662476
I0130 01:19:31.207095 139864058095360 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8103220462799072, loss=1.523916244506836
I0130 01:20:05.107185 139864049702656 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.921346664428711, loss=1.6464169025421143
I0130 01:20:39.049097 139864058095360 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.1618106365203857, loss=1.5459681749343872
I0130 01:21:12.948774 139864049702656 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.2845098972320557, loss=1.664903998374939
I0130 01:21:46.806483 139864058095360 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.867759108543396, loss=1.498967170715332
I0130 01:22:20.681126 139864049702656 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.7865245342254639, loss=1.5310617685317993
I0130 01:22:54.600972 139864058095360 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.848777174949646, loss=1.5531446933746338
I0130 01:23:28.456032 139864049702656 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.8841983079910278, loss=1.5320425033569336
I0130 01:24:02.312125 139864058095360 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.9523932933807373, loss=1.553051471710205
I0130 01:24:18.728011 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:24:25.057701 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:24:33.915601 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:24:36.637188 140027215431488 submission_runner.py:408] Time since start: 21171.26s, 	Step: 60150, 	{'train/accuracy': 0.7673987150192261, 'train/loss': 0.8855634927749634, 'validation/accuracy': 0.6535999774932861, 'validation/loss': 1.4235299825668335, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.1435821056365967, 'test/num_examples': 10000, 'score': 20435.165104150772, 'total_duration': 21171.26289153099, 'accumulated_submission_time': 20435.165104150772, 'accumulated_eval_time': 731.4343252182007, 'accumulated_logging_time': 2.5222463607788086}
I0130 01:24:36.664997 139865224107776 logging_writer.py:48] [60150] accumulated_eval_time=731.434325, accumulated_logging_time=2.522246, accumulated_submission_time=20435.165104, global_step=60150, preemption_count=0, score=20435.165104, test/accuracy=0.530800, test/loss=2.143582, test/num_examples=10000, total_duration=21171.262892, train/accuracy=0.767399, train/loss=0.885563, validation/accuracy=0.653600, validation/loss=1.423530, validation/num_examples=50000
I0130 01:24:53.920783 139865232500480 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.851292371749878, loss=1.6110502481460571
I0130 01:25:27.800129 139865224107776 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9171003103256226, loss=1.5866985321044922
I0130 01:26:01.675868 139865232500480 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9388233423233032, loss=1.6081352233886719
I0130 01:26:35.582236 139865224107776 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.761339545249939, loss=1.4465521574020386
I0130 01:27:09.637692 139865232500480 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9376347064971924, loss=1.6522986888885498
I0130 01:27:43.487937 139865224107776 logging_writer.py:48] [60700] global_step=60700, grad_norm=2.0273971557617188, loss=1.4959213733673096
I0130 01:28:17.392208 139865232500480 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.8026974201202393, loss=1.5655020475387573
I0130 01:28:51.277467 139865224107776 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.1639671325683594, loss=1.741002082824707
I0130 01:29:25.195100 139865232500480 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8506934642791748, loss=1.6929444074630737
I0130 01:29:59.076639 139865224107776 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.8047038316726685, loss=1.6691687107086182
I0130 01:30:32.986714 139865232500480 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.8959840536117554, loss=1.608575463294983
I0130 01:31:06.872174 139865224107776 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.0498874187469482, loss=1.6601707935333252
I0130 01:31:40.756345 139865232500480 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8958837985992432, loss=1.5090012550354004
I0130 01:32:14.668703 139865224107776 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8698686361312866, loss=1.5442718267440796
I0130 01:32:48.548850 139865232500480 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.7943599224090576, loss=1.5274991989135742
I0130 01:33:06.638161 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:33:13.122615 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:33:21.701567 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:33:24.383998 140027215431488 submission_runner.py:408] Time since start: 21699.01s, 	Step: 61655, 	{'train/accuracy': 0.7128706574440002, 'train/loss': 1.1097934246063232, 'validation/accuracy': 0.6341599822044373, 'validation/loss': 1.5179804563522339, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.302015781402588, 'test/num_examples': 10000, 'score': 20945.074239969254, 'total_duration': 21699.009701251984, 'accumulated_submission_time': 20945.074239969254, 'accumulated_eval_time': 749.1801223754883, 'accumulated_logging_time': 2.560462474822998}
I0130 01:33:24.418030 139864049702656 logging_writer.py:48] [61655] accumulated_eval_time=749.180122, accumulated_logging_time=2.560462, accumulated_submission_time=20945.074240, global_step=61655, preemption_count=0, score=20945.074240, test/accuracy=0.500300, test/loss=2.302016, test/num_examples=10000, total_duration=21699.009701, train/accuracy=0.712871, train/loss=1.109793, validation/accuracy=0.634160, validation/loss=1.517980, validation/num_examples=50000
I0130 01:33:40.149537 139864058095360 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.7337645292282104, loss=1.489854097366333
I0130 01:34:14.023452 139864049702656 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.8782241344451904, loss=1.5241124629974365
I0130 01:34:47.920354 139864058095360 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.0889041423797607, loss=1.574733018875122
I0130 01:35:21.780403 139864049702656 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9860455989837646, loss=1.4835573434829712
I0130 01:35:55.658402 139864058095360 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.9381945133209229, loss=1.529569149017334
I0130 01:36:29.521104 139864049702656 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.0475051403045654, loss=1.5109505653381348
I0130 01:37:03.378090 139864058095360 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.7787641286849976, loss=1.59016752243042
I0130 01:37:37.276412 139864049702656 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.1206765174865723, loss=1.6059577465057373
I0130 01:38:11.137727 139864058095360 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.11059308052063, loss=1.5797045230865479
I0130 01:38:45.074835 139864049702656 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.857395887374878, loss=1.5330086946487427
I0130 01:39:18.952983 139864058095360 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.807408094406128, loss=1.5468553304672241
I0130 01:39:52.864250 139864049702656 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.1688618659973145, loss=1.593122959136963
I0130 01:40:26.779986 139864058095360 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.1063613891601562, loss=1.6696548461914062
I0130 01:41:00.690951 139864049702656 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.0494630336761475, loss=1.449508547782898
I0130 01:41:34.591379 139864058095360 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.228330373764038, loss=1.712705373764038
I0130 01:41:54.385972 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:42:00.707718 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:42:09.383414 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:42:12.046737 140027215431488 submission_runner.py:408] Time since start: 22226.67s, 	Step: 63160, 	{'train/accuracy': 0.7302096486091614, 'train/loss': 1.038726806640625, 'validation/accuracy': 0.6582399606704712, 'validation/loss': 1.4035637378692627, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.1373679637908936, 'test/num_examples': 10000, 'score': 21454.97639608383, 'total_duration': 22226.67244052887, 'accumulated_submission_time': 21454.97639608383, 'accumulated_eval_time': 766.840833902359, 'accumulated_logging_time': 2.6059324741363525}
I0130 01:42:12.076266 139865224107776 logging_writer.py:48] [63160] accumulated_eval_time=766.840834, accumulated_logging_time=2.605932, accumulated_submission_time=21454.976396, global_step=63160, preemption_count=0, score=21454.976396, test/accuracy=0.528100, test/loss=2.137368, test/num_examples=10000, total_duration=22226.672441, train/accuracy=0.730210, train/loss=1.038727, validation/accuracy=0.658240, validation/loss=1.403564, validation/num_examples=50000
I0130 01:42:25.988241 139865232500480 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.8698147535324097, loss=1.5757322311401367
I0130 01:42:59.853998 139865224107776 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.9348751306533813, loss=1.5453823804855347
I0130 01:43:33.699687 139865232500480 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.0003018379211426, loss=1.4659591913223267
I0130 01:44:07.600209 139865224107776 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.8774772882461548, loss=1.579150676727295
I0130 01:44:41.474641 139865232500480 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9361246824264526, loss=1.6035782098770142
I0130 01:45:15.332115 139865224107776 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.0378801822662354, loss=1.639580249786377
I0130 01:45:49.221268 139865232500480 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.8970140218734741, loss=1.4266018867492676
I0130 01:46:23.136159 139865224107776 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.001863479614258, loss=1.5410854816436768
I0130 01:46:57.115508 139865232500480 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.8644222021102905, loss=1.468986988067627
I0130 01:47:30.975904 139865224107776 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.080110549926758, loss=1.5435281991958618
I0130 01:48:04.847349 139865232500480 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.888795256614685, loss=1.3853418827056885
I0130 01:48:38.758514 139865224107776 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.8905518054962158, loss=1.4914796352386475
I0130 01:49:12.627268 139865232500480 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.0080785751342773, loss=1.5077273845672607
I0130 01:49:46.530516 139865224107776 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.837724208831787, loss=1.559266448020935
I0130 01:50:20.405081 139865232500480 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.891613483428955, loss=1.4816317558288574
I0130 01:50:42.227349 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:50:48.598350 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:50:57.426050 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:51:00.091380 140027215431488 submission_runner.py:408] Time since start: 22754.72s, 	Step: 64666, 	{'train/accuracy': 0.7167769074440002, 'train/loss': 1.0958020687103271, 'validation/accuracy': 0.6484799981117249, 'validation/loss': 1.438090443611145, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.1825969219207764, 'test/num_examples': 10000, 'score': 21965.061421632767, 'total_duration': 22754.716769218445, 'accumulated_submission_time': 21965.061421632767, 'accumulated_eval_time': 784.704512834549, 'accumulated_logging_time': 2.6474947929382324}
I0130 01:51:00.125562 139864049702656 logging_writer.py:48] [64666] accumulated_eval_time=784.704513, accumulated_logging_time=2.647495, accumulated_submission_time=21965.061422, global_step=64666, preemption_count=0, score=21965.061422, test/accuracy=0.520700, test/loss=2.182597, test/num_examples=10000, total_duration=22754.716769, train/accuracy=0.716777, train/loss=1.095802, validation/accuracy=0.648480, validation/loss=1.438090, validation/num_examples=50000
I0130 01:51:11.999914 139864058095360 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9721617698669434, loss=1.5201387405395508
I0130 01:51:45.852486 139864049702656 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.050422430038452, loss=1.4699958562850952
I0130 01:52:19.710211 139864058095360 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.0682666301727295, loss=1.484302282333374
I0130 01:52:53.570048 139864049702656 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.0225274562835693, loss=1.5208165645599365
I0130 01:53:27.569927 139864058095360 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.9915437698364258, loss=1.6525256633758545
I0130 01:54:01.402724 139864049702656 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.9487197399139404, loss=1.551697015762329
I0130 01:54:35.279150 139864058095360 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.8719555139541626, loss=1.605806827545166
I0130 01:55:09.147849 139864049702656 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.938387155532837, loss=1.6159822940826416
I0130 01:55:43.055658 139864058095360 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.0201659202575684, loss=1.5887693166732788
I0130 01:56:16.892395 139864049702656 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.0645029544830322, loss=1.537889003753662
I0130 01:56:50.750749 139864058095360 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.07967472076416, loss=1.5303349494934082
I0130 01:57:24.662514 139864049702656 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.9592676162719727, loss=1.508249044418335
I0130 01:57:58.540884 139864058095360 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.086486339569092, loss=1.5455858707427979
I0130 01:58:32.452914 139864049702656 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.8350802659988403, loss=1.5772953033447266
I0130 01:59:06.334869 139864058095360 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.0310909748077393, loss=1.4098834991455078
I0130 01:59:30.144239 140027215431488 spec.py:321] Evaluating on the training split.
I0130 01:59:36.518679 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 01:59:45.199778 140027215431488 spec.py:349] Evaluating on the test split.
I0130 01:59:47.946960 140027215431488 submission_runner.py:408] Time since start: 23282.57s, 	Step: 66172, 	{'train/accuracy': 0.7169762253761292, 'train/loss': 1.0895507335662842, 'validation/accuracy': 0.6572399735450745, 'validation/loss': 1.4136441946029663, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.1981096267700195, 'test/num_examples': 10000, 'score': 22475.013607740402, 'total_duration': 23282.572645425797, 'accumulated_submission_time': 22475.013607740402, 'accumulated_eval_time': 802.5071756839752, 'accumulated_logging_time': 2.695244073867798}
I0130 01:59:47.976394 139864041309952 logging_writer.py:48] [66172] accumulated_eval_time=802.507176, accumulated_logging_time=2.695244, accumulated_submission_time=22475.013608, global_step=66172, preemption_count=0, score=22475.013608, test/accuracy=0.521500, test/loss=2.198110, test/num_examples=10000, total_duration=23282.572645, train/accuracy=0.716976, train/loss=1.089551, validation/accuracy=0.657240, validation/loss=1.413644, validation/num_examples=50000
I0130 01:59:57.912631 139864049702656 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.8480234146118164, loss=1.5611175298690796
I0130 02:00:31.730508 139864041309952 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.7584083080291748, loss=1.536203384399414
I0130 02:01:05.606583 139864049702656 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.047905683517456, loss=1.5357954502105713
I0130 02:01:39.464818 139864041309952 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.8832899332046509, loss=1.5179682970046997
I0130 02:02:13.332625 139864049702656 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.0660622119903564, loss=1.513861894607544
I0130 02:02:47.245876 139864041309952 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.960557460784912, loss=1.6060123443603516
I0130 02:03:21.105138 139864049702656 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.1399712562561035, loss=1.4923007488250732
I0130 02:03:55.005887 139864041309952 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.98641037940979, loss=1.5517793893814087
I0130 02:04:28.876699 139864049702656 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.9535030126571655, loss=1.517114281654358
I0130 02:05:02.743985 139864041309952 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.040844678878784, loss=1.5957642793655396
I0130 02:05:36.631002 139864049702656 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.1158368587493896, loss=1.5116236209869385
I0130 02:06:10.526459 139864041309952 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.023615598678589, loss=1.455254077911377
I0130 02:06:44.493343 139864049702656 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.9549221992492676, loss=1.4782353639602661
I0130 02:07:18.366275 139864041309952 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.895601749420166, loss=1.4543713331222534
I0130 02:07:52.230523 139864049702656 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.1228158473968506, loss=1.551823377609253
I0130 02:08:18.114879 140027215431488 spec.py:321] Evaluating on the training split.
I0130 02:08:24.528003 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 02:08:33.455226 140027215431488 spec.py:349] Evaluating on the test split.
I0130 02:08:36.597285 140027215431488 submission_runner.py:408] Time since start: 23811.22s, 	Step: 67678, 	{'train/accuracy': 0.727937638759613, 'train/loss': 1.04031240940094, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.3774571418762207, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.1053407192230225, 'test/num_examples': 10000, 'score': 22985.089703559875, 'total_duration': 23811.222986221313, 'accumulated_submission_time': 22985.089703559875, 'accumulated_eval_time': 820.9895300865173, 'accumulated_logging_time': 2.7346694469451904}
I0130 02:08:36.623541 139864041309952 logging_writer.py:48] [67678] accumulated_eval_time=820.989530, accumulated_logging_time=2.734669, accumulated_submission_time=22985.089704, global_step=67678, preemption_count=0, score=22985.089704, test/accuracy=0.536500, test/loss=2.105341, test/num_examples=10000, total_duration=23811.222986, train/accuracy=0.727938, train/loss=1.040312, validation/accuracy=0.662080, validation/loss=1.377457, validation/num_examples=50000
I0130 02:08:44.440666 139864049702656 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0262818336486816, loss=1.4481027126312256
I0130 02:09:18.286047 139864041309952 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.155344009399414, loss=1.5128847360610962
I0130 02:09:52.127241 139864049702656 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.0402088165283203, loss=1.4953440427780151
I0130 02:10:26.024141 139864041309952 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.9512141942977905, loss=1.569825530052185
I0130 02:10:59.897433 139864049702656 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.0866291522979736, loss=1.5117669105529785
I0130 02:11:33.783697 139864041309952 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.9658654928207397, loss=1.4807682037353516
I0130 02:12:07.714902 139864049702656 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.8758389949798584, loss=1.4508693218231201
I0130 02:12:41.579161 139864041309952 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0392343997955322, loss=1.5184029340744019
I0130 02:13:15.555930 139864049702656 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.1661183834075928, loss=1.5274592638015747
I0130 02:13:49.441192 139864041309952 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.0775294303894043, loss=1.409027338027954
I0130 02:14:23.356466 139864049702656 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.9424772262573242, loss=1.5330884456634521
I0130 02:14:57.221448 139864041309952 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.1331262588500977, loss=1.4789944887161255
I0130 02:15:31.122430 139864049702656 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.8681583404541016, loss=1.6126394271850586
I0130 02:16:04.983425 139864041309952 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8584941625595093, loss=1.4402046203613281
I0130 02:16:38.866446 139864049702656 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.908295750617981, loss=1.6030832529067993
I0130 02:17:06.811844 140027215431488 spec.py:321] Evaluating on the training split.
I0130 02:17:13.121224 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 02:17:21.695883 140027215431488 spec.py:349] Evaluating on the test split.
I0130 02:17:24.348012 140027215431488 submission_runner.py:408] Time since start: 24338.97s, 	Step: 69184, 	{'train/accuracy': 0.7716637253761292, 'train/loss': 0.8626237511634827, 'validation/accuracy': 0.661579966545105, 'validation/loss': 1.3844743967056274, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.0840163230895996, 'test/num_examples': 10000, 'score': 23495.214922189713, 'total_duration': 24338.97371149063, 'accumulated_submission_time': 23495.214922189713, 'accumulated_eval_time': 838.5256464481354, 'accumulated_logging_time': 2.7713065147399902}
I0130 02:17:24.379358 139864049702656 logging_writer.py:48] [69184] accumulated_eval_time=838.525646, accumulated_logging_time=2.771307, accumulated_submission_time=23495.214922, global_step=69184, preemption_count=0, score=23495.214922, test/accuracy=0.534900, test/loss=2.084016, test/num_examples=10000, total_duration=24338.973711, train/accuracy=0.771664, train/loss=0.862624, validation/accuracy=0.661580, validation/loss=1.384474, validation/num_examples=50000
I0130 02:17:30.123779 139865232500480 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.8855595588684082, loss=1.42098069190979
I0130 02:18:03.960816 139864049702656 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.0782389640808105, loss=1.4564801454544067
I0130 02:18:37.803224 139865232500480 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.0075576305389404, loss=1.4521812200546265
I0130 02:19:11.696122 139864049702656 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.0454752445220947, loss=1.565445899963379
I0130 02:19:45.577909 139865232500480 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.9698388576507568, loss=1.3620790243148804
I0130 02:20:19.422366 139864049702656 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.083184003829956, loss=1.4971141815185547
I0130 02:20:53.297693 139865232500480 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.0043838024139404, loss=1.481314778327942
I0130 02:21:27.167820 139864049702656 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.9711991548538208, loss=1.4784742593765259
I0130 02:22:01.046643 139865232500480 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.26006817817688, loss=1.4917460680007935
I0130 02:22:34.942406 139864049702656 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.185856342315674, loss=1.5329043865203857
I0130 02:23:08.805845 139865232500480 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.874711275100708, loss=1.4703651666641235
I0130 02:23:42.681203 139864049702656 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.8826044797897339, loss=1.437744379043579
I0130 02:24:16.535743 139865232500480 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.118722677230835, loss=1.5856952667236328
I0130 02:24:50.402017 139864049702656 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.229497194290161, loss=1.4970382452011108
I0130 02:25:24.297240 139865232500480 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0038604736328125, loss=1.5197304487228394
I0130 02:25:54.615281 140027215431488 spec.py:321] Evaluating on the training split.
I0130 02:26:00.962383 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 02:26:09.590102 140027215431488 spec.py:349] Evaluating on the test split.
I0130 02:26:12.242395 140027215431488 submission_runner.py:408] Time since start: 24866.87s, 	Step: 70691, 	{'train/accuracy': 0.7581313848495483, 'train/loss': 0.9096571207046509, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.3560611009597778, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.067430257797241, 'test/num_examples': 10000, 'score': 24005.384941101074, 'total_duration': 24866.868101119995, 'accumulated_submission_time': 24005.384941101074, 'accumulated_eval_time': 856.152738571167, 'accumulated_logging_time': 2.8141791820526123}
I0130 02:26:12.272579 139865088849664 logging_writer.py:48] [70691] accumulated_eval_time=856.152739, accumulated_logging_time=2.814179, accumulated_submission_time=24005.384941, global_step=70691, preemption_count=0, score=24005.384941, test/accuracy=0.548800, test/loss=2.067430, test/num_examples=10000, total_duration=24866.868101, train/accuracy=0.758131, train/loss=0.909657, validation/accuracy=0.670280, validation/loss=1.356061, validation/num_examples=50000
I0130 02:26:15.662924 139865224107776 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.8577834367752075, loss=1.4099127054214478
I0130 02:26:49.628613 139865088849664 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.8009814023971558, loss=1.4645439386367798
I0130 02:27:23.495284 139865224107776 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.0677521228790283, loss=1.5261530876159668
I0130 02:27:57.351731 139865088849664 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.029618740081787, loss=1.578639268875122
I0130 02:28:31.265069 139865224107776 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.288789987564087, loss=1.6676716804504395
I0130 02:29:05.115761 139865088849664 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.9391319751739502, loss=1.555921196937561
I0130 02:29:38.976548 139865224107776 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.3342456817626953, loss=1.4774363040924072
I0130 02:30:12.862756 139865088849664 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.117495059967041, loss=1.4622331857681274
I0130 02:30:46.717532 139865224107776 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.1311676502227783, loss=1.4965264797210693
I0130 02:31:20.592872 139865088849664 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.1600096225738525, loss=1.5734472274780273
I0130 02:31:54.497101 139865224107776 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.8831731081008911, loss=1.4479763507843018
I0130 02:32:28.352654 139865088849664 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.307588815689087, loss=1.5542476177215576
I0130 02:33:02.340062 139865224107776 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.8929734230041504, loss=1.5019394159317017
I0130 02:33:36.218999 139865088849664 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.259230852127075, loss=1.5744233131408691
I0130 02:34:10.143050 139865224107776 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.9180231094360352, loss=1.5677285194396973
I0130 02:34:42.458704 140027215431488 spec.py:321] Evaluating on the training split.
I0130 02:34:48.923060 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 02:34:57.714091 140027215431488 spec.py:349] Evaluating on the test split.
I0130 02:35:00.232522 140027215431488 submission_runner.py:408] Time since start: 25394.86s, 	Step: 72197, 	{'train/accuracy': 0.7457548975944519, 'train/loss': 0.974716067314148, 'validation/accuracy': 0.6640200018882751, 'validation/loss': 1.391406536102295, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.1257283687591553, 'test/num_examples': 10000, 'score': 24515.506544589996, 'total_duration': 25394.858213186264, 'accumulated_submission_time': 24515.506544589996, 'accumulated_eval_time': 873.9265124797821, 'accumulated_logging_time': 2.855938196182251}
I0130 02:35:00.263072 139864049702656 logging_writer.py:48] [72197] accumulated_eval_time=873.926512, accumulated_logging_time=2.855938, accumulated_submission_time=24515.506545, global_step=72197, preemption_count=0, score=24515.506545, test/accuracy=0.534500, test/loss=2.125728, test/num_examples=10000, total_duration=25394.858213, train/accuracy=0.745755, train/loss=0.974716, validation/accuracy=0.664020, validation/loss=1.391407, validation/num_examples=50000
I0130 02:35:01.630208 139864066488064 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.2894296646118164, loss=1.4381247758865356
I0130 02:35:35.523462 139864049702656 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.998875379562378, loss=1.512662410736084
I0130 02:36:09.394726 139864066488064 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.0027527809143066, loss=1.532698154449463
I0130 02:36:43.233280 139864049702656 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.213665246963501, loss=1.642685055732727
I0130 02:37:17.105165 139864066488064 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.132746696472168, loss=1.6340296268463135
I0130 02:37:51.016157 139864049702656 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.0515551567077637, loss=1.447397232055664
I0130 02:38:24.872423 139864066488064 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.2606537342071533, loss=1.537827730178833
I0130 02:38:58.742746 139864049702656 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.9725035429000854, loss=1.4586442708969116
I0130 02:39:32.712285 139864066488064 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.2604758739471436, loss=1.5028471946716309
I0130 02:40:06.628445 139864049702656 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.949269413948059, loss=1.4390658140182495
I0130 02:40:40.499809 139864066488064 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.9940507411956787, loss=1.507781982421875
I0130 02:41:14.386345 139864049702656 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.9606369733810425, loss=1.416473388671875
I0130 02:41:48.260788 139864066488064 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.197105884552002, loss=1.5136549472808838
I0130 02:42:22.098655 139864049702656 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.1435134410858154, loss=1.5046586990356445
I0130 02:42:55.960149 139864066488064 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.134904384613037, loss=1.4738845825195312
I0130 02:43:29.877164 139864049702656 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.073021411895752, loss=1.559846043586731
I0130 02:43:30.367337 140027215431488 spec.py:321] Evaluating on the training split.
I0130 02:43:36.700429 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 02:43:45.278596 140027215431488 spec.py:349] Evaluating on the test split.
I0130 02:43:47.941904 140027215431488 submission_runner.py:408] Time since start: 25922.57s, 	Step: 73703, 	{'train/accuracy': 0.7359893321990967, 'train/loss': 1.0156553983688354, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.3957440853118896, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.1294972896575928, 'test/num_examples': 10000, 'score': 25025.546664714813, 'total_duration': 25922.5676074028, 'accumulated_submission_time': 25025.546664714813, 'accumulated_eval_time': 891.5010304450989, 'accumulated_logging_time': 2.8975298404693604}
I0130 02:43:47.973701 139864058095360 logging_writer.py:48] [73703] accumulated_eval_time=891.501030, accumulated_logging_time=2.897530, accumulated_submission_time=25025.546665, global_step=73703, preemption_count=0, score=25025.546665, test/accuracy=0.533900, test/loss=2.129497, test/num_examples=10000, total_duration=25922.567607, train/accuracy=0.735989, train/loss=1.015655, validation/accuracy=0.657780, validation/loss=1.395744, validation/num_examples=50000
I0130 02:44:21.115597 139865224107776 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.1167337894439697, loss=1.6046348810195923
I0130 02:44:54.989293 139864058095360 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.9637044668197632, loss=1.507489562034607
I0130 02:45:28.812704 139865224107776 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.912514328956604, loss=1.3978208303451538
I0130 02:46:02.794410 139864058095360 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.1624841690063477, loss=1.4840980768203735
I0130 02:46:36.671845 139865224107776 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.0283560752868652, loss=1.5106703042984009
I0130 02:47:10.563669 139864058095360 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.334713935852051, loss=1.5519130229949951
I0130 02:47:44.443571 139865224107776 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.0661463737487793, loss=1.6051301956176758
I0130 02:48:18.300798 139864058095360 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.0109448432922363, loss=1.4843804836273193
I0130 02:48:52.197017 139865224107776 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.9187105894088745, loss=1.5489776134490967
I0130 02:49:26.055115 139864058095360 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.862712025642395, loss=1.5018336772918701
I0130 02:49:59.916564 139865224107776 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.0006532669067383, loss=1.4177896976470947
I0130 02:50:33.801771 139864058095360 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.064290761947632, loss=1.582546591758728
I0130 02:51:07.712282 139865224107776 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1705939769744873, loss=1.5493602752685547
I0130 02:51:41.615304 139864058095360 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.18947172164917, loss=1.4968502521514893
I0130 02:52:15.498703 139865224107776 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.983293056488037, loss=1.4480053186416626
I0130 02:52:18.015984 140027215431488 spec.py:321] Evaluating on the training split.
I0130 02:52:24.464961 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 02:52:33.207769 140027215431488 spec.py:349] Evaluating on the test split.
I0130 02:52:36.252825 140027215431488 submission_runner.py:408] Time since start: 26450.88s, 	Step: 75209, 	{'train/accuracy': 0.7361288070678711, 'train/loss': 1.0147454738616943, 'validation/accuracy': 0.6642199754714966, 'validation/loss': 1.3678507804870605, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.108184576034546, 'test/num_examples': 10000, 'score': 25535.526324272156, 'total_duration': 26450.878532886505, 'accumulated_submission_time': 25535.526324272156, 'accumulated_eval_time': 909.7378346920013, 'accumulated_logging_time': 2.939307928085327}
I0130 02:52:36.279357 139864049702656 logging_writer.py:48] [75209] accumulated_eval_time=909.737835, accumulated_logging_time=2.939308, accumulated_submission_time=25535.526324, global_step=75209, preemption_count=0, score=25535.526324, test/accuracy=0.532200, test/loss=2.108185, test/num_examples=10000, total_duration=26450.878533, train/accuracy=0.736129, train/loss=1.014745, validation/accuracy=0.664220, validation/loss=1.367851, validation/num_examples=50000
I0130 02:53:07.466063 139864058095360 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.03859543800354, loss=1.3790531158447266
I0130 02:53:41.341439 139864049702656 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.1032462120056152, loss=1.3785042762756348
I0130 02:54:15.217923 139864058095360 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.181629180908203, loss=1.5240252017974854
I0130 02:54:49.076473 139864049702656 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.24111008644104, loss=1.5339174270629883
I0130 02:55:23.008545 139864058095360 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.302027463912964, loss=1.497259497642517
I0130 02:55:56.855367 139864049702656 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.006071090698242, loss=1.5051389932632446
I0130 02:56:30.708196 139864058095360 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.230992317199707, loss=1.5276992321014404
I0130 02:57:04.587193 139864049702656 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0948867797851562, loss=1.4408681392669678
I0130 02:57:38.481570 139864058095360 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.9620734453201294, loss=1.4768739938735962
I0130 02:58:12.350301 139864049702656 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.086385488510132, loss=1.3453373908996582
I0130 02:58:46.218677 139864058095360 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.093730926513672, loss=1.3712913990020752
I0130 02:59:20.212733 139864049702656 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.928411841392517, loss=1.4897987842559814
I0130 02:59:54.102116 139864058095360 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.1653196811676025, loss=1.4356110095977783
I0130 03:00:27.954195 139864049702656 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.0093798637390137, loss=1.4954533576965332
I0130 03:01:01.850133 139864058095360 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.2387208938598633, loss=1.606989860534668
I0130 03:01:06.387628 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:01:12.792458 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:01:21.240155 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:01:23.938912 140027215431488 submission_runner.py:408] Time since start: 26978.56s, 	Step: 76715, 	{'train/accuracy': 0.725027859210968, 'train/loss': 1.0601333379745483, 'validation/accuracy': 0.6577199697494507, 'validation/loss': 1.3992598056793213, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.151803970336914, 'test/num_examples': 10000, 'score': 26045.5726313591, 'total_duration': 26978.564618349075, 'accumulated_submission_time': 26045.5726313591, 'accumulated_eval_time': 927.2890992164612, 'accumulated_logging_time': 2.9757421016693115}
I0130 03:01:23.972847 139865224107776 logging_writer.py:48] [76715] accumulated_eval_time=927.289099, accumulated_logging_time=2.975742, accumulated_submission_time=26045.572631, global_step=76715, preemption_count=0, score=26045.572631, test/accuracy=0.528100, test/loss=2.151804, test/num_examples=10000, total_duration=26978.564618, train/accuracy=0.725028, train/loss=1.060133, validation/accuracy=0.657720, validation/loss=1.399260, validation/num_examples=50000
I0130 03:01:53.107507 139865232500480 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.2582316398620605, loss=1.4752464294433594
I0130 03:02:26.961027 139865224107776 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.9416247606277466, loss=1.381945252418518
I0130 03:03:00.814002 139865232500480 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.129284143447876, loss=1.386324167251587
I0130 03:03:34.740688 139865224107776 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.9431846141815186, loss=1.4830563068389893
I0130 03:04:08.606651 139865232500480 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.0371904373168945, loss=1.4409661293029785
I0130 03:04:42.477752 139865224107776 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.265174150466919, loss=1.5090886354446411
I0130 03:05:16.381239 139865232500480 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.191707134246826, loss=1.4927887916564941
I0130 03:05:50.232231 139865224107776 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.1112301349639893, loss=1.5190304517745972
I0130 03:06:24.171049 139865232500480 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.0658206939697266, loss=1.4680590629577637
I0130 03:06:58.028731 139865224107776 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.3322596549987793, loss=1.499286413192749
I0130 03:07:31.926098 139865232500480 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.140153646469116, loss=1.449763298034668
I0130 03:08:05.807190 139865224107776 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.9515711069107056, loss=1.40361487865448
I0130 03:08:39.668287 139865232500480 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.9839633703231812, loss=1.3851943016052246
I0130 03:09:13.552092 139865224107776 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.091095209121704, loss=1.3723018169403076
I0130 03:09:47.418331 139865232500480 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.974501609802246, loss=1.411586046218872
I0130 03:09:54.004693 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:10:00.333020 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:10:09.095048 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:10:11.789530 140027215431488 submission_runner.py:408] Time since start: 27506.42s, 	Step: 78221, 	{'train/accuracy': 0.7487444281578064, 'train/loss': 0.9544544219970703, 'validation/accuracy': 0.6531999707221985, 'validation/loss': 1.4107705354690552, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.1556520462036133, 'test/num_examples': 10000, 'score': 26555.53935289383, 'total_duration': 27506.41523051262, 'accumulated_submission_time': 26555.53935289383, 'accumulated_eval_time': 945.0738813877106, 'accumulated_logging_time': 3.020139217376709}
I0130 03:10:11.820953 139865088849664 logging_writer.py:48] [78221] accumulated_eval_time=945.073881, accumulated_logging_time=3.020139, accumulated_submission_time=26555.539353, global_step=78221, preemption_count=0, score=26555.539353, test/accuracy=0.523300, test/loss=2.155652, test/num_examples=10000, total_duration=27506.415231, train/accuracy=0.748744, train/loss=0.954454, validation/accuracy=0.653200, validation/loss=1.410771, validation/num_examples=50000
I0130 03:10:38.922082 139865760950016 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1040148735046387, loss=1.4962321519851685
I0130 03:11:12.758024 139865088849664 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.0060369968414307, loss=1.4506953954696655
I0130 03:11:46.622030 139865760950016 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.103224277496338, loss=1.3594411611557007
I0130 03:12:20.514240 139865088849664 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.503523588180542, loss=1.4942599534988403
I0130 03:12:54.476226 139865760950016 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.265082836151123, loss=1.444279670715332
I0130 03:13:28.329836 139865088849664 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.1180646419525146, loss=1.338014006614685
I0130 03:14:02.225767 139865760950016 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.022329568862915, loss=1.4245525598526
I0130 03:14:36.131474 139865088849664 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.1757898330688477, loss=1.492922067642212
I0130 03:15:10.040395 139865760950016 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.2916460037231445, loss=1.5058672428131104
I0130 03:15:43.904042 139865088849664 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.4781205654144287, loss=1.5838896036148071
I0130 03:16:17.792263 139865760950016 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.144294261932373, loss=1.4076381921768188
I0130 03:16:51.641950 139865088849664 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.399129867553711, loss=1.4828420877456665
I0130 03:17:25.532086 139865760950016 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.228450059890747, loss=1.6191476583480835
I0130 03:17:59.416392 139865088849664 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.186272144317627, loss=1.486149787902832
I0130 03:18:33.301986 139865760950016 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.220930814743042, loss=1.4263060092926025
I0130 03:18:41.914238 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:18:48.222307 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:18:56.834823 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:18:59.420269 140027215431488 submission_runner.py:408] Time since start: 28034.05s, 	Step: 79727, 	{'train/accuracy': 0.7374043464660645, 'train/loss': 0.9866397380828857, 'validation/accuracy': 0.6531199812889099, 'validation/loss': 1.437100887298584, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.229564905166626, 'test/num_examples': 10000, 'score': 27065.569350004196, 'total_duration': 28034.045956611633, 'accumulated_submission_time': 27065.569350004196, 'accumulated_eval_time': 962.5798609256744, 'accumulated_logging_time': 3.0614492893218994}
I0130 03:18:59.452726 139864058095360 logging_writer.py:48] [79727] accumulated_eval_time=962.579861, accumulated_logging_time=3.061449, accumulated_submission_time=27065.569350, global_step=79727, preemption_count=0, score=27065.569350, test/accuracy=0.521000, test/loss=2.229565, test/num_examples=10000, total_duration=28034.045957, train/accuracy=0.737404, train/loss=0.986640, validation/accuracy=0.653120, validation/loss=1.437101, validation/num_examples=50000
I0130 03:19:24.578214 139864066488064 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.1483678817749023, loss=1.4400215148925781
I0130 03:19:58.476438 139864058095360 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.087013006210327, loss=1.523653268814087
I0130 03:20:32.349346 139864066488064 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.117140769958496, loss=1.3976118564605713
I0130 03:21:06.207715 139864058095360 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.1863465309143066, loss=1.4914971590042114
I0130 03:21:40.114011 139864066488064 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.225398302078247, loss=1.4384466409683228
I0130 03:22:13.973119 139864058095360 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.298537492752075, loss=1.5129374265670776
I0130 03:22:47.835930 139864066488064 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.2293331623077393, loss=1.429847002029419
I0130 03:23:21.759053 139864058095360 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.298158645629883, loss=1.4669609069824219
I0130 03:23:55.628379 139864066488064 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.041188955307007, loss=1.4220800399780273
I0130 03:24:29.510077 139864058095360 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.0914244651794434, loss=1.5209898948669434
I0130 03:25:03.399576 139864066488064 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.0181849002838135, loss=1.4655234813690186
I0130 03:25:37.357661 139864058095360 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.923941969871521, loss=1.3535476922988892
I0130 03:26:11.233647 139864066488064 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.145159959793091, loss=1.4546465873718262
I0130 03:26:45.067498 139864058095360 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.215146541595459, loss=1.5243048667907715
I0130 03:27:18.941254 139864066488064 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.1887598037719727, loss=1.4023926258087158
I0130 03:27:29.578261 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:27:35.903962 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:27:44.794768 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:27:47.480109 140027215431488 submission_runner.py:408] Time since start: 28562.11s, 	Step: 81233, 	{'train/accuracy': 0.7581512928009033, 'train/loss': 0.9172185659408569, 'validation/accuracy': 0.6740999817848206, 'validation/loss': 1.3255287408828735, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.054157257080078, 'test/num_examples': 10000, 'score': 27575.630492925644, 'total_duration': 28562.10580611229, 'accumulated_submission_time': 27575.630492925644, 'accumulated_eval_time': 980.4816539287567, 'accumulated_logging_time': 3.104302167892456}
I0130 03:27:47.511408 139865240893184 logging_writer.py:48] [81233] accumulated_eval_time=980.481654, accumulated_logging_time=3.104302, accumulated_submission_time=27575.630493, global_step=81233, preemption_count=0, score=27575.630493, test/accuracy=0.545100, test/loss=2.054157, test/num_examples=10000, total_duration=28562.105806, train/accuracy=0.758151, train/loss=0.917219, validation/accuracy=0.674100, validation/loss=1.325529, validation/num_examples=50000
I0130 03:28:10.534090 139865760950016 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.9630012512207031, loss=1.45681893825531
I0130 03:28:44.415786 139865240893184 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.210300922393799, loss=1.431688904762268
I0130 03:29:18.268123 139865760950016 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.367386817932129, loss=1.6268892288208008
I0130 03:29:52.166836 139865240893184 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.108360528945923, loss=1.4542016983032227
I0130 03:30:26.002141 139865760950016 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.146944284439087, loss=1.4660221338272095
I0130 03:30:59.857284 139865240893184 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.0924060344696045, loss=1.4145594835281372
I0130 03:31:33.741981 139865760950016 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.318293333053589, loss=1.369039535522461
I0130 03:32:07.730962 139865240893184 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.00425386428833, loss=1.472642421722412
I0130 03:32:41.622378 139865760950016 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.167750120162964, loss=1.3983585834503174
I0130 03:33:15.532065 139865240893184 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.2981629371643066, loss=1.3807202577590942
I0130 03:33:49.403089 139865760950016 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.3614959716796875, loss=1.460870623588562
I0130 03:34:23.289797 139865240893184 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.136349678039551, loss=1.4311378002166748
I0130 03:34:57.213092 139865760950016 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.173121690750122, loss=1.4090434312820435
I0130 03:35:31.054510 139865240893184 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.091188669204712, loss=1.5640249252319336
I0130 03:36:04.924668 139865760950016 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.0451231002807617, loss=1.4695247411727905
I0130 03:36:17.625586 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:36:23.937559 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:36:32.542610 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:36:35.437701 140027215431488 submission_runner.py:408] Time since start: 29090.06s, 	Step: 82739, 	{'train/accuracy': 0.7515744566917419, 'train/loss': 0.9364935159683228, 'validation/accuracy': 0.6704399585723877, 'validation/loss': 1.3452812433242798, 'validation/num_examples': 50000, 'test/accuracy': 0.547700047492981, 'test/loss': 2.067952871322632, 'test/num_examples': 10000, 'score': 28085.67872595787, 'total_duration': 29090.06341791153, 'accumulated_submission_time': 28085.67872595787, 'accumulated_eval_time': 998.2937302589417, 'accumulated_logging_time': 3.1469409465789795}
I0130 03:36:35.468552 139864066488064 logging_writer.py:48] [82739] accumulated_eval_time=998.293730, accumulated_logging_time=3.146941, accumulated_submission_time=28085.678726, global_step=82739, preemption_count=0, score=28085.678726, test/accuracy=0.547700, test/loss=2.067953, test/num_examples=10000, total_duration=29090.063418, train/accuracy=0.751574, train/loss=0.936494, validation/accuracy=0.670440, validation/loss=1.345281, validation/num_examples=50000
I0130 03:36:56.404423 139865088849664 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.033229351043701, loss=1.395556926727295
I0130 03:37:30.288002 139864066488064 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.139601469039917, loss=1.4285491704940796
I0130 03:38:04.170294 139865088849664 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.027527093887329, loss=1.4059441089630127
I0130 03:38:38.052147 139864066488064 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.327521324157715, loss=1.4908219575881958
I0130 03:39:12.027280 139865088849664 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.0322718620300293, loss=1.3004109859466553
I0130 03:39:45.940339 139864066488064 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.135697603225708, loss=1.4616000652313232
I0130 03:40:19.834489 139865088849664 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.358159303665161, loss=1.4484519958496094
I0130 03:40:53.748267 139864066488064 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.214043617248535, loss=1.4039158821105957
I0130 03:41:27.635485 139865088849664 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.151796817779541, loss=1.4654090404510498
I0130 03:42:01.522970 139864066488064 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.2195186614990234, loss=1.4547817707061768
I0130 03:42:35.420704 139865088849664 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.242441177368164, loss=1.5148792266845703
I0130 03:43:09.291244 139864066488064 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.38815975189209, loss=1.4864907264709473
I0130 03:43:43.155255 139865088849664 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.102724313735962, loss=1.5023455619812012
I0130 03:44:17.034761 139864066488064 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.3844943046569824, loss=1.463544487953186
I0130 03:44:50.903074 139865088849664 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.321364402770996, loss=1.5440068244934082
I0130 03:45:05.623147 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:45:11.962377 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:45:20.840796 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:45:23.493799 140027215431488 submission_runner.py:408] Time since start: 29618.12s, 	Step: 84245, 	{'train/accuracy': 0.749043345451355, 'train/loss': 0.9529641270637512, 'validation/accuracy': 0.6751599907875061, 'validation/loss': 1.3240749835968018, 'validation/num_examples': 50000, 'test/accuracy': 0.544700026512146, 'test/loss': 2.050497055053711, 'test/num_examples': 10000, 'score': 28595.771988630295, 'total_duration': 29618.119492292404, 'accumulated_submission_time': 28595.771988630295, 'accumulated_eval_time': 1016.1643199920654, 'accumulated_logging_time': 3.1869609355926514}
I0130 03:45:23.527867 139864058095360 logging_writer.py:48] [84245] accumulated_eval_time=1016.164320, accumulated_logging_time=3.186961, accumulated_submission_time=28595.771989, global_step=84245, preemption_count=0, score=28595.771989, test/accuracy=0.544700, test/loss=2.050497, test/num_examples=10000, total_duration=29618.119492, train/accuracy=0.749043, train/loss=0.952964, validation/accuracy=0.675160, validation/loss=1.324075, validation/num_examples=50000
I0130 03:45:42.622848 139864066488064 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.1981043815612793, loss=1.4021499156951904
I0130 03:46:16.454216 139864058095360 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.1799724102020264, loss=1.444572925567627
I0130 03:46:50.288160 139864066488064 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.1310343742370605, loss=1.4142507314682007
I0130 03:47:24.151973 139864058095360 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.2006564140319824, loss=1.4293465614318848
I0130 03:47:58.052848 139864066488064 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.1107017993927, loss=1.3254050016403198
I0130 03:48:31.911603 139864058095360 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.4483585357666016, loss=1.3311488628387451
I0130 03:49:05.814872 139864066488064 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.813340425491333, loss=1.5732338428497314
I0130 03:49:39.680811 139864058095360 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.169316291809082, loss=1.4795414209365845
I0130 03:50:13.519434 139864066488064 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.212585210800171, loss=1.4729562997817993
I0130 03:50:47.424166 139864058095360 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.1507456302642822, loss=1.5187060832977295
I0130 03:51:21.304662 139864066488064 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.297574043273926, loss=1.3497596979141235
I0130 03:51:55.239893 139864058095360 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.4979591369628906, loss=1.4799270629882812
I0130 03:52:29.120250 139864066488064 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.3279812335968018, loss=1.408926010131836
I0130 03:53:02.976329 139864058095360 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.2133195400238037, loss=1.3850746154785156
I0130 03:53:36.868058 139864066488064 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.1726155281066895, loss=1.5301653146743774
I0130 03:53:53.625931 140027215431488 spec.py:321] Evaluating on the training split.
I0130 03:54:00.080893 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 03:54:08.643488 140027215431488 spec.py:349] Evaluating on the test split.
I0130 03:54:11.338326 140027215431488 submission_runner.py:408] Time since start: 30145.96s, 	Step: 85751, 	{'train/accuracy': 0.7525310516357422, 'train/loss': 0.9264184236526489, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.2922544479370117, 'validation/num_examples': 50000, 'test/accuracy': 0.5537000298500061, 'test/loss': 1.9823843240737915, 'test/num_examples': 10000, 'score': 29105.806889295578, 'total_duration': 30145.964017629623, 'accumulated_submission_time': 29105.806889295578, 'accumulated_eval_time': 1033.8766658306122, 'accumulated_logging_time': 3.23142409324646}
I0130 03:54:11.373439 139864058095360 logging_writer.py:48] [85751] accumulated_eval_time=1033.876666, accumulated_logging_time=3.231424, accumulated_submission_time=29105.806889, global_step=85751, preemption_count=0, score=29105.806889, test/accuracy=0.553700, test/loss=1.982384, test/num_examples=10000, total_duration=30145.964018, train/accuracy=0.752531, train/loss=0.926418, validation/accuracy=0.680160, validation/loss=1.292254, validation/num_examples=50000
I0130 03:54:28.290178 139864066488064 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.1526052951812744, loss=1.4593315124511719
I0130 03:55:02.186371 139864058095360 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.236124038696289, loss=1.4576634168624878
I0130 03:55:36.063766 139864066488064 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.2207677364349365, loss=1.4916479587554932
I0130 03:56:09.951315 139864058095360 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.515972375869751, loss=1.444025993347168
I0130 03:56:43.855769 139864066488064 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.276181697845459, loss=1.3635585308074951
I0130 03:57:17.726289 139864058095360 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.2536725997924805, loss=1.5108740329742432
I0130 03:57:51.641733 139864066488064 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.167956590652466, loss=1.3395065069198608
I0130 03:58:25.517253 139864058095360 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.3628616333007812, loss=1.3851237297058105
I0130 03:58:59.505533 139864066488064 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.4367287158966064, loss=1.4773404598236084
I0130 03:59:33.424232 139864058095360 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.2015957832336426, loss=1.498928189277649
I0130 04:00:07.316377 139864066488064 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.432269811630249, loss=1.535884976387024
I0130 04:00:41.211205 139864058095360 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.448465347290039, loss=1.4656857252120972
I0130 04:01:15.101331 139864066488064 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.2481203079223633, loss=1.4233736991882324
I0130 04:01:49.003110 139864058095360 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.3441524505615234, loss=1.4267162084579468
I0130 04:02:22.893569 139864066488064 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.4105188846588135, loss=1.4669941663742065
I0130 04:02:41.623909 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:02:47.961510 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:02:56.966545 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:02:59.656538 140027215431488 submission_runner.py:408] Time since start: 30674.28s, 	Step: 87257, 	{'train/accuracy': 0.7596858739852905, 'train/loss': 0.9148987531661987, 'validation/accuracy': 0.6706599593162537, 'validation/loss': 1.3406606912612915, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.032165765762329, 'test/num_examples': 10000, 'score': 29615.993060588837, 'total_duration': 30674.282242536545, 'accumulated_submission_time': 29615.993060588837, 'accumulated_eval_time': 1051.9092426300049, 'accumulated_logging_time': 3.2774195671081543}
I0130 04:02:59.696159 139864049702656 logging_writer.py:48] [87257] accumulated_eval_time=1051.909243, accumulated_logging_time=3.277420, accumulated_submission_time=29615.993061, global_step=87257, preemption_count=0, score=29615.993061, test/accuracy=0.548400, test/loss=2.032166, test/num_examples=10000, total_duration=30674.282243, train/accuracy=0.759686, train/loss=0.914899, validation/accuracy=0.670660, validation/loss=1.340661, validation/num_examples=50000
I0130 04:03:14.582309 139864058095360 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.9461580514907837, loss=1.3365447521209717
I0130 04:03:48.412507 139864049702656 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.060269355773926, loss=1.3833798170089722
I0130 04:04:22.303183 139864058095360 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.1953799724578857, loss=1.4500010013580322
I0130 04:04:56.172317 139864049702656 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.1903505325317383, loss=1.4539639949798584
I0130 04:05:30.103498 139864058095360 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.2011518478393555, loss=1.3748013973236084
I0130 04:06:03.973601 139864049702656 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.2580792903900146, loss=1.4281964302062988
I0130 04:06:37.825526 139864058095360 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.3261771202087402, loss=1.3827918767929077
I0130 04:07:11.728968 139864049702656 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.211672306060791, loss=1.3733644485473633
I0130 04:07:45.601222 139864058095360 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.251507043838501, loss=1.4917330741882324
I0130 04:08:19.487185 139864049702656 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.112805128097534, loss=1.3323663473129272
I0130 04:08:53.374810 139864058095360 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.3453590869903564, loss=1.4162096977233887
I0130 04:09:27.264756 139864049702656 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.1952717304229736, loss=1.408781886100769
I0130 04:10:01.125447 139864058095360 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.1680984497070312, loss=1.3718655109405518
I0130 04:10:34.971574 139864049702656 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.303894281387329, loss=1.4134764671325684
I0130 04:11:08.870410 139864058095360 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.4740850925445557, loss=1.350641131401062
I0130 04:11:29.683853 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:11:36.052194 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:11:44.538867 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:11:47.239683 140027215431488 submission_runner.py:408] Time since start: 31201.87s, 	Step: 88763, 	{'train/accuracy': 0.7681162357330322, 'train/loss': 0.8765470385551453, 'validation/accuracy': 0.6717999577522278, 'validation/loss': 1.342298150062561, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.063876152038574, 'test/num_examples': 10000, 'score': 30125.917417526245, 'total_duration': 31201.865349769592, 'accumulated_submission_time': 30125.917417526245, 'accumulated_eval_time': 1069.4649865627289, 'accumulated_logging_time': 3.327728748321533}
I0130 04:11:47.275850 139864058095360 logging_writer.py:48] [88763] accumulated_eval_time=1069.464987, accumulated_logging_time=3.327729, accumulated_submission_time=30125.917418, global_step=88763, preemption_count=0, score=30125.917418, test/accuracy=0.541100, test/loss=2.063876, test/num_examples=10000, total_duration=31201.865350, train/accuracy=0.768116, train/loss=0.876547, validation/accuracy=0.671800, validation/loss=1.342298, validation/num_examples=50000
I0130 04:12:00.232643 139865224107776 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.098353624343872, loss=1.4227259159088135
I0130 04:12:34.104708 139864058095360 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.223402976989746, loss=1.4419101476669312
I0130 04:13:07.992148 139865224107776 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.4204652309417725, loss=1.4760648012161255
I0130 04:13:41.844724 139864058095360 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.378593921661377, loss=1.426613211631775
I0130 04:14:15.706000 139865224107776 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.517632246017456, loss=1.3043761253356934
I0130 04:14:49.599859 139864058095360 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.209242820739746, loss=1.3813138008117676
I0130 04:15:23.493654 139865224107776 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.278602123260498, loss=1.3782867193222046
I0130 04:15:57.348768 139864058095360 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.1876187324523926, loss=1.3555824756622314
I0130 04:16:31.203166 139865224107776 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.250286102294922, loss=1.4087245464324951
I0130 04:17:05.094778 139864058095360 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.5318074226379395, loss=1.3093141317367554
I0130 04:17:38.959504 139865224107776 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.4995665550231934, loss=1.5278184413909912
I0130 04:18:12.826150 139864058095360 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.564265727996826, loss=1.4986387491226196
I0130 04:18:46.786165 139865224107776 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.145573854446411, loss=1.4365277290344238
I0130 04:19:20.678630 139864058095360 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.1895341873168945, loss=1.421479344367981
I0130 04:19:54.554910 139865224107776 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.2374300956726074, loss=1.3753970861434937
I0130 04:20:17.388062 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:20:23.823053 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:20:32.360304 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:20:35.129769 140027215431488 submission_runner.py:408] Time since start: 31729.76s, 	Step: 90269, 	{'train/accuracy': 0.7680763602256775, 'train/loss': 0.8671398162841797, 'validation/accuracy': 0.6808800101280212, 'validation/loss': 1.2917416095733643, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.005854368209839, 'test/num_examples': 10000, 'score': 30635.965329885483, 'total_duration': 31729.75548362732, 'accumulated_submission_time': 30635.965329885483, 'accumulated_eval_time': 1087.2066555023193, 'accumulated_logging_time': 3.3750007152557373}
I0130 04:20:35.158979 139865088849664 logging_writer.py:48] [90269] accumulated_eval_time=1087.206656, accumulated_logging_time=3.375001, accumulated_submission_time=30635.965330, global_step=90269, preemption_count=0, score=30635.965330, test/accuracy=0.551600, test/loss=2.005854, test/num_examples=10000, total_duration=31729.755484, train/accuracy=0.768076, train/loss=0.867140, validation/accuracy=0.680880, validation/loss=1.291742, validation/num_examples=50000
I0130 04:20:46.003680 139865224107776 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.500826597213745, loss=1.3580480813980103
I0130 04:21:19.812666 139865088849664 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.2615270614624023, loss=1.4932634830474854
I0130 04:21:53.691334 139865224107776 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.3032050132751465, loss=1.4039705991744995
I0130 04:22:27.559607 139865088849664 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.1213982105255127, loss=1.4201452732086182
I0130 04:23:01.401752 139865224107776 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.150908946990967, loss=1.3105502128601074
I0130 04:23:35.255788 139865088849664 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.4892375469207764, loss=1.3952986001968384
I0130 04:24:09.131017 139865224107776 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.2051141262054443, loss=1.3545140027999878
I0130 04:24:43.029189 139865088849664 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.1692068576812744, loss=1.3562195301055908
I0130 04:25:16.924223 139865224107776 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.166414260864258, loss=1.3322391510009766
I0130 04:25:50.815641 139865088849664 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.2464983463287354, loss=1.3636765480041504
I0130 04:26:24.652529 139865224107776 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.106759786605835, loss=1.3954603672027588
I0130 04:26:58.524108 139865088849664 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.405369997024536, loss=1.317215919494629
I0130 04:27:32.409711 139865224107776 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.285874128341675, loss=1.4198507070541382
I0130 04:28:06.275430 139865088849664 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.398289442062378, loss=1.4147629737854004
I0130 04:28:40.105456 139865224107776 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.408055543899536, loss=1.4620387554168701
I0130 04:29:05.297717 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:29:11.739457 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:29:20.573713 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:29:23.228676 140027215431488 submission_runner.py:408] Time since start: 32257.85s, 	Step: 91776, 	{'train/accuracy': 0.7579320669174194, 'train/loss': 0.906115710735321, 'validation/accuracy': 0.6772199869155884, 'validation/loss': 1.3106396198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.02983021736145, 'test/num_examples': 10000, 'score': 31146.04019165039, 'total_duration': 32257.85404109955, 'accumulated_submission_time': 31146.04019165039, 'accumulated_eval_time': 1105.1372406482697, 'accumulated_logging_time': 3.414245128631592}
I0130 04:29:23.264132 139864058095360 logging_writer.py:48] [91776] accumulated_eval_time=1105.137241, accumulated_logging_time=3.414245, accumulated_submission_time=31146.040192, global_step=91776, preemption_count=0, score=31146.040192, test/accuracy=0.553300, test/loss=2.029830, test/num_examples=10000, total_duration=32257.854041, train/accuracy=0.757932, train/loss=0.906116, validation/accuracy=0.677220, validation/loss=1.310640, validation/num_examples=50000
I0130 04:29:31.764999 139864066488064 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.3262739181518555, loss=1.4398064613342285
I0130 04:30:05.605719 139864058095360 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3252832889556885, loss=1.328755259513855
I0130 04:30:39.438129 139864066488064 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.1023528575897217, loss=1.4411122798919678
I0130 04:31:13.340136 139864058095360 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.459693193435669, loss=1.3704243898391724
I0130 04:31:47.325599 139864066488064 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.027245283126831, loss=1.2970077991485596
I0130 04:32:21.198621 139864058095360 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.1806747913360596, loss=1.3573445081710815
I0130 04:32:55.083042 139864066488064 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.316012382507324, loss=1.44264817237854
I0130 04:33:28.980191 139864058095360 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4130537509918213, loss=1.483630895614624
I0130 04:34:02.865132 139864066488064 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.5697238445281982, loss=1.3413416147232056
I0130 04:34:36.757730 139864058095360 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.2220818996429443, loss=1.3631811141967773
I0130 04:35:10.655366 139864066488064 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.5063982009887695, loss=1.5344147682189941
I0130 04:35:44.550487 139864058095360 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.4735381603240967, loss=1.373604655265808
I0130 04:36:18.451832 139864066488064 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.5078680515289307, loss=1.4560595750808716
I0130 04:36:52.335638 139864058095360 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.919635772705078, loss=1.3917686939239502
I0130 04:37:26.230949 139864066488064 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.360745429992676, loss=1.4648517370224
I0130 04:37:53.469393 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:37:59.764434 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:38:08.361401 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:38:11.592107 140027215431488 submission_runner.py:408] Time since start: 32786.22s, 	Step: 93282, 	{'train/accuracy': 0.7669204473495483, 'train/loss': 0.8753052949905396, 'validation/accuracy': 0.6854999661445618, 'validation/loss': 1.2829898595809937, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9982883930206299, 'test/num_examples': 10000, 'score': 31656.18217587471, 'total_duration': 32786.21779823303, 'accumulated_submission_time': 31656.18217587471, 'accumulated_eval_time': 1123.2598929405212, 'accumulated_logging_time': 3.460043430328369}
I0130 04:38:11.624238 139865224107776 logging_writer.py:48] [93282] accumulated_eval_time=1123.259893, accumulated_logging_time=3.460043, accumulated_submission_time=31656.182176, global_step=93282, preemption_count=0, score=31656.182176, test/accuracy=0.558900, test/loss=1.998288, test/num_examples=10000, total_duration=32786.217798, train/accuracy=0.766920, train/loss=0.875305, validation/accuracy=0.685500, validation/loss=1.282990, validation/num_examples=50000
I0130 04:38:18.061843 139865232500480 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.4275712966918945, loss=1.4551918506622314
I0130 04:38:51.935223 139865224107776 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.370004415512085, loss=1.3361040353775024
I0130 04:39:25.796914 139865232500480 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.3146753311157227, loss=1.396869421005249
I0130 04:39:59.636803 139865224107776 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.478667736053467, loss=1.3943876028060913
I0130 04:40:33.507958 139865232500480 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.5257210731506348, loss=1.5622788667678833
I0130 04:41:07.413777 139865224107776 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.3507394790649414, loss=1.3708120584487915
I0130 04:41:41.288017 139865232500480 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.5735366344451904, loss=1.37270188331604
I0130 04:42:15.193997 139865224107776 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.1916279792785645, loss=1.3804367780685425
I0130 04:42:49.054862 139865232500480 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.555018901824951, loss=1.3219051361083984
I0130 04:43:22.910714 139865224107776 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.3311007022857666, loss=1.3949750661849976
I0130 04:43:56.819980 139865232500480 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.661895751953125, loss=1.537393569946289
I0130 04:44:30.680510 139865224107776 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.3269975185394287, loss=1.3118420839309692
I0130 04:45:04.646315 139865232500480 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.169423818588257, loss=1.5460113286972046
I0130 04:45:38.504474 139865224107776 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.4309632778167725, loss=1.354532241821289
I0130 04:46:12.370620 139865232500480 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.511753559112549, loss=1.4990758895874023
I0130 04:46:41.670169 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:46:48.123257 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:46:56.661964 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:46:59.391934 140027215431488 submission_runner.py:408] Time since start: 33314.02s, 	Step: 94788, 	{'train/accuracy': 0.7631935477256775, 'train/loss': 0.8865559697151184, 'validation/accuracy': 0.6861400008201599, 'validation/loss': 1.2788174152374268, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 2.0052812099456787, 'test/num_examples': 10000, 'score': 32166.163598299026, 'total_duration': 33314.017624139786, 'accumulated_submission_time': 32166.163598299026, 'accumulated_eval_time': 1140.9816081523895, 'accumulated_logging_time': 3.503222703933716}
I0130 04:46:59.429618 139864058095360 logging_writer.py:48] [94788] accumulated_eval_time=1140.981608, accumulated_logging_time=3.503223, accumulated_submission_time=32166.163598, global_step=94788, preemption_count=0, score=32166.163598, test/accuracy=0.560000, test/loss=2.005281, test/num_examples=10000, total_duration=33314.017624, train/accuracy=0.763194, train/loss=0.886556, validation/accuracy=0.686140, validation/loss=1.278817, validation/num_examples=50000
I0130 04:47:03.834005 139864066488064 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.5941970348358154, loss=1.5080702304840088
I0130 04:47:37.722402 139864058095360 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.4837961196899414, loss=1.4514412879943848
I0130 04:48:11.601608 139864066488064 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.456326961517334, loss=1.4034334421157837
I0130 04:48:45.530956 139864058095360 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.1603212356567383, loss=1.2889020442962646
I0130 04:49:19.411571 139864066488064 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.2774572372436523, loss=1.3542132377624512
I0130 04:49:53.730269 139864058095360 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3516507148742676, loss=1.3140467405319214
I0130 04:50:27.618193 139864066488064 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.3761701583862305, loss=1.4168107509613037
I0130 04:51:01.461121 139864058095360 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.3307318687438965, loss=1.4075390100479126
I0130 04:51:35.446463 139864066488064 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.2947444915771484, loss=1.33805513381958
I0130 04:52:09.325096 139864058095360 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.596163511276245, loss=1.3580780029296875
I0130 04:52:43.200621 139864066488064 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.2463338375091553, loss=1.4417238235473633
I0130 04:53:17.113417 139864058095360 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.3803656101226807, loss=1.3249626159667969
I0130 04:53:50.993355 139864066488064 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.492600917816162, loss=1.3720498085021973
I0130 04:54:24.895791 139864058095360 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.382735013961792, loss=1.3519805669784546
I0130 04:54:58.798077 139864066488064 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.610348701477051, loss=1.3999239206314087
I0130 04:55:29.451408 140027215431488 spec.py:321] Evaluating on the training split.
I0130 04:55:36.563637 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 04:55:45.085077 140027215431488 spec.py:349] Evaluating on the test split.
I0130 04:55:47.812716 140027215431488 submission_runner.py:408] Time since start: 33842.44s, 	Step: 96292, 	{'train/accuracy': 0.7673788070678711, 'train/loss': 0.8581953048706055, 'validation/accuracy': 0.6811400055885315, 'validation/loss': 1.2850172519683838, 'validation/num_examples': 50000, 'test/accuracy': 0.5563000440597534, 'test/loss': 2.0307154655456543, 'test/num_examples': 10000, 'score': 32676.12209534645, 'total_duration': 33842.43831539154, 'accumulated_submission_time': 32676.12209534645, 'accumulated_eval_time': 1159.3427624702454, 'accumulated_logging_time': 3.55094575881958}
I0130 04:55:47.847590 139864049702656 logging_writer.py:48] [96292] accumulated_eval_time=1159.342762, accumulated_logging_time=3.550946, accumulated_submission_time=32676.122095, global_step=96292, preemption_count=0, score=32676.122095, test/accuracy=0.556300, test/loss=2.030715, test/num_examples=10000, total_duration=33842.438315, train/accuracy=0.767379, train/loss=0.858195, validation/accuracy=0.681140, validation/loss=1.285017, validation/num_examples=50000
I0130 04:55:50.905308 139865232500480 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.4396588802337646, loss=1.3533425331115723
I0130 04:56:24.791351 139864049702656 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.541400909423828, loss=1.4284461736679077
I0130 04:56:58.618836 139865232500480 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.5051708221435547, loss=1.389665126800537
I0130 04:57:32.485163 139864049702656 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.506349563598633, loss=1.4507170915603638
I0130 04:58:06.447924 139865232500480 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.6097354888916016, loss=1.3347876071929932
I0130 04:58:40.350188 139864049702656 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.4132416248321533, loss=1.4104728698730469
I0130 04:59:14.202077 139865232500480 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.330965042114258, loss=1.344353437423706
I0130 04:59:48.062202 139864049702656 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.4179635047912598, loss=1.284772276878357
I0130 05:00:21.954496 139865232500480 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.448692798614502, loss=1.4467064142227173
I0130 05:00:55.841751 139864049702656 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.419189214706421, loss=1.3707959651947021
I0130 05:01:29.715776 139865232500480 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.4552879333496094, loss=1.3852711915969849
I0130 05:02:03.592285 139864049702656 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.4595963954925537, loss=1.3195827007293701
I0130 05:02:37.452585 139865232500480 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.3906517028808594, loss=1.3300780057907104
I0130 05:03:11.320533 139864049702656 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.51648211479187, loss=1.4253417253494263
I0130 05:03:45.227732 139865232500480 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.3356027603149414, loss=1.315012812614441
I0130 05:04:17.923905 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:04:24.256994 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:04:32.780680 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:04:36.059157 140027215431488 submission_runner.py:408] Time since start: 34370.68s, 	Step: 97798, 	{'train/accuracy': 0.7949816584587097, 'train/loss': 0.7670559883117676, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.256996750831604, 'validation/num_examples': 50000, 'test/accuracy': 0.5625, 'test/loss': 1.9770840406417847, 'test/num_examples': 10000, 'score': 33186.13390159607, 'total_duration': 34370.68486762047, 'accumulated_submission_time': 33186.13390159607, 'accumulated_eval_time': 1177.4779727458954, 'accumulated_logging_time': 3.5960216522216797}
I0130 05:04:36.098199 139864058095360 logging_writer.py:48] [97798] accumulated_eval_time=1177.477973, accumulated_logging_time=3.596022, accumulated_submission_time=33186.133902, global_step=97798, preemption_count=0, score=33186.133902, test/accuracy=0.562500, test/loss=1.977084, test/num_examples=10000, total_duration=34370.684868, train/accuracy=0.794982, train/loss=0.767056, validation/accuracy=0.689120, validation/loss=1.256997, validation/num_examples=50000
I0130 05:04:37.116544 139865088849664 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.7094802856445312, loss=1.4531924724578857
I0130 05:05:11.002429 139864058095360 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.5047264099121094, loss=1.295118808746338
I0130 05:05:44.888724 139865088849664 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.2835962772369385, loss=1.3025649785995483
I0130 05:06:18.747199 139864058095360 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.42182993888855, loss=1.4408382177352905
I0130 05:06:52.633774 139865088849664 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.2929506301879883, loss=1.279314637184143
I0130 05:07:26.537108 139864058095360 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.312466859817505, loss=1.3553746938705444
I0130 05:08:00.392709 139865088849664 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.2763590812683105, loss=1.3033207654953003
I0130 05:08:34.260008 139864058095360 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.539700508117676, loss=1.3334565162658691
I0130 05:09:08.172256 139865088849664 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.5062742233276367, loss=1.428586483001709
I0130 05:09:42.033491 139864058095360 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.5991711616516113, loss=1.3658324480056763
I0130 05:10:15.941046 139865088849664 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.5911576747894287, loss=1.4163479804992676
I0130 05:10:49.809680 139864058095360 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.373643636703491, loss=1.3040509223937988
I0130 05:11:23.921580 139865088849664 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.586601734161377, loss=1.435030221939087
I0130 05:11:57.841028 139864058095360 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.6145074367523193, loss=1.387740969657898
I0130 05:12:31.720195 139865088849664 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.544935464859009, loss=1.3344476222991943
I0130 05:13:05.592537 139864058095360 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.5116419792175293, loss=1.3627928495407104
I0130 05:13:06.079156 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:13:12.400034 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:13:21.232711 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:13:23.928280 140027215431488 submission_runner.py:408] Time since start: 34898.55s, 	Step: 99303, 	{'train/accuracy': 0.7777224183082581, 'train/loss': 0.8399938344955444, 'validation/accuracy': 0.6829800009727478, 'validation/loss': 1.2816671133041382, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 1.9916038513183594, 'test/num_examples': 10000, 'score': 33696.05191516876, 'total_duration': 34898.55397820473, 'accumulated_submission_time': 33696.05191516876, 'accumulated_eval_time': 1195.3270378112793, 'accumulated_logging_time': 3.6447410583496094}
I0130 05:13:23.963015 139864058095360 logging_writer.py:48] [99303] accumulated_eval_time=1195.327038, accumulated_logging_time=3.644741, accumulated_submission_time=33696.051915, global_step=99303, preemption_count=0, score=33696.051915, test/accuracy=0.555200, test/loss=1.991604, test/num_examples=10000, total_duration=34898.553978, train/accuracy=0.777722, train/loss=0.839994, validation/accuracy=0.682980, validation/loss=1.281667, validation/num_examples=50000
I0130 05:13:57.125795 139864066488064 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.3697054386138916, loss=1.351532220840454
I0130 05:14:31.020290 139864058095360 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.338867425918579, loss=1.3395226001739502
I0130 05:15:04.916203 139864066488064 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.51253604888916, loss=1.3955929279327393
I0130 05:15:38.784303 139864058095360 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.6464853286743164, loss=1.3598346710205078
I0130 05:16:12.665434 139864066488064 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.3999688625335693, loss=1.2636659145355225
I0130 05:16:46.524194 139864058095360 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.4708902835845947, loss=1.3924190998077393
I0130 05:17:20.394791 139864066488064 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.3705286979675293, loss=1.2399533987045288
I0130 05:17:54.441595 139864058095360 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.618781328201294, loss=1.3573435544967651
I0130 05:18:28.343250 139864066488064 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.5700831413269043, loss=1.333478569984436
I0130 05:19:02.226335 139864058095360 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.3235714435577393, loss=1.3145174980163574
I0130 05:19:36.129287 139864066488064 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.672032594680786, loss=1.3407340049743652
I0130 05:20:09.997717 139864058095360 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.4467034339904785, loss=1.2846750020980835
I0130 05:20:43.866771 139864066488064 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.582102060317993, loss=1.2378066778182983
I0130 05:21:17.791218 139864058095360 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.2902910709381104, loss=1.2953553199768066
I0130 05:21:51.644590 139864066488064 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.3435144424438477, loss=1.2959699630737305
I0130 05:21:54.157531 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:22:00.541919 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:22:09.419452 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:22:12.093240 140027215431488 submission_runner.py:408] Time since start: 35426.72s, 	Step: 100809, 	{'train/accuracy': 0.7700493931770325, 'train/loss': 0.8601894378662109, 'validation/accuracy': 0.6826599836349487, 'validation/loss': 1.296310305595398, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.0301966667175293, 'test/num_examples': 10000, 'score': 34206.17853355408, 'total_duration': 35426.718936920166, 'accumulated_submission_time': 34206.17853355408, 'accumulated_eval_time': 1213.2627115249634, 'accumulated_logging_time': 3.693167209625244}
I0130 05:22:12.134147 139864058095360 logging_writer.py:48] [100809] accumulated_eval_time=1213.262712, accumulated_logging_time=3.693167, accumulated_submission_time=34206.178534, global_step=100809, preemption_count=0, score=34206.178534, test/accuracy=0.558400, test/loss=2.030197, test/num_examples=10000, total_duration=35426.718937, train/accuracy=0.770049, train/loss=0.860189, validation/accuracy=0.682660, validation/loss=1.296310, validation/num_examples=50000
I0130 05:22:43.260851 139865224107776 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.6309878826141357, loss=1.3829431533813477
I0130 05:23:17.088677 139864058095360 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.546226978302002, loss=1.3598294258117676
I0130 05:23:50.983723 139865224107776 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.4772872924804688, loss=1.3621206283569336
I0130 05:24:24.993683 139864058095360 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.432640552520752, loss=1.2914299964904785
I0130 05:24:58.869430 139865224107776 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.6240625381469727, loss=1.2607125043869019
I0130 05:25:32.766379 139864058095360 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.521941661834717, loss=1.2881355285644531
I0130 05:26:06.644386 139865224107776 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.5058088302612305, loss=1.3353745937347412
I0130 05:26:40.523522 139864058095360 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.487060546875, loss=1.3524993658065796
I0130 05:27:14.414949 139865224107776 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.360844850540161, loss=1.4310871362686157
I0130 05:27:48.295008 139864058095360 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.5031824111938477, loss=1.250985026359558
I0130 05:28:22.195842 139865224107776 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.257326364517212, loss=1.3163375854492188
I0130 05:28:56.065708 139864058095360 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.5258562564849854, loss=1.3345009088516235
I0130 05:29:29.938790 139865224107776 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.3347830772399902, loss=1.3367586135864258
I0130 05:30:03.809139 139864058095360 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.4469375610351562, loss=1.3631495237350464
I0130 05:30:37.719302 139865224107776 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.7322030067443848, loss=1.3588342666625977
I0130 05:30:42.256867 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:30:48.593586 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:30:57.347248 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:31:00.161585 140027215431488 submission_runner.py:408] Time since start: 35954.79s, 	Step: 102315, 	{'train/accuracy': 0.7760483026504517, 'train/loss': 0.8299661874771118, 'validation/accuracy': 0.6898399591445923, 'validation/loss': 1.2503280639648438, 'validation/num_examples': 50000, 'test/accuracy': 0.5603000521659851, 'test/loss': 1.9734432697296143, 'test/num_examples': 10000, 'score': 34716.23765563965, 'total_duration': 35954.78728866577, 'accumulated_submission_time': 34716.23765563965, 'accumulated_eval_time': 1231.1673793792725, 'accumulated_logging_time': 3.744328260421753}
I0130 05:31:00.200981 139864049702656 logging_writer.py:48] [102315] accumulated_eval_time=1231.167379, accumulated_logging_time=3.744328, accumulated_submission_time=34716.237656, global_step=102315, preemption_count=0, score=34716.237656, test/accuracy=0.560300, test/loss=1.973443, test/num_examples=10000, total_duration=35954.787289, train/accuracy=0.776048, train/loss=0.829966, validation/accuracy=0.689840, validation/loss=1.250328, validation/num_examples=50000
I0130 05:31:29.322982 139864058095360 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.436436653137207, loss=1.3092734813690186
I0130 05:32:03.180986 139864049702656 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.285621404647827, loss=1.2570139169692993
I0130 05:32:37.040087 139864058095360 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.5363852977752686, loss=1.2834545373916626
I0130 05:33:10.893727 139864049702656 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.3159563541412354, loss=1.2870019674301147
I0130 05:33:44.735491 139864058095360 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.6533162593841553, loss=1.3335882425308228
I0130 05:34:18.621485 139864049702656 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.506011486053467, loss=1.3172569274902344
I0130 05:34:52.534596 139864058095360 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.5280814170837402, loss=1.2910178899765015
I0130 05:35:26.394332 139864049702656 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.2988877296447754, loss=1.2195777893066406
I0130 05:36:00.321136 139864058095360 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.235358476638794, loss=1.3110642433166504
I0130 05:36:34.178865 139864049702656 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.6077721118927, loss=1.3929550647735596
I0130 05:37:08.084245 139864058095360 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.5885705947875977, loss=1.3165584802627563
I0130 05:37:42.015436 139864049702656 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.459296703338623, loss=1.3293536901474
I0130 05:38:15.925916 139864058095360 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.635283946990967, loss=1.3786426782608032
I0130 05:38:49.803037 139864049702656 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.435725212097168, loss=1.2304105758666992
I0130 05:39:23.707654 139864058095360 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.3118743896484375, loss=1.2281874418258667
I0130 05:39:30.298888 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:39:36.719264 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:39:45.534730 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:39:48.210748 140027215431488 submission_runner.py:408] Time since start: 36482.84s, 	Step: 103821, 	{'train/accuracy': 0.76761794090271, 'train/loss': 0.8790847659111023, 'validation/accuracy': 0.685699999332428, 'validation/loss': 1.2791662216186523, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.9737340211868286, 'test/num_examples': 10000, 'score': 35226.270773649216, 'total_duration': 36482.83643579483, 'accumulated_submission_time': 35226.270773649216, 'accumulated_eval_time': 1249.0791852474213, 'accumulated_logging_time': 3.7941339015960693}
I0130 05:39:48.245906 139865232500480 logging_writer.py:48] [103821] accumulated_eval_time=1249.079185, accumulated_logging_time=3.794134, accumulated_submission_time=35226.270774, global_step=103821, preemption_count=0, score=35226.270774, test/accuracy=0.568100, test/loss=1.973734, test/num_examples=10000, total_duration=36482.836436, train/accuracy=0.767618, train/loss=0.879085, validation/accuracy=0.685700, validation/loss=1.279166, validation/num_examples=50000
I0130 05:40:15.339639 139865240893184 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.499704360961914, loss=1.2596462965011597
I0130 05:40:49.193668 139865232500480 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.6101505756378174, loss=1.2067253589630127
I0130 05:41:23.031543 139865240893184 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.624129295349121, loss=1.2873693704605103
I0130 05:41:56.918025 139865232500480 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.606447458267212, loss=1.3449254035949707
I0130 05:42:30.813270 139865240893184 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.34659743309021, loss=1.3157223463058472
I0130 05:43:04.709566 139865232500480 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.380556583404541, loss=1.3199384212493896
I0130 05:43:38.591995 139865240893184 logging_writer.py:48] [104500] global_step=104500, grad_norm=3.169513702392578, loss=1.350298285484314
I0130 05:44:12.568340 139865232500480 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.7188491821289062, loss=1.393682599067688
I0130 05:44:46.474670 139865240893184 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.390958070755005, loss=1.291368007659912
I0130 05:45:20.375599 139865232500480 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.57026743888855, loss=1.333850622177124
I0130 05:45:54.236641 139865240893184 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.67507266998291, loss=1.3246749639511108
I0130 05:46:28.161473 139865232500480 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.385249376296997, loss=1.1737351417541504
I0130 05:47:02.010363 139865240893184 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.45882248878479, loss=1.246537208557129
I0130 05:47:35.880785 139865232500480 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.6521384716033936, loss=1.427524447441101
I0130 05:48:09.788412 139865240893184 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.3354785442352295, loss=1.2697019577026367
I0130 05:48:18.427381 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:48:24.766788 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:48:33.458623 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:48:36.828125 140027215431488 submission_runner.py:408] Time since start: 37011.45s, 	Step: 105327, 	{'train/accuracy': 0.7800143361091614, 'train/loss': 0.810962438583374, 'validation/accuracy': 0.6937800049781799, 'validation/loss': 1.2509777545928955, 'validation/num_examples': 50000, 'test/accuracy': 0.5646000504493713, 'test/loss': 1.9978750944137573, 'test/num_examples': 10000, 'score': 35736.3866379261, 'total_duration': 37011.45383000374, 'accumulated_submission_time': 35736.3866379261, 'accumulated_eval_time': 1267.4798917770386, 'accumulated_logging_time': 3.8419394493103027}
I0130 05:48:36.861368 139864058095360 logging_writer.py:48] [105327] accumulated_eval_time=1267.479892, accumulated_logging_time=3.841939, accumulated_submission_time=35736.386638, global_step=105327, preemption_count=0, score=35736.386638, test/accuracy=0.564600, test/loss=1.997875, test/num_examples=10000, total_duration=37011.453830, train/accuracy=0.780014, train/loss=0.810962, validation/accuracy=0.693780, validation/loss=1.250978, validation/num_examples=50000
I0130 05:49:01.900766 139864066488064 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.6197078227996826, loss=1.3679028749465942
I0130 05:49:35.758762 139864058095360 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.885911464691162, loss=1.3660539388656616
I0130 05:50:09.662291 139864066488064 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.588698387145996, loss=1.2892719507217407
I0130 05:50:43.631782 139864058095360 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.5035855770111084, loss=1.301285982131958
I0130 05:51:17.502249 139864066488064 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.461190938949585, loss=1.2927465438842773
I0130 05:51:51.380034 139864058095360 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.318668842315674, loss=1.252331256866455
I0130 05:52:25.296097 139864066488064 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.6103739738464355, loss=1.3488956689834595
I0130 05:52:59.144799 139864058095360 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.5093650817871094, loss=1.2738821506500244
I0130 05:53:33.006671 139864066488064 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.787187099456787, loss=1.2303779125213623
I0130 05:54:06.900818 139864058095360 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.375981092453003, loss=1.2297574281692505
I0130 05:54:40.764155 139864066488064 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.568749189376831, loss=1.378654956817627
I0130 05:55:14.630767 139864058095360 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.685739517211914, loss=1.3722699880599976
I0130 05:55:48.530869 139864066488064 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.773348331451416, loss=1.3602051734924316
I0130 05:56:22.395265 139864058095360 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.6547346115112305, loss=1.3874006271362305
I0130 05:56:56.279988 139864066488064 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.7206194400787354, loss=1.263831615447998
I0130 05:57:07.203846 140027215431488 spec.py:321] Evaluating on the training split.
I0130 05:57:13.525736 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 05:57:22.051401 140027215431488 spec.py:349] Evaluating on the test split.
I0130 05:57:24.899798 140027215431488 submission_runner.py:408] Time since start: 37539.53s, 	Step: 106833, 	{'train/accuracy': 0.8098094463348389, 'train/loss': 0.6974734663963318, 'validation/accuracy': 0.6970799565315247, 'validation/loss': 1.2289150953292847, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.9629000425338745, 'test/num_examples': 10000, 'score': 36246.66552352905, 'total_duration': 37539.52548789978, 'accumulated_submission_time': 36246.66552352905, 'accumulated_eval_time': 1285.1757638454437, 'accumulated_logging_time': 3.8856961727142334}
I0130 05:57:24.938703 139865232500480 logging_writer.py:48] [106833] accumulated_eval_time=1285.175764, accumulated_logging_time=3.885696, accumulated_submission_time=36246.665524, global_step=106833, preemption_count=0, score=36246.665524, test/accuracy=0.570100, test/loss=1.962900, test/num_examples=10000, total_duration=37539.525488, train/accuracy=0.809809, train/loss=0.697473, validation/accuracy=0.697080, validation/loss=1.228915, validation/num_examples=50000
I0130 05:57:47.947913 139865240893184 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.7610175609588623, loss=1.275016188621521
I0130 05:58:21.795603 139865232500480 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.4083633422851562, loss=1.246017575263977
I0130 05:58:55.678060 139865240893184 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6866848468780518, loss=1.2817844152450562
I0130 05:59:29.537502 139865232500480 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.8169870376586914, loss=1.2433269023895264
I0130 06:00:03.394912 139865240893184 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.4855942726135254, loss=1.2433122396469116
I0130 06:00:37.305575 139865232500480 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.486546277999878, loss=1.2669695615768433
I0130 06:01:11.178153 139865240893184 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.5540390014648438, loss=1.2074962854385376
I0130 06:01:45.037630 139865232500480 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.4114651679992676, loss=1.2300680875778198
I0130 06:02:18.941390 139865240893184 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.7706048488616943, loss=1.4150824546813965
I0130 06:02:52.797061 139865232500480 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.4608314037323, loss=1.3040320873260498
I0130 06:03:26.690185 139865240893184 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.9401051998138428, loss=1.3397709131240845
I0130 06:04:00.776891 139865232500480 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.613210439682007, loss=1.3555330038070679
I0130 06:04:34.653714 139865240893184 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.5459413528442383, loss=1.308694839477539
I0130 06:05:08.554295 139865232500480 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.7674944400787354, loss=1.2469910383224487
I0130 06:05:42.414692 139865240893184 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.39821457862854, loss=1.2365715503692627
I0130 06:05:55.063427 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:06:01.414135 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:06:10.233450 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:06:12.922960 140027215431488 submission_runner.py:408] Time since start: 38067.55s, 	Step: 108339, 	{'train/accuracy': 0.8022361397743225, 'train/loss': 0.7281384468078613, 'validation/accuracy': 0.6987000107765198, 'validation/loss': 1.2137079238891602, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9371442794799805, 'test/num_examples': 10000, 'score': 36756.7243309021, 'total_duration': 38067.54865336418, 'accumulated_submission_time': 36756.7243309021, 'accumulated_eval_time': 1303.0352365970612, 'accumulated_logging_time': 3.9351346492767334}
I0130 06:06:12.959355 139864058095360 logging_writer.py:48] [108339] accumulated_eval_time=1303.035237, accumulated_logging_time=3.935135, accumulated_submission_time=36756.724331, global_step=108339, preemption_count=0, score=36756.724331, test/accuracy=0.572700, test/loss=1.937144, test/num_examples=10000, total_duration=38067.548653, train/accuracy=0.802236, train/loss=0.728138, validation/accuracy=0.698700, validation/loss=1.213708, validation/num_examples=50000
I0130 06:06:33.912351 139864066488064 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.7946853637695312, loss=1.3354119062423706
I0130 06:07:07.767308 139864058095360 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.5097572803497314, loss=1.2262157201766968
I0130 06:07:41.647519 139864066488064 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.678915023803711, loss=1.3332340717315674
I0130 06:08:15.506793 139864058095360 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.774744749069214, loss=1.3079404830932617
I0130 06:08:49.398725 139864066488064 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.672701120376587, loss=1.2939494848251343
I0130 06:09:23.229478 139864058095360 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.716053009033203, loss=1.2377368211746216
I0130 06:09:57.111719 139864066488064 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.894444465637207, loss=1.3505580425262451
I0130 06:10:31.141035 139864058095360 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.7367923259735107, loss=1.3807923793792725
I0130 06:11:05.019222 139864066488064 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.488265037536621, loss=1.2833560705184937
I0130 06:11:38.890269 139864058095360 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.6518259048461914, loss=1.3409535884857178
I0130 06:12:12.767569 139864066488064 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.5264244079589844, loss=1.2507455348968506
I0130 06:12:46.669197 139864058095360 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.545515775680542, loss=1.2471858263015747
I0130 06:13:20.570287 139864066488064 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.8835203647613525, loss=1.4147754907608032
I0130 06:13:54.461241 139864058095360 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.621321678161621, loss=1.2842286825180054
I0130 06:14:28.355259 139864066488064 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.6006035804748535, loss=1.3022959232330322
I0130 06:14:43.044706 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:14:49.378929 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:14:57.909216 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:15:00.592705 140027215431488 submission_runner.py:408] Time since start: 38595.22s, 	Step: 109845, 	{'train/accuracy': 0.795320451259613, 'train/loss': 0.7437325119972229, 'validation/accuracy': 0.6987400054931641, 'validation/loss': 1.229027509689331, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.9284789562225342, 'test/num_examples': 10000, 'score': 37266.74565792084, 'total_duration': 38595.21840763092, 'accumulated_submission_time': 37266.74565792084, 'accumulated_eval_time': 1320.5831859111786, 'accumulated_logging_time': 3.982501983642578}
I0130 06:15:00.631710 139865088849664 logging_writer.py:48] [109845] accumulated_eval_time=1320.583186, accumulated_logging_time=3.982502, accumulated_submission_time=37266.745658, global_step=109845, preemption_count=0, score=37266.745658, test/accuracy=0.574200, test/loss=1.928479, test/num_examples=10000, total_duration=38595.218408, train/accuracy=0.795320, train/loss=0.743733, validation/accuracy=0.698740, validation/loss=1.229028, validation/num_examples=50000
I0130 06:15:19.631294 139865224107776 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.545607805252075, loss=1.2794597148895264
I0130 06:15:53.714471 139865088849664 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.7574682235717773, loss=1.3204255104064941
I0130 06:16:27.588612 139865224107776 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.9253687858581543, loss=1.2864489555358887
I0130 06:17:01.646793 139865088849664 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.6573784351348877, loss=1.2795391082763672
I0130 06:17:35.517707 139865224107776 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.7364344596862793, loss=1.1812872886657715
I0130 06:18:09.401217 139865088849664 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.893122673034668, loss=1.2994436025619507
I0130 06:18:43.253329 139865224107776 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.6634998321533203, loss=1.2286268472671509
I0130 06:19:17.126968 139865088849664 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.7143197059631348, loss=1.2431895732879639
I0130 06:19:51.006459 139865224107776 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.409968137741089, loss=1.3274868726730347
I0130 06:20:24.853452 139865088849664 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.677394151687622, loss=1.2339024543762207
I0130 06:20:58.745608 139865224107776 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.822709798812866, loss=1.326515793800354
I0130 06:21:32.635619 139865088849664 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.6942975521087646, loss=1.1731805801391602
I0130 06:22:06.539217 139865224107776 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.7078728675842285, loss=1.1359663009643555
I0130 06:22:40.427482 139865088849664 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.5997745990753174, loss=1.287529468536377
I0130 06:23:14.287048 139865224107776 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.745236873626709, loss=1.2188341617584229
I0130 06:23:30.796931 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:23:37.112711 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:23:45.634895 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:23:48.299901 140027215431488 submission_runner.py:408] Time since start: 39122.93s, 	Step: 111350, 	{'train/accuracy': 0.8005022406578064, 'train/loss': 0.7239665985107422, 'validation/accuracy': 0.7046999931335449, 'validation/loss': 1.1938434839248657, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8825327157974243, 'test/num_examples': 10000, 'score': 37776.84398150444, 'total_duration': 39122.9255297184, 'accumulated_submission_time': 37776.84398150444, 'accumulated_eval_time': 1338.08602809906, 'accumulated_logging_time': 4.032949924468994}
I0130 06:23:48.337148 139865115051776 logging_writer.py:48] [111350] accumulated_eval_time=1338.086028, accumulated_logging_time=4.032950, accumulated_submission_time=37776.843982, global_step=111350, preemption_count=0, score=37776.843982, test/accuracy=0.587800, test/loss=1.882533, test/num_examples=10000, total_duration=39122.925530, train/accuracy=0.800502, train/loss=0.723967, validation/accuracy=0.704700, validation/loss=1.193843, validation/num_examples=50000
I0130 06:24:05.630934 139865123444480 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.5219483375549316, loss=1.2777208089828491
I0130 06:24:39.467251 139865115051776 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.619504690170288, loss=1.209435224533081
I0130 06:25:13.315696 139865123444480 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.84030818939209, loss=1.261436939239502
I0130 06:25:47.208300 139865115051776 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.8020565509796143, loss=1.258238434791565
I0130 06:26:21.051814 139865123444480 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.8124067783355713, loss=1.3407162427902222
I0130 06:26:54.970566 139865115051776 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.7188422679901123, loss=1.2296150922775269
I0130 06:27:28.830964 139865123444480 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.655397653579712, loss=1.2590242624282837
I0130 06:28:02.688186 139865115051776 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.592599630355835, loss=1.2006618976593018
I0130 06:28:36.571402 139865123444480 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6180129051208496, loss=1.3178859949111938
I0130 06:29:10.446894 139865115051776 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.589270830154419, loss=1.2420742511749268
I0130 06:29:44.284891 139865123444480 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.8813869953155518, loss=1.2251205444335938
I0130 06:30:18.248714 139865115051776 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.764427423477173, loss=1.2880336046218872
I0130 06:30:52.101285 139865123444480 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.6861727237701416, loss=1.312511920928955
I0130 06:31:25.985701 139865115051776 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.6127235889434814, loss=1.2514971494674683
I0130 06:31:59.850569 139865123444480 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.5311501026153564, loss=1.193282127380371
I0130 06:32:18.636454 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:32:24.987451 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:32:33.785077 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:32:37.146870 140027215431488 submission_runner.py:408] Time since start: 39651.77s, 	Step: 112857, 	{'train/accuracy': 0.7922711968421936, 'train/loss': 0.7695667147636414, 'validation/accuracy': 0.7007399797439575, 'validation/loss': 1.2252343893051147, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 1.942015290260315, 'test/num_examples': 10000, 'score': 38287.077071905136, 'total_duration': 39651.77258491516, 'accumulated_submission_time': 38287.077071905136, 'accumulated_eval_time': 1356.5964069366455, 'accumulated_logging_time': 4.082559108734131}
I0130 06:32:37.187973 139865224107776 logging_writer.py:48] [112857] accumulated_eval_time=1356.596407, accumulated_logging_time=4.082559, accumulated_submission_time=38287.077072, global_step=112857, preemption_count=0, score=38287.077072, test/accuracy=0.576100, test/loss=1.942015, test/num_examples=10000, total_duration=39651.772585, train/accuracy=0.792271, train/loss=0.769567, validation/accuracy=0.700740, validation/loss=1.225234, validation/num_examples=50000
I0130 06:32:52.096007 139865232500480 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.786977767944336, loss=1.24300217628479
I0130 06:33:25.918317 139865224107776 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.9880690574645996, loss=1.336350917816162
I0130 06:33:59.809402 139865232500480 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.763745069503784, loss=1.1874828338623047
I0130 06:34:33.690816 139865224107776 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.956376075744629, loss=1.2499228715896606
I0130 06:35:07.559283 139865232500480 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.86661434173584, loss=1.3432271480560303
I0130 06:35:41.409928 139865224107776 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.6410598754882812, loss=1.1317545175552368
I0130 06:36:15.279507 139865232500480 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.790677547454834, loss=1.2189472913742065
I0130 06:36:49.245450 139865224107776 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.9079959392547607, loss=1.25149405002594
I0130 06:37:23.098178 139865232500480 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.7563507556915283, loss=1.1742888689041138
I0130 06:37:56.979471 139865224107776 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.9957447052001953, loss=1.2867472171783447
I0130 06:38:30.890499 139865232500480 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.654952049255371, loss=1.1981358528137207
I0130 06:39:04.770573 139865224107776 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.8041000366210938, loss=1.1970298290252686
I0130 06:39:38.630746 139865232500480 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.7319281101226807, loss=1.23048734664917
I0130 06:40:12.515456 139865224107776 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.906968593597412, loss=1.3395390510559082
I0130 06:40:46.344052 139865232500480 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.835151195526123, loss=1.2976980209350586
I0130 06:41:07.150839 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:41:13.509090 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:41:22.375979 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:41:25.041501 140027215431488 submission_runner.py:408] Time since start: 40179.67s, 	Step: 114363, 	{'train/accuracy': 0.797293484210968, 'train/loss': 0.7358038425445557, 'validation/accuracy': 0.7057999968528748, 'validation/loss': 1.2084553241729736, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 1.9387891292572021, 'test/num_examples': 10000, 'score': 38796.97853899002, 'total_duration': 40179.667202949524, 'accumulated_submission_time': 38796.97853899002, 'accumulated_eval_time': 1374.4870157241821, 'accumulated_logging_time': 4.132639169692993}
I0130 06:41:25.079697 139865123444480 logging_writer.py:48] [114363] accumulated_eval_time=1374.487016, accumulated_logging_time=4.132639, accumulated_submission_time=38796.978539, global_step=114363, preemption_count=0, score=38796.978539, test/accuracy=0.576100, test/loss=1.938789, test/num_examples=10000, total_duration=40179.667203, train/accuracy=0.797293, train/loss=0.735804, validation/accuracy=0.705800, validation/loss=1.208455, validation/num_examples=50000
I0130 06:41:37.937197 139865131837184 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.676337718963623, loss=1.2502498626708984
I0130 06:42:11.798614 139865123444480 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.84905743598938, loss=1.1348222494125366
I0130 06:42:45.673752 139865131837184 logging_writer.py:48] [114600] global_step=114600, grad_norm=3.017714738845825, loss=1.333676815032959
I0130 06:43:19.693288 139865123444480 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.8353896141052246, loss=1.1591020822525024
I0130 06:43:53.566741 139865131837184 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.8199305534362793, loss=1.2103477716445923
I0130 06:44:27.438989 139865123444480 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.838895797729492, loss=1.189772367477417
I0130 06:45:01.295770 139865131837184 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.938429832458496, loss=1.2262221574783325
I0130 06:45:35.160730 139865123444480 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.7354729175567627, loss=1.1947321891784668
I0130 06:46:09.049591 139865131837184 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.597752094268799, loss=1.2650521993637085
I0130 06:46:42.889641 139865123444480 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.591538906097412, loss=1.093819260597229
I0130 06:47:16.736248 139865131837184 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.1239864826202393, loss=1.344772219657898
I0130 06:47:50.600320 139865123444480 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.7524302005767822, loss=1.2034624814987183
I0130 06:48:24.479133 139865131837184 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.625892400741577, loss=1.2473464012145996
I0130 06:48:58.317829 139865123444480 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.806462049484253, loss=1.3647419214248657
I0130 06:49:32.153026 139865131837184 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.754547357559204, loss=1.1845355033874512
I0130 06:49:55.123838 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:50:01.515288 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:50:10.391065 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:50:13.064756 140027215431488 submission_runner.py:408] Time since start: 40707.69s, 	Step: 115869, 	{'train/accuracy': 0.8302773833274841, 'train/loss': 0.6240458488464355, 'validation/accuracy': 0.703719973564148, 'validation/loss': 1.2063405513763428, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.9542590379714966, 'test/num_examples': 10000, 'score': 39306.96019792557, 'total_duration': 40707.69045972824, 'accumulated_submission_time': 39306.96019792557, 'accumulated_eval_time': 1392.4278795719147, 'accumulated_logging_time': 4.181373596191406}
I0130 06:50:13.101545 139865115051776 logging_writer.py:48] [115869] accumulated_eval_time=1392.427880, accumulated_logging_time=4.181374, accumulated_submission_time=39306.960198, global_step=115869, preemption_count=0, score=39306.960198, test/accuracy=0.575000, test/loss=1.954259, test/num_examples=10000, total_duration=40707.690460, train/accuracy=0.830277, train/loss=0.624046, validation/accuracy=0.703720, validation/loss=1.206341, validation/num_examples=50000
I0130 06:50:23.948349 139865123444480 logging_writer.py:48] [115900] global_step=115900, grad_norm=3.042574644088745, loss=1.3154629468917847
I0130 06:50:57.830160 139865115051776 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.7597362995147705, loss=1.2003909349441528
I0130 06:51:31.694609 139865123444480 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.8367979526519775, loss=1.3407261371612549
I0130 06:52:05.536501 139865115051776 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.779871702194214, loss=1.2252446413040161
I0130 06:52:39.425060 139865123444480 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.7918825149536133, loss=1.1818029880523682
I0130 06:53:13.301595 139865115051776 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.8058907985687256, loss=1.216988444328308
I0130 06:53:47.160613 139865123444480 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.6709048748016357, loss=1.1498799324035645
I0130 06:54:21.082175 139865115051776 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.643082618713379, loss=1.235005259513855
I0130 06:54:54.931683 139865123444480 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.8309149742126465, loss=1.2520244121551514
I0130 06:55:28.832952 139865115051776 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.974714517593384, loss=1.2017240524291992
I0130 06:56:02.682310 139865123444480 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.974571943283081, loss=1.2415666580200195
I0130 06:56:36.662441 139865115051776 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.605983018875122, loss=1.1062281131744385
I0130 06:57:10.562096 139865123444480 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.910745143890381, loss=1.1699023246765137
I0130 06:57:44.435355 139865115051776 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.0009493827819824, loss=1.1833717823028564
I0130 06:58:18.344802 139865123444480 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.968247890472412, loss=1.261928677558899
I0130 06:58:43.227437 140027215431488 spec.py:321] Evaluating on the training split.
I0130 06:58:49.558110 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 06:58:58.341887 140027215431488 spec.py:349] Evaluating on the test split.
I0130 06:59:01.016170 140027215431488 submission_runner.py:408] Time since start: 41235.64s, 	Step: 117375, 	{'train/accuracy': 0.8184191584587097, 'train/loss': 0.661994993686676, 'validation/accuracy': 0.702019989490509, 'validation/loss': 1.2063285112380981, 'validation/num_examples': 50000, 'test/accuracy': 0.5832000374794006, 'test/loss': 1.9158483743667603, 'test/num_examples': 10000, 'score': 39817.018862485886, 'total_duration': 41235.641859054565, 'accumulated_submission_time': 39817.018862485886, 'accumulated_eval_time': 1410.2165460586548, 'accumulated_logging_time': 4.232245206832886}
I0130 06:59:01.059148 139865115051776 logging_writer.py:48] [117375] accumulated_eval_time=1410.216546, accumulated_logging_time=4.232245, accumulated_submission_time=39817.018862, global_step=117375, preemption_count=0, score=39817.018862, test/accuracy=0.583200, test/loss=1.915848, test/num_examples=10000, total_duration=41235.641859, train/accuracy=0.818419, train/loss=0.661995, validation/accuracy=0.702020, validation/loss=1.206329, validation/num_examples=50000
I0130 06:59:09.883911 139865140229888 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.9026355743408203, loss=1.2510734796524048
I0130 06:59:43.761031 139865115051776 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.743375539779663, loss=1.2017288208007812
I0130 07:00:17.621445 139865140229888 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.842921257019043, loss=1.2010552883148193
I0130 07:00:51.522464 139865115051776 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.3214924335479736, loss=1.1917331218719482
I0130 07:01:25.372884 139865140229888 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.9400219917297363, loss=1.3140684366226196
I0130 07:01:59.246872 139865115051776 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.9500529766082764, loss=1.2397661209106445
I0130 07:02:33.145040 139865140229888 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.9777090549468994, loss=1.269838571548462
I0130 07:03:07.150419 139865115051776 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.8315818309783936, loss=1.2840772867202759
I0130 07:03:41.017253 139865140229888 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8771350383758545, loss=1.2654032707214355
I0130 07:04:14.878117 139865115051776 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.092343330383301, loss=1.258610725402832
I0130 07:04:48.735034 139865140229888 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.957252025604248, loss=1.1034823656082153
I0130 07:05:22.614612 139865115051776 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.7423887252807617, loss=1.217047095298767
I0130 07:05:56.519356 139865140229888 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.8491005897521973, loss=1.1579468250274658
I0130 07:06:30.327409 139865115051776 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.854773998260498, loss=1.1746591329574585
I0130 07:07:04.202394 139865140229888 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.2829434871673584, loss=1.1654599905014038
I0130 07:07:31.143765 140027215431488 spec.py:321] Evaluating on the training split.
I0130 07:07:37.454141 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 07:07:46.043715 140027215431488 spec.py:349] Evaluating on the test split.
I0130 07:07:48.644208 140027215431488 submission_runner.py:408] Time since start: 41763.27s, 	Step: 118881, 	{'train/accuracy': 0.8126594424247742, 'train/loss': 0.6735084652900696, 'validation/accuracy': 0.7055599689483643, 'validation/loss': 1.2065515518188477, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.93263578414917, 'test/num_examples': 10000, 'score': 40327.03925204277, 'total_duration': 41763.26990413666, 'accumulated_submission_time': 40327.03925204277, 'accumulated_eval_time': 1427.716940164566, 'accumulated_logging_time': 4.2863757610321045}
I0130 07:07:48.685898 139865224107776 logging_writer.py:48] [118881] accumulated_eval_time=1427.716940, accumulated_logging_time=4.286376, accumulated_submission_time=40327.039252, global_step=118881, preemption_count=0, score=40327.039252, test/accuracy=0.580700, test/loss=1.932636, test/num_examples=10000, total_duration=41763.269904, train/accuracy=0.812659, train/loss=0.673508, validation/accuracy=0.705560, validation/loss=1.206552, validation/num_examples=50000
I0130 07:07:55.464827 139865232500480 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.9063525199890137, loss=1.2066963911056519
I0130 07:08:29.337308 139865224107776 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.1129021644592285, loss=1.2269821166992188
I0130 07:09:03.176584 139865232500480 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.9767942428588867, loss=1.2502647638320923
I0130 07:09:37.141566 139865224107776 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.983767032623291, loss=1.1697298288345337
I0130 07:10:10.984071 139865232500480 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.723789691925049, loss=1.1956052780151367
I0130 07:10:44.854402 139865224107776 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.142014980316162, loss=1.1758146286010742
I0130 07:11:18.734286 139865232500480 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.8113303184509277, loss=1.268344759941101
I0130 07:11:52.591485 139865224107776 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.1122589111328125, loss=1.2687565088272095
I0130 07:12:26.449888 139865232500480 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.789263963699341, loss=1.1642109155654907
I0130 07:13:00.326594 139865224107776 logging_writer.py:48] [119800] global_step=119800, grad_norm=3.1446101665496826, loss=1.320090651512146
I0130 07:13:34.211976 139865232500480 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.012960910797119, loss=1.177907943725586
I0130 07:14:08.098967 139865224107776 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.680434226989746, loss=1.1614553928375244
I0130 07:14:41.985233 139865232500480 logging_writer.py:48] [120100] global_step=120100, grad_norm=3.0237479209899902, loss=1.199796199798584
I0130 07:15:15.847355 139865224107776 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.8016817569732666, loss=1.198294997215271
I0130 07:15:49.715665 139865232500480 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.2690649032592773, loss=1.20453941822052
I0130 07:16:18.762342 140027215431488 spec.py:321] Evaluating on the training split.
I0130 07:16:25.121973 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 07:16:33.931334 140027215431488 spec.py:349] Evaluating on the test split.
I0130 07:16:37.247804 140027215431488 submission_runner.py:408] Time since start: 42291.87s, 	Step: 120387, 	{'train/accuracy': 0.8160474896430969, 'train/loss': 0.6632678508758545, 'validation/accuracy': 0.7124399542808533, 'validation/loss': 1.1698178052902222, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.8709394931793213, 'test/num_examples': 10000, 'score': 40837.051486730576, 'total_duration': 42291.873514175415, 'accumulated_submission_time': 40837.051486730576, 'accumulated_eval_time': 1446.2023582458496, 'accumulated_logging_time': 4.338989973068237}
I0130 07:16:37.283374 139865123444480 logging_writer.py:48] [120387] accumulated_eval_time=1446.202358, accumulated_logging_time=4.338990, accumulated_submission_time=40837.051487, global_step=120387, preemption_count=0, score=40837.051487, test/accuracy=0.587100, test/loss=1.870939, test/num_examples=10000, total_duration=42291.873514, train/accuracy=0.816047, train/loss=0.663268, validation/accuracy=0.712440, validation/loss=1.169818, validation/num_examples=50000
I0130 07:16:42.030827 139865131837184 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.8637683391571045, loss=1.1396350860595703
I0130 07:17:15.895714 139865123444480 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.845111131668091, loss=1.1356736421585083
I0130 07:17:49.749321 139865131837184 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.9075307846069336, loss=1.2258546352386475
I0130 07:18:23.585389 139865123444480 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.125436782836914, loss=1.205854058265686
I0130 07:18:57.432425 139865131837184 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.900871992111206, loss=1.140724778175354
I0130 07:19:31.328414 139865123444480 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.7596518993377686, loss=1.0693562030792236
I0130 07:20:05.173675 139865131837184 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.9703526496887207, loss=1.2285151481628418
I0130 07:20:39.037578 139865123444480 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.962439775466919, loss=1.1385116577148438
I0130 07:21:12.882814 139865131837184 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.9086034297943115, loss=1.047612190246582
I0130 07:21:46.777344 139865123444480 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.843637228012085, loss=1.1402870416641235
I0130 07:22:20.649808 139865131837184 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.962604284286499, loss=1.176024317741394
I0130 07:22:54.612268 139865123444480 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.770582675933838, loss=1.1203489303588867
I0130 07:23:28.458779 139865131837184 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.134129285812378, loss=1.223268985748291
I0130 07:24:02.318076 139865123444480 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.030156373977661, loss=1.1638343334197998
I0130 07:24:36.215491 139865131837184 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.2159624099731445, loss=1.2136503458023071
I0130 07:25:07.487378 140027215431488 spec.py:321] Evaluating on the training split.
I0130 07:25:13.791629 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 07:25:22.316335 140027215431488 spec.py:349] Evaluating on the test split.
I0130 07:25:24.984749 140027215431488 submission_runner.py:408] Time since start: 42819.61s, 	Step: 121894, 	{'train/accuracy': 0.8153699040412903, 'train/loss': 0.6547273993492126, 'validation/accuracy': 0.7078199982643127, 'validation/loss': 1.1856532096862793, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8801075220108032, 'test/num_examples': 10000, 'score': 41347.19075655937, 'total_duration': 42819.61029410362, 'accumulated_submission_time': 41347.19075655937, 'accumulated_eval_time': 1463.6995224952698, 'accumulated_logging_time': 4.385130882263184}
I0130 07:25:25.024894 139865232500480 logging_writer.py:48] [121894] accumulated_eval_time=1463.699522, accumulated_logging_time=4.385131, accumulated_submission_time=41347.190757, global_step=121894, preemption_count=0, score=41347.190757, test/accuracy=0.591800, test/loss=1.880108, test/num_examples=10000, total_duration=42819.610294, train/accuracy=0.815370, train/loss=0.654727, validation/accuracy=0.707820, validation/loss=1.185653, validation/num_examples=50000
I0130 07:25:27.402569 139865240893184 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.6874914169311523, loss=1.0869479179382324
I0130 07:26:01.265053 139865232500480 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.8772342205047607, loss=1.1599860191345215
I0130 07:26:35.129208 139865240893184 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.893862247467041, loss=1.2137866020202637
I0130 07:27:08.990395 139865232500480 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.8396801948547363, loss=1.1303924322128296
I0130 07:27:42.854534 139865240893184 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.0849087238311768, loss=1.1655173301696777
I0130 07:28:16.740207 139865232500480 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.1286845207214355, loss=1.1409049034118652
I0130 07:28:50.619775 139865240893184 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.8672311305999756, loss=1.1395035982131958
I0130 07:29:24.571962 139865232500480 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.1291415691375732, loss=1.1171045303344727
I0130 07:29:58.451505 139865240893184 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.4238128662109375, loss=1.1924645900726318
I0130 07:30:32.352654 139865232500480 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.1269686222076416, loss=1.1811082363128662
I0130 07:31:06.249443 139865240893184 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.178969383239746, loss=1.2086024284362793
I0130 07:31:40.142643 139865232500480 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.0792508125305176, loss=1.2048606872558594
I0130 07:32:14.025535 139865240893184 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.8740785121917725, loss=1.076115369796753
I0130 07:32:47.879817 139865232500480 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.203701972961426, loss=1.1405854225158691
I0130 07:33:21.754683 139865240893184 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.9930977821350098, loss=1.1848320960998535
I0130 07:33:55.117730 140027215431488 spec.py:321] Evaluating on the training split.
I0130 07:34:01.461143 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 07:34:10.316735 140027215431488 spec.py:349] Evaluating on the test split.
I0130 07:34:12.978539 140027215431488 submission_runner.py:408] Time since start: 43347.60s, 	Step: 123400, 	{'train/accuracy': 0.8108059167861938, 'train/loss': 0.6760615706443787, 'validation/accuracy': 0.704539954662323, 'validation/loss': 1.2123322486877441, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9420335292816162, 'test/num_examples': 10000, 'score': 41857.216725349426, 'total_duration': 43347.604243040085, 'accumulated_submission_time': 41857.216725349426, 'accumulated_eval_time': 1481.5602872371674, 'accumulated_logging_time': 4.439347743988037}
I0130 07:34:13.017497 139865140229888 logging_writer.py:48] [123400] accumulated_eval_time=1481.560287, accumulated_logging_time=4.439348, accumulated_submission_time=41857.216725, global_step=123400, preemption_count=0, score=41857.216725, test/accuracy=0.582100, test/loss=1.942034, test/num_examples=10000, total_duration=43347.604243, train/accuracy=0.810806, train/loss=0.676062, validation/accuracy=0.704540, validation/loss=1.212332, validation/num_examples=50000
I0130 07:34:13.367972 139865224107776 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.231951951980591, loss=1.2667814493179321
I0130 07:34:47.227679 139865140229888 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.0437417030334473, loss=1.144091248512268
I0130 07:35:21.095506 139865224107776 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.1357603073120117, loss=1.0980573892593384
I0130 07:35:55.041401 139865140229888 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.92687726020813, loss=1.2003270387649536
I0130 07:36:28.922240 139865224107776 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.946148633956909, loss=1.0888453722000122
I0130 07:37:02.776135 139865140229888 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.7319209575653076, loss=1.1841853857040405
I0130 07:37:36.633682 139865224107776 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.9798178672790527, loss=1.145789623260498
I0130 07:38:10.514083 139865140229888 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.94085955619812, loss=1.1837162971496582
I0130 07:38:44.359787 139865224107776 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.096097707748413, loss=1.119299054145813
I0130 07:39:18.216118 139865140229888 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.0011837482452393, loss=1.0172308683395386
I0130 07:39:52.126283 139865224107776 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.1043131351470947, loss=1.199192762374878
I0130 07:40:25.987102 139865140229888 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.1578118801116943, loss=1.1912953853607178
I0130 07:40:59.816958 139865224107776 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.3596019744873047, loss=1.255185842514038
I0130 07:41:33.658184 139865140229888 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.0316171646118164, loss=1.1276558637619019
I0130 07:42:07.570531 139865224107776 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0057833194732666, loss=1.1087493896484375
I0130 07:42:41.503711 139865140229888 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.0290207862854004, loss=1.1986018419265747
I0130 07:42:43.010125 140027215431488 spec.py:321] Evaluating on the training split.
I0130 07:42:49.338794 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 07:42:57.862379 140027215431488 spec.py:349] Evaluating on the test split.
I0130 07:43:00.562228 140027215431488 submission_runner.py:408] Time since start: 43875.19s, 	Step: 124906, 	{'train/accuracy': 0.8531568646430969, 'train/loss': 0.5148501992225647, 'validation/accuracy': 0.7152799963951111, 'validation/loss': 1.1633607149124146, 'validation/num_examples': 50000, 'test/accuracy': 0.591200053691864, 'test/loss': 1.8790547847747803, 'test/num_examples': 10000, 'score': 42367.14495229721, 'total_duration': 43875.18793177605, 'accumulated_submission_time': 42367.14495229721, 'accumulated_eval_time': 1499.1123352050781, 'accumulated_logging_time': 4.4895360469818115}
I0130 07:43:00.601706 139865131837184 logging_writer.py:48] [124906] accumulated_eval_time=1499.112335, accumulated_logging_time=4.489536, accumulated_submission_time=42367.144952, global_step=124906, preemption_count=0, score=42367.144952, test/accuracy=0.591200, test/loss=1.879055, test/num_examples=10000, total_duration=43875.187932, train/accuracy=0.853157, train/loss=0.514850, validation/accuracy=0.715280, validation/loss=1.163361, validation/num_examples=50000
I0130 07:43:32.759783 139865232500480 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.0098910331726074, loss=1.2049226760864258
I0130 07:44:06.624957 139865131837184 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.1958158016204834, loss=1.1778309345245361
I0130 07:44:40.477199 139865232500480 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.886028289794922, loss=1.0996365547180176
I0130 07:45:14.312139 139865131837184 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.07651948928833, loss=1.1379859447479248
I0130 07:45:48.210239 139865232500480 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.3104894161224365, loss=1.1175932884216309
I0130 07:46:22.091279 139865131837184 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.1747684478759766, loss=1.227578043937683
I0130 07:46:55.935575 139865232500480 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.1462242603302, loss=1.1375659704208374
I0130 07:47:29.850009 139865131837184 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.2960455417633057, loss=1.1305606365203857
I0130 07:48:03.720560 139865232500480 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.188190460205078, loss=1.1994316577911377
I0130 07:48:37.595085 139865131837184 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.8632402420043945, loss=0.9918040633201599
I0130 07:49:11.590793 139865232500480 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.9778218269348145, loss=1.116187572479248
I0130 07:49:45.476501 139865131837184 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.082684278488159, loss=0.987494945526123
I0130 07:50:19.343912 139865232500480 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.1237552165985107, loss=1.1176024675369263
I0130 07:50:53.201699 139865131837184 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.117056131362915, loss=1.1720844507217407
I0130 07:51:27.076927 139865232500480 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.932682514190674, loss=1.0858523845672607
I0130 07:51:30.625207 140027215431488 spec.py:321] Evaluating on the training split.
I0130 07:51:36.970313 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 07:51:45.531755 140027215431488 spec.py:349] Evaluating on the test split.
I0130 07:51:48.214633 140027215431488 submission_runner.py:408] Time since start: 44402.84s, 	Step: 126412, 	{'train/accuracy': 0.8422951102256775, 'train/loss': 0.5681675672531128, 'validation/accuracy': 0.7170599699020386, 'validation/loss': 1.1488673686981201, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8772876262664795, 'test/num_examples': 10000, 'score': 42877.105335474014, 'total_duration': 44402.84032511711, 'accumulated_submission_time': 42877.105335474014, 'accumulated_eval_time': 1516.701696395874, 'accumulated_logging_time': 4.539835691452026}
I0130 07:51:48.252914 139865115051776 logging_writer.py:48] [126412] accumulated_eval_time=1516.701696, accumulated_logging_time=4.539836, accumulated_submission_time=42877.105335, global_step=126412, preemption_count=0, score=42877.105335, test/accuracy=0.594400, test/loss=1.877288, test/num_examples=10000, total_duration=44402.840325, train/accuracy=0.842295, train/loss=0.568168, validation/accuracy=0.717060, validation/loss=1.148867, validation/num_examples=50000
I0130 07:52:18.396252 139865123444480 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.1416561603546143, loss=1.1752111911773682
I0130 07:52:52.252981 139865115051776 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.975604772567749, loss=1.0578908920288086
I0130 07:53:26.115085 139865123444480 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.2931692600250244, loss=1.1281756162643433
I0130 07:54:00.013757 139865115051776 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.249748468399048, loss=1.1806504726409912
I0130 07:54:33.896045 139865123444480 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.029602289199829, loss=1.0526258945465088
I0130 07:55:07.795982 139865115051776 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.286191701889038, loss=1.0903046131134033
I0130 07:55:41.804685 139865123444480 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.207308292388916, loss=1.1630510091781616
I0130 07:56:15.679772 139865115051776 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.207315683364868, loss=1.096549391746521
I0130 07:56:49.553743 139865123444480 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.256502151489258, loss=1.1040213108062744
I0130 07:57:23.461722 139865115051776 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.207406997680664, loss=1.168817400932312
I0130 07:57:57.353351 139865123444480 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.2871320247650146, loss=1.1768548488616943
I0130 07:58:31.263601 139865115051776 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.0745656490325928, loss=1.0864479541778564
I0130 07:59:05.155634 139865123444480 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.1064066886901855, loss=1.107029676437378
I0130 07:59:39.056676 139865115051776 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.8912220001220703, loss=1.0525047779083252
I0130 08:00:12.953705 139865123444480 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.3435120582580566, loss=1.1548088788986206
I0130 08:00:18.515016 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:00:24.938788 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:00:33.659028 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:00:37.005152 140027215431488 submission_runner.py:408] Time since start: 44931.63s, 	Step: 127918, 	{'train/accuracy': 0.8375916481018066, 'train/loss': 0.573846697807312, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.1557798385620117, 'validation/num_examples': 50000, 'test/accuracy': 0.5874000191688538, 'test/loss': 1.8768752813339233, 'test/num_examples': 10000, 'score': 43387.303060531616, 'total_duration': 44931.63086462021, 'accumulated_submission_time': 43387.303060531616, 'accumulated_eval_time': 1535.1918017864227, 'accumulated_logging_time': 4.589509010314941}
I0130 08:00:37.038085 139865232500480 logging_writer.py:48] [127918] accumulated_eval_time=1535.191802, accumulated_logging_time=4.589509, accumulated_submission_time=43387.303061, global_step=127918, preemption_count=0, score=43387.303061, test/accuracy=0.587400, test/loss=1.876875, test/num_examples=10000, total_duration=44931.630865, train/accuracy=0.837592, train/loss=0.573847, validation/accuracy=0.715480, validation/loss=1.155780, validation/num_examples=50000
I0130 08:01:05.122497 139865240893184 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.1459131240844727, loss=1.151379942893982
I0130 08:01:38.979663 139865232500480 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.2998993396759033, loss=1.1682442426681519
I0130 08:02:12.863781 139865240893184 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.1081032752990723, loss=1.093973159790039
I0130 08:02:46.815880 139865232500480 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.3237380981445312, loss=1.1290682554244995
I0130 08:03:20.701608 139865240893184 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.116745710372925, loss=1.1040056943893433
I0130 08:03:54.560484 139865232500480 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.1013519763946533, loss=1.1147760152816772
I0130 08:04:28.447507 139865240893184 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.430678606033325, loss=1.1045156717300415
I0130 08:05:02.345279 139865232500480 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.04673433303833, loss=1.0302079916000366
I0130 08:05:36.209636 139865240893184 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1942081451416016, loss=1.1308438777923584
I0130 08:06:10.067462 139865232500480 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.1282362937927246, loss=1.058022379875183
I0130 08:06:43.940273 139865240893184 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.109050989151001, loss=1.0602025985717773
I0130 08:07:17.829961 139865232500480 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.0739355087280273, loss=1.0279099941253662
I0130 08:07:51.687945 139865240893184 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.2704412937164307, loss=1.1371970176696777
I0130 08:08:25.530785 139865232500480 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.3908398151397705, loss=1.1530871391296387
I0130 08:08:59.483505 139865240893184 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.955655574798584, loss=0.992363691329956
I0130 08:09:07.098524 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:09:13.427017 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:09:22.144655 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:09:24.848649 140027215431488 submission_runner.py:408] Time since start: 45459.47s, 	Step: 129424, 	{'train/accuracy': 0.8389867544174194, 'train/loss': 0.5688159465789795, 'validation/accuracy': 0.7219199538230896, 'validation/loss': 1.1433396339416504, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.8539307117462158, 'test/num_examples': 10000, 'score': 43897.30212020874, 'total_duration': 45459.47427010536, 'accumulated_submission_time': 43897.30212020874, 'accumulated_eval_time': 1552.9417910575867, 'accumulated_logging_time': 4.631393909454346}
I0130 08:09:24.888191 139865106659072 logging_writer.py:48] [129424] accumulated_eval_time=1552.941791, accumulated_logging_time=4.631394, accumulated_submission_time=43897.302120, global_step=129424, preemption_count=0, score=43897.302120, test/accuracy=0.600300, test/loss=1.853931, test/num_examples=10000, total_duration=45459.474270, train/accuracy=0.838987, train/loss=0.568816, validation/accuracy=0.721920, validation/loss=1.143340, validation/num_examples=50000
I0130 08:09:50.945562 139865123444480 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.2055654525756836, loss=1.1553317308425903
I0130 08:10:24.796302 139865106659072 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.108095407485962, loss=1.1731090545654297
I0130 08:10:58.672853 139865123444480 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.3612656593322754, loss=1.1060307025909424
I0130 08:11:32.567216 139865106659072 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.1188979148864746, loss=0.9529386758804321
I0130 08:12:06.470959 139865123444480 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.0247700214385986, loss=1.0533905029296875
I0130 08:12:40.325533 139865106659072 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.3490307331085205, loss=1.0885071754455566
I0130 08:13:14.187796 139865123444480 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.857205390930176, loss=1.0308218002319336
I0130 08:13:48.103258 139865106659072 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.119842290878296, loss=0.9836019277572632
I0130 08:14:21.961243 139865123444480 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.2087013721466064, loss=1.0587003231048584
I0130 08:14:55.859574 139865106659072 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.104951858520508, loss=1.0139654874801636
I0130 08:15:29.841284 139865123444480 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.0748915672302246, loss=1.0030580759048462
I0130 08:16:03.688578 139865106659072 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.3265514373779297, loss=1.115564227104187
I0130 08:16:37.582671 139865123444480 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.0371906757354736, loss=1.0633783340454102
I0130 08:17:11.465349 139865106659072 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.5205767154693604, loss=1.2009475231170654
I0130 08:17:45.321290 139865123444480 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.4654078483581543, loss=1.1195217370986938
I0130 08:17:54.944981 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:18:01.271991 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:18:09.837610 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:18:12.520149 140027215431488 submission_runner.py:408] Time since start: 45987.15s, 	Step: 130930, 	{'train/accuracy': 0.8404615521430969, 'train/loss': 0.56621253490448, 'validation/accuracy': 0.7183199524879456, 'validation/loss': 1.1442562341690063, 'validation/num_examples': 50000, 'test/accuracy': 0.5950000286102295, 'test/loss': 1.887262225151062, 'test/num_examples': 10000, 'score': 44407.29587292671, 'total_duration': 45987.14557003975, 'accumulated_submission_time': 44407.29587292671, 'accumulated_eval_time': 1570.5166273117065, 'accumulated_logging_time': 4.68116021156311}
I0130 08:18:12.563603 139865232500480 logging_writer.py:48] [130930] accumulated_eval_time=1570.516627, accumulated_logging_time=4.681160, accumulated_submission_time=44407.295873, global_step=130930, preemption_count=0, score=44407.295873, test/accuracy=0.595000, test/loss=1.887262, test/num_examples=10000, total_duration=45987.145570, train/accuracy=0.840462, train/loss=0.566213, validation/accuracy=0.718320, validation/loss=1.144256, validation/num_examples=50000
I0130 08:18:36.606532 139865240893184 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.241248607635498, loss=1.0628387928009033
I0130 08:19:10.450662 139865232500480 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.2249605655670166, loss=1.0523606538772583
I0130 08:19:44.336053 139865240893184 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.0696189403533936, loss=1.0354220867156982
I0130 08:20:18.221714 139865232500480 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.1695468425750732, loss=1.1057069301605225
I0130 08:20:52.080447 139865240893184 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.3405230045318604, loss=1.1409269571304321
I0130 08:21:25.956664 139865232500480 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.2572150230407715, loss=1.0543122291564941
I0130 08:21:59.915720 139865240893184 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.4154696464538574, loss=1.1282280683517456
I0130 08:22:33.811598 139865232500480 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.322503089904785, loss=1.0443977117538452
I0130 08:23:07.659214 139865240893184 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.126457929611206, loss=0.9901530742645264
I0130 08:23:41.500787 139865232500480 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.10007643699646, loss=0.9564962387084961
I0130 08:24:15.377103 139865240893184 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.034268856048584, loss=1.052140235900879
I0130 08:24:49.258520 139865232500480 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.2829055786132812, loss=1.006041407585144
I0130 08:25:23.141107 139865240893184 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.348191261291504, loss=1.0848488807678223
I0130 08:25:57.006050 139865232500480 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.4584507942199707, loss=1.0716521739959717
I0130 08:26:30.856664 139865240893184 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.097075939178467, loss=1.0291218757629395
I0130 08:26:42.531823 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:26:48.853940 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:26:57.420138 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:27:00.110636 140027215431488 submission_runner.py:408] Time since start: 46514.74s, 	Step: 132436, 	{'train/accuracy': 0.8365752100944519, 'train/loss': 0.582433819770813, 'validation/accuracy': 0.7178199887275696, 'validation/loss': 1.1573113203048706, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8838881254196167, 'test/num_examples': 10000, 'score': 44917.199598789215, 'total_duration': 46514.73631954193, 'accumulated_submission_time': 44917.199598789215, 'accumulated_eval_time': 1588.0953686237335, 'accumulated_logging_time': 4.735187530517578}
I0130 08:27:00.155738 139865240893184 logging_writer.py:48] [132436] accumulated_eval_time=1588.095369, accumulated_logging_time=4.735188, accumulated_submission_time=44917.199599, global_step=132436, preemption_count=0, score=44917.199599, test/accuracy=0.589300, test/loss=1.883888, test/num_examples=10000, total_duration=46514.736320, train/accuracy=0.836575, train/loss=0.582434, validation/accuracy=0.717820, validation/loss=1.157311, validation/num_examples=50000
I0130 08:27:22.163935 139865760950016 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.3148651123046875, loss=1.0738307237625122
I0130 08:27:56.049355 139865240893184 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.1960859298706055, loss=1.0461962223052979
I0130 08:28:29.924945 139865760950016 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.098637342453003, loss=1.0755562782287598
I0130 08:29:03.882513 139865240893184 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2244551181793213, loss=1.095866084098816
I0130 08:29:37.749581 139865760950016 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.4059650897979736, loss=1.0690398216247559
I0130 08:30:11.609641 139865240893184 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.3926613330841064, loss=1.012503981590271
I0130 08:30:45.518115 139865760950016 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.273483991622925, loss=1.093455195426941
I0130 08:31:19.364467 139865240893184 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.4258666038513184, loss=0.9920700192451477
I0130 08:31:53.268795 139865760950016 logging_writer.py:48] [133300] global_step=133300, grad_norm=4.040251731872559, loss=1.084810733795166
I0130 08:32:27.134294 139865240893184 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.4148120880126953, loss=1.0544918775558472
I0130 08:33:00.993695 139865760950016 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3758397102355957, loss=1.0659096240997314
I0130 08:33:34.865258 139865240893184 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.2691566944122314, loss=0.9390217661857605
I0130 08:34:08.773385 139865760950016 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.2291393280029297, loss=1.0689778327941895
I0130 08:34:42.633618 139865240893184 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.262890100479126, loss=0.9675739407539368
I0130 08:35:16.584677 139865760950016 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.1411550045013428, loss=0.9910268783569336
I0130 08:35:30.303892 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:35:37.291416 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:35:46.141342 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:35:48.827839 140027215431488 submission_runner.py:408] Time since start: 47043.45s, 	Step: 133942, 	{'train/accuracy': 0.8755381107330322, 'train/loss': 0.43746116757392883, 'validation/accuracy': 0.7226799726486206, 'validation/loss': 1.1405651569366455, 'validation/num_examples': 50000, 'test/accuracy': 0.596500039100647, 'test/loss': 1.8780877590179443, 'test/num_examples': 10000, 'score': 45427.28427886963, 'total_duration': 47043.453533411026, 'accumulated_submission_time': 45427.28427886963, 'accumulated_eval_time': 1606.6192646026611, 'accumulated_logging_time': 4.7905237674713135}
I0130 08:35:48.876360 139865123444480 logging_writer.py:48] [133942] accumulated_eval_time=1606.619265, accumulated_logging_time=4.790524, accumulated_submission_time=45427.284279, global_step=133942, preemption_count=0, score=45427.284279, test/accuracy=0.596500, test/loss=1.878088, test/num_examples=10000, total_duration=47043.453533, train/accuracy=0.875538, train/loss=0.437461, validation/accuracy=0.722680, validation/loss=1.140565, validation/num_examples=50000
I0130 08:36:08.875672 139865131837184 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.3650059700012207, loss=0.9991515874862671
I0130 08:36:42.745020 139865123444480 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.4240548610687256, loss=1.0753334760665894
I0130 08:37:16.610111 139865131837184 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.5770890712738037, loss=0.9963288903236389
I0130 08:37:50.485108 139865123444480 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.278984785079956, loss=1.0133209228515625
I0130 08:38:24.356458 139865131837184 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.2653191089630127, loss=1.0459814071655273
I0130 08:38:58.250832 139865123444480 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.3021340370178223, loss=1.0365066528320312
I0130 08:39:32.094131 139865131837184 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.584299325942993, loss=1.053039789199829
I0130 08:40:05.981277 139865123444480 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.2535147666931152, loss=1.018812894821167
I0130 08:40:39.862859 139865131837184 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.5736806392669678, loss=1.0503044128417969
I0130 08:41:13.754231 139865123444480 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.1442713737487793, loss=1.062926173210144
I0130 08:41:47.735627 139865131837184 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.486759662628174, loss=1.1196320056915283
I0130 08:42:21.597310 139865123444480 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.5700151920318604, loss=1.0465056896209717
I0130 08:42:55.494669 139865131837184 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.7259418964385986, loss=1.061490535736084
I0130 08:43:29.375977 139865123444480 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.375161647796631, loss=0.9590578675270081
I0130 08:44:03.242066 139865131837184 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.179764986038208, loss=0.9760222434997559
I0130 08:44:18.987245 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:44:25.317437 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:44:34.146784 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:44:36.807304 140027215431488 submission_runner.py:408] Time since start: 47571.43s, 	Step: 135448, 	{'train/accuracy': 0.8627630472183228, 'train/loss': 0.4806753098964691, 'validation/accuracy': 0.7238199710845947, 'validation/loss': 1.131712555885315, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8806487321853638, 'test/num_examples': 10000, 'score': 45937.331146240234, 'total_duration': 47571.43300700188, 'accumulated_submission_time': 45937.331146240234, 'accumulated_eval_time': 1624.4392714500427, 'accumulated_logging_time': 4.849410057067871}
I0130 08:44:36.848481 139865224107776 logging_writer.py:48] [135448] accumulated_eval_time=1624.439271, accumulated_logging_time=4.849410, accumulated_submission_time=45937.331146, global_step=135448, preemption_count=0, score=45937.331146, test/accuracy=0.596600, test/loss=1.880649, test/num_examples=10000, total_duration=47571.433007, train/accuracy=0.862763, train/loss=0.480675, validation/accuracy=0.723820, validation/loss=1.131713, validation/num_examples=50000
I0130 08:44:54.822024 139865232500480 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.1732335090637207, loss=1.0414761304855347
I0130 08:45:28.653424 139865224107776 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.2748796939849854, loss=1.0049995183944702
I0130 08:46:02.502573 139865232500480 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.482717275619507, loss=1.0588865280151367
I0130 08:46:36.382694 139865224107776 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.53085994720459, loss=1.0603845119476318
I0130 08:47:10.279832 139865232500480 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.5934784412384033, loss=1.0869865417480469
I0130 08:47:44.162643 139865224107776 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.5262081623077393, loss=1.0916286706924438
I0130 08:48:18.036960 139865232500480 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.180009126663208, loss=0.9928756952285767
I0130 08:48:52.025845 139865224107776 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.2349560260772705, loss=1.0026427507400513
I0130 08:49:25.933055 139865232500480 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.652956247329712, loss=1.0857155323028564
I0130 08:49:59.786015 139865224107776 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.5935630798339844, loss=1.055648684501648
I0130 08:50:33.629252 139865232500480 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.6139144897460938, loss=1.0905145406723022
I0130 08:51:07.533391 139865224107776 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.527210235595703, loss=1.143419861793518
I0130 08:51:41.422601 139865232500480 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.423961639404297, loss=1.0412286520004272
I0130 08:52:15.316907 139865224107776 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.403350830078125, loss=1.0893244743347168
I0130 08:52:49.239524 139865232500480 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.315833568572998, loss=0.9966346025466919
I0130 08:53:07.018098 140027215431488 spec.py:321] Evaluating on the training split.
I0130 08:53:13.406887 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 08:53:22.200567 140027215431488 spec.py:349] Evaluating on the test split.
I0130 08:53:25.036794 140027215431488 submission_runner.py:408] Time since start: 48099.66s, 	Step: 136954, 	{'train/accuracy': 0.8583984375, 'train/loss': 0.4998326003551483, 'validation/accuracy': 0.7249599695205688, 'validation/loss': 1.131963849067688, 'validation/num_examples': 50000, 'test/accuracy': 0.597000002861023, 'test/loss': 1.8573275804519653, 'test/num_examples': 10000, 'score': 46447.43666052818, 'total_duration': 48099.66242289543, 'accumulated_submission_time': 46447.43666052818, 'accumulated_eval_time': 1642.457855463028, 'accumulated_logging_time': 4.900918006896973}
I0130 08:53:25.086576 139865123444480 logging_writer.py:48] [136954] accumulated_eval_time=1642.457855, accumulated_logging_time=4.900918, accumulated_submission_time=46447.436661, global_step=136954, preemption_count=0, score=46447.436661, test/accuracy=0.597000, test/loss=1.857328, test/num_examples=10000, total_duration=48099.662423, train/accuracy=0.858398, train/loss=0.499833, validation/accuracy=0.724960, validation/loss=1.131964, validation/num_examples=50000
I0130 08:53:41.021940 139865131837184 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.1732516288757324, loss=0.915345311164856
I0130 08:54:14.845538 139865123444480 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.076188564300537, loss=0.9683629274368286
I0130 08:54:48.710646 139865131837184 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.1725680828094482, loss=0.943665623664856
I0130 08:55:22.798360 139865123444480 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.8442766666412354, loss=0.9895366430282593
I0130 08:55:56.713999 139865131837184 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.479766607284546, loss=1.0364501476287842
I0130 08:56:30.602629 139865123444480 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.4067578315734863, loss=1.020935297012329
I0130 08:57:04.500879 139865131837184 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.802691698074341, loss=1.086141586303711
I0130 08:57:38.397889 139865123444480 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.408224582672119, loss=0.9894577264785767
I0130 08:58:12.276333 139865131837184 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.9012303352355957, loss=0.9330848455429077
I0130 08:58:46.194370 139865123444480 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.538801670074463, loss=1.0642354488372803
I0130 08:59:20.041966 139865131837184 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.722795009613037, loss=1.0438683032989502
I0130 08:59:53.915647 139865123444480 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.63844633102417, loss=1.0108333826065063
I0130 09:00:27.817084 139865131837184 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.586888074874878, loss=1.137075424194336
I0130 09:01:01.697765 139865123444480 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.6204309463500977, loss=0.908172070980072
I0130 09:01:35.673871 139865131837184 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.5140035152435303, loss=1.099766492843628
I0130 09:01:55.136172 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:02:01.495440 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:02:10.137942 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:02:12.686855 140027215431488 submission_runner.py:408] Time since start: 48627.31s, 	Step: 138459, 	{'train/accuracy': 0.859773576259613, 'train/loss': 0.48492223024368286, 'validation/accuracy': 0.7234799861907959, 'validation/loss': 1.1349554061889648, 'validation/num_examples': 50000, 'test/accuracy': 0.5968000292778015, 'test/loss': 1.881799340248108, 'test/num_examples': 10000, 'score': 46957.421921014786, 'total_duration': 48627.312551259995, 'accumulated_submission_time': 46957.421921014786, 'accumulated_eval_time': 1660.0084781646729, 'accumulated_logging_time': 4.961538314819336}
I0130 09:02:12.728556 139865232500480 logging_writer.py:48] [138459] accumulated_eval_time=1660.008478, accumulated_logging_time=4.961538, accumulated_submission_time=46957.421921, global_step=138459, preemption_count=0, score=46957.421921, test/accuracy=0.596800, test/loss=1.881799, test/num_examples=10000, total_duration=48627.312551, train/accuracy=0.859774, train/loss=0.484922, validation/accuracy=0.723480, validation/loss=1.134955, validation/num_examples=50000
I0130 09:02:26.956807 139865240893184 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.5323891639709473, loss=0.9432060718536377
I0130 09:03:00.831325 139865232500480 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.239417791366577, loss=0.9532907605171204
I0130 09:03:34.704649 139865240893184 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.4141030311584473, loss=1.0095494985580444
I0130 09:04:08.503619 139865232500480 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.566408634185791, loss=1.005165696144104
I0130 09:04:42.378704 139865240893184 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.612515449523926, loss=0.9720714092254639
I0130 09:05:16.250936 139865232500480 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.838742256164551, loss=1.080927848815918
I0130 09:05:50.149533 139865240893184 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.628653049468994, loss=0.9936504364013672
I0130 09:06:24.004551 139865232500480 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.285323143005371, loss=0.9581682682037354
I0130 09:06:57.907900 139865240893184 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.469027042388916, loss=0.92388516664505
I0130 09:07:31.743440 139865232500480 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.5325188636779785, loss=1.0773372650146484
I0130 09:08:05.629993 139865240893184 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.724261522293091, loss=0.94671630859375
I0130 09:08:39.589490 139865232500480 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.6301257610321045, loss=0.9961226582527161
I0130 09:09:13.448159 139865240893184 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.520592212677002, loss=1.0038645267486572
I0130 09:09:47.314168 139865232500480 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.8580150604248047, loss=1.0377424955368042
I0130 09:10:21.208394 139865240893184 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.5749385356903076, loss=0.9944605231285095
I0130 09:10:42.974348 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:10:49.405983 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:10:58.202117 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:11:00.809966 140027215431488 submission_runner.py:408] Time since start: 49155.44s, 	Step: 139966, 	{'train/accuracy': 0.8603116869926453, 'train/loss': 0.486088365316391, 'validation/accuracy': 0.7274799942970276, 'validation/loss': 1.1127536296844482, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.8506639003753662, 'test/num_examples': 10000, 'score': 47467.603432655334, 'total_duration': 49155.43559336662, 'accumulated_submission_time': 47467.603432655334, 'accumulated_eval_time': 1677.843991279602, 'accumulated_logging_time': 5.014599561691284}
I0130 09:11:00.851718 139865140229888 logging_writer.py:48] [139966] accumulated_eval_time=1677.843991, accumulated_logging_time=5.014600, accumulated_submission_time=47467.603433, global_step=139966, preemption_count=0, score=47467.603433, test/accuracy=0.603500, test/loss=1.850664, test/num_examples=10000, total_duration=49155.435593, train/accuracy=0.860312, train/loss=0.486088, validation/accuracy=0.727480, validation/loss=1.112754, validation/num_examples=50000
I0130 09:11:12.723035 139865224107776 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.5031139850616455, loss=0.8589240312576294
I0130 09:11:46.575054 139865140229888 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.5024163722991943, loss=0.968415379524231
I0130 09:12:20.412884 139865224107776 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.45721173286438, loss=0.9299827814102173
I0130 09:12:54.256740 139865140229888 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.642030954360962, loss=0.9708887338638306
I0130 09:13:28.157389 139865224107776 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.550443649291992, loss=0.9861730337142944
I0130 09:14:02.042647 139865140229888 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.5740158557891846, loss=0.9852617979049683
I0130 09:14:35.937796 139865224107776 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.9508445262908936, loss=1.0509164333343506
I0130 09:15:09.886465 139865140229888 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.5487332344055176, loss=0.870506763458252
I0130 09:15:43.812736 139865224107776 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.5042076110839844, loss=1.0123603343963623
I0130 09:16:17.690996 139865140229888 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.617528200149536, loss=0.9233028888702393
I0130 09:16:51.568100 139865224107776 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.3928563594818115, loss=0.9349110722541809
I0130 09:17:25.443575 139865140229888 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.3925235271453857, loss=0.9858730435371399
I0130 09:17:59.304405 139865224107776 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.5677363872528076, loss=0.9110637307167053
I0130 09:18:33.148526 139865140229888 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.338622808456421, loss=0.9297341704368591
I0130 09:19:07.036856 139865224107776 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.639176607131958, loss=0.9340919256210327
I0130 09:19:30.911605 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:19:37.313118 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:19:46.032058 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:19:48.787870 140027215431488 submission_runner.py:408] Time since start: 49683.41s, 	Step: 141472, 	{'train/accuracy': 0.8621651530265808, 'train/loss': 0.47984156012535095, 'validation/accuracy': 0.7297399640083313, 'validation/loss': 1.1351284980773926, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.8711603879928589, 'test/num_examples': 10000, 'score': 47977.597479343414, 'total_duration': 49683.41357302666, 'accumulated_submission_time': 47977.597479343414, 'accumulated_eval_time': 1695.720237493515, 'accumulated_logging_time': 5.068366050720215}
I0130 09:19:48.831730 139865131837184 logging_writer.py:48] [141472] accumulated_eval_time=1695.720237, accumulated_logging_time=5.068366, accumulated_submission_time=47977.597479, global_step=141472, preemption_count=0, score=47977.597479, test/accuracy=0.601900, test/loss=1.871160, test/num_examples=10000, total_duration=49683.413573, train/accuracy=0.862165, train/loss=0.479842, validation/accuracy=0.729740, validation/loss=1.135128, validation/num_examples=50000
I0130 09:19:58.678552 139865232500480 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.6050243377685547, loss=1.0293970108032227
I0130 09:20:32.514737 139865131837184 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.40901780128479, loss=1.0011770725250244
I0130 09:21:06.350310 139865232500480 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.751366376876831, loss=0.9707932472229004
I0130 09:21:40.360553 139865131837184 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.9169394969940186, loss=0.9987396001815796
I0130 09:22:14.250375 139865232500480 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.636561632156372, loss=0.9951939582824707
I0130 09:22:48.107277 139865131837184 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.7324635982513428, loss=1.0243443250656128
I0130 09:23:21.957321 139865232500480 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.5777671337127686, loss=0.9262664318084717
I0130 09:23:55.817725 139865131837184 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.4017512798309326, loss=0.9246595501899719
I0130 09:24:29.681319 139865232500480 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.722435235977173, loss=0.9133481979370117
I0130 09:25:03.549424 139865131837184 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.6693472862243652, loss=0.9881341457366943
I0130 09:25:37.389854 139865232500480 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.357382297515869, loss=0.8921562433242798
I0130 09:26:11.263002 139865131837184 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.7104504108428955, loss=0.9612591862678528
I0130 09:26:45.154083 139865232500480 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.6385152339935303, loss=0.9778406620025635
I0130 09:27:19.020880 139865131837184 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.860790491104126, loss=0.9833407402038574
I0130 09:27:52.993589 139865232500480 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.7079577445983887, loss=1.0313892364501953
I0130 09:28:18.868059 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:28:25.252498 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:28:34.041646 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:28:37.035028 140027215431488 submission_runner.py:408] Time since start: 50211.66s, 	Step: 142978, 	{'train/accuracy': 0.8984972834587097, 'train/loss': 0.35278594493865967, 'validation/accuracy': 0.7320599555969238, 'validation/loss': 1.1209580898284912, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.8868118524551392, 'test/num_examples': 10000, 'score': 48487.5690472126, 'total_duration': 50211.66066980362, 'accumulated_submission_time': 48487.5690472126, 'accumulated_eval_time': 1713.8870961666107, 'accumulated_logging_time': 5.12295126914978}
I0130 09:28:37.068982 139865140229888 logging_writer.py:48] [142978] accumulated_eval_time=1713.887096, accumulated_logging_time=5.122951, accumulated_submission_time=48487.569047, global_step=142978, preemption_count=0, score=48487.569047, test/accuracy=0.603500, test/loss=1.886812, test/num_examples=10000, total_duration=50211.660670, train/accuracy=0.898497, train/loss=0.352786, validation/accuracy=0.732060, validation/loss=1.120958, validation/num_examples=50000
I0130 09:28:44.855093 139865224107776 logging_writer.py:48] [143000] global_step=143000, grad_norm=4.247407913208008, loss=0.9419457912445068
I0130 09:29:18.721494 139865140229888 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.5012047290802, loss=0.9146223664283752
I0130 09:29:52.538320 139865224107776 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.585843563079834, loss=0.9421498775482178
I0130 09:30:26.411774 139865140229888 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.8877768516540527, loss=0.9969387054443359
I0130 09:31:00.307505 139865224107776 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.8724913597106934, loss=0.9611495733261108
I0130 09:31:34.197569 139865140229888 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.868777275085449, loss=0.9358454346656799
I0130 09:32:08.056346 139865224107776 logging_writer.py:48] [143600] global_step=143600, grad_norm=4.028435230255127, loss=1.0161808729171753
I0130 09:32:41.947333 139865140229888 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.9032046794891357, loss=1.0054295063018799
I0130 09:33:15.799248 139865224107776 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.6404967308044434, loss=0.9278470277786255
I0130 09:33:49.664166 139865140229888 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.64249849319458, loss=0.8658444881439209
I0130 09:34:23.534802 139865224107776 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.714881420135498, loss=0.9618377685546875
I0130 09:34:57.516543 139865140229888 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.5103445053100586, loss=0.9721750020980835
I0130 09:35:31.417661 139865224107776 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.614718437194824, loss=0.8740440607070923
I0130 09:36:05.291402 139865140229888 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.6182782649993896, loss=0.9055231809616089
I0130 09:36:39.205227 139865224107776 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.9396631717681885, loss=1.0475174188613892
I0130 09:37:07.140826 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:37:13.506223 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:37:22.132410 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:37:24.838865 140027215431488 submission_runner.py:408] Time since start: 50739.46s, 	Step: 144484, 	{'train/accuracy': 0.8881736397743225, 'train/loss': 0.3926560580730438, 'validation/accuracy': 0.7305600047111511, 'validation/loss': 1.1178728342056274, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.855978012084961, 'test/num_examples': 10000, 'score': 48997.5791516304, 'total_duration': 50739.464485645294, 'accumulated_submission_time': 48997.5791516304, 'accumulated_eval_time': 1731.585001707077, 'accumulated_logging_time': 5.165935516357422}
I0130 09:37:24.882336 139865115051776 logging_writer.py:48] [144484] accumulated_eval_time=1731.585002, accumulated_logging_time=5.165936, accumulated_submission_time=48997.579152, global_step=144484, preemption_count=0, score=48997.579152, test/accuracy=0.608000, test/loss=1.855978, test/num_examples=10000, total_duration=50739.464486, train/accuracy=0.888174, train/loss=0.392656, validation/accuracy=0.730560, validation/loss=1.117873, validation/num_examples=50000
I0130 09:37:30.631882 139865123444480 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.4779415130615234, loss=0.8934869170188904
I0130 09:38:04.531923 139865115051776 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.6346404552459717, loss=0.9467824697494507
I0130 09:38:38.389415 139865123444480 logging_writer.py:48] [144700] global_step=144700, grad_norm=4.101164817810059, loss=0.979864776134491
I0130 09:39:12.264026 139865115051776 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.889901876449585, loss=0.8982823491096497
I0130 09:39:46.179257 139865123444480 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.8836352825164795, loss=1.0228450298309326
I0130 09:40:20.064720 139865115051776 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.0918660163879395, loss=0.9540648460388184
I0130 09:40:53.940587 139865123444480 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.8544650077819824, loss=0.8856487274169922
I0130 09:41:27.918179 139865115051776 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.846646308898926, loss=0.9410640001296997
I0130 09:42:01.808161 139865123444480 logging_writer.py:48] [145300] global_step=145300, grad_norm=4.064632415771484, loss=1.05906081199646
I0130 09:42:35.663210 139865115051776 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.9453322887420654, loss=0.9649959206581116
I0130 09:43:10.012587 139865123444480 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.614746570587158, loss=0.8655964136123657
I0130 09:43:43.902758 139865115051776 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.8962886333465576, loss=0.94496089220047
I0130 09:44:17.775259 139865123444480 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.8636324405670166, loss=0.9263991713523865
I0130 09:44:51.669994 139865115051776 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.546874523162842, loss=0.8845705986022949
I0130 09:45:25.555049 139865123444480 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.92244815826416, loss=0.9521331191062927
I0130 09:45:55.167982 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:46:01.560480 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:46:10.154162 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:46:12.981945 140027215431488 submission_runner.py:408] Time since start: 51267.61s, 	Step: 145989, 	{'train/accuracy': 0.8858019709587097, 'train/loss': 0.39276427030563354, 'validation/accuracy': 0.7334799766540527, 'validation/loss': 1.106014609336853, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.8426967859268188, 'test/num_examples': 10000, 'score': 49507.801383018494, 'total_duration': 51267.6076464653, 'accumulated_submission_time': 49507.801383018494, 'accumulated_eval_time': 1749.3989126682281, 'accumulated_logging_time': 5.220576286315918}
I0130 09:46:13.026874 139865224107776 logging_writer.py:48] [145989] accumulated_eval_time=1749.398913, accumulated_logging_time=5.220576, accumulated_submission_time=49507.801383, global_step=145989, preemption_count=0, score=49507.801383, test/accuracy=0.605600, test/loss=1.842697, test/num_examples=10000, total_duration=51267.607646, train/accuracy=0.885802, train/loss=0.392764, validation/accuracy=0.733480, validation/loss=1.106015, validation/num_examples=50000
I0130 09:46:17.100127 139865232500480 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.258169174194336, loss=0.9577268362045288
I0130 09:46:50.923686 139865224107776 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.5900418758392334, loss=0.8359330892562866
I0130 09:47:24.768414 139865232500480 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.705659866333008, loss=0.9702286124229431
I0130 09:47:58.738465 139865224107776 logging_writer.py:48] [146300] global_step=146300, grad_norm=4.086975574493408, loss=0.8840862512588501
I0130 09:48:32.627223 139865232500480 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.835482358932495, loss=0.9810562133789062
I0130 09:49:06.504294 139865224107776 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.774502992630005, loss=0.9017654061317444
I0130 09:49:40.362302 139865232500480 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.8545398712158203, loss=0.9122366905212402
I0130 09:50:14.264242 139865224107776 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.7425177097320557, loss=0.8883433938026428
I0130 09:50:48.118349 139865232500480 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.8539023399353027, loss=0.9126117825508118
I0130 09:51:22.010008 139865224107776 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.797168731689453, loss=0.922003984451294
I0130 09:51:55.862792 139865232500480 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.6892430782318115, loss=0.8675070405006409
I0130 09:52:29.700260 139865224107776 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.5573251247406006, loss=0.8774480819702148
I0130 09:53:03.587571 139865232500480 logging_writer.py:48] [147200] global_step=147200, grad_norm=4.002161502838135, loss=0.9025885462760925
I0130 09:53:37.485693 139865224107776 logging_writer.py:48] [147300] global_step=147300, grad_norm=4.0782060623168945, loss=0.9879016876220703
I0130 09:54:11.322218 139865232500480 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.926464796066284, loss=0.8740084767341614
I0130 09:54:43.068541 140027215431488 spec.py:321] Evaluating on the training split.
I0130 09:54:49.400453 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 09:54:58.211312 140027215431488 spec.py:349] Evaluating on the test split.
I0130 09:55:00.879791 140027215431488 submission_runner.py:408] Time since start: 51795.51s, 	Step: 147495, 	{'train/accuracy': 0.8859414458274841, 'train/loss': 0.3913815915584564, 'validation/accuracy': 0.7366200089454651, 'validation/loss': 1.1027542352676392, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.8579843044281006, 'test/num_examples': 10000, 'score': 50017.77842974663, 'total_duration': 51795.50549149513, 'accumulated_submission_time': 50017.77842974663, 'accumulated_eval_time': 1767.210108757019, 'accumulated_logging_time': 5.27645468711853}
I0130 09:55:00.928085 139865123444480 logging_writer.py:48] [147495] accumulated_eval_time=1767.210109, accumulated_logging_time=5.276455, accumulated_submission_time=50017.778430, global_step=147495, preemption_count=0, score=50017.778430, test/accuracy=0.613100, test/loss=1.857984, test/num_examples=10000, total_duration=51795.505491, train/accuracy=0.885941, train/loss=0.391382, validation/accuracy=0.736620, validation/loss=1.102754, validation/num_examples=50000
I0130 09:55:02.965878 139865131837184 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.7375218868255615, loss=0.8808248043060303
I0130 09:55:36.850879 139865123444480 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.9673972129821777, loss=0.7913549542427063
I0130 09:56:10.715290 139865131837184 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.8366105556488037, loss=0.8947324752807617
I0130 09:56:44.543695 139865123444480 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.6522436141967773, loss=0.8481428623199463
I0130 09:57:18.390825 139865131837184 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.1965460777282715, loss=0.8942233920097351
I0130 09:57:52.276409 139865123444480 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.020683288574219, loss=0.9060214757919312
I0130 09:58:26.155693 139865131837184 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.8380062580108643, loss=0.849951446056366
I0130 09:58:59.982742 139865123444480 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.855085849761963, loss=0.9826666116714478
I0130 09:59:33.873971 139865131837184 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.5876386165618896, loss=0.8719951510429382
I0130 10:00:07.737597 139865123444480 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.990086078643799, loss=0.9362303018569946
I0130 10:00:41.622699 139865131837184 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.9637253284454346, loss=0.8830136656761169
I0130 10:01:15.585379 139865123444480 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.642592906951904, loss=1.0045732259750366
I0130 10:01:49.483749 139865131837184 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.8980467319488525, loss=0.8572177290916443
I0130 10:02:23.339838 139865123444480 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.8776638507843018, loss=0.8813505172729492
I0130 10:02:57.227209 139865131837184 logging_writer.py:48] [148900] global_step=148900, grad_norm=4.288392543792725, loss=0.9044811129570007
I0130 10:03:31.064383 139865123444480 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.6323130130767822, loss=0.8087331652641296
I0130 10:03:31.075528 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:03:37.471120 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:03:46.352568 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:03:48.888865 140027215431488 submission_runner.py:408] Time since start: 52323.51s, 	Step: 149001, 	{'train/accuracy': 0.8880141973495483, 'train/loss': 0.386345237493515, 'validation/accuracy': 0.735319972038269, 'validation/loss': 1.1073721647262573, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.8357858657836914, 'test/num_examples': 10000, 'score': 50527.860815286636, 'total_duration': 52323.51455950737, 'accumulated_submission_time': 50527.860815286636, 'accumulated_eval_time': 1785.0233714580536, 'accumulated_logging_time': 5.3356263637542725}
I0130 10:03:48.935619 139865123444480 logging_writer.py:48] [149001] accumulated_eval_time=1785.023371, accumulated_logging_time=5.335626, accumulated_submission_time=50527.860815, global_step=149001, preemption_count=0, score=50527.860815, test/accuracy=0.609700, test/loss=1.835786, test/num_examples=10000, total_duration=52323.514560, train/accuracy=0.888014, train/loss=0.386345, validation/accuracy=0.735320, validation/loss=1.107372, validation/num_examples=50000
I0130 10:04:22.782608 139865224107776 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.984342575073242, loss=0.899948000907898
I0130 10:04:56.678816 139865123444480 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.310928821563721, loss=0.954784631729126
I0130 10:05:30.523304 139865224107776 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.7963664531707764, loss=0.9050161838531494
I0130 10:06:04.384223 139865123444480 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.9844894409179688, loss=0.9285203814506531
I0130 10:06:38.284193 139865224107776 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.004958152770996, loss=0.886582612991333
I0130 10:07:12.124638 139865123444480 logging_writer.py:48] [149600] global_step=149600, grad_norm=4.0297346115112305, loss=0.903775691986084
I0130 10:07:46.127541 139865224107776 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.7945075035095215, loss=0.8752503395080566
I0130 10:08:19.993782 139865123444480 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.88195538520813, loss=0.974272608757019
I0130 10:08:53.879647 139865224107776 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.865741014480591, loss=0.9045127034187317
I0130 10:09:27.748547 139865123444480 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.924633502960205, loss=0.834629476070404
I0130 10:10:01.615541 139865224107776 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.9710659980773926, loss=0.8375265002250671
I0130 10:10:35.534179 139865123444480 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.5131771564483643, loss=0.7829816341400146
I0130 10:11:09.436857 139865224107776 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.165841102600098, loss=0.9285281300544739
I0130 10:11:43.295707 139865123444480 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.9890384674072266, loss=0.8616191148757935
I0130 10:12:17.221508 139865224107776 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.9981112480163574, loss=0.857120156288147
I0130 10:12:19.063704 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:12:25.395875 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:12:34.214237 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:12:36.871775 140027215431488 submission_runner.py:408] Time since start: 52851.50s, 	Step: 150507, 	{'train/accuracy': 0.8951889276504517, 'train/loss': 0.3608154356479645, 'validation/accuracy': 0.7378399968147278, 'validation/loss': 1.096303939819336, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.8414394855499268, 'test/num_examples': 10000, 'score': 51037.921318769455, 'total_duration': 52851.49748301506, 'accumulated_submission_time': 51037.921318769455, 'accumulated_eval_time': 1802.831404209137, 'accumulated_logging_time': 5.39626669883728}
I0130 10:12:36.911284 139865131837184 logging_writer.py:48] [150507] accumulated_eval_time=1802.831404, accumulated_logging_time=5.396267, accumulated_submission_time=51037.921319, global_step=150507, preemption_count=0, score=51037.921319, test/accuracy=0.614200, test/loss=1.841439, test/num_examples=10000, total_duration=52851.497483, train/accuracy=0.895189, train/loss=0.360815, validation/accuracy=0.737840, validation/loss=1.096304, validation/num_examples=50000
I0130 10:13:08.732160 139865140229888 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.973208427429199, loss=0.856624186038971
I0130 10:13:42.627410 139865131837184 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.8572134971618652, loss=0.8127262592315674
I0130 10:14:16.567594 139865140229888 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.057273864746094, loss=0.9683213233947754
I0130 10:14:50.445665 139865131837184 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.0729851722717285, loss=0.8611786365509033
I0130 10:15:24.317053 139865140229888 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.864227056503296, loss=0.8496556282043457
I0130 10:15:58.161128 139865131837184 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.8579349517822266, loss=0.8271589279174805
I0130 10:16:32.033261 139865140229888 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.015150547027588, loss=0.8975183963775635
I0130 10:17:05.937785 139865131837184 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.060218334197998, loss=0.838615357875824
I0130 10:17:39.809807 139865140229888 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.955890655517578, loss=0.7499304413795471
I0130 10:18:13.716197 139865131837184 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.091678142547607, loss=0.8612899780273438
I0130 10:18:47.576152 139865140229888 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.072624683380127, loss=0.8091112971305847
I0130 10:19:21.465947 139865131837184 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.361575603485107, loss=0.9073188900947571
I0130 10:19:55.350732 139865140229888 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.8393447399139404, loss=0.7873346209526062
I0130 10:20:29.193936 139865131837184 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.042130470275879, loss=0.8751114010810852
I0130 10:21:03.165202 139865140229888 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.144350528717041, loss=0.8110998868942261
I0130 10:21:07.041855 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:21:13.371781 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:21:21.857902 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:21:24.557313 140027215431488 submission_runner.py:408] Time since start: 53379.18s, 	Step: 152013, 	{'train/accuracy': 0.9246850609779358, 'train/loss': 0.2611102759838104, 'validation/accuracy': 0.7400599718093872, 'validation/loss': 1.1027374267578125, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.8491344451904297, 'test/num_examples': 10000, 'score': 51547.98945236206, 'total_duration': 53379.18301439285, 'accumulated_submission_time': 51547.98945236206, 'accumulated_eval_time': 1820.3468084335327, 'accumulated_logging_time': 5.445066690444946}
I0130 10:21:24.602735 139865224107776 logging_writer.py:48] [152013] accumulated_eval_time=1820.346808, accumulated_logging_time=5.445067, accumulated_submission_time=51547.989452, global_step=152013, preemption_count=0, score=51547.989452, test/accuracy=0.621100, test/loss=1.849134, test/num_examples=10000, total_duration=53379.183014, train/accuracy=0.924685, train/loss=0.261110, validation/accuracy=0.740060, validation/loss=1.102737, validation/num_examples=50000
I0130 10:21:54.403743 139865232500480 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.829301595687866, loss=0.7799088358879089
I0130 10:22:28.294099 139865224107776 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.813951253890991, loss=0.7889532446861267
I0130 10:23:02.200036 139865232500480 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.027961730957031, loss=0.8216720819473267
I0130 10:23:36.076378 139865224107776 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.990175247192383, loss=0.7596591114997864
I0130 10:24:09.936261 139865232500480 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.962660312652588, loss=0.86294025182724
I0130 10:24:43.802725 139865224107776 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.218234062194824, loss=0.8765844702720642
I0130 10:25:17.663665 139865232500480 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.495047092437744, loss=0.9405791163444519
I0130 10:25:51.559928 139865224107776 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.11704158782959, loss=0.9151016473770142
I0130 10:26:25.448435 139865232500480 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.481225490570068, loss=0.8754557967185974
I0130 10:26:59.346611 139865224107776 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.006988525390625, loss=0.8120467066764832
I0130 10:27:33.420270 139865232500480 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.9063282012939453, loss=0.7593856453895569
I0130 10:28:07.292933 139865224107776 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.102982044219971, loss=0.8421761393547058
I0130 10:28:41.172401 139865232500480 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.197227478027344, loss=0.805815577507019
I0130 10:29:15.025305 139865224107776 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.118894577026367, loss=0.832787036895752
I0130 10:29:48.894635 139865232500480 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.060284614562988, loss=0.8738629221916199
I0130 10:29:54.818957 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:30:01.247773 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:30:09.867729 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:30:12.591372 140027215431488 submission_runner.py:408] Time since start: 53907.22s, 	Step: 153519, 	{'train/accuracy': 0.9136838316917419, 'train/loss': 0.2980218827724457, 'validation/accuracy': 0.7379800081253052, 'validation/loss': 1.1061500310897827, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.8494164943695068, 'test/num_examples': 10000, 'score': 52058.14172434807, 'total_duration': 53907.217074632645, 'accumulated_submission_time': 52058.14172434807, 'accumulated_eval_time': 1838.1192009449005, 'accumulated_logging_time': 5.501033782958984}
I0130 10:30:12.635708 139865115051776 logging_writer.py:48] [153519] accumulated_eval_time=1838.119201, accumulated_logging_time=5.501034, accumulated_submission_time=52058.141724, global_step=153519, preemption_count=0, score=52058.141724, test/accuracy=0.613900, test/loss=1.849416, test/num_examples=10000, total_duration=53907.217075, train/accuracy=0.913684, train/loss=0.298022, validation/accuracy=0.737980, validation/loss=1.106150, validation/num_examples=50000
I0130 10:30:40.397153 139865123444480 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.079966068267822, loss=0.8627414703369141
I0130 10:31:14.265207 139865115051776 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.9830992221832275, loss=0.7875022292137146
I0130 10:31:48.109698 139865123444480 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.050328731536865, loss=0.8288969993591309
I0130 10:32:21.962167 139865115051776 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.804072618484497, loss=0.7904963493347168
I0130 10:32:55.813973 139865123444480 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.385110855102539, loss=0.8262006640434265
I0130 10:33:29.668012 139865115051776 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.935328960418701, loss=0.7548379302024841
I0130 10:34:03.677646 139865123444480 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.285556793212891, loss=0.8732060790061951
I0130 10:34:37.569475 139865115051776 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.977768898010254, loss=0.7306314706802368
I0130 10:35:11.446255 139865123444480 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.939474582672119, loss=0.7741275429725647
I0130 10:35:45.369059 139865115051776 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.128496170043945, loss=0.8106868267059326
I0130 10:36:19.252530 139865123444480 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.123726844787598, loss=0.8614019155502319
I0130 10:36:53.188524 139865115051776 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.066242218017578, loss=0.8139866590499878
I0130 10:37:27.048686 139865123444480 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.8049888610839844, loss=0.7966088652610779
I0130 10:38:00.922229 139865115051776 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.421864032745361, loss=0.8082711696624756
I0130 10:38:34.807063 139865123444480 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.379507541656494, loss=0.8684124946594238
I0130 10:38:42.750028 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:38:49.027148 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:38:57.797872 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:39:00.455482 140027215431488 submission_runner.py:408] Time since start: 54435.08s, 	Step: 155025, 	{'train/accuracy': 0.9156568646430969, 'train/loss': 0.2907183766365051, 'validation/accuracy': 0.7414599657058716, 'validation/loss': 1.100637435913086, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.840550422668457, 'test/num_examples': 10000, 'score': 52568.189259290695, 'total_duration': 54435.08118414879, 'accumulated_submission_time': 52568.189259290695, 'accumulated_eval_time': 1855.824599981308, 'accumulated_logging_time': 5.557091951370239}
I0130 10:39:00.503097 139865115051776 logging_writer.py:48] [155025] accumulated_eval_time=1855.824600, accumulated_logging_time=5.557092, accumulated_submission_time=52568.189259, global_step=155025, preemption_count=0, score=52568.189259, test/accuracy=0.617400, test/loss=1.840550, test/num_examples=10000, total_duration=54435.081184, train/accuracy=0.915657, train/loss=0.290718, validation/accuracy=0.741460, validation/loss=1.100637, validation/num_examples=50000
I0130 10:39:26.240500 139865123444480 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.187324047088623, loss=0.8568293452262878
I0130 10:40:00.085157 139865115051776 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.309388637542725, loss=0.8789414763450623
I0130 10:40:34.169625 139865123444480 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.984478712081909, loss=0.7977739572525024
I0130 10:41:08.024509 139865115051776 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.999274492263794, loss=0.8847669363021851
I0130 10:41:41.876531 139865123444480 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.896287441253662, loss=0.7817891836166382
I0130 10:42:15.785339 139865115051776 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.307007312774658, loss=0.8310145735740662
I0130 10:42:49.665160 139865123444480 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.22965669631958, loss=0.8685243725776672
I0130 10:43:23.527476 139865115051776 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.05806827545166, loss=0.803347647190094
I0130 10:43:57.443459 139865123444480 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.5575947761535645, loss=0.9331336617469788
I0130 10:44:31.286384 139865115051776 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.239791393280029, loss=0.8013913035392761
I0130 10:45:05.158967 139865123444480 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.144314289093018, loss=0.8103235960006714
I0130 10:45:39.053700 139865115051776 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.326107501983643, loss=0.8906630277633667
I0130 10:46:12.911206 139865123444480 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.134740352630615, loss=0.7022103071212769
I0130 10:46:46.764986 139865115051776 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.101624965667725, loss=0.7769370079040527
I0130 10:47:20.785938 139865123444480 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.9542324542999268, loss=0.8335360884666443
I0130 10:47:30.764292 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:47:37.073436 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:47:45.650629 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:47:48.329206 140027215431488 submission_runner.py:408] Time since start: 54962.95s, 	Step: 156531, 	{'train/accuracy': 0.9124282598495483, 'train/loss': 0.294901579618454, 'validation/accuracy': 0.7405799627304077, 'validation/loss': 1.1055474281311035, 'validation/num_examples': 50000, 'test/accuracy': 0.6130000352859497, 'test/loss': 1.8647652864456177, 'test/num_examples': 10000, 'score': 53078.3854739666, 'total_duration': 54962.95487737656, 'accumulated_submission_time': 53078.3854739666, 'accumulated_eval_time': 1873.3894336223602, 'accumulated_logging_time': 5.6150617599487305}
I0130 10:47:48.385892 139865232500480 logging_writer.py:48] [156531] accumulated_eval_time=1873.389434, accumulated_logging_time=5.615062, accumulated_submission_time=53078.385474, global_step=156531, preemption_count=0, score=53078.385474, test/accuracy=0.613000, test/loss=1.864765, test/num_examples=10000, total_duration=54962.954877, train/accuracy=0.912428, train/loss=0.294902, validation/accuracy=0.740580, validation/loss=1.105547, validation/num_examples=50000
I0130 10:48:12.062673 139865240893184 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.926344633102417, loss=0.6860086917877197
I0130 10:48:45.957354 139865232500480 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.572378158569336, loss=0.7849472165107727
I0130 10:49:19.823309 139865240893184 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.009138107299805, loss=0.7836984395980835
I0130 10:49:53.700202 139865232500480 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.96960186958313, loss=0.7042681574821472
I0130 10:50:27.602266 139865240893184 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.668827772140503, loss=0.6806907057762146
I0130 10:51:01.501033 139865232500480 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.028533458709717, loss=0.7814286351203918
I0130 10:51:35.377453 139865240893184 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.812680721282959, loss=0.7778506278991699
I0130 10:52:09.261776 139865232500480 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.0312089920043945, loss=0.7717186212539673
I0130 10:52:43.130628 139865240893184 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.38916015625, loss=0.8575460910797119
I0130 10:53:16.993904 139865232500480 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.0051469802856445, loss=0.7647632360458374
I0130 10:53:50.979668 139865240893184 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.306367874145508, loss=0.8144501447677612
I0130 10:54:24.869283 139865232500480 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.337109088897705, loss=0.7682482600212097
I0130 10:54:58.750259 139865240893184 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.12352180480957, loss=0.8630820512771606
I0130 10:55:32.592564 139865232500480 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.3345746994018555, loss=0.8270312547683716
I0130 10:56:06.441395 139865240893184 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.27046012878418, loss=0.7414136528968811
I0130 10:56:18.447956 140027215431488 spec.py:321] Evaluating on the training split.
I0130 10:56:24.791489 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 10:56:33.251408 140027215431488 spec.py:349] Evaluating on the test split.
I0130 10:56:36.378342 140027215431488 submission_runner.py:408] Time since start: 55491.00s, 	Step: 158037, 	{'train/accuracy': 0.91796875, 'train/loss': 0.2793106138706207, 'validation/accuracy': 0.7449199557304382, 'validation/loss': 1.0848394632339478, 'validation/num_examples': 50000, 'test/accuracy': 0.6182000041007996, 'test/loss': 1.8408807516098022, 'test/num_examples': 10000, 'score': 53588.38181400299, 'total_duration': 55491.00404858589, 'accumulated_submission_time': 53588.38181400299, 'accumulated_eval_time': 1891.3197746276855, 'accumulated_logging_time': 5.684006452560425}
I0130 10:56:36.416797 139865123444480 logging_writer.py:48] [158037] accumulated_eval_time=1891.319775, accumulated_logging_time=5.684006, accumulated_submission_time=53588.381814, global_step=158037, preemption_count=0, score=53588.381814, test/accuracy=0.618200, test/loss=1.840881, test/num_examples=10000, total_duration=55491.004049, train/accuracy=0.917969, train/loss=0.279311, validation/accuracy=0.744920, validation/loss=1.084839, validation/num_examples=50000
I0130 10:56:58.091202 139865131837184 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.3777618408203125, loss=0.7703202366828918
I0130 10:57:31.957983 139865123444480 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.0928239822387695, loss=0.7394189238548279
I0130 10:58:05.794160 139865131837184 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.4362053871154785, loss=0.7509593963623047
I0130 10:58:39.661472 139865123444480 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.106410503387451, loss=0.7139022350311279
I0130 10:59:13.587027 139865131837184 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.262588977813721, loss=0.8271574378013611
I0130 10:59:47.459177 139865123444480 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.389606475830078, loss=0.8016751408576965
I0130 11:00:21.445581 139865131837184 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.1270222663879395, loss=0.8549156188964844
I0130 11:00:55.308460 139865123444480 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.990933656692505, loss=0.7668411135673523
I0130 11:01:29.230607 139865131837184 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.26827335357666, loss=0.7877975702285767
I0130 11:02:03.104723 139865123444480 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.113216400146484, loss=0.7111067771911621
I0130 11:02:37.036687 139865131837184 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.744785785675049, loss=0.7722367644309998
I0130 11:03:10.900712 139865123444480 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.098203182220459, loss=0.7639824151992798
I0130 11:03:44.785476 139865131837184 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.8900320529937744, loss=0.70725417137146
I0130 11:04:18.681327 139865123444480 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.945136547088623, loss=0.770999014377594
I0130 11:04:52.547745 139865131837184 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.505578994750977, loss=0.7132619023323059
I0130 11:05:06.599961 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:05:12.897258 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:05:21.574566 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:05:24.280823 140027215431488 submission_runner.py:408] Time since start: 56018.91s, 	Step: 159543, 	{'train/accuracy': 0.9270168542861938, 'train/loss': 0.25450384616851807, 'validation/accuracy': 0.7486400008201599, 'validation/loss': 1.0716850757598877, 'validation/num_examples': 50000, 'test/accuracy': 0.6185000538825989, 'test/loss': 1.8318090438842773, 'test/num_examples': 10000, 'score': 54098.502118587494, 'total_duration': 56018.906514406204, 'accumulated_submission_time': 54098.502118587494, 'accumulated_eval_time': 1909.0005717277527, 'accumulated_logging_time': 5.731633424758911}
I0130 11:05:24.327359 139865115051776 logging_writer.py:48] [159543] accumulated_eval_time=1909.000572, accumulated_logging_time=5.731633, accumulated_submission_time=54098.502119, global_step=159543, preemption_count=0, score=54098.502119, test/accuracy=0.618500, test/loss=1.831809, test/num_examples=10000, total_duration=56018.906514, train/accuracy=0.927017, train/loss=0.254504, validation/accuracy=0.748640, validation/loss=1.071685, validation/num_examples=50000
I0130 11:05:43.970961 139865232500480 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.445965766906738, loss=0.8111180067062378
I0130 11:06:17.817693 139865115051776 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.357021808624268, loss=0.7788958549499512
I0130 11:06:51.926203 139865232500480 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.314091205596924, loss=0.7758923768997192
I0130 11:07:25.802017 139865115051776 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.363389015197754, loss=0.745198130607605
I0130 11:07:59.695392 139865232500480 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.898904800415039, loss=0.655802845954895
I0130 11:08:33.543666 139865115051776 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.280546188354492, loss=0.7017702460289001
I0130 11:09:07.443711 139865232500480 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.285046100616455, loss=0.734976589679718
I0130 11:09:41.328816 139865115051776 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.0880327224731445, loss=0.7275105118751526
I0130 11:10:15.193357 139865232500480 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.515210151672363, loss=0.7559476494789124
I0130 11:10:49.130202 139865115051776 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.393747329711914, loss=0.7518831491470337
I0130 11:11:23.005976 139865232500480 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.0027241706848145, loss=0.6575158834457397
I0130 11:11:56.929047 139865115051776 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.25277853012085, loss=0.8226656913757324
I0130 11:12:30.797514 139865232500480 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.5985188484191895, loss=0.7780781984329224
I0130 11:13:04.690548 139865115051776 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.18198299407959, loss=0.6944937705993652
I0130 11:13:38.725967 139865232500480 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.223043918609619, loss=0.7580691576004028
I0130 11:13:54.436163 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:14:00.751363 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:14:09.633877 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:14:12.322067 140027215431488 submission_runner.py:408] Time since start: 56546.95s, 	Step: 161048, 	{'train/accuracy': 0.9447743892669678, 'train/loss': 0.20134520530700684, 'validation/accuracy': 0.7463200092315674, 'validation/loss': 1.0841114521026611, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.8436534404754639, 'test/num_examples': 10000, 'score': 54608.54653549194, 'total_duration': 56546.94776725769, 'accumulated_submission_time': 54608.54653549194, 'accumulated_eval_time': 1926.8864195346832, 'accumulated_logging_time': 5.78792929649353}
I0130 11:14:12.367085 139865115051776 logging_writer.py:48] [161048] accumulated_eval_time=1926.886420, accumulated_logging_time=5.787929, accumulated_submission_time=54608.546535, global_step=161048, preemption_count=0, score=54608.546535, test/accuracy=0.620500, test/loss=1.843653, test/num_examples=10000, total_duration=56546.947767, train/accuracy=0.944774, train/loss=0.201345, validation/accuracy=0.746320, validation/loss=1.084111, validation/num_examples=50000
I0130 11:14:30.305037 139865123444480 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.7462077140808105, loss=0.816401481628418
I0130 11:15:04.148521 139865115051776 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.21111536026001, loss=0.7604446411132812
I0130 11:15:38.050489 139865123444480 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.9947006702423096, loss=0.7608359456062317
I0130 11:16:11.934949 139865115051776 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.198600769042969, loss=0.670801043510437
I0130 11:16:45.839019 139865123444480 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.496291160583496, loss=0.7094697952270508
I0130 11:17:19.715806 139865115051776 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.41649866104126, loss=0.7636555433273315
I0130 11:17:53.607496 139865123444480 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.310314178466797, loss=0.6919792890548706
I0130 11:18:27.512552 139865115051776 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.235108375549316, loss=0.6966425776481628
I0130 11:19:01.414367 139865123444480 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.6507039070129395, loss=0.7091464400291443
I0130 11:19:35.323845 139865115051776 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.175333499908447, loss=0.7590010166168213
I0130 11:20:09.336675 139865123444480 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.345851898193359, loss=0.7682604193687439
I0130 11:20:43.182448 139865115051776 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.914233922958374, loss=0.7227226495742798
I0130 11:21:17.088813 139865123444480 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.410395622253418, loss=0.8277007341384888
I0130 11:21:50.995485 139865115051776 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.396818161010742, loss=0.7888948917388916
I0130 11:22:24.906086 139865123444480 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.177459239959717, loss=0.752191424369812
I0130 11:22:42.359950 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:22:48.736604 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:22:57.299323 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:23:00.022544 140027215431488 submission_runner.py:408] Time since start: 57074.65s, 	Step: 162553, 	{'train/accuracy': 0.9421635866165161, 'train/loss': 0.20348194241523743, 'validation/accuracy': 0.7483599781990051, 'validation/loss': 1.0738941431045532, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.835035800933838, 'test/num_examples': 10000, 'score': 55118.473893880844, 'total_duration': 57074.64824795723, 'accumulated_submission_time': 55118.473893880844, 'accumulated_eval_time': 1944.548994064331, 'accumulated_logging_time': 5.8435704708099365}
I0130 11:23:00.069504 139865106659072 logging_writer.py:48] [162553] accumulated_eval_time=1944.548994, accumulated_logging_time=5.843570, accumulated_submission_time=55118.473894, global_step=162553, preemption_count=0, score=55118.473894, test/accuracy=0.620600, test/loss=1.835036, test/num_examples=10000, total_duration=57074.648248, train/accuracy=0.942164, train/loss=0.203482, validation/accuracy=0.748360, validation/loss=1.073894, validation/num_examples=50000
I0130 11:23:16.308002 139865224107776 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.44649600982666, loss=0.7275636196136475
I0130 11:23:50.217340 139865106659072 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.586341381072998, loss=0.8585326075553894
I0130 11:24:24.054740 139865224107776 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.392533302307129, loss=0.6643534898757935
I0130 11:24:57.930491 139865106659072 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.889981746673584, loss=0.7696623802185059
I0130 11:25:31.835694 139865224107776 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.239534854888916, loss=0.7380306124687195
I0130 11:26:05.708555 139865106659072 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.359883785247803, loss=0.7147676348686218
I0130 11:26:39.687802 139865224107776 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.534971714019775, loss=0.7509688138961792
I0130 11:27:13.613484 139865106659072 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.325560092926025, loss=0.7167301177978516
I0130 11:27:47.470371 139865224107776 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.192102909088135, loss=0.7472755312919617
I0130 11:28:21.337511 139865106659072 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.236753940582275, loss=0.7991405129432678
I0130 11:28:55.236289 139865224107776 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.555582046508789, loss=0.7578461170196533
I0130 11:29:29.095476 139865106659072 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.129240989685059, loss=0.7484554052352905
I0130 11:30:02.981935 139865224107776 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.592998504638672, loss=0.6936275362968445
I0130 11:30:36.879101 139865106659072 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.266533374786377, loss=0.7395703792572021
I0130 11:31:10.753622 139865224107776 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.337464809417725, loss=0.6961707472801208
I0130 11:31:30.222583 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:31:36.573011 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:31:45.174275 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:31:47.878268 140027215431488 submission_runner.py:408] Time since start: 57602.50s, 	Step: 164059, 	{'train/accuracy': 0.9382772445678711, 'train/loss': 0.2111508697271347, 'validation/accuracy': 0.7485199570655823, 'validation/loss': 1.0775885581970215, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8518248796463013, 'test/num_examples': 10000, 'score': 55628.56163620949, 'total_duration': 57602.503945589066, 'accumulated_submission_time': 55628.56163620949, 'accumulated_eval_time': 1962.2046110630035, 'accumulated_logging_time': 5.903212070465088}
I0130 11:31:47.926956 139865123444480 logging_writer.py:48] [164059] accumulated_eval_time=1962.204611, accumulated_logging_time=5.903212, accumulated_submission_time=55628.561636, global_step=164059, preemption_count=0, score=55628.561636, test/accuracy=0.621200, test/loss=1.851825, test/num_examples=10000, total_duration=57602.503946, train/accuracy=0.938277, train/loss=0.211151, validation/accuracy=0.748520, validation/loss=1.077589, validation/num_examples=50000
I0130 11:32:02.168477 139865131837184 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.146561145782471, loss=0.7283324003219604
I0130 11:32:36.063045 139865123444480 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.520120620727539, loss=0.7194212675094604
I0130 11:33:09.910123 139865131837184 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.4068732261657715, loss=0.7250797748565674
I0130 11:33:43.898742 139865123444480 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.673527240753174, loss=0.7619637250900269
I0130 11:34:17.756222 139865131837184 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.330656051635742, loss=0.754652202129364
I0130 11:34:51.610321 139865123444480 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.372151851654053, loss=0.7295995950698853
I0130 11:35:25.525881 139865131837184 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.345979690551758, loss=0.6805697679519653
I0130 11:35:59.404238 139865123444480 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.635186672210693, loss=0.7234823703765869
I0130 11:36:33.298940 139865131837184 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.0951151847839355, loss=0.6620742082595825
I0130 11:37:07.208261 139865123444480 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.514458179473877, loss=0.7144309282302856
I0130 11:37:41.092598 139865131837184 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.361973762512207, loss=0.7672417163848877
I0130 11:38:14.989461 139865123444480 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.0280537605285645, loss=0.6711479425430298
I0130 11:38:48.854814 139865131837184 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.415842533111572, loss=0.7220126986503601
I0130 11:39:22.768602 139865123444480 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.50714111328125, loss=0.7248240113258362
I0130 11:39:56.694440 139865131837184 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.4868245124816895, loss=0.6954626441001892
I0130 11:40:18.190220 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:40:24.507421 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:40:33.058667 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:40:35.787480 140027215431488 submission_runner.py:408] Time since start: 58130.41s, 	Step: 165565, 	{'train/accuracy': 0.9431201815605164, 'train/loss': 0.20247571170330048, 'validation/accuracy': 0.7490999698638916, 'validation/loss': 1.0750706195831299, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.850699543952942, 'test/num_examples': 10000, 'score': 56138.76058793068, 'total_duration': 58130.41319346428, 'accumulated_submission_time': 56138.76058793068, 'accumulated_eval_time': 1979.8018288612366, 'accumulated_logging_time': 5.963002681732178}
I0130 11:40:35.825625 139865115051776 logging_writer.py:48] [165565] accumulated_eval_time=1979.801829, accumulated_logging_time=5.963003, accumulated_submission_time=56138.760588, global_step=165565, preemption_count=0, score=56138.760588, test/accuracy=0.622400, test/loss=1.850700, test/num_examples=10000, total_duration=58130.413193, train/accuracy=0.943120, train/loss=0.202476, validation/accuracy=0.749100, validation/loss=1.075071, validation/num_examples=50000
I0130 11:40:48.020969 139865123444480 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.600582599639893, loss=0.721882700920105
I0130 11:41:21.890778 139865115051776 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.55026912689209, loss=0.7024405598640442
I0130 11:41:55.766445 139865123444480 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.297402858734131, loss=0.7293523550033569
I0130 11:42:29.648204 139865115051776 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.311439514160156, loss=0.6548840403556824
I0130 11:43:03.563497 139865123444480 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.458774089813232, loss=0.6480153799057007
I0130 11:43:37.479850 139865115051776 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.176110744476318, loss=0.6389106512069702
I0130 11:44:11.381307 139865123444480 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.590181827545166, loss=0.6933021545410156
I0130 11:44:45.294934 139865115051776 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.703094482421875, loss=0.7005472779273987
I0130 11:45:19.202619 139865123444480 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.459428310394287, loss=0.7073255181312561
I0130 11:45:53.121734 139865115051776 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.9871625900268555, loss=0.6488024592399597
I0130 11:46:27.131495 139865123444480 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.5448198318481445, loss=0.6705944538116455
I0130 11:47:00.989726 139865115051776 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.725329399108887, loss=0.7138017416000366
I0130 11:47:34.860053 139865123444480 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.293394088745117, loss=0.6665530204772949
I0130 11:48:08.771529 139865115051776 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.590862274169922, loss=0.620417058467865
I0130 11:48:42.680575 139865123444480 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.042727947235107, loss=0.6398358941078186
I0130 11:49:05.870277 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:49:12.505551 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:49:21.243766 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:49:23.912700 140027215431488 submission_runner.py:408] Time since start: 58658.54s, 	Step: 167070, 	{'train/accuracy': 0.9422233700752258, 'train/loss': 0.20181764662265778, 'validation/accuracy': 0.7506600022315979, 'validation/loss': 1.0700610876083374, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.8532116413116455, 'test/num_examples': 10000, 'score': 56648.743233919144, 'total_duration': 58658.53840446472, 'accumulated_submission_time': 56648.743233919144, 'accumulated_eval_time': 1997.844202041626, 'accumulated_logging_time': 6.011024236679077}
I0130 11:49:23.963775 139865131837184 logging_writer.py:48] [167070] accumulated_eval_time=1997.844202, accumulated_logging_time=6.011024, accumulated_submission_time=56648.743234, global_step=167070, preemption_count=0, score=56648.743234, test/accuracy=0.623800, test/loss=1.853212, test/num_examples=10000, total_duration=58658.538404, train/accuracy=0.942223, train/loss=0.201818, validation/accuracy=0.750660, validation/loss=1.070061, validation/num_examples=50000
I0130 11:49:34.452077 139865140229888 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.241357803344727, loss=0.716744065284729
I0130 11:50:08.335615 139865131837184 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.3769450187683105, loss=0.7380180358886719
I0130 11:50:42.207245 139865140229888 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.251634120941162, loss=0.6595210433006287
I0130 11:51:16.077322 139865131837184 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.109668731689453, loss=0.682103157043457
I0130 11:51:49.967373 139865140229888 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.227292537689209, loss=0.6660574674606323
I0130 11:52:23.870972 139865131837184 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.229586601257324, loss=0.6672428250312805
I0130 11:52:57.834997 139865140229888 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.838687896728516, loss=0.7431268692016602
I0130 11:53:31.689181 139865131837184 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.328704357147217, loss=0.7092280387878418
I0130 11:54:05.611631 139865140229888 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.27784538269043, loss=0.6737562417984009
I0130 11:54:39.481686 139865131837184 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.245273113250732, loss=0.6409105658531189
I0130 11:55:13.370377 139865140229888 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.37274169921875, loss=0.6848503947257996
I0130 11:55:47.267381 139865131837184 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.610382556915283, loss=0.643297016620636
I0130 11:56:21.139204 139865140229888 logging_writer.py:48] [168300] global_step=168300, grad_norm=3.8992972373962402, loss=0.6102895140647888
I0130 11:56:55.038720 139865131837184 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.542838096618652, loss=0.7518118023872375
I0130 11:57:28.896386 139865140229888 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.476288318634033, loss=0.651437520980835
I0130 11:57:54.102870 140027215431488 spec.py:321] Evaluating on the training split.
I0130 11:58:00.454531 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 11:58:09.162056 140027215431488 spec.py:349] Evaluating on the test split.
I0130 11:58:11.848515 140027215431488 submission_runner.py:408] Time since start: 59186.47s, 	Step: 168576, 	{'train/accuracy': 0.9480428695678711, 'train/loss': 0.18233726918697357, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 1.0641475915908813, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.8473761081695557, 'test/num_examples': 10000, 'score': 57158.817729473114, 'total_duration': 59186.4742205143, 'accumulated_submission_time': 57158.817729473114, 'accumulated_eval_time': 2015.589801311493, 'accumulated_logging_time': 6.071972846984863}
I0130 11:58:11.897391 139865123444480 logging_writer.py:48] [168576] accumulated_eval_time=2015.589801, accumulated_logging_time=6.071973, accumulated_submission_time=57158.817729, global_step=168576, preemption_count=0, score=57158.817729, test/accuracy=0.625700, test/loss=1.847376, test/num_examples=10000, total_duration=59186.474221, train/accuracy=0.948043, train/loss=0.182337, validation/accuracy=0.752320, validation/loss=1.064148, validation/num_examples=50000
I0130 11:58:20.353824 139865224107776 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.573614120483398, loss=0.7458133697509766
I0130 11:58:54.218609 139865123444480 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.804744720458984, loss=0.7299622297286987
I0130 11:59:28.081068 139865224107776 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.492613792419434, loss=0.6963788270950317
I0130 12:00:02.045702 139865123444480 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.20538854598999, loss=0.6552119255065918
I0130 12:00:35.920094 139865224107776 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.1602911949157715, loss=0.6484306454658508
I0130 12:01:09.836080 139865123444480 logging_writer.py:48] [169100] global_step=169100, grad_norm=3.8844940662384033, loss=0.5829677581787109
I0130 12:01:43.725696 139865224107776 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.47890043258667, loss=0.6426217555999756
I0130 12:02:17.598114 139865123444480 logging_writer.py:48] [169300] global_step=169300, grad_norm=3.9195356369018555, loss=0.6822879910469055
I0130 12:02:51.508612 139865224107776 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.451035022735596, loss=0.7741508483886719
I0130 12:03:25.350605 139865123444480 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.640092372894287, loss=0.638873279094696
I0130 12:03:59.238003 139865224107776 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.079406261444092, loss=0.6986840963363647
I0130 12:04:33.120086 139865123444480 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.789407253265381, loss=0.7212014198303223
I0130 12:05:06.999381 139865224107776 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.272858619689941, loss=0.6317509412765503
I0130 12:05:40.910083 139865123444480 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.651647090911865, loss=0.7524839639663696
I0130 12:06:14.894619 139865224107776 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.517521381378174, loss=0.7149918675422668
I0130 12:06:42.128362 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:06:48.546151 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:06:57.010810 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:06:59.666362 140027215431488 submission_runner.py:408] Time since start: 59714.29s, 	Step: 170082, 	{'train/accuracy': 0.9587252736091614, 'train/loss': 0.15863607823848724, 'validation/accuracy': 0.7521199584007263, 'validation/loss': 1.072721004486084, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.8547792434692383, 'test/num_examples': 10000, 'score': 57668.98382782936, 'total_duration': 59714.29183888435, 'accumulated_submission_time': 57668.98382782936, 'accumulated_eval_time': 2033.127522945404, 'accumulated_logging_time': 6.13216757774353}
I0130 12:06:59.715322 139865123444480 logging_writer.py:48] [170082] accumulated_eval_time=2033.127523, accumulated_logging_time=6.132168, accumulated_submission_time=57668.983828, global_step=170082, preemption_count=0, score=57668.983828, test/accuracy=0.625300, test/loss=1.854779, test/num_examples=10000, total_duration=59714.291839, train/accuracy=0.958725, train/loss=0.158636, validation/accuracy=0.752120, validation/loss=1.072721, validation/num_examples=50000
I0130 12:07:06.169305 139865131837184 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.358362197875977, loss=0.633906364440918
I0130 12:07:40.060527 139865123444480 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.023072242736816, loss=0.648879885673523
I0130 12:08:13.906558 139865131837184 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.666100025177002, loss=0.6978490352630615
I0130 12:08:47.790765 139865123444480 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.414040565490723, loss=0.648903489112854
I0130 12:09:21.649938 139865131837184 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.160686492919922, loss=0.651070237159729
I0130 12:09:55.504740 139865123444480 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.290200710296631, loss=0.6992062926292419
I0130 12:10:29.400354 139865131837184 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.2040557861328125, loss=0.6290749311447144
I0130 12:11:03.267759 139865123444480 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.631464958190918, loss=0.6394197940826416
I0130 12:11:37.151900 139865131837184 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.562414646148682, loss=0.6332016587257385
I0130 12:12:11.060603 139865123444480 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.278924942016602, loss=0.614126443862915
I0130 12:12:45.051279 139865131837184 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.377981185913086, loss=0.6648495197296143
I0130 12:13:18.935324 139865123444480 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.096437454223633, loss=0.6024788618087769
I0130 12:13:52.830568 139865131837184 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.538392543792725, loss=0.6862049698829651
I0130 12:14:26.702724 139865123444480 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.194836139678955, loss=0.5473083257675171
I0130 12:15:00.567765 139865131837184 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.389491081237793, loss=0.6507511138916016
I0130 12:15:29.845091 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:15:36.261271 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:15:44.941223 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:15:47.642096 140027215431488 submission_runner.py:408] Time since start: 60242.27s, 	Step: 171588, 	{'train/accuracy': 0.9563336968421936, 'train/loss': 0.16189979016780853, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.0672721862792969, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.851597547531128, 'test/num_examples': 10000, 'score': 58179.04970908165, 'total_duration': 60242.267706632614, 'accumulated_submission_time': 58179.04970908165, 'accumulated_eval_time': 2050.924390077591, 'accumulated_logging_time': 6.191601753234863}
I0130 12:15:47.695594 139865232500480 logging_writer.py:48] [171588] accumulated_eval_time=2050.924390, accumulated_logging_time=6.191602, accumulated_submission_time=58179.049709, global_step=171588, preemption_count=0, score=58179.049709, test/accuracy=0.623000, test/loss=1.851598, test/num_examples=10000, total_duration=60242.267707, train/accuracy=0.956334, train/loss=0.161900, validation/accuracy=0.751040, validation/loss=1.067272, validation/num_examples=50000
I0130 12:15:52.113541 139865240893184 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.265092849731445, loss=0.6393514275550842
I0130 12:16:25.998632 139865232500480 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.371250152587891, loss=0.7135152220726013
I0130 12:16:59.856832 139865240893184 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.56288480758667, loss=0.6028528213500977
I0130 12:17:33.725199 139865232500480 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.639115810394287, loss=0.6196068525314331
I0130 12:18:07.647662 139865240893184 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.396407127380371, loss=0.6467900276184082
I0130 12:18:41.530933 139865232500480 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.799567699432373, loss=0.6522120237350464
I0130 12:19:15.447044 139865240893184 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.906142234802246, loss=0.6860367655754089
I0130 12:19:49.422041 139865232500480 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.455748558044434, loss=0.6501588225364685
I0130 12:20:23.305521 139865240893184 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.496756553649902, loss=0.6156101226806641
I0130 12:20:57.217428 139865232500480 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.802206993103027, loss=0.6846457719802856
I0130 12:21:31.091362 139865240893184 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.324065208435059, loss=0.6263085603713989
I0130 12:22:05.005807 139865232500480 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.263660430908203, loss=0.6111212968826294
I0130 12:22:38.881283 139865240893184 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.130080223083496, loss=0.6051546335220337
I0130 12:23:12.772334 139865232500480 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.485752105712891, loss=0.67087322473526
I0130 12:23:46.653130 139865240893184 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.1347880363464355, loss=0.5881746411323547
I0130 12:24:17.948575 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:24:25.084757 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:24:33.755652 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:24:37.109071 140027215431488 submission_runner.py:408] Time since start: 60771.73s, 	Step: 173094, 	{'train/accuracy': 0.9559350609779358, 'train/loss': 0.16148580610752106, 'validation/accuracy': 0.7530199885368347, 'validation/loss': 1.0638294219970703, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.836564540863037, 'test/num_examples': 10000, 'score': 58689.24033522606, 'total_duration': 60771.73478055, 'accumulated_submission_time': 58689.24033522606, 'accumulated_eval_time': 2070.0848445892334, 'accumulated_logging_time': 6.254953384399414}
I0130 12:24:37.150840 139865131837184 logging_writer.py:48] [173094] accumulated_eval_time=2070.084845, accumulated_logging_time=6.254953, accumulated_submission_time=58689.240335, global_step=173094, preemption_count=0, score=58689.240335, test/accuracy=0.628300, test/loss=1.836565, test/num_examples=10000, total_duration=60771.734781, train/accuracy=0.955935, train/loss=0.161486, validation/accuracy=0.753020, validation/loss=1.063829, validation/num_examples=50000
I0130 12:24:39.523067 139865140229888 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.16021203994751, loss=0.641599178314209
I0130 12:25:13.383150 139865131837184 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.288546562194824, loss=0.664570689201355
I0130 12:25:47.216751 139865140229888 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.699148178100586, loss=0.7096750736236572
I0130 12:26:21.197691 139865131837184 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.254508018493652, loss=0.5605567693710327
I0130 12:26:55.067894 139865140229888 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.370097637176514, loss=0.6303349137306213
I0130 12:27:28.962878 139865131837184 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.634158611297607, loss=0.6584739685058594
I0130 12:28:02.857230 139865140229888 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.619319915771484, loss=0.6975081562995911
I0130 12:28:36.733191 139865131837184 logging_writer.py:48] [173800] global_step=173800, grad_norm=5.0065693855285645, loss=0.7027830481529236
I0130 12:29:10.652832 139865140229888 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.670669078826904, loss=0.6677155494689941
I0130 12:29:44.515382 139865131837184 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.573701858520508, loss=0.6337281465530396
I0130 12:30:18.443039 139865140229888 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.659018039703369, loss=0.6649563312530518
I0130 12:30:52.326334 139865131837184 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.3829665184021, loss=0.6634215712547302
I0130 12:31:26.234985 139865140229888 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.512719631195068, loss=0.6839827299118042
I0130 12:32:00.116927 139865131837184 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.190685272216797, loss=0.6408528685569763
I0130 12:32:34.107313 139865140229888 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.910759925842285, loss=0.6856111288070679
I0130 12:33:07.136898 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:33:13.470777 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:33:22.223444 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:33:24.886302 140027215431488 submission_runner.py:408] Time since start: 61299.51s, 	Step: 174599, 	{'train/accuracy': 0.9558952450752258, 'train/loss': 0.1617143154144287, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 1.0610108375549316, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8357188701629639, 'test/num_examples': 10000, 'score': 59199.163927316666, 'total_duration': 61299.51197075844, 'accumulated_submission_time': 59199.163927316666, 'accumulated_eval_time': 2087.834163427353, 'accumulated_logging_time': 6.306419849395752}
I0130 12:33:24.936303 139865232500480 logging_writer.py:48] [174599] accumulated_eval_time=2087.834163, accumulated_logging_time=6.306420, accumulated_submission_time=59199.163927, global_step=174599, preemption_count=0, score=59199.163927, test/accuracy=0.627100, test/loss=1.835719, test/num_examples=10000, total_duration=61299.511971, train/accuracy=0.955895, train/loss=0.161714, validation/accuracy=0.753900, validation/loss=1.061011, validation/num_examples=50000
I0130 12:33:25.626945 139865240893184 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.380220890045166, loss=0.6919979453086853
I0130 12:33:59.516277 139865232500480 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.472410202026367, loss=0.6775675415992737
I0130 12:34:33.386162 139865240893184 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.45281457901001, loss=0.6505778431892395
I0130 12:35:07.253658 139865232500480 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.488529205322266, loss=0.6450718641281128
I0130 12:35:41.147677 139865240893184 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.576517105102539, loss=0.6899723410606384
I0130 12:36:15.020124 139865232500480 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.600994110107422, loss=0.6628512144088745
I0130 12:36:48.925062 139865240893184 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.647006511688232, loss=0.6435484886169434
I0130 12:37:22.805727 139865232500480 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.766134738922119, loss=0.7373831868171692
I0130 12:37:56.697105 139865240893184 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.4601545333862305, loss=0.6139330267906189
I0130 12:38:30.583919 139865232500480 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.799648761749268, loss=0.680072009563446
I0130 12:39:04.633581 139865240893184 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.5087971687316895, loss=0.6112421751022339
I0130 12:39:38.528807 139865232500480 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.075836181640625, loss=0.6471264958381653
I0130 12:40:12.414765 139865240893184 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.385287761688232, loss=0.6202571988105774
I0130 12:40:46.318241 139865232500480 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.964282989501953, loss=0.7550104856491089
I0130 12:41:20.211123 139865240893184 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.946447372436523, loss=0.729084849357605
I0130 12:41:54.106683 139865232500480 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.437423229217529, loss=0.607842743396759
I0130 12:41:54.936009 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:42:01.341001 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:42:10.055146 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:42:12.645478 140027215431488 submission_runner.py:408] Time since start: 61827.27s, 	Step: 176104, 	{'train/accuracy': 0.9573700428009033, 'train/loss': 0.15628387033939362, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0589247941970825, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8355635404586792, 'test/num_examples': 10000, 'score': 59709.09991669655, 'total_duration': 61827.27109313011, 'accumulated_submission_time': 59709.09991669655, 'accumulated_eval_time': 2105.543501853943, 'accumulated_logging_time': 6.366549730300903}
I0130 12:42:12.694253 139865115051776 logging_writer.py:48] [176104] accumulated_eval_time=2105.543502, accumulated_logging_time=6.366550, accumulated_submission_time=59709.099917, global_step=176104, preemption_count=0, score=59709.099917, test/accuracy=0.629700, test/loss=1.835564, test/num_examples=10000, total_duration=61827.271093, train/accuracy=0.957370, train/loss=0.156284, validation/accuracy=0.755240, validation/loss=1.058925, validation/num_examples=50000
I0130 12:42:45.579849 139865123444480 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.63024377822876, loss=0.6555191278457642
I0130 12:43:19.449329 139865115051776 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.605604648590088, loss=0.6437644958496094
I0130 12:43:53.359714 139865123444480 logging_writer.py:48] [176400] global_step=176400, grad_norm=5.040497303009033, loss=0.6595234274864197
I0130 12:44:27.224534 139865115051776 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.569871425628662, loss=0.6686805486679077
I0130 12:45:01.147847 139865123444480 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.341032028198242, loss=0.5942121744155884
I0130 12:45:35.020735 139865115051776 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.9352521896362305, loss=0.7187809348106384
I0130 12:46:09.126801 139865123444480 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.343472957611084, loss=0.6222251653671265
I0130 12:46:42.975480 139865115051776 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.516055107116699, loss=0.6696518659591675
I0130 12:47:16.824562 139865123444480 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.5543012619018555, loss=0.6595895290374756
I0130 12:47:50.744300 139865115051776 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.917612075805664, loss=0.7591307163238525
I0130 12:48:24.609202 139865123444480 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.09144401550293, loss=0.6004488468170166
I0130 12:48:58.543735 139865115051776 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.658407688140869, loss=0.6983533501625061
I0130 12:49:32.408993 139865123444480 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.136247634887695, loss=0.5743709802627563
I0130 12:50:06.327954 139865115051776 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.742642879486084, loss=0.7032992243766785
I0130 12:50:40.208760 139865123444480 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.522429943084717, loss=0.6172414422035217
I0130 12:50:42.721956 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:50:49.053859 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:50:57.813640 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:51:00.420055 140027215431488 submission_runner.py:408] Time since start: 62355.05s, 	Step: 177609, 	{'train/accuracy': 0.9576889276504517, 'train/loss': 0.15669949352741241, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.05818510055542, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8384114503860474, 'test/num_examples': 10000, 'score': 60219.06253504753, 'total_duration': 62355.04538965225, 'accumulated_submission_time': 60219.06253504753, 'accumulated_eval_time': 2123.2411789894104, 'accumulated_logging_time': 6.4263880252838135}
I0130 12:51:00.472328 139865232500480 logging_writer.py:48] [177609] accumulated_eval_time=2123.241179, accumulated_logging_time=6.426388, accumulated_submission_time=60219.062535, global_step=177609, preemption_count=0, score=60219.062535, test/accuracy=0.630000, test/loss=1.838411, test/num_examples=10000, total_duration=62355.045390, train/accuracy=0.957689, train/loss=0.156699, validation/accuracy=0.754660, validation/loss=1.058185, validation/num_examples=50000
I0130 12:51:31.632416 139865240893184 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.241816520690918, loss=0.631919801235199
I0130 12:52:05.488647 139865232500480 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.626449108123779, loss=0.7026026248931885
I0130 12:52:39.516676 139865240893184 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.776490688323975, loss=0.7078354358673096
I0130 12:53:13.426379 139865232500480 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.808776378631592, loss=0.6704067587852478
I0130 12:53:47.309527 139865240893184 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.814863204956055, loss=0.6458997130393982
I0130 12:54:21.224675 139865232500480 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.348995208740234, loss=0.6356518268585205
I0130 12:54:55.126048 139865240893184 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.211941242218018, loss=0.6140130162239075
I0130 12:55:29.024090 139865232500480 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.220364093780518, loss=0.6022757887840271
I0130 12:56:02.920608 139865240893184 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.562171936035156, loss=0.6366267800331116
I0130 12:56:36.828831 139865232500480 logging_writer.py:48] [178600] global_step=178600, grad_norm=5.062126159667969, loss=0.7319251894950867
I0130 12:57:10.692758 139865240893184 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.432129859924316, loss=0.634988009929657
I0130 12:57:44.590893 139865232500480 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.4744157791137695, loss=0.6326274871826172
I0130 12:58:18.477620 139865240893184 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.317526817321777, loss=0.6226159334182739
I0130 12:58:52.432054 139865232500480 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.816504001617432, loss=0.59791499376297
I0130 12:59:26.281919 139865240893184 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.942334175109863, loss=0.6899635791778564
I0130 12:59:30.497350 140027215431488 spec.py:321] Evaluating on the training split.
I0130 12:59:37.095414 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 12:59:45.474590 140027215431488 spec.py:349] Evaluating on the test split.
I0130 12:59:48.316050 140027215431488 submission_runner.py:408] Time since start: 62882.94s, 	Step: 179114, 	{'train/accuracy': 0.9618940949440002, 'train/loss': 0.14577092230319977, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.0542857646942139, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.8273247480392456, 'test/num_examples': 10000, 'score': 60729.021720170975, 'total_duration': 62882.94174575806, 'accumulated_submission_time': 60729.021720170975, 'accumulated_eval_time': 2141.0598311424255, 'accumulated_logging_time': 6.489727973937988}
I0130 12:59:48.370618 139865123444480 logging_writer.py:48] [179114] accumulated_eval_time=2141.059831, accumulated_logging_time=6.489728, accumulated_submission_time=60729.021720, global_step=179114, preemption_count=0, score=60729.021720, test/accuracy=0.633100, test/loss=1.827325, test/num_examples=10000, total_duration=62882.941746, train/accuracy=0.961894, train/loss=0.145771, validation/accuracy=0.754640, validation/loss=1.054286, validation/num_examples=50000
I0130 13:00:17.779854 139865131837184 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.74094820022583, loss=0.5705928802490234
I0130 13:00:51.650627 139865123444480 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.573966979980469, loss=0.6661837697029114
I0130 13:01:25.513784 139865131837184 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.339869976043701, loss=0.568077802658081
I0130 13:01:59.396365 139865123444480 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.463925838470459, loss=0.6365370154380798
I0130 13:02:33.294836 139865131837184 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.264066219329834, loss=0.6447027921676636
I0130 13:03:07.160540 139865123444480 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.0317158699035645, loss=0.5894790887832642
I0130 13:03:41.038668 139865131837184 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.204725742340088, loss=0.6212873458862305
I0130 13:04:14.926284 139865123444480 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.772824287414551, loss=0.5961810946464539
I0130 13:04:48.822322 139865131837184 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.18929386138916, loss=0.5605238080024719
I0130 13:05:22.762409 139865123444480 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.413083076477051, loss=0.641008198261261
I0130 13:05:56.647716 139865131837184 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.36527681350708, loss=0.5808233618736267
I0130 13:06:30.550961 139865123444480 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.562582015991211, loss=0.6169248819351196
I0130 13:07:04.437814 139865131837184 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.24838399887085, loss=0.6438649892807007
I0130 13:07:38.307393 139865123444480 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.270569324493408, loss=0.6356393098831177
I0130 13:08:12.195806 139865131837184 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.340384006500244, loss=0.5510176420211792
I0130 13:08:18.433318 140027215431488 spec.py:321] Evaluating on the training split.
I0130 13:08:24.786480 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 13:08:33.309136 140027215431488 spec.py:349] Evaluating on the test split.
I0130 13:08:36.288030 140027215431488 submission_runner.py:408] Time since start: 63410.91s, 	Step: 180620, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14520682394504547, 'validation/accuracy': 0.7542200088500977, 'validation/loss': 1.0534926652908325, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.829865574836731, 'test/num_examples': 10000, 'score': 61239.021084070206, 'total_duration': 63410.9137442112, 'accumulated_submission_time': 61239.021084070206, 'accumulated_eval_time': 2158.9144999980927, 'accumulated_logging_time': 6.554764747619629}
I0130 13:08:36.330116 139865240893184 logging_writer.py:48] [180620] accumulated_eval_time=2158.914500, accumulated_logging_time=6.554765, accumulated_submission_time=61239.021084, global_step=180620, preemption_count=0, score=61239.021084, test/accuracy=0.630400, test/loss=1.829866, test/num_examples=10000, total_duration=63410.913744, train/accuracy=0.960798, train/loss=0.145207, validation/accuracy=0.754220, validation/loss=1.053493, validation/num_examples=50000
I0130 13:09:03.783798 139865760950016 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.763286590576172, loss=0.6533339023590088
I0130 13:09:37.671200 139865240893184 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.476523399353027, loss=0.6588698625564575
I0130 13:10:11.558100 139865760950016 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.163753509521484, loss=0.5701147317886353
I0130 13:10:45.488066 139865240893184 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.528071403503418, loss=0.5982453227043152
I0130 13:11:19.399016 139865760950016 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.510805606842041, loss=0.6753511428833008
I0130 13:11:53.282870 139865240893184 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.227744102478027, loss=0.6147080659866333
I0130 13:12:27.242396 139865760950016 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.40601921081543, loss=0.6112076640129089
I0130 13:13:01.105420 139865240893184 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.713854789733887, loss=0.6374678611755371
I0130 13:13:35.021832 139865760950016 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.028846263885498, loss=0.5866422653198242
I0130 13:14:08.894540 139865240893184 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.58737850189209, loss=0.6247180700302124
I0130 13:14:42.778000 139865760950016 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.366111755371094, loss=0.6231191158294678
I0130 13:15:16.701980 139865240893184 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.908829212188721, loss=0.7017720937728882
I0130 13:15:50.615801 139865760950016 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.80007266998291, loss=0.6904023885726929
I0130 13:16:24.504053 139865240893184 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.571173667907715, loss=0.624592125415802
I0130 13:16:58.387122 139865760950016 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.365740776062012, loss=0.629606306552887
I0130 13:17:06.348778 140027215431488 spec.py:321] Evaluating on the training split.
I0130 13:17:12.692523 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 13:17:21.476426 140027215431488 spec.py:349] Evaluating on the test split.
I0130 13:17:24.152538 140027215431488 submission_runner.py:408] Time since start: 63938.78s, 	Step: 182125, 	{'train/accuracy': 0.9617745280265808, 'train/loss': 0.1454741656780243, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0536754131317139, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.830521583557129, 'test/num_examples': 10000, 'score': 61748.976459264755, 'total_duration': 63938.778237342834, 'accumulated_submission_time': 61748.976459264755, 'accumulated_eval_time': 2176.718202829361, 'accumulated_logging_time': 6.606281042098999}
I0130 13:17:24.206897 139865106659072 logging_writer.py:48] [182125] accumulated_eval_time=2176.718203, accumulated_logging_time=6.606281, accumulated_submission_time=61748.976459, global_step=182125, preemption_count=0, score=61748.976459, test/accuracy=0.633100, test/loss=1.830522, test/num_examples=10000, total_duration=63938.778237, train/accuracy=0.961775, train/loss=0.145474, validation/accuracy=0.754740, validation/loss=1.053675, validation/num_examples=50000
I0130 13:17:49.955365 139865123444480 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.392650127410889, loss=0.6675838828086853
I0130 13:18:23.836397 139865106659072 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.680056095123291, loss=0.6429657936096191
I0130 13:18:57.808999 139865123444480 logging_writer.py:48] [182400] global_step=182400, grad_norm=5.682377338409424, loss=0.6983717679977417
I0130 13:19:31.688939 139865106659072 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.920973777770996, loss=0.6424816846847534
I0130 13:20:05.587116 139865123444480 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.125351428985596, loss=0.6399270296096802
I0130 13:20:39.504178 139865106659072 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.292835712432861, loss=0.608960747718811
I0130 13:21:13.376622 139865123444480 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.984213352203369, loss=0.6306653022766113
I0130 13:21:47.306988 139865106659072 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.323523044586182, loss=0.6214144825935364
I0130 13:22:21.180526 139865123444480 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.95153284072876, loss=0.6525999903678894
I0130 13:22:55.116641 139865106659072 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.228034973144531, loss=0.6641152501106262
I0130 13:23:28.990236 139865123444480 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.465935707092285, loss=0.5926960110664368
I0130 13:24:02.914476 139865106659072 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.087120056152344, loss=0.7037517428398132
I0130 13:24:36.816119 139865123444480 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.531076908111572, loss=0.6512959003448486
I0130 13:25:10.785646 139865106659072 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.441320419311523, loss=0.6352229714393616
I0130 13:25:44.633570 139865123444480 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.6349921226501465, loss=0.7126469612121582
I0130 13:25:54.275097 140027215431488 spec.py:321] Evaluating on the training split.
I0130 13:26:00.723917 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 13:26:09.579533 140027215431488 spec.py:349] Evaluating on the test split.
I0130 13:26:12.227092 140027215431488 submission_runner.py:408] Time since start: 64466.85s, 	Step: 183630, 	{'train/accuracy': 0.9620137214660645, 'train/loss': 0.14126259088516235, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0554081201553345, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.831824541091919, 'test/num_examples': 10000, 'score': 62258.98048186302, 'total_duration': 64466.852714538574, 'accumulated_submission_time': 62258.98048186302, 'accumulated_eval_time': 2194.670075416565, 'accumulated_logging_time': 6.670706748962402}
I0130 13:26:12.276920 139865232500480 logging_writer.py:48] [183630] accumulated_eval_time=2194.670075, accumulated_logging_time=6.670707, accumulated_submission_time=62258.980482, global_step=183630, preemption_count=0, score=62258.980482, test/accuracy=0.630600, test/loss=1.831825, test/num_examples=10000, total_duration=64466.852715, train/accuracy=0.962014, train/loss=0.141263, validation/accuracy=0.754860, validation/loss=1.055408, validation/num_examples=50000
I0130 13:26:36.321804 139865240893184 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.236907958984375, loss=0.5400254130363464
I0130 13:27:10.193438 139865232500480 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.557526588439941, loss=0.6100406050682068
I0130 13:27:44.060405 139865240893184 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.250188827514648, loss=0.668776273727417
I0130 13:28:17.917938 139865232500480 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.190150737762451, loss=0.6625432968139648
I0130 13:28:51.812788 139865240893184 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.900625705718994, loss=0.6531587243080139
I0130 13:29:25.651324 139865232500480 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.207749843597412, loss=0.647942304611206
I0130 13:29:59.547207 139865240893184 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.2544732093811035, loss=0.6222254037857056
I0130 13:30:33.431187 139865232500480 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.243406772613525, loss=0.6510787010192871
I0130 13:31:07.342722 139865240893184 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.692767143249512, loss=0.678874135017395
I0130 13:31:41.231924 139865232500480 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.211574077606201, loss=0.598483681678772
I0130 13:32:15.188052 139865240893184 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.609542369842529, loss=0.6228283047676086
I0130 13:32:49.056956 139865232500480 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.4074201583862305, loss=0.5850969552993774
I0130 13:33:22.958859 139865240893184 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.624539375305176, loss=0.6402795314788818
I0130 13:33:56.842524 139865232500480 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.323085308074951, loss=0.6283890604972839
I0130 13:34:30.745724 139865240893184 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.662482738494873, loss=0.6481297016143799
I0130 13:34:42.389313 140027215431488 spec.py:321] Evaluating on the training split.
I0130 13:34:48.779260 140027215431488 spec.py:333] Evaluating on the validation split.
I0130 13:34:57.598564 140027215431488 spec.py:349] Evaluating on the test split.
I0130 13:35:00.165685 140027215431488 submission_runner.py:408] Time since start: 64994.79s, 	Step: 185136, 	{'train/accuracy': 0.9618940949440002, 'train/loss': 0.14330562949180603, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0536677837371826, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8308967351913452, 'test/num_examples': 10000, 'score': 62769.02976322174, 'total_duration': 64994.79127025604, 'accumulated_submission_time': 62769.02976322174, 'accumulated_eval_time': 2212.4462909698486, 'accumulated_logging_time': 6.730313539505005}
I0130 13:35:00.216615 139865106659072 logging_writer.py:48] [185136] accumulated_eval_time=2212.446291, accumulated_logging_time=6.730314, accumulated_submission_time=62769.029763, global_step=185136, preemption_count=0, score=62769.029763, test/accuracy=0.632000, test/loss=1.830897, test/num_examples=10000, total_duration=64994.791270, train/accuracy=0.961894, train/loss=0.143306, validation/accuracy=0.754860, validation/loss=1.053668, validation/num_examples=50000
I0130 13:35:22.236222 139865115051776 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.6287946701049805, loss=0.628445029258728
I0130 13:35:56.073513 139865106659072 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.400024890899658, loss=0.61362624168396
I0130 13:36:29.945787 139865115051776 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.216524124145508, loss=0.5957145094871521
I0130 13:37:03.838970 139865106659072 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.317546844482422, loss=0.6017560362815857
I0130 13:37:37.678234 139865115051776 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.382364749908447, loss=0.6747405529022217
I0130 13:38:11.583088 139865106659072 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.231921672821045, loss=0.6373645663261414
I0130 13:38:45.568670 139865115051776 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.4148101806640625, loss=0.6047026515007019
I0130 13:38:59.291433 139865106659072 logging_writer.py:48] [185842] global_step=185842, preemption_count=0, score=63008.036757
I0130 13:38:59.757640 140027215431488 checkpoints.py:490] Saving checkpoint at step: 185842
I0130 13:39:00.854024 140027215431488 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5/checkpoint_185842
I0130 13:39:00.878414 140027215431488 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_0/imagenet_resnet_jax/trial_5/checkpoint_185842.
I0130 13:39:01.620323 140027215431488 submission_runner.py:583] Tuning trial 5/5
I0130 13:39:01.620537 140027215431488 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0130 13:39:01.631201 140027215431488 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005978954141028225, 'train/loss': 6.911577224731445, 'validation/accuracy': 0.0006799999973736703, 'validation/loss': 6.912051200866699, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9117279052734375, 'test/num_examples': 10000, 'score': 33.085120677948, 'total_duration': 50.62336611747742, 'accumulated_submission_time': 33.085120677948, 'accumulated_eval_time': 17.53779411315918, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1499, {'train/accuracy': 0.16603554785251617, 'train/loss': 4.276123523712158, 'validation/accuracy': 0.15143999457359314, 'validation/loss': 4.407378196716309, 'validation/num_examples': 50000, 'test/accuracy': 0.1145000085234642, 'test/loss': 4.87145471572876, 'test/num_examples': 10000, 'score': 543.0786073207855, 'total_duration': 578.6318206787109, 'accumulated_submission_time': 543.0786073207855, 'accumulated_eval_time': 35.47820210456848, 'accumulated_logging_time': 0.02154088020324707, 'global_step': 1499, 'preemption_count': 0}), (2998, {'train/accuracy': 0.3397241532802582, 'train/loss': 3.0705549716949463, 'validation/accuracy': 0.31525999307632446, 'validation/loss': 3.223424196243286, 'validation/num_examples': 50000, 'test/accuracy': 0.2322000116109848, 'test/loss': 3.897366523742676, 'test/num_examples': 10000, 'score': 1053.2543814182281, 'total_duration': 1106.8110687732697, 'accumulated_submission_time': 1053.2543814182281, 'accumulated_eval_time': 53.395957469940186, 'accumulated_logging_time': 0.0544428825378418, 'global_step': 2998, 'preemption_count': 0}), (4498, {'train/accuracy': 0.4709024131298065, 'train/loss': 2.321305751800537, 'validation/accuracy': 0.4215799868106842, 'validation/loss': 2.6061043739318848, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.398090362548828, 'test/num_examples': 10000, 'score': 1563.3771555423737, 'total_duration': 1634.8487372398376, 'accumulated_submission_time': 1563.3771555423737, 'accumulated_eval_time': 71.222341299057, 'accumulated_logging_time': 0.0895986557006836, 'global_step': 4498, 'preemption_count': 0}), (6000, {'train/accuracy': 0.5570989847183228, 'train/loss': 1.8586288690567017, 'validation/accuracy': 0.4995799958705902, 'validation/loss': 2.171689748764038, 'validation/num_examples': 50000, 'test/accuracy': 0.3790000081062317, 'test/loss': 2.932342052459717, 'test/num_examples': 10000, 'score': 2073.5986137390137, 'total_duration': 2162.8251535892487, 'accumulated_submission_time': 2073.5986137390137, 'accumulated_eval_time': 88.89337491989136, 'accumulated_logging_time': 0.12088155746459961, 'global_step': 6000, 'preemption_count': 0}), (7503, {'train/accuracy': 0.5871731638908386, 'train/loss': 1.7035075426101685, 'validation/accuracy': 0.5399199724197388, 'validation/loss': 1.9825940132141113, 'validation/num_examples': 50000, 'test/accuracy': 0.4199000298976898, 'test/loss': 2.727141857147217, 'test/num_examples': 10000, 'score': 2583.842520713806, 'total_duration': 2690.7365441322327, 'accumulated_submission_time': 2583.842520713806, 'accumulated_eval_time': 106.47591543197632, 'accumulated_logging_time': 0.15299773216247559, 'global_step': 7503, 'preemption_count': 0}), (9006, {'train/accuracy': 0.6002869606018066, 'train/loss': 1.646314024925232, 'validation/accuracy': 0.550819993019104, 'validation/loss': 1.9125394821166992, 'validation/num_examples': 50000, 'test/accuracy': 0.42350003123283386, 'test/loss': 2.7236905097961426, 'test/num_examples': 10000, 'score': 3093.868242740631, 'total_duration': 3218.64040517807, 'accumulated_submission_time': 3093.868242740631, 'accumulated_eval_time': 124.2677731513977, 'accumulated_logging_time': 0.18589282035827637, 'global_step': 9006, 'preemption_count': 0}), (10510, {'train/accuracy': 0.629324734210968, 'train/loss': 1.4985932111740112, 'validation/accuracy': 0.5819999575614929, 'validation/loss': 1.7651735544204712, 'validation/num_examples': 50000, 'test/accuracy': 0.4556000232696533, 'test/loss': 2.51633882522583, 'test/num_examples': 10000, 'score': 3603.961983203888, 'total_duration': 3746.58158493042, 'accumulated_submission_time': 3603.961983203888, 'accumulated_eval_time': 142.03222179412842, 'accumulated_logging_time': 0.21620941162109375, 'global_step': 10510, 'preemption_count': 0}), (12014, {'train/accuracy': 0.629304826259613, 'train/loss': 1.5169682502746582, 'validation/accuracy': 0.5853399634361267, 'validation/loss': 1.7463421821594238, 'validation/num_examples': 50000, 'test/accuracy': 0.46230003237724304, 'test/loss': 2.5003979206085205, 'test/num_examples': 10000, 'score': 4113.984125375748, 'total_duration': 4274.320744037628, 'accumulated_submission_time': 4113.984125375748, 'accumulated_eval_time': 159.66547989845276, 'accumulated_logging_time': 0.24596118927001953, 'global_step': 12014, 'preemption_count': 0}), (13517, {'train/accuracy': 0.6426379084587097, 'train/loss': 1.466149091720581, 'validation/accuracy': 0.589959979057312, 'validation/loss': 1.7425481081008911, 'validation/num_examples': 50000, 'test/accuracy': 0.46140003204345703, 'test/loss': 2.5226640701293945, 'test/num_examples': 10000, 'score': 4623.965626001358, 'total_duration': 4802.236160516739, 'accumulated_submission_time': 4623.965626001358, 'accumulated_eval_time': 177.51647090911865, 'accumulated_logging_time': 0.2762629985809326, 'global_step': 13517, 'preemption_count': 0}), (15021, {'train/accuracy': 0.6707589030265808, 'train/loss': 1.304468035697937, 'validation/accuracy': 0.5949400067329407, 'validation/loss': 1.69130539894104, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.4317922592163086, 'test/num_examples': 10000, 'score': 5134.007486343384, 'total_duration': 5330.208964586258, 'accumulated_submission_time': 5134.007486343384, 'accumulated_eval_time': 195.36043906211853, 'accumulated_logging_time': 0.3092031478881836, 'global_step': 15021, 'preemption_count': 0}), (16526, {'train/accuracy': 0.6666334271430969, 'train/loss': 1.3141627311706543, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.657135009765625, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.4198639392852783, 'test/num_examples': 10000, 'score': 5644.199923276901, 'total_duration': 5858.348840475082, 'accumulated_submission_time': 5644.199923276901, 'accumulated_eval_time': 213.22563099861145, 'accumulated_logging_time': 0.33670783042907715, 'global_step': 16526, 'preemption_count': 0}), (18030, {'train/accuracy': 0.6596978306770325, 'train/loss': 1.3637175559997559, 'validation/accuracy': 0.6011999845504761, 'validation/loss': 1.6728837490081787, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.3898165225982666, 'test/num_examples': 10000, 'score': 6154.205993413925, 'total_duration': 6386.336737394333, 'accumulated_submission_time': 6154.205993413925, 'accumulated_eval_time': 231.12113857269287, 'accumulated_logging_time': 0.36826539039611816, 'global_step': 18030, 'preemption_count': 0}), (19534, {'train/accuracy': 0.6613320708274841, 'train/loss': 1.3531110286712646, 'validation/accuracy': 0.6044999957084656, 'validation/loss': 1.6473594903945923, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.365743398666382, 'test/num_examples': 10000, 'score': 6664.1773364543915, 'total_duration': 6914.852823257446, 'accumulated_submission_time': 6664.1773364543915, 'accumulated_eval_time': 249.5828001499176, 'accumulated_logging_time': 0.398468017578125, 'global_step': 19534, 'preemption_count': 0}), (21038, {'train/accuracy': 0.6508888602256775, 'train/loss': 1.4064011573791504, 'validation/accuracy': 0.5990999937057495, 'validation/loss': 1.6903232336044312, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.4506237506866455, 'test/num_examples': 10000, 'score': 7174.233859062195, 'total_duration': 7442.757306337357, 'accumulated_submission_time': 7174.233859062195, 'accumulated_eval_time': 267.3385720252991, 'accumulated_logging_time': 0.4360337257385254, 'global_step': 21038, 'preemption_count': 0}), (22542, {'train/accuracy': 0.6646006107330322, 'train/loss': 1.3387823104858398, 'validation/accuracy': 0.6141799688339233, 'validation/loss': 1.617658257484436, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.391890287399292, 'test/num_examples': 10000, 'score': 7684.207465171814, 'total_duration': 7970.789026260376, 'accumulated_submission_time': 7684.207465171814, 'accumulated_eval_time': 285.31112122535706, 'accumulated_logging_time': 0.46795201301574707, 'global_step': 22542, 'preemption_count': 0}), (24047, {'train/accuracy': 0.6842713356018066, 'train/loss': 1.2391905784606934, 'validation/accuracy': 0.6031399965286255, 'validation/loss': 1.66865074634552, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.405961513519287, 'test/num_examples': 10000, 'score': 8194.420420408249, 'total_duration': 8498.789740085602, 'accumulated_submission_time': 8194.420420408249, 'accumulated_eval_time': 303.01013803482056, 'accumulated_logging_time': 0.5014698505401611, 'global_step': 24047, 'preemption_count': 0}), (25552, {'train/accuracy': 0.6896723508834839, 'train/loss': 1.2271901369094849, 'validation/accuracy': 0.6210399866104126, 'validation/loss': 1.5833314657211304, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.319498300552368, 'test/num_examples': 10000, 'score': 8704.557604551315, 'total_duration': 9026.98484826088, 'accumulated_submission_time': 8704.557604551315, 'accumulated_eval_time': 320.98294949531555, 'accumulated_logging_time': 0.5339152812957764, 'global_step': 25552, 'preemption_count': 0}), (27056, {'train/accuracy': 0.683015763759613, 'train/loss': 1.2480367422103882, 'validation/accuracy': 0.6202600002288818, 'validation/loss': 1.574621319770813, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.30348801612854, 'test/num_examples': 10000, 'score': 9214.546523332596, 'total_duration': 9554.83399772644, 'accumulated_submission_time': 9214.546523332596, 'accumulated_eval_time': 338.75679183006287, 'accumulated_logging_time': 0.567237377166748, 'global_step': 27056, 'preemption_count': 0}), (28561, {'train/accuracy': 0.6730906963348389, 'train/loss': 1.2945865392684937, 'validation/accuracy': 0.6128399968147278, 'validation/loss': 1.6189769506454468, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.4035277366638184, 'test/num_examples': 10000, 'score': 9724.683393001556, 'total_duration': 10082.776176929474, 'accumulated_submission_time': 9724.683393001556, 'accumulated_eval_time': 356.4754137992859, 'accumulated_logging_time': 0.6004829406738281, 'global_step': 28561, 'preemption_count': 0}), (30066, {'train/accuracy': 0.6851283311843872, 'train/loss': 1.2363115549087524, 'validation/accuracy': 0.6257599592208862, 'validation/loss': 1.53518807888031, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.274780035018921, 'test/num_examples': 10000, 'score': 10234.78894495964, 'total_duration': 10610.840461969376, 'accumulated_submission_time': 10234.78894495964, 'accumulated_eval_time': 374.3439898490906, 'accumulated_logging_time': 0.6367506980895996, 'global_step': 30066, 'preemption_count': 0}), (31570, {'train/accuracy': 0.686922013759613, 'train/loss': 1.2258983850479126, 'validation/accuracy': 0.6343799829483032, 'validation/loss': 1.5134379863739014, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.246699333190918, 'test/num_examples': 10000, 'score': 10744.854972839355, 'total_duration': 11138.650857448578, 'accumulated_submission_time': 10744.854972839355, 'accumulated_eval_time': 391.9957549571991, 'accumulated_logging_time': 0.6751341819763184, 'global_step': 31570, 'preemption_count': 0}), (33074, {'train/accuracy': 0.7069116830825806, 'train/loss': 1.1511811017990112, 'validation/accuracy': 0.6170399785041809, 'validation/loss': 1.5918534994125366, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.316349744796753, 'test/num_examples': 10000, 'score': 11254.89800453186, 'total_duration': 11666.672231435776, 'accumulated_submission_time': 11254.89800453186, 'accumulated_eval_time': 409.8847246170044, 'accumulated_logging_time': 0.7104458808898926, 'global_step': 33074, 'preemption_count': 0}), (34579, {'train/accuracy': 0.6890744566917419, 'train/loss': 1.2211211919784546, 'validation/accuracy': 0.6181399822235107, 'validation/loss': 1.5859718322753906, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.303445816040039, 'test/num_examples': 10000, 'score': 11765.143156051636, 'total_duration': 12194.597846269608, 'accumulated_submission_time': 11765.143156051636, 'accumulated_eval_time': 427.47814416885376, 'accumulated_logging_time': 0.7436776161193848, 'global_step': 34579, 'preemption_count': 0}), (36083, {'train/accuracy': 0.6932995915412903, 'train/loss': 1.1908233165740967, 'validation/accuracy': 0.6310399770736694, 'validation/loss': 1.5338072776794434, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.258314847946167, 'test/num_examples': 10000, 'score': 12275.155267238617, 'total_duration': 12722.646920681, 'accumulated_submission_time': 12275.155267238617, 'accumulated_eval_time': 445.4265134334564, 'accumulated_logging_time': 0.7787868976593018, 'global_step': 36083, 'preemption_count': 0}), (37588, {'train/accuracy': 0.6861447691917419, 'train/loss': 1.233417272567749, 'validation/accuracy': 0.6261999607086182, 'validation/loss': 1.5477724075317383, 'validation/num_examples': 50000, 'test/accuracy': 0.49640002846717834, 'test/loss': 2.281170606613159, 'test/num_examples': 10000, 'score': 12785.271156549454, 'total_duration': 13251.140579938889, 'accumulated_submission_time': 12785.271156549454, 'accumulated_eval_time': 463.7156083583832, 'accumulated_logging_time': 0.8150899410247803, 'global_step': 37588, 'preemption_count': 0}), (39093, {'train/accuracy': 0.7010124325752258, 'train/loss': 1.1792408227920532, 'validation/accuracy': 0.64028000831604, 'validation/loss': 1.4864907264709473, 'validation/num_examples': 50000, 'test/accuracy': 0.5124000310897827, 'test/loss': 2.243726968765259, 'test/num_examples': 10000, 'score': 13295.242151021957, 'total_duration': 13778.91663146019, 'accumulated_submission_time': 13295.242151021957, 'accumulated_eval_time': 481.4382588863373, 'accumulated_logging_time': 0.8444697856903076, 'global_step': 39093, 'preemption_count': 0}), (40598, {'train/accuracy': 0.6853276491165161, 'train/loss': 1.2317752838134766, 'validation/accuracy': 0.6255800127983093, 'validation/loss': 1.5425355434417725, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.290236473083496, 'test/num_examples': 10000, 'score': 13805.37430691719, 'total_duration': 14307.025272130966, 'accumulated_submission_time': 13805.37430691719, 'accumulated_eval_time': 499.3242871761322, 'accumulated_logging_time': 0.8821501731872559, 'global_step': 40598, 'preemption_count': 0}), (42104, {'train/accuracy': 0.7300103306770325, 'train/loss': 1.0265074968338013, 'validation/accuracy': 0.6344799995422363, 'validation/loss': 1.5053465366363525, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.22822904586792, 'test/num_examples': 10000, 'score': 14315.556084632874, 'total_duration': 14835.124782562256, 'accumulated_submission_time': 14315.556084632874, 'accumulated_eval_time': 517.1469714641571, 'accumulated_logging_time': 0.9237699508666992, 'global_step': 42104, 'preemption_count': 0}), (43609, {'train/accuracy': 0.7144451141357422, 'train/loss': 1.1129549741744995, 'validation/accuracy': 0.638700008392334, 'validation/loss': 1.4881618022918701, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.2399520874023438, 'test/num_examples': 10000, 'score': 14825.481731653214, 'total_duration': 15362.819847106934, 'accumulated_submission_time': 14825.481731653214, 'accumulated_eval_time': 534.8262553215027, 'accumulated_logging_time': 0.9605207443237305, 'global_step': 43609, 'preemption_count': 0}), (45096, {'train/accuracy': 0.7016900181770325, 'train/loss': 1.154717206954956, 'validation/accuracy': 0.6298800110816956, 'validation/loss': 1.5299546718597412, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.263049840927124, 'test/num_examples': 10000, 'score': 15335.629780054092, 'total_duration': 15890.73356628418, 'accumulated_submission_time': 15335.629780054092, 'accumulated_eval_time': 552.4992291927338, 'accumulated_logging_time': 0.9993839263916016, 'global_step': 45096, 'preemption_count': 0}), (46598, {'train/accuracy': 0.7108976244926453, 'train/loss': 1.125377893447876, 'validation/accuracy': 0.6459800004959106, 'validation/loss': 1.4500062465667725, 'validation/num_examples': 50000, 'test/accuracy': 0.5160000324249268, 'test/loss': 2.1649107933044434, 'test/num_examples': 10000, 'score': 15844.633338212967, 'total_duration': 16418.882928848267, 'accumulated_submission_time': 15844.633338212967, 'accumulated_eval_time': 570.4190890789032, 'accumulated_logging_time': 2.1719322204589844, 'global_step': 46598, 'preemption_count': 0}), (48103, {'train/accuracy': 0.6979033946990967, 'train/loss': 1.1757428646087646, 'validation/accuracy': 0.6360799670219421, 'validation/loss': 1.504656195640564, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.2049496173858643, 'test/num_examples': 10000, 'score': 16354.717643976212, 'total_duration': 16946.749007940292, 'accumulated_submission_time': 16354.717643976212, 'accumulated_eval_time': 588.1098670959473, 'accumulated_logging_time': 2.20971941947937, 'global_step': 48103, 'preemption_count': 0}), (49608, {'train/accuracy': 0.7112563848495483, 'train/loss': 1.128156304359436, 'validation/accuracy': 0.6507200002670288, 'validation/loss': 1.4424986839294434, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.1950442790985107, 'test/num_examples': 10000, 'score': 16864.65782546997, 'total_duration': 17474.416180849075, 'accumulated_submission_time': 16864.65782546997, 'accumulated_eval_time': 605.7434694766998, 'accumulated_logging_time': 2.248587131500244, 'global_step': 49608, 'preemption_count': 0}), (51114, {'train/accuracy': 0.7567960619926453, 'train/loss': 0.9291717410087585, 'validation/accuracy': 0.64301997423172, 'validation/loss': 1.4702551364898682, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.2189431190490723, 'test/num_examples': 10000, 'score': 17374.636246919632, 'total_duration': 18002.36827158928, 'accumulated_submission_time': 17374.636246919632, 'accumulated_eval_time': 623.6246781349182, 'accumulated_logging_time': 2.2868897914886475, 'global_step': 51114, 'preemption_count': 0}), (52620, {'train/accuracy': 0.7090441584587097, 'train/loss': 1.130107045173645, 'validation/accuracy': 0.6304599642753601, 'validation/loss': 1.5274958610534668, 'validation/num_examples': 50000, 'test/accuracy': 0.49820002913475037, 'test/loss': 2.276724100112915, 'test/num_examples': 10000, 'score': 17884.815304279327, 'total_duration': 18531.05840587616, 'accumulated_submission_time': 17884.815304279327, 'accumulated_eval_time': 642.0388751029968, 'accumulated_logging_time': 2.3303985595703125, 'global_step': 52620, 'preemption_count': 0}), (54126, {'train/accuracy': 0.7151626348495483, 'train/loss': 1.1085048913955688, 'validation/accuracy': 0.640720009803772, 'validation/loss': 1.4724503755569458, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.202472686767578, 'test/num_examples': 10000, 'score': 18394.996651649475, 'total_duration': 19058.955248355865, 'accumulated_submission_time': 18394.996651649475, 'accumulated_eval_time': 659.6608362197876, 'accumulated_logging_time': 2.370500087738037, 'global_step': 54126, 'preemption_count': 0}), (55632, {'train/accuracy': 0.7169164419174194, 'train/loss': 1.0998347997665405, 'validation/accuracy': 0.6484999656677246, 'validation/loss': 1.4559392929077148, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.200547933578491, 'test/num_examples': 10000, 'score': 18905.095246076584, 'total_duration': 19586.952585458755, 'accumulated_submission_time': 18905.095246076584, 'accumulated_eval_time': 677.4676706790924, 'accumulated_logging_time': 2.4084765911102295, 'global_step': 55632, 'preemption_count': 0}), (57138, {'train/accuracy': 0.71000075340271, 'train/loss': 1.1082898378372192, 'validation/accuracy': 0.6432799696922302, 'validation/loss': 1.463404655456543, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.2013065814971924, 'test/num_examples': 10000, 'score': 19415.004539966583, 'total_duration': 20115.204418182373, 'accumulated_submission_time': 19415.004539966583, 'accumulated_eval_time': 695.7189378738403, 'accumulated_logging_time': 2.4459259510040283, 'global_step': 57138, 'preemption_count': 0}), (58644, {'train/accuracy': 0.7024473547935486, 'train/loss': 1.1483705043792725, 'validation/accuracy': 0.6444000005722046, 'validation/loss': 1.4588139057159424, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.179934501647949, 'test/num_examples': 10000, 'score': 19925.25050020218, 'total_duration': 20643.348040819168, 'accumulated_submission_time': 19925.25050020218, 'accumulated_eval_time': 713.5251975059509, 'accumulated_logging_time': 2.4837570190429688, 'global_step': 58644, 'preemption_count': 0}), (60150, {'train/accuracy': 0.7673987150192261, 'train/loss': 0.8855634927749634, 'validation/accuracy': 0.6535999774932861, 'validation/loss': 1.4235299825668335, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.1435821056365967, 'test/num_examples': 10000, 'score': 20435.165104150772, 'total_duration': 21171.26289153099, 'accumulated_submission_time': 20435.165104150772, 'accumulated_eval_time': 731.4343252182007, 'accumulated_logging_time': 2.5222463607788086, 'global_step': 60150, 'preemption_count': 0}), (61655, {'train/accuracy': 0.7128706574440002, 'train/loss': 1.1097934246063232, 'validation/accuracy': 0.6341599822044373, 'validation/loss': 1.5179804563522339, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.302015781402588, 'test/num_examples': 10000, 'score': 20945.074239969254, 'total_duration': 21699.009701251984, 'accumulated_submission_time': 20945.074239969254, 'accumulated_eval_time': 749.1801223754883, 'accumulated_logging_time': 2.560462474822998, 'global_step': 61655, 'preemption_count': 0}), (63160, {'train/accuracy': 0.7302096486091614, 'train/loss': 1.038726806640625, 'validation/accuracy': 0.6582399606704712, 'validation/loss': 1.4035637378692627, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.1373679637908936, 'test/num_examples': 10000, 'score': 21454.97639608383, 'total_duration': 22226.67244052887, 'accumulated_submission_time': 21454.97639608383, 'accumulated_eval_time': 766.840833902359, 'accumulated_logging_time': 2.6059324741363525, 'global_step': 63160, 'preemption_count': 0}), (64666, {'train/accuracy': 0.7167769074440002, 'train/loss': 1.0958020687103271, 'validation/accuracy': 0.6484799981117249, 'validation/loss': 1.438090443611145, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.1825969219207764, 'test/num_examples': 10000, 'score': 21965.061421632767, 'total_duration': 22754.716769218445, 'accumulated_submission_time': 21965.061421632767, 'accumulated_eval_time': 784.704512834549, 'accumulated_logging_time': 2.6474947929382324, 'global_step': 64666, 'preemption_count': 0}), (66172, {'train/accuracy': 0.7169762253761292, 'train/loss': 1.0895507335662842, 'validation/accuracy': 0.6572399735450745, 'validation/loss': 1.4136441946029663, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.1981096267700195, 'test/num_examples': 10000, 'score': 22475.013607740402, 'total_duration': 23282.572645425797, 'accumulated_submission_time': 22475.013607740402, 'accumulated_eval_time': 802.5071756839752, 'accumulated_logging_time': 2.695244073867798, 'global_step': 66172, 'preemption_count': 0}), (67678, {'train/accuracy': 0.727937638759613, 'train/loss': 1.04031240940094, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.3774571418762207, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.1053407192230225, 'test/num_examples': 10000, 'score': 22985.089703559875, 'total_duration': 23811.222986221313, 'accumulated_submission_time': 22985.089703559875, 'accumulated_eval_time': 820.9895300865173, 'accumulated_logging_time': 2.7346694469451904, 'global_step': 67678, 'preemption_count': 0}), (69184, {'train/accuracy': 0.7716637253761292, 'train/loss': 0.8626237511634827, 'validation/accuracy': 0.661579966545105, 'validation/loss': 1.3844743967056274, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.0840163230895996, 'test/num_examples': 10000, 'score': 23495.214922189713, 'total_duration': 24338.97371149063, 'accumulated_submission_time': 23495.214922189713, 'accumulated_eval_time': 838.5256464481354, 'accumulated_logging_time': 2.7713065147399902, 'global_step': 69184, 'preemption_count': 0}), (70691, {'train/accuracy': 0.7581313848495483, 'train/loss': 0.9096571207046509, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.3560611009597778, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.067430257797241, 'test/num_examples': 10000, 'score': 24005.384941101074, 'total_duration': 24866.868101119995, 'accumulated_submission_time': 24005.384941101074, 'accumulated_eval_time': 856.152738571167, 'accumulated_logging_time': 2.8141791820526123, 'global_step': 70691, 'preemption_count': 0}), (72197, {'train/accuracy': 0.7457548975944519, 'train/loss': 0.974716067314148, 'validation/accuracy': 0.6640200018882751, 'validation/loss': 1.391406536102295, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.1257283687591553, 'test/num_examples': 10000, 'score': 24515.506544589996, 'total_duration': 25394.858213186264, 'accumulated_submission_time': 24515.506544589996, 'accumulated_eval_time': 873.9265124797821, 'accumulated_logging_time': 2.855938196182251, 'global_step': 72197, 'preemption_count': 0}), (73703, {'train/accuracy': 0.7359893321990967, 'train/loss': 1.0156553983688354, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.3957440853118896, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.1294972896575928, 'test/num_examples': 10000, 'score': 25025.546664714813, 'total_duration': 25922.5676074028, 'accumulated_submission_time': 25025.546664714813, 'accumulated_eval_time': 891.5010304450989, 'accumulated_logging_time': 2.8975298404693604, 'global_step': 73703, 'preemption_count': 0}), (75209, {'train/accuracy': 0.7361288070678711, 'train/loss': 1.0147454738616943, 'validation/accuracy': 0.6642199754714966, 'validation/loss': 1.3678507804870605, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.108184576034546, 'test/num_examples': 10000, 'score': 25535.526324272156, 'total_duration': 26450.878532886505, 'accumulated_submission_time': 25535.526324272156, 'accumulated_eval_time': 909.7378346920013, 'accumulated_logging_time': 2.939307928085327, 'global_step': 75209, 'preemption_count': 0}), (76715, {'train/accuracy': 0.725027859210968, 'train/loss': 1.0601333379745483, 'validation/accuracy': 0.6577199697494507, 'validation/loss': 1.3992598056793213, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.151803970336914, 'test/num_examples': 10000, 'score': 26045.5726313591, 'total_duration': 26978.564618349075, 'accumulated_submission_time': 26045.5726313591, 'accumulated_eval_time': 927.2890992164612, 'accumulated_logging_time': 2.9757421016693115, 'global_step': 76715, 'preemption_count': 0}), (78221, {'train/accuracy': 0.7487444281578064, 'train/loss': 0.9544544219970703, 'validation/accuracy': 0.6531999707221985, 'validation/loss': 1.4107705354690552, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.1556520462036133, 'test/num_examples': 10000, 'score': 26555.53935289383, 'total_duration': 27506.41523051262, 'accumulated_submission_time': 26555.53935289383, 'accumulated_eval_time': 945.0738813877106, 'accumulated_logging_time': 3.020139217376709, 'global_step': 78221, 'preemption_count': 0}), (79727, {'train/accuracy': 0.7374043464660645, 'train/loss': 0.9866397380828857, 'validation/accuracy': 0.6531199812889099, 'validation/loss': 1.437100887298584, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.229564905166626, 'test/num_examples': 10000, 'score': 27065.569350004196, 'total_duration': 28034.045956611633, 'accumulated_submission_time': 27065.569350004196, 'accumulated_eval_time': 962.5798609256744, 'accumulated_logging_time': 3.0614492893218994, 'global_step': 79727, 'preemption_count': 0}), (81233, {'train/accuracy': 0.7581512928009033, 'train/loss': 0.9172185659408569, 'validation/accuracy': 0.6740999817848206, 'validation/loss': 1.3255287408828735, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.054157257080078, 'test/num_examples': 10000, 'score': 27575.630492925644, 'total_duration': 28562.10580611229, 'accumulated_submission_time': 27575.630492925644, 'accumulated_eval_time': 980.4816539287567, 'accumulated_logging_time': 3.104302167892456, 'global_step': 81233, 'preemption_count': 0}), (82739, {'train/accuracy': 0.7515744566917419, 'train/loss': 0.9364935159683228, 'validation/accuracy': 0.6704399585723877, 'validation/loss': 1.3452812433242798, 'validation/num_examples': 50000, 'test/accuracy': 0.547700047492981, 'test/loss': 2.067952871322632, 'test/num_examples': 10000, 'score': 28085.67872595787, 'total_duration': 29090.06341791153, 'accumulated_submission_time': 28085.67872595787, 'accumulated_eval_time': 998.2937302589417, 'accumulated_logging_time': 3.1469409465789795, 'global_step': 82739, 'preemption_count': 0}), (84245, {'train/accuracy': 0.749043345451355, 'train/loss': 0.9529641270637512, 'validation/accuracy': 0.6751599907875061, 'validation/loss': 1.3240749835968018, 'validation/num_examples': 50000, 'test/accuracy': 0.544700026512146, 'test/loss': 2.050497055053711, 'test/num_examples': 10000, 'score': 28595.771988630295, 'total_duration': 29618.119492292404, 'accumulated_submission_time': 28595.771988630295, 'accumulated_eval_time': 1016.1643199920654, 'accumulated_logging_time': 3.1869609355926514, 'global_step': 84245, 'preemption_count': 0}), (85751, {'train/accuracy': 0.7525310516357422, 'train/loss': 0.9264184236526489, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.2922544479370117, 'validation/num_examples': 50000, 'test/accuracy': 0.5537000298500061, 'test/loss': 1.9823843240737915, 'test/num_examples': 10000, 'score': 29105.806889295578, 'total_duration': 30145.964017629623, 'accumulated_submission_time': 29105.806889295578, 'accumulated_eval_time': 1033.8766658306122, 'accumulated_logging_time': 3.23142409324646, 'global_step': 85751, 'preemption_count': 0}), (87257, {'train/accuracy': 0.7596858739852905, 'train/loss': 0.9148987531661987, 'validation/accuracy': 0.6706599593162537, 'validation/loss': 1.3406606912612915, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.032165765762329, 'test/num_examples': 10000, 'score': 29615.993060588837, 'total_duration': 30674.282242536545, 'accumulated_submission_time': 29615.993060588837, 'accumulated_eval_time': 1051.9092426300049, 'accumulated_logging_time': 3.2774195671081543, 'global_step': 87257, 'preemption_count': 0}), (88763, {'train/accuracy': 0.7681162357330322, 'train/loss': 0.8765470385551453, 'validation/accuracy': 0.6717999577522278, 'validation/loss': 1.342298150062561, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.063876152038574, 'test/num_examples': 10000, 'score': 30125.917417526245, 'total_duration': 31201.865349769592, 'accumulated_submission_time': 30125.917417526245, 'accumulated_eval_time': 1069.4649865627289, 'accumulated_logging_time': 3.327728748321533, 'global_step': 88763, 'preemption_count': 0}), (90269, {'train/accuracy': 0.7680763602256775, 'train/loss': 0.8671398162841797, 'validation/accuracy': 0.6808800101280212, 'validation/loss': 1.2917416095733643, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.005854368209839, 'test/num_examples': 10000, 'score': 30635.965329885483, 'total_duration': 31729.75548362732, 'accumulated_submission_time': 30635.965329885483, 'accumulated_eval_time': 1087.2066555023193, 'accumulated_logging_time': 3.3750007152557373, 'global_step': 90269, 'preemption_count': 0}), (91776, {'train/accuracy': 0.7579320669174194, 'train/loss': 0.906115710735321, 'validation/accuracy': 0.6772199869155884, 'validation/loss': 1.3106396198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.02983021736145, 'test/num_examples': 10000, 'score': 31146.04019165039, 'total_duration': 32257.85404109955, 'accumulated_submission_time': 31146.04019165039, 'accumulated_eval_time': 1105.1372406482697, 'accumulated_logging_time': 3.414245128631592, 'global_step': 91776, 'preemption_count': 0}), (93282, {'train/accuracy': 0.7669204473495483, 'train/loss': 0.8753052949905396, 'validation/accuracy': 0.6854999661445618, 'validation/loss': 1.2829898595809937, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9982883930206299, 'test/num_examples': 10000, 'score': 31656.18217587471, 'total_duration': 32786.21779823303, 'accumulated_submission_time': 31656.18217587471, 'accumulated_eval_time': 1123.2598929405212, 'accumulated_logging_time': 3.460043430328369, 'global_step': 93282, 'preemption_count': 0}), (94788, {'train/accuracy': 0.7631935477256775, 'train/loss': 0.8865559697151184, 'validation/accuracy': 0.6861400008201599, 'validation/loss': 1.2788174152374268, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 2.0052812099456787, 'test/num_examples': 10000, 'score': 32166.163598299026, 'total_duration': 33314.017624139786, 'accumulated_submission_time': 32166.163598299026, 'accumulated_eval_time': 1140.9816081523895, 'accumulated_logging_time': 3.503222703933716, 'global_step': 94788, 'preemption_count': 0}), (96292, {'train/accuracy': 0.7673788070678711, 'train/loss': 0.8581953048706055, 'validation/accuracy': 0.6811400055885315, 'validation/loss': 1.2850172519683838, 'validation/num_examples': 50000, 'test/accuracy': 0.5563000440597534, 'test/loss': 2.0307154655456543, 'test/num_examples': 10000, 'score': 32676.12209534645, 'total_duration': 33842.43831539154, 'accumulated_submission_time': 32676.12209534645, 'accumulated_eval_time': 1159.3427624702454, 'accumulated_logging_time': 3.55094575881958, 'global_step': 96292, 'preemption_count': 0}), (97798, {'train/accuracy': 0.7949816584587097, 'train/loss': 0.7670559883117676, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.256996750831604, 'validation/num_examples': 50000, 'test/accuracy': 0.5625, 'test/loss': 1.9770840406417847, 'test/num_examples': 10000, 'score': 33186.13390159607, 'total_duration': 34370.68486762047, 'accumulated_submission_time': 33186.13390159607, 'accumulated_eval_time': 1177.4779727458954, 'accumulated_logging_time': 3.5960216522216797, 'global_step': 97798, 'preemption_count': 0}), (99303, {'train/accuracy': 0.7777224183082581, 'train/loss': 0.8399938344955444, 'validation/accuracy': 0.6829800009727478, 'validation/loss': 1.2816671133041382, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 1.9916038513183594, 'test/num_examples': 10000, 'score': 33696.05191516876, 'total_duration': 34898.55397820473, 'accumulated_submission_time': 33696.05191516876, 'accumulated_eval_time': 1195.3270378112793, 'accumulated_logging_time': 3.6447410583496094, 'global_step': 99303, 'preemption_count': 0}), (100809, {'train/accuracy': 0.7700493931770325, 'train/loss': 0.8601894378662109, 'validation/accuracy': 0.6826599836349487, 'validation/loss': 1.296310305595398, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.0301966667175293, 'test/num_examples': 10000, 'score': 34206.17853355408, 'total_duration': 35426.718936920166, 'accumulated_submission_time': 34206.17853355408, 'accumulated_eval_time': 1213.2627115249634, 'accumulated_logging_time': 3.693167209625244, 'global_step': 100809, 'preemption_count': 0}), (102315, {'train/accuracy': 0.7760483026504517, 'train/loss': 0.8299661874771118, 'validation/accuracy': 0.6898399591445923, 'validation/loss': 1.2503280639648438, 'validation/num_examples': 50000, 'test/accuracy': 0.5603000521659851, 'test/loss': 1.9734432697296143, 'test/num_examples': 10000, 'score': 34716.23765563965, 'total_duration': 35954.78728866577, 'accumulated_submission_time': 34716.23765563965, 'accumulated_eval_time': 1231.1673793792725, 'accumulated_logging_time': 3.744328260421753, 'global_step': 102315, 'preemption_count': 0}), (103821, {'train/accuracy': 0.76761794090271, 'train/loss': 0.8790847659111023, 'validation/accuracy': 0.685699999332428, 'validation/loss': 1.2791662216186523, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.9737340211868286, 'test/num_examples': 10000, 'score': 35226.270773649216, 'total_duration': 36482.83643579483, 'accumulated_submission_time': 35226.270773649216, 'accumulated_eval_time': 1249.0791852474213, 'accumulated_logging_time': 3.7941339015960693, 'global_step': 103821, 'preemption_count': 0}), (105327, {'train/accuracy': 0.7800143361091614, 'train/loss': 0.810962438583374, 'validation/accuracy': 0.6937800049781799, 'validation/loss': 1.2509777545928955, 'validation/num_examples': 50000, 'test/accuracy': 0.5646000504493713, 'test/loss': 1.9978750944137573, 'test/num_examples': 10000, 'score': 35736.3866379261, 'total_duration': 37011.45383000374, 'accumulated_submission_time': 35736.3866379261, 'accumulated_eval_time': 1267.4798917770386, 'accumulated_logging_time': 3.8419394493103027, 'global_step': 105327, 'preemption_count': 0}), (106833, {'train/accuracy': 0.8098094463348389, 'train/loss': 0.6974734663963318, 'validation/accuracy': 0.6970799565315247, 'validation/loss': 1.2289150953292847, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.9629000425338745, 'test/num_examples': 10000, 'score': 36246.66552352905, 'total_duration': 37539.52548789978, 'accumulated_submission_time': 36246.66552352905, 'accumulated_eval_time': 1285.1757638454437, 'accumulated_logging_time': 3.8856961727142334, 'global_step': 106833, 'preemption_count': 0}), (108339, {'train/accuracy': 0.8022361397743225, 'train/loss': 0.7281384468078613, 'validation/accuracy': 0.6987000107765198, 'validation/loss': 1.2137079238891602, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9371442794799805, 'test/num_examples': 10000, 'score': 36756.7243309021, 'total_duration': 38067.54865336418, 'accumulated_submission_time': 36756.7243309021, 'accumulated_eval_time': 1303.0352365970612, 'accumulated_logging_time': 3.9351346492767334, 'global_step': 108339, 'preemption_count': 0}), (109845, {'train/accuracy': 0.795320451259613, 'train/loss': 0.7437325119972229, 'validation/accuracy': 0.6987400054931641, 'validation/loss': 1.229027509689331, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.9284789562225342, 'test/num_examples': 10000, 'score': 37266.74565792084, 'total_duration': 38595.21840763092, 'accumulated_submission_time': 37266.74565792084, 'accumulated_eval_time': 1320.5831859111786, 'accumulated_logging_time': 3.982501983642578, 'global_step': 109845, 'preemption_count': 0}), (111350, {'train/accuracy': 0.8005022406578064, 'train/loss': 0.7239665985107422, 'validation/accuracy': 0.7046999931335449, 'validation/loss': 1.1938434839248657, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8825327157974243, 'test/num_examples': 10000, 'score': 37776.84398150444, 'total_duration': 39122.9255297184, 'accumulated_submission_time': 37776.84398150444, 'accumulated_eval_time': 1338.08602809906, 'accumulated_logging_time': 4.032949924468994, 'global_step': 111350, 'preemption_count': 0}), (112857, {'train/accuracy': 0.7922711968421936, 'train/loss': 0.7695667147636414, 'validation/accuracy': 0.7007399797439575, 'validation/loss': 1.2252343893051147, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 1.942015290260315, 'test/num_examples': 10000, 'score': 38287.077071905136, 'total_duration': 39651.77258491516, 'accumulated_submission_time': 38287.077071905136, 'accumulated_eval_time': 1356.5964069366455, 'accumulated_logging_time': 4.082559108734131, 'global_step': 112857, 'preemption_count': 0}), (114363, {'train/accuracy': 0.797293484210968, 'train/loss': 0.7358038425445557, 'validation/accuracy': 0.7057999968528748, 'validation/loss': 1.2084553241729736, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 1.9387891292572021, 'test/num_examples': 10000, 'score': 38796.97853899002, 'total_duration': 40179.667202949524, 'accumulated_submission_time': 38796.97853899002, 'accumulated_eval_time': 1374.4870157241821, 'accumulated_logging_time': 4.132639169692993, 'global_step': 114363, 'preemption_count': 0}), (115869, {'train/accuracy': 0.8302773833274841, 'train/loss': 0.6240458488464355, 'validation/accuracy': 0.703719973564148, 'validation/loss': 1.2063405513763428, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.9542590379714966, 'test/num_examples': 10000, 'score': 39306.96019792557, 'total_duration': 40707.69045972824, 'accumulated_submission_time': 39306.96019792557, 'accumulated_eval_time': 1392.4278795719147, 'accumulated_logging_time': 4.181373596191406, 'global_step': 115869, 'preemption_count': 0}), (117375, {'train/accuracy': 0.8184191584587097, 'train/loss': 0.661994993686676, 'validation/accuracy': 0.702019989490509, 'validation/loss': 1.2063285112380981, 'validation/num_examples': 50000, 'test/accuracy': 0.5832000374794006, 'test/loss': 1.9158483743667603, 'test/num_examples': 10000, 'score': 39817.018862485886, 'total_duration': 41235.641859054565, 'accumulated_submission_time': 39817.018862485886, 'accumulated_eval_time': 1410.2165460586548, 'accumulated_logging_time': 4.232245206832886, 'global_step': 117375, 'preemption_count': 0}), (118881, {'train/accuracy': 0.8126594424247742, 'train/loss': 0.6735084652900696, 'validation/accuracy': 0.7055599689483643, 'validation/loss': 1.2065515518188477, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.93263578414917, 'test/num_examples': 10000, 'score': 40327.03925204277, 'total_duration': 41763.26990413666, 'accumulated_submission_time': 40327.03925204277, 'accumulated_eval_time': 1427.716940164566, 'accumulated_logging_time': 4.2863757610321045, 'global_step': 118881, 'preemption_count': 0}), (120387, {'train/accuracy': 0.8160474896430969, 'train/loss': 0.6632678508758545, 'validation/accuracy': 0.7124399542808533, 'validation/loss': 1.1698178052902222, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.8709394931793213, 'test/num_examples': 10000, 'score': 40837.051486730576, 'total_duration': 42291.873514175415, 'accumulated_submission_time': 40837.051486730576, 'accumulated_eval_time': 1446.2023582458496, 'accumulated_logging_time': 4.338989973068237, 'global_step': 120387, 'preemption_count': 0}), (121894, {'train/accuracy': 0.8153699040412903, 'train/loss': 0.6547273993492126, 'validation/accuracy': 0.7078199982643127, 'validation/loss': 1.1856532096862793, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8801075220108032, 'test/num_examples': 10000, 'score': 41347.19075655937, 'total_duration': 42819.61029410362, 'accumulated_submission_time': 41347.19075655937, 'accumulated_eval_time': 1463.6995224952698, 'accumulated_logging_time': 4.385130882263184, 'global_step': 121894, 'preemption_count': 0}), (123400, {'train/accuracy': 0.8108059167861938, 'train/loss': 0.6760615706443787, 'validation/accuracy': 0.704539954662323, 'validation/loss': 1.2123322486877441, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9420335292816162, 'test/num_examples': 10000, 'score': 41857.216725349426, 'total_duration': 43347.604243040085, 'accumulated_submission_time': 41857.216725349426, 'accumulated_eval_time': 1481.5602872371674, 'accumulated_logging_time': 4.439347743988037, 'global_step': 123400, 'preemption_count': 0}), (124906, {'train/accuracy': 0.8531568646430969, 'train/loss': 0.5148501992225647, 'validation/accuracy': 0.7152799963951111, 'validation/loss': 1.1633607149124146, 'validation/num_examples': 50000, 'test/accuracy': 0.591200053691864, 'test/loss': 1.8790547847747803, 'test/num_examples': 10000, 'score': 42367.14495229721, 'total_duration': 43875.18793177605, 'accumulated_submission_time': 42367.14495229721, 'accumulated_eval_time': 1499.1123352050781, 'accumulated_logging_time': 4.4895360469818115, 'global_step': 124906, 'preemption_count': 0}), (126412, {'train/accuracy': 0.8422951102256775, 'train/loss': 0.5681675672531128, 'validation/accuracy': 0.7170599699020386, 'validation/loss': 1.1488673686981201, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8772876262664795, 'test/num_examples': 10000, 'score': 42877.105335474014, 'total_duration': 44402.84032511711, 'accumulated_submission_time': 42877.105335474014, 'accumulated_eval_time': 1516.701696395874, 'accumulated_logging_time': 4.539835691452026, 'global_step': 126412, 'preemption_count': 0}), (127918, {'train/accuracy': 0.8375916481018066, 'train/loss': 0.573846697807312, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.1557798385620117, 'validation/num_examples': 50000, 'test/accuracy': 0.5874000191688538, 'test/loss': 1.8768752813339233, 'test/num_examples': 10000, 'score': 43387.303060531616, 'total_duration': 44931.63086462021, 'accumulated_submission_time': 43387.303060531616, 'accumulated_eval_time': 1535.1918017864227, 'accumulated_logging_time': 4.589509010314941, 'global_step': 127918, 'preemption_count': 0}), (129424, {'train/accuracy': 0.8389867544174194, 'train/loss': 0.5688159465789795, 'validation/accuracy': 0.7219199538230896, 'validation/loss': 1.1433396339416504, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.8539307117462158, 'test/num_examples': 10000, 'score': 43897.30212020874, 'total_duration': 45459.47427010536, 'accumulated_submission_time': 43897.30212020874, 'accumulated_eval_time': 1552.9417910575867, 'accumulated_logging_time': 4.631393909454346, 'global_step': 129424, 'preemption_count': 0}), (130930, {'train/accuracy': 0.8404615521430969, 'train/loss': 0.56621253490448, 'validation/accuracy': 0.7183199524879456, 'validation/loss': 1.1442562341690063, 'validation/num_examples': 50000, 'test/accuracy': 0.5950000286102295, 'test/loss': 1.887262225151062, 'test/num_examples': 10000, 'score': 44407.29587292671, 'total_duration': 45987.14557003975, 'accumulated_submission_time': 44407.29587292671, 'accumulated_eval_time': 1570.5166273117065, 'accumulated_logging_time': 4.68116021156311, 'global_step': 130930, 'preemption_count': 0}), (132436, {'train/accuracy': 0.8365752100944519, 'train/loss': 0.582433819770813, 'validation/accuracy': 0.7178199887275696, 'validation/loss': 1.1573113203048706, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8838881254196167, 'test/num_examples': 10000, 'score': 44917.199598789215, 'total_duration': 46514.73631954193, 'accumulated_submission_time': 44917.199598789215, 'accumulated_eval_time': 1588.0953686237335, 'accumulated_logging_time': 4.735187530517578, 'global_step': 132436, 'preemption_count': 0}), (133942, {'train/accuracy': 0.8755381107330322, 'train/loss': 0.43746116757392883, 'validation/accuracy': 0.7226799726486206, 'validation/loss': 1.1405651569366455, 'validation/num_examples': 50000, 'test/accuracy': 0.596500039100647, 'test/loss': 1.8780877590179443, 'test/num_examples': 10000, 'score': 45427.28427886963, 'total_duration': 47043.453533411026, 'accumulated_submission_time': 45427.28427886963, 'accumulated_eval_time': 1606.6192646026611, 'accumulated_logging_time': 4.7905237674713135, 'global_step': 133942, 'preemption_count': 0}), (135448, {'train/accuracy': 0.8627630472183228, 'train/loss': 0.4806753098964691, 'validation/accuracy': 0.7238199710845947, 'validation/loss': 1.131712555885315, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8806487321853638, 'test/num_examples': 10000, 'score': 45937.331146240234, 'total_duration': 47571.43300700188, 'accumulated_submission_time': 45937.331146240234, 'accumulated_eval_time': 1624.4392714500427, 'accumulated_logging_time': 4.849410057067871, 'global_step': 135448, 'preemption_count': 0}), (136954, {'train/accuracy': 0.8583984375, 'train/loss': 0.4998326003551483, 'validation/accuracy': 0.7249599695205688, 'validation/loss': 1.131963849067688, 'validation/num_examples': 50000, 'test/accuracy': 0.597000002861023, 'test/loss': 1.8573275804519653, 'test/num_examples': 10000, 'score': 46447.43666052818, 'total_duration': 48099.66242289543, 'accumulated_submission_time': 46447.43666052818, 'accumulated_eval_time': 1642.457855463028, 'accumulated_logging_time': 4.900918006896973, 'global_step': 136954, 'preemption_count': 0}), (138459, {'train/accuracy': 0.859773576259613, 'train/loss': 0.48492223024368286, 'validation/accuracy': 0.7234799861907959, 'validation/loss': 1.1349554061889648, 'validation/num_examples': 50000, 'test/accuracy': 0.5968000292778015, 'test/loss': 1.881799340248108, 'test/num_examples': 10000, 'score': 46957.421921014786, 'total_duration': 48627.312551259995, 'accumulated_submission_time': 46957.421921014786, 'accumulated_eval_time': 1660.0084781646729, 'accumulated_logging_time': 4.961538314819336, 'global_step': 138459, 'preemption_count': 0}), (139966, {'train/accuracy': 0.8603116869926453, 'train/loss': 0.486088365316391, 'validation/accuracy': 0.7274799942970276, 'validation/loss': 1.1127536296844482, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.8506639003753662, 'test/num_examples': 10000, 'score': 47467.603432655334, 'total_duration': 49155.43559336662, 'accumulated_submission_time': 47467.603432655334, 'accumulated_eval_time': 1677.843991279602, 'accumulated_logging_time': 5.014599561691284, 'global_step': 139966, 'preemption_count': 0}), (141472, {'train/accuracy': 0.8621651530265808, 'train/loss': 0.47984156012535095, 'validation/accuracy': 0.7297399640083313, 'validation/loss': 1.1351284980773926, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.8711603879928589, 'test/num_examples': 10000, 'score': 47977.597479343414, 'total_duration': 49683.41357302666, 'accumulated_submission_time': 47977.597479343414, 'accumulated_eval_time': 1695.720237493515, 'accumulated_logging_time': 5.068366050720215, 'global_step': 141472, 'preemption_count': 0}), (142978, {'train/accuracy': 0.8984972834587097, 'train/loss': 0.35278594493865967, 'validation/accuracy': 0.7320599555969238, 'validation/loss': 1.1209580898284912, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.8868118524551392, 'test/num_examples': 10000, 'score': 48487.5690472126, 'total_duration': 50211.66066980362, 'accumulated_submission_time': 48487.5690472126, 'accumulated_eval_time': 1713.8870961666107, 'accumulated_logging_time': 5.12295126914978, 'global_step': 142978, 'preemption_count': 0}), (144484, {'train/accuracy': 0.8881736397743225, 'train/loss': 0.3926560580730438, 'validation/accuracy': 0.7305600047111511, 'validation/loss': 1.1178728342056274, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.855978012084961, 'test/num_examples': 10000, 'score': 48997.5791516304, 'total_duration': 50739.464485645294, 'accumulated_submission_time': 48997.5791516304, 'accumulated_eval_time': 1731.585001707077, 'accumulated_logging_time': 5.165935516357422, 'global_step': 144484, 'preemption_count': 0}), (145989, {'train/accuracy': 0.8858019709587097, 'train/loss': 0.39276427030563354, 'validation/accuracy': 0.7334799766540527, 'validation/loss': 1.106014609336853, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.8426967859268188, 'test/num_examples': 10000, 'score': 49507.801383018494, 'total_duration': 51267.6076464653, 'accumulated_submission_time': 49507.801383018494, 'accumulated_eval_time': 1749.3989126682281, 'accumulated_logging_time': 5.220576286315918, 'global_step': 145989, 'preemption_count': 0}), (147495, {'train/accuracy': 0.8859414458274841, 'train/loss': 0.3913815915584564, 'validation/accuracy': 0.7366200089454651, 'validation/loss': 1.1027542352676392, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.8579843044281006, 'test/num_examples': 10000, 'score': 50017.77842974663, 'total_duration': 51795.50549149513, 'accumulated_submission_time': 50017.77842974663, 'accumulated_eval_time': 1767.210108757019, 'accumulated_logging_time': 5.27645468711853, 'global_step': 147495, 'preemption_count': 0}), (149001, {'train/accuracy': 0.8880141973495483, 'train/loss': 0.386345237493515, 'validation/accuracy': 0.735319972038269, 'validation/loss': 1.1073721647262573, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.8357858657836914, 'test/num_examples': 10000, 'score': 50527.860815286636, 'total_duration': 52323.51455950737, 'accumulated_submission_time': 50527.860815286636, 'accumulated_eval_time': 1785.0233714580536, 'accumulated_logging_time': 5.3356263637542725, 'global_step': 149001, 'preemption_count': 0}), (150507, {'train/accuracy': 0.8951889276504517, 'train/loss': 0.3608154356479645, 'validation/accuracy': 0.7378399968147278, 'validation/loss': 1.096303939819336, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.8414394855499268, 'test/num_examples': 10000, 'score': 51037.921318769455, 'total_duration': 52851.49748301506, 'accumulated_submission_time': 51037.921318769455, 'accumulated_eval_time': 1802.831404209137, 'accumulated_logging_time': 5.39626669883728, 'global_step': 150507, 'preemption_count': 0}), (152013, {'train/accuracy': 0.9246850609779358, 'train/loss': 0.2611102759838104, 'validation/accuracy': 0.7400599718093872, 'validation/loss': 1.1027374267578125, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.8491344451904297, 'test/num_examples': 10000, 'score': 51547.98945236206, 'total_duration': 53379.18301439285, 'accumulated_submission_time': 51547.98945236206, 'accumulated_eval_time': 1820.3468084335327, 'accumulated_logging_time': 5.445066690444946, 'global_step': 152013, 'preemption_count': 0}), (153519, {'train/accuracy': 0.9136838316917419, 'train/loss': 0.2980218827724457, 'validation/accuracy': 0.7379800081253052, 'validation/loss': 1.1061500310897827, 'validation/num_examples': 50000, 'test/accuracy': 0.6139000058174133, 'test/loss': 1.8494164943695068, 'test/num_examples': 10000, 'score': 52058.14172434807, 'total_duration': 53907.217074632645, 'accumulated_submission_time': 52058.14172434807, 'accumulated_eval_time': 1838.1192009449005, 'accumulated_logging_time': 5.501033782958984, 'global_step': 153519, 'preemption_count': 0}), (155025, {'train/accuracy': 0.9156568646430969, 'train/loss': 0.2907183766365051, 'validation/accuracy': 0.7414599657058716, 'validation/loss': 1.100637435913086, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.840550422668457, 'test/num_examples': 10000, 'score': 52568.189259290695, 'total_duration': 54435.08118414879, 'accumulated_submission_time': 52568.189259290695, 'accumulated_eval_time': 1855.824599981308, 'accumulated_logging_time': 5.557091951370239, 'global_step': 155025, 'preemption_count': 0}), (156531, {'train/accuracy': 0.9124282598495483, 'train/loss': 0.294901579618454, 'validation/accuracy': 0.7405799627304077, 'validation/loss': 1.1055474281311035, 'validation/num_examples': 50000, 'test/accuracy': 0.6130000352859497, 'test/loss': 1.8647652864456177, 'test/num_examples': 10000, 'score': 53078.3854739666, 'total_duration': 54962.95487737656, 'accumulated_submission_time': 53078.3854739666, 'accumulated_eval_time': 1873.3894336223602, 'accumulated_logging_time': 5.6150617599487305, 'global_step': 156531, 'preemption_count': 0}), (158037, {'train/accuracy': 0.91796875, 'train/loss': 0.2793106138706207, 'validation/accuracy': 0.7449199557304382, 'validation/loss': 1.0848394632339478, 'validation/num_examples': 50000, 'test/accuracy': 0.6182000041007996, 'test/loss': 1.8408807516098022, 'test/num_examples': 10000, 'score': 53588.38181400299, 'total_duration': 55491.00404858589, 'accumulated_submission_time': 53588.38181400299, 'accumulated_eval_time': 1891.3197746276855, 'accumulated_logging_time': 5.684006452560425, 'global_step': 158037, 'preemption_count': 0}), (159543, {'train/accuracy': 0.9270168542861938, 'train/loss': 0.25450384616851807, 'validation/accuracy': 0.7486400008201599, 'validation/loss': 1.0716850757598877, 'validation/num_examples': 50000, 'test/accuracy': 0.6185000538825989, 'test/loss': 1.8318090438842773, 'test/num_examples': 10000, 'score': 54098.502118587494, 'total_duration': 56018.906514406204, 'accumulated_submission_time': 54098.502118587494, 'accumulated_eval_time': 1909.0005717277527, 'accumulated_logging_time': 5.731633424758911, 'global_step': 159543, 'preemption_count': 0}), (161048, {'train/accuracy': 0.9447743892669678, 'train/loss': 0.20134520530700684, 'validation/accuracy': 0.7463200092315674, 'validation/loss': 1.0841114521026611, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.8436534404754639, 'test/num_examples': 10000, 'score': 54608.54653549194, 'total_duration': 56546.94776725769, 'accumulated_submission_time': 54608.54653549194, 'accumulated_eval_time': 1926.8864195346832, 'accumulated_logging_time': 5.78792929649353, 'global_step': 161048, 'preemption_count': 0}), (162553, {'train/accuracy': 0.9421635866165161, 'train/loss': 0.20348194241523743, 'validation/accuracy': 0.7483599781990051, 'validation/loss': 1.0738941431045532, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.835035800933838, 'test/num_examples': 10000, 'score': 55118.473893880844, 'total_duration': 57074.64824795723, 'accumulated_submission_time': 55118.473893880844, 'accumulated_eval_time': 1944.548994064331, 'accumulated_logging_time': 5.8435704708099365, 'global_step': 162553, 'preemption_count': 0}), (164059, {'train/accuracy': 0.9382772445678711, 'train/loss': 0.2111508697271347, 'validation/accuracy': 0.7485199570655823, 'validation/loss': 1.0775885581970215, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8518248796463013, 'test/num_examples': 10000, 'score': 55628.56163620949, 'total_duration': 57602.503945589066, 'accumulated_submission_time': 55628.56163620949, 'accumulated_eval_time': 1962.2046110630035, 'accumulated_logging_time': 5.903212070465088, 'global_step': 164059, 'preemption_count': 0}), (165565, {'train/accuracy': 0.9431201815605164, 'train/loss': 0.20247571170330048, 'validation/accuracy': 0.7490999698638916, 'validation/loss': 1.0750706195831299, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.850699543952942, 'test/num_examples': 10000, 'score': 56138.76058793068, 'total_duration': 58130.41319346428, 'accumulated_submission_time': 56138.76058793068, 'accumulated_eval_time': 1979.8018288612366, 'accumulated_logging_time': 5.963002681732178, 'global_step': 165565, 'preemption_count': 0}), (167070, {'train/accuracy': 0.9422233700752258, 'train/loss': 0.20181764662265778, 'validation/accuracy': 0.7506600022315979, 'validation/loss': 1.0700610876083374, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.8532116413116455, 'test/num_examples': 10000, 'score': 56648.743233919144, 'total_duration': 58658.53840446472, 'accumulated_submission_time': 56648.743233919144, 'accumulated_eval_time': 1997.844202041626, 'accumulated_logging_time': 6.011024236679077, 'global_step': 167070, 'preemption_count': 0}), (168576, {'train/accuracy': 0.9480428695678711, 'train/loss': 0.18233726918697357, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 1.0641475915908813, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.8473761081695557, 'test/num_examples': 10000, 'score': 57158.817729473114, 'total_duration': 59186.4742205143, 'accumulated_submission_time': 57158.817729473114, 'accumulated_eval_time': 2015.589801311493, 'accumulated_logging_time': 6.071972846984863, 'global_step': 168576, 'preemption_count': 0}), (170082, {'train/accuracy': 0.9587252736091614, 'train/loss': 0.15863607823848724, 'validation/accuracy': 0.7521199584007263, 'validation/loss': 1.072721004486084, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.8547792434692383, 'test/num_examples': 10000, 'score': 57668.98382782936, 'total_duration': 59714.29183888435, 'accumulated_submission_time': 57668.98382782936, 'accumulated_eval_time': 2033.127522945404, 'accumulated_logging_time': 6.13216757774353, 'global_step': 170082, 'preemption_count': 0}), (171588, {'train/accuracy': 0.9563336968421936, 'train/loss': 0.16189979016780853, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.0672721862792969, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.851597547531128, 'test/num_examples': 10000, 'score': 58179.04970908165, 'total_duration': 60242.267706632614, 'accumulated_submission_time': 58179.04970908165, 'accumulated_eval_time': 2050.924390077591, 'accumulated_logging_time': 6.191601753234863, 'global_step': 171588, 'preemption_count': 0}), (173094, {'train/accuracy': 0.9559350609779358, 'train/loss': 0.16148580610752106, 'validation/accuracy': 0.7530199885368347, 'validation/loss': 1.0638294219970703, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.836564540863037, 'test/num_examples': 10000, 'score': 58689.24033522606, 'total_duration': 60771.73478055, 'accumulated_submission_time': 58689.24033522606, 'accumulated_eval_time': 2070.0848445892334, 'accumulated_logging_time': 6.254953384399414, 'global_step': 173094, 'preemption_count': 0}), (174599, {'train/accuracy': 0.9558952450752258, 'train/loss': 0.1617143154144287, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 1.0610108375549316, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8357188701629639, 'test/num_examples': 10000, 'score': 59199.163927316666, 'total_duration': 61299.51197075844, 'accumulated_submission_time': 59199.163927316666, 'accumulated_eval_time': 2087.834163427353, 'accumulated_logging_time': 6.306419849395752, 'global_step': 174599, 'preemption_count': 0}), (176104, {'train/accuracy': 0.9573700428009033, 'train/loss': 0.15628387033939362, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0589247941970825, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8355635404586792, 'test/num_examples': 10000, 'score': 59709.09991669655, 'total_duration': 61827.27109313011, 'accumulated_submission_time': 59709.09991669655, 'accumulated_eval_time': 2105.543501853943, 'accumulated_logging_time': 6.366549730300903, 'global_step': 176104, 'preemption_count': 0}), (177609, {'train/accuracy': 0.9576889276504517, 'train/loss': 0.15669949352741241, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.05818510055542, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8384114503860474, 'test/num_examples': 10000, 'score': 60219.06253504753, 'total_duration': 62355.04538965225, 'accumulated_submission_time': 60219.06253504753, 'accumulated_eval_time': 2123.2411789894104, 'accumulated_logging_time': 6.4263880252838135, 'global_step': 177609, 'preemption_count': 0}), (179114, {'train/accuracy': 0.9618940949440002, 'train/loss': 0.14577092230319977, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.0542857646942139, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.8273247480392456, 'test/num_examples': 10000, 'score': 60729.021720170975, 'total_duration': 62882.94174575806, 'accumulated_submission_time': 60729.021720170975, 'accumulated_eval_time': 2141.0598311424255, 'accumulated_logging_time': 6.489727973937988, 'global_step': 179114, 'preemption_count': 0}), (180620, {'train/accuracy': 0.9607979655265808, 'train/loss': 0.14520682394504547, 'validation/accuracy': 0.7542200088500977, 'validation/loss': 1.0534926652908325, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.829865574836731, 'test/num_examples': 10000, 'score': 61239.021084070206, 'total_duration': 63410.9137442112, 'accumulated_submission_time': 61239.021084070206, 'accumulated_eval_time': 2158.9144999980927, 'accumulated_logging_time': 6.554764747619629, 'global_step': 180620, 'preemption_count': 0}), (182125, {'train/accuracy': 0.9617745280265808, 'train/loss': 0.1454741656780243, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0536754131317139, 'validation/num_examples': 50000, 'test/accuracy': 0.6331000328063965, 'test/loss': 1.830521583557129, 'test/num_examples': 10000, 'score': 61748.976459264755, 'total_duration': 63938.778237342834, 'accumulated_submission_time': 61748.976459264755, 'accumulated_eval_time': 2176.718202829361, 'accumulated_logging_time': 6.606281042098999, 'global_step': 182125, 'preemption_count': 0}), (183630, {'train/accuracy': 0.9620137214660645, 'train/loss': 0.14126259088516235, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0554081201553345, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.831824541091919, 'test/num_examples': 10000, 'score': 62258.98048186302, 'total_duration': 64466.852714538574, 'accumulated_submission_time': 62258.98048186302, 'accumulated_eval_time': 2194.670075416565, 'accumulated_logging_time': 6.670706748962402, 'global_step': 183630, 'preemption_count': 0}), (185136, {'train/accuracy': 0.9618940949440002, 'train/loss': 0.14330562949180603, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0536677837371826, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8308967351913452, 'test/num_examples': 10000, 'score': 62769.02976322174, 'total_duration': 64994.79127025604, 'accumulated_submission_time': 62769.02976322174, 'accumulated_eval_time': 2212.4462909698486, 'accumulated_logging_time': 6.730313539505005, 'global_step': 185136, 'preemption_count': 0})], 'global_step': 185842}
I0130 13:39:01.631638 140027215431488 submission_runner.py:586] Timing: 63008.0367565155
I0130 13:39:01.631738 140027215431488 submission_runner.py:588] Total number of evals: 124
I0130 13:39:01.631780 140027215431488 submission_runner.py:589] ====================
I0130 13:39:01.633120 140027215431488 submission_runner.py:673] Final imagenet_resnet score: 62951.208920001984
